{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Nebula Graph 2.0 Documentation \u00b6 Nebula Graph is a distributed, scalable, and lightning-fast graph database. It is the optimal solution in the world capable of hosting graphs with dozens of billions of vertices (nodes) and trillions of edges with millisecond latency. Tutorial Video \u00b6 YouTube bilibili Preface \u00b6 Manual Change Log Introduction \u00b6 What is Nebula Graph Quick start (for beginners) \u00b6 Quick start workflow Deploy Nebula Graph with Docker Compose Connect to Nebula Graph Nebula Graph CRUD nGQL guide (for all users) \u00b6 Operators Comparison Pipe Reference operators Set String Precedence Functions and expressions Math String Date and time Schema Case expressions General query statements Match Space statements Create space Use space Show spaces Describe space Drop space Vertex statements Insert vertex Update vertex Upsert vertex Delete vertex Subgraph and path Get subgraph Query tuning statements Explain and profile Deployment and installation (for Developers and DBA) \u00b6 Resource preparations Compile and install Nebula Graph Install Nebula Graph by compiling the source code","title":"Welcome to Nebula Graph 2.0 Documentation"},{"location":"#welcome_to_nebula_graph_20_documentation","text":"Nebula Graph is a distributed, scalable, and lightning-fast graph database. It is the optimal solution in the world capable of hosting graphs with dozens of billions of vertices (nodes) and trillions of edges with millisecond latency.","title":"Welcome to Nebula Graph 2.0 Documentation"},{"location":"#tutorial_video","text":"YouTube bilibili","title":"Tutorial Video"},{"location":"#preface","text":"Manual Change Log","title":"Preface"},{"location":"#introduction","text":"What is Nebula Graph","title":"Introduction"},{"location":"#quick_start_for_beginners","text":"Quick start workflow Deploy Nebula Graph with Docker Compose Connect to Nebula Graph Nebula Graph CRUD","title":"Quick start (for beginners)"},{"location":"#ngql_guide_for_all_users","text":"Operators Comparison Pipe Reference operators Set String Precedence Functions and expressions Math String Date and time Schema Case expressions General query statements Match Space statements Create space Use space Show spaces Describe space Drop space Vertex statements Insert vertex Update vertex Upsert vertex Delete vertex Subgraph and path Get subgraph Query tuning statements Explain and profile","title":"nGQL guide (for all users)"},{"location":"#deployment_and_installation_for_developers_and_dba","text":"Resource preparations Compile and install Nebula Graph Install Nebula Graph by compiling the source code","title":"Deployment and installation (for Developers and DBA)"},{"location":"CHANGELOG/","text":"Manual Changes \u00b6 0.1.1 - Initial release Nebula Graph alpha","title":"Manual Changes"},{"location":"CHANGELOG/#manual_changes","text":"0.1.1 - Initial release Nebula Graph alpha","title":"Manual Changes"},{"location":"1.introduction/1.what-is-nebula-graph/","text":"What is Nebula Graph \u00b6 Nebula Graph is an open-source, distributed, easily scalable, and native graph database. It is capable of hosting graphs with billions of vertices and trillions of edges, and serving queries with millisecond-latency. What is a graph database \u00b6 A graph database, such as Nebula Graph, is a database that specializes in storing vast graph networks and retrieving information from them. It efficiently stores data as vertices (nodes) and edges (relationships) in labeled property graphs. Properties can be attached to both vertices and edges. Each vertex can have one or multiple tags (labels). Graph databases are well suited for storing most kinds of data models abstracted from reality. Things are connected in almost all fields in the world. Modeling systems like relational databases extract the relationships between entities and squeeze them into table columns alone, with their types and properties stored in other columns or even other tables. This makes the data management time-consuming and cost-ineffective. Nebula Graph, as a typical native graph database, allows you to store the rich relationships as edges with edge types and properties directly attached to them. Benefits of Nebula Graph \u00b6 Open-source \u00b6 Nebula Graph is open under the Apache 2.0 and the Commons Clause 1.0 licenses. More and more people such as database developers, data scientists, security experts, and algorithm engineers are participating in the designing and development of Nebula Graph. To join the opening of source code and ideas, surf the Nebula Graph GitHub page . Outstanding performance \u00b6 Written in C++ and born for graph, Nebula Graph handles graph queries in milliseconds. Among most databases, Nebula Graph shows superior performance in providing graph data services. The larger the data size, the greater the superiority of Nebula Graph. For more information, see Nebula Graph benchmarking . Developer friendly \u00b6 Nebula Graph supports clients in popular programming languages like Java, Python, C++, and Go, and more are being developed. For more information, see Nebula Graph clients . Diversified ecosystem \u00b6 More and more native tools of Nebula Graph have been released, such as Nebula Graph Studio , nebula-console , and Nebula Graph Exchange . Besides, Nebula Graph has the ability to be integrated with many cutting-edge technologies, such as Spark, Flink, and HBase, for the purpose of mutual strengthening in a world of increasing challenges and chances. For more information, see Ecosystem development . OpenCypher-compatible query language \u00b6 The native Nebula Graph Query Language, also known as nGQL, is a declarative, openCypher-compatible textual query language. It is easy to understand and easy to use. For more information, see nGQL guide . Easy data modeling and high flexibility \u00b6 You can easily model the connected data into Nebula Graph for your business without forcing them into a structure such as a relational table, and properties can be added, updated, and deleted freely. For more information, see Data modeling . Reliable access control \u00b6 Nebula Graph supports strict role-based access control and external authentication servers such as LDAP (Lightweight Directory Access Protocol) servers to enhance data security. For more information, see Authentication and authorization . High scalability \u00b6 Nebula Graph is designed in a shared-nothing architecture and supports scaling in and out without interrupting the database service. High popularity \u00b6 Nebula Graph is being used by tech leaders such as Tencent, Vivo, Meituan, and JD Digits. For more information, visit the Nebula Graph official website . Use cases \u00b6 Nebula Graph can be used to support various graph-based scenarios. To spare the time spent on pushing the kinds of data mentioned in this section into relational databases and on bothering with join queries, use Nebula Graph. Fraud detection \u00b6 Financial institutions have to traverse countless transactions to piece together potential crimes and understand how combinations of transactions and devices might be related to a single fraud scheme. This kind of scenario can be modeled in graphs, and with the help of Nebula Graph, fraud rings and other sophisticated scams can be easily detected. Real-time recommendation \u00b6 Nebula Graph offers the ability to instantly process the real-time information produced by a visitor and make accurate recommendations on articles, videos, products, and services. Intelligent question-answer system \u00b6 Natural languages can be transformed into knowledge graphs and stored in Nebula Graph. A question organized in a natural language can be resolved by a semantic parser in an intelligent question-answer system and re-organized. Then, possible answers to the question can be retrieved from the knowledge graph and provided to the one who asked the question. Social networking \u00b6 Information on people and their relationships are typical graph data. Nebula Graph can easily handle the social networking information of billions of people and trillions of relationships, and provide lightning-fast queries for friend recommendations and job promotions in the case of massive concurrency.","title":"What is Nebula Graph"},{"location":"1.introduction/1.what-is-nebula-graph/#what_is_nebula_graph","text":"Nebula Graph is an open-source, distributed, easily scalable, and native graph database. It is capable of hosting graphs with billions of vertices and trillions of edges, and serving queries with millisecond-latency.","title":"What is Nebula Graph"},{"location":"1.introduction/1.what-is-nebula-graph/#what_is_a_graph_database","text":"A graph database, such as Nebula Graph, is a database that specializes in storing vast graph networks and retrieving information from them. It efficiently stores data as vertices (nodes) and edges (relationships) in labeled property graphs. Properties can be attached to both vertices and edges. Each vertex can have one or multiple tags (labels). Graph databases are well suited for storing most kinds of data models abstracted from reality. Things are connected in almost all fields in the world. Modeling systems like relational databases extract the relationships between entities and squeeze them into table columns alone, with their types and properties stored in other columns or even other tables. This makes the data management time-consuming and cost-ineffective. Nebula Graph, as a typical native graph database, allows you to store the rich relationships as edges with edge types and properties directly attached to them.","title":"What is a graph database"},{"location":"1.introduction/1.what-is-nebula-graph/#benefits_of_nebula_graph","text":"","title":"Benefits of Nebula Graph"},{"location":"1.introduction/1.what-is-nebula-graph/#open-source","text":"Nebula Graph is open under the Apache 2.0 and the Commons Clause 1.0 licenses. More and more people such as database developers, data scientists, security experts, and algorithm engineers are participating in the designing and development of Nebula Graph. To join the opening of source code and ideas, surf the Nebula Graph GitHub page .","title":"Open-source"},{"location":"1.introduction/1.what-is-nebula-graph/#outstanding_performance","text":"Written in C++ and born for graph, Nebula Graph handles graph queries in milliseconds. Among most databases, Nebula Graph shows superior performance in providing graph data services. The larger the data size, the greater the superiority of Nebula Graph. For more information, see Nebula Graph benchmarking .","title":"Outstanding performance"},{"location":"1.introduction/1.what-is-nebula-graph/#developer_friendly","text":"Nebula Graph supports clients in popular programming languages like Java, Python, C++, and Go, and more are being developed. For more information, see Nebula Graph clients .","title":"Developer friendly"},{"location":"1.introduction/1.what-is-nebula-graph/#diversified_ecosystem","text":"More and more native tools of Nebula Graph have been released, such as Nebula Graph Studio , nebula-console , and Nebula Graph Exchange . Besides, Nebula Graph has the ability to be integrated with many cutting-edge technologies, such as Spark, Flink, and HBase, for the purpose of mutual strengthening in a world of increasing challenges and chances. For more information, see Ecosystem development .","title":"Diversified ecosystem"},{"location":"1.introduction/1.what-is-nebula-graph/#opencypher-compatible_query_language","text":"The native Nebula Graph Query Language, also known as nGQL, is a declarative, openCypher-compatible textual query language. It is easy to understand and easy to use. For more information, see nGQL guide .","title":"OpenCypher-compatible query language"},{"location":"1.introduction/1.what-is-nebula-graph/#easy_data_modeling_and_high_flexibility","text":"You can easily model the connected data into Nebula Graph for your business without forcing them into a structure such as a relational table, and properties can be added, updated, and deleted freely. For more information, see Data modeling .","title":"Easy data modeling and high flexibility"},{"location":"1.introduction/1.what-is-nebula-graph/#reliable_access_control","text":"Nebula Graph supports strict role-based access control and external authentication servers such as LDAP (Lightweight Directory Access Protocol) servers to enhance data security. For more information, see Authentication and authorization .","title":"Reliable access control"},{"location":"1.introduction/1.what-is-nebula-graph/#high_scalability","text":"Nebula Graph is designed in a shared-nothing architecture and supports scaling in and out without interrupting the database service.","title":"High scalability"},{"location":"1.introduction/1.what-is-nebula-graph/#high_popularity","text":"Nebula Graph is being used by tech leaders such as Tencent, Vivo, Meituan, and JD Digits. For more information, visit the Nebula Graph official website .","title":"High popularity"},{"location":"1.introduction/1.what-is-nebula-graph/#use_cases","text":"Nebula Graph can be used to support various graph-based scenarios. To spare the time spent on pushing the kinds of data mentioned in this section into relational databases and on bothering with join queries, use Nebula Graph.","title":"Use cases"},{"location":"1.introduction/1.what-is-nebula-graph/#fraud_detection","text":"Financial institutions have to traverse countless transactions to piece together potential crimes and understand how combinations of transactions and devices might be related to a single fraud scheme. This kind of scenario can be modeled in graphs, and with the help of Nebula Graph, fraud rings and other sophisticated scams can be easily detected.","title":"Fraud detection"},{"location":"1.introduction/1.what-is-nebula-graph/#real-time_recommendation","text":"Nebula Graph offers the ability to instantly process the real-time information produced by a visitor and make accurate recommendations on articles, videos, products, and services.","title":"Real-time recommendation"},{"location":"1.introduction/1.what-is-nebula-graph/#intelligent_question-answer_system","text":"Natural languages can be transformed into knowledge graphs and stored in Nebula Graph. A question organized in a natural language can be resolved by a semantic parser in an intelligent question-answer system and re-organized. Then, possible answers to the question can be retrieved from the knowledge graph and provided to the one who asked the question.","title":"Intelligent question-answer system"},{"location":"1.introduction/1.what-is-nebula-graph/#social_networking","text":"Information on people and their relationships are typical graph data. Nebula Graph can easily handle the social networking information of billions of people and trillions of relationships, and provide lightning-fast queries for friend recommendations and job promotions in the case of massive concurrency.","title":"Social networking"},{"location":"1.introduction/2.data-model/","text":"Data modeling \u00b6 A data model is a model that organizes data and specifies how they are related to one another. This topic describes the Nebula Graph data model and provides suggestions for data modeling with Nebula Graph. Data structures \u00b6 Nebula Graph data model uses five data structures to store data. They are vertices, edges, properties, tags, and edge types. Vertices : Vertices are used to store entities. In Nebula Graph, vertices are identified with vertex identifiers (i.e. VID ). The VID must be unique in the same graph space. A vertex must have at least one tag. Edges : Edges are used to connect vertices. An edge is a connection or behavior between two vertices. An edge is identified uniquely with a source vertex, an edge type, a rank value, and a destination vertex. Edges are directed. -> identifies the directions of edges. Edges can be traversed in either direction. An edge must have one and only one edge type. The rank value is an immutable user-assigned 64-bit signed integer. It identifies the edges with the same edge type between two vertices. Edges are sorted by their rank values. The edge with the greatest rank value is listed first. The default rank value is zero. Properties : Properties are key-value pairs. Both vertices and edges are containers for properties. Tags : Tags are used to categorize vertices. Vertices that have the same tag share the same definition of properties. Edge types : Edge types are used to categorize edges. Edges that have the same edge type share the same definition of properties. Directed property graph \u00b6 Nebula Graph stores data in directed property graphs. A directed property graph has a set of vertices connected by edges. And the edges have directions. A directed property graph is represented as: G = < V, E, P V , P E > V is a set of vertices. E is a set of directed edges. P V is the property of vertices. P E is the property of edges. We use the example graph below to introduce the basic concepts of property graph: To introduce the proceeding concepts, we use the following graph: This picture shows a data set about the players and teams of NBA. We have two types of vertices, that is player and team , and two types of edges, that is serve and like . This table gives detailed information on the sample data set. Element Name Property name (Data type) Description Tag player name (string) age (int) Represents players in the NBA. Tag team name (string) Represents the teams in the NBA. Edge type serve start_year (int) end_year (int) Represents actions taken by players in the NBA. An action links a player and a team and the direction is from a player to a team. Edge type like likeness (int) Represents actions taken by players in the NBA. An action links a player and another player and the direction is from one player to the other player. Graph data modeling suggestions \u00b6 This section provides general suggestions for modeling data in Nebula Graph. NOTE: The following suggestions may not apply to some special scenarios. In these cases, find help in the Nebula Graph community . Model for performance \u00b6 There is no perfect method to model in Nebula Graph. Graph modeling depends on the questions that you want to know from the data. Your data drives your graph model. Graph data modeling is intuitive and convenient. Create your data model based on your business model. Test your model and gradually optimize it to fit your business. To get better performance, you can change or re-design your model multiple times. Edges as properties \u00b6 Traversal depth decreases the traversal performance. To decrease the traversal depth, use vertex properties instead of edges. For example, to model a graph that have the name, age, and eye color elements, you can: (RECOMMENDED) Create a tag person , then add the name, age, and eye color as its properties. (WRONG WAY) Create a new tag eye color and a new edge type has , then create an edge to indicate that a person has an eye color. The first modeling solution leads to much better performance. DO NOT use the second solution unless you have to. Multiple properties under one tag are permitted. But make sure that tags are fine-grained. For more information, see the Granulated vertices section. Granulated vertices \u00b6 In graph modeling, use the data models with a higher level of granularity. Put a set of parallel properties into one tag, i.e., separate different concepts. Use indexes correctly \u00b6 Correct use of indexes speeds up queries, but indexes reduce the write performance by 90% or more. ONLY use indexes when you locate vertices or edges by their properties. No long string properties on edges \u00b6 Be careful when you create long string properties for edges. Nebula Graph supports storing such properties on edges. But note that these properties are stored both in the outgoing edges and the incoming edges. Thus be careful with the write amplification.","title":"Date model"},{"location":"1.introduction/2.data-model/#data_modeling","text":"A data model is a model that organizes data and specifies how they are related to one another. This topic describes the Nebula Graph data model and provides suggestions for data modeling with Nebula Graph.","title":"Data modeling"},{"location":"1.introduction/2.data-model/#data_structures","text":"Nebula Graph data model uses five data structures to store data. They are vertices, edges, properties, tags, and edge types. Vertices : Vertices are used to store entities. In Nebula Graph, vertices are identified with vertex identifiers (i.e. VID ). The VID must be unique in the same graph space. A vertex must have at least one tag. Edges : Edges are used to connect vertices. An edge is a connection or behavior between two vertices. An edge is identified uniquely with a source vertex, an edge type, a rank value, and a destination vertex. Edges are directed. -> identifies the directions of edges. Edges can be traversed in either direction. An edge must have one and only one edge type. The rank value is an immutable user-assigned 64-bit signed integer. It identifies the edges with the same edge type between two vertices. Edges are sorted by their rank values. The edge with the greatest rank value is listed first. The default rank value is zero. Properties : Properties are key-value pairs. Both vertices and edges are containers for properties. Tags : Tags are used to categorize vertices. Vertices that have the same tag share the same definition of properties. Edge types : Edge types are used to categorize edges. Edges that have the same edge type share the same definition of properties.","title":"Data structures"},{"location":"1.introduction/2.data-model/#directed_property_graph","text":"Nebula Graph stores data in directed property graphs. A directed property graph has a set of vertices connected by edges. And the edges have directions. A directed property graph is represented as: G = < V, E, P V , P E > V is a set of vertices. E is a set of directed edges. P V is the property of vertices. P E is the property of edges. We use the example graph below to introduce the basic concepts of property graph: To introduce the proceeding concepts, we use the following graph: This picture shows a data set about the players and teams of NBA. We have two types of vertices, that is player and team , and two types of edges, that is serve and like . This table gives detailed information on the sample data set. Element Name Property name (Data type) Description Tag player name (string) age (int) Represents players in the NBA. Tag team name (string) Represents the teams in the NBA. Edge type serve start_year (int) end_year (int) Represents actions taken by players in the NBA. An action links a player and a team and the direction is from a player to a team. Edge type like likeness (int) Represents actions taken by players in the NBA. An action links a player and another player and the direction is from one player to the other player.","title":"Directed property graph"},{"location":"1.introduction/2.data-model/#graph_data_modeling_suggestions","text":"This section provides general suggestions for modeling data in Nebula Graph. NOTE: The following suggestions may not apply to some special scenarios. In these cases, find help in the Nebula Graph community .","title":"Graph data modeling suggestions"},{"location":"1.introduction/2.data-model/#model_for_performance","text":"There is no perfect method to model in Nebula Graph. Graph modeling depends on the questions that you want to know from the data. Your data drives your graph model. Graph data modeling is intuitive and convenient. Create your data model based on your business model. Test your model and gradually optimize it to fit your business. To get better performance, you can change or re-design your model multiple times.","title":"Model for performance"},{"location":"1.introduction/2.data-model/#edges_as_properties","text":"Traversal depth decreases the traversal performance. To decrease the traversal depth, use vertex properties instead of edges. For example, to model a graph that have the name, age, and eye color elements, you can: (RECOMMENDED) Create a tag person , then add the name, age, and eye color as its properties. (WRONG WAY) Create a new tag eye color and a new edge type has , then create an edge to indicate that a person has an eye color. The first modeling solution leads to much better performance. DO NOT use the second solution unless you have to. Multiple properties under one tag are permitted. But make sure that tags are fine-grained. For more information, see the Granulated vertices section.","title":"Edges as properties"},{"location":"1.introduction/2.data-model/#granulated_vertices","text":"In graph modeling, use the data models with a higher level of granularity. Put a set of parallel properties into one tag, i.e., separate different concepts.","title":"Granulated vertices"},{"location":"1.introduction/2.data-model/#use_indexes_correctly","text":"Correct use of indexes speeds up queries, but indexes reduce the write performance by 90% or more. ONLY use indexes when you locate vertices or edges by their properties.","title":"Use indexes correctly"},{"location":"1.introduction/2.data-model/#no_long_string_properties_on_edges","text":"Be careful when you create long string properties for edges. Nebula Graph supports storing such properties on edges. But note that these properties are stored both in the outgoing edges and the incoming edges. Thus be careful with the write amplification.","title":"No long string properties on edges"},{"location":"1.introduction/3.nebula-graph-architecture/1.architecture-overview/","text":"Architecture overview \u00b6 Nebula Graph consists of three services: the Graph Service, the Storage Service, and the Meta Service. Each service has its executable binaries and processes launched from the binaries. You can deploy a Nebula Graph cluster on a single machine or multiple machines using these binaries. The following figure shows the architecture of a typical Nebula Graph cluster. The Meta Service \u00b6 The Meta Service in the Nebula Graph architecture is run by the nebula-metad processes. It is responsible for metadata management, such as schema operations, cluster administration, and user privilege management. For details on the Meta Service, see Meta Service . The Graph Service and the Storage Service \u00b6 Nebula Graph applies a disaggregated storage and compute architecture. The Graph Service is responsible for querying. The Storage Service is responsible for storage. And they run on different processes, i.e., nebula-graphd and nebula-storaged. The benefits of disaggregated storage and compute are as follows: Great scalability. A disaggregated structure makes both the Graph Service and the Storage Service flexible and easy to scale in or out. High availability. If part of the Graph Service fails, the data stored by the Storage Service suffers no loss. And if the rest part of the Graph Service is still able to serve the clients, service recovery can be performed quickly, or even unfelt by the users. Cost-effective. The separation of computing and storage provides a higher resource utilization rate, and it enables you to manage the cost flexibly according to business demands. The cost savings can be more significant if you use the Nebula Graph Cloud service. Open to more possibilities. With the ability to run separately, the Graph Service may work with multiple types of storage engines, and the Storage Service may serve more types of computing engines. For details on the Graph Service and the Storage Service, see Graph Service and Storage Service .","title":"Architecture overview"},{"location":"1.introduction/3.nebula-graph-architecture/1.architecture-overview/#architecture_overview","text":"Nebula Graph consists of three services: the Graph Service, the Storage Service, and the Meta Service. Each service has its executable binaries and processes launched from the binaries. You can deploy a Nebula Graph cluster on a single machine or multiple machines using these binaries. The following figure shows the architecture of a typical Nebula Graph cluster.","title":"Architecture overview"},{"location":"1.introduction/3.nebula-graph-architecture/1.architecture-overview/#the_meta_service","text":"The Meta Service in the Nebula Graph architecture is run by the nebula-metad processes. It is responsible for metadata management, such as schema operations, cluster administration, and user privilege management. For details on the Meta Service, see Meta Service .","title":"The Meta Service"},{"location":"1.introduction/3.nebula-graph-architecture/1.architecture-overview/#the_graph_service_and_the_storage_service","text":"Nebula Graph applies a disaggregated storage and compute architecture. The Graph Service is responsible for querying. The Storage Service is responsible for storage. And they run on different processes, i.e., nebula-graphd and nebula-storaged. The benefits of disaggregated storage and compute are as follows: Great scalability. A disaggregated structure makes both the Graph Service and the Storage Service flexible and easy to scale in or out. High availability. If part of the Graph Service fails, the data stored by the Storage Service suffers no loss. And if the rest part of the Graph Service is still able to serve the clients, service recovery can be performed quickly, or even unfelt by the users. Cost-effective. The separation of computing and storage provides a higher resource utilization rate, and it enables you to manage the cost flexibly according to business demands. The cost savings can be more significant if you use the Nebula Graph Cloud service. Open to more possibilities. With the ability to run separately, the Graph Service may work with multiple types of storage engines, and the Storage Service may serve more types of computing engines. For details on the Graph Service and the Storage Service, see Graph Service and Storage Service .","title":"The Graph Service and the Storage Service"},{"location":"1.introduction/3.nebula-graph-architecture/2.meta-service/","text":"Meta Service \u00b6 This topic describes the architecture and functions of the Meta Service. The architecture of the Meta Service \u00b6 The architecture of the Meta Service is as follows. The Meta Service is run by the nebula-metad processes. You can deploy nebula-metad processes according to the scenario: In a test environment, you can deploy one or three nebula-metad processes on different machines or a single machine. In a production environment, we recommend that you deploy three processes on different machines for high availability. All the nebula-metad processes form a Raft-based cluster, with one process as the leader and the others as the followers. The leader is elected by quorum, and only the leader can provide service to the clients and other components of Nebula Graph. The followers run in a standby way and each has a data replication of the leader. Once the leader fails, one of the followers will be elected as the new leader. Functions of the Meta Service \u00b6 Manages user accounts \u00b6 The Meta Service stores the information of user accounts and the privileges granted to the accounts. When the clients send queries to the Graph Service through an account, the Graph Service checks the account information and whether the account has the right privileges to execute the queries or not. For more information on Nebula Graph access control, see Authentication and authorization . Manages partitions \u00b6 The Meta Service stores and manages the locations of the storage partitions and helps balance the partitions. Manages graph spaces \u00b6 Nebula Graph supports multiple graph spaces. Data stored in different graph spaces are securely isolated. The Meta Service stores the metadata of all graph spaces and tracks the changes of them, such as adding or dropping a graph space. Manages schema information \u00b6 Nebula Graph is a strong-typed graph database. Its schema contains tags (i.e., the vertex types), edge types, tag properties, and edge type properties. The Meta Service stores the schema information. Besides, it performs the addition, modification, and deletion of the schema, and logs the versions of them. For more information on Nebula Graph schema, see Data model . Manages TTL-based data eviction \u00b6 The Meta Service provides automatic data eviction and space reclamation based on TTL (time to live) options for Nebula Graph. For more information on TTL, see TTL options . Manages jobs \u00b6 The Job Manager module in the Meta Service is responsible for the creation, queuing, querying and deletion of jobs.","title":"Meta Service"},{"location":"1.introduction/3.nebula-graph-architecture/2.meta-service/#meta_service","text":"This topic describes the architecture and functions of the Meta Service.","title":"Meta Service"},{"location":"1.introduction/3.nebula-graph-architecture/2.meta-service/#the_architecture_of_the_meta_service","text":"The architecture of the Meta Service is as follows. The Meta Service is run by the nebula-metad processes. You can deploy nebula-metad processes according to the scenario: In a test environment, you can deploy one or three nebula-metad processes on different machines or a single machine. In a production environment, we recommend that you deploy three processes on different machines for high availability. All the nebula-metad processes form a Raft-based cluster, with one process as the leader and the others as the followers. The leader is elected by quorum, and only the leader can provide service to the clients and other components of Nebula Graph. The followers run in a standby way and each has a data replication of the leader. Once the leader fails, one of the followers will be elected as the new leader.","title":"The architecture of the Meta Service"},{"location":"1.introduction/3.nebula-graph-architecture/2.meta-service/#functions_of_the_meta_service","text":"","title":"Functions of the Meta Service"},{"location":"1.introduction/3.nebula-graph-architecture/2.meta-service/#manages_user_accounts","text":"The Meta Service stores the information of user accounts and the privileges granted to the accounts. When the clients send queries to the Graph Service through an account, the Graph Service checks the account information and whether the account has the right privileges to execute the queries or not. For more information on Nebula Graph access control, see Authentication and authorization .","title":"Manages user accounts"},{"location":"1.introduction/3.nebula-graph-architecture/2.meta-service/#manages_partitions","text":"The Meta Service stores and manages the locations of the storage partitions and helps balance the partitions.","title":"Manages partitions"},{"location":"1.introduction/3.nebula-graph-architecture/2.meta-service/#manages_graph_spaces","text":"Nebula Graph supports multiple graph spaces. Data stored in different graph spaces are securely isolated. The Meta Service stores the metadata of all graph spaces and tracks the changes of them, such as adding or dropping a graph space.","title":"Manages graph spaces"},{"location":"1.introduction/3.nebula-graph-architecture/2.meta-service/#manages_schema_information","text":"Nebula Graph is a strong-typed graph database. Its schema contains tags (i.e., the vertex types), edge types, tag properties, and edge type properties. The Meta Service stores the schema information. Besides, it performs the addition, modification, and deletion of the schema, and logs the versions of them. For more information on Nebula Graph schema, see Data model .","title":"Manages schema information"},{"location":"1.introduction/3.nebula-graph-architecture/2.meta-service/#manages_ttl-based_data_eviction","text":"The Meta Service provides automatic data eviction and space reclamation based on TTL (time to live) options for Nebula Graph. For more information on TTL, see TTL options .","title":"Manages TTL-based data eviction"},{"location":"1.introduction/3.nebula-graph-architecture/2.meta-service/#manages_jobs","text":"The Job Manager module in the Meta Service is responsible for the creation, queuing, querying and deletion of jobs.","title":"Manages jobs"},{"location":"1.introduction/3.nebula-graph-architecture/3.graph-service/","text":"Graph Service \u00b6 NOTE: Writing this topic is listed in the training plan for the next Nebula Graph Technical Writer. If you want to learn about the Graph Service, see An Introduction to Nebula Graph 2.0 Query Engine for now.","title":"Graph Service"},{"location":"1.introduction/3.nebula-graph-architecture/3.graph-service/#graph_service","text":"NOTE: Writing this topic is listed in the training plan for the next Nebula Graph Technical Writer. If you want to learn about the Graph Service, see An Introduction to Nebula Graph 2.0 Query Engine for now.","title":"Graph Service"},{"location":"1.introduction/3.nebula-graph-architecture/4.storage-service/","text":"Storage Service \u00b6 NOTE: We are using this topic in recruitment tests. So the official version of it won't be released until the end of April. Feel free to contact us if you want to join the team. You may also contribute to this topic if interested. References: An Introduction to Nebula Graph's Storage Engine Architecture overview Meta Service","title":"Storage Service"},{"location":"1.introduction/3.nebula-graph-architecture/4.storage-service/#storage_service","text":"NOTE: We are using this topic in recruitment tests. So the official version of it won't be released until the end of April. Feel free to contact us if you want to join the team. You may also contribute to this topic if interested. References: An Introduction to Nebula Graph's Storage Engine Architecture overview Meta Service","title":"Storage Service"},{"location":"15.contribution/how-to-contribute/","text":"How to Contribute \u00b6 Before you get started \u00b6 File an issue on the github or forum \u00b6 You are welcome to contribute any code or files to the project. But first we suggest you raise an issue on the github or on the forum to start a discussion with the community. Check through the topic for Github. Sign the Contributor License Agreement (CLA) \u00b6 What is CLA ? Here is the vesoft inc. Contributor License Agreement . Click the Sign in with Github to agree button to sign the CLA. If you have any question, send an email to info@vesoft.com . Step 1: Fork in the github.com \u00b6 The Nebula Graph project has many repositories . Take the graph engine repository for example: Visit https://github.com/vesoft-inc/nebula-graph Click the Fork button (top right) to establish an online fork. Step 2: Clone Fork to Local Storage \u00b6 Define a local working directory: # Define your working directory working_dir = $HOME /Workspace Set user to match your Github profile name: user ={ your Github profile name } Create your clone: mkdir -p $working_dir cd $working_dir git clone https://github.com/ $user /nebula-graph.git # the following is recommended # or: git clone git@github.com:$user/nebula-graph.git cd $working_dir /nebula git remote add upstream https://github.com/vesoft-inc/nebula-graph.git # or: git remote add upstream git@github.com:vesoft-inc/nebula-graph.git # Never push to upstream master since you do not have write access. git remote set-url --push upstream no_push # Confirm that your remotes make sense: # It should look like: # origin git@github.com:$(user)/nebula-graph.git (fetch) # origin git@github.com:$(user)/nebula-graph.git (push) # upstream https://github.com/vesoft-inc/nebula-graph (fetch) # upstream no_push (push) git remote -v Define a Pre-Commit Hook \u00b6 Please link the Nebula Graph pre-commit hook into your .git directory. This hook checks your commits for formatting, building, doc generation, etc. cd $working_dir /nebula-graph/.git/hooks ln -s $working_dir /nebula-graph/.linters/cpp/hooks/pre-commit.sh . Sometimes, pre-commit hook can not be executable. You have to make it executable manually. cd $working_dir /nebula-graph/.git/hooks chmod +x pre-commit Step 3: Branch \u00b6 Get your local master up to date: cd $working_dir /nebula-graph git fetch upstream git checkout master git rebase upstream/master Checkout a new branch from master: git checkout -b myfeature NOTE : Because your PR often consists of several commits, which might be squashed while being merged into upstream, we strongly suggest you open a separate topic branch to make your changes on. After merged, this topic branch could be just abandoned, thus you could synchronize your master branch with upstream easily with a rebase like above. Otherwise, if you commit your changes directly into master, maybe you must use a hard reset on the master branch, like: git fetch upstream git checkout master git reset --hard upstream/master git push --force origin master Step 4: Develop \u00b6 Code Style \u00b6 We adopt cpplint to make sure that the project conforms to Google's coding style guides. The checker will be implemented before the code is committed. Unit Tests Required \u00b6 Please add unit tests for your new features or bug fixes. Build Your Code with Unit Tests Enable \u00b6 Please refer to the build source code documentation to compile. Make sure you have enabled the build of unit tests by setting -DENABLE_TESTING=ON . Run Tests \u00b6 In the root folder of nebula-graph , run the following command: ctest -j $( nproc ) Step 5: Bring Your Branch Update to Date \u00b6 # While on your myfeature branch. git fetch upstream git rebase upstream/master You need to bring the head branch up to date after other collaborators merge pull requests to the base branch. Step 6: Commit \u00b6 Commit your changes. git commit -a Likely you'll go back and edit/build/test some more than --amend in a few cycles. Step 7: Push \u00b6 When ready to review (or just to establish an offsite backup or your work), push your branch to your fork on github.com : git push origin myfeature Step 8: Create a Pull Request \u00b6 Visit your fork at https://github.com/$user/nebula-graph (replace $user obviously). Click the Compare & pull request button next to your myfeature branch. Step 9: Get a Code Review \u00b6 Once your pull request has been opened, it will be assigned to at least two reviewers. Those reviewers will do a thorough code review to make sure that the changes meet the repository's contributing guidelines and other quality standards.","title":"How to Contribute"},{"location":"15.contribution/how-to-contribute/#how_to_contribute","text":"","title":"How to Contribute"},{"location":"15.contribution/how-to-contribute/#before_you_get_started","text":"","title":"Before you get started"},{"location":"15.contribution/how-to-contribute/#file_an_issue_on_the_github_or_forum","text":"You are welcome to contribute any code or files to the project. But first we suggest you raise an issue on the github or on the forum to start a discussion with the community. Check through the topic for Github.","title":"File an issue on the github or forum"},{"location":"15.contribution/how-to-contribute/#sign_the_contributor_license_agreement_cla","text":"What is CLA ? Here is the vesoft inc. Contributor License Agreement . Click the Sign in with Github to agree button to sign the CLA. If you have any question, send an email to info@vesoft.com .","title":"Sign the Contributor License Agreement (CLA)"},{"location":"15.contribution/how-to-contribute/#step_1_fork_in_the_githubcom","text":"The Nebula Graph project has many repositories . Take the graph engine repository for example: Visit https://github.com/vesoft-inc/nebula-graph Click the Fork button (top right) to establish an online fork.","title":"Step 1: Fork in the github.com"},{"location":"15.contribution/how-to-contribute/#step_2_clone_fork_to_local_storage","text":"Define a local working directory: # Define your working directory working_dir = $HOME /Workspace Set user to match your Github profile name: user ={ your Github profile name } Create your clone: mkdir -p $working_dir cd $working_dir git clone https://github.com/ $user /nebula-graph.git # the following is recommended # or: git clone git@github.com:$user/nebula-graph.git cd $working_dir /nebula git remote add upstream https://github.com/vesoft-inc/nebula-graph.git # or: git remote add upstream git@github.com:vesoft-inc/nebula-graph.git # Never push to upstream master since you do not have write access. git remote set-url --push upstream no_push # Confirm that your remotes make sense: # It should look like: # origin git@github.com:$(user)/nebula-graph.git (fetch) # origin git@github.com:$(user)/nebula-graph.git (push) # upstream https://github.com/vesoft-inc/nebula-graph (fetch) # upstream no_push (push) git remote -v","title":"Step 2: Clone Fork to Local Storage"},{"location":"15.contribution/how-to-contribute/#define_a_pre-commit_hook","text":"Please link the Nebula Graph pre-commit hook into your .git directory. This hook checks your commits for formatting, building, doc generation, etc. cd $working_dir /nebula-graph/.git/hooks ln -s $working_dir /nebula-graph/.linters/cpp/hooks/pre-commit.sh . Sometimes, pre-commit hook can not be executable. You have to make it executable manually. cd $working_dir /nebula-graph/.git/hooks chmod +x pre-commit","title":"Define a Pre-Commit Hook"},{"location":"15.contribution/how-to-contribute/#step_3_branch","text":"Get your local master up to date: cd $working_dir /nebula-graph git fetch upstream git checkout master git rebase upstream/master Checkout a new branch from master: git checkout -b myfeature NOTE : Because your PR often consists of several commits, which might be squashed while being merged into upstream, we strongly suggest you open a separate topic branch to make your changes on. After merged, this topic branch could be just abandoned, thus you could synchronize your master branch with upstream easily with a rebase like above. Otherwise, if you commit your changes directly into master, maybe you must use a hard reset on the master branch, like: git fetch upstream git checkout master git reset --hard upstream/master git push --force origin master","title":"Step 3: Branch"},{"location":"15.contribution/how-to-contribute/#step_4_develop","text":"","title":"Step 4: Develop"},{"location":"15.contribution/how-to-contribute/#code_style","text":"We adopt cpplint to make sure that the project conforms to Google's coding style guides. The checker will be implemented before the code is committed.","title":"Code Style"},{"location":"15.contribution/how-to-contribute/#unit_tests_required","text":"Please add unit tests for your new features or bug fixes.","title":"Unit Tests Required"},{"location":"15.contribution/how-to-contribute/#build_your_code_with_unit_tests_enable","text":"Please refer to the build source code documentation to compile. Make sure you have enabled the build of unit tests by setting -DENABLE_TESTING=ON .","title":"Build Your Code with Unit Tests Enable"},{"location":"15.contribution/how-to-contribute/#run_tests","text":"In the root folder of nebula-graph , run the following command: ctest -j $( nproc )","title":"Run Tests"},{"location":"15.contribution/how-to-contribute/#step_5_bring_your_branch_update_to_date","text":"# While on your myfeature branch. git fetch upstream git rebase upstream/master You need to bring the head branch up to date after other collaborators merge pull requests to the base branch.","title":"Step 5: Bring Your Branch Update to Date"},{"location":"15.contribution/how-to-contribute/#step_6_commit","text":"Commit your changes. git commit -a Likely you'll go back and edit/build/test some more than --amend in a few cycles.","title":"Step 6: Commit"},{"location":"15.contribution/how-to-contribute/#step_7_push","text":"When ready to review (or just to establish an offsite backup or your work), push your branch to your fork on github.com : git push origin myfeature","title":"Step 7: Push"},{"location":"15.contribution/how-to-contribute/#step_8_create_a_pull_request","text":"Visit your fork at https://github.com/$user/nebula-graph (replace $user obviously). Click the Compare & pull request button next to your myfeature branch.","title":"Step 8: Create a Pull Request"},{"location":"15.contribution/how-to-contribute/#step_9_get_a_code_review","text":"Once your pull request has been opened, it will be assigned to at least two reviewers. Those reviewers will do a thorough code review to make sure that the changes meet the repository's contributing guidelines and other quality standards.","title":"Step 9: Get a Code Review"},{"location":"2.quick-start/0.FAQ/","text":"FAQ \u00b6 About openCypher compatibility \u00b6 Is nGQL compatible with openCypher 9? \u00b6 nGQL is partially compatible with openCypher 9. Known incompatible items are listed in Nebula Graph Issues . Submit an issue with the incompatible tag if you find a new issue of this type. You can search in this manual with the keyword compatibility to find major compatibility issues. The following are some major differences (by design incompatible) between nGQL and openCypher. openCypher 9 nGQL schema optional strong schema equality operator '=' equality operator '==' math exponentiation ^ ^ not supported. Use pow(x, y) instead. no such concept edge rank (reference by @) all DMLs ( CREATE , MERGE , etc), and OPTIONAL MATCH are not supported. NOTE: openCypher 9 and Cypher have some differences (in grammar and licence). For example, Cypher requires that All Cypher statements are explicitly run within a transaction . While openCypher has no such requirement of transaction . And nGQL does not support transaction. Where can I find more nGQL examples? \u00b6 Find more than 2500 nGQL examples in the features directory on the Nebula Graph GitHub page. The features directory consists of .feature files. Each file records scenarios that you can use as nGQL examples. Here is an example: Feature: Match seek by tag Background: Prepare space Given a graph with space named \"nba\" Scenario: seek by empty tag index When executing query: \"\"\" MATCH (v:bachelor) RETURN id(v) AS vid \"\"\" Then the result should be, in any order: | vid | | 'Tim Duncan' | And no side effects When executing query: \"\"\" MATCH (v:bachelor) RETURN id(v) AS vid, v.age AS age \"\"\" Then the result should be, in any order: | vid | age | | 'Tim Duncan' | 42 | And no side effects The keywords in the preceding example are described as follows: Keyword Description Feature Describes the topic of the current .feature file. Background Describes the background information of the current .feature file. Given Describes the prerequisites of running the test statements in the current .feature file. Scenario Describes the purpose of the scenario. If there is the @skip before Scenario , this scenario may not work and don't use it as a working example. When Describes the nGQL statement to be executed. Then Describes the expected result of running the statement in the When clause. If the result in your environment does not match the result described in the .feature file, submit an issue to inform the Nebula Graph team. And Describes the side effects of running the statement in the When clause. @skip This test case will be skipped. Commonly, the to-be-tested code is not ready. Welcome to add more practical scenarios and become a Nebula Graph contributor. About Data Model \u00b6 Does Nebula Graph support W3C RDF (or SPARQL , GraphQL )? \u00b6 No. Nebula Graph's data model is the property graph, and it is a strong schema system. It doesn't support rdf. Nebula Graph Query Language does not support SPARQL nor GraphQL . About executions \u00b6 How is the time spent value at the end of each return message calculated? \u00b6 Take the return message of SHOW SPACES as an example: nebula> SHOW SPACES; +------+ | Name | +------+ | nba | +------+ Got 1 rows (time spent 1235/1934 us) The first number 1235 shows the time spent by the database itself, that is, the time it takes for the query engine to receive a query from the client, fetch the data from the storage server and perform a series of calculations. The second number 1934 shows the time spent from the client's perspective, that is, the time it takes for the client from sending a request, receiving a response, and displaying the result on the screen. Can I set replica_factor as an even number in CREATE SPACE (e.g., replica_factor = 2) ? \u00b6 NO. The Storage Service garantees its availability based on the Raft consensus protocol. The number of failed replicas must not exceed half of the total replica number. When replica_factor=2 , if one replica fails, the Storage Service fails. No matter replica_factor=3 or replica_factor=4 , if more than one replica fails, the Storage Service fails, so replica_factor=3 is recommended. To prevent unnecessary waste of resources, we recommend that you set an odd replica number. We suggest that you set replica_factor to 3 for the production environment and 1 for the test environment. Do not use an even number. [ERROR (-7)]: SyntaxError: syntax error near '` \u00b6 In most cases, a query statement requires a YIELD or a RETURN . Check your query statement to see if YIELD or RETURN is provided. How to count the vertices/edges number of each tag/edge type? \u00b6 See show-stats . How to get all the vertices/edge of each tag/edge type? \u00b6 create and rebuild index > CREATE TAG INDEX i_player ON player(); > REBUILD TAG INDEX i_player; use LOOKUP or MATCH > LOOKUP ON player; > MATCH (n:player) RETURN n; See INDEX , LOOKUP and MATCH . Error can\u2019t solve the start vids from the sentence \u00b6 The graphd requires start vids to begin a graph traversal. The start vids can either be specified by the user, for example, > GO FROM ${vids} ... > MATCH (src) WHERE id(src) == ${vids} # The start vids are explicitly given by ${vids}. or be found from a (property) index, for example, # CREATE TAG INDEX i_player ON player(name(20)); # REBUILD TAG INDEX i_player; > LOOKUP ON player WHERE player.name == \"abc\" | ... YIELD ... > MATCH (src) WHERE src.name == \"abc\" ... # The start vids are found from the property index on name. Otherwise, an error like can\u2019t solve the start vids from the sentence will be raised. Error Storage Error: The VID must be a 64-bit integer or a string. \u00b6 Check your vid is an integer or a fix_string(N) . If it is a string type, make sure your input is not longer than N (default value is 8 ). See create space . About operations \u00b6 The log files are too large. How to recycle the logs \u00b6 Nebula Graph uses glog to print logs. glog can't recycle the outdated files. You can use crontab to delete them by yourself. Refer to the discussions of Glog should delete old log files automaticly . About manual updates \u00b6 The behavior of manual is not consistent with the system \u00b6 Nebula Graph 2.0 is still under development. Its behavior changes from time to time. Please tell us if the manual and the system are not consistent.","title":"FAQ"},{"location":"2.quick-start/0.FAQ/#faq","text":"","title":"FAQ"},{"location":"2.quick-start/0.FAQ/#about_opencypher_compatibility","text":"","title":"About openCypher compatibility"},{"location":"2.quick-start/0.FAQ/#is_ngql_compatible_with_opencypher_9","text":"nGQL is partially compatible with openCypher 9. Known incompatible items are listed in Nebula Graph Issues . Submit an issue with the incompatible tag if you find a new issue of this type. You can search in this manual with the keyword compatibility to find major compatibility issues. The following are some major differences (by design incompatible) between nGQL and openCypher. openCypher 9 nGQL schema optional strong schema equality operator '=' equality operator '==' math exponentiation ^ ^ not supported. Use pow(x, y) instead. no such concept edge rank (reference by @) all DMLs ( CREATE , MERGE , etc), and OPTIONAL MATCH are not supported. NOTE: openCypher 9 and Cypher have some differences (in grammar and licence). For example, Cypher requires that All Cypher statements are explicitly run within a transaction . While openCypher has no such requirement of transaction . And nGQL does not support transaction.","title":"Is nGQL compatible with openCypher 9?"},{"location":"2.quick-start/0.FAQ/#where_can_i_find_more_ngql_examples","text":"Find more than 2500 nGQL examples in the features directory on the Nebula Graph GitHub page. The features directory consists of .feature files. Each file records scenarios that you can use as nGQL examples. Here is an example: Feature: Match seek by tag Background: Prepare space Given a graph with space named \"nba\" Scenario: seek by empty tag index When executing query: \"\"\" MATCH (v:bachelor) RETURN id(v) AS vid \"\"\" Then the result should be, in any order: | vid | | 'Tim Duncan' | And no side effects When executing query: \"\"\" MATCH (v:bachelor) RETURN id(v) AS vid, v.age AS age \"\"\" Then the result should be, in any order: | vid | age | | 'Tim Duncan' | 42 | And no side effects The keywords in the preceding example are described as follows: Keyword Description Feature Describes the topic of the current .feature file. Background Describes the background information of the current .feature file. Given Describes the prerequisites of running the test statements in the current .feature file. Scenario Describes the purpose of the scenario. If there is the @skip before Scenario , this scenario may not work and don't use it as a working example. When Describes the nGQL statement to be executed. Then Describes the expected result of running the statement in the When clause. If the result in your environment does not match the result described in the .feature file, submit an issue to inform the Nebula Graph team. And Describes the side effects of running the statement in the When clause. @skip This test case will be skipped. Commonly, the to-be-tested code is not ready. Welcome to add more practical scenarios and become a Nebula Graph contributor.","title":"Where can I find more nGQL examples?"},{"location":"2.quick-start/0.FAQ/#about_data_model","text":"","title":"About Data Model"},{"location":"2.quick-start/0.FAQ/#does_nebula_graph_support_w3c_rdf_or_sparql_graphql","text":"No. Nebula Graph's data model is the property graph, and it is a strong schema system. It doesn't support rdf. Nebula Graph Query Language does not support SPARQL nor GraphQL .","title":"Does Nebula Graph support W3C RDF (or SPARQL, GraphQL)?"},{"location":"2.quick-start/0.FAQ/#about_executions","text":"","title":"About executions"},{"location":"2.quick-start/0.FAQ/#how_is_the_time_spent_value_at_the_end_of_each_return_message_calculated","text":"Take the return message of SHOW SPACES as an example: nebula> SHOW SPACES; +------+ | Name | +------+ | nba | +------+ Got 1 rows (time spent 1235/1934 us) The first number 1235 shows the time spent by the database itself, that is, the time it takes for the query engine to receive a query from the client, fetch the data from the storage server and perform a series of calculations. The second number 1934 shows the time spent from the client's perspective, that is, the time it takes for the client from sending a request, receiving a response, and displaying the result on the screen.","title":"How is the time spent value at the end of each return message calculated?"},{"location":"2.quick-start/0.FAQ/#can_i_set_replica_factor_as_an_even_number_in_create_space_eg_replica_factor_2","text":"NO. The Storage Service garantees its availability based on the Raft consensus protocol. The number of failed replicas must not exceed half of the total replica number. When replica_factor=2 , if one replica fails, the Storage Service fails. No matter replica_factor=3 or replica_factor=4 , if more than one replica fails, the Storage Service fails, so replica_factor=3 is recommended. To prevent unnecessary waste of resources, we recommend that you set an odd replica number. We suggest that you set replica_factor to 3 for the production environment and 1 for the test environment. Do not use an even number.","title":"Can I set replica_factor as an even number in CREATE SPACE (e.g., replica_factor = 2) ?"},{"location":"2.quick-start/0.FAQ/#error_-7_syntaxerror_syntax_error_near","text":"In most cases, a query statement requires a YIELD or a RETURN . Check your query statement to see if YIELD or RETURN is provided.","title":"[ERROR (-7)]: SyntaxError: syntax error near '`"},{"location":"2.quick-start/0.FAQ/#how_to_count_the_verticesedges_number_of_each_tagedge_type","text":"See show-stats .","title":"How to count the vertices/edges number of each tag/edge type?"},{"location":"2.quick-start/0.FAQ/#how_to_get_all_the_verticesedge_of_each_tagedge_type","text":"create and rebuild index > CREATE TAG INDEX i_player ON player(); > REBUILD TAG INDEX i_player; use LOOKUP or MATCH > LOOKUP ON player; > MATCH (n:player) RETURN n; See INDEX , LOOKUP and MATCH .","title":"How to get all the vertices/edge of each tag/edge type?"},{"location":"2.quick-start/0.FAQ/#error_cant_solve_the_start_vids_from_the_sentence","text":"The graphd requires start vids to begin a graph traversal. The start vids can either be specified by the user, for example, > GO FROM ${vids} ... > MATCH (src) WHERE id(src) == ${vids} # The start vids are explicitly given by ${vids}. or be found from a (property) index, for example, # CREATE TAG INDEX i_player ON player(name(20)); # REBUILD TAG INDEX i_player; > LOOKUP ON player WHERE player.name == \"abc\" | ... YIELD ... > MATCH (src) WHERE src.name == \"abc\" ... # The start vids are found from the property index on name. Otherwise, an error like can\u2019t solve the start vids from the sentence will be raised.","title":"Error can\u2019t solve the start vids from the sentence"},{"location":"2.quick-start/0.FAQ/#error_storage_error_the_vid_must_be_a_64-bit_integer_or_a_string","text":"Check your vid is an integer or a fix_string(N) . If it is a string type, make sure your input is not longer than N (default value is 8 ). See create space .","title":"Error Storage Error: The VID must be a 64-bit integer or a string."},{"location":"2.quick-start/0.FAQ/#about_operations","text":"","title":"About operations"},{"location":"2.quick-start/0.FAQ/#the_log_files_are_too_large_how_to_recycle_the_logs","text":"Nebula Graph uses glog to print logs. glog can't recycle the outdated files. You can use crontab to delete them by yourself. Refer to the discussions of Glog should delete old log files automaticly .","title":"The log files are too large. How to recycle the logs"},{"location":"2.quick-start/0.FAQ/#about_manual_updates","text":"","title":"About manual updates"},{"location":"2.quick-start/0.FAQ/#the_behavior_of_manual_is_not_consistent_with_the_system","text":"Nebula Graph 2.0 is still under development. Its behavior changes from time to time. Please tell us if the manual and the system are not consistent.","title":"The behavior of manual is not consistent with the system"},{"location":"2.quick-start/1.quick-start-workflow/","text":"Quick start workflow \u00b6 The quick start introduces the simplest workflow to using Nebula Graph, including deploying Nebula Graph, connecting to Nebula Graph, and doing basic CRUD. Deploy Nebula Graph with Docker Compose Connect to Nebula Graph CRUD in Nebula Graph Other frequently read topics are recommended as follows. They are not in the quick start, but you may need them as soon as you pass the quick start phase. Read FAQ Deploy a Nebula Graph cluster Some useful links Compaction","title":"Quick start workflow"},{"location":"2.quick-start/1.quick-start-workflow/#quick_start_workflow","text":"The quick start introduces the simplest workflow to using Nebula Graph, including deploying Nebula Graph, connecting to Nebula Graph, and doing basic CRUD. Deploy Nebula Graph with Docker Compose Connect to Nebula Graph CRUD in Nebula Graph Other frequently read topics are recommended as follows. They are not in the quick start, but you may need them as soon as you pass the quick start phase. Read FAQ Deploy a Nebula Graph cluster Some useful links Compaction","title":"Quick start workflow"},{"location":"2.quick-start/2.deploy-nebula-graph-with-docker-compose/","text":"Deploy Nebula Graph with Docker Compose \u00b6 There are multiple ways to deploy Nebula Graph, but using Docker Compose is usually considered to be a fast starter. Reading guide \u00b6 If you are reading this topic with the questions listed below, click them to jump to their answers. What do I need to do before deploying Nebula Graph? How to fast deploy Nebula Graph with Docker Compose? How to check the status and ports of the Nebula Graph services? How to check the data and logs of the Nebula Graph services? How to stop the Nebula Graph services? What are the other ways to install Nebula Graph? Prerequisites \u00b6 You have installed the following applications on your host. Application Recommended version Official installation reference Docker Latest Install Docker Engine Docker Compose Latest Install Docker Compose Git Latest Download Git If you are deploying Nebula Graph as a non-root user, grant the user with Docker-related privileges. For a detailed instruction, see Docker document: Manage Docker as a non-root user . You have started the Docker service on your host. If you have already deployed another version of Nebula Graph with Docker Compose on your host, to avoid compatibility issues\uff0cback up the service data if you need, and delete the nebula-docker-compose/data directory. NOTE: To backup the Nebula Graph data, see Use B&R to backup data . TODO: It is not released. How to deploy \u00b6 Clone the master branch of the nebula-docker-compose repository to your host with Git. DON'T : The master branch contains the Docker Compose solution for the latest Nebula Graph development release. DON'T use this release for production. $ git clone https://github.com/vesoft-inc/nebula-docker-compose.git Go to the nebula-docker-compose directory. $ cd nebula-docker-compose/ Run the following command to start all the Nebula Graph services. NOTE: Update the Nebula Graph images and Nebula Console images first if they are out of date. nebula-docker-compose ] $ docker-compose up -d Creating nebula-docker-compose_metad0_1 ... done Creating nebula-docker-compose_metad2_1 ... done Creating nebula-docker-compose_metad1_1 ... done Creating nebula-docker-compose_graphd2_1 ... done Creating nebula-docker-compose_graphd_1 ... done Creating nebula-docker-compose_graphd1_1 ... done Creating nebula-docker-compose_storaged0_1 ... done Creating nebula-docker-compose_storaged2_1 ... done Creating nebula-docker-compose_storaged1_1 ... done NOTE : For more information of the preceding services, see Nebula Graph architecture . Connect to Nebula Graph. 1. Run the following command to start a new docker container with the Nebula Console image, and connect the container to the network where Nebula Graph is deployed. $ docker run --rm -ti --network nebula-docker-compose_nebula-net --entrypoint = /bin/sh vesoft/nebula-console:v2-nightly Note : Your local network (nebula-docker-compose_nebula-net) may be different from the example above. Use the following command. $ docker network ls NETWORK ID NAME DRIVER SCOPE a74c312b1d16 bridge bridge local dbfa82505f0e host host local ed55ccf356ae nebula-docker-compose_nebula-net bridge local 93ba48b4b288 none null local 2. Connect to Nebula Graph with Nebula Console. docker> nebula-console -u user -p password --address = graphd --port = 9669 Note : By default, the authentication is off, you can log in with any user name and password. To turn it on, see Enable authentication . 3. Run the SHOW HOSTS statement to check the status of the nebula-storaged processes. nebula> SHOW HOSTS ; +-------------+------+----------+--------------+----------------------+------------------------+ | Host | Port | Status | Leader count | Leader distribution | Partition distribution | +-------------+------+----------+--------------+----------------------+------------------------+ | \"storaged0\" | 9779 | \"ONLINE\" | 0 | \"No valid partition\" | \"No valid partition\" | +-------------+------+----------+--------------+----------------------+------------------------+ | \"storaged1\" | 9779 | \"ONLINE\" | 0 | \"No valid partition\" | \"No valid partition\" | +-------------+------+----------+--------------+----------------------+------------------------+ | \"storaged2\" | 9779 | \"ONLINE\" | 0 | \"No valid partition\" | \"No valid partition\" | +-------------+------+----------+--------------+----------------------+------------------------+ | \"Total\" | | | 0 | | | +-------------+------+----------+--------------+----------------------+------------------------+ Run exit twice to switch back to your terminal (shell). You can run Step 4 to login Nebula Graph again. Check the Nebula Graph service status and port \u00b6 Run docker-compose ps to list all the services of Nebula Graph and their status and ports. $ docker-compose ps nebula-docker-compose_graphd1_1 /usr/local/nebula/bin/nebu ... Up ( healthy ) 0 .0.0.0:33170->19669/tcp, 0 .0.0.0:33169->19670/tcp, 0 .0.0.0:33173->9669/tcp nebula-docker-compose_graphd2_1 /usr/local/nebula/bin/nebu ... Up ( healthy ) 0 .0.0.0:33174->19669/tcp, 0 .0.0.0:33171->19670/tcp, 0 .0.0.0:33176->9669/tcp nebula-docker-compose_graphd_1 /usr/local/nebula/bin/nebu ... Up ( healthy ) 0 .0.0.0:33205->19669/tcp, 0 .0.0.0:33204->19670/tcp, 0 .0.0.0:9669->9669/tcp nebula-docker-compose_metad0_1 ./bin/nebula-metad --flagf ... Up ( healthy ) 0 .0.0.0:33165->19559/tcp, 0 .0.0.0:33162->19560/tcp, 0 .0.0.0:33167->9559/tcp, 9560 /tcp nebula-docker-compose_metad1_1 ./bin/nebula-metad --flagf ... Up ( healthy ) 0 .0.0.0:33166->19559/tcp, 0 .0.0.0:33163->19560/tcp, 0 .0.0.0:33168->9559/tcp, 9560 /tcp nebula-docker-compose_metad2_1 ./bin/nebula-metad --flagf ... Up ( healthy ) 0 .0.0.0:33161->19559/tcp, 0 .0.0.0:33160->19560/tcp, 0 .0.0.0:33164->9559/tcp, 9560 /tcp nebula-docker-compose_storaged0_1 ./bin/nebula-storaged --fl ... Up ( healthy ) 0 .0.0.0:33180->19779/tcp, 0 .0.0.0:33178->19780/tcp, 9777 /tcp, 9778 /tcp, 0 .0.0.0:33183->9779/tcp, 9780 /tcp nebula-docker-compose_storaged1_1 ./bin/nebula-storaged --fl ... Up ( healthy ) 0 .0.0.0:33175->19779/tcp, 0 .0.0.0:33172->19780/tcp, 9777 /tcp, 9778 /tcp, 0 .0.0.0:33177->9779/tcp, 9780 /tcp nebula-docker-compose_storaged2_1 ./bin/nebula-storaged --fl ... Up ( healthy ) 0 .0.0.0:33184->19779/tcp, 0 .0.0.0:33181->19780/tcp, 9777 /tcp, 9778 /tcp, 0 .0.0.0:33185->9779/tcp, 9780 /tcp Nebula Graph provides services to the clients through port 9669 by default. To use other ports, modify the docker-compose.yaml file in the nebula-docker-compose directory and restart the Nebula Graph services. Check the service data and logs \u00b6 All the data and logs of Nebula Graph are stored persistently in the nebula-docker-compose/data and nebula-docker-compose/logs directories. The structure of the directories is as follows: nebula-docker-compose/ |-- docker-compose.yaml \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 meta0 \u2502 \u251c\u2500\u2500 meta1 \u2502 \u251c\u2500\u2500 meta2 \u2502 \u251c\u2500\u2500 storage0 \u2502 \u251c\u2500\u2500 storage1 \u2502 \u2514\u2500\u2500 storage2 \u2514\u2500\u2500 logs \u251c\u2500\u2500 graph \u251c\u2500\u2500 graph1 \u251c\u2500\u2500 graph2 \u251c\u2500\u2500 meta0 \u251c\u2500\u2500 meta1 \u251c\u2500\u2500 meta2 \u251c\u2500\u2500 storage0 \u251c\u2500\u2500 storage1 \u2514\u2500\u2500 storage2 Stop the Nebula Graph services \u00b6 You can run the following command to stop the Nebula Graph services: $ docker-compose down The following information indicates you have successfully stopped the Nebula Graph services: Stopping nebula-docker-compose_storaged0_1 ... done Stopping nebula-docker-compose_graphd1_1 ... done Stopping nebula-docker-compose_graphd_1 ... done Stopping nebula-docker-compose_storaged1_1 ... done Stopping nebula-docker-compose_graphd2_1 ... done Stopping nebula-docker-compose_storaged2_1 ... done Stopping nebula-docker-compose_metad0_1 ... done Stopping nebula-docker-compose_metad2_1 ... done Stopping nebula-docker-compose_metad1_1 ... done Removing nebula-docker-compose_storaged0_1 ... done Removing nebula-docker-compose_graphd1_1 ... done Removing nebula-docker-compose_graphd_1 ... done Removing nebula-docker-compose_storaged1_1 ... done Removing nebula-docker-compose_graphd2_1 ... done Removing nebula-docker-compose_storaged2_1 ... done Removing nebula-docker-compose_metad0_1 ... done Removing nebula-docker-compose_metad2_1 ... done Removing nebula-docker-compose_metad1_1 ... done Removing network nebula-docker-compose_nebula-net Note : Command docker-compose down -v will delete all your local Nebula Graph storage data. Try this command if you're using a developing/nightly version and having some compatibility issues. Other ways to install Nebula Graph \u00b6 Use Source Code Use RPM or DEB package Deploy Nebula Graph cluster FAQ \u00b6 How to update the docker images of Nebula Graph services? \u00b6 To update the images of the Graph Service, Storage Service, and Meta Service, run docker-compose pull in the nebula-docker-compose directory. ERROR: toomanyrequests when docker-compose pull \u00b6 You may meet the following error. ERROR: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit . You have met the rate limit of Docker Hub. Learn more on Understanding Docker Hub Rate Limiting . How to update the Nebula Console client? \u00b6 To update the Nebula Console client, run the following command. docker pull vesoft/nebula-console:v2-nightly How to upgrade Nebula Graph services? \u00b6 To upgrade Nebula Graph, update the Nebula Graph docker images and restart the services. In the nebula-docker-compose directory, run docker-compose pull to update the Nebula Graph docker images. Run docker-compose down to stop the Nebula Graph services. Run docker-compose up -d to start the Nebula Graph services again. Why can't I connect to Nebula Graph through port 3699 after updating the nebula-docker-compose repository? (Nebula Graph 2.0.0-RC) \u00b6 On the release of Nebula Graph 2.0.0-RC, the default port for connection changed from 3699 to 9669. To connect to Nebula Graph after updating the repository, use port 9669 or modify the port number in the docker-compose.yaml file. Why can't I access the data after updating the nebula-docker-compose repository? (Jan 4, 2021) \u00b6 If you updated the nebula-docker-compose repository after Jan 4, 2021 and there are pre-existing data, modify the docker-compose.yaml file and change the port numbers to the previous ones before connecting to Nebula Graph. Why can't I access the data after updating the nebula-docker-compose repository? (Jan 27, 2021) \u00b6 The data format is incompatible before and after in Jan 27, 2021. Run docker-compose down -v to delete all your local data.","title":"Deploy Nebula\u00a0Graph with Docker Compose"},{"location":"2.quick-start/2.deploy-nebula-graph-with-docker-compose/#deploy_nebula_graph_with_docker_compose","text":"There are multiple ways to deploy Nebula Graph, but using Docker Compose is usually considered to be a fast starter.","title":"Deploy Nebula Graph with Docker Compose"},{"location":"2.quick-start/2.deploy-nebula-graph-with-docker-compose/#reading_guide","text":"If you are reading this topic with the questions listed below, click them to jump to their answers. What do I need to do before deploying Nebula Graph? How to fast deploy Nebula Graph with Docker Compose? How to check the status and ports of the Nebula Graph services? How to check the data and logs of the Nebula Graph services? How to stop the Nebula Graph services? What are the other ways to install Nebula Graph?","title":"Reading guide"},{"location":"2.quick-start/2.deploy-nebula-graph-with-docker-compose/#prerequisites","text":"You have installed the following applications on your host. Application Recommended version Official installation reference Docker Latest Install Docker Engine Docker Compose Latest Install Docker Compose Git Latest Download Git If you are deploying Nebula Graph as a non-root user, grant the user with Docker-related privileges. For a detailed instruction, see Docker document: Manage Docker as a non-root user . You have started the Docker service on your host. If you have already deployed another version of Nebula Graph with Docker Compose on your host, to avoid compatibility issues\uff0cback up the service data if you need, and delete the nebula-docker-compose/data directory. NOTE: To backup the Nebula Graph data, see Use B&R to backup data . TODO: It is not released.","title":"Prerequisites"},{"location":"2.quick-start/2.deploy-nebula-graph-with-docker-compose/#how_to_deploy","text":"Clone the master branch of the nebula-docker-compose repository to your host with Git. DON'T : The master branch contains the Docker Compose solution for the latest Nebula Graph development release. DON'T use this release for production. $ git clone https://github.com/vesoft-inc/nebula-docker-compose.git Go to the nebula-docker-compose directory. $ cd nebula-docker-compose/ Run the following command to start all the Nebula Graph services. NOTE: Update the Nebula Graph images and Nebula Console images first if they are out of date. nebula-docker-compose ] $ docker-compose up -d Creating nebula-docker-compose_metad0_1 ... done Creating nebula-docker-compose_metad2_1 ... done Creating nebula-docker-compose_metad1_1 ... done Creating nebula-docker-compose_graphd2_1 ... done Creating nebula-docker-compose_graphd_1 ... done Creating nebula-docker-compose_graphd1_1 ... done Creating nebula-docker-compose_storaged0_1 ... done Creating nebula-docker-compose_storaged2_1 ... done Creating nebula-docker-compose_storaged1_1 ... done NOTE : For more information of the preceding services, see Nebula Graph architecture . Connect to Nebula Graph. 1. Run the following command to start a new docker container with the Nebula Console image, and connect the container to the network where Nebula Graph is deployed. $ docker run --rm -ti --network nebula-docker-compose_nebula-net --entrypoint = /bin/sh vesoft/nebula-console:v2-nightly Note : Your local network (nebula-docker-compose_nebula-net) may be different from the example above. Use the following command. $ docker network ls NETWORK ID NAME DRIVER SCOPE a74c312b1d16 bridge bridge local dbfa82505f0e host host local ed55ccf356ae nebula-docker-compose_nebula-net bridge local 93ba48b4b288 none null local 2. Connect to Nebula Graph with Nebula Console. docker> nebula-console -u user -p password --address = graphd --port = 9669 Note : By default, the authentication is off, you can log in with any user name and password. To turn it on, see Enable authentication . 3. Run the SHOW HOSTS statement to check the status of the nebula-storaged processes. nebula> SHOW HOSTS ; +-------------+------+----------+--------------+----------------------+------------------------+ | Host | Port | Status | Leader count | Leader distribution | Partition distribution | +-------------+------+----------+--------------+----------------------+------------------------+ | \"storaged0\" | 9779 | \"ONLINE\" | 0 | \"No valid partition\" | \"No valid partition\" | +-------------+------+----------+--------------+----------------------+------------------------+ | \"storaged1\" | 9779 | \"ONLINE\" | 0 | \"No valid partition\" | \"No valid partition\" | +-------------+------+----------+--------------+----------------------+------------------------+ | \"storaged2\" | 9779 | \"ONLINE\" | 0 | \"No valid partition\" | \"No valid partition\" | +-------------+------+----------+--------------+----------------------+------------------------+ | \"Total\" | | | 0 | | | +-------------+------+----------+--------------+----------------------+------------------------+ Run exit twice to switch back to your terminal (shell). You can run Step 4 to login Nebula Graph again.","title":"How to deploy"},{"location":"2.quick-start/2.deploy-nebula-graph-with-docker-compose/#check_the_nebula_graph_service_status_and_port","text":"Run docker-compose ps to list all the services of Nebula Graph and their status and ports. $ docker-compose ps nebula-docker-compose_graphd1_1 /usr/local/nebula/bin/nebu ... Up ( healthy ) 0 .0.0.0:33170->19669/tcp, 0 .0.0.0:33169->19670/tcp, 0 .0.0.0:33173->9669/tcp nebula-docker-compose_graphd2_1 /usr/local/nebula/bin/nebu ... Up ( healthy ) 0 .0.0.0:33174->19669/tcp, 0 .0.0.0:33171->19670/tcp, 0 .0.0.0:33176->9669/tcp nebula-docker-compose_graphd_1 /usr/local/nebula/bin/nebu ... Up ( healthy ) 0 .0.0.0:33205->19669/tcp, 0 .0.0.0:33204->19670/tcp, 0 .0.0.0:9669->9669/tcp nebula-docker-compose_metad0_1 ./bin/nebula-metad --flagf ... Up ( healthy ) 0 .0.0.0:33165->19559/tcp, 0 .0.0.0:33162->19560/tcp, 0 .0.0.0:33167->9559/tcp, 9560 /tcp nebula-docker-compose_metad1_1 ./bin/nebula-metad --flagf ... Up ( healthy ) 0 .0.0.0:33166->19559/tcp, 0 .0.0.0:33163->19560/tcp, 0 .0.0.0:33168->9559/tcp, 9560 /tcp nebula-docker-compose_metad2_1 ./bin/nebula-metad --flagf ... Up ( healthy ) 0 .0.0.0:33161->19559/tcp, 0 .0.0.0:33160->19560/tcp, 0 .0.0.0:33164->9559/tcp, 9560 /tcp nebula-docker-compose_storaged0_1 ./bin/nebula-storaged --fl ... Up ( healthy ) 0 .0.0.0:33180->19779/tcp, 0 .0.0.0:33178->19780/tcp, 9777 /tcp, 9778 /tcp, 0 .0.0.0:33183->9779/tcp, 9780 /tcp nebula-docker-compose_storaged1_1 ./bin/nebula-storaged --fl ... Up ( healthy ) 0 .0.0.0:33175->19779/tcp, 0 .0.0.0:33172->19780/tcp, 9777 /tcp, 9778 /tcp, 0 .0.0.0:33177->9779/tcp, 9780 /tcp nebula-docker-compose_storaged2_1 ./bin/nebula-storaged --fl ... Up ( healthy ) 0 .0.0.0:33184->19779/tcp, 0 .0.0.0:33181->19780/tcp, 9777 /tcp, 9778 /tcp, 0 .0.0.0:33185->9779/tcp, 9780 /tcp Nebula Graph provides services to the clients through port 9669 by default. To use other ports, modify the docker-compose.yaml file in the nebula-docker-compose directory and restart the Nebula Graph services.","title":"Check the Nebula Graph service status and port"},{"location":"2.quick-start/2.deploy-nebula-graph-with-docker-compose/#check_the_service_data_and_logs","text":"All the data and logs of Nebula Graph are stored persistently in the nebula-docker-compose/data and nebula-docker-compose/logs directories. The structure of the directories is as follows: nebula-docker-compose/ |-- docker-compose.yaml \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 meta0 \u2502 \u251c\u2500\u2500 meta1 \u2502 \u251c\u2500\u2500 meta2 \u2502 \u251c\u2500\u2500 storage0 \u2502 \u251c\u2500\u2500 storage1 \u2502 \u2514\u2500\u2500 storage2 \u2514\u2500\u2500 logs \u251c\u2500\u2500 graph \u251c\u2500\u2500 graph1 \u251c\u2500\u2500 graph2 \u251c\u2500\u2500 meta0 \u251c\u2500\u2500 meta1 \u251c\u2500\u2500 meta2 \u251c\u2500\u2500 storage0 \u251c\u2500\u2500 storage1 \u2514\u2500\u2500 storage2","title":"Check the service data and logs"},{"location":"2.quick-start/2.deploy-nebula-graph-with-docker-compose/#stop_the_nebula_graph_services","text":"You can run the following command to stop the Nebula Graph services: $ docker-compose down The following information indicates you have successfully stopped the Nebula Graph services: Stopping nebula-docker-compose_storaged0_1 ... done Stopping nebula-docker-compose_graphd1_1 ... done Stopping nebula-docker-compose_graphd_1 ... done Stopping nebula-docker-compose_storaged1_1 ... done Stopping nebula-docker-compose_graphd2_1 ... done Stopping nebula-docker-compose_storaged2_1 ... done Stopping nebula-docker-compose_metad0_1 ... done Stopping nebula-docker-compose_metad2_1 ... done Stopping nebula-docker-compose_metad1_1 ... done Removing nebula-docker-compose_storaged0_1 ... done Removing nebula-docker-compose_graphd1_1 ... done Removing nebula-docker-compose_graphd_1 ... done Removing nebula-docker-compose_storaged1_1 ... done Removing nebula-docker-compose_graphd2_1 ... done Removing nebula-docker-compose_storaged2_1 ... done Removing nebula-docker-compose_metad0_1 ... done Removing nebula-docker-compose_metad2_1 ... done Removing nebula-docker-compose_metad1_1 ... done Removing network nebula-docker-compose_nebula-net Note : Command docker-compose down -v will delete all your local Nebula Graph storage data. Try this command if you're using a developing/nightly version and having some compatibility issues.","title":"Stop the Nebula Graph services"},{"location":"2.quick-start/2.deploy-nebula-graph-with-docker-compose/#other_ways_to_install_nebula_graph","text":"Use Source Code Use RPM or DEB package Deploy Nebula Graph cluster","title":"Other ways to install Nebula Graph"},{"location":"2.quick-start/2.deploy-nebula-graph-with-docker-compose/#faq","text":"","title":"FAQ"},{"location":"2.quick-start/2.deploy-nebula-graph-with-docker-compose/#how_to_update_the_docker_images_of_nebula_graph_services","text":"To update the images of the Graph Service, Storage Service, and Meta Service, run docker-compose pull in the nebula-docker-compose directory.","title":"How to update the docker images of Nebula Graph services?"},{"location":"2.quick-start/2.deploy-nebula-graph-with-docker-compose/#error_toomanyrequests_when_docker-compose_pull","text":"You may meet the following error. ERROR: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit . You have met the rate limit of Docker Hub. Learn more on Understanding Docker Hub Rate Limiting .","title":"ERROR: toomanyrequests when docker-compose pull"},{"location":"2.quick-start/2.deploy-nebula-graph-with-docker-compose/#how_to_update_the_nebula_console_client","text":"To update the Nebula Console client, run the following command. docker pull vesoft/nebula-console:v2-nightly","title":"How to update the Nebula Console client?"},{"location":"2.quick-start/2.deploy-nebula-graph-with-docker-compose/#how_to_upgrade_nebula_graph_services","text":"To upgrade Nebula Graph, update the Nebula Graph docker images and restart the services. In the nebula-docker-compose directory, run docker-compose pull to update the Nebula Graph docker images. Run docker-compose down to stop the Nebula Graph services. Run docker-compose up -d to start the Nebula Graph services again.","title":"How to upgrade Nebula Graph services?"},{"location":"2.quick-start/2.deploy-nebula-graph-with-docker-compose/#why_cant_i_connect_to_nebula_graph_through_port_3699_after_updating_the_nebula-docker-compose_repository_nebula_graph_200-rc","text":"On the release of Nebula Graph 2.0.0-RC, the default port for connection changed from 3699 to 9669. To connect to Nebula Graph after updating the repository, use port 9669 or modify the port number in the docker-compose.yaml file.","title":"Why can't I connect to Nebula Graph through port 3699 after updating the nebula-docker-compose repository? (Nebula Graph 2.0.0-RC)"},{"location":"2.quick-start/2.deploy-nebula-graph-with-docker-compose/#why_cant_i_access_the_data_after_updating_the_nebula-docker-compose_repository_jan_4_2021","text":"If you updated the nebula-docker-compose repository after Jan 4, 2021 and there are pre-existing data, modify the docker-compose.yaml file and change the port numbers to the previous ones before connecting to Nebula Graph.","title":"Why can't I access the data after updating the nebula-docker-compose repository? (Jan 4, 2021)"},{"location":"2.quick-start/2.deploy-nebula-graph-with-docker-compose/#why_cant_i_access_the_data_after_updating_the_nebula-docker-compose_repository_jan_27_2021","text":"The data format is incompatible before and after in Jan 27, 2021. Run docker-compose down -v to delete all your local data.","title":"Why can't I access the data after updating the nebula-docker-compose repository? (Jan 27, 2021)"},{"location":"2.quick-start/3.connect-to-nebula-graph/","text":"Connect to Nebula Graph \u00b6 Nebula Graph supports multiple types of clients, including a CLI client, a GUI client, and clients developed in popular programming languages. This topic provides an overview of Nebula Graph clients and basic instructions on how to use the native CLI client, Nebula Console. Nebula Graph clients \u00b6 You can use supported clients or console to connect to Nebula Graph. Use Nebula Console to connect to Nebula Graph \u00b6 Prerequisites \u00b6 You have started the Nebula Graph services. For how to start the services, see Start and Stop Nebula Graph . The machine you plan to run Nebula Console on has network access to the Nebula Graph services. Steps \u00b6 On the nebula-console page, select a Nebula Console version and click Assets . NOTE: We recommend that you select the latest release. In the Assets area, find the correct binary file for the machine where you want to run Nebula Console and download the file to the machine. (Optional) Rename the binary file to nebula-console for convenience. NOTE: For Windows, rename the file to nebula-console.exe . On the machine to run Nebula Console, grant the execute permission of the nebula-console binary file to the user. NOTE: For Windows, skip this step. $ chmod 111 nebula-console In the command line interface, change the working directory to the one where the nebula-console binary file is stored. Run the following command to connect to Nebula Graph. * For Linux or macOS: $ ./nebula-console -addr <ip> -port <port> -u <username> -p <password> [ -t 120 ] [ -e \"nGQL_statement\" | -f filename.nGQL ] * For Windows: > nebula-console . exe -addr < ip > -port < port > -u < username > -p < password > [ -t 120 ] [ -e \"nGQL_statement\" | -f filename . nGQL ] The description of the parameters is as follows. Option Description -h Shows the help menu. -addr Sets the IP address of the graphd service. The default address is 127.0.0.1. -port Sets the port number of the graphd service. The default port number is 9669. -u/-user Sets the username of your Nebula Graph account. Before enabling authentication, you can use any characters as the username. -p/-password Sets the password of your Nebula Graph account. Before enabling authentication, you can use any characters as the password. -t/-timeout Sets an integer-type timeout threshold of the connection. The unit is second. The default value is 120. -e/-eval Sets a string-type nGQL statement. The nGQL statement is executed once the connection succeeds. The connection stops after the result is returned. -f/-file Sets the path of an nGQL file. The nGQL statements in the file are executed once the connection succeeds. You'll get the return messages and the connection stops then. You can find more details in the Nebula Console Repository . Nebula Console export mode \u00b6 When the export mode is enabled, Nebula Console exports all the query results into a CSV file. When the export mode is disabled, the export stops. The syntax is as follows. NOTE : The following commands are case insensitive. The CSV file is stored in the working directory. Run the Linux command pwd to show the working directory. Enable Nebula Console export mode: nebula> :SET CSV <your_file.csv> Disable Nebula Console export mode: nebula> :UNSET CSV Disconnect Nebula Console from Nebula Graph \u00b6 You can use :EXIT or :QUIT to disconnect from Nebula Graph. For convenience, Nebula Console supports using these commands in lower case without the colon (\":\"), such as quit . nebula> :QUIT Bye root! FAQ \u00b6 How can I install Nebula Console from the source code \u00b6 To download and compile the latest source code of Nebula Console, follow the instructions on the nebula console GitHub page .","title":"Connect to Nebula\u00a0Graph"},{"location":"2.quick-start/3.connect-to-nebula-graph/#connect_to_nebula_graph","text":"Nebula Graph supports multiple types of clients, including a CLI client, a GUI client, and clients developed in popular programming languages. This topic provides an overview of Nebula Graph clients and basic instructions on how to use the native CLI client, Nebula Console.","title":"Connect to Nebula Graph"},{"location":"2.quick-start/3.connect-to-nebula-graph/#nebula_graph_clients","text":"You can use supported clients or console to connect to Nebula Graph.","title":"Nebula Graph clients"},{"location":"2.quick-start/3.connect-to-nebula-graph/#use_nebula_console_to_connect_to_nebula_graph","text":"","title":"Use Nebula Console to connect to Nebula Graph"},{"location":"2.quick-start/3.connect-to-nebula-graph/#prerequisites","text":"You have started the Nebula Graph services. For how to start the services, see Start and Stop Nebula Graph . The machine you plan to run Nebula Console on has network access to the Nebula Graph services.","title":"Prerequisites"},{"location":"2.quick-start/3.connect-to-nebula-graph/#steps","text":"On the nebula-console page, select a Nebula Console version and click Assets . NOTE: We recommend that you select the latest release. In the Assets area, find the correct binary file for the machine where you want to run Nebula Console and download the file to the machine. (Optional) Rename the binary file to nebula-console for convenience. NOTE: For Windows, rename the file to nebula-console.exe . On the machine to run Nebula Console, grant the execute permission of the nebula-console binary file to the user. NOTE: For Windows, skip this step. $ chmod 111 nebula-console In the command line interface, change the working directory to the one where the nebula-console binary file is stored. Run the following command to connect to Nebula Graph. * For Linux or macOS: $ ./nebula-console -addr <ip> -port <port> -u <username> -p <password> [ -t 120 ] [ -e \"nGQL_statement\" | -f filename.nGQL ] * For Windows: > nebula-console . exe -addr < ip > -port < port > -u < username > -p < password > [ -t 120 ] [ -e \"nGQL_statement\" | -f filename . nGQL ] The description of the parameters is as follows. Option Description -h Shows the help menu. -addr Sets the IP address of the graphd service. The default address is 127.0.0.1. -port Sets the port number of the graphd service. The default port number is 9669. -u/-user Sets the username of your Nebula Graph account. Before enabling authentication, you can use any characters as the username. -p/-password Sets the password of your Nebula Graph account. Before enabling authentication, you can use any characters as the password. -t/-timeout Sets an integer-type timeout threshold of the connection. The unit is second. The default value is 120. -e/-eval Sets a string-type nGQL statement. The nGQL statement is executed once the connection succeeds. The connection stops after the result is returned. -f/-file Sets the path of an nGQL file. The nGQL statements in the file are executed once the connection succeeds. You'll get the return messages and the connection stops then. You can find more details in the Nebula Console Repository .","title":"Steps"},{"location":"2.quick-start/3.connect-to-nebula-graph/#nebula_console_export_mode","text":"When the export mode is enabled, Nebula Console exports all the query results into a CSV file. When the export mode is disabled, the export stops. The syntax is as follows. NOTE : The following commands are case insensitive. The CSV file is stored in the working directory. Run the Linux command pwd to show the working directory. Enable Nebula Console export mode: nebula> :SET CSV <your_file.csv> Disable Nebula Console export mode: nebula> :UNSET CSV","title":"Nebula Console export mode"},{"location":"2.quick-start/3.connect-to-nebula-graph/#disconnect_nebula_console_from_nebula_graph","text":"You can use :EXIT or :QUIT to disconnect from Nebula Graph. For convenience, Nebula Console supports using these commands in lower case without the colon (\":\"), such as quit . nebula> :QUIT Bye root!","title":"Disconnect Nebula Console from Nebula Graph"},{"location":"2.quick-start/3.connect-to-nebula-graph/#faq","text":"","title":"FAQ"},{"location":"2.quick-start/3.connect-to-nebula-graph/#how_can_i_install_nebula_console_from_the_source_code","text":"To download and compile the latest source code of Nebula Console, follow the instructions on the nebula console GitHub page .","title":"How can I install Nebula Console from the source code"},{"location":"2.quick-start/4.nebula-graph-crud/","text":"Nebula Graph CRUD \u00b6 This topic describes the basic CRUD operations in Nebula Graph. Graph space and Nebula Graph schema \u00b6 A Nebula Graph instance consists of one or more graph spaces. Graph spaces are physically isolated from each other. You can use different graph spaces in the same instance to store different datasets. To insert data into a graph space, define a schema for the graph database. Nebula Graph schema is based on the following components. Schema component Description Vertex Represents an entity in the real world. A vertex can have one or more tags. Tag The type of a vertex. It defines a group of properties that describes a type of vertices. Edge Represents a directed relationship between two vertices. Edge type The type of an edge. It defines a group of properties that describes a type of edges. For more information, see Data modeling . In this topic, we use the following dataset to demonstrate basic CRUD operations. Check the machine status in the Nebula Graph cluster \u00b6 First, we recommend that you check the machine status to make sure that all the Storage services are connected to the Meta Services. Run SHOW HOSTS as follows. nebula> SHOW HOSTS; +-------------+-----------+-----------+--------------+----------------------+------------------------+ | Host | Port | Status | Leader count | Leader distribution | Partition distribution | +-------------+-----------+-----------+--------------+----------------------+------------------------+ | \"storaged0\" | 9779 | \"ONLINE\" | 0 | \"No valid partition\" | \"No valid partition\" | +-------------+-----------+-----------+--------------+----------------------+------------------------+ | \"storaged1\" | 9779 | \"ONLINE\" | 0 | \"No valid partition\" | \"No valid partition\" | +-------------+-----------+-----------+--------------+----------------------+------------------------+ | \"storaged2\" | 9779 | \"ONLINE\" | 0 | \"No valid partition\" | \"No valid partition\" | +-------------+-----------+-----------+--------------+----------------------+------------------------+ | \"Total\" | __EMPTY__ | __EMPTY__ | 0 | __EMPTY__ | __EMPTY__ | +-------------+-----------+-----------+--------------+----------------------+------------------------+ Got 4 rows (time spent 1061/2251 us) From the Status column of the table in the return message, you can see that all the Storage services are online. Asynchronous implementation of creation and alteration \u00b6 Nebula Graph implements the following creation or alteration operations asynchronously in the next heartbeat cycle. The operations won't take effect until they finish. CREATE SPACE CREATE TAG CREATE EDGE ALTER TAG ALTER EDGE CREATE TAG INDEX CREATE EDGE INDEX NOTE :The default heartbeat interval is 10 seconds. To change the heartbeat interval, modify the heartbeat_interval_secs parameter in the configuration files for all services. To make sure the follow-up operations work as expected, take one of the following approaches: Run SHOW or DESCRIBE statements accordingly to check the status of the objects, and make sure the creation or alteration is complete. If it is not, wait a few seconds and try again. Wait for two heartbeat cycles, i.e., 20 seconds. Create and use a graph space \u00b6 nGQL syntax \u00b6 Create a graph space: CREATE SPACE [IF NOT EXISTS] <graph_space_name> [(partition_num = <partition_number>, replica_factor = <replica_number>, vid_type = {FIXED_STRING(<N>)) | INT64}]; | Property | Description | | -------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | | partition_num | Specifies the number of partitions in each replica. The suggested number is the number of hard disks in the cluster times 5. For example, if you have 3 hard disks in the cluster, we recommend that you set 15 partitions. | | replica_factor | Specifies the number of replicas in the Nebula Graph cluster. The suggested number is 3 in a production environment and 1 in a test environment. The replica number must always be an **odd** number for the need of quorum-based voting. | |vid_type | Specifies the data type of VIDs in a graph space. Available values are `FIXED_STRING(N)` and `INT64`. `N` represents the maximum length of the VIDs and it must be a positive integer. The default value is `FIXED_STRING(8)`. If you set a VID length greater than `N`, Nebula Graph throws an error. To set the integer VID for vertices, set `vid_type` to `INT64`. | List graph spaces and check if the creation is successful: nebula> SHOW SPACES; Use a graph space: USE <graph_space_name> Examples \u00b6 Use the following statement to create a graph space named nba . nebula> CREATE SPACE nba(partition_num=15, replica_factor=1, vid_type=fixed_string(30)); Execution succeeded (time spent 2817/3280 us) Check the partition distribution with SHOW HOSTS to make sure that the partitions are distributed in a balanced way. nebula> SHOW HOSTS; +-------------+-----------+-----------+--------------+---------------------+------------------------+ | Host | Port | Status | Leader count | Leader distribution | Partition distribution | +-------------+-----------+-----------+--------------+---------------------+------------------------+ | \"storaged0\" | 9779 | \"ONLINE\" | 5 | \"nba:5\" | \"nba:5\" | +-------------+-----------+-----------+--------------+---------------------+------------------------+ | \"storaged1\" | 9779 | \"ONLINE\" | 5 | \"nba:5\" | \"nba:5\" | +-------------+-----------+-----------+--------------+---------------------+------------------------+ | \"storaged2\" | 9779 | \"ONLINE\" | 5 | \"nba:5\" | \"nba:5\" | +-------------+-----------+-----------+--------------+---------------------+------------------------+ | \"Total\" | __EMPTY__ | __EMPTY__ | 15 | \"nba:15\" | \"nba:15\" | +-------------+-----------+-----------+--------------+---------------------+------------------------+ Got 4 rows (time spent 1633/2867 us) If the Leader distribution is uneven, use BALANCE LEADER to redistribute the partitions. For more information, see BALANCE . Use the nba graph space. nebula> USE nba; Execution succeeded (time spent 1322/2206 us) You can use SHOW SPACES to check the graph space you created. nebula> SHOW SPACES; +------+ | Name | +------+ | nba | +------+ Got 1 rows (time spent 1235/1934 us) Create tags and edge types \u00b6 nGQL syntax \u00b6 CREATE {TAG | EDGE} {<tag_name> | <edge_type>}(<property_name> <data_type> [, <property_name> <data_type> ...]); Examples \u00b6 Create tags player and team , edge types follow and serve . Component name Type Property player Tag name (string), age (int) team Tag name (string) follow Edge type degree (int) serve Edge type start_year (int), end_year (int) nebula> CREATE TAG player(name string, age int); Execution succeeded (time spent 2694/3116 us) Thu, 15 Oct 2020 06:22:29 UTC nebula> CREATE TAG team(name string); Execution succeeded (time spent 2630/3002 us) Thu, 15 Oct 2020 06:22:37 UTC nebula> CREATE EDGE follow(degree int); Execution succeeded (time spent 3087/3467 us) Thu, 15 Oct 2020 06:22:43 UTC nebula> CREATE EDGE serve(start_year int, end_year int); Execution succeeded (time spent 2645/3123 us) Thu, 15 Oct 2020 06:22:50 UTC Insert vertices and edges \u00b6 You can use the INSERT statement to insert vertices or edges based on existing tags or edge types. nGQL syntax \u00b6 Insert vertices: INSERT VERTEX <tag_name> (<property_name>[, <property_name>...]) [, <tag_name> (<property_name>[, <property_name>...]), ...] {VALUES | VALUE} <vid>: (<property_value>[, <property_value>...]) [, <vid>: (<property_value>[, <property_value>...]; VID is short for vertex ID. A VID must be a unique string value in a graph space. Insert edges: INSERT EDGE <edge_type> (<property_name>[, <property_name>...]) {VALUES | VALUE} <src_vid> -> <dst_vid>[@<rank>] : (<property_value>[, <property_value>...]) [, <src_vid> -> <dst_vid>[@<rank> : (<property_name>[, <property_name>...]), ...] Examples \u00b6 Insert vertices representing NBA players and teams: nebula> INSERT VERTEX player(name, age) VALUES \"player100\":(\"Tim Duncan\", 42); Execution succeeded (time spent 2919/3485 us) Fri, 16 Oct 2020 03:41:00 UTC nebula> INSERT VERTEX player(name, age) VALUES \"player101\":(\"Tony Parker\", 36); Execution succeeded (time spent 3007/3539 us) Fri, 16 Oct 2020 03:41:58 UTC nebula> INSERT VERTEX player(name, age) VALUES \"player102\":(\"LaMarcus Aldridge\", 33); Execution succeeded (time spent 2449/2934 us) Fri, 16 Oct 2020 03:42:16 UTC nebula> INSERT VERTEX team(name) VALUES \"team200\":(\"Warriors\"), \"team201\":(\"Nuggets\"); Execution succeeded (time spent 3514/4331 us) Fri, 16 Oct 2020 03:42:45 UTC Insert edges representing the relations between NBA players and teams: nebula> INSERT EDGE follow(degree) VALUES \"player100\" -> \"player101\":(95); Execution succeeded (time spent 1488/1918 us) Wed, 21 Oct 2020 06:57:32 UTC nebula> INSERT EDGE follow(degree) VALUES \"player100\" -> \"player102\":(90); Execution succeeded (time spent 2483/2890 us) Wed, 21 Oct 2020 07:05:48 UTC nebula> INSERT EDGE follow(degree) VALUES \"player102\" -> \"player101\":(75); Execution succeeded (time spent 1208/1689 us) Wed, 21 Oct 2020 07:07:12 UTC nebula> INSERT EDGE serve(start_year, end_year) VALUES \"player100\" -> \"team200\":(1997, 2016), \"player101\" -> \"team201\":(1999, 2018); Execution succeeded (time spent 2170/2651 us) Wed, 21 Oct 2020 07:08:59 UTC Read data \u00b6 The GO statement traverses the database based on specific conditions. A GO traversal starts from one or more vertices, along one or more edges, and return information in a form specified in the YIELD clause. The FETCH statement is used to get properties from vertices or edges. The LOOKUP statement is based on indexes . It is used together with the WHERE clause to search for the data that meet the specific conditions. The MATCH statement is the most commonly used statement for graph data querying. But, it relies on indexes to match data patterns in Nebula Graph. nGQL syntax \u00b6 GO GO [[<M> TO] <N> STEPS ] FROM <vertex_list> OVER <edge_type_list> [REVERSELY] [BIDIRECT] [WHERE <expression> [AND | OR expression ...])] YIELD [DISTINCT] <return_list> FETCH Fetch properties on tags: FETCH PROP ON {<tag_name> | <tag_name_list> | *} <vid_list> [YIELD [DISTINCT] <return_list>] Fetch properties on edges: FETCH PROP ON <edge_type> <src_vid> -> <dst_vid>[@<rank>] [, <src_vid> -> <dst_vid> ...] [YIELD [DISTINCT] <return_list>] LOOKUP LOOKUP ON {<tag_name> | <edge_type>} WHERE <expression> [AND expression ...])] [YIELD <return_list>] MATCH MATCH <pattern> [<WHERE clause>] RETURN <output> Examples of GO \u00b6 Find the vertices that VID \"player100\" follows. nebula> GO FROM \"player100\" OVER follow; +-------------+ | follow._dst | +-------------+ | player101 | +-------------+ | player102 | +-------------+ Got 2 rows (time spent 1935/2420 us) Search for the players that the player with VID \"player100\" follows. Filter the players that the player with VID \"player100\" follows whose age is equal to or greater than 35. Rename the columns in the result with Teammate and Age . nebula> GO FROM \"player100\" OVER follow WHERE $$.player.age >= 35 \\ YIELD $$.player.name AS Teammate, $$.player.age AS Age; +-------------+-----+ | Teammate | Age | +-------------+-----+ | Tony Parker | 36 | +-------------+-----+ Got 1 rows (time spent 3871/4349 us) Clause/Sign Description YIELD Specifies what values or results you want to return from the query. $$ Represents the target vertices. \\ A line-breaker. Search for the players that the player with VID \"player100\" follows. Then Retrieve the teams of the players that the player with VID \"player100\" follows. To combine the two queries, use a pipe or a temporary variable. With a pipe: nebula> GO FROM \"player100\" OVER follow YIELD follow._dst AS id | \\ GO FROM $-.id OVER serve YIELD $$.team.name AS Team, \\ $^.player.name AS Player; +---------+-------------+ | Team | Player | +---------+-------------+ | Nuggets | Tony Parker | +---------+-------------+ Got 1 rows (time spent 2902/3496 us) Clause/Sign Description $^ Represents the source vertex of the edge. \\| A pipe symbol that can combine multiple queries. $- Represents the output of the query before the pipe symbol. With a temporary variable: NOTE : Once a compound statement is submitted to the server as a whole, the life cycle of the temporary variables in the statement ends. nebula> $var = GO FROM \"player100\" OVER follow YIELD follow._dst AS id; \\ GO FROM $var.id OVER serve YIELD $$.team.name AS Team, \\ $^.player.name AS Player; +---------+-------------+ | Team | Player | +---------+-------------+ | Nuggets | Tony Parker | +---------+-------------+ Got 1 rows (time spent 3103/3711 us) Example of FETCH \u00b6 Use FETCH : Fetch the properties of the player with VID player100. nebula> FETCH PROP ON player \"player100\"; +----------------------------------------------------+ | vertices_ | +----------------------------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | +----------------------------------------------------+ Got 1 rows (time spent 2006/2406 us) Examples of Match and Lookup are provided after index is introduced in the following. Update vertices and edges \u00b6 You can use the UPDATE statement or the UPSERT statement to update existing data. UPSERT is the combination of UPDATE and INSERT . If you update a vertex or an edge with UPSERT , it inserts a new vertex or edge if it does not exist. Note: UPSERT operates in serial a (partition-based) order and therefore is slower comparing with INSERT OR UPDATE . nGQL syntax \u00b6 UPDATE vertices: UPDATE VERTEX <vid> SET <properties to be updated> [WHEN <condition>] [YIELD <columns>] UPDATE edges: UPDATE EDGE <source vid> -> <destination vid> [@rank] OF <edge_type> SET <properties to be updated> [WHEN <condition>] [YIELD <columns to be output>] UPSERT vertices or edges: UPSERT {VERTEX <vid> | EDGE <edge_type>} SET <update_columns> [WHEN <condition>] [YIELD <columns>] Examples \u00b6 UPDATE the name property of the vertex with VID \"player100\" and check the result with the FETCH statement: nebula> UPDATE VERTEX \"player100\" SET player.name = \"Tim\"; Execution succeeded (time spent 3483/3914 us) Wed, 21 Oct 2020 10:53:14 UTC nebula> FETCH PROP ON player \"player100\"; +---------------------------------------------+ | vertices_ | +---------------------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim\"}) | +---------------------------------------------+ Got 1 rows (time spent 2463/3042 us) UPDATE the degree value of an edge and check the result with the FETCH statement: nebula> UPDATE EDGE \"player100\" -> \"player101\" OF follow SET degree = 96; Execution succeeded (time spent 3932/4432 us) nebula> FETCH PROP ON follow \"player100\" -> \"player101\"; +----------------------------------------------------+ | edges_ | +----------------------------------------------------+ | [:follow \"player100\"->\"player101\" @0 {degree: 96}] | +----------------------------------------------------+ Got 1 rows (time spent 2205/2800 us) Insert a vertex with VID \"player111\" and UPSERT it. nebula> INSERT VERTEX player(name, age) VALUES \"player111\":(\"Ben Simmons\", 22); Execution succeeded (time spent 2115/2900 us) Wed, 21 Oct 2020 11:11:50 UTC nebula> UPSERT VERTEX \"player111\" SET player.name = \"Dwight Howard\", player.age = $^.player.age + 11 \\ WHEN $^.player.name == \"Ben Simmons\" AND $^.player.age > 20 \\ YIELD $^.player.name AS Name, $^.player.age AS Age; +---------------+-----+ | Name | Age | +---------------+-----+ | Dwight Howard | 33 | +---------------+-----+ Got 1 rows (time spent 1815/2329 us) Delete vertices and edges \u00b6 nGQL syntax \u00b6 Delete vertices: DELETE VERTEX <vid1>[, <vid2>...] Delete edges: DELETE EDGE <edge_type> <src_vid> -> <dst_vid>[@<rank>] [, <src_vid> -> <dst_vid>...] Examples \u00b6 Delete vertices: nebula> DELETE VERTEX \"team1\", \"team2\"; Execution succeeded (time spent 4337/4782 us) Delete edges: nebula> DELETE EDGE follow \"team1\" -> \"team2\"; Execution succeeded (time spent 3700/4101 us) About indexes \u00b6 You can add indexes to tags or edge types with the CREATE INDEX statement. Must-read for using index \u00b6 Both MATCH and LOOKUP depend on index. But indexes can dramatically reduce the write performance. The performance reduction can be as much as 90% or even more. DO NOT use indexes in production environments unless you are fully aware of their influences on your service. You MUST rebuild indexes for pre-existing data. Otherwise, the pre-existing data can't be indexed (and therefore can't be returned in Match or Lookup ). For more information, see REBUILD INDEX . nGQL syntax \u00b6 Create an index: CREATE {TAG | EDGE} INDEX [IF NOT EXISTS] <index_name> ON {<tag_name> | <edge_name>} (prop_name_list); Rebuild an index: REBUILD {TAG | EDGE} INDEX <index_name> Examples \u00b6 Create an index for the name property on all vertices with the tag player . nebula> CREATE TAG INDEX player_index_0 on player(name(20)); nebula> REBUILD TAG INDEX player_index_0; NOTE: Define the index length when creating an index for a variable-length property. For more information, see CREATE INDEX Examples of LOOKUP and MATCH (index-based) \u00b6 Make sure there is an index for LOOKUP or MATCH to use. If there is not, create an index first. Find the information of the vertex with the tag player and its value of the name property is \"Tony Parker\" . // Create an index on the player name property. nebula> CREATE TAG INDEX player_name_0 on player(name(10)); Execution succeeded (time spent 3465/4150 us) // Rebuild the index to make sure it takes effect on pre-existing data. nebula> REBUILD TAG INDEX player_name_0 +------------+ | New Job Id | +------------+ | 31 | +------------+ Got 1 rows (time spent 2379/3033 us) // Use LOOKUP to retrieve the vertex property. nebula> LOOKUP ON player WHERE player.name == \"Tony Parker\" \\ YIELD player.name, player.age; +-------------+---------------+------------+ | VertexID | player.name | player.age | +-------------+---------------+------------+ | \"player101\" | \"Tony Parker\" | 36 | +-------------+---------------+------------+ // Use MATCH to retrieve the vertex. nebula> MATCH (v:player{name:\"Tony Parker\"}) RETURN v; +-----------------------------------------------------+ | v | +-----------------------------------------------------+ | (\"player101\" :player{age: 36, name: \"Tony Parker\"}) | +-----------------------------------------------------+ Got 1 rows (time spent 5132/6246 us)","title":"Nebula\u00a0Graph CRUD"},{"location":"2.quick-start/4.nebula-graph-crud/#nebula_graph_crud","text":"This topic describes the basic CRUD operations in Nebula Graph.","title":"Nebula Graph CRUD"},{"location":"2.quick-start/4.nebula-graph-crud/#graph_space_and_nebula_graph_schema","text":"A Nebula Graph instance consists of one or more graph spaces. Graph spaces are physically isolated from each other. You can use different graph spaces in the same instance to store different datasets. To insert data into a graph space, define a schema for the graph database. Nebula Graph schema is based on the following components. Schema component Description Vertex Represents an entity in the real world. A vertex can have one or more tags. Tag The type of a vertex. It defines a group of properties that describes a type of vertices. Edge Represents a directed relationship between two vertices. Edge type The type of an edge. It defines a group of properties that describes a type of edges. For more information, see Data modeling . In this topic, we use the following dataset to demonstrate basic CRUD operations.","title":"Graph space and Nebula Graph schema"},{"location":"2.quick-start/4.nebula-graph-crud/#check_the_machine_status_in_the_nebula_graph_cluster","text":"First, we recommend that you check the machine status to make sure that all the Storage services are connected to the Meta Services. Run SHOW HOSTS as follows. nebula> SHOW HOSTS; +-------------+-----------+-----------+--------------+----------------------+------------------------+ | Host | Port | Status | Leader count | Leader distribution | Partition distribution | +-------------+-----------+-----------+--------------+----------------------+------------------------+ | \"storaged0\" | 9779 | \"ONLINE\" | 0 | \"No valid partition\" | \"No valid partition\" | +-------------+-----------+-----------+--------------+----------------------+------------------------+ | \"storaged1\" | 9779 | \"ONLINE\" | 0 | \"No valid partition\" | \"No valid partition\" | +-------------+-----------+-----------+--------------+----------------------+------------------------+ | \"storaged2\" | 9779 | \"ONLINE\" | 0 | \"No valid partition\" | \"No valid partition\" | +-------------+-----------+-----------+--------------+----------------------+------------------------+ | \"Total\" | __EMPTY__ | __EMPTY__ | 0 | __EMPTY__ | __EMPTY__ | +-------------+-----------+-----------+--------------+----------------------+------------------------+ Got 4 rows (time spent 1061/2251 us) From the Status column of the table in the return message, you can see that all the Storage services are online.","title":"Check the machine status in the Nebula Graph cluster"},{"location":"2.quick-start/4.nebula-graph-crud/#asynchronous_implementation_of_creation_and_alteration","text":"Nebula Graph implements the following creation or alteration operations asynchronously in the next heartbeat cycle. The operations won't take effect until they finish. CREATE SPACE CREATE TAG CREATE EDGE ALTER TAG ALTER EDGE CREATE TAG INDEX CREATE EDGE INDEX NOTE :The default heartbeat interval is 10 seconds. To change the heartbeat interval, modify the heartbeat_interval_secs parameter in the configuration files for all services. To make sure the follow-up operations work as expected, take one of the following approaches: Run SHOW or DESCRIBE statements accordingly to check the status of the objects, and make sure the creation or alteration is complete. If it is not, wait a few seconds and try again. Wait for two heartbeat cycles, i.e., 20 seconds.","title":"Asynchronous implementation of creation and alteration"},{"location":"2.quick-start/4.nebula-graph-crud/#create_and_use_a_graph_space","text":"","title":"Create and use a graph space"},{"location":"2.quick-start/4.nebula-graph-crud/#ngql_syntax","text":"Create a graph space: CREATE SPACE [IF NOT EXISTS] <graph_space_name> [(partition_num = <partition_number>, replica_factor = <replica_number>, vid_type = {FIXED_STRING(<N>)) | INT64}]; | Property | Description | | -------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | | partition_num | Specifies the number of partitions in each replica. The suggested number is the number of hard disks in the cluster times 5. For example, if you have 3 hard disks in the cluster, we recommend that you set 15 partitions. | | replica_factor | Specifies the number of replicas in the Nebula Graph cluster. The suggested number is 3 in a production environment and 1 in a test environment. The replica number must always be an **odd** number for the need of quorum-based voting. | |vid_type | Specifies the data type of VIDs in a graph space. Available values are `FIXED_STRING(N)` and `INT64`. `N` represents the maximum length of the VIDs and it must be a positive integer. The default value is `FIXED_STRING(8)`. If you set a VID length greater than `N`, Nebula Graph throws an error. To set the integer VID for vertices, set `vid_type` to `INT64`. | List graph spaces and check if the creation is successful: nebula> SHOW SPACES; Use a graph space: USE <graph_space_name>","title":"nGQL syntax"},{"location":"2.quick-start/4.nebula-graph-crud/#examples","text":"Use the following statement to create a graph space named nba . nebula> CREATE SPACE nba(partition_num=15, replica_factor=1, vid_type=fixed_string(30)); Execution succeeded (time spent 2817/3280 us) Check the partition distribution with SHOW HOSTS to make sure that the partitions are distributed in a balanced way. nebula> SHOW HOSTS; +-------------+-----------+-----------+--------------+---------------------+------------------------+ | Host | Port | Status | Leader count | Leader distribution | Partition distribution | +-------------+-----------+-----------+--------------+---------------------+------------------------+ | \"storaged0\" | 9779 | \"ONLINE\" | 5 | \"nba:5\" | \"nba:5\" | +-------------+-----------+-----------+--------------+---------------------+------------------------+ | \"storaged1\" | 9779 | \"ONLINE\" | 5 | \"nba:5\" | \"nba:5\" | +-------------+-----------+-----------+--------------+---------------------+------------------------+ | \"storaged2\" | 9779 | \"ONLINE\" | 5 | \"nba:5\" | \"nba:5\" | +-------------+-----------+-----------+--------------+---------------------+------------------------+ | \"Total\" | __EMPTY__ | __EMPTY__ | 15 | \"nba:15\" | \"nba:15\" | +-------------+-----------+-----------+--------------+---------------------+------------------------+ Got 4 rows (time spent 1633/2867 us) If the Leader distribution is uneven, use BALANCE LEADER to redistribute the partitions. For more information, see BALANCE . Use the nba graph space. nebula> USE nba; Execution succeeded (time spent 1322/2206 us) You can use SHOW SPACES to check the graph space you created. nebula> SHOW SPACES; +------+ | Name | +------+ | nba | +------+ Got 1 rows (time spent 1235/1934 us)","title":"Examples"},{"location":"2.quick-start/4.nebula-graph-crud/#create_tags_and_edge_types","text":"","title":"Create tags and edge types"},{"location":"2.quick-start/4.nebula-graph-crud/#ngql_syntax_1","text":"CREATE {TAG | EDGE} {<tag_name> | <edge_type>}(<property_name> <data_type> [, <property_name> <data_type> ...]);","title":"nGQL syntax"},{"location":"2.quick-start/4.nebula-graph-crud/#examples_1","text":"Create tags player and team , edge types follow and serve . Component name Type Property player Tag name (string), age (int) team Tag name (string) follow Edge type degree (int) serve Edge type start_year (int), end_year (int) nebula> CREATE TAG player(name string, age int); Execution succeeded (time spent 2694/3116 us) Thu, 15 Oct 2020 06:22:29 UTC nebula> CREATE TAG team(name string); Execution succeeded (time spent 2630/3002 us) Thu, 15 Oct 2020 06:22:37 UTC nebula> CREATE EDGE follow(degree int); Execution succeeded (time spent 3087/3467 us) Thu, 15 Oct 2020 06:22:43 UTC nebula> CREATE EDGE serve(start_year int, end_year int); Execution succeeded (time spent 2645/3123 us) Thu, 15 Oct 2020 06:22:50 UTC","title":"Examples"},{"location":"2.quick-start/4.nebula-graph-crud/#insert_vertices_and_edges","text":"You can use the INSERT statement to insert vertices or edges based on existing tags or edge types.","title":"Insert vertices and edges"},{"location":"2.quick-start/4.nebula-graph-crud/#ngql_syntax_2","text":"Insert vertices: INSERT VERTEX <tag_name> (<property_name>[, <property_name>...]) [, <tag_name> (<property_name>[, <property_name>...]), ...] {VALUES | VALUE} <vid>: (<property_value>[, <property_value>...]) [, <vid>: (<property_value>[, <property_value>...]; VID is short for vertex ID. A VID must be a unique string value in a graph space. Insert edges: INSERT EDGE <edge_type> (<property_name>[, <property_name>...]) {VALUES | VALUE} <src_vid> -> <dst_vid>[@<rank>] : (<property_value>[, <property_value>...]) [, <src_vid> -> <dst_vid>[@<rank> : (<property_name>[, <property_name>...]), ...]","title":"nGQL syntax"},{"location":"2.quick-start/4.nebula-graph-crud/#examples_2","text":"Insert vertices representing NBA players and teams: nebula> INSERT VERTEX player(name, age) VALUES \"player100\":(\"Tim Duncan\", 42); Execution succeeded (time spent 2919/3485 us) Fri, 16 Oct 2020 03:41:00 UTC nebula> INSERT VERTEX player(name, age) VALUES \"player101\":(\"Tony Parker\", 36); Execution succeeded (time spent 3007/3539 us) Fri, 16 Oct 2020 03:41:58 UTC nebula> INSERT VERTEX player(name, age) VALUES \"player102\":(\"LaMarcus Aldridge\", 33); Execution succeeded (time spent 2449/2934 us) Fri, 16 Oct 2020 03:42:16 UTC nebula> INSERT VERTEX team(name) VALUES \"team200\":(\"Warriors\"), \"team201\":(\"Nuggets\"); Execution succeeded (time spent 3514/4331 us) Fri, 16 Oct 2020 03:42:45 UTC Insert edges representing the relations between NBA players and teams: nebula> INSERT EDGE follow(degree) VALUES \"player100\" -> \"player101\":(95); Execution succeeded (time spent 1488/1918 us) Wed, 21 Oct 2020 06:57:32 UTC nebula> INSERT EDGE follow(degree) VALUES \"player100\" -> \"player102\":(90); Execution succeeded (time spent 2483/2890 us) Wed, 21 Oct 2020 07:05:48 UTC nebula> INSERT EDGE follow(degree) VALUES \"player102\" -> \"player101\":(75); Execution succeeded (time spent 1208/1689 us) Wed, 21 Oct 2020 07:07:12 UTC nebula> INSERT EDGE serve(start_year, end_year) VALUES \"player100\" -> \"team200\":(1997, 2016), \"player101\" -> \"team201\":(1999, 2018); Execution succeeded (time spent 2170/2651 us) Wed, 21 Oct 2020 07:08:59 UTC","title":"Examples"},{"location":"2.quick-start/4.nebula-graph-crud/#read_data","text":"The GO statement traverses the database based on specific conditions. A GO traversal starts from one or more vertices, along one or more edges, and return information in a form specified in the YIELD clause. The FETCH statement is used to get properties from vertices or edges. The LOOKUP statement is based on indexes . It is used together with the WHERE clause to search for the data that meet the specific conditions. The MATCH statement is the most commonly used statement for graph data querying. But, it relies on indexes to match data patterns in Nebula Graph.","title":"Read data"},{"location":"2.quick-start/4.nebula-graph-crud/#ngql_syntax_3","text":"GO GO [[<M> TO] <N> STEPS ] FROM <vertex_list> OVER <edge_type_list> [REVERSELY] [BIDIRECT] [WHERE <expression> [AND | OR expression ...])] YIELD [DISTINCT] <return_list> FETCH Fetch properties on tags: FETCH PROP ON {<tag_name> | <tag_name_list> | *} <vid_list> [YIELD [DISTINCT] <return_list>] Fetch properties on edges: FETCH PROP ON <edge_type> <src_vid> -> <dst_vid>[@<rank>] [, <src_vid> -> <dst_vid> ...] [YIELD [DISTINCT] <return_list>] LOOKUP LOOKUP ON {<tag_name> | <edge_type>} WHERE <expression> [AND expression ...])] [YIELD <return_list>] MATCH MATCH <pattern> [<WHERE clause>] RETURN <output>","title":"nGQL syntax"},{"location":"2.quick-start/4.nebula-graph-crud/#examples_of_go","text":"Find the vertices that VID \"player100\" follows. nebula> GO FROM \"player100\" OVER follow; +-------------+ | follow._dst | +-------------+ | player101 | +-------------+ | player102 | +-------------+ Got 2 rows (time spent 1935/2420 us) Search for the players that the player with VID \"player100\" follows. Filter the players that the player with VID \"player100\" follows whose age is equal to or greater than 35. Rename the columns in the result with Teammate and Age . nebula> GO FROM \"player100\" OVER follow WHERE $$.player.age >= 35 \\ YIELD $$.player.name AS Teammate, $$.player.age AS Age; +-------------+-----+ | Teammate | Age | +-------------+-----+ | Tony Parker | 36 | +-------------+-----+ Got 1 rows (time spent 3871/4349 us) Clause/Sign Description YIELD Specifies what values or results you want to return from the query. $$ Represents the target vertices. \\ A line-breaker. Search for the players that the player with VID \"player100\" follows. Then Retrieve the teams of the players that the player with VID \"player100\" follows. To combine the two queries, use a pipe or a temporary variable. With a pipe: nebula> GO FROM \"player100\" OVER follow YIELD follow._dst AS id | \\ GO FROM $-.id OVER serve YIELD $$.team.name AS Team, \\ $^.player.name AS Player; +---------+-------------+ | Team | Player | +---------+-------------+ | Nuggets | Tony Parker | +---------+-------------+ Got 1 rows (time spent 2902/3496 us) Clause/Sign Description $^ Represents the source vertex of the edge. \\| A pipe symbol that can combine multiple queries. $- Represents the output of the query before the pipe symbol. With a temporary variable: NOTE : Once a compound statement is submitted to the server as a whole, the life cycle of the temporary variables in the statement ends. nebula> $var = GO FROM \"player100\" OVER follow YIELD follow._dst AS id; \\ GO FROM $var.id OVER serve YIELD $$.team.name AS Team, \\ $^.player.name AS Player; +---------+-------------+ | Team | Player | +---------+-------------+ | Nuggets | Tony Parker | +---------+-------------+ Got 1 rows (time spent 3103/3711 us)","title":"Examples of GO"},{"location":"2.quick-start/4.nebula-graph-crud/#example_of_fetch","text":"Use FETCH : Fetch the properties of the player with VID player100. nebula> FETCH PROP ON player \"player100\"; +----------------------------------------------------+ | vertices_ | +----------------------------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | +----------------------------------------------------+ Got 1 rows (time spent 2006/2406 us) Examples of Match and Lookup are provided after index is introduced in the following.","title":"Example of FETCH"},{"location":"2.quick-start/4.nebula-graph-crud/#update_vertices_and_edges","text":"You can use the UPDATE statement or the UPSERT statement to update existing data. UPSERT is the combination of UPDATE and INSERT . If you update a vertex or an edge with UPSERT , it inserts a new vertex or edge if it does not exist. Note: UPSERT operates in serial a (partition-based) order and therefore is slower comparing with INSERT OR UPDATE .","title":"Update vertices and edges"},{"location":"2.quick-start/4.nebula-graph-crud/#ngql_syntax_4","text":"UPDATE vertices: UPDATE VERTEX <vid> SET <properties to be updated> [WHEN <condition>] [YIELD <columns>] UPDATE edges: UPDATE EDGE <source vid> -> <destination vid> [@rank] OF <edge_type> SET <properties to be updated> [WHEN <condition>] [YIELD <columns to be output>] UPSERT vertices or edges: UPSERT {VERTEX <vid> | EDGE <edge_type>} SET <update_columns> [WHEN <condition>] [YIELD <columns>]","title":"nGQL syntax"},{"location":"2.quick-start/4.nebula-graph-crud/#examples_3","text":"UPDATE the name property of the vertex with VID \"player100\" and check the result with the FETCH statement: nebula> UPDATE VERTEX \"player100\" SET player.name = \"Tim\"; Execution succeeded (time spent 3483/3914 us) Wed, 21 Oct 2020 10:53:14 UTC nebula> FETCH PROP ON player \"player100\"; +---------------------------------------------+ | vertices_ | +---------------------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim\"}) | +---------------------------------------------+ Got 1 rows (time spent 2463/3042 us) UPDATE the degree value of an edge and check the result with the FETCH statement: nebula> UPDATE EDGE \"player100\" -> \"player101\" OF follow SET degree = 96; Execution succeeded (time spent 3932/4432 us) nebula> FETCH PROP ON follow \"player100\" -> \"player101\"; +----------------------------------------------------+ | edges_ | +----------------------------------------------------+ | [:follow \"player100\"->\"player101\" @0 {degree: 96}] | +----------------------------------------------------+ Got 1 rows (time spent 2205/2800 us) Insert a vertex with VID \"player111\" and UPSERT it. nebula> INSERT VERTEX player(name, age) VALUES \"player111\":(\"Ben Simmons\", 22); Execution succeeded (time spent 2115/2900 us) Wed, 21 Oct 2020 11:11:50 UTC nebula> UPSERT VERTEX \"player111\" SET player.name = \"Dwight Howard\", player.age = $^.player.age + 11 \\ WHEN $^.player.name == \"Ben Simmons\" AND $^.player.age > 20 \\ YIELD $^.player.name AS Name, $^.player.age AS Age; +---------------+-----+ | Name | Age | +---------------+-----+ | Dwight Howard | 33 | +---------------+-----+ Got 1 rows (time spent 1815/2329 us)","title":"Examples"},{"location":"2.quick-start/4.nebula-graph-crud/#delete_vertices_and_edges","text":"","title":"Delete vertices and edges"},{"location":"2.quick-start/4.nebula-graph-crud/#ngql_syntax_5","text":"Delete vertices: DELETE VERTEX <vid1>[, <vid2>...] Delete edges: DELETE EDGE <edge_type> <src_vid> -> <dst_vid>[@<rank>] [, <src_vid> -> <dst_vid>...]","title":"nGQL syntax"},{"location":"2.quick-start/4.nebula-graph-crud/#examples_4","text":"Delete vertices: nebula> DELETE VERTEX \"team1\", \"team2\"; Execution succeeded (time spent 4337/4782 us) Delete edges: nebula> DELETE EDGE follow \"team1\" -> \"team2\"; Execution succeeded (time spent 3700/4101 us)","title":"Examples"},{"location":"2.quick-start/4.nebula-graph-crud/#about_indexes","text":"You can add indexes to tags or edge types with the CREATE INDEX statement.","title":"About indexes"},{"location":"2.quick-start/4.nebula-graph-crud/#must-read_for_using_index","text":"Both MATCH and LOOKUP depend on index. But indexes can dramatically reduce the write performance. The performance reduction can be as much as 90% or even more. DO NOT use indexes in production environments unless you are fully aware of their influences on your service. You MUST rebuild indexes for pre-existing data. Otherwise, the pre-existing data can't be indexed (and therefore can't be returned in Match or Lookup ). For more information, see REBUILD INDEX .","title":"Must-read for using index"},{"location":"2.quick-start/4.nebula-graph-crud/#ngql_syntax_6","text":"Create an index: CREATE {TAG | EDGE} INDEX [IF NOT EXISTS] <index_name> ON {<tag_name> | <edge_name>} (prop_name_list); Rebuild an index: REBUILD {TAG | EDGE} INDEX <index_name>","title":"nGQL syntax"},{"location":"2.quick-start/4.nebula-graph-crud/#examples_5","text":"Create an index for the name property on all vertices with the tag player . nebula> CREATE TAG INDEX player_index_0 on player(name(20)); nebula> REBUILD TAG INDEX player_index_0; NOTE: Define the index length when creating an index for a variable-length property. For more information, see CREATE INDEX","title":"Examples"},{"location":"2.quick-start/4.nebula-graph-crud/#examples_of_lookup_and_match_index-based","text":"Make sure there is an index for LOOKUP or MATCH to use. If there is not, create an index first. Find the information of the vertex with the tag player and its value of the name property is \"Tony Parker\" . // Create an index on the player name property. nebula> CREATE TAG INDEX player_name_0 on player(name(10)); Execution succeeded (time spent 3465/4150 us) // Rebuild the index to make sure it takes effect on pre-existing data. nebula> REBUILD TAG INDEX player_name_0 +------------+ | New Job Id | +------------+ | 31 | +------------+ Got 1 rows (time spent 2379/3033 us) // Use LOOKUP to retrieve the vertex property. nebula> LOOKUP ON player WHERE player.name == \"Tony Parker\" \\ YIELD player.name, player.age; +-------------+---------------+------------+ | VertexID | player.name | player.age | +-------------+---------------+------------+ | \"player101\" | \"Tony Parker\" | 36 | +-------------+---------------+------------+ // Use MATCH to retrieve the vertex. nebula> MATCH (v:player{name:\"Tony Parker\"}) RETURN v; +-----------------------------------------------------+ | v | +-----------------------------------------------------+ | (\"player101\" :player{age: 36, name: \"Tony Parker\"}) | +-----------------------------------------------------+ Got 1 rows (time spent 5132/6246 us)","title":"Examples of LOOKUP and MATCH (index-based)"},{"location":"2.quick-start/5.start-stop-service/","text":"Manage Nebula Graph services \u00b6 You can use the nebula.service script to start, stop, restart, terminate, and check the Nebula Graph services. This topic takes starting, stopping and checking the Nebula Graph services for examples. nebula.service is stored in the /usr/local/nebula/ directory by default, which is also the default installation path of Nebula Graph. If you have customized the path, use the actual path in your environment. Syntax \u00b6 $ sudo /usr/local/nebula/scripts/nebula.service [ -v ] [ -c <config_file_path> ] <start | stop | restart | status | kill> <metad | graphd | storaged | all> Parameter Description -v Display detailed debugging information. -c Specify the configuration file path. The default path is /usr/local/nebula/etc/ . start Start the target services. stop Stop the target services. restart Restart the target services. kill Terminate the target services. status Check the status of the target services. metad Set the Meta Service as the target service. graphd Set the Graph Service as the target service. storaged Set the Storage Service as the target service. all Set all the Nebula Graph services as the target services. Start Nebula Graph \u00b6 In non-container environment \u00b6 Run the following command to start Nebula Graph. $ sudo /usr/local/nebula/scripts/nebula.service start all [ INFO ] Starting nebula-metad... [ INFO ] Done [ INFO ] Starting nebula-graphd... [ INFO ] Done [ INFO ] Starting nebula-storaged... [ INFO ] Done In docker container (deployed with docker-compose) \u00b6 Run the following command in the nebula-docker-compose/ directory to start Nebula Graph. nebula-docker-compose ] $ docker-compose up -d Building with native build. Learn about native build in Compose here: https://docs.docker.com/go/compose-native-build/ Creating network \"nebula-docker-compose_nebula-net\" with the default driver Creating nebula-docker-compose_metad0_1 ... done Creating nebula-docker-compose_metad2_1 ... done Creating nebula-docker-compose_metad1_1 ... done Creating nebula-docker-compose_storaged2_1 ... done Creating nebula-docker-compose_graphd1_1 ... done Creating nebula-docker-compose_storaged1_1 ... done Creating nebula-docker-compose_storaged0_1 ... done Creating nebula-docker-compose_graphd2_1 ... done Creating nebula-docker-compose_graphd_1 ... done Stop Nebula Graph \u00b6 DON'T: Don't run kill -9 to forcibly terminate the processes, otherwise, there is a low probability of data loss. In non-container environment \u00b6 Run the following command to stop Nebula Graph. sudo /usr/local/nebula/scripts/nebula.service stop all [ INFO ] Stopping nebula-metad... [ INFO ] Done [ INFO ] Stopping nebula-graphd... [ INFO ] Done [ INFO ] Stopping nebula-storaged... [ INFO ] Done In docker container (deployed with docker-compose) \u00b6 Run the following command in the nebula-docker-compose/ directory to stop Nebula Graph. nebula-docker-compose ] $ docker-compose down Stopping nebula-docker-compose_graphd_1 ... done Stopping nebula-docker-compose_graphd2_1 ... done Stopping nebula-docker-compose_storaged0_1 ... done Stopping nebula-docker-compose_storaged1_1 ... done Stopping nebula-docker-compose_graphd1_1 ... done Stopping nebula-docker-compose_storaged2_1 ... done Stopping nebula-docker-compose_metad1_1 ... done Stopping nebula-docker-compose_metad2_1 ... done Stopping nebula-docker-compose_metad0_1 ... done Removing nebula-docker-compose_graphd_1 ... done Removing nebula-docker-compose_graphd2_1 ... done Removing nebula-docker-compose_storaged0_1 ... done Removing nebula-docker-compose_storaged1_1 ... done Removing nebula-docker-compose_graphd1_1 ... done Removing nebula-docker-compose_storaged2_1 ... done Removing nebula-docker-compose_metad1_1 ... done Removing nebula-docker-compose_metad2_1 ... done Removing nebula-docker-compose_metad0_1 ... done Removing network nebula-docker-compose_nebula-net If you are using a development or nightly version for testing and have compatibility issues, try to run 'docker-compose down-v' to DELETE all data stored in Nebula Graph and import data again. Check the service status \u00b6 In non-container environment \u00b6 Run the following command to check the service status of Nebula Graph. $ sudo /usr/local/nebula/scripts/nebula.service status all Nebula Graph is running normally if the following information is returned. [ INFO ] nebula-metad: Running as 26601 , Listening on 9559 [ INFO ] nebula-graphd: Running as 26644 , Listening on 9669 [ INFO ] nebula-storaged: Running as 26709 , Listening on 9779 If the return information is similar to the following one, there is a problem. [ INFO ] nebula-metad: Running as 25600 , Listening on 9559 [ INFO ] nebula-graphd: Exited [ INFO ] nebula-storaged: Running as 25646 , Listening on 9779 The Nebula Graph services consist of the Meta Service, Graph Service, and Storage Service. The configuration files for all three services are stored in the /usr/local/nebula/etc/ directory by default. You can check the configuration files according to the return information to troubleshoot problems. You may also go to the Nebula Graph community for help. In docker container (deployed with docker-compose) \u00b6 Run the following command in the nebula-docker-compose/ directory to check the service status of Nebula Graph. nebula-docker-compose ] $ docker-compose ps Name Command State Ports --------------------------------------------------------------------------------------------------------------------------------------------------------------------- nebula-docker-compose_graphd1_1 /usr/local/nebula/bin/nebu ... Up ( healthy ) 0 .0.0.0:49223->19669/tcp, 0 .0.0.0:49222->19670/tcp, 0 .0.0.0:49224->9669/tcp nebula-docker-compose_graphd2_1 /usr/local/nebula/bin/nebu ... Up ( healthy ) 0 .0.0.0:49229->19669/tcp, 0 .0.0.0:49228->19670/tcp, 0 .0.0.0:49230->9669/tcp nebula-docker-compose_graphd_1 /usr/local/nebula/bin/nebu ... Up ( healthy ) 0 .0.0.0:49221->19669/tcp, 0 .0.0.0:49220->19670/tcp, 0 .0.0.0:9669->9669/tcp nebula-docker-compose_metad0_1 ./bin/nebula-metad --flagf ... Up ( healthy ) 0 .0.0.0:49212->19559/tcp, 0 .0.0.0:49211->19560/tcp, 0 .0.0.0:49213->9559/tcp, 9560 /tcp nebula-docker-compose_metad1_1 ./bin/nebula-metad --flagf ... Up ( healthy ) 0 .0.0.0:49209->19559/tcp, 0 .0.0.0:49208->19560/tcp, 0 .0.0.0:49210->9559/tcp, 9560 /tcp nebula-docker-compose_metad2_1 ./bin/nebula-metad --flagf ... Up ( healthy ) 0 .0.0.0:49206->19559/tcp, 0 .0.0.0:49205->19560/tcp, 0 .0.0.0:49207->9559/tcp, 9560 /tcp nebula-docker-compose_storaged0_1 ./bin/nebula-storaged --fl ... Up ( healthy ) 0 .0.0.0:49218->19779/tcp, 0 .0.0.0:49217->19780/tcp, 9777 /tcp, 9778 /tcp, 0 .0.0.0:49219->9779/tcp, 9780 /tcp nebula-docker-compose_storaged1_1 ./bin/nebula-storaged --fl ... Up ( healthy ) 0 .0.0.0:49215->19779/tcp, 0 .0.0.0:49214->19780/tcp, 9777 /tcp, 9778 /tcp, 0 .0.0.0:49216->9779/tcp, 9780 /tcp nebula-docker-compose_storaged2_1 ./bin/nebula-storaged --fl ... Up ( healthy ) 0 .0.0.0:49226->19779/tcp, 0 .0.0.0:49225->19780/tcp, 9777 /tcp, 9778 /tcp, 0 .0.0.0:49227->9779/tcp, 9780 /tcp To troubleshoot for a specific service: Confirm the container name in the preceding return information. Run docker ps to find the CONTAINER ID . Use the CONTAINER ID to log in the container and troubleshoot. nebula-docker-compose]$ docker exec -it 2a6c56c405f5 bash [root@2a6c56c405f5 nebula]#","title":"Manage Nebula Graph services"},{"location":"2.quick-start/5.start-stop-service/#manage_nebula_graph_services","text":"You can use the nebula.service script to start, stop, restart, terminate, and check the Nebula Graph services. This topic takes starting, stopping and checking the Nebula Graph services for examples. nebula.service is stored in the /usr/local/nebula/ directory by default, which is also the default installation path of Nebula Graph. If you have customized the path, use the actual path in your environment.","title":"Manage Nebula Graph services"},{"location":"2.quick-start/5.start-stop-service/#syntax","text":"$ sudo /usr/local/nebula/scripts/nebula.service [ -v ] [ -c <config_file_path> ] <start | stop | restart | status | kill> <metad | graphd | storaged | all> Parameter Description -v Display detailed debugging information. -c Specify the configuration file path. The default path is /usr/local/nebula/etc/ . start Start the target services. stop Stop the target services. restart Restart the target services. kill Terminate the target services. status Check the status of the target services. metad Set the Meta Service as the target service. graphd Set the Graph Service as the target service. storaged Set the Storage Service as the target service. all Set all the Nebula Graph services as the target services.","title":"Syntax"},{"location":"2.quick-start/5.start-stop-service/#start_nebula_graph","text":"","title":"Start Nebula Graph"},{"location":"2.quick-start/5.start-stop-service/#in_non-container_environment","text":"Run the following command to start Nebula Graph. $ sudo /usr/local/nebula/scripts/nebula.service start all [ INFO ] Starting nebula-metad... [ INFO ] Done [ INFO ] Starting nebula-graphd... [ INFO ] Done [ INFO ] Starting nebula-storaged... [ INFO ] Done","title":"In non-container environment"},{"location":"2.quick-start/5.start-stop-service/#in_docker_container_deployed_with_docker-compose","text":"Run the following command in the nebula-docker-compose/ directory to start Nebula Graph. nebula-docker-compose ] $ docker-compose up -d Building with native build. Learn about native build in Compose here: https://docs.docker.com/go/compose-native-build/ Creating network \"nebula-docker-compose_nebula-net\" with the default driver Creating nebula-docker-compose_metad0_1 ... done Creating nebula-docker-compose_metad2_1 ... done Creating nebula-docker-compose_metad1_1 ... done Creating nebula-docker-compose_storaged2_1 ... done Creating nebula-docker-compose_graphd1_1 ... done Creating nebula-docker-compose_storaged1_1 ... done Creating nebula-docker-compose_storaged0_1 ... done Creating nebula-docker-compose_graphd2_1 ... done Creating nebula-docker-compose_graphd_1 ... done","title":"In docker container (deployed with docker-compose)"},{"location":"2.quick-start/5.start-stop-service/#stop_nebula_graph","text":"DON'T: Don't run kill -9 to forcibly terminate the processes, otherwise, there is a low probability of data loss.","title":"Stop Nebula Graph"},{"location":"2.quick-start/5.start-stop-service/#in_non-container_environment_1","text":"Run the following command to stop Nebula Graph. sudo /usr/local/nebula/scripts/nebula.service stop all [ INFO ] Stopping nebula-metad... [ INFO ] Done [ INFO ] Stopping nebula-graphd... [ INFO ] Done [ INFO ] Stopping nebula-storaged... [ INFO ] Done","title":"In non-container environment"},{"location":"2.quick-start/5.start-stop-service/#in_docker_container_deployed_with_docker-compose_1","text":"Run the following command in the nebula-docker-compose/ directory to stop Nebula Graph. nebula-docker-compose ] $ docker-compose down Stopping nebula-docker-compose_graphd_1 ... done Stopping nebula-docker-compose_graphd2_1 ... done Stopping nebula-docker-compose_storaged0_1 ... done Stopping nebula-docker-compose_storaged1_1 ... done Stopping nebula-docker-compose_graphd1_1 ... done Stopping nebula-docker-compose_storaged2_1 ... done Stopping nebula-docker-compose_metad1_1 ... done Stopping nebula-docker-compose_metad2_1 ... done Stopping nebula-docker-compose_metad0_1 ... done Removing nebula-docker-compose_graphd_1 ... done Removing nebula-docker-compose_graphd2_1 ... done Removing nebula-docker-compose_storaged0_1 ... done Removing nebula-docker-compose_storaged1_1 ... done Removing nebula-docker-compose_graphd1_1 ... done Removing nebula-docker-compose_storaged2_1 ... done Removing nebula-docker-compose_metad1_1 ... done Removing nebula-docker-compose_metad2_1 ... done Removing nebula-docker-compose_metad0_1 ... done Removing network nebula-docker-compose_nebula-net If you are using a development or nightly version for testing and have compatibility issues, try to run 'docker-compose down-v' to DELETE all data stored in Nebula Graph and import data again.","title":"In docker container (deployed with docker-compose)"},{"location":"2.quick-start/5.start-stop-service/#check_the_service_status","text":"","title":"Check the service status"},{"location":"2.quick-start/5.start-stop-service/#in_non-container_environment_2","text":"Run the following command to check the service status of Nebula Graph. $ sudo /usr/local/nebula/scripts/nebula.service status all Nebula Graph is running normally if the following information is returned. [ INFO ] nebula-metad: Running as 26601 , Listening on 9559 [ INFO ] nebula-graphd: Running as 26644 , Listening on 9669 [ INFO ] nebula-storaged: Running as 26709 , Listening on 9779 If the return information is similar to the following one, there is a problem. [ INFO ] nebula-metad: Running as 25600 , Listening on 9559 [ INFO ] nebula-graphd: Exited [ INFO ] nebula-storaged: Running as 25646 , Listening on 9779 The Nebula Graph services consist of the Meta Service, Graph Service, and Storage Service. The configuration files for all three services are stored in the /usr/local/nebula/etc/ directory by default. You can check the configuration files according to the return information to troubleshoot problems. You may also go to the Nebula Graph community for help.","title":"In non-container environment"},{"location":"2.quick-start/5.start-stop-service/#in_docker_container_deployed_with_docker-compose_2","text":"Run the following command in the nebula-docker-compose/ directory to check the service status of Nebula Graph. nebula-docker-compose ] $ docker-compose ps Name Command State Ports --------------------------------------------------------------------------------------------------------------------------------------------------------------------- nebula-docker-compose_graphd1_1 /usr/local/nebula/bin/nebu ... Up ( healthy ) 0 .0.0.0:49223->19669/tcp, 0 .0.0.0:49222->19670/tcp, 0 .0.0.0:49224->9669/tcp nebula-docker-compose_graphd2_1 /usr/local/nebula/bin/nebu ... Up ( healthy ) 0 .0.0.0:49229->19669/tcp, 0 .0.0.0:49228->19670/tcp, 0 .0.0.0:49230->9669/tcp nebula-docker-compose_graphd_1 /usr/local/nebula/bin/nebu ... Up ( healthy ) 0 .0.0.0:49221->19669/tcp, 0 .0.0.0:49220->19670/tcp, 0 .0.0.0:9669->9669/tcp nebula-docker-compose_metad0_1 ./bin/nebula-metad --flagf ... Up ( healthy ) 0 .0.0.0:49212->19559/tcp, 0 .0.0.0:49211->19560/tcp, 0 .0.0.0:49213->9559/tcp, 9560 /tcp nebula-docker-compose_metad1_1 ./bin/nebula-metad --flagf ... Up ( healthy ) 0 .0.0.0:49209->19559/tcp, 0 .0.0.0:49208->19560/tcp, 0 .0.0.0:49210->9559/tcp, 9560 /tcp nebula-docker-compose_metad2_1 ./bin/nebula-metad --flagf ... Up ( healthy ) 0 .0.0.0:49206->19559/tcp, 0 .0.0.0:49205->19560/tcp, 0 .0.0.0:49207->9559/tcp, 9560 /tcp nebula-docker-compose_storaged0_1 ./bin/nebula-storaged --fl ... Up ( healthy ) 0 .0.0.0:49218->19779/tcp, 0 .0.0.0:49217->19780/tcp, 9777 /tcp, 9778 /tcp, 0 .0.0.0:49219->9779/tcp, 9780 /tcp nebula-docker-compose_storaged1_1 ./bin/nebula-storaged --fl ... Up ( healthy ) 0 .0.0.0:49215->19779/tcp, 0 .0.0.0:49214->19780/tcp, 9777 /tcp, 9778 /tcp, 0 .0.0.0:49216->9779/tcp, 9780 /tcp nebula-docker-compose_storaged2_1 ./bin/nebula-storaged --fl ... Up ( healthy ) 0 .0.0.0:49226->19779/tcp, 0 .0.0.0:49225->19780/tcp, 9777 /tcp, 9778 /tcp, 0 .0.0.0:49227->9779/tcp, 9780 /tcp To troubleshoot for a specific service: Confirm the container name in the preceding return information. Run docker ps to find the CONTAINER ID . Use the CONTAINER ID to log in the container and troubleshoot. nebula-docker-compose]$ docker exec -it 2a6c56c405f5 bash [root@2a6c56c405f5 nebula]#","title":"In docker container (deployed with docker-compose)"},{"location":"2.quick-start/6.useful-links/","text":"Useful links \u00b6 API Clients by Nebula Graph \u00b6 links commit id C++ Client 7305c72 Go Client 542ed24 Python Client cb48e8a The following repositories of v2.0.0 are not released yet. (TODO Mar. 23 2021): Java Client Rust Client Node.js Client HTTP Client Graph tools \u00b6 links commit id Command Line Console 1f32236 The following repositories of v2.0.0 are not released yet. (TODO Mar. 23 2021): * Studio * Dashboard * [Graph Computing TODO] Big Data and other Systems support \u00b6 links commit id csv (a.k.a. importer) 1d87c7b Spark af3fdf4 nebula-docker-compose 2c2549a The following repositories of v2.0.0 are not released yet. (TODO Mar. 23 2021): Flink Promethus ElasticSearch Benchmark, test, and Backup tools \u00b6 The following repositories of v2.0.0 are not released yet. (TODO Mar. 23 2021): Benchmark Chaos Test Backup&Restore Misc \u00b6 links commit id Nebula Graph 1.2 (deprecate) 53f56b6 Open Source Community \u4e2d\u6587\u624b\u518c","title":"Useful Links"},{"location":"2.quick-start/6.useful-links/#useful_links","text":"","title":"Useful links"},{"location":"2.quick-start/6.useful-links/#api_clients_by_nebula_graph","text":"links commit id C++ Client 7305c72 Go Client 542ed24 Python Client cb48e8a The following repositories of v2.0.0 are not released yet. (TODO Mar. 23 2021): Java Client Rust Client Node.js Client HTTP Client","title":"API Clients by Nebula Graph"},{"location":"2.quick-start/6.useful-links/#graph_tools","text":"links commit id Command Line Console 1f32236 The following repositories of v2.0.0 are not released yet. (TODO Mar. 23 2021): * Studio * Dashboard * [Graph Computing TODO]","title":"Graph tools"},{"location":"2.quick-start/6.useful-links/#big_data_and_other_systems_support","text":"links commit id csv (a.k.a. importer) 1d87c7b Spark af3fdf4 nebula-docker-compose 2c2549a The following repositories of v2.0.0 are not released yet. (TODO Mar. 23 2021): Flink Promethus ElasticSearch","title":"Big Data and other Systems support"},{"location":"2.quick-start/6.useful-links/#benchmark_test_and_backup_tools","text":"The following repositories of v2.0.0 are not released yet. (TODO Mar. 23 2021): Benchmark Chaos Test Backup&Restore","title":"Benchmark, test, and Backup tools"},{"location":"2.quick-start/6.useful-links/#misc","text":"links commit id Nebula Graph 1.2 (deprecate) 53f56b6 Open Source Community \u4e2d\u6587\u624b\u518c","title":"Misc"},{"location":"3.ngql-guide/1.nGQL-overview/1.overview/","text":"Nebula Graph Query Language (nGQL) \u00b6 This document gives an introduction to the query language of Nebula Graph, nGQL. What is nGQL \u00b6 nGQL is a declarative graph query language for Nebula Graph. It allows expressive and efficient graph patterns. nGQL is designed for both developers and operations professionals. nGQL is an SQL-like query language, so it's easy to learn. nGQL is a project in progress. New features and optimizations are done steadily. There can be differences between syntax and implementation. Nebula Graph 2.0 or later version support openCypher 9 . What can nGQL do \u00b6 Supports graph traverse Supports pattern match Supports aggregation Supports graph mutation Supports access control Supports composite queries Supports index Supports most openCypher 9 graph query syntax (but mutations and controls syntax are not supported). Example Data \u00b6 The example data in Nebula Graph document statements can be downloaded here . After downloading the example data, you can import it to Nebula Graph by using the -f option in Nebula Graph Console . Placeholder Identifiers and Values \u00b6 Refer to the following standards in nGQL: ISO/IEC 10646 ISO/IEC 39075 ISO/IEC NP 39075 (Draft) OpenCypher 9 In template code, any token that is not a keyword, a literal value, or punctuation is a placeholder identifier or a placeholder value. For details of the symbols in nGQL, see the following table: Token Meaning < > name of a syntactic element ::= formula that defines an element [ ] optional elements { } explicitly specified elements | complete alternative elements ... may be repeated any number of times","title":"Overview"},{"location":"3.ngql-guide/1.nGQL-overview/1.overview/#nebula_graph_query_language_ngql","text":"This document gives an introduction to the query language of Nebula Graph, nGQL.","title":"Nebula Graph Query Language (nGQL)"},{"location":"3.ngql-guide/1.nGQL-overview/1.overview/#what_is_ngql","text":"nGQL is a declarative graph query language for Nebula Graph. It allows expressive and efficient graph patterns. nGQL is designed for both developers and operations professionals. nGQL is an SQL-like query language, so it's easy to learn. nGQL is a project in progress. New features and optimizations are done steadily. There can be differences between syntax and implementation. Nebula Graph 2.0 or later version support openCypher 9 .","title":"What is nGQL"},{"location":"3.ngql-guide/1.nGQL-overview/1.overview/#what_can_ngql_do","text":"Supports graph traverse Supports pattern match Supports aggregation Supports graph mutation Supports access control Supports composite queries Supports index Supports most openCypher 9 graph query syntax (but mutations and controls syntax are not supported).","title":"What can nGQL do"},{"location":"3.ngql-guide/1.nGQL-overview/1.overview/#example_data","text":"The example data in Nebula Graph document statements can be downloaded here . After downloading the example data, you can import it to Nebula Graph by using the -f option in Nebula Graph Console .","title":"Example Data"},{"location":"3.ngql-guide/1.nGQL-overview/1.overview/#placeholder_identifiers_and_values","text":"Refer to the following standards in nGQL: ISO/IEC 10646 ISO/IEC 39075 ISO/IEC NP 39075 (Draft) OpenCypher 9 In template code, any token that is not a keyword, a literal value, or punctuation is a placeholder identifier or a placeholder value. For details of the symbols in nGQL, see the following table: Token Meaning < > name of a syntactic element ::= formula that defines an element [ ] optional elements { } explicitly specified elements | complete alternative elements ... may be repeated any number of times","title":"Placeholder Identifiers and Values"},{"location":"3.ngql-guide/1.nGQL-overview/3.graph-patterns/","text":"Patterns \u00b6 Patterns and graph pattern matching are the very heart of a graph query language. Patterns for vertices \u00b6 A vertex is described using a pair of parentheses, and is typically given a name. For example: (a) This simple pattern describes a single vertex, and names that vertex using the variable a . Patterns for related vertices \u00b6 A more powerful construct is a pattern that describes multiple vertices and edges between them. Patterns describe edges by employing an arrow between two vertices. For example: (a)-[]->(b) This pattern describes a very simple data shape: two vertices, and a single edge from one to the other. In this example, the two vertices are both named as a and b respectively, and the edge is directed : it goes from a to b . This manner of describing vertices and edges can be extended to cover an arbitrary number of vertices and the edges between them, for example: (a)-[]->(b)<-[]-(c) Such a series of connected vertices and edges is called a \"path\" . Note that the naming of the vertices in these patterns is only necessary should one need to refer to the same vertex again, either later in the pattern or elsewhere in the query. If this is not necessary, then the name may be omitted, as follows: (a)-[]->()<-[]-(c) Patterns for tags \u00b6 OpenCypher compatibility : The concept tag in nGQL have a few differences from label in openCypher. E.g., You must create a tag before using it. And a tag also defines the properties' type. In addition to simply describing the shape of a vertex in the pattern, one can also describe attributes. The most simple attribute that can be described in the pattern is a tag that the vertex must have. For example: (a:User)-[]->(b) One can also describe a vertex that has multiple tags: (a:User:Admin)-[]->(b) . Patterns for properties \u00b6 Nodes and edges are the fundamental structures in a graph. nGQL uses properties on both of these to allow for far richer models. Properties can be expressed in patterns using a map-construct: curly brackets surrounding a number of key-expression pairs, separated by commas. E.g. a vertex with two properties on it would look like: (a {name: 'Andres', sport: 'Brazilian Ju-Jitsu'}) An edge with expectations on it is given by: (a)-[{blocked: false}]->(b) Patterns for edges \u00b6 The simplest way to describe an edge is by using the arrow between two vertices, as in the previous examples. Using this syntax, you can describe that the edge should exist and the directionality of it. If you don\u2019t care about the direction of the edge, the arrowhead is omitted, as exemplified by: (a)-[]-(b) As with vertices, edges may also be given names. In this case, a pair of square brackets is used to break up the arrow and the variable is placed between. For example: (a)-[r]->(b) Much like tags on vertices, edges can have types. To describe an edge with a specific type, use the pattern as follows: (a)-[r:REL_TYPE]->(b) An edge can only have one edge type. But if we\u2019d like to describe some data such that the edge could have any one of a set of types, then they can all be listed in the pattern, separating them with the pipe symbol | like this: (a)-[r:TYPE1|TYPE2]->(b) As with vertices, the name of the edge can always be omitted, as exemplified by: (a)-[:REL_TYPE]->(b) Variable-length pattern \u00b6 Rather than describing a long path using a sequence of many vertex and edge descriptions in a pattern, many edges (and the intermediate vertices) can be described by specifying a length in the edge description of a pattern. For example: (a)-[*2]->(b) This describes a graph of three vertices and two edges, all in one path (a path of length 2). This is equivalent to: (a)-[]->()-[]->(b) A range of lengths can also be specified: such edge patterns are called 'variable-length edges'. For example: (a)-[*3..5]->(b) The preceding example defines a path with a minimum length of 3, and a maximum length of 5. It describes a graph of either 4 vertices and 3 edges, 5 vertices and 4 edges, or 6 vertices and 5 edges, all connected in a single path. the lower bound can be omitted. For example, to describe paths of length 5 or less, use: (a)-[*..5]->(b) OpenCypher compatibility : The upper bound must be specified. The following are NOT accepted. (a)-[*3..]->(b) (a)-[*]->(b) Assigning to path variables \u00b6 As described above, a series of connected vertices and edges is called a \"path\". nGQL allows paths to be named using a variable, as exemplified by: p = (a)-[*3..5]->(b) You can do this in MATCH .","title":"Graph patterns"},{"location":"3.ngql-guide/1.nGQL-overview/3.graph-patterns/#patterns","text":"Patterns and graph pattern matching are the very heart of a graph query language.","title":"Patterns"},{"location":"3.ngql-guide/1.nGQL-overview/3.graph-patterns/#patterns_for_vertices","text":"A vertex is described using a pair of parentheses, and is typically given a name. For example: (a) This simple pattern describes a single vertex, and names that vertex using the variable a .","title":"Patterns for vertices"},{"location":"3.ngql-guide/1.nGQL-overview/3.graph-patterns/#patterns_for_related_vertices","text":"A more powerful construct is a pattern that describes multiple vertices and edges between them. Patterns describe edges by employing an arrow between two vertices. For example: (a)-[]->(b) This pattern describes a very simple data shape: two vertices, and a single edge from one to the other. In this example, the two vertices are both named as a and b respectively, and the edge is directed : it goes from a to b . This manner of describing vertices and edges can be extended to cover an arbitrary number of vertices and the edges between them, for example: (a)-[]->(b)<-[]-(c) Such a series of connected vertices and edges is called a \"path\" . Note that the naming of the vertices in these patterns is only necessary should one need to refer to the same vertex again, either later in the pattern or elsewhere in the query. If this is not necessary, then the name may be omitted, as follows: (a)-[]->()<-[]-(c)","title":"Patterns for related vertices"},{"location":"3.ngql-guide/1.nGQL-overview/3.graph-patterns/#patterns_for_tags","text":"OpenCypher compatibility : The concept tag in nGQL have a few differences from label in openCypher. E.g., You must create a tag before using it. And a tag also defines the properties' type. In addition to simply describing the shape of a vertex in the pattern, one can also describe attributes. The most simple attribute that can be described in the pattern is a tag that the vertex must have. For example: (a:User)-[]->(b) One can also describe a vertex that has multiple tags: (a:User:Admin)-[]->(b) .","title":"Patterns for tags"},{"location":"3.ngql-guide/1.nGQL-overview/3.graph-patterns/#patterns_for_properties","text":"Nodes and edges are the fundamental structures in a graph. nGQL uses properties on both of these to allow for far richer models. Properties can be expressed in patterns using a map-construct: curly brackets surrounding a number of key-expression pairs, separated by commas. E.g. a vertex with two properties on it would look like: (a {name: 'Andres', sport: 'Brazilian Ju-Jitsu'}) An edge with expectations on it is given by: (a)-[{blocked: false}]->(b)","title":"Patterns for properties"},{"location":"3.ngql-guide/1.nGQL-overview/3.graph-patterns/#patterns_for_edges","text":"The simplest way to describe an edge is by using the arrow between two vertices, as in the previous examples. Using this syntax, you can describe that the edge should exist and the directionality of it. If you don\u2019t care about the direction of the edge, the arrowhead is omitted, as exemplified by: (a)-[]-(b) As with vertices, edges may also be given names. In this case, a pair of square brackets is used to break up the arrow and the variable is placed between. For example: (a)-[r]->(b) Much like tags on vertices, edges can have types. To describe an edge with a specific type, use the pattern as follows: (a)-[r:REL_TYPE]->(b) An edge can only have one edge type. But if we\u2019d like to describe some data such that the edge could have any one of a set of types, then they can all be listed in the pattern, separating them with the pipe symbol | like this: (a)-[r:TYPE1|TYPE2]->(b) As with vertices, the name of the edge can always be omitted, as exemplified by: (a)-[:REL_TYPE]->(b)","title":"Patterns for edges"},{"location":"3.ngql-guide/1.nGQL-overview/3.graph-patterns/#variable-length_pattern","text":"Rather than describing a long path using a sequence of many vertex and edge descriptions in a pattern, many edges (and the intermediate vertices) can be described by specifying a length in the edge description of a pattern. For example: (a)-[*2]->(b) This describes a graph of three vertices and two edges, all in one path (a path of length 2). This is equivalent to: (a)-[]->()-[]->(b) A range of lengths can also be specified: such edge patterns are called 'variable-length edges'. For example: (a)-[*3..5]->(b) The preceding example defines a path with a minimum length of 3, and a maximum length of 5. It describes a graph of either 4 vertices and 3 edges, 5 vertices and 4 edges, or 6 vertices and 5 edges, all connected in a single path. the lower bound can be omitted. For example, to describe paths of length 5 or less, use: (a)-[*..5]->(b) OpenCypher compatibility : The upper bound must be specified. The following are NOT accepted. (a)-[*3..]->(b) (a)-[*]->(b)","title":"Variable-length pattern"},{"location":"3.ngql-guide/1.nGQL-overview/3.graph-patterns/#assigning_to_path_variables","text":"As described above, a series of connected vertices and edges is called a \"path\". nGQL allows paths to be named using a variable, as exemplified by: p = (a)-[*3..5]->(b) You can do this in MATCH .","title":"Assigning to path variables"},{"location":"3.ngql-guide/10.tag-statements/1.create-tag/","text":"CREATE TAG \u00b6 CREATE TAG [IF NOT EXISTS] <tag_name> ([<create_definition>, ...]) [tag_options] <create_definition> ::= <prop_name> <data_type> [NULL | NOT NULL] <tag_options> ::= <option> [, <option> ...] <option> ::= TTL_DURATION [=] <ttl_duration> | TTL_COL [=] <prop_name> | DEFAULT <default_value> CREATE TAG creates a tag with the given name in a graph space. You must have the CREATE privilege for the graph space. To create a tag in a specific graph space, you must use the graph space first. Tag name \u00b6 IF NOT EXISTS : Creating an existent tag results in an error. You can use the IF NOT EXISTS option to conditionally create the tag and avoid the error. NOTE : The tag existence detection here compares only the tag names (excluding properties). - tag_name : The tag name must be unique in a graph space. Once the tag name is set, it can not be altered. The rules for permitted tag names are the same as those for graph space names. For prohibited names, see Keywords and reserved words . Property names and data types \u00b6 prop_name prop_name is the name of the property. It must be unique for each tag. data_type data_type shows the data type of each property. For a full description of the property data types, see Data types . NULL | NOT NULL Specifies if the property supports NULL | NOT NULL . The default value is NULL . DEFAULT Specifies a default value for a property. The default value can be a literal value or an expression supported by Nebula Graph. If no value is specified, the default value is used when inserting a new vertex. Time-to-Live (TTL) \u00b6 TTL_DURATION Specifies the life cycle for the data. Data that exceeds the specified TTL expires. The expiration threshold is the TTL_COL value plus the TTL_DURATION . The default value of TTL_DURATION is zero. It means the data never expires. TTL_COL The data type of prop_name must be either int or timestamp . single TTL definition Only one TTL_COL field can be specified in a tag. For more information on TTL, see TTL options . Examples \u00b6 nebula> CREATE TAG player(name string, age int); // Create a tag with no properties. nebula> CREATE TAG no_property(); // Create a tag with a default value. nebula> CREATE TAG player_with_default(name string, age int DEFAULT 20); // Time interval is 100s, starting from the create_time field nebula> CREATE TAG woman(name string, age int, \\ married bool, salary double, create_time timestamp) \\ TTL_DURATION = 100, TTL_COL = \"create_time\"; // Data expires after TTL_DURATION nebula> CREATE TAG icec_ream(made timestamp, temperature int) \\ TTL_DURATION = 100, TTL_COL = \"made\"; Implementation of the operation \u00b6 Trying to insert vertices with a newly created tag may fail, because the creation of the tag is implemented asynchronously. Nebula Graph implements the creation in the next heartbeat cycle. To make sure the creation is successful, take one of the following approaches: Find the new tag in the result of SHOW TAGS . If you can't, wait a few seconds and try again. Wait for two heartbeat cycles, i.e., 20 seconds. To change the heartbeat interval, modify the heartbeat_interval_secs parameter in the configuration files for all services.","title":"CREATE TAG"},{"location":"3.ngql-guide/10.tag-statements/1.create-tag/#create_tag","text":"CREATE TAG [IF NOT EXISTS] <tag_name> ([<create_definition>, ...]) [tag_options] <create_definition> ::= <prop_name> <data_type> [NULL | NOT NULL] <tag_options> ::= <option> [, <option> ...] <option> ::= TTL_DURATION [=] <ttl_duration> | TTL_COL [=] <prop_name> | DEFAULT <default_value> CREATE TAG creates a tag with the given name in a graph space. You must have the CREATE privilege for the graph space. To create a tag in a specific graph space, you must use the graph space first.","title":"CREATE TAG"},{"location":"3.ngql-guide/10.tag-statements/1.create-tag/#tag_name","text":"IF NOT EXISTS : Creating an existent tag results in an error. You can use the IF NOT EXISTS option to conditionally create the tag and avoid the error. NOTE : The tag existence detection here compares only the tag names (excluding properties). - tag_name : The tag name must be unique in a graph space. Once the tag name is set, it can not be altered. The rules for permitted tag names are the same as those for graph space names. For prohibited names, see Keywords and reserved words .","title":"Tag name"},{"location":"3.ngql-guide/10.tag-statements/1.create-tag/#property_names_and_data_types","text":"prop_name prop_name is the name of the property. It must be unique for each tag. data_type data_type shows the data type of each property. For a full description of the property data types, see Data types . NULL | NOT NULL Specifies if the property supports NULL | NOT NULL . The default value is NULL . DEFAULT Specifies a default value for a property. The default value can be a literal value or an expression supported by Nebula Graph. If no value is specified, the default value is used when inserting a new vertex.","title":"Property names and data types"},{"location":"3.ngql-guide/10.tag-statements/1.create-tag/#time-to-live_ttl","text":"TTL_DURATION Specifies the life cycle for the data. Data that exceeds the specified TTL expires. The expiration threshold is the TTL_COL value plus the TTL_DURATION . The default value of TTL_DURATION is zero. It means the data never expires. TTL_COL The data type of prop_name must be either int or timestamp . single TTL definition Only one TTL_COL field can be specified in a tag. For more information on TTL, see TTL options .","title":"Time-to-Live (TTL)"},{"location":"3.ngql-guide/10.tag-statements/1.create-tag/#examples","text":"nebula> CREATE TAG player(name string, age int); // Create a tag with no properties. nebula> CREATE TAG no_property(); // Create a tag with a default value. nebula> CREATE TAG player_with_default(name string, age int DEFAULT 20); // Time interval is 100s, starting from the create_time field nebula> CREATE TAG woman(name string, age int, \\ married bool, salary double, create_time timestamp) \\ TTL_DURATION = 100, TTL_COL = \"create_time\"; // Data expires after TTL_DURATION nebula> CREATE TAG icec_ream(made timestamp, temperature int) \\ TTL_DURATION = 100, TTL_COL = \"made\";","title":"Examples"},{"location":"3.ngql-guide/10.tag-statements/1.create-tag/#implementation_of_the_operation","text":"Trying to insert vertices with a newly created tag may fail, because the creation of the tag is implemented asynchronously. Nebula Graph implements the creation in the next heartbeat cycle. To make sure the creation is successful, take one of the following approaches: Find the new tag in the result of SHOW TAGS . If you can't, wait a few seconds and try again. Wait for two heartbeat cycles, i.e., 20 seconds. To change the heartbeat interval, modify the heartbeat_interval_secs parameter in the configuration files for all services.","title":"Implementation of the operation"},{"location":"3.ngql-guide/10.tag-statements/2.drop-tag/","text":"DROP TAG \u00b6 DROP TAG [IF EXISTS] <tag_name> DROP TAG drops a tag with the given name in a graph space. You must have the DROP privilege for the graph space. To drop a tag in a specific graph space, you must use the graph space first. Before you drop a tag, make sure that the tag does not contain any indexes or TTLs. If a tag contains any indexes or TTLs, a conflict error ( [ERROR (-8)]: Conflict! ) occurs when you drop it. See drop index . A vertex can have one or more tags. When a vertex has only one tag, after you drop it, the vertex CANNOT be accessible. But its edges are available. The vertex is deleted in the next compaction. When a vertex has multiple tags, after you drop one of them, the vertex is still accessible. But all the properties defined by this dropped tag are not accessible. This operation only deletes the Schema data. All the files and directories in the disk are NOT deleted directly. Data is deleted in the next compaction. Tag name \u00b6 IF EXISTS : Dropping a non-existent tag results in an error. You can use the IF EXISTS option to conditionally drop the tag and avoid the error. NOTE : The tag existence detection here compares only the tag names (excluding properties). tag_name : Specifies the tag name that you want to drop. You can drop only one tag in one statement. Example \u00b6 nebula> CREATE TAG test(p1 string, p2 int); nebula> DROP TAG test;","title":"DROP TAGS"},{"location":"3.ngql-guide/10.tag-statements/2.drop-tag/#drop_tag","text":"DROP TAG [IF EXISTS] <tag_name> DROP TAG drops a tag with the given name in a graph space. You must have the DROP privilege for the graph space. To drop a tag in a specific graph space, you must use the graph space first. Before you drop a tag, make sure that the tag does not contain any indexes or TTLs. If a tag contains any indexes or TTLs, a conflict error ( [ERROR (-8)]: Conflict! ) occurs when you drop it. See drop index . A vertex can have one or more tags. When a vertex has only one tag, after you drop it, the vertex CANNOT be accessible. But its edges are available. The vertex is deleted in the next compaction. When a vertex has multiple tags, after you drop one of them, the vertex is still accessible. But all the properties defined by this dropped tag are not accessible. This operation only deletes the Schema data. All the files and directories in the disk are NOT deleted directly. Data is deleted in the next compaction.","title":"DROP TAG"},{"location":"3.ngql-guide/10.tag-statements/2.drop-tag/#tag_name","text":"IF EXISTS : Dropping a non-existent tag results in an error. You can use the IF EXISTS option to conditionally drop the tag and avoid the error. NOTE : The tag existence detection here compares only the tag names (excluding properties). tag_name : Specifies the tag name that you want to drop. You can drop only one tag in one statement.","title":"Tag name"},{"location":"3.ngql-guide/10.tag-statements/2.drop-tag/#example","text":"nebula> CREATE TAG test(p1 string, p2 int); nebula> DROP TAG test;","title":"Example"},{"location":"3.ngql-guide/10.tag-statements/3.alter-tag/","text":"ALTER TAG \u00b6 ALTER TAG <tag_name> <alter_definition> [, alter_definition] ...] [ttl_definition [, ttl_definition] ... ] alter_definition: | ADD (prop_name data_type) | DROP (prop_name) | CHANGE (prop_name data_type) ttl_definition: TTL_DURATION = ttl_duration, TTL_COL = prop_name ALTER TAG alters the structure of a tag with the given name in a graph space. You must have the ALTER privilege for the graph space. To alter a tag in a specific graph space, you must use the graph space first. You can add or drop properties, change the data type of an existing property. You can also set TTL (Time-To-Live) for a property, or change the TTL duration. TTL_COL only supports the properties whose values are of the INT or the TIMESTAMP type. Before you alter properties for a tag, make sure that the properties are not indexed. If the properties contain any indexes, a conflict error occurs when you alter them. For information about index, see Index . Multiple ADD , DROP , and CHANGE clauses are permitted in a single ALTER statement, separated by commas. Tag name \u00b6 tag_name : Specifies the tag name that you want to alter. You can alter only one tag in one statement. Before you alter a tag, make sure that the tag exists in the graph space. If the tag does not exist, an error occurs when you alter it. Example \u00b6 nebula> CREATE TAG t1 (p1 string, p2 int); nebula> ALTER TAG t1 ADD (p3 int, p4 string); nebula> ALTER TAG t1 TTL_DURATION = 2, TTL_COL = \"p2\"; Implementation of the operation \u00b6 Nebula Graph implements the alteration asynchronously in the next heartbeat cycle. Before the process finishes, the alteration does not take effect. To make sure the alteration is successful, take the following approaches: Use DESCRIBE TAG to confirm that the tag information is updated. If it is not, wait a few seconds and try again. Wait for two heartbeat cycles, i.e., 20 seconds. To change the heartbeat interval, modify the heartbeat_interval_secs parameter in the configuration files for all services.","title":"ALTER TAG"},{"location":"3.ngql-guide/10.tag-statements/3.alter-tag/#alter_tag","text":"ALTER TAG <tag_name> <alter_definition> [, alter_definition] ...] [ttl_definition [, ttl_definition] ... ] alter_definition: | ADD (prop_name data_type) | DROP (prop_name) | CHANGE (prop_name data_type) ttl_definition: TTL_DURATION = ttl_duration, TTL_COL = prop_name ALTER TAG alters the structure of a tag with the given name in a graph space. You must have the ALTER privilege for the graph space. To alter a tag in a specific graph space, you must use the graph space first. You can add or drop properties, change the data type of an existing property. You can also set TTL (Time-To-Live) for a property, or change the TTL duration. TTL_COL only supports the properties whose values are of the INT or the TIMESTAMP type. Before you alter properties for a tag, make sure that the properties are not indexed. If the properties contain any indexes, a conflict error occurs when you alter them. For information about index, see Index . Multiple ADD , DROP , and CHANGE clauses are permitted in a single ALTER statement, separated by commas.","title":"ALTER TAG"},{"location":"3.ngql-guide/10.tag-statements/3.alter-tag/#tag_name","text":"tag_name : Specifies the tag name that you want to alter. You can alter only one tag in one statement. Before you alter a tag, make sure that the tag exists in the graph space. If the tag does not exist, an error occurs when you alter it.","title":"Tag name"},{"location":"3.ngql-guide/10.tag-statements/3.alter-tag/#example","text":"nebula> CREATE TAG t1 (p1 string, p2 int); nebula> ALTER TAG t1 ADD (p3 int, p4 string); nebula> ALTER TAG t1 TTL_DURATION = 2, TTL_COL = \"p2\";","title":"Example"},{"location":"3.ngql-guide/10.tag-statements/3.alter-tag/#implementation_of_the_operation","text":"Nebula Graph implements the alteration asynchronously in the next heartbeat cycle. Before the process finishes, the alteration does not take effect. To make sure the alteration is successful, take the following approaches: Use DESCRIBE TAG to confirm that the tag information is updated. If it is not, wait a few seconds and try again. Wait for two heartbeat cycles, i.e., 20 seconds. To change the heartbeat interval, modify the heartbeat_interval_secs parameter in the configuration files for all services.","title":"Implementation of the operation"},{"location":"3.ngql-guide/10.tag-statements/4.show-tags/","text":"SHOW TAGS \u00b6 SHOW TAGS SHOW TAGS shows all tags in the current graph space. You do not need any privileges for the graph space to run this statement. But the returned results are different based on role privileges . To show tags in a specific graph space, you must use the graph space first. Examples \u00b6 nebula> SHOW TAGS; +----------+ | Name | +----------+ | \"player\" | +----------+ | \"team\" | +----------+ Got 2 rows (time spent 1461/2114 us)","title":"SHOW TAGS"},{"location":"3.ngql-guide/10.tag-statements/4.show-tags/#show_tags","text":"SHOW TAGS SHOW TAGS shows all tags in the current graph space. You do not need any privileges for the graph space to run this statement. But the returned results are different based on role privileges . To show tags in a specific graph space, you must use the graph space first.","title":"SHOW TAGS"},{"location":"3.ngql-guide/10.tag-statements/4.show-tags/#examples","text":"nebula> SHOW TAGS; +----------+ | Name | +----------+ | \"player\" | +----------+ | \"team\" | +----------+ Got 2 rows (time spent 1461/2114 us)","title":"Examples"},{"location":"3.ngql-guide/10.tag-statements/5.describe-tag/","text":"DESCRIBE TAG \u00b6 DESC[RIBE] TAG <tag_name> DESCRIBE TAG returns information about a tag with the given name in a graph space. You must have the read Schema privilege for the graph space. To describe a tag in a specific graph space, you must use the graph space first. You can use DESC instead of DESCRIBE for short. DESCRIBE TAG is different from SHOW TAGS . For details about SHOW TAGS , see SHOW TAGS . Example \u00b6 Get information about a tag named player . nebula> DESCRIBE TAG player; +--------+----------+-------+-----------+ | Field | Type | Null | Default | +--------+----------+-------+-----------+ | \"name\" | \"string\" | \"YES\" | __EMPTY__ | +--------+----------+-------+-----------+ | \"age\" | \"int64\" | \"YES\" | __EMPTY__ | +--------+----------+-------+-----------+","title":"DESCRIBE TAG"},{"location":"3.ngql-guide/10.tag-statements/5.describe-tag/#describe_tag","text":"DESC[RIBE] TAG <tag_name> DESCRIBE TAG returns information about a tag with the given name in a graph space. You must have the read Schema privilege for the graph space. To describe a tag in a specific graph space, you must use the graph space first. You can use DESC instead of DESCRIBE for short. DESCRIBE TAG is different from SHOW TAGS . For details about SHOW TAGS , see SHOW TAGS .","title":"DESCRIBE TAG"},{"location":"3.ngql-guide/10.tag-statements/5.describe-tag/#example","text":"Get information about a tag named player . nebula> DESCRIBE TAG player; +--------+----------+-------+-----------+ | Field | Type | Null | Default | +--------+----------+-------+-----------+ | \"name\" | \"string\" | \"YES\" | __EMPTY__ | +--------+----------+-------+-----------+ | \"age\" | \"int64\" | \"YES\" | __EMPTY__ | +--------+----------+-------+-----------+","title":"Example"},{"location":"3.ngql-guide/11.edge-type-statements/1.create-edge/","text":"CREATE EDGE \u00b6 CREATE EDGE [IF NOT EXISTS] <edge_type_name> ([<create_definition>, ...]) [edge_type_options] <create_definition> ::= <prop_name> <data_type> <edge_type_options> ::= <option> [, <option> ...] <option> ::= TTL_DURATION [=] <ttl_duration> | TTL_COL [=] <prop_name> | DEFAULT <default_value> CREATE EDGE creates an edge type with the given name in a graph space. You must have the CREATE privilege for the graph space. To create an edge type in a specific graph space, you must use the graph space first. Edge type name \u00b6 IF NOT EXISTS : Creating an existent edge type causes an error. You can use the IF NOT EXISTS option to conditionally create the edge type and avoid the error. NOTE : The edge type existence detection here compares only the edge type names (excluding properties). - edge_type_name : The edge type name must be unique in a graph space. Once the edge type name is set, it can not be altered. The rules for permitted edge type names are the same as those for graph space names. For prohibited names, see Keywords and reserved words . Property names and data types \u00b6 prop_name prop_name is the name of the property. It must be unique for each edge type. data_type data_type shows the data type of each property. For a full description of the property data types, see Data types . NULL | NOT NULL Specifies if the property supports NULL | NOT NULL . The default value is NULL . DEFAULT Specifies a default value for a property. The default value can be a literal value or an expression supported by Nebula Graph. If no value is specified, the default value is used when inserting a new vertex. Time-to-Live (TTL) \u00b6 TTL_DURATION Specifies the life cycle for the data. Data that exceeds the specified TTL expires. The expiration threshold is the TTL_COL value plus the TTL_DURATION . The default value of TTL_DURATION is 0 . It means the data never expires. TTL_COL The data type of prop_name must be either int or timestamp . single TTL definition Only one TTL_COL field can be specified in an edge type. For more information about TTL, see TTL options . Examples \u00b6 nebula> CREATE EDGE follow(degree int); // Create an edge type with no properties. nebula> CREATE EDGE no_property(); // Create an edge type with a default value. nebula> CREATE EDGE follow_with_default(degree int DEFAULT 20); // Time interval is 100s, starting from the p2 field // Data expires after TTL_DURATION nebula> CREATE EDGE e1(p1 string, p2 int, \\ p3 timestamp) \\ TTL_DURATION = 100, TTL_COL = \"p2\"; Implementation of the operation \u00b6 Trying to insert edges of a newly created edge type may fail, because the creation of the edge type is implemented asynchronously. Nebula Graph implements the creation in the next heartbeat cycle. To make sure the creation is successful, take the following approaches: Find the new edge type in the result of SHOW EDGES . If you can't, wait a few seconds and try again. Wait for two heartbeat cycles, i.e., 20 seconds. To change the heartbeat interval, modify the heartbeat_interval_secs parameter in the configuration files for all services.","title":"CREATE EDGE"},{"location":"3.ngql-guide/11.edge-type-statements/1.create-edge/#create_edge","text":"CREATE EDGE [IF NOT EXISTS] <edge_type_name> ([<create_definition>, ...]) [edge_type_options] <create_definition> ::= <prop_name> <data_type> <edge_type_options> ::= <option> [, <option> ...] <option> ::= TTL_DURATION [=] <ttl_duration> | TTL_COL [=] <prop_name> | DEFAULT <default_value> CREATE EDGE creates an edge type with the given name in a graph space. You must have the CREATE privilege for the graph space. To create an edge type in a specific graph space, you must use the graph space first.","title":"CREATE EDGE"},{"location":"3.ngql-guide/11.edge-type-statements/1.create-edge/#edge_type_name","text":"IF NOT EXISTS : Creating an existent edge type causes an error. You can use the IF NOT EXISTS option to conditionally create the edge type and avoid the error. NOTE : The edge type existence detection here compares only the edge type names (excluding properties). - edge_type_name : The edge type name must be unique in a graph space. Once the edge type name is set, it can not be altered. The rules for permitted edge type names are the same as those for graph space names. For prohibited names, see Keywords and reserved words .","title":"Edge type name"},{"location":"3.ngql-guide/11.edge-type-statements/1.create-edge/#property_names_and_data_types","text":"prop_name prop_name is the name of the property. It must be unique for each edge type. data_type data_type shows the data type of each property. For a full description of the property data types, see Data types . NULL | NOT NULL Specifies if the property supports NULL | NOT NULL . The default value is NULL . DEFAULT Specifies a default value for a property. The default value can be a literal value or an expression supported by Nebula Graph. If no value is specified, the default value is used when inserting a new vertex.","title":"Property names and data types"},{"location":"3.ngql-guide/11.edge-type-statements/1.create-edge/#time-to-live_ttl","text":"TTL_DURATION Specifies the life cycle for the data. Data that exceeds the specified TTL expires. The expiration threshold is the TTL_COL value plus the TTL_DURATION . The default value of TTL_DURATION is 0 . It means the data never expires. TTL_COL The data type of prop_name must be either int or timestamp . single TTL definition Only one TTL_COL field can be specified in an edge type. For more information about TTL, see TTL options .","title":"Time-to-Live (TTL)"},{"location":"3.ngql-guide/11.edge-type-statements/1.create-edge/#examples","text":"nebula> CREATE EDGE follow(degree int); // Create an edge type with no properties. nebula> CREATE EDGE no_property(); // Create an edge type with a default value. nebula> CREATE EDGE follow_with_default(degree int DEFAULT 20); // Time interval is 100s, starting from the p2 field // Data expires after TTL_DURATION nebula> CREATE EDGE e1(p1 string, p2 int, \\ p3 timestamp) \\ TTL_DURATION = 100, TTL_COL = \"p2\";","title":"Examples"},{"location":"3.ngql-guide/11.edge-type-statements/1.create-edge/#implementation_of_the_operation","text":"Trying to insert edges of a newly created edge type may fail, because the creation of the edge type is implemented asynchronously. Nebula Graph implements the creation in the next heartbeat cycle. To make sure the creation is successful, take the following approaches: Find the new edge type in the result of SHOW EDGES . If you can't, wait a few seconds and try again. Wait for two heartbeat cycles, i.e., 20 seconds. To change the heartbeat interval, modify the heartbeat_interval_secs parameter in the configuration files for all services.","title":"Implementation of the operation"},{"location":"3.ngql-guide/11.edge-type-statements/2.drop-edge/","text":"DROP EDGE \u00b6 DROP EDGE [IF EXISTS] <edge_type_name> DROP EDGE drops an edge type with the given name in a graph space. You must have the DROP privilege for the graph space. To drop an edge type in a specific graph space, you must use the graph space first. Before you drop an edge type, make sure that the edge type does not contain any indexes or TTLs. If an edge type contains any indexes or TTLs, a conflict error occurs ( [ERROR (-8)]: Conflict! ) when you drop it. See drop index . An edge can have only one edge type. After you drop it, the edge CANNOT be accessible. The edge is deleted in the next compaction. Edge type name \u00b6 IF EXISTS : Dropping a non-existent edge type causes an error. You can use the IF EXISTS option to conditionally drop the edge type and avoid the error. NOTE : The edge type existence detection here compares only the edge type names (excluding properties). edge_type_name : Specifies the edge type name that you want to drop. You can drop only one edge type in one statement. Example \u00b6 nebula> CREATE EDGE e1(p1 string, p2 int); nebula> DROP EDGE e1;","title":"DROP EDGE"},{"location":"3.ngql-guide/11.edge-type-statements/2.drop-edge/#drop_edge","text":"DROP EDGE [IF EXISTS] <edge_type_name> DROP EDGE drops an edge type with the given name in a graph space. You must have the DROP privilege for the graph space. To drop an edge type in a specific graph space, you must use the graph space first. Before you drop an edge type, make sure that the edge type does not contain any indexes or TTLs. If an edge type contains any indexes or TTLs, a conflict error occurs ( [ERROR (-8)]: Conflict! ) when you drop it. See drop index . An edge can have only one edge type. After you drop it, the edge CANNOT be accessible. The edge is deleted in the next compaction.","title":"DROP EDGE"},{"location":"3.ngql-guide/11.edge-type-statements/2.drop-edge/#edge_type_name","text":"IF EXISTS : Dropping a non-existent edge type causes an error. You can use the IF EXISTS option to conditionally drop the edge type and avoid the error. NOTE : The edge type existence detection here compares only the edge type names (excluding properties). edge_type_name : Specifies the edge type name that you want to drop. You can drop only one edge type in one statement.","title":"Edge type name"},{"location":"3.ngql-guide/11.edge-type-statements/2.drop-edge/#example","text":"nebula> CREATE EDGE e1(p1 string, p2 int); nebula> DROP EDGE e1;","title":"Example"},{"location":"3.ngql-guide/11.edge-type-statements/3.alter-edge/","text":"ALTER EDGE \u00b6 ALTER EDGE <edge_type_name> <alter_definition> [, alter_definition] ...] [ttl_definition [, ttl_definition] ... ] alter_definition: | ADD (prop_name data_type) | DROP (prop_name) | CHANGE (prop_name data_type) ttl_definition: TTL_DURATION = ttl_duration, TTL_COL = prop_name ALTER EDGE alters the structure of an edge type with the given name in a graph space. You must have the ALTER privilege for the graph space. To alter an edge type in a specific graph space, you must use the graph space first. You can add or drop properties, change the data type of an existing property. You can also set TTL (Time-To-Live) for a property, or change the TTL duration. TTL_COL only supports INT or TIMESTAMP type properties. Before you alter properties for an edge type, make sure that the properties are not indexed. If the properties contain any indexes, a conflict error occurs when you alter them. For information about index, see Index . Multiple ADD , DROP , and CHANGE clauses are permitted in a single ALTER statement, separated by commas. Edge type name \u00b6 edge_type_name specifies the edge type name that you want to alter. You can alter only one edge type in one statement. Before you alter an edge type, make sure that the edge type exists in the graph space. If the edge type does not exist, an error occurs when you alter it. Example \u00b6 nebula> CREATE EDGE e1(p1 string, p2 int); nebula> ALTER EDGE e1 ADD (p3 int, p4 string); nebula> ALTER EDGE e1 TTL_DURATION = 2, TTL_COL = \"p2\"; Implementation of the operation \u00b6 Nebula Graph implements the alteration asynchronously in the next heartbeat cycle. Before the process finishes, the alteration does not take effect. To make sure the alteration is successful, take the following approaches: Use DESCRIBE EDGE to confirm that the edge information is updated. If it is not, wait a few seconds and try again. Wait for two heartbeat cycles, i.e., 20 seconds. To change the heartbeat interval, modify the heartbeat_interval_secs parameter in the configuration files for all services.","title":"ALTER EDGE"},{"location":"3.ngql-guide/11.edge-type-statements/3.alter-edge/#alter_edge","text":"ALTER EDGE <edge_type_name> <alter_definition> [, alter_definition] ...] [ttl_definition [, ttl_definition] ... ] alter_definition: | ADD (prop_name data_type) | DROP (prop_name) | CHANGE (prop_name data_type) ttl_definition: TTL_DURATION = ttl_duration, TTL_COL = prop_name ALTER EDGE alters the structure of an edge type with the given name in a graph space. You must have the ALTER privilege for the graph space. To alter an edge type in a specific graph space, you must use the graph space first. You can add or drop properties, change the data type of an existing property. You can also set TTL (Time-To-Live) for a property, or change the TTL duration. TTL_COL only supports INT or TIMESTAMP type properties. Before you alter properties for an edge type, make sure that the properties are not indexed. If the properties contain any indexes, a conflict error occurs when you alter them. For information about index, see Index . Multiple ADD , DROP , and CHANGE clauses are permitted in a single ALTER statement, separated by commas.","title":"ALTER EDGE"},{"location":"3.ngql-guide/11.edge-type-statements/3.alter-edge/#edge_type_name","text":"edge_type_name specifies the edge type name that you want to alter. You can alter only one edge type in one statement. Before you alter an edge type, make sure that the edge type exists in the graph space. If the edge type does not exist, an error occurs when you alter it.","title":"Edge type name"},{"location":"3.ngql-guide/11.edge-type-statements/3.alter-edge/#example","text":"nebula> CREATE EDGE e1(p1 string, p2 int); nebula> ALTER EDGE e1 ADD (p3 int, p4 string); nebula> ALTER EDGE e1 TTL_DURATION = 2, TTL_COL = \"p2\";","title":"Example"},{"location":"3.ngql-guide/11.edge-type-statements/3.alter-edge/#implementation_of_the_operation","text":"Nebula Graph implements the alteration asynchronously in the next heartbeat cycle. Before the process finishes, the alteration does not take effect. To make sure the alteration is successful, take the following approaches: Use DESCRIBE EDGE to confirm that the edge information is updated. If it is not, wait a few seconds and try again. Wait for two heartbeat cycles, i.e., 20 seconds. To change the heartbeat interval, modify the heartbeat_interval_secs parameter in the configuration files for all services.","title":"Implementation of the operation"},{"location":"3.ngql-guide/11.edge-type-statements/4.show-edges/","text":"SHOW EDGES \u00b6 SHOW EDGES SHOW EDGES shows all edge types in the current graph space. You do not need any privileges for the graph space to run this statement. But the returned results are different based on role privileges . To show edge types in a specific graph space, you must use the graph space first. Examples \u00b6 nebula> SHOW EDGES; +----------+ | Name | +----------+ | \"follow\" | +----------+ | \"serve\" | +----------+ Got 2 rows (time spent 1039/1687 us)","title":"SHOW EDGES"},{"location":"3.ngql-guide/11.edge-type-statements/4.show-edges/#show_edges","text":"SHOW EDGES SHOW EDGES shows all edge types in the current graph space. You do not need any privileges for the graph space to run this statement. But the returned results are different based on role privileges . To show edge types in a specific graph space, you must use the graph space first.","title":"SHOW EDGES"},{"location":"3.ngql-guide/11.edge-type-statements/4.show-edges/#examples","text":"nebula> SHOW EDGES; +----------+ | Name | +----------+ | \"follow\" | +----------+ | \"serve\" | +----------+ Got 2 rows (time spent 1039/1687 us)","title":"Examples"},{"location":"3.ngql-guide/11.edge-type-statements/5.describe-edge/","text":"DESCRIBE EDGE \u00b6 DESC[RIBE] EDGE <edge_type_name> DESCRIBE EDGE returns information about an edge type with the given name in a graph space. You must have the read Schema privilege for the graph space. To describe an edge type in a specific graph space, you must use the graph space first. You can use DESC instead of DESCRIBE for short. DESCRIBE EDGE is different from SHOW EDGE . For details about SHOW EDGE , see SHOW EDGE . Example \u00b6 Get information about an edge type named follow . nebula> DESCRIBE EDGE follow; +----------+---------+-------+-----------+ | Field | Type | Null | Default | +----------+---------+-------+-----------+ | \"degree\" | \"int64\" | \"YES\" | __EMPTY__ | +----------+---------+-------+-----------+","title":"DESCRIBE EDGE"},{"location":"3.ngql-guide/11.edge-type-statements/5.describe-edge/#describe_edge","text":"DESC[RIBE] EDGE <edge_type_name> DESCRIBE EDGE returns information about an edge type with the given name in a graph space. You must have the read Schema privilege for the graph space. To describe an edge type in a specific graph space, you must use the graph space first. You can use DESC instead of DESCRIBE for short. DESCRIBE EDGE is different from SHOW EDGE . For details about SHOW EDGE , see SHOW EDGE .","title":"DESCRIBE EDGE"},{"location":"3.ngql-guide/11.edge-type-statements/5.describe-edge/#example","text":"Get information about an edge type named follow . nebula> DESCRIBE EDGE follow; +----------+---------+-------+-----------+ | Field | Type | Null | Default | +----------+---------+-------+-----------+ | \"degree\" | \"int64\" | \"YES\" | __EMPTY__ | +----------+---------+-------+-----------+","title":"Example"},{"location":"3.ngql-guide/12.vertex-statements/1.insert-vertex/","text":"INSERT VERTEX \u00b6 INSERT VERTEX <tag_name> (<prop_name_list>) [, <tag_name> (<prop_name_list>), ...] {VALUES | VALUE} VID: (<prop_value_list>[, <prop_value_list>]) prop_name_list: [prop_name [, prop_name] ...] prop_value_list: [prop_value [, prop_value] ...] The INSERT VERTEX statement inserts one or more vertices into a graph space in a Nebula Graph instance. tag_name denotes the tag (vertex type), which must be created before INSERT VERTEX . prop_name_list contains the names of the properties on the tag. VID is the vertex ID. In Nebula Graph 2.X, string and integer VID types are supported. The VID type is set when a graph space is created. For detail information on the maximum VID length, see CREATE SPACE . prop_value_list must provide the property values according to the prop_name_list . If the property values do not match the data type in the tag, an error is returned. When the NOT NULL constraint is set for a given property, an error is returned if no property is given. When the default value for a property is NULL , you can omit to specify the property value. For details, see CREATE TAG . Examples \u00b6 nebula> CREATE TAG t1(); -- Create tag t1 with no property nebula> INSERT VERTEX t1() VALUE \"10\":(); -- Insert vertex \"10\" with no property nebula> CREATE TAG t2 (name string, age int); -- Create tag t2 with two properties nebula> INSERT VERTEX t2 (name, age) VALUES \"11\":(\"n1\", 12); -- Insert vertex \"11\" with two properties nebula> INSERT VERTEX t2 (name, age) VALUES \"12\":(\"n1\", \"a13\"); -- Failed. \"a13\" is not int nebula> INSERT VERTEX t2 (name, age) VALUES \"13\":(\"n3\", 12), \"14\":(\"n4\", 8); -- Insert two vertices nebula> CREATE TAG t3(p1 int); nebula> CREATE TAG t4(p2 string); nebula> INSERT VERTEX t3 (p1), t4(p2) VALUES \"21\": (321, \"hello\"); -- Insert vertex \"21\" with two tags. A vertex can be inserted/written multiple times. Only the last written values can be read. // Insert vertex \"11\" with the new values. nebula> INSERT VERTEX t2 (name, age) VALUES \"11\":(\"n2\", 13); nebula> INSERT VERTEX t2 (name, age) VALUES \"11\":(\"n3\", 14); nebula> INSERT VERTEX t2 (name, age) VALUES \"11\":(\"n4\", 15); // Only the last version can be read nebula> FETCH PROP ON t2 \"11\"; +---------------------------------+ | vertices_ | +---------------------------------+ | (\"11\" :t2{age: 15, name: \"n4\"}) | +---------------------------------+ nebula> CREATE TAG t5(p1 fixed_string(5) NOT NULL, p2 int, p3 int DEFAULT NULL); nebula> INSERT VERTEX t5(p1, p2, p3) VALUES \"001\":(\"Abe\", 2, 3); nebula> INSERT VERTEX t5(p1, p2, p3) VALUES \"002\":(NULL, 4, 5); [ERROR (-8)]: Storage Error: The not null field cannot be null. nebula> INSERT VERTEX t5(p1, p2) VALUES \"003\":(\"cd\", 5); // The value for p3 is the default NULL. nebula> FETCH PROP ON t5 \"003\"; +--------------------------------------------+ | vertices_ | +--------------------------------------------+ | (\"003\" :t5{p1: \"cd\", p2: 5, p3: __NULL__}) | +--------------------------------------------+ nebula> INSERT VERTEX t5(p1, p2) VALUES \"004\":(\"shalalalala\", 4); // The allowed maximum length for property p1 is 5. nebula> FETCH PROP on t5 \"004\"; +-----------------------------------------------+ | vertices_ | +-----------------------------------------------+ | (\"004\" :t5{p1: \"shala\", p2: 4, p3: __NULL__}) | +-----------------------------------------------+","title":"INSERT VERTEX"},{"location":"3.ngql-guide/12.vertex-statements/1.insert-vertex/#insert_vertex","text":"INSERT VERTEX <tag_name> (<prop_name_list>) [, <tag_name> (<prop_name_list>), ...] {VALUES | VALUE} VID: (<prop_value_list>[, <prop_value_list>]) prop_name_list: [prop_name [, prop_name] ...] prop_value_list: [prop_value [, prop_value] ...] The INSERT VERTEX statement inserts one or more vertices into a graph space in a Nebula Graph instance. tag_name denotes the tag (vertex type), which must be created before INSERT VERTEX . prop_name_list contains the names of the properties on the tag. VID is the vertex ID. In Nebula Graph 2.X, string and integer VID types are supported. The VID type is set when a graph space is created. For detail information on the maximum VID length, see CREATE SPACE . prop_value_list must provide the property values according to the prop_name_list . If the property values do not match the data type in the tag, an error is returned. When the NOT NULL constraint is set for a given property, an error is returned if no property is given. When the default value for a property is NULL , you can omit to specify the property value. For details, see CREATE TAG .","title":"INSERT VERTEX"},{"location":"3.ngql-guide/12.vertex-statements/1.insert-vertex/#examples","text":"nebula> CREATE TAG t1(); -- Create tag t1 with no property nebula> INSERT VERTEX t1() VALUE \"10\":(); -- Insert vertex \"10\" with no property nebula> CREATE TAG t2 (name string, age int); -- Create tag t2 with two properties nebula> INSERT VERTEX t2 (name, age) VALUES \"11\":(\"n1\", 12); -- Insert vertex \"11\" with two properties nebula> INSERT VERTEX t2 (name, age) VALUES \"12\":(\"n1\", \"a13\"); -- Failed. \"a13\" is not int nebula> INSERT VERTEX t2 (name, age) VALUES \"13\":(\"n3\", 12), \"14\":(\"n4\", 8); -- Insert two vertices nebula> CREATE TAG t3(p1 int); nebula> CREATE TAG t4(p2 string); nebula> INSERT VERTEX t3 (p1), t4(p2) VALUES \"21\": (321, \"hello\"); -- Insert vertex \"21\" with two tags. A vertex can be inserted/written multiple times. Only the last written values can be read. // Insert vertex \"11\" with the new values. nebula> INSERT VERTEX t2 (name, age) VALUES \"11\":(\"n2\", 13); nebula> INSERT VERTEX t2 (name, age) VALUES \"11\":(\"n3\", 14); nebula> INSERT VERTEX t2 (name, age) VALUES \"11\":(\"n4\", 15); // Only the last version can be read nebula> FETCH PROP ON t2 \"11\"; +---------------------------------+ | vertices_ | +---------------------------------+ | (\"11\" :t2{age: 15, name: \"n4\"}) | +---------------------------------+ nebula> CREATE TAG t5(p1 fixed_string(5) NOT NULL, p2 int, p3 int DEFAULT NULL); nebula> INSERT VERTEX t5(p1, p2, p3) VALUES \"001\":(\"Abe\", 2, 3); nebula> INSERT VERTEX t5(p1, p2, p3) VALUES \"002\":(NULL, 4, 5); [ERROR (-8)]: Storage Error: The not null field cannot be null. nebula> INSERT VERTEX t5(p1, p2) VALUES \"003\":(\"cd\", 5); // The value for p3 is the default NULL. nebula> FETCH PROP ON t5 \"003\"; +--------------------------------------------+ | vertices_ | +--------------------------------------------+ | (\"003\" :t5{p1: \"cd\", p2: 5, p3: __NULL__}) | +--------------------------------------------+ nebula> INSERT VERTEX t5(p1, p2) VALUES \"004\":(\"shalalalala\", 4); // The allowed maximum length for property p1 is 5. nebula> FETCH PROP on t5 \"004\"; +-----------------------------------------------+ | vertices_ | +-----------------------------------------------+ | (\"004\" :t5{p1: \"shala\", p2: 4, p3: __NULL__}) | +-----------------------------------------------+","title":"Examples"},{"location":"3.ngql-guide/12.vertex-statements/2.update-vertex/","text":"UPDATE VERTEX \u00b6 UPDATE VERTEX <vid> SET <update_columns> [WHEN <condition>] [YIELD <columns>] Use UPDATE VERTEX to update properties on a vertex. The UPDATE VERTEX statement only updates one tag of a vertex at a time. Nebula Graph supports compare-and-set (CAS) operation. NOTE: WHEN and YIELD are optional. vid is the ID of the vertex to be updated. update_columns is the properties of the vertex to be updated. For example, tag1.col1 = $^.tag2.col2 + 1 means to update tag1.col1 to tag2.col2+1 . NOTE: $^ indicates the vertex to be updated. condition is some constraints. Only when the constraints are met, UPDATE is executed successfully. condition supports expression operations. columns is the columns to be returned. YIELD returns the latest updated values. Consider the following example: nebula> UPDATE VERTEX \"player100\" \\ SET player.age = $^.player.age + 1 \\ WHEN $^.player.name == \"Tim Duncan\" \\ YIELD $^.player.name AS name, $^.player.age AS age; +--------------+-----+ | name | age | +--------------+-----+ | \"Tim Duncan\" | 43 | +--------------+-----+","title":"UPDATE VERTEX"},{"location":"3.ngql-guide/12.vertex-statements/2.update-vertex/#update_vertex","text":"UPDATE VERTEX <vid> SET <update_columns> [WHEN <condition>] [YIELD <columns>] Use UPDATE VERTEX to update properties on a vertex. The UPDATE VERTEX statement only updates one tag of a vertex at a time. Nebula Graph supports compare-and-set (CAS) operation. NOTE: WHEN and YIELD are optional. vid is the ID of the vertex to be updated. update_columns is the properties of the vertex to be updated. For example, tag1.col1 = $^.tag2.col2 + 1 means to update tag1.col1 to tag2.col2+1 . NOTE: $^ indicates the vertex to be updated. condition is some constraints. Only when the constraints are met, UPDATE is executed successfully. condition supports expression operations. columns is the columns to be returned. YIELD returns the latest updated values. Consider the following example: nebula> UPDATE VERTEX \"player100\" \\ SET player.age = $^.player.age + 1 \\ WHEN $^.player.name == \"Tim Duncan\" \\ YIELD $^.player.name AS name, $^.player.age AS age; +--------------+-----+ | name | age | +--------------+-----+ | \"Tim Duncan\" | 43 | +--------------+-----+","title":"UPDATE VERTEX"},{"location":"3.ngql-guide/12.vertex-statements/3.upsert-vertex/","text":"UPSERT VERTEX \u00b6 UPSERT VERTEX <vid> SET <update_columns> [WHEN <condition>] [YIELD <columns>] vid is the ID of the vertex to be updated. update_columns is the properties of the vertex to be updated. For example, tag1.col1 = $^.tag2.col2 + 1 means to update tag1.col1 to tag2.col2+1 . NOTE: $^ indicates the vertex to be updated. condition is some constraints. Only when the conditions are met, UPSERT is executed successfully. condition supports expression operations. columns is the columns to be returned, YIELD returns the latest updated values. UPSERT is a combination of UPDATE and INSERT . Use UPSERT VERTEX to update properties on a vertex if it exists or insert a new vertex if it does not exist. The UPDATE VERTEX statement only updates one tag of a vertex at a time. The performance of UPSERT is much lower than that of INSERT , because UPSERT is a read-modify-write serialization operation at the partition level. DON'T: DO NOT use UPSERT for scenarios with highly concurrent writes. If the vertex does not exist, a new vertex is created no matter whether the condition in the WHEN clause is met or not. The property columns not specified by the SET statement use the default values of the columns. If there are no default values, an error is returned. If the vertex exists and the WHEN condition is met, the vertex is updated. If the vertex exists and the WHEN condition is not met, Nebula Graph does nothing. Consider the following example: nebula> INSERT VERTEX player(name, age) VALUES \"player111\":(\"Ben Simmons\", 22); -- Insert a new vertex. nebula> UPSERT VERTEX \"player111\" SET player.name = \"Dwight Howard\", player.age = $^.player.age + 11 WHEN $^.player.name == \"Ben Simmons\" AND $^.player.age > 20 YIELD $^.player.name AS Name, $^.player.age AS Age; -- Do an upsert operation on the vertex. +-----------------+-----+ | Name | Age | +-----------------+-----+ | \"Dwight Howard\" | 33 | +-----------------+-----+ // An empty set is returned. Because vertex \"player123\" does not exist. nebula> FETCH PROP ON * \"player123\"; Empty set (Time spent: 3.069/4.382 ms) nebula> UPSERT VERTEX \"player123\" SET player.age = $^.player.age + 1; If the vertex \"player123\" does not exist and the default value of age is NULL , the player.age of vertex \"player123\" is NULL . If player.age has a default value, the player.age of vertex \"player123\" is the default value plus one. nebula> CREATE TAG person(followers int, age int DEFAULT 0); -- Create example tag person nebula> UPSERT VERTEX \"300\" SET person.followers = $^.person.age + 1, person.age = 8; -- the number of followers is 1, age is 8 nebula> UPSERT VERTEX \"300\" SET person.age = 8, person.followers = $^.person.age + 1; -- the number of followers is 9, age is 8","title":"UPSERT VERTEX"},{"location":"3.ngql-guide/12.vertex-statements/3.upsert-vertex/#upsert_vertex","text":"UPSERT VERTEX <vid> SET <update_columns> [WHEN <condition>] [YIELD <columns>] vid is the ID of the vertex to be updated. update_columns is the properties of the vertex to be updated. For example, tag1.col1 = $^.tag2.col2 + 1 means to update tag1.col1 to tag2.col2+1 . NOTE: $^ indicates the vertex to be updated. condition is some constraints. Only when the conditions are met, UPSERT is executed successfully. condition supports expression operations. columns is the columns to be returned, YIELD returns the latest updated values. UPSERT is a combination of UPDATE and INSERT . Use UPSERT VERTEX to update properties on a vertex if it exists or insert a new vertex if it does not exist. The UPDATE VERTEX statement only updates one tag of a vertex at a time. The performance of UPSERT is much lower than that of INSERT , because UPSERT is a read-modify-write serialization operation at the partition level. DON'T: DO NOT use UPSERT for scenarios with highly concurrent writes. If the vertex does not exist, a new vertex is created no matter whether the condition in the WHEN clause is met or not. The property columns not specified by the SET statement use the default values of the columns. If there are no default values, an error is returned. If the vertex exists and the WHEN condition is met, the vertex is updated. If the vertex exists and the WHEN condition is not met, Nebula Graph does nothing. Consider the following example: nebula> INSERT VERTEX player(name, age) VALUES \"player111\":(\"Ben Simmons\", 22); -- Insert a new vertex. nebula> UPSERT VERTEX \"player111\" SET player.name = \"Dwight Howard\", player.age = $^.player.age + 11 WHEN $^.player.name == \"Ben Simmons\" AND $^.player.age > 20 YIELD $^.player.name AS Name, $^.player.age AS Age; -- Do an upsert operation on the vertex. +-----------------+-----+ | Name | Age | +-----------------+-----+ | \"Dwight Howard\" | 33 | +-----------------+-----+ // An empty set is returned. Because vertex \"player123\" does not exist. nebula> FETCH PROP ON * \"player123\"; Empty set (Time spent: 3.069/4.382 ms) nebula> UPSERT VERTEX \"player123\" SET player.age = $^.player.age + 1; If the vertex \"player123\" does not exist and the default value of age is NULL , the player.age of vertex \"player123\" is NULL . If player.age has a default value, the player.age of vertex \"player123\" is the default value plus one. nebula> CREATE TAG person(followers int, age int DEFAULT 0); -- Create example tag person nebula> UPSERT VERTEX \"300\" SET person.followers = $^.person.age + 1, person.age = 8; -- the number of followers is 1, age is 8 nebula> UPSERT VERTEX \"300\" SET person.age = 8, person.followers = $^.person.age + 1; -- the number of followers is 9, age is 8","title":"UPSERT VERTEX"},{"location":"3.ngql-guide/12.vertex-statements/4.delete-vertex/","text":"DELETE VERTEX \u00b6 DELETE VERTEX <vid> [, <vid> ...] Use DELETE VERTEX to delete vertices and the related incoming and outgoing edges of the vertices. The DELETE VERTEX statement deletes one vertex or multiple vertices at a time. You can use DELETE VERTEX together with pipe. For more information about pipe, see Pipe operator . Examples \u00b6 nebula> DELETE VERTEX \"team1\"; This query deletes the vertex whose ID is \"team1\". nebula> GO FROM \"player100\" OVER serve YIELD serve._dst AS id | DELETE VERTEX $-.id; This query shows that you can use DELETE VERTEX together with pipe. Nebula Graph traverses the incoming and outgoing edges related to the vertices and deletes them all. Then Nebula Graph deletes information related to the vertices. NOTE: Atomic operation is not guaranteed during the entire process for now, so please retry when a failure occurs.","title":"DELETE VERTEX"},{"location":"3.ngql-guide/12.vertex-statements/4.delete-vertex/#delete_vertex","text":"DELETE VERTEX <vid> [, <vid> ...] Use DELETE VERTEX to delete vertices and the related incoming and outgoing edges of the vertices. The DELETE VERTEX statement deletes one vertex or multiple vertices at a time. You can use DELETE VERTEX together with pipe. For more information about pipe, see Pipe operator .","title":"DELETE VERTEX"},{"location":"3.ngql-guide/12.vertex-statements/4.delete-vertex/#examples","text":"nebula> DELETE VERTEX \"team1\"; This query deletes the vertex whose ID is \"team1\". nebula> GO FROM \"player100\" OVER serve YIELD serve._dst AS id | DELETE VERTEX $-.id; This query shows that you can use DELETE VERTEX together with pipe. Nebula Graph traverses the incoming and outgoing edges related to the vertices and deletes them all. Then Nebula Graph deletes information related to the vertices. NOTE: Atomic operation is not guaranteed during the entire process for now, so please retry when a failure occurs.","title":"Examples"},{"location":"3.ngql-guide/12.vertex-statements/5.fetch-vertex/","text":"FETCH VERTEX \u00b6 FETCH PROP ON {<tag_name_list> | *} <vertex_id_list> [YIELD [DISTINCT] <return_list>] tag_name_list ::= [tag_name [, tag_name] ...] vertex_id_list ::= [vertex_id [, vertex_id] ...] return_list ::= [col_name [AS col_alias] [, col_name [AS col_alias]] ...] Use FETCH to retrieve properties for vertices. Pipe and user-defined variables are permitted in a FETCH statement. Multiple tags are permitted in a singe FETCH statement, separated by commas. * returns all the properties of the specified vertex. Multiple vertices are permitted in a singe FETCH statement, separated by commas. YIELD specifies the properties to be returned. For more information, see YIELD . Example \u00b6 nebula> FETCH PROP ON player \"player101\"; +-----------------------------------------------------+ | vertices_ | +-----------------------------------------------------+ | (\"player101\" :player{age: 36, name: \"Tony Parker\"}) | +-----------------------------------------------------+ // Two tags: team and player. nebula> FETCH PROP ON * \"player101\", \"team202\", \"player103\"; +-----------------------------------------------------+ | vertices_ | +-----------------------------------------------------+ | (\"player101\" :player{age: 36, name: \"Tony Parker\"}) | +-----------------------------------------------------+ | (\"team202\" :team{name: \"Rockets\"}) | +-----------------------------------------------------+ | (\"player103\" :player{age: 32, name: \"Rudy Gay\"}) | +-----------------------------------------------------+ nebula> GO FROM \"player100\" OVER follow YIELD follow._dst AS id | \\ FETCH PROP ON player $-.id YIELD player.name, player.age; +-------------+-----------------+------------+ | VertexID | player.name | player.age | +-------------+-----------------+------------+ | \"player101\" | \"Tony Parker\" | 36 | +-------------+-----------------+------------+ | \"player125\" | \"Manu Ginobili\" | 41 | +-------------+-----------------+------------+ nebula> GO FROM \"player100\" OVER follow \\ YIELD follow._dst, $$.player.name, $$.player.age; +-------------+-----------------+---------------+ | follow._dst | $$.player.name | $$.player.age | +-------------+-----------------+---------------+ | \"player101\" | \"Tony Parker\" | 36 | +-------------+-----------------+---------------+ | \"player125\" | \"Manu Ginobili\" | 41 | +-------------+-----------------+---------------+ nebula> $var=GO FROM \"player100\" OVER follow YIELD follow._dst AS id; \\ FETCH PROP ON player $var.id YIELD player.name, player.age; +-------------+-----------------+------------+ | VertexID | player.name | player.age | +-------------+-----------------+------------+ | \"player101\" | \"Tony Parker\" | 36 | +-------------+-----------------+------------+ | \"player125\" | \"Manu Ginobili\" | 41 | +-------------+-----------------+------------+","title":"FETCH VERTEX"},{"location":"3.ngql-guide/12.vertex-statements/5.fetch-vertex/#fetch_vertex","text":"FETCH PROP ON {<tag_name_list> | *} <vertex_id_list> [YIELD [DISTINCT] <return_list>] tag_name_list ::= [tag_name [, tag_name] ...] vertex_id_list ::= [vertex_id [, vertex_id] ...] return_list ::= [col_name [AS col_alias] [, col_name [AS col_alias]] ...] Use FETCH to retrieve properties for vertices. Pipe and user-defined variables are permitted in a FETCH statement. Multiple tags are permitted in a singe FETCH statement, separated by commas. * returns all the properties of the specified vertex. Multiple vertices are permitted in a singe FETCH statement, separated by commas. YIELD specifies the properties to be returned. For more information, see YIELD .","title":"FETCH VERTEX"},{"location":"3.ngql-guide/12.vertex-statements/5.fetch-vertex/#example","text":"nebula> FETCH PROP ON player \"player101\"; +-----------------------------------------------------+ | vertices_ | +-----------------------------------------------------+ | (\"player101\" :player{age: 36, name: \"Tony Parker\"}) | +-----------------------------------------------------+ // Two tags: team and player. nebula> FETCH PROP ON * \"player101\", \"team202\", \"player103\"; +-----------------------------------------------------+ | vertices_ | +-----------------------------------------------------+ | (\"player101\" :player{age: 36, name: \"Tony Parker\"}) | +-----------------------------------------------------+ | (\"team202\" :team{name: \"Rockets\"}) | +-----------------------------------------------------+ | (\"player103\" :player{age: 32, name: \"Rudy Gay\"}) | +-----------------------------------------------------+ nebula> GO FROM \"player100\" OVER follow YIELD follow._dst AS id | \\ FETCH PROP ON player $-.id YIELD player.name, player.age; +-------------+-----------------+------------+ | VertexID | player.name | player.age | +-------------+-----------------+------------+ | \"player101\" | \"Tony Parker\" | 36 | +-------------+-----------------+------------+ | \"player125\" | \"Manu Ginobili\" | 41 | +-------------+-----------------+------------+ nebula> GO FROM \"player100\" OVER follow \\ YIELD follow._dst, $$.player.name, $$.player.age; +-------------+-----------------+---------------+ | follow._dst | $$.player.name | $$.player.age | +-------------+-----------------+---------------+ | \"player101\" | \"Tony Parker\" | 36 | +-------------+-----------------+---------------+ | \"player125\" | \"Manu Ginobili\" | 41 | +-------------+-----------------+---------------+ nebula> $var=GO FROM \"player100\" OVER follow YIELD follow._dst AS id; \\ FETCH PROP ON player $var.id YIELD player.name, player.age; +-------------+-----------------+------------+ | VertexID | player.name | player.age | +-------------+-----------------+------------+ | \"player101\" | \"Tony Parker\" | 36 | +-------------+-----------------+------------+ | \"player125\" | \"Manu Ginobili\" | 41 | +-------------+-----------------+------------+","title":"Example"},{"location":"3.ngql-guide/13.edge-statements/1.insert-edge/","text":"INSERT EDGE \u00b6 INSERT EDGE <edge_type> ( <prop_name_list> ) {VALUES | VALUE} <src_vid> -> <dst_vid>[@<rank>] : ( <prop_value_list> ) [, <src_vid> -> <dst_vid>[@<rank>] : ( <prop_value_list> ), ...] <prop_name_list> ::= [ <prop_name> [, <prop_name> ] ...] <prop_value_list> ::= [ <prop_value> [, <prop_value> ] ...] The INSERT EDGE statement inserts an edge from a source vertex (given by src_vid) to a destination vertex (given by dst_vid). <edge_type> denotes the edge type, which must be created before INSERT EDGE . Only one edge type can be specified in this statement. <prop_name_list> is the property name list in the given <edge_type> . <prop_value_list> must provide the value list according to <prop_name_list> . If the property values do not match the data type in the edge type, an error is returned. When the NOT NULL constraint is set for a given property, an error is returned if no property is given. When the default value for a property is NULL , you can omit to specify the property value. rank is optional. It specifies the edge rank of the same edge type. If not specified, the default value is 0. You can insert many edges with the same edge type for two vertices by using different rank values. OpenCypher compatibility: OpenCypher has no such a concept as rank. Examples \u00b6 nebula> CREATE EDGE e1(); -- create edge type t1 with empty property nebula> INSERT EDGE e1 () VALUES \"10\"->\"11\":(); -- insert an edge from vertex \"10\" to vertex \"11\" with empty property nebula> INSERT EDGE e1 () VALUES \"10\"->\"11\"@1:(); -- insert an edge from vertex \"10\" to vertex \"11\" with empty property, the edge rank is 1 nebula> CREATE EDGE e2 (name string, age int); -- create edge type e2 with two properties nebula> INSERT EDGE e2 (name, age) VALUES \"11\"->\"13\":(\"n1\", 1); -- insert edge from \"11\" to \"13\" with two properties nebula> INSERT EDGE e2 (name, age) VALUES \\ \"12\"->\"13\":(\"n1\", 1), \"13\"->\"14\":(\"n2\", 2); -- insert two edges nebula> INSERT EDGE e2 (name, age) VALUES \"11\"->\"13\":(\"n1\", \"a13\"); -- ERROR. \"a13\" is not int An edge can be inserted/written multiple times. Only the last written values can be read. -- insert edge with the new values. nebula> INSERT EDGE e2 (name, age) VALUES \"11\"->\"13\":(\"n1\", 12); nebula> INSERT EDGE e2 (name, age) VALUES \"11\"->\"13\":(\"n1\", 13); nebula> INSERT EDGE e2 (name, age) VALUES \"11\"->\"13\":(\"n1\", 14); // Only the last write can be read nebula> FETCH PROP ON e2 \"11\"->\"13\"; +-------------------------------------------+ | edges_ | +-------------------------------------------+ | [:e2 \"11\"->\"13\" @0 {age: 14, name: \"n1\"}] | +-------------------------------------------+","title":"INSERT EDGE"},{"location":"3.ngql-guide/13.edge-statements/1.insert-edge/#insert_edge","text":"INSERT EDGE <edge_type> ( <prop_name_list> ) {VALUES | VALUE} <src_vid> -> <dst_vid>[@<rank>] : ( <prop_value_list> ) [, <src_vid> -> <dst_vid>[@<rank>] : ( <prop_value_list> ), ...] <prop_name_list> ::= [ <prop_name> [, <prop_name> ] ...] <prop_value_list> ::= [ <prop_value> [, <prop_value> ] ...] The INSERT EDGE statement inserts an edge from a source vertex (given by src_vid) to a destination vertex (given by dst_vid). <edge_type> denotes the edge type, which must be created before INSERT EDGE . Only one edge type can be specified in this statement. <prop_name_list> is the property name list in the given <edge_type> . <prop_value_list> must provide the value list according to <prop_name_list> . If the property values do not match the data type in the edge type, an error is returned. When the NOT NULL constraint is set for a given property, an error is returned if no property is given. When the default value for a property is NULL , you can omit to specify the property value. rank is optional. It specifies the edge rank of the same edge type. If not specified, the default value is 0. You can insert many edges with the same edge type for two vertices by using different rank values. OpenCypher compatibility: OpenCypher has no such a concept as rank.","title":"INSERT EDGE"},{"location":"3.ngql-guide/13.edge-statements/1.insert-edge/#examples","text":"nebula> CREATE EDGE e1(); -- create edge type t1 with empty property nebula> INSERT EDGE e1 () VALUES \"10\"->\"11\":(); -- insert an edge from vertex \"10\" to vertex \"11\" with empty property nebula> INSERT EDGE e1 () VALUES \"10\"->\"11\"@1:(); -- insert an edge from vertex \"10\" to vertex \"11\" with empty property, the edge rank is 1 nebula> CREATE EDGE e2 (name string, age int); -- create edge type e2 with two properties nebula> INSERT EDGE e2 (name, age) VALUES \"11\"->\"13\":(\"n1\", 1); -- insert edge from \"11\" to \"13\" with two properties nebula> INSERT EDGE e2 (name, age) VALUES \\ \"12\"->\"13\":(\"n1\", 1), \"13\"->\"14\":(\"n2\", 2); -- insert two edges nebula> INSERT EDGE e2 (name, age) VALUES \"11\"->\"13\":(\"n1\", \"a13\"); -- ERROR. \"a13\" is not int An edge can be inserted/written multiple times. Only the last written values can be read. -- insert edge with the new values. nebula> INSERT EDGE e2 (name, age) VALUES \"11\"->\"13\":(\"n1\", 12); nebula> INSERT EDGE e2 (name, age) VALUES \"11\"->\"13\":(\"n1\", 13); nebula> INSERT EDGE e2 (name, age) VALUES \"11\"->\"13\":(\"n1\", 14); // Only the last write can be read nebula> FETCH PROP ON e2 \"11\"->\"13\"; +-------------------------------------------+ | edges_ | +-------------------------------------------+ | [:e2 \"11\"->\"13\" @0 {age: 14, name: \"n1\"}] | +-------------------------------------------+","title":"Examples"},{"location":"3.ngql-guide/13.edge-statements/2.update-edge/","text":"UPDATE EDGE \u00b6 UPDATE EDGE <src_vid> -> <dest_vid> [@<rank>] OF <edge_type> SET <update_properties> [WHEN <condition>] [YIELD <properties>] Use UPDATE EDGE to update properties on an edge. The UPDATE EDGE statement only updates one edge at a time. Nebula Graph supports compare-and-set (CAS). NOTE: WHEN and YIELD are optional. update_properties is the properties of the edge to be updated. For example, e1.col1 = $^.e1.col2 + 1 means to update e1.col1 to e1.col2+1 . NOTE: $^ indicates the edge to be updated. condition is some constraints. Only when the condition is met, UPDATE is executed successfully. condition supports expression operations. properties is the properties to be returned, YIELD returns the latest updated values. Consider the following example: nebula> UPDATE EDGE \"player100\" -> \"team204\"@0 \\ OF serve \\ SET start_year = start_year + 1;","title":"UPDATE EDGE"},{"location":"3.ngql-guide/13.edge-statements/2.update-edge/#update_edge","text":"UPDATE EDGE <src_vid> -> <dest_vid> [@<rank>] OF <edge_type> SET <update_properties> [WHEN <condition>] [YIELD <properties>] Use UPDATE EDGE to update properties on an edge. The UPDATE EDGE statement only updates one edge at a time. Nebula Graph supports compare-and-set (CAS). NOTE: WHEN and YIELD are optional. update_properties is the properties of the edge to be updated. For example, e1.col1 = $^.e1.col2 + 1 means to update e1.col1 to e1.col2+1 . NOTE: $^ indicates the edge to be updated. condition is some constraints. Only when the condition is met, UPDATE is executed successfully. condition supports expression operations. properties is the properties to be returned, YIELD returns the latest updated values. Consider the following example: nebula> UPDATE EDGE \"player100\" -> \"team204\"@0 \\ OF serve \\ SET start_year = start_year + 1;","title":"UPDATE EDGE"},{"location":"3.ngql-guide/13.edge-statements/3.upsert-edge/","text":"UPSERT EDGE \u00b6 UPSERT EDGE <src_vid> -> <dst_vid> [@rank] OF <edge_type> SET <update_properties> [WHEN <condition>] [YIELD <properties>] update_properties is the properties of the edge to be updated. For example, e1.col1 = $^.e1.col2 + 1 means to update e1.col1 to e1.col2+1 . NOTE: $^ indicates the edge to be updated. condition is some constraints. Only when the condition is met, UPSERT is executed successfully. condition supports expression operations. properties specifies the properties to be returned, YIELD returns the latest updated values. UPSERT is a combination of UPDATE and INSERT . Use UPSERT EDGE to update properties on an edge if it exists or insert a new edge if it does not exist. The UPDATE EDGE statement updates only one edge at a time. The performance of UPSERT is much lower than that of INSERT , because UPSERT is a read-modify-write serialization operation at the partition level. DON'T: DO NOT use UPSERT for scenarios with highly concurrent writes. If the edge does not exist, a new edge is created no matter whether the condition in the WHEN clause is met or not. The properties not specified by the SET statement use the default property values. If there are no default values, an error is returned. If the edge exists and the WHEN condition is met, the edge is updated. If the edge exists and the WHEN condition is not met, Nebula Graph does nothing. Consider the following example: //Insert a new edge. nebula> INSERT EDGE serve(start_year, end_year) VALUES \"player100\" -> \"team200\":(1997, 2016); -- nebula> UPSERT EDGE \"player100\" -> \"team200\" OF serve SET start_year = serve.start_year + 2 WHEN serve.end_year == 2016 YIELD serve.start_year AS Start, serve.end_year AS End; +-------+------+ | Start | End | +-------+------+ | 1999 | 2016 | +-------+------+ nebula> FETCH PROP ON serve \"player100\" -> \"team200\"; +-----------------------------------------------------------------------+ | edges_ | +-----------------------------------------------------------------------+ | [:serve \"player100\"->\"team200\" @0 {end_year: 2016, start_year: 1999}] | +-----------------------------------------------------------------------+","title":"UPSERT EDGE"},{"location":"3.ngql-guide/13.edge-statements/3.upsert-edge/#upsert_edge","text":"UPSERT EDGE <src_vid> -> <dst_vid> [@rank] OF <edge_type> SET <update_properties> [WHEN <condition>] [YIELD <properties>] update_properties is the properties of the edge to be updated. For example, e1.col1 = $^.e1.col2 + 1 means to update e1.col1 to e1.col2+1 . NOTE: $^ indicates the edge to be updated. condition is some constraints. Only when the condition is met, UPSERT is executed successfully. condition supports expression operations. properties specifies the properties to be returned, YIELD returns the latest updated values. UPSERT is a combination of UPDATE and INSERT . Use UPSERT EDGE to update properties on an edge if it exists or insert a new edge if it does not exist. The UPDATE EDGE statement updates only one edge at a time. The performance of UPSERT is much lower than that of INSERT , because UPSERT is a read-modify-write serialization operation at the partition level. DON'T: DO NOT use UPSERT for scenarios with highly concurrent writes. If the edge does not exist, a new edge is created no matter whether the condition in the WHEN clause is met or not. The properties not specified by the SET statement use the default property values. If there are no default values, an error is returned. If the edge exists and the WHEN condition is met, the edge is updated. If the edge exists and the WHEN condition is not met, Nebula Graph does nothing. Consider the following example: //Insert a new edge. nebula> INSERT EDGE serve(start_year, end_year) VALUES \"player100\" -> \"team200\":(1997, 2016); -- nebula> UPSERT EDGE \"player100\" -> \"team200\" OF serve SET start_year = serve.start_year + 2 WHEN serve.end_year == 2016 YIELD serve.start_year AS Start, serve.end_year AS End; +-------+------+ | Start | End | +-------+------+ | 1999 | 2016 | +-------+------+ nebula> FETCH PROP ON serve \"player100\" -> \"team200\"; +-----------------------------------------------------------------------+ | edges_ | +-----------------------------------------------------------------------+ | [:serve \"player100\"->\"team200\" @0 {end_year: 2016, start_year: 1999}] | +-----------------------------------------------------------------------+","title":"UPSERT EDGE"},{"location":"3.ngql-guide/13.edge-statements/4.delete-edge/","text":"DELETE EDGE \u00b6 DELETE EDGE <edge_type> <src_vid> -> <dst_vid>[@<rank>] [, <edge_type> <src_vid> -> <dst_vid>[@<rank>] ...] Use DELETE EDGE to delete edges. The DELETE EDGE statement deletes one edge or multiple edges at a time. You can use DELETE EDGE together with pipe. For more information about pipe, see Pipe operator . Examples \u00b6 nebula> DELETE EDGE serve \"player100\" -> \"team200\"@0; This query deletes the serve edge from \"player100\" to \"team200\" , of which the rank value is 0. nebula> GO FROM \"player100\" OVER follow WHERE follow._dst == \"team200\" YIELD follow._src AS src, follow._dst AS dst, follow._rank AS rank | \\ DELETE EDGE follow $-.src->$-.dst @ $-.rank; This query shows that you can use DELETE EDGE together with pipe. This query first traverses all the follow edges with different rank values from \"player100\" to \"team200\" then deletes them. To delete all the outgoing edges for a vertex, delete the vertex. For more information, see DELETE VERTEX . NOTE: Atomic operation is not guaranteed during the entire process for now, so please retry when a failure occurs.","title":"DELETE EDGE"},{"location":"3.ngql-guide/13.edge-statements/4.delete-edge/#delete_edge","text":"DELETE EDGE <edge_type> <src_vid> -> <dst_vid>[@<rank>] [, <edge_type> <src_vid> -> <dst_vid>[@<rank>] ...] Use DELETE EDGE to delete edges. The DELETE EDGE statement deletes one edge or multiple edges at a time. You can use DELETE EDGE together with pipe. For more information about pipe, see Pipe operator .","title":"DELETE EDGE"},{"location":"3.ngql-guide/13.edge-statements/4.delete-edge/#examples","text":"nebula> DELETE EDGE serve \"player100\" -> \"team200\"@0; This query deletes the serve edge from \"player100\" to \"team200\" , of which the rank value is 0. nebula> GO FROM \"player100\" OVER follow WHERE follow._dst == \"team200\" YIELD follow._src AS src, follow._dst AS dst, follow._rank AS rank | \\ DELETE EDGE follow $-.src->$-.dst @ $-.rank; This query shows that you can use DELETE EDGE together with pipe. This query first traverses all the follow edges with different rank values from \"player100\" to \"team200\" then deletes them. To delete all the outgoing edges for a vertex, delete the vertex. For more information, see DELETE VERTEX . NOTE: Atomic operation is not guaranteed during the entire process for now, so please retry when a failure occurs.","title":"Examples"},{"location":"3.ngql-guide/13.edge-statements/5.fetch-edge/","text":"FETCH EDGE \u00b6 FETCH PROP ON <edge_type> <vid> -> <vid>[@<rank>] [, <vid> -> <vid> ...] [YIELD [DISTINCT] <return_list>] return_list ::= [col_name [AS col_alias] [, col_name [AS col_alias]] ...] Use FETCH to retrieve properties for edges. Pipe and user-defined variables are permitted in a FETCH statement. ONLY ONE edge type is permitted in a singe FETCH statement. Multiple edges are permitted in a singe FETCH statement, separated by commas. YIELD specifies the properties to be returned. For more information, see YIELD . Example \u00b6 nebula> FETCH PROP ON follow \"player100\"->\"player101\"; +----------------------------------------------------+ | edges_ | +----------------------------------------------------+ | [:follow \"player100\"->\"player101\" @0 {degree: 95}] | +----------------------------------------------------+ nebula> FETCH PROP ON follow \"player100\"->\"player101\", \"player104\"->\"player101\"; +----------------------------------------------------+ | edges_ | +----------------------------------------------------+ | [:follow \"player100\"->\"player101\" @0 {degree: 95}] | +----------------------------------------------------+ | [:follow \"player104\"->\"player101\" @0 {degree: 50}] | +----------------------------------------------------+ nebula> GO FROM \"player100\" OVER follow \\ YIELD follow._src, follow._dst, follow._rank, follow.degree; +-------------+-------------+--------------+---------------+ | follow._src | follow._dst | follow._rank | follow.degree | +-------------+-------------+--------------+---------------+ | \"player100\" | \"player101\" | 0 | 95 | +-------------+-------------+--------------+---------------+ | \"player100\" | \"player125\" | 0 | 95 | +-------------+-------------+--------------+---------------+ nebula> GO FROM \"player100\" OVER follow \\ YIELD follow._src AS s, follow._dst AS d | \\ FETCH PROP ON follow $-.s -> $-.d YIELD follow.degree; +-------------+-------------+--------------+---------------+ | follow._src | follow._dst | follow._rank | follow.degree | +-------------+-------------+--------------+---------------+ | \"player100\" | \"player101\" | 0 | 95 | +-------------+-------------+--------------+---------------+ | \"player100\" | \"player125\" | 0 | 95 | +-------------+-------------+--------------+---------------+ nebula> $var = GO FROM \"player100\" OVER follow YIELD follow._src AS s, follow._dst AS d; \\ FETCH PROP ON follow $var.s -> $var.d YIELD follow.degree; +-------------+-------------+--------------+---------------+ | follow._src | follow._dst | follow._rank | follow.degree | +-------------+-------------+--------------+---------------+ | \"player100\" | \"player101\" | 0 | 95 | +-------------+-------------+--------------+---------------+ | \"player100\" | \"player125\" | 0 | 95 | +-------------+-------------+--------------+---------------+","title":"FETCH EDGE"},{"location":"3.ngql-guide/13.edge-statements/5.fetch-edge/#fetch_edge","text":"FETCH PROP ON <edge_type> <vid> -> <vid>[@<rank>] [, <vid> -> <vid> ...] [YIELD [DISTINCT] <return_list>] return_list ::= [col_name [AS col_alias] [, col_name [AS col_alias]] ...] Use FETCH to retrieve properties for edges. Pipe and user-defined variables are permitted in a FETCH statement. ONLY ONE edge type is permitted in a singe FETCH statement. Multiple edges are permitted in a singe FETCH statement, separated by commas. YIELD specifies the properties to be returned. For more information, see YIELD .","title":"FETCH EDGE"},{"location":"3.ngql-guide/13.edge-statements/5.fetch-edge/#example","text":"nebula> FETCH PROP ON follow \"player100\"->\"player101\"; +----------------------------------------------------+ | edges_ | +----------------------------------------------------+ | [:follow \"player100\"->\"player101\" @0 {degree: 95}] | +----------------------------------------------------+ nebula> FETCH PROP ON follow \"player100\"->\"player101\", \"player104\"->\"player101\"; +----------------------------------------------------+ | edges_ | +----------------------------------------------------+ | [:follow \"player100\"->\"player101\" @0 {degree: 95}] | +----------------------------------------------------+ | [:follow \"player104\"->\"player101\" @0 {degree: 50}] | +----------------------------------------------------+ nebula> GO FROM \"player100\" OVER follow \\ YIELD follow._src, follow._dst, follow._rank, follow.degree; +-------------+-------------+--------------+---------------+ | follow._src | follow._dst | follow._rank | follow.degree | +-------------+-------------+--------------+---------------+ | \"player100\" | \"player101\" | 0 | 95 | +-------------+-------------+--------------+---------------+ | \"player100\" | \"player125\" | 0 | 95 | +-------------+-------------+--------------+---------------+ nebula> GO FROM \"player100\" OVER follow \\ YIELD follow._src AS s, follow._dst AS d | \\ FETCH PROP ON follow $-.s -> $-.d YIELD follow.degree; +-------------+-------------+--------------+---------------+ | follow._src | follow._dst | follow._rank | follow.degree | +-------------+-------------+--------------+---------------+ | \"player100\" | \"player101\" | 0 | 95 | +-------------+-------------+--------------+---------------+ | \"player100\" | \"player125\" | 0 | 95 | +-------------+-------------+--------------+---------------+ nebula> $var = GO FROM \"player100\" OVER follow YIELD follow._src AS s, follow._dst AS d; \\ FETCH PROP ON follow $var.s -> $var.d YIELD follow.degree; +-------------+-------------+--------------+---------------+ | follow._src | follow._dst | follow._rank | follow.degree | +-------------+-------------+--------------+---------------+ | \"player100\" | \"player101\" | 0 | 95 | +-------------+-------------+--------------+---------------+ | \"player100\" | \"player125\" | 0 | 95 | +-------------+-------------+--------------+---------------+","title":"Example"},{"location":"3.ngql-guide/14.native-index-statements/","text":"Index overview \u00b6 Indexes are built to fast process graph queries. Nebula Graph supports two kinds of indexes: native indexes and full-text indexes. This topic introduces the index types and helps choose the right index. Native indexes \u00b6 Native indexes allow querying data based on a given property. There are two kinds of native indexes: tag index and edge type index. Native indexes must be updated manually. You can use the REBUILD INDEX statement to update native indexes. Native indexes support indexing multiple properties on a tag or an edge type (composite indexes), but do not support indexing across multiple tags or edge types. You can do partial match search by using composite indexes. Use composite indexes only for partial match searches when the declared fields in the composite index are used from left to right. For more information, see LOOKUP FAQ . String operators like CONTAINS and STARTS WITH are not allowed in native index searching. Use full-text indexes to do fuzzy search. Operations on native indexes \u00b6 You can do the following operations against native indexes: Create index Show index Describe index Rebuild index Show index status Drop index Query index Full-text indexes \u00b6 Full-text indexes are used to do prefix, wildcard, regexp, and fuzzy search on a string property. Full-text indexes allow indexing just one property. Only strings within a specified length (no longer than 256 bytes) are indexed. Full-text indexes do not support logical operations such as AND , OR and NOT . To do complete text match, use native indexes. Operations on full-text indexes \u00b6 Before doing any operations on full-text indexes, please mak sure that you deploy full-text indexes. Details on full-text indexes deployment, see Deploy Elasticsearch and Deploy Listener . At this time, full-text indexes are created automatically on the Elasticsearch cluster. And rebuilding or altering full-text indexes are not supported. To drop full-text indexes, you need to drop them on the Elasticsearch cluster manually. To query full-text indexes, see Search with full-text indexes . Null values \u00b6 Indexes do not support indexing null values at this time. Range queries \u00b6 In addition to querying single results from native indexes, you can also do range queries. Not all the native indexes support range queries. You can only do range search for numeric, date, and time type properties.","title":"Index overview"},{"location":"3.ngql-guide/14.native-index-statements/#index_overview","text":"Indexes are built to fast process graph queries. Nebula Graph supports two kinds of indexes: native indexes and full-text indexes. This topic introduces the index types and helps choose the right index.","title":"Index overview"},{"location":"3.ngql-guide/14.native-index-statements/#native_indexes","text":"Native indexes allow querying data based on a given property. There are two kinds of native indexes: tag index and edge type index. Native indexes must be updated manually. You can use the REBUILD INDEX statement to update native indexes. Native indexes support indexing multiple properties on a tag or an edge type (composite indexes), but do not support indexing across multiple tags or edge types. You can do partial match search by using composite indexes. Use composite indexes only for partial match searches when the declared fields in the composite index are used from left to right. For more information, see LOOKUP FAQ . String operators like CONTAINS and STARTS WITH are not allowed in native index searching. Use full-text indexes to do fuzzy search.","title":"Native indexes"},{"location":"3.ngql-guide/14.native-index-statements/#operations_on_native_indexes","text":"You can do the following operations against native indexes: Create index Show index Describe index Rebuild index Show index status Drop index Query index","title":"Operations on native indexes"},{"location":"3.ngql-guide/14.native-index-statements/#full-text_indexes","text":"Full-text indexes are used to do prefix, wildcard, regexp, and fuzzy search on a string property. Full-text indexes allow indexing just one property. Only strings within a specified length (no longer than 256 bytes) are indexed. Full-text indexes do not support logical operations such as AND , OR and NOT . To do complete text match, use native indexes.","title":"Full-text indexes"},{"location":"3.ngql-guide/14.native-index-statements/#operations_on_full-text_indexes","text":"Before doing any operations on full-text indexes, please mak sure that you deploy full-text indexes. Details on full-text indexes deployment, see Deploy Elasticsearch and Deploy Listener . At this time, full-text indexes are created automatically on the Elasticsearch cluster. And rebuilding or altering full-text indexes are not supported. To drop full-text indexes, you need to drop them on the Elasticsearch cluster manually. To query full-text indexes, see Search with full-text indexes .","title":"Operations on full-text indexes"},{"location":"3.ngql-guide/14.native-index-statements/#null_values","text":"Indexes do not support indexing null values at this time.","title":"Null values"},{"location":"3.ngql-guide/14.native-index-statements/#range_queries","text":"In addition to querying single results from native indexes, you can also do range queries. Not all the native indexes support range queries. You can only do range search for numeric, date, and time type properties.","title":"Range queries"},{"location":"3.ngql-guide/14.native-index-statements/1.create-native-index/","text":"CREATE INDEX \u00b6 Use CREATE INDEX to add native indexes for existing tags, edge types or properties. NOTE: For how to create text-based indexes, see Deploy full-text index . Most graph queries start the traversal from a list of vertices or edges that are identified by their properties. Indexes make these global retrieval operations efficient on large graphs. Prerequisites \u00b6 Before you create an index, make sure that the relative tag or edge type is created. For how to create tags or edge types, see CREATE TAG and CREATE EDGE . Must-read for using index \u00b6 Correct use of indexes can speed up queries, but indexes can dramatically reduce the write performance. The performance reduction can be as much as 90% or even more. DO NOT use indexes in production environments unless you are fully aware of their influences on your service. If you must use indexes, we suggest that you: Import data into Nebula Graph. Create indexes. Rebuild the indexes. The preceding workflow minimizes the negative influences of using indexes. Syntax \u00b6 CREATE {TAG | EDGE} INDEX [IF NOT EXISTS] <index_name> ON {<tag_name> | <edge_name>} ([prop_name_list]) IF NOT EXISTS : Creating an existent index results in an error. You can use the IF NOT EXISTS option to conditionally create the index and avoid the error. prop_name_list : To index a variable string property, you must use the prop_name(length) syntax to specify an index length. NOTE: Long indexes decrease the scan performance of the Storage Service and use more memory. We suggest that you set the indexing length the same as that of the longest string to be indexed. The longest indexing length is 255. Strings longer than 255 are truncated. To index a fixed-length string property, you must use the prop_name syntax, and the index length is the string length you set. To index a tag or an edge type, ignore the prop_name_list in the parentheses. DON'T: DO NOT index a tag or an edge type if you have indexed any properties in the tag or edge type. Implementation of the operation \u00b6 Nebula Graph implements the creation of the index asynchronously in the next heartbeat cycle. To make sure the creation is successful, take one of the following approaches: Find the new index in the result of SHOW TAG/EDGE INDEXES . Wait for two heartbeat cycles, i.e., 20 seconds. To change the heartbeat interval, modify the heartbeat_interval_secs parameter in the [configuration files] for all services. Create tag/edge type indexes \u00b6 The following statement creates an index on the player tag. nebula> CREATE TAG INDEX player_index on player(); The following statement creates indexes on the edge type like . nebula> CREATE EDGE INDEX like_index on like(); After indexing a tag or an edge type, you can use the LOOKUP statement to retrieve the VID of all vertices with the tag, or the source vertex ID, destination vertex ID, and ranks of all edges with the edge type. For more information, see List vertices or edges with a tag or an edge type . Create single-property indexes \u00b6 nebula> CREATE TAG INDEX player_index_0 on player(name(10)); The preceding statement creates an index for the name property on all vertices carrying the player tag. This statement creates an index using the first 10 characters of the name property. nebula> CREATE TAG var_string(p1 string); nebula> CREATE TAG INDEX var ON var_string(p1(10)); nebula> CREATE TAG fix_string(p1 FIXED_STRING(10)); nebula> CREATE TAG INDEX fix ON fix_string(p1); nebula> CREATE EDGE INDEX follow_index_0 on follow(degree); The preceding statement creates an index for the degree property on all edges carrying the follow edge type. Create composite property indexes \u00b6 An index on multiple properties is called a composite index. NOTE : Creating index across multiple tags is not supported. Consider the following example: nebula> CREATE TAG INDEX player_index_1 on player(name(10), age); This statement creates a composite index for the name and age property on all vertices carrying the player tag. Using index \u00b6 After the index is created and data is inserted, you can use the LOOKUP statement to query the data. You do not need to specify which indexes to use in a query, Nebula Graph figures that out by itself.","title":"CREATE INDEX"},{"location":"3.ngql-guide/14.native-index-statements/1.create-native-index/#create_index","text":"Use CREATE INDEX to add native indexes for existing tags, edge types or properties. NOTE: For how to create text-based indexes, see Deploy full-text index . Most graph queries start the traversal from a list of vertices or edges that are identified by their properties. Indexes make these global retrieval operations efficient on large graphs.","title":"CREATE INDEX"},{"location":"3.ngql-guide/14.native-index-statements/1.create-native-index/#prerequisites","text":"Before you create an index, make sure that the relative tag or edge type is created. For how to create tags or edge types, see CREATE TAG and CREATE EDGE .","title":"Prerequisites"},{"location":"3.ngql-guide/14.native-index-statements/1.create-native-index/#must-read_for_using_index","text":"Correct use of indexes can speed up queries, but indexes can dramatically reduce the write performance. The performance reduction can be as much as 90% or even more. DO NOT use indexes in production environments unless you are fully aware of their influences on your service. If you must use indexes, we suggest that you: Import data into Nebula Graph. Create indexes. Rebuild the indexes. The preceding workflow minimizes the negative influences of using indexes.","title":"Must-read for using index"},{"location":"3.ngql-guide/14.native-index-statements/1.create-native-index/#syntax","text":"CREATE {TAG | EDGE} INDEX [IF NOT EXISTS] <index_name> ON {<tag_name> | <edge_name>} ([prop_name_list]) IF NOT EXISTS : Creating an existent index results in an error. You can use the IF NOT EXISTS option to conditionally create the index and avoid the error. prop_name_list : To index a variable string property, you must use the prop_name(length) syntax to specify an index length. NOTE: Long indexes decrease the scan performance of the Storage Service and use more memory. We suggest that you set the indexing length the same as that of the longest string to be indexed. The longest indexing length is 255. Strings longer than 255 are truncated. To index a fixed-length string property, you must use the prop_name syntax, and the index length is the string length you set. To index a tag or an edge type, ignore the prop_name_list in the parentheses. DON'T: DO NOT index a tag or an edge type if you have indexed any properties in the tag or edge type.","title":"Syntax"},{"location":"3.ngql-guide/14.native-index-statements/1.create-native-index/#implementation_of_the_operation","text":"Nebula Graph implements the creation of the index asynchronously in the next heartbeat cycle. To make sure the creation is successful, take one of the following approaches: Find the new index in the result of SHOW TAG/EDGE INDEXES . Wait for two heartbeat cycles, i.e., 20 seconds. To change the heartbeat interval, modify the heartbeat_interval_secs parameter in the [configuration files] for all services.","title":"Implementation of the operation"},{"location":"3.ngql-guide/14.native-index-statements/1.create-native-index/#create_tagedge_type_indexes","text":"The following statement creates an index on the player tag. nebula> CREATE TAG INDEX player_index on player(); The following statement creates indexes on the edge type like . nebula> CREATE EDGE INDEX like_index on like(); After indexing a tag or an edge type, you can use the LOOKUP statement to retrieve the VID of all vertices with the tag, or the source vertex ID, destination vertex ID, and ranks of all edges with the edge type. For more information, see List vertices or edges with a tag or an edge type .","title":"Create tag/edge type indexes"},{"location":"3.ngql-guide/14.native-index-statements/1.create-native-index/#create_single-property_indexes","text":"nebula> CREATE TAG INDEX player_index_0 on player(name(10)); The preceding statement creates an index for the name property on all vertices carrying the player tag. This statement creates an index using the first 10 characters of the name property. nebula> CREATE TAG var_string(p1 string); nebula> CREATE TAG INDEX var ON var_string(p1(10)); nebula> CREATE TAG fix_string(p1 FIXED_STRING(10)); nebula> CREATE TAG INDEX fix ON fix_string(p1); nebula> CREATE EDGE INDEX follow_index_0 on follow(degree); The preceding statement creates an index for the degree property on all edges carrying the follow edge type.","title":"Create single-property indexes"},{"location":"3.ngql-guide/14.native-index-statements/1.create-native-index/#create_composite_property_indexes","text":"An index on multiple properties is called a composite index. NOTE : Creating index across multiple tags is not supported. Consider the following example: nebula> CREATE TAG INDEX player_index_1 on player(name(10), age); This statement creates a composite index for the name and age property on all vertices carrying the player tag.","title":"Create composite property indexes"},{"location":"3.ngql-guide/14.native-index-statements/1.create-native-index/#using_index","text":"After the index is created and data is inserted, you can use the LOOKUP statement to query the data. You do not need to specify which indexes to use in a query, Nebula Graph figures that out by itself.","title":"Using index"},{"location":"3.ngql-guide/14.native-index-statements/2.1.show-create-index/","text":"SHOW CREATE INDEX \u00b6 SHOW CREATE INDEX shows the statement that an index was created with. You can find the detailed information of the index, such as the property that the index is created for. Syntax \u00b6 SHOW CREATE {TAG | EDGE} INDEX <index_name>; Examples \u00b6 You can run SHOW TAG INDEXES to list all tag indexes, and then use SHOW CREATE TAG INDEX to show how a tag index was created. nebula> SHOW TAG INDEXES; +------------------+ | Names | +------------------+ | \"player_index_0\" | +------------------+ | \"player_index_1\" | +------------------+ nebula> SHOW CREATE TAG INDEX player_index_1; +------------------+--------------------------------------------------+ | Tag Index Name | Create Tag Index | +------------------+--------------------------------------------------+ | \"player_index_1\" | \"CREATE TAG INDEX `player_index_1` ON `player` ( | | | `name(20)` | | | )\" | +------------------+--------------------------------------------------+ Edge indexes can be queried through a similar approach: nebula> SHOW EDGE INDEXES; +----------------+ | Names | +----------------+ | \"index_follow\" | +----------------+ nebula> SHOW CREATE EDGE INDEX index index_follow; +-----------------+-------------------------------------------------+ | Edge Index Name | Create Edge Index | +-----------------+-------------------------------------------------+ | \"index_follow\" | \"CREATE EDGE INDEX `index_follow` ON `follow` ( | | | `degree` | | | )\" | +-----------------+-------------------------------------------------+","title":"SHOW CREATE INDEX"},{"location":"3.ngql-guide/14.native-index-statements/2.1.show-create-index/#show_create_index","text":"SHOW CREATE INDEX shows the statement that an index was created with. You can find the detailed information of the index, such as the property that the index is created for.","title":"SHOW CREATE INDEX"},{"location":"3.ngql-guide/14.native-index-statements/2.1.show-create-index/#syntax","text":"SHOW CREATE {TAG | EDGE} INDEX <index_name>;","title":"Syntax"},{"location":"3.ngql-guide/14.native-index-statements/2.1.show-create-index/#examples","text":"You can run SHOW TAG INDEXES to list all tag indexes, and then use SHOW CREATE TAG INDEX to show how a tag index was created. nebula> SHOW TAG INDEXES; +------------------+ | Names | +------------------+ | \"player_index_0\" | +------------------+ | \"player_index_1\" | +------------------+ nebula> SHOW CREATE TAG INDEX player_index_1; +------------------+--------------------------------------------------+ | Tag Index Name | Create Tag Index | +------------------+--------------------------------------------------+ | \"player_index_1\" | \"CREATE TAG INDEX `player_index_1` ON `player` ( | | | `name(20)` | | | )\" | +------------------+--------------------------------------------------+ Edge indexes can be queried through a similar approach: nebula> SHOW EDGE INDEXES; +----------------+ | Names | +----------------+ | \"index_follow\" | +----------------+ nebula> SHOW CREATE EDGE INDEX index index_follow; +-----------------+-------------------------------------------------+ | Edge Index Name | Create Edge Index | +-----------------+-------------------------------------------------+ | \"index_follow\" | \"CREATE EDGE INDEX `index_follow` ON `follow` ( | | | `degree` | | | )\" | +-----------------+-------------------------------------------------+","title":"Examples"},{"location":"3.ngql-guide/14.native-index-statements/2.show-native-indexes/","text":"Show INDEXES \u00b6 SHOW {TAG | EDGE} INDEXES Use SHOW INDEXES to list the defined tag or edge type indexes names. Example \u00b6 nebula> SHOW TAG INDEXES; +------------------+ | Names | +------------------+ | \"fix\" | +------------------+ | \"player_index_0\" | +------------------+ | \"player_index_1\" | +------------------+ | \"var\" | +------------------+ nebula> SHOW EDGE INDEXES; +------------------+ | Names | +------------------+ | \"follow_index_0\" | +------------------+","title":"SHOW INDEX"},{"location":"3.ngql-guide/14.native-index-statements/2.show-native-indexes/#show_indexes","text":"SHOW {TAG | EDGE} INDEXES Use SHOW INDEXES to list the defined tag or edge type indexes names.","title":"Show INDEXES"},{"location":"3.ngql-guide/14.native-index-statements/2.show-native-indexes/#example","text":"nebula> SHOW TAG INDEXES; +------------------+ | Names | +------------------+ | \"fix\" | +------------------+ | \"player_index_0\" | +------------------+ | \"player_index_1\" | +------------------+ | \"var\" | +------------------+ nebula> SHOW EDGE INDEXES; +------------------+ | Names | +------------------+ | \"follow_index_0\" | +------------------+","title":"Example"},{"location":"3.ngql-guide/14.native-index-statements/3.describe-native-index/","text":"DESCRIBE INDEX \u00b6 DESCRIBE {TAG | EDGE} INDEX <index_name> Use DESCRIBE INDEX to get information about the index. DESCRIBE INDEX returns the following columns: Field The property name. - Type The property type. Example \u00b6 nebula> DESCRIBE TAG INDEX player_index_0; +--------+--------------------+ | Field | Type | +--------+--------------------+ | \"name\" | \"fixed_string(30)\" | +--------+--------------------+ nebula> DESCRIBE TAG INDEX player_index_1; +--------+--------------------+ | Field | Type | +--------+--------------------+ | \"name\" | \"fixed_string(10)\" | +--------+--------------------+ | \"age\" | \"int64\" | +--------+--------------------+","title":"DESCRIBE INDEX"},{"location":"3.ngql-guide/14.native-index-statements/3.describe-native-index/#describe_index","text":"DESCRIBE {TAG | EDGE} INDEX <index_name> Use DESCRIBE INDEX to get information about the index. DESCRIBE INDEX returns the following columns: Field The property name. - Type The property type.","title":"DESCRIBE INDEX"},{"location":"3.ngql-guide/14.native-index-statements/3.describe-native-index/#example","text":"nebula> DESCRIBE TAG INDEX player_index_0; +--------+--------------------+ | Field | Type | +--------+--------------------+ | \"name\" | \"fixed_string(30)\" | +--------+--------------------+ nebula> DESCRIBE TAG INDEX player_index_1; +--------+--------------------+ | Field | Type | +--------+--------------------+ | \"name\" | \"fixed_string(10)\" | +--------+--------------------+ | \"age\" | \"int64\" | +--------+--------------------+","title":"Example"},{"location":"3.ngql-guide/14.native-index-statements/4.rebuild-native-index/","text":"REBUILD INDEX \u00b6 REBUILD {TAG | EDGE} INDEX [<index_name_list>] <index_name_list>::= [index_name [, index_name] ...] Use REBUILD INDEX to rebuild the created tag or edge type index. For details on how to create an index, see CREATE INDEX . Multiple indexes are permitted in a single REBUILD statement, separated by commas. When the index name is not specified, all tag or edge indexes are rebuilt. If the index is created before data insertion, there is no need to rebuild the index. If data is updated or newly inserted before the index creation, you need to rebuild the indexes to make sure that the indexes contain the previously added data. NOTE: During the rebuilding, all queries skip the index and perform sequential scans. This means that the return results can be different because not all the data is indexed during rebuilding. After rebuilding is complete, you can use the SHOW {TAG | EDGE} INDEX STATUS command to check if the index is successfully rebuilt. For details on index status, see SHOW INDEX STATUS . Example \u00b6 nebula> CREATE TAG person(name string, age int, gender string, email string); Execution succeeded (Time spent: 10.051/11.397 ms) nebula> CREATE TAG INDEX single_person_index ON person(name(10)); Execution succeeded (Time spent: 2.168/3.379 ms) nebula> REBUILD TAG INDEX single_person_index; +------------+ | New Job Id | +------------+ | 66 | +------------+ nebula> SHOW TAG INDEX STATUS; Nebula Graph creates a job to rebuild the index. The job ID is displayed in the preceding return message. To check if the rebuilding process is complete, use the SHOW JOB <job_id> statement. For more information, see SHOW JOB . Legacy version compatibility \u00b6 In Nebula Graph 2.x, the OFFLINE options is no longer needed and not supported.","title":"REBUILD INDEX"},{"location":"3.ngql-guide/14.native-index-statements/4.rebuild-native-index/#rebuild_index","text":"REBUILD {TAG | EDGE} INDEX [<index_name_list>] <index_name_list>::= [index_name [, index_name] ...] Use REBUILD INDEX to rebuild the created tag or edge type index. For details on how to create an index, see CREATE INDEX . Multiple indexes are permitted in a single REBUILD statement, separated by commas. When the index name is not specified, all tag or edge indexes are rebuilt. If the index is created before data insertion, there is no need to rebuild the index. If data is updated or newly inserted before the index creation, you need to rebuild the indexes to make sure that the indexes contain the previously added data. NOTE: During the rebuilding, all queries skip the index and perform sequential scans. This means that the return results can be different because not all the data is indexed during rebuilding. After rebuilding is complete, you can use the SHOW {TAG | EDGE} INDEX STATUS command to check if the index is successfully rebuilt. For details on index status, see SHOW INDEX STATUS .","title":"REBUILD INDEX"},{"location":"3.ngql-guide/14.native-index-statements/4.rebuild-native-index/#example","text":"nebula> CREATE TAG person(name string, age int, gender string, email string); Execution succeeded (Time spent: 10.051/11.397 ms) nebula> CREATE TAG INDEX single_person_index ON person(name(10)); Execution succeeded (Time spent: 2.168/3.379 ms) nebula> REBUILD TAG INDEX single_person_index; +------------+ | New Job Id | +------------+ | 66 | +------------+ nebula> SHOW TAG INDEX STATUS; Nebula Graph creates a job to rebuild the index. The job ID is displayed in the preceding return message. To check if the rebuilding process is complete, use the SHOW JOB <job_id> statement. For more information, see SHOW JOB .","title":"Example"},{"location":"3.ngql-guide/14.native-index-statements/4.rebuild-native-index/#legacy_version_compatibility","text":"In Nebula Graph 2.x, the OFFLINE options is no longer needed and not supported.","title":"Legacy version compatibility"},{"location":"3.ngql-guide/14.native-index-statements/5.show-native-index-status/","text":"SHOW INDEX STATUS \u00b6 SHOW {TAG | EDGE} INDEX STATUS SHOW INDEX STATUS returns the created tag or edge type index status. For details on how to create index, see CREATE INDEX . SHOW INDEX STATUS returns the following fields: Name The index name. Index Status Index Status includes QUEUE , RUNNING , FINISHED , FAILED , STOPPED , INVALID . Example \u00b6 nebula> SHOW TAG INDEX STATUS; +----------------------+--------------+ | Name | Index Status | +----------------------+--------------+ | \"player_index_0\" | \"FINISHED\" | +----------------------+--------------+ | \"player_index_1\" | \"FINISHED\" | +----------------------+--------------+","title":"SHOW INDEX STATUS"},{"location":"3.ngql-guide/14.native-index-statements/5.show-native-index-status/#show_index_status","text":"SHOW {TAG | EDGE} INDEX STATUS SHOW INDEX STATUS returns the created tag or edge type index status. For details on how to create index, see CREATE INDEX . SHOW INDEX STATUS returns the following fields: Name The index name. Index Status Index Status includes QUEUE , RUNNING , FINISHED , FAILED , STOPPED , INVALID .","title":"SHOW INDEX STATUS"},{"location":"3.ngql-guide/14.native-index-statements/5.show-native-index-status/#example","text":"nebula> SHOW TAG INDEX STATUS; +----------------------+--------------+ | Name | Index Status | +----------------------+--------------+ | \"player_index_0\" | \"FINISHED\" | +----------------------+--------------+ | \"player_index_1\" | \"FINISHED\" | +----------------------+--------------+","title":"Example"},{"location":"3.ngql-guide/14.native-index-statements/6.drop-native-index/","text":"DROP INDEX \u00b6 DROP {TAG | EDGE} INDEX [IF EXISTS] <index_name> The DROP INDEX statement removes an existing index from the current graph space. Removing a nonexistent index results in an error. You can use the IF EXISTS option to conditionally drop the index and avoid the error. To run this statement you need some privilege. For information about the built-in roles in Nebula Graph, see Built-in roles . Example \u00b6 nebula> DROP TAG INDEX player_index_0; This query drops a tag index names player_index_0 .","title":"DROP INDEX"},{"location":"3.ngql-guide/14.native-index-statements/6.drop-native-index/#drop_index","text":"DROP {TAG | EDGE} INDEX [IF EXISTS] <index_name> The DROP INDEX statement removes an existing index from the current graph space. Removing a nonexistent index results in an error. You can use the IF EXISTS option to conditionally drop the index and avoid the error. To run this statement you need some privilege. For information about the built-in roles in Nebula Graph, see Built-in roles .","title":"DROP INDEX"},{"location":"3.ngql-guide/14.native-index-statements/6.drop-native-index/#example","text":"nebula> DROP TAG INDEX player_index_0; This query drops a tag index names player_index_0 .","title":"Example"},{"location":"3.ngql-guide/15.full-text-index-statements/1.search-with-text-based-index/","text":"Full-text search \u00b6 LOOKUP ON {<tag> | <edge_type>} WHERE <expression> [YIELD <return_list>] <expression> ::= PREFIX | WILDCARD | REGEXP | FUZZY <return_list> <prop_name> [AS <prop_alias>] [, <prop_name> [AS <prop_alias>] ...] PREFIX(schema_name.prop_name, prefix_string, row_limit, timeout) WILDCARD(schema_name.prop_name, wildcard_string, row_limit, timeout) REGEXP(schema_name.prop_name, regexp_string, row_limit, timeout) FUZZY(schema_name.prop_name, fuzzy_string, fuzziness, operator, row_limit, timeout) fuzziness (optional): Maximum edit distance allowed for matching. The default value is AUTO . For other valid values and more information, see Elasticsearch document . operator (optional): Boolean logic used to interpret text. Valid values are OR (default) and AND . row_limit (optional): Specifies the number of rows to return. The default value is 100. timeout (optional): Specifies the timeout time. The default value is 200ms. Use the LOOKUP ON statement to do full-text search. The search string is specified in the WHERE clause. Before doing a full-text search, make sure that you deployed a Elasticsearch cluster and a Listener cluster. For more information, see Deploy Elasticsearch and Deploy Listener . Before you start \u00b6 Before you start using the full-text index, please make sure that you know the restrictions . Natural language full-text search \u00b6 A natural language search interprets the search string as a phrase in natural human language. The search is case-insensitive. Examples \u00b6 nebula> CREATE SPACE nba (partition_num=3,replica_factor=1, vid_type=fixed_string(30)); nebula> SIGN IN TEXT SERVICE (127.0.0.1:9200); nebula> USE nba; nebula> ADD LISTENER ELASTICSEARCH 192.168.8.5:46780; nebula> CREATE TAG player(name string, age int); nebula> CREATE TAG INDEX name ON player(name(20)); nebula> INSERT VERTEX player(name, age) VALUES \\ \"Russell Westbrook\": (\"Russell Westbrook\", 30), \\ \"Chris Paul\": (\"Chris Paul\", 33),\\ \"Boris Diaw\": (\"Boris Diaw\", 36),\\ \"David West\": (\"David West\", 38),\\ \"Danny Green\": (\"Danny Green\", 31),\\ \"Tim Duncan\": (\"Tim Duncan\", 42),\\ \"James Harden\": (\"James Harden\", 29),\\ \"Tony Parker\": (\"Tony Parker\", 36),\\ \"Aron Baynes\": (\"Aron Baynes\", 32),\\ \"Ben Simmons\": (\"Ben Simmons\", 22),\\ \"Blake Griffin\": (\"Blake Griffin\", 30); nebula> LOOKUP ON player WHERE PREFIX(player.name, \"B\"); +-----------------+ | _vid | +-----------------+ | \"Boris Diaw\" | +-----------------+ | \"Ben Simmons\" | +-----------------+ | \"Blake Griffin\" | +-----------------+ nebula> LOOKUP ON player WHERE WILDCARD(player.name, \"*ri*\") YIELD player.name, player.age; +-----------------+-----------------+-----+ | _vid | name | age | +-----------------+-----------------+-----+ | \"Chris Paul\" | \"Chris Paul\" | 33 | +-----------------+-----------------+-----+ | \"Boris Diaw\" | \"Boris Diaw\" | 36 | +-----------------+-----------------+-----+ | \"Blake Griffin\" | \"Blake Griffin\" | 30 | +-----------------+-----------------+-----+ nebula> LOOKUP ON player WHERE WILDCARD(player.name, \"*ri*\") | YIELD count(*); +----------+ | COUNT(*) | +----------+ | 3 | +----------+ nebula> LOOKUP ON player WHERE REGEXP(player.name, \"R.*\") YIELD player.name, player.age; +---------------------+---------------------+-----+ | _vid | name | age | +---------------------+---------------------+-----+ | \"Russell Westbrook\" | \"Russell Westbrook\" | 30 | +---------------------+---------------------+-----+ nebula> LOOKUP ON player WHERE REGEXP(player.name, \".*\"); +---------------------+ | _vid | +---------------------+ | \"Danny Green\" | +---------------------+ | \"David West\" | +---------------------+ | \"Russell Westbrook\" | +---------------------+ ... nebula> LOOKUP ON player WHERE FUZZY(player.name, \"Tim Dunncan\", AUTO, OR) YIELD player.name; +--------------+--------------+ | _vid | name | +--------------+--------------+ | \"Tim Duncan\" | \"Tim Duncan\" | +--------------+--------------+","title":"Search with full-text index"},{"location":"3.ngql-guide/15.full-text-index-statements/1.search-with-text-based-index/#full-text_search","text":"LOOKUP ON {<tag> | <edge_type>} WHERE <expression> [YIELD <return_list>] <expression> ::= PREFIX | WILDCARD | REGEXP | FUZZY <return_list> <prop_name> [AS <prop_alias>] [, <prop_name> [AS <prop_alias>] ...] PREFIX(schema_name.prop_name, prefix_string, row_limit, timeout) WILDCARD(schema_name.prop_name, wildcard_string, row_limit, timeout) REGEXP(schema_name.prop_name, regexp_string, row_limit, timeout) FUZZY(schema_name.prop_name, fuzzy_string, fuzziness, operator, row_limit, timeout) fuzziness (optional): Maximum edit distance allowed for matching. The default value is AUTO . For other valid values and more information, see Elasticsearch document . operator (optional): Boolean logic used to interpret text. Valid values are OR (default) and AND . row_limit (optional): Specifies the number of rows to return. The default value is 100. timeout (optional): Specifies the timeout time. The default value is 200ms. Use the LOOKUP ON statement to do full-text search. The search string is specified in the WHERE clause. Before doing a full-text search, make sure that you deployed a Elasticsearch cluster and a Listener cluster. For more information, see Deploy Elasticsearch and Deploy Listener .","title":"Full-text search"},{"location":"3.ngql-guide/15.full-text-index-statements/1.search-with-text-based-index/#before_you_start","text":"Before you start using the full-text index, please make sure that you know the restrictions .","title":"Before you start"},{"location":"3.ngql-guide/15.full-text-index-statements/1.search-with-text-based-index/#natural_language_full-text_search","text":"A natural language search interprets the search string as a phrase in natural human language. The search is case-insensitive.","title":"Natural language full-text search"},{"location":"3.ngql-guide/15.full-text-index-statements/1.search-with-text-based-index/#examples","text":"nebula> CREATE SPACE nba (partition_num=3,replica_factor=1, vid_type=fixed_string(30)); nebula> SIGN IN TEXT SERVICE (127.0.0.1:9200); nebula> USE nba; nebula> ADD LISTENER ELASTICSEARCH 192.168.8.5:46780; nebula> CREATE TAG player(name string, age int); nebula> CREATE TAG INDEX name ON player(name(20)); nebula> INSERT VERTEX player(name, age) VALUES \\ \"Russell Westbrook\": (\"Russell Westbrook\", 30), \\ \"Chris Paul\": (\"Chris Paul\", 33),\\ \"Boris Diaw\": (\"Boris Diaw\", 36),\\ \"David West\": (\"David West\", 38),\\ \"Danny Green\": (\"Danny Green\", 31),\\ \"Tim Duncan\": (\"Tim Duncan\", 42),\\ \"James Harden\": (\"James Harden\", 29),\\ \"Tony Parker\": (\"Tony Parker\", 36),\\ \"Aron Baynes\": (\"Aron Baynes\", 32),\\ \"Ben Simmons\": (\"Ben Simmons\", 22),\\ \"Blake Griffin\": (\"Blake Griffin\", 30); nebula> LOOKUP ON player WHERE PREFIX(player.name, \"B\"); +-----------------+ | _vid | +-----------------+ | \"Boris Diaw\" | +-----------------+ | \"Ben Simmons\" | +-----------------+ | \"Blake Griffin\" | +-----------------+ nebula> LOOKUP ON player WHERE WILDCARD(player.name, \"*ri*\") YIELD player.name, player.age; +-----------------+-----------------+-----+ | _vid | name | age | +-----------------+-----------------+-----+ | \"Chris Paul\" | \"Chris Paul\" | 33 | +-----------------+-----------------+-----+ | \"Boris Diaw\" | \"Boris Diaw\" | 36 | +-----------------+-----------------+-----+ | \"Blake Griffin\" | \"Blake Griffin\" | 30 | +-----------------+-----------------+-----+ nebula> LOOKUP ON player WHERE WILDCARD(player.name, \"*ri*\") | YIELD count(*); +----------+ | COUNT(*) | +----------+ | 3 | +----------+ nebula> LOOKUP ON player WHERE REGEXP(player.name, \"R.*\") YIELD player.name, player.age; +---------------------+---------------------+-----+ | _vid | name | age | +---------------------+---------------------+-----+ | \"Russell Westbrook\" | \"Russell Westbrook\" | 30 | +---------------------+---------------------+-----+ nebula> LOOKUP ON player WHERE REGEXP(player.name, \".*\"); +---------------------+ | _vid | +---------------------+ | \"Danny Green\" | +---------------------+ | \"David West\" | +---------------------+ | \"Russell Westbrook\" | +---------------------+ ... nebula> LOOKUP ON player WHERE FUZZY(player.name, \"Tim Dunncan\", AUTO, OR) YIELD player.name; +--------------+--------------+ | _vid | name | +--------------+--------------+ | \"Tim Duncan\" | \"Tim Duncan\" | +--------------+--------------+","title":"Examples"},{"location":"3.ngql-guide/16.subgraph-and-path/1.get-subgraph/","text":"GET SUBGRAPH \u00b6 The GET SUBGRAPH statement retrieves information of vertices and edges reachable from the start vertices over the specified types of edges. Syntax \u00b6 GET SUBGRAPH [<step_count> STEPS] FROM {<vid>, <vid>...} [IN <edge_type>, <edge_type>...] [OUT <edge_type>, <edge_type>...] [BOTH <edge_type>, <edge_type>...] Clause Description STEPS Specifies the steps to go from the start vertices. A step_count must be a non-negative integer. Its default value is 1. When <step_count> is specified to N , the Nebula Graph returns zero to N steps subgraph. FROM Specifies the start vertices. IN Gets the subgraphs from the start vertices over the specified incoming edges (edges pointing to the start vertices). OUT Gets the subgraphs from the start vertices over the specified outgoing edges (edges pointing out from the start vertices). BOTH Gets the subgraphs from the start vertices over the specified types of edges, both incoming and outgoing. When the traversal direction is not specified, both the incoming and outgoing edges are returned. Examples \u00b6 The following graph is used as the sample. Go one step from the vertex with VID \"player100\" over all types of edges and get the subgraph. nebula> GET SUBGRAPH 1 STEPS FROM \"player100\"; +--------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------+ | _vertices | _edges | +--------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------+ | [(player100) player.name:Tim,player.age:42] | [player100-[follow]->player101@0 degree:96,player100-[follow]->player102@0 degree:90,player100-[serve]->team200@0 end_year:2016,start_year:1997] | +--------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------+ | [(player101) player.age:36,player.name:Tony Parker,(player102) player.age:33,player.name:LaMarcus Aldridge,(team200) team.name:Warriors] | [player102-[follow]->player101@0 degree:75] | +--------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------+ Got 2 rows (time spent 6289/7423 us) The returned subgraph is as follows. Go one step from the vertex with VID \"player100\" over incoming follow edges and get the subgraph. nebula> GET SUBGRAPH 1 STEPS FROM \"player100\" IN follow; +-----------+--------+ | _vertices | _edges | +-----------+--------+ | [] | [] | +-----------+--------+ | [] | [] | +-----------+--------+ Got 2 rows (time spent 2292/3091 us) There is no incoming follow edge to \"player100\", so no vertex or edge is returned. Go one step from the vertex \"player100\" over outgoing serve edges and get the subgraph. nebula> GET SUBGRAPH 1 STEPS FROM \"player100\" OUT serve; +---------------------------------------------+--------------------------------------------------------------+ | _vertices | _edges | +---------------------------------------------+--------------------------------------------------------------+ | [(player100) player.age:42,player.name:Tim] | [player100-[serve]->team200@0 start_year:1997,end_year:2016] | +---------------------------------------------+--------------------------------------------------------------+ | [(team200) team.name:Warriors] | [] | +---------------------------------------------+--------------------------------------------------------------+ Got 2 rows (time spent 2107/2547 us) The returned subgraph is as follows.","title":"GET SUBGRAPH"},{"location":"3.ngql-guide/16.subgraph-and-path/1.get-subgraph/#get_subgraph","text":"The GET SUBGRAPH statement retrieves information of vertices and edges reachable from the start vertices over the specified types of edges.","title":"GET SUBGRAPH"},{"location":"3.ngql-guide/16.subgraph-and-path/1.get-subgraph/#syntax","text":"GET SUBGRAPH [<step_count> STEPS] FROM {<vid>, <vid>...} [IN <edge_type>, <edge_type>...] [OUT <edge_type>, <edge_type>...] [BOTH <edge_type>, <edge_type>...] Clause Description STEPS Specifies the steps to go from the start vertices. A step_count must be a non-negative integer. Its default value is 1. When <step_count> is specified to N , the Nebula Graph returns zero to N steps subgraph. FROM Specifies the start vertices. IN Gets the subgraphs from the start vertices over the specified incoming edges (edges pointing to the start vertices). OUT Gets the subgraphs from the start vertices over the specified outgoing edges (edges pointing out from the start vertices). BOTH Gets the subgraphs from the start vertices over the specified types of edges, both incoming and outgoing. When the traversal direction is not specified, both the incoming and outgoing edges are returned.","title":"Syntax"},{"location":"3.ngql-guide/16.subgraph-and-path/1.get-subgraph/#examples","text":"The following graph is used as the sample. Go one step from the vertex with VID \"player100\" over all types of edges and get the subgraph. nebula> GET SUBGRAPH 1 STEPS FROM \"player100\"; +--------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------+ | _vertices | _edges | +--------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------+ | [(player100) player.name:Tim,player.age:42] | [player100-[follow]->player101@0 degree:96,player100-[follow]->player102@0 degree:90,player100-[serve]->team200@0 end_year:2016,start_year:1997] | +--------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------+ | [(player101) player.age:36,player.name:Tony Parker,(player102) player.age:33,player.name:LaMarcus Aldridge,(team200) team.name:Warriors] | [player102-[follow]->player101@0 degree:75] | +--------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------+ Got 2 rows (time spent 6289/7423 us) The returned subgraph is as follows. Go one step from the vertex with VID \"player100\" over incoming follow edges and get the subgraph. nebula> GET SUBGRAPH 1 STEPS FROM \"player100\" IN follow; +-----------+--------+ | _vertices | _edges | +-----------+--------+ | [] | [] | +-----------+--------+ | [] | [] | +-----------+--------+ Got 2 rows (time spent 2292/3091 us) There is no incoming follow edge to \"player100\", so no vertex or edge is returned. Go one step from the vertex \"player100\" over outgoing serve edges and get the subgraph. nebula> GET SUBGRAPH 1 STEPS FROM \"player100\" OUT serve; +---------------------------------------------+--------------------------------------------------------------+ | _vertices | _edges | +---------------------------------------------+--------------------------------------------------------------+ | [(player100) player.age:42,player.name:Tim] | [player100-[serve]->team200@0 start_year:1997,end_year:2016] | +---------------------------------------------+--------------------------------------------------------------+ | [(team200) team.name:Warriors] | [] | +---------------------------------------------+--------------------------------------------------------------+ Got 2 rows (time spent 2107/2547 us) The returned subgraph is as follows.","title":"Examples"},{"location":"3.ngql-guide/16.subgraph-and-path/2.find-path/","text":"FIND PATH \u00b6 FIND { SHORTEST | ALL | NOLOOP } PATH FROM <vertex_id_list> TO <vertex_id_list> OVER <edge_type_list> [REVERSELY | BIDIRECT] [UPTO <N> STEPS] [| ORDER BY $-.path] [| LIMIT <M>] <vertex_id_list> ::= [vertex_id [, vertex_id] ...] The FIND PATH statement finds the paths between the selected source vertices and destination vertices. SHORTEST finds the shortest path. ALL finds all the paths. <vertex_id_list> is a list of vertex IDs separated with commas (,). It supports $- and $var . <edge_type_list> is a list of edge types separated with commas (,). * is all edge types. <N> is the hop number. The default value is 5. <M> specifies the maximum number of rows to return. Limitations \u00b6 When a list of source and/or destination vertex IDs are specified, the shortest path between any source vertices and the destination vertices is returned. There can be cycles when searching all paths. FIND PATH does not support property filtering. FIND PATH does not support specifying a direction. FIND PATH is a single-thread procedure, so it uses much memory. When finding path with the NOLOOP keyword, the returned paths do not include cycles. If NOLOOP is not used, FIND PATH can retrieve paths containing cycles. If NOLOOP is used, FIND PATH can retrieve paths without cycles. Examples \u00b6 In Nebula Console, a path is shown as vertex_id <edge_name, rank> vertex_id . nebula> FIND SHORTEST PATH FROM \"player102\" TO \"team201\" OVER *; +------------------------------------------------------------------+ | path | +------------------------------------------------------------------+ | (\"player102\")-[:follow@0]->(\"player101\")-[:serve@0]->(\"team201\") | +------------------------------------------------------------------+ nebula> FIND SHORTEST PATH FROM \"team200\" TO \"player100\" OVER * REVERSELY; +---------------------------------------+ | path | +---------------------------------------+ | (\"team200\")<-[:serve@0]-(\"player100\") | +---------------------------------------+ nebula> FIND ALL PATH FROM \"player100\" TO \"team200\" OVER *; +---------------------------------------+ | path | +---------------------------------------+ | (\"player100\")-[:serve@0]->(\"team200\") | +---------------------------------------+ nebula> FIND NOLOOP PATH FROM \"player100\" TO \"team200\" OVER *; +---------------------------------------+ | path | +---------------------------------------+ | (\"player100\")-[:serve@0]->(\"team200\") | +---------------------------------------+","title":"FIND PATH"},{"location":"3.ngql-guide/16.subgraph-and-path/2.find-path/#find_path","text":"FIND { SHORTEST | ALL | NOLOOP } PATH FROM <vertex_id_list> TO <vertex_id_list> OVER <edge_type_list> [REVERSELY | BIDIRECT] [UPTO <N> STEPS] [| ORDER BY $-.path] [| LIMIT <M>] <vertex_id_list> ::= [vertex_id [, vertex_id] ...] The FIND PATH statement finds the paths between the selected source vertices and destination vertices. SHORTEST finds the shortest path. ALL finds all the paths. <vertex_id_list> is a list of vertex IDs separated with commas (,). It supports $- and $var . <edge_type_list> is a list of edge types separated with commas (,). * is all edge types. <N> is the hop number. The default value is 5. <M> specifies the maximum number of rows to return.","title":"FIND PATH"},{"location":"3.ngql-guide/16.subgraph-and-path/2.find-path/#limitations","text":"When a list of source and/or destination vertex IDs are specified, the shortest path between any source vertices and the destination vertices is returned. There can be cycles when searching all paths. FIND PATH does not support property filtering. FIND PATH does not support specifying a direction. FIND PATH is a single-thread procedure, so it uses much memory. When finding path with the NOLOOP keyword, the returned paths do not include cycles. If NOLOOP is not used, FIND PATH can retrieve paths containing cycles. If NOLOOP is used, FIND PATH can retrieve paths without cycles.","title":"Limitations"},{"location":"3.ngql-guide/16.subgraph-and-path/2.find-path/#examples","text":"In Nebula Console, a path is shown as vertex_id <edge_name, rank> vertex_id . nebula> FIND SHORTEST PATH FROM \"player102\" TO \"team201\" OVER *; +------------------------------------------------------------------+ | path | +------------------------------------------------------------------+ | (\"player102\")-[:follow@0]->(\"player101\")-[:serve@0]->(\"team201\") | +------------------------------------------------------------------+ nebula> FIND SHORTEST PATH FROM \"team200\" TO \"player100\" OVER * REVERSELY; +---------------------------------------+ | path | +---------------------------------------+ | (\"team200\")<-[:serve@0]-(\"player100\") | +---------------------------------------+ nebula> FIND ALL PATH FROM \"player100\" TO \"team200\" OVER *; +---------------------------------------+ | path | +---------------------------------------+ | (\"player100\")-[:serve@0]->(\"team200\") | +---------------------------------------+ nebula> FIND NOLOOP PATH FROM \"player100\" TO \"team200\" OVER *; +---------------------------------------+ | path | +---------------------------------------+ | (\"player100\")-[:serve@0]->(\"team200\") | +---------------------------------------+","title":"Examples"},{"location":"3.ngql-guide/17.query-tuning-statements/1.explain-and-profile/","text":"EXPLAIN and PROFILE \u00b6 EXPLAIN helps output the execution plan of an nGQL statement without executing the statement. PROFILE executes the statement, then outputs the execution plan as well as the execution profile. You can optimize the queries for better performance with the execution plan and profile. Execution Plan \u00b6 The execution plan is determined by the execution planner in the Nebula Graph query engine. The execution planner processes the parsed nGQL statements into actions. An action is the smallest unit that can be executed. A typical action fetches all neighbors of a given vertex, gets the properties of an edge, or filters vertices or edges based on the given conditions. Each action is assigned to an operator that performs the action. For example, a SHOW TAGS statement is processed into two actions and assigned to a Start operator and a ShowTags operator, while a more complex GO statement may be processed into more than 10 actions and assigned to 10 operators. Syntax \u00b6 EXPLAIN EXPLAIN [format=\"row\" | \"dot\"] <your_nGQL_statement> PROFILE PROFILE [format=\"row\" | \"dot\"] <your_nGQL_statement> Output formats \u00b6 The output of an EXPLAIN or a PROFILE statement has two formats, the default \"row\" format and the \"dot\" format. You can use the format option to modify the output format. Omitting the format option indicates using the default \"row\" format. Format \"row\" \u00b6 The \"row\" format outputs the return message in a table as follows. EXPLAIN : nebula> EXPLAIN format=\"row\" SHOW TAGS; Execution succeeded (time spent 104/705 us) Execution Plan +----+----------+--------------+----------------+-----------------------------------------------------------------------+ | id | name | dependencies | profiling data | operator info | +----+----------+--------------+----------------+-----------------------------------------------------------------------+ | 0 | ShowTags | 2 | | outputVar: [ {\"colNames\":[],\"name\":\"__ShowTags_0\",\"type\":\"DATASET\"}] | | | | | | inputVar: | +----+----------+--------------+----------------+-----------------------------------------------------------------------+ | 2 | Start | | | outputVar: [ {\"colNames\":[],\"name\":\"__Start_2\",\"type\":\"DATASET\"}] | +----+----------+--------------+----------------+-----------------------------------------------------------------------+ PROFILE : nebula> PROFILE format=\"row\" SHOW TAGS; +--------+ | Name | +--------+ | player | +--------+ | team | +--------+ Got 2 rows (time spent 2038/2728 us) Execution Plan +----+----------+--------------+----------------------------------------------------+---------------------------------------------------------------------+ | id | name | dependencies | profiling data | operator info | +----+----------+--------------+----------------------------------------------------+---------------------------------------------------------------------+ | 0 | ShowTags | 2 | ver: 0, rows: 1, execTime: 79us, totalTime: 1692us | outputVar: [{\"colNames\":[],\"name\":\"__ShowTags_0\",\"type\":\"DATASET\"}] | | | | | | inputVar: | +----+----------+--------------+----------------------------------------------------+---------------------------------------------------------------------+ | 2 | Start | | ver: 0, rows: 0, execTime: 1us, totalTime: 57us | outputVar: [{\"colNames\":[],\"name\":\"__Start_2\",\"type\":\"DATASET\"}] | +----+----------+--------------+----------------------------------------------------+---------------------------------------------------------------------+ The descriptions of the columns are as follows: Column Description id Indicates the ID of the operator. name Indicates the name of the operator. dependencies Shows the ID of the operator that the current operator depends on. profiling data Shows the execution profile. ver is the version of the operator, which you can use to identify loops; rows shows the number of rows to be output by the operator; execTime shows the execution time only; totalTime contains the execution time and the system scheduling and queueing time. operator info Shows the detailed information of the operator. Format \"dot\" \u00b6 You can use the format=\"dot\" option to output the return message in the DOT language, and then use Graphviz to generate a graph of the plan. NOTE : Graphviz is open source graph visualization software. Graphviz provides an online tool for previewing DOT language files and exporting them to other formats such as SVG or JSON. For more information, see Graphviz Online . nebula> EXPLAIN format=\"dot\" SHOW TAGS; Execution succeeded (time spent 161/665 us) Execution Plan --------------------------------------------------------------------------------------------------------------------------------------------- ------------- plan --------------------------------------------------------------------------------------------------------------------------------------------- ------------- digraph exec_plan { rankdir=LR; \"ShowTags_0\"[label=\"ShowTags_0|outputVar: \\[\\{\\\"colNames\\\":\\[\\],\\\"name\\\":\\\"__ShowTags_0\\\",\\\"type\\\":\\\"DATASET\\\"\\}\\]\\l|inputVar:\\l\", shape=Mrecord]; \"Start_2\"->\"ShowTags_0\"; \"Start_2\"[label=\"Start_2|outputVar: \\[\\{\\\"colNames\\\":\\[\\],\\\"name\\\":\\\"__Start_2\\\",\\\"type\\\":\\\"DATASET\\\"\\}\\]\\l|inputVar: \\l\", shape=Mrecord]; } --------------------------------------------------------------------------------------------------------------------------------------------- ------------- Transformed into a Graphviz graph, it is as follows:","title":"EXPLAIN and PROFILE"},{"location":"3.ngql-guide/17.query-tuning-statements/1.explain-and-profile/#explain_and_profile","text":"EXPLAIN helps output the execution plan of an nGQL statement without executing the statement. PROFILE executes the statement, then outputs the execution plan as well as the execution profile. You can optimize the queries for better performance with the execution plan and profile.","title":"EXPLAIN and PROFILE"},{"location":"3.ngql-guide/17.query-tuning-statements/1.explain-and-profile/#execution_plan","text":"The execution plan is determined by the execution planner in the Nebula Graph query engine. The execution planner processes the parsed nGQL statements into actions. An action is the smallest unit that can be executed. A typical action fetches all neighbors of a given vertex, gets the properties of an edge, or filters vertices or edges based on the given conditions. Each action is assigned to an operator that performs the action. For example, a SHOW TAGS statement is processed into two actions and assigned to a Start operator and a ShowTags operator, while a more complex GO statement may be processed into more than 10 actions and assigned to 10 operators.","title":"Execution Plan"},{"location":"3.ngql-guide/17.query-tuning-statements/1.explain-and-profile/#syntax","text":"EXPLAIN EXPLAIN [format=\"row\" | \"dot\"] <your_nGQL_statement> PROFILE PROFILE [format=\"row\" | \"dot\"] <your_nGQL_statement>","title":"Syntax"},{"location":"3.ngql-guide/17.query-tuning-statements/1.explain-and-profile/#output_formats","text":"The output of an EXPLAIN or a PROFILE statement has two formats, the default \"row\" format and the \"dot\" format. You can use the format option to modify the output format. Omitting the format option indicates using the default \"row\" format.","title":"Output formats"},{"location":"3.ngql-guide/17.query-tuning-statements/1.explain-and-profile/#format_row","text":"The \"row\" format outputs the return message in a table as follows. EXPLAIN : nebula> EXPLAIN format=\"row\" SHOW TAGS; Execution succeeded (time spent 104/705 us) Execution Plan +----+----------+--------------+----------------+-----------------------------------------------------------------------+ | id | name | dependencies | profiling data | operator info | +----+----------+--------------+----------------+-----------------------------------------------------------------------+ | 0 | ShowTags | 2 | | outputVar: [ {\"colNames\":[],\"name\":\"__ShowTags_0\",\"type\":\"DATASET\"}] | | | | | | inputVar: | +----+----------+--------------+----------------+-----------------------------------------------------------------------+ | 2 | Start | | | outputVar: [ {\"colNames\":[],\"name\":\"__Start_2\",\"type\":\"DATASET\"}] | +----+----------+--------------+----------------+-----------------------------------------------------------------------+ PROFILE : nebula> PROFILE format=\"row\" SHOW TAGS; +--------+ | Name | +--------+ | player | +--------+ | team | +--------+ Got 2 rows (time spent 2038/2728 us) Execution Plan +----+----------+--------------+----------------------------------------------------+---------------------------------------------------------------------+ | id | name | dependencies | profiling data | operator info | +----+----------+--------------+----------------------------------------------------+---------------------------------------------------------------------+ | 0 | ShowTags | 2 | ver: 0, rows: 1, execTime: 79us, totalTime: 1692us | outputVar: [{\"colNames\":[],\"name\":\"__ShowTags_0\",\"type\":\"DATASET\"}] | | | | | | inputVar: | +----+----------+--------------+----------------------------------------------------+---------------------------------------------------------------------+ | 2 | Start | | ver: 0, rows: 0, execTime: 1us, totalTime: 57us | outputVar: [{\"colNames\":[],\"name\":\"__Start_2\",\"type\":\"DATASET\"}] | +----+----------+--------------+----------------------------------------------------+---------------------------------------------------------------------+ The descriptions of the columns are as follows: Column Description id Indicates the ID of the operator. name Indicates the name of the operator. dependencies Shows the ID of the operator that the current operator depends on. profiling data Shows the execution profile. ver is the version of the operator, which you can use to identify loops; rows shows the number of rows to be output by the operator; execTime shows the execution time only; totalTime contains the execution time and the system scheduling and queueing time. operator info Shows the detailed information of the operator.","title":"Format \"row\""},{"location":"3.ngql-guide/17.query-tuning-statements/1.explain-and-profile/#format_dot","text":"You can use the format=\"dot\" option to output the return message in the DOT language, and then use Graphviz to generate a graph of the plan. NOTE : Graphviz is open source graph visualization software. Graphviz provides an online tool for previewing DOT language files and exporting them to other formats such as SVG or JSON. For more information, see Graphviz Online . nebula> EXPLAIN format=\"dot\" SHOW TAGS; Execution succeeded (time spent 161/665 us) Execution Plan --------------------------------------------------------------------------------------------------------------------------------------------- ------------- plan --------------------------------------------------------------------------------------------------------------------------------------------- ------------- digraph exec_plan { rankdir=LR; \"ShowTags_0\"[label=\"ShowTags_0|outputVar: \\[\\{\\\"colNames\\\":\\[\\],\\\"name\\\":\\\"__ShowTags_0\\\",\\\"type\\\":\\\"DATASET\\\"\\}\\]\\l|inputVar:\\l\", shape=Mrecord]; \"Start_2\"->\"ShowTags_0\"; \"Start_2\"[label=\"Start_2|outputVar: \\[\\{\\\"colNames\\\":\\[\\],\\\"name\\\":\\\"__Start_2\\\",\\\"type\\\":\\\"DATASET\\\"\\}\\]\\l|inputVar: \\l\", shape=Mrecord]; } --------------------------------------------------------------------------------------------------------------------------------------------- ------------- Transformed into a Graphviz graph, it is as follows:","title":"Format \"dot\""},{"location":"3.ngql-guide/18.operation-and-maintenance-statements/2.balance-syntax/","text":"BALANCE syntax \u00b6 The BALANCE statements support the load balancing operations of the Nebula Graph Storage services. For more information about storage load balancing and examples for using the BALANCE statements, see Storage load balance . The BALANCE statements are listed as follows. Syntax Description BALANCE DATA Starts a task to balance the distribution of storage partitions in a Nebula Graph cluster. BALANCE DATA <balance_id> Shows the status of the balance task. BALANCE DATA STOP Stops the BALANCE DATA task. BALANCE DATA REMOVE <host_list> Scales in the Nebula Graph cluster and detaches specific storage hosts. BALANCE LEADER Balances the distribution of storage raft leaders in a Nebula Graph cluster.","title":"BALANCE syntax"},{"location":"3.ngql-guide/18.operation-and-maintenance-statements/2.balance-syntax/#balance_syntax","text":"The BALANCE statements support the load balancing operations of the Nebula Graph Storage services. For more information about storage load balancing and examples for using the BALANCE statements, see Storage load balance . The BALANCE statements are listed as follows. Syntax Description BALANCE DATA Starts a task to balance the distribution of storage partitions in a Nebula Graph cluster. BALANCE DATA <balance_id> Shows the status of the balance task. BALANCE DATA STOP Stops the BALANCE DATA task. BALANCE DATA REMOVE <host_list> Scales in the Nebula Graph cluster and detaches specific storage hosts. BALANCE LEADER Balances the distribution of storage raft leaders in a Nebula Graph cluster.","title":"BALANCE syntax"},{"location":"3.ngql-guide/18.operation-and-maintenance-statements/4.job-statements/","text":"Job manager and the JOB statements \u00b6 The long-term tasks running by the Storage Service are called jobs. For example, there are jobs for COMPACT , FLUSH , and STATS . These jobs can be time-consuming if the data size in the graph space is large. The job manager helps you run, show, stop, and recover the jobs. SUBMIT JOB COMPACT \u00b6 The SUBMIT JOB COMPACT statement triggers the long-term RocksDB compact operation. nebula> SUBMIT JOB COMPACT; +------------+ | New Job Id | +------------+ | 40 | +------------+ For more information about compact configuration, see Storage Service configuration . SUBMIT JOB FLUSH \u00b6 The SUBMIT JOB FLUSH statement writes the RocksDB memfile in memory to the hard disk. nebula> SUBMIT JOB FLUSH; +------------+ | New Job Id | +------------+ | 96 | +------------+ SUBMIT JOB STATS \u00b6 The SUBMIT JOB STATS statement starts a job that makes the statistics of the current graph space. Once this job succeeds, you can use the SHOW STATS statement to list the statistics. For more information, see SHOW STATS . NOTE: If the data stored in the graph space changes, in order to get the latest statistics, you have to run SUBMIT JOB STATS again. nebula> SUBMIT JOB STATS; +------------+ | New Job Id | +------------+ | 97 | +------------+ SHOW JOB \u00b6 The Meta Service parses a SUBMIT JOB request into tasks and assigns them to the nebula-storaged processes. The SHOW JOB <job_id> statement shows the information about a specific job and all its tasks. The job ID is created when you run the SUBMIT JOB statement. nebula> SHOW JOB 96; +----------------+---------------+------------+-------------------------+-------------------------+ | Job Id(TaskId) | Command(Dest) | Status | Start Time | Stop Time | +----------------+---------------+------------+-------------------------+-------------------------+ | 96 | \"FLUSH\" | \"FINISHED\" | 2020-11-28T14:14:29.000 | 2020-11-28T14:14:29.000 | +----------------+---------------+------------+-------------------------+-------------------------+ | 0 | \"storaged2\" | \"FINISHED\" | 2020-11-28T14:14:29.000 | 2020-11-28T14:14:29.000 | +----------------+---------------+-------------------------+------------+-------------------------+ | 1 | \"storaged0\" | \"FINISHED\" | 2020-11-28T14:14:29.000 | 2020-11-28T14:14:29.000 | +----------------+---------------+------------+-------------------------+-------------------------+ | 2 | \"storaged1\" | \"FINISHED\" | 2020-11-28T14:14:29.000 | 2020-11-28T14:14:29.000 | +----------------+---------------+------------+-------------------------+-------------------------+ The description of the return message is as follows. Column Description Job Id(TaskId) The first row shows the job ID, and the other rows show the task IDs. Command(Dest) The first row shows the command executed, and the other rows show on which storaged processes the task is running. Status Shows the status of the job or task. For more information about job status, see Job status . Start Time Shows a timestamp indicating the time when the job or task enters the RUNNING phase. Stop Time Shows a timestamp indicating the time when the job or task gets FINISHED , FAILED , or STOPPED . Job status \u00b6 The description of the job status is as follows. Status Description QUEUE The job or task is waiting in a queue. The Start Time is empty in this phase. RUNNING The job or task is running. The Start Time shows the beginning of this phase. FINISHED The job or task is successfully finished. The Stop Time shows the time when the job or task enters this phase. FAILED The job or task failed. STOPPED The job or task is stopped without running. REMOVED The job or task is removed. Status switching is described as follows. Queue -- running -- finished -- removed \\ \\ / \\ \\ -- failed -- / \\ \\ / \\ ---------- stopped -/ SHOW JOBS \u00b6 The SHOW JOBS statement lists all the unexpired jobs. The default job expiration interval is one week. You can change it by modifying the job_expired_secs parameter of the Meta Service. For how to modify job_expired_secs , see Meta Service configuration . nebula> SHOW JOBS; +--------+----------------------+------------+-------------------------+-------------------------+ | Job Id | Command | Status | Start Time | Stop Time | +--------+----------------------+------------+-------------------------+-------------------------+ | 97 | \"STATS\" | \"FINISHED\" | 2020-11-28T14:48:52.000 | 2020-11-28T14:48:52.000 | +--------+----------------------+------------+-------------------------+-------------------------+ | 96 | \"FLUSH\" | \"FINISHED\" | 2020-11-28T14:14:29.000 | 2020-11-28T14:14:29.000 | +--------+----------------------+------------+-------------------------+-------------------------+ | 95 | \"STATS\" | \"FINISHED\" | 2020-11-28T13:02:11.000 | 2020-11-28T13:02:11.000 | +--------+----------------------+------------+-------------------------+-------------------------+ | 86 | \"REBUILD_EDGE_INDEX\" | \"FINISHED\" | 2020-11-26T13:38:24.000 | 2020-11-26T13:38:24.000 | +--------+----------------------+------------+-------------------------+-------------------------+ STOP JOB \u00b6 The STOP JOB statement stops jobs that are not finished. nebula> STOP JOB 22; +---------------+ | Result | +---------------+ | \"Job stopped\" | +---------------+ RECOVER JOB \u00b6 The RECOVER JOB statement re-executes the failed jobs and returns the number of recovered jobs. nebula> RECOVER JOB; +-------------------+ | Recovered job num | +-------------------+ | 5 job recovered | +-------------------+ FAQ \u00b6 How to troubleshoot job problems \u00b6 The SUBMIT JOB operations use the HTTP port. Please check if the HTTP ports on the machines where the Storage Service is running are working well. You can use the following command to debug. curl \"http://{storaged-ip}:19779/admin?space={space_name}&op=compact\"","title":"Job statements"},{"location":"3.ngql-guide/18.operation-and-maintenance-statements/4.job-statements/#job_manager_and_the_job_statements","text":"The long-term tasks running by the Storage Service are called jobs. For example, there are jobs for COMPACT , FLUSH , and STATS . These jobs can be time-consuming if the data size in the graph space is large. The job manager helps you run, show, stop, and recover the jobs.","title":"Job manager and the JOB statements"},{"location":"3.ngql-guide/18.operation-and-maintenance-statements/4.job-statements/#submit_job_compact","text":"The SUBMIT JOB COMPACT statement triggers the long-term RocksDB compact operation. nebula> SUBMIT JOB COMPACT; +------------+ | New Job Id | +------------+ | 40 | +------------+ For more information about compact configuration, see Storage Service configuration .","title":"SUBMIT JOB COMPACT"},{"location":"3.ngql-guide/18.operation-and-maintenance-statements/4.job-statements/#submit_job_flush","text":"The SUBMIT JOB FLUSH statement writes the RocksDB memfile in memory to the hard disk. nebula> SUBMIT JOB FLUSH; +------------+ | New Job Id | +------------+ | 96 | +------------+","title":"SUBMIT JOB FLUSH"},{"location":"3.ngql-guide/18.operation-and-maintenance-statements/4.job-statements/#submit_job_stats","text":"The SUBMIT JOB STATS statement starts a job that makes the statistics of the current graph space. Once this job succeeds, you can use the SHOW STATS statement to list the statistics. For more information, see SHOW STATS . NOTE: If the data stored in the graph space changes, in order to get the latest statistics, you have to run SUBMIT JOB STATS again. nebula> SUBMIT JOB STATS; +------------+ | New Job Id | +------------+ | 97 | +------------+","title":"SUBMIT JOB STATS"},{"location":"3.ngql-guide/18.operation-and-maintenance-statements/4.job-statements/#show_job","text":"The Meta Service parses a SUBMIT JOB request into tasks and assigns them to the nebula-storaged processes. The SHOW JOB <job_id> statement shows the information about a specific job and all its tasks. The job ID is created when you run the SUBMIT JOB statement. nebula> SHOW JOB 96; +----------------+---------------+------------+-------------------------+-------------------------+ | Job Id(TaskId) | Command(Dest) | Status | Start Time | Stop Time | +----------------+---------------+------------+-------------------------+-------------------------+ | 96 | \"FLUSH\" | \"FINISHED\" | 2020-11-28T14:14:29.000 | 2020-11-28T14:14:29.000 | +----------------+---------------+------------+-------------------------+-------------------------+ | 0 | \"storaged2\" | \"FINISHED\" | 2020-11-28T14:14:29.000 | 2020-11-28T14:14:29.000 | +----------------+---------------+-------------------------+------------+-------------------------+ | 1 | \"storaged0\" | \"FINISHED\" | 2020-11-28T14:14:29.000 | 2020-11-28T14:14:29.000 | +----------------+---------------+------------+-------------------------+-------------------------+ | 2 | \"storaged1\" | \"FINISHED\" | 2020-11-28T14:14:29.000 | 2020-11-28T14:14:29.000 | +----------------+---------------+------------+-------------------------+-------------------------+ The description of the return message is as follows. Column Description Job Id(TaskId) The first row shows the job ID, and the other rows show the task IDs. Command(Dest) The first row shows the command executed, and the other rows show on which storaged processes the task is running. Status Shows the status of the job or task. For more information about job status, see Job status . Start Time Shows a timestamp indicating the time when the job or task enters the RUNNING phase. Stop Time Shows a timestamp indicating the time when the job or task gets FINISHED , FAILED , or STOPPED .","title":"SHOW JOB "},{"location":"3.ngql-guide/18.operation-and-maintenance-statements/4.job-statements/#job_status","text":"The description of the job status is as follows. Status Description QUEUE The job or task is waiting in a queue. The Start Time is empty in this phase. RUNNING The job or task is running. The Start Time shows the beginning of this phase. FINISHED The job or task is successfully finished. The Stop Time shows the time when the job or task enters this phase. FAILED The job or task failed. STOPPED The job or task is stopped without running. REMOVED The job or task is removed. Status switching is described as follows. Queue -- running -- finished -- removed \\ \\ / \\ \\ -- failed -- / \\ \\ / \\ ---------- stopped -/","title":"Job status"},{"location":"3.ngql-guide/18.operation-and-maintenance-statements/4.job-statements/#show_jobs","text":"The SHOW JOBS statement lists all the unexpired jobs. The default job expiration interval is one week. You can change it by modifying the job_expired_secs parameter of the Meta Service. For how to modify job_expired_secs , see Meta Service configuration . nebula> SHOW JOBS; +--------+----------------------+------------+-------------------------+-------------------------+ | Job Id | Command | Status | Start Time | Stop Time | +--------+----------------------+------------+-------------------------+-------------------------+ | 97 | \"STATS\" | \"FINISHED\" | 2020-11-28T14:48:52.000 | 2020-11-28T14:48:52.000 | +--------+----------------------+------------+-------------------------+-------------------------+ | 96 | \"FLUSH\" | \"FINISHED\" | 2020-11-28T14:14:29.000 | 2020-11-28T14:14:29.000 | +--------+----------------------+------------+-------------------------+-------------------------+ | 95 | \"STATS\" | \"FINISHED\" | 2020-11-28T13:02:11.000 | 2020-11-28T13:02:11.000 | +--------+----------------------+------------+-------------------------+-------------------------+ | 86 | \"REBUILD_EDGE_INDEX\" | \"FINISHED\" | 2020-11-26T13:38:24.000 | 2020-11-26T13:38:24.000 | +--------+----------------------+------------+-------------------------+-------------------------+","title":"SHOW JOBS"},{"location":"3.ngql-guide/18.operation-and-maintenance-statements/4.job-statements/#stop_job","text":"The STOP JOB statement stops jobs that are not finished. nebula> STOP JOB 22; +---------------+ | Result | +---------------+ | \"Job stopped\" | +---------------+","title":"STOP JOB"},{"location":"3.ngql-guide/18.operation-and-maintenance-statements/4.job-statements/#recover_job","text":"The RECOVER JOB statement re-executes the failed jobs and returns the number of recovered jobs. nebula> RECOVER JOB; +-------------------+ | Recovered job num | +-------------------+ | 5 job recovered | +-------------------+","title":"RECOVER JOB"},{"location":"3.ngql-guide/18.operation-and-maintenance-statements/4.job-statements/#faq","text":"","title":"FAQ"},{"location":"3.ngql-guide/18.operation-and-maintenance-statements/4.job-statements/#how_to_troubleshoot_job_problems","text":"The SUBMIT JOB operations use the HTTP port. Please check if the HTTP ports on the machines where the Storage Service is running are working well. You can use the following command to debug. curl \"http://{storaged-ip}:19779/admin?space={space_name}&op=compact\"","title":"How to troubleshoot job problems"},{"location":"3.ngql-guide/20.appendix/comments/","text":"Comments \u00b6 Legacy version compatibility \u00b6 In Nebula Graph 1.0, four comment styles: # , -- , // , /* */ . In Nebula Graph 2.0, -- represents an edge, and can not be used as comments. Examples \u00b6 nebula> # Do nothing this line nebula> RETURN 1+1; # This comment continues to the end of line nebula> RETURN 1+1; // This comment continues to the end of line nebula> RETURN 1 /* This is an in-line comment */ + 1 == 2; nebula> RETURN 11 + \\ /* Multiple-line comment \\ Use backslash as line break. \\ */ 12; The backslash \\ in a line indicates a line break. OpenCypher Compatibility \u00b6 You must add a \\ at the end of every line, even in multi-line comments \\* *\\ . /* The openCypher style: The following comment spans more than one line */ MATCH (n:label) RETURN n /* The ngql style: \\ The following comment \\ spans more than \\ one line */ \\ MATCH (n:tag) \\ RETURN n","title":"Comments"},{"location":"3.ngql-guide/20.appendix/comments/#comments","text":"","title":"Comments"},{"location":"3.ngql-guide/20.appendix/comments/#legacy_version_compatibility","text":"In Nebula Graph 1.0, four comment styles: # , -- , // , /* */ . In Nebula Graph 2.0, -- represents an edge, and can not be used as comments.","title":"Legacy version compatibility"},{"location":"3.ngql-guide/20.appendix/comments/#examples","text":"nebula> # Do nothing this line nebula> RETURN 1+1; # This comment continues to the end of line nebula> RETURN 1+1; // This comment continues to the end of line nebula> RETURN 1 /* This is an in-line comment */ + 1 == 2; nebula> RETURN 11 + \\ /* Multiple-line comment \\ Use backslash as line break. \\ */ 12; The backslash \\ in a line indicates a line break.","title":"Examples"},{"location":"3.ngql-guide/20.appendix/comments/#opencypher_compatibility","text":"You must add a \\ at the end of every line, even in multi-line comments \\* *\\ . /* The openCypher style: The following comment spans more than one line */ MATCH (n:label) RETURN n /* The ngql style: \\ The following comment \\ spans more than \\ one line */ \\ MATCH (n:tag) \\ RETURN n","title":"OpenCypher Compatibility"},{"location":"3.ngql-guide/20.appendix/hash/","text":"Hash \u00b6 This document gives some explain about the hash() function used in nGQL. The source code of hashing function ( MurmurHash2 ), seed ( 0xc70f6907UL ) and other parameters can be found in this C++ MurmurHahs2.h file . For Java, call like follows. MurmurHash2 . hash64 ( \"to_be_hashed\" . getBytes (), \"to_be_hashed\" . getBytes (). length , 0xc70f6907 ) In nGQL text, call like follows. nebula> YIELD hash(\"to_be_hashed\") +----------------------+ | hash(to_be_hashed) | +----------------------+ | -1098333533029391540 | +----------------------+ nebula> YIELD hash(-1) +------------+ | hash(-(1)) | +------------+ | -1 | +------------+ NOTE : Roughly, The chance of collision is about 1/10 in the case of 1 billion vertices. (The number of edges are irrelevant to the collision possibility).","title":"Hash"},{"location":"3.ngql-guide/20.appendix/hash/#hash","text":"This document gives some explain about the hash() function used in nGQL. The source code of hashing function ( MurmurHash2 ), seed ( 0xc70f6907UL ) and other parameters can be found in this C++ MurmurHahs2.h file . For Java, call like follows. MurmurHash2 . hash64 ( \"to_be_hashed\" . getBytes (), \"to_be_hashed\" . getBytes (). length , 0xc70f6907 ) In nGQL text, call like follows. nebula> YIELD hash(\"to_be_hashed\") +----------------------+ | hash(to_be_hashed) | +----------------------+ | -1098333533029391540 | +----------------------+ nebula> YIELD hash(-1) +------------+ | hash(-(1)) | +------------+ | -1 | +------------+ NOTE : Roughly, The chance of collision is about 1/10 in the case of 1 billion vertices. (The number of edges are irrelevant to the collision possibility).","title":"Hash"},{"location":"3.ngql-guide/20.appendix/identifier-case-sensitivity/","text":"Identifer Case Sensitivity \u00b6 Identifiers are Case-Sensitive \u00b6 The following statements would not work because they refer to two different spaces, i.e. my_space and MY_SPACE : nebula> CREATE SPACE my_space; nebula> use MY_SPACE; [ERROR (-8)]: SpaceNotFound: # my_space and MY_SPACE are two different spaces Keywords and Reserved Words are Case-Insensitive \u00b6 The following statements are equivalent: nebula> show spaces; # show and spaces are keywords. nebula> SHOW SPACES; nebula> SHOW spaces; nebula> show SPACES;","title":"Identifer Case Sensitivity"},{"location":"3.ngql-guide/20.appendix/identifier-case-sensitivity/#identifer_case_sensitivity","text":"","title":"Identifer Case Sensitivity"},{"location":"3.ngql-guide/20.appendix/identifier-case-sensitivity/#identifiers_are_case-sensitive","text":"The following statements would not work because they refer to two different spaces, i.e. my_space and MY_SPACE : nebula> CREATE SPACE my_space; nebula> use MY_SPACE; [ERROR (-8)]: SpaceNotFound: # my_space and MY_SPACE are two different spaces","title":"Identifiers are Case-Sensitive"},{"location":"3.ngql-guide/20.appendix/identifier-case-sensitivity/#keywords_and_reserved_words_are_case-insensitive","text":"The following statements are equivalent: nebula> show spaces; # show and spaces are keywords. nebula> SHOW SPACES; nebula> SHOW spaces; nebula> show SPACES;","title":"Keywords and Reserved Words are Case-Insensitive"},{"location":"3.ngql-guide/20.appendix/keywords-and-reserved-words/","text":"Keywords and Reserved Words \u00b6 Keywords have significance in nGQL. Certain keywords are reserved and require special treatment for use as identifiers. Non-reserved keywords are permitted as identifiers without quoting. Non-reserved keywords are case-insensitive. To use reserved keywords as identifiers, quote them with back quotes such as `AND`. nebula> CREATE TAG TAG(name string); [ERROR (-7)]: SyntaxError: syntax error near `TAG' // SPACE is an unreserved keyword. nebula> CREATE TAG SPACE(name string); Execution succeeded TAG is a reserved keyword. To use TAG as an identifier, you must quote it with a backtick. SPACE is a non-reserved keyword. You can use SPACE as an identifier without quoting it. // TAG is a reserved keyword here. nebula> CREATE TAG `TAG` (name string); Execution succeeded Reserved Words \u00b6 The following list shows reserved words in nGQL. ADD ALTER AND AS ASC BALANCE BOOL BY CASE CHANGE COMPACT CREATE DATE DATETIME DELETE DESC DESCRIBE DISTINCT DOUBLE DOWNLOAD DROP EDGE EDGES EXISTS EXPLAIN FETCH FIND FIXED_STRING FLOAT FLUSH FORMAT FROM GET GO GRANT IF IN INDEX INDEXES INGEST INSERT INT INT16 INT32 INT64 INT8 INTERSECT IS LIMIT LOOKUP MATCH MINUS NO NOT NULL OF OFFSET ON OR ORDER OVER OVERWRITE PROFILE PROP REBUILD RECOVER REMOVE RETURN REVERSELY REVOKE SET SHOW STEP STEPS STOP STRING SUBMIT TAG TAGS TIME TIMESTAMP TO UNION UPDATE UPSERT UPTO USE VERTEX WHEN WHERE WITH XOR YIELD Non-Reserved Keywords \u00b6 ACCOUNT ADMIN ALL ANY ATOMIC_EDGE AUTO AVG BIDIRECT BIT_AND BIT_OR BIT_XOR BOTH CHARSET CLIENTS COLLATE COLLATION COLLECT COLLECT_SET CONFIGS CONTAINS COUNT COUNT_DISTINCT DATA DBA DEFAULT ELASTICSEARCH ELSE END ENDS FALSE FORCE FUZZY GOD GRAPH GROUP GROUPS GUEST HDFS HOST HOSTS INTO JOB JOBS LEADER LISTENER MAX META MIN NOLOOP NONE OPTIONAL OUT PART PARTITION_NUM PARTS PASSWORD PATH PLAN PREFIX REGEXP REPLICA_FACTOR RESET ROLE ROLES SEARCH SERVICE SHORTEST SIGN SINGLE SKIP SNAPSHOT SNAPSHOTS SPACE SPACES STARTS STATS STATUS STD STORAGE SUBGRAPH SUM TEXT TEXT_SEARCH THEN TRUE TTL_COL TTL_DURATION UNWIND USER USERS UUID VALUE VALUES VID_TYPE WILDCARD ZONE ZONES","title":"Keywords and Reserved Words"},{"location":"3.ngql-guide/20.appendix/keywords-and-reserved-words/#keywords_and_reserved_words","text":"Keywords have significance in nGQL. Certain keywords are reserved and require special treatment for use as identifiers. Non-reserved keywords are permitted as identifiers without quoting. Non-reserved keywords are case-insensitive. To use reserved keywords as identifiers, quote them with back quotes such as `AND`. nebula> CREATE TAG TAG(name string); [ERROR (-7)]: SyntaxError: syntax error near `TAG' // SPACE is an unreserved keyword. nebula> CREATE TAG SPACE(name string); Execution succeeded TAG is a reserved keyword. To use TAG as an identifier, you must quote it with a backtick. SPACE is a non-reserved keyword. You can use SPACE as an identifier without quoting it. // TAG is a reserved keyword here. nebula> CREATE TAG `TAG` (name string); Execution succeeded","title":"Keywords and Reserved Words"},{"location":"3.ngql-guide/20.appendix/keywords-and-reserved-words/#reserved_words","text":"The following list shows reserved words in nGQL. ADD ALTER AND AS ASC BALANCE BOOL BY CASE CHANGE COMPACT CREATE DATE DATETIME DELETE DESC DESCRIBE DISTINCT DOUBLE DOWNLOAD DROP EDGE EDGES EXISTS EXPLAIN FETCH FIND FIXED_STRING FLOAT FLUSH FORMAT FROM GET GO GRANT IF IN INDEX INDEXES INGEST INSERT INT INT16 INT32 INT64 INT8 INTERSECT IS LIMIT LOOKUP MATCH MINUS NO NOT NULL OF OFFSET ON OR ORDER OVER OVERWRITE PROFILE PROP REBUILD RECOVER REMOVE RETURN REVERSELY REVOKE SET SHOW STEP STEPS STOP STRING SUBMIT TAG TAGS TIME TIMESTAMP TO UNION UPDATE UPSERT UPTO USE VERTEX WHEN WHERE WITH XOR YIELD","title":"Reserved Words"},{"location":"3.ngql-guide/20.appendix/keywords-and-reserved-words/#non-reserved_keywords","text":"ACCOUNT ADMIN ALL ANY ATOMIC_EDGE AUTO AVG BIDIRECT BIT_AND BIT_OR BIT_XOR BOTH CHARSET CLIENTS COLLATE COLLATION COLLECT COLLECT_SET CONFIGS CONTAINS COUNT COUNT_DISTINCT DATA DBA DEFAULT ELASTICSEARCH ELSE END ENDS FALSE FORCE FUZZY GOD GRAPH GROUP GROUPS GUEST HDFS HOST HOSTS INTO JOB JOBS LEADER LISTENER MAX META MIN NOLOOP NONE OPTIONAL OUT PART PARTITION_NUM PARTS PASSWORD PATH PLAN PREFIX REGEXP REPLICA_FACTOR RESET ROLE ROLES SEARCH SERVICE SHORTEST SIGN SINGLE SKIP SNAPSHOT SNAPSHOTS SPACE SPACES STARTS STATS STATUS STD STORAGE SUBGRAPH SUM TEXT TEXT_SEARCH THEN TRUE TTL_COL TTL_DURATION UNWIND USER USERS UUID VALUE VALUES VID_TYPE WILDCARD ZONE ZONES","title":"Non-Reserved Keywords"},{"location":"3.ngql-guide/20.appendix/vid-partition/","text":"Vertex identifier and partition ID \u00b6 VID \u00b6 VID is short for vertex identifier. In Nebula Graph, vertices are identified with vertex identifiers (i.e. VID s). The VID can be an int64 or a fixed length string. When inserting a vertex, you must specify a VID for it. You can also call hash() to generate an int64 VID if the graph has less than one billion vertices. VID must be unique in a graph space. That is, in the same graph space, two vertices that have the same VID are considered as the same vertex. In addition, one VID can have multiple TAG s. E.g., One person ( VID ) can have two roles ( tags ). Two VID s in two different graph spaces are totally independent of each other. Partition ID \u00b6 When inserting into Nebula Graph, vertices and edges are distributed across different partitions. And the partitions are located on different machines. If you want certain vertices to locate on the same partition (i.e., on the same machine), you can control the generation of the VID s by using the following formula / code . // If the length of the id is 8, we will treat it as int64_t to be compatible // with the version 1.0 uint64_t vid = 0 ; if ( id . size () == 8 ) { memcpy ( static_cast < void *> ( & vid ), id . data (), 8 ); } else { MurmurHash2 hash ; vid = hash ( id . data ()); } PartitionID pId = vid % numParts + 1 ; Roughly say, after hashing a fixed string to int64, (the hashing of int64 is the number itself), do modulo and then plus one. pId = vid % numParts + 1 ; In the preceding formula, % is the modulo operation. numParts is the number of partition for the graph space where the VID is located, namely the value of partition_num in the CREATE SPACE statement. pId is the ID for the partition where the VID is located. For example, if there are 100 partitions, the vertices with VID 1, 101, 1001 will be stored on the same partition. But, the mapping between the partition ID and the machine address is random. Therefore, you can't assume that any two partitions are located on the same machine.","title":"Vertex identifier and partition ID"},{"location":"3.ngql-guide/20.appendix/vid-partition/#vertex_identifier_and_partition_id","text":"","title":"Vertex identifier and partition ID"},{"location":"3.ngql-guide/20.appendix/vid-partition/#vid","text":"VID is short for vertex identifier. In Nebula Graph, vertices are identified with vertex identifiers (i.e. VID s). The VID can be an int64 or a fixed length string. When inserting a vertex, you must specify a VID for it. You can also call hash() to generate an int64 VID if the graph has less than one billion vertices. VID must be unique in a graph space. That is, in the same graph space, two vertices that have the same VID are considered as the same vertex. In addition, one VID can have multiple TAG s. E.g., One person ( VID ) can have two roles ( tags ). Two VID s in two different graph spaces are totally independent of each other.","title":"VID"},{"location":"3.ngql-guide/20.appendix/vid-partition/#partition_id","text":"When inserting into Nebula Graph, vertices and edges are distributed across different partitions. And the partitions are located on different machines. If you want certain vertices to locate on the same partition (i.e., on the same machine), you can control the generation of the VID s by using the following formula / code . // If the length of the id is 8, we will treat it as int64_t to be compatible // with the version 1.0 uint64_t vid = 0 ; if ( id . size () == 8 ) { memcpy ( static_cast < void *> ( & vid ), id . data (), 8 ); } else { MurmurHash2 hash ; vid = hash ( id . data ()); } PartitionID pId = vid % numParts + 1 ; Roughly say, after hashing a fixed string to int64, (the hashing of int64 is the number itself), do modulo and then plus one. pId = vid % numParts + 1 ; In the preceding formula, % is the modulo operation. numParts is the number of partition for the graph space where the VID is located, namely the value of partition_num in the CREATE SPACE statement. pId is the ID for the partition where the VID is located. For example, if there are 100 partitions, the vertices with VID 1, 101, 1001 will be stored on the same partition. But, the mapping between the partition ID and the machine address is random. Therefore, you can't assume that any two partitions are located on the same machine.","title":"Partition ID"},{"location":"3.ngql-guide/3.data-types/1.numeric/","text":"Numeric types \u00b6 Integer \u00b6 An integer is declared with keyword int , which is 64-bit signed . The supported range is [-9223372036854775808, 9223372036854775807]. Integer constants support multiple formats: Decimal, for example 123456 . Hexadecimal, for example 0xdeadbeaf . Octal, for example 01234567 . Double-precision floating-point \u00b6 double-precision floating-point values is used for storing double precision floating point values. E.g., 1.2, -3.0000001. The keyword used for double floating point data type is double . Scientific notation is also supported. For example, 1e2, 1.1e2, .3e4, 1.e4, -1234E-10 .","title":"Numeric"},{"location":"3.ngql-guide/3.data-types/1.numeric/#numeric_types","text":"","title":"Numeric types"},{"location":"3.ngql-guide/3.data-types/1.numeric/#integer","text":"An integer is declared with keyword int , which is 64-bit signed . The supported range is [-9223372036854775808, 9223372036854775807]. Integer constants support multiple formats: Decimal, for example 123456 . Hexadecimal, for example 0xdeadbeaf . Octal, for example 01234567 .","title":"Integer"},{"location":"3.ngql-guide/3.data-types/1.numeric/#double-precision_floating-point","text":"double-precision floating-point values is used for storing double precision floating point values. E.g., 1.2, -3.0000001. The keyword used for double floating point data type is double . Scientific notation is also supported. For example, 1e2, 1.1e2, .3e4, 1.e4, -1234E-10 .","title":"Double-precision floating-point"},{"location":"3.ngql-guide/3.data-types/2.boolean/","text":"Boolean \u00b6 A boolean data type is declared with the bool keyword and can only take the values true or false .","title":"Boolean"},{"location":"3.ngql-guide/3.data-types/2.boolean/#boolean","text":"A boolean data type is declared with the bool keyword and can only take the values true or false .","title":"Boolean"},{"location":"3.ngql-guide/3.data-types/3.string/","text":"String \u00b6 The string type is used to store a sequence of characters (text). The literal constant is a sequence of characters of any length surrounded by double or single quotes. For example \"Shaquille O'Neal\" or '\"This is a double-quoted literal string\"' . Line breaks are not allowed in a string. Embedded escape sequences are supported within strings, for example: \"\\n\\t\\r\\b\\f\" \"\\110ello world\" Nebula Graph supports two kind of strings: fixed length string and variable length string. For example: nebula> CREATE TAG t1 (p1 FIXED_STRING(10)); -- Fixed length string type nebula> CREATE TAG t2 (p2 string); -- Variable length string type OpenCypher Compatibility \u00b6 Here is a tiny difference between openCypher and Cypher, as well as nGQL. The following is what openCypher requires. Single-quotes can't be converted to double-quotes. #File: Literals.feature Feature: Literals Background: Given any graph Scenario: Return a single-quoted string When executing query: \"\"\" RETURN '' AS literal \"\"\" Then the result should be, in any order: | literal | | '' | # Note: it should return single-quotes as openCypher required. And no side effects While Cypher accepts both single-quotes and double quotes as the return results. nGQL follows the Cypher way. nebula > YIELD '' AS quote1, \"\" AS quote2, \"'\" AS quote3, '\"' AS quote4 +--------+--------+--------+--------+ | quote1 | quote2 | quote3 | quote4 | +--------+--------+--------+--------+ | \"\" | \"\" | \"'\" | \"\"\" | +--------+--------+--------+--------+","title":"String"},{"location":"3.ngql-guide/3.data-types/3.string/#string","text":"The string type is used to store a sequence of characters (text). The literal constant is a sequence of characters of any length surrounded by double or single quotes. For example \"Shaquille O'Neal\" or '\"This is a double-quoted literal string\"' . Line breaks are not allowed in a string. Embedded escape sequences are supported within strings, for example: \"\\n\\t\\r\\b\\f\" \"\\110ello world\" Nebula Graph supports two kind of strings: fixed length string and variable length string. For example: nebula> CREATE TAG t1 (p1 FIXED_STRING(10)); -- Fixed length string type nebula> CREATE TAG t2 (p2 string); -- Variable length string type","title":"String"},{"location":"3.ngql-guide/3.data-types/3.string/#opencypher_compatibility","text":"Here is a tiny difference between openCypher and Cypher, as well as nGQL. The following is what openCypher requires. Single-quotes can't be converted to double-quotes. #File: Literals.feature Feature: Literals Background: Given any graph Scenario: Return a single-quoted string When executing query: \"\"\" RETURN '' AS literal \"\"\" Then the result should be, in any order: | literal | | '' | # Note: it should return single-quotes as openCypher required. And no side effects While Cypher accepts both single-quotes and double quotes as the return results. nGQL follows the Cypher way. nebula > YIELD '' AS quote1, \"\" AS quote2, \"'\" AS quote3, '\"' AS quote4 +--------+--------+--------+--------+ | quote1 | quote2 | quote3 | quote4 | +--------+--------+--------+--------+ | \"\" | \"\" | \"'\" | \"\"\" | +--------+--------+--------+--------+","title":"OpenCypher Compatibility"},{"location":"3.ngql-guide/3.data-types/4.date-and-time/","text":"Date and time types \u00b6 This document describes the DATE , TIME , DATETIME , and TIMESTAMP types. Nebula Graph converts the DATE , TIME , DATETIME , and TIMESTAMP values from the current time zone to UTC for storage. Nebula Graph converts back from UTC to the current time zone for retrieval. Combined with RETURN , functions date() , time() , datetime() all accept empty parameters to return the current date, time and datetime. OpenCypher Compatibility \u00b6 In nGQL: Year, month, day, hour, minute, and second are supported. The millisecond field is displayed in 000 . localdatetime() , duration() are not supported. Most string time formats are not supported. The only exception is 2017-03-04T22:30:40 . DATE \u00b6 The DATE type is used for values with a date part but no time part. Nebula Graph retrieves and displays DATE values in the YYYY-MM-DD format. The supported range is -32768-01-01 to 32767-12-31 . TIME \u00b6 The TIME type is used for values with a time part but no date part. Nebula Graph retrieves and displays TIME values in hh:mm:ss:usus format. The supported range is 0:0:0:0 to 23:59:59:999999 . DATETIME \u00b6 The DATETIME type is used for values that contain both date and time parts. Nebula Graph retrieves and displays DATETIME values in YYYY-MM-DD hh:mm:ss:ususus format. The supported range is -32768-01-01 00:00:00:00 to 32767-12-31 23:59:59:999999 . TIMESTAMP \u00b6 The TIMESTAMP data type is used for values that contain both date and time parts. TIMESTAMP has a range of 1970-01-01 00:00:01 UTC to 2262-04-11 23:47:16 UTC. Timestamp is measured in units of seconds. Supported TIMESTAMP inserting methods: Call the now() function. Input TIMESTAMP by using a string. For example: 2019-10-01 10:00:00 . Input TIMESTAMP directly, namely the number of seconds from 1970-01-01 00:00:00 . The underlying storage data type is: int64 . Examples \u00b6 Create a tag named date. nebula> CREATE TAG date(p1 date, p2 time, p3 datetime); Insert a vertex named Date1. nebula> INSERT VERTEX date(p1, p2, p3) VALUES \"Date1\":(date(\"2017-03-04\"), time(\"23:01:00\"), datetime(\"2017-03-04T22:30:40\")); Create a tag named school. nebula> CREATE TAG school(name string , found_time timestamp); Insert a vertex named \"stanford\" with the foundation date \"1885-10-01T08:00:00\" . nebula> INSERT VERTEX school(name, found_time) VALUES \"Stanford\":(\"Stanford\", timestamp(\"1885-10-01T08:00:00\")); Insert a vertex named \"dut\" with the foundation date now. nebula> INSERT VERTEX school(name, found_time) VALUES \"dut\":(\"dut\", now()); nebula> WITH time({hour: 12, minute: 31, second: 14}) AS d RETURN d; +--------------+ | d | +--------------+ | 12:31:14.000 | +--------------+ nebula> WITH date({year: 1984, month: 10, day: 11}) AS x RETURN x + 1; +------------+ | x | +------------+ | 1984-10-12 | +------------+ nebula> WITH datetime({year: 1984, month: 10, day: 11, hour: 12, minute: 31, second: 14}) AS d \\ RETURN toString(d) AS ts, datetime(toString(d)) == d AS b +-------------------------+------+ | ts | b | +-------------------------+------+ | \"1984-10-11T12:31:14.0\" | true | +-------------------------+------+","title":"Date and time"},{"location":"3.ngql-guide/3.data-types/4.date-and-time/#date_and_time_types","text":"This document describes the DATE , TIME , DATETIME , and TIMESTAMP types. Nebula Graph converts the DATE , TIME , DATETIME , and TIMESTAMP values from the current time zone to UTC for storage. Nebula Graph converts back from UTC to the current time zone for retrieval. Combined with RETURN , functions date() , time() , datetime() all accept empty parameters to return the current date, time and datetime.","title":"Date and time types"},{"location":"3.ngql-guide/3.data-types/4.date-and-time/#opencypher_compatibility","text":"In nGQL: Year, month, day, hour, minute, and second are supported. The millisecond field is displayed in 000 . localdatetime() , duration() are not supported. Most string time formats are not supported. The only exception is 2017-03-04T22:30:40 .","title":"OpenCypher Compatibility"},{"location":"3.ngql-guide/3.data-types/4.date-and-time/#date","text":"The DATE type is used for values with a date part but no time part. Nebula Graph retrieves and displays DATE values in the YYYY-MM-DD format. The supported range is -32768-01-01 to 32767-12-31 .","title":"DATE"},{"location":"3.ngql-guide/3.data-types/4.date-and-time/#time","text":"The TIME type is used for values with a time part but no date part. Nebula Graph retrieves and displays TIME values in hh:mm:ss:usus format. The supported range is 0:0:0:0 to 23:59:59:999999 .","title":"TIME"},{"location":"3.ngql-guide/3.data-types/4.date-and-time/#datetime","text":"The DATETIME type is used for values that contain both date and time parts. Nebula Graph retrieves and displays DATETIME values in YYYY-MM-DD hh:mm:ss:ususus format. The supported range is -32768-01-01 00:00:00:00 to 32767-12-31 23:59:59:999999 .","title":"DATETIME"},{"location":"3.ngql-guide/3.data-types/4.date-and-time/#timestamp","text":"The TIMESTAMP data type is used for values that contain both date and time parts. TIMESTAMP has a range of 1970-01-01 00:00:01 UTC to 2262-04-11 23:47:16 UTC. Timestamp is measured in units of seconds. Supported TIMESTAMP inserting methods: Call the now() function. Input TIMESTAMP by using a string. For example: 2019-10-01 10:00:00 . Input TIMESTAMP directly, namely the number of seconds from 1970-01-01 00:00:00 . The underlying storage data type is: int64 .","title":"TIMESTAMP"},{"location":"3.ngql-guide/3.data-types/4.date-and-time/#examples","text":"Create a tag named date. nebula> CREATE TAG date(p1 date, p2 time, p3 datetime); Insert a vertex named Date1. nebula> INSERT VERTEX date(p1, p2, p3) VALUES \"Date1\":(date(\"2017-03-04\"), time(\"23:01:00\"), datetime(\"2017-03-04T22:30:40\")); Create a tag named school. nebula> CREATE TAG school(name string , found_time timestamp); Insert a vertex named \"stanford\" with the foundation date \"1885-10-01T08:00:00\" . nebula> INSERT VERTEX school(name, found_time) VALUES \"Stanford\":(\"Stanford\", timestamp(\"1885-10-01T08:00:00\")); Insert a vertex named \"dut\" with the foundation date now. nebula> INSERT VERTEX school(name, found_time) VALUES \"dut\":(\"dut\", now()); nebula> WITH time({hour: 12, minute: 31, second: 14}) AS d RETURN d; +--------------+ | d | +--------------+ | 12:31:14.000 | +--------------+ nebula> WITH date({year: 1984, month: 10, day: 11}) AS x RETURN x + 1; +------------+ | x | +------------+ | 1984-10-12 | +------------+ nebula> WITH datetime({year: 1984, month: 10, day: 11, hour: 12, minute: 31, second: 14}) AS d \\ RETURN toString(d) AS ts, datetime(toString(d)) == d AS b +-------------------------+------+ | ts | b | +-------------------------+------+ | \"1984-10-11T12:31:14.0\" | true | +-------------------------+------+","title":"Examples"},{"location":"3.ngql-guide/3.data-types/5.null/","text":"NULL \u00b6 You can set the properties for vertices or edges to NULL . Also, you can set NOT NULL constraint to make sure that the property values are NOT NULL . If not specified, the property is set to NULL by default. Logical operations with NULL \u00b6 The logical operations with NULL is the same as openCypher. Here is the truth table for AND, OR, XOR, and NOT. a b a AND b a OR b a XOR b NOT a false false false false false true false null false null null true false true false true true true true false false true true false true null null true null false true true true true false false null false false null null null null null null null null null null true null true null null OpenCypher compatibility \u00b6 The comparisons and operations about NULL are different from openCypher. The behavior may change later. Comparisons with NULL \u00b6 The comparison operations with NULL is incompatible with openCypher. Operations and expression with NULL \u00b6 The NULL operations and RETURN with NULL is incompatible with openCypher. Examples \u00b6 Create a tag named player. Specify the property name with NOT NULL . Ignore the property age constraint. nebula> CREATE TAG player(name string NOT NULL, age int); Execution succeeded (time spent 5001/5980 us) The property name is NOT NULL . The property age is NULL by default. nebula> SHOW CREATE TAG player; +-----------+-----------------------------------+ | Tag | Create Tag | +-----------+-----------------------------------+ | \"student\" | \"CREATE TAG `player` ( | | | `name` string NOT NULL, | | | `age` int64 NULL | | | ) ttl_duration = 0, ttl_col = \"\"\" | +-----------+-----------------------------------+ nebula> INSERT VERTEX player(name, age) VALUES \"Kobe\":(\"Kobe\",null); Execution succeeded (time spent 6367/7357 us)","title":"NULL"},{"location":"3.ngql-guide/3.data-types/5.null/#null","text":"You can set the properties for vertices or edges to NULL . Also, you can set NOT NULL constraint to make sure that the property values are NOT NULL . If not specified, the property is set to NULL by default.","title":"NULL"},{"location":"3.ngql-guide/3.data-types/5.null/#logical_operations_with_null","text":"The logical operations with NULL is the same as openCypher. Here is the truth table for AND, OR, XOR, and NOT. a b a AND b a OR b a XOR b NOT a false false false false false true false null false null null true false true false true true true true false false true true false true null null true null false true true true true false false null false false null null null null null null null null null null true null true null null","title":"Logical operations with NULL"},{"location":"3.ngql-guide/3.data-types/5.null/#opencypher_compatibility","text":"The comparisons and operations about NULL are different from openCypher. The behavior may change later.","title":"OpenCypher compatibility"},{"location":"3.ngql-guide/3.data-types/5.null/#comparisons_with_null","text":"The comparison operations with NULL is incompatible with openCypher.","title":"Comparisons with NULL"},{"location":"3.ngql-guide/3.data-types/5.null/#operations_and_expression_with_null","text":"The NULL operations and RETURN with NULL is incompatible with openCypher.","title":"Operations and expression with NULL"},{"location":"3.ngql-guide/3.data-types/5.null/#examples","text":"Create a tag named player. Specify the property name with NOT NULL . Ignore the property age constraint. nebula> CREATE TAG player(name string NOT NULL, age int); Execution succeeded (time spent 5001/5980 us) The property name is NOT NULL . The property age is NULL by default. nebula> SHOW CREATE TAG player; +-----------+-----------------------------------+ | Tag | Create Tag | +-----------+-----------------------------------+ | \"student\" | \"CREATE TAG `player` ( | | | `name` string NOT NULL, | | | `age` int64 NULL | | | ) ttl_duration = 0, ttl_col = \"\"\" | +-----------+-----------------------------------+ nebula> INSERT VERTEX player(name, age) VALUES \"Kobe\":(\"Kobe\",null); Execution succeeded (time spent 6367/7357 us)","title":"Examples"},{"location":"3.ngql-guide/3.data-types/6.list/","text":"Lists \u00b6 The list is a composite data type. A list is a sequence of values. Individual list elements can be accessed by their positions. A list starts with a left square bracket [ and ends with a right square bracket ] . A list contains zero, one, or more expressions. List elements are separated from each other with commas ( , ). Whitespace around elements is ignored in list, thus line breaks, tab stops, and blanks can be used for formatting. Examples \u00b6 nebula> RETURN [1, 2, 3] AS List; +-----------+ | List | +-----------+ | [1, 2, 3] | +-----------+ nebula> RETURN range(1,5)[3]; +---------------+ | range(1,5)[3] | +---------------+ | 4 | +---------------+ nebula> RETURN range(1,5)[-2]; +------------------+ | range(1,5)[-(2)] | +------------------+ | 4 | +------------------+ nebula> RETURN [n IN range(1,5) WHERE n > 2] AS a; +-----------+ | a | +-----------+ | [3, 4, 5] | +-----------+ nebula> RETURN [n IN range(1,5) WHERE n > 2 | n + 10] AS a; +--------------+ | a | +--------------+ | [13, 14, 15] | +--------------+ nebula> RETURN [n IN range(1,5) | n + 10] AS a; +----------------------+ | a | +----------------------+ | [11, 12, 13, 14, 15] | +----------------------+ nebula> RETURN tail([n IN range(1, 5) | 2 * n - 10]) AS a; +-----------------+ | a | +-----------------+ | [-6, -4, -2, 0] | +-----------------+ nebula> RETURN [n IN range(1, 3) WHERE true | n] AS r; +-----------+ | r | +-----------+ | [1, 2, 3] | +-----------+ nebula> GO FROM \"player100\" OVER follow WHERE follow.degree NOT IN [x IN [92, 90] | x + $$.player.age] \\ YIELD follow._dst AS id, follow.degree AS degree; +-------------+--------+ | id | degree | +-------------+--------+ | \"player101\" | 95 | +-------------+--------+ | \"player102\" | 90 | +-------------+--------+ nebula> MATCH p = (n:player{name:\"Tim Duncan\"})-[:follow]->(m) \\ RETURN [n IN nodes(p) | n.age + 100] AS r; +------------+ | r | +------------+ | [142, 136] | +------------+ | [142, 133] | +------------+ nebula> RETURN size([1,2,3]); +---------------+ | size([1,2,3]) | +---------------+ | 3 | +---------------+ OpenCypher compatibility \u00b6 A composite data type (i.e., set, map, and list) CAN NOT be stored as properties for vertices or edges. Use the range() function to return the range of a list. nebula> RETURN range(0,5)[0..3]; [ERROR (-7)]: SyntaxError: syntax error near `3]' In openCypher, out-of-bound single elements returns null . However, in nGQL, out-of-bound single elements returns OUT_OF_RANGE . nebula> RETURN range(0,5)[-12]; +-------------------+ | range(0,5)[-(12)] | +-------------------+ | OUT_OF_RANGE | +-------------------+","title":"List"},{"location":"3.ngql-guide/3.data-types/6.list/#lists","text":"The list is a composite data type. A list is a sequence of values. Individual list elements can be accessed by their positions. A list starts with a left square bracket [ and ends with a right square bracket ] . A list contains zero, one, or more expressions. List elements are separated from each other with commas ( , ). Whitespace around elements is ignored in list, thus line breaks, tab stops, and blanks can be used for formatting.","title":"Lists"},{"location":"3.ngql-guide/3.data-types/6.list/#examples","text":"nebula> RETURN [1, 2, 3] AS List; +-----------+ | List | +-----------+ | [1, 2, 3] | +-----------+ nebula> RETURN range(1,5)[3]; +---------------+ | range(1,5)[3] | +---------------+ | 4 | +---------------+ nebula> RETURN range(1,5)[-2]; +------------------+ | range(1,5)[-(2)] | +------------------+ | 4 | +------------------+ nebula> RETURN [n IN range(1,5) WHERE n > 2] AS a; +-----------+ | a | +-----------+ | [3, 4, 5] | +-----------+ nebula> RETURN [n IN range(1,5) WHERE n > 2 | n + 10] AS a; +--------------+ | a | +--------------+ | [13, 14, 15] | +--------------+ nebula> RETURN [n IN range(1,5) | n + 10] AS a; +----------------------+ | a | +----------------------+ | [11, 12, 13, 14, 15] | +----------------------+ nebula> RETURN tail([n IN range(1, 5) | 2 * n - 10]) AS a; +-----------------+ | a | +-----------------+ | [-6, -4, -2, 0] | +-----------------+ nebula> RETURN [n IN range(1, 3) WHERE true | n] AS r; +-----------+ | r | +-----------+ | [1, 2, 3] | +-----------+ nebula> GO FROM \"player100\" OVER follow WHERE follow.degree NOT IN [x IN [92, 90] | x + $$.player.age] \\ YIELD follow._dst AS id, follow.degree AS degree; +-------------+--------+ | id | degree | +-------------+--------+ | \"player101\" | 95 | +-------------+--------+ | \"player102\" | 90 | +-------------+--------+ nebula> MATCH p = (n:player{name:\"Tim Duncan\"})-[:follow]->(m) \\ RETURN [n IN nodes(p) | n.age + 100] AS r; +------------+ | r | +------------+ | [142, 136] | +------------+ | [142, 133] | +------------+ nebula> RETURN size([1,2,3]); +---------------+ | size([1,2,3]) | +---------------+ | 3 | +---------------+","title":"Examples"},{"location":"3.ngql-guide/3.data-types/6.list/#opencypher_compatibility","text":"A composite data type (i.e., set, map, and list) CAN NOT be stored as properties for vertices or edges. Use the range() function to return the range of a list. nebula> RETURN range(0,5)[0..3]; [ERROR (-7)]: SyntaxError: syntax error near `3]' In openCypher, out-of-bound single elements returns null . However, in nGQL, out-of-bound single elements returns OUT_OF_RANGE . nebula> RETURN range(0,5)[-12]; +-------------------+ | range(0,5)[-(12)] | +-------------------+ | OUT_OF_RANGE | +-------------------+","title":"OpenCypher compatibility"},{"location":"3.ngql-guide/3.data-types/7.set/","text":"Sets \u00b6 Set is a composite data type. OpenCypher compatibility \u00b6 Set is not a data type in openCypher. The behavior of set in nGQL is not determined yet.","title":"Set"},{"location":"3.ngql-guide/3.data-types/7.set/#sets","text":"Set is a composite data type.","title":"Sets"},{"location":"3.ngql-guide/3.data-types/7.set/#opencypher_compatibility","text":"Set is not a data type in openCypher. The behavior of set in nGQL is not determined yet.","title":"OpenCypher compatibility"},{"location":"3.ngql-guide/3.data-types/8.map/","text":"Maps \u00b6 Map is a composite data type. A composite data type cannot be stored as properties. Maps are unordered collections of key-value pairs. In maps, the key is a string. The value can have any data type. You can get the map element by using map['key'] . Literal maps \u00b6 nebula> YIELD {key: 'Value', listKey: [{inner: 'Map1'}, {inner: 'Map2'}]} +-------------------------------------------------------------+ | {key:Value,listKey:[{inner:Map1},{inner:Map2}]} | +-------------------------------------------------------------+ | {key: \"Value\", listKey: [{inner: \"Map1\"}, {inner: \"Map2\"}]} | +-------------------------------------------------------------+ OpenCypher compatibility \u00b6 A composite data type (i.e. set, map, and list) CANNOT be stored as properties of vertices or edges. Map projection is not supported.","title":"Map"},{"location":"3.ngql-guide/3.data-types/8.map/#maps","text":"Map is a composite data type. A composite data type cannot be stored as properties. Maps are unordered collections of key-value pairs. In maps, the key is a string. The value can have any data type. You can get the map element by using map['key'] .","title":"Maps"},{"location":"3.ngql-guide/3.data-types/8.map/#literal_maps","text":"nebula> YIELD {key: 'Value', listKey: [{inner: 'Map1'}, {inner: 'Map2'}]} +-------------------------------------------------------------+ | {key:Value,listKey:[{inner:Map1},{inner:Map2}]} | +-------------------------------------------------------------+ | {key: \"Value\", listKey: [{inner: \"Map1\"}, {inner: \"Map2\"}]} | +-------------------------------------------------------------+","title":"Literal maps"},{"location":"3.ngql-guide/3.data-types/8.map/#opencypher_compatibility","text":"A composite data type (i.e. set, map, and list) CANNOT be stored as properties of vertices or edges. Map projection is not supported.","title":"OpenCypher compatibility"},{"location":"3.ngql-guide/3.data-types/9.type-conversion/","text":"Type Conversion/Type coercions \u00b6 Converting an expression of a given type to another type is known as type conversion. Legacy version compatibility \u00b6 NGQL 1.0 adopted the C -style of type conversion (implicitly or explicitly). (type_name)expression . For example, The results of YIELD (int)(TRUE) is 1 . But it is error-prone to users who are not familiar with C language. NGQL 2.0 chooses the openCypher way of type coercions. Type coercions functions \u00b6 Function Description toBoolean() Converts a string value to a boolean value. toFloat() Converts an integer or string value to a floating point number. toInteger() Converts a floating point or string value to an integer value. type() Returns the string representation of the relationship type. Examples \u00b6 nebula> UNWIND [true, false, 'true', 'false', NULL] AS b RETURN toBoolean(b) AS b +----------+ | b | +----------+ | true | +----------+ | false | +----------+ | true | +----------+ | false | +----------+ | __NULL__ | +----------+ nebula> RETURN toFloat(1), toFloat('1.3'), toFloat('1e3'), toFloat('not a number') +------------+----------------+----------------+-------------------------+ | toFloat(1) | toFloat(\"1.3\") | toFloat(\"1e3\") | toFloat(\"not a number\") | +------------+----------------+----------------+-------------------------+ | 1.0 | 1.3 | 1000.0 | __NULL__ | +------------+----------------+----------------+-------------------------+ nebula> RETURN toInteger(1), toInteger('1'), toInteger('1e3'), toInteger('not a number') +--------------+----------------+------------------+---------------------------+ | toInteger(1) | toInteger(\"1\") | toInteger(\"1e3\") | toInteger(\"not a number\") | +--------------+----------------+------------------+---------------------------+ | 1 | 1 | 1000 | __NULL__ | +--------------+----------------+------------------+---------------------------+ nebula> MATCH (a:player)-[e]-() RETURN type(e) +----------+ | type(e) | +----------+ | \"follow\" | +----------+ | \"follow\" | nebula> MATCH (a:player {name: \"Tim Duncan\"}) WHERE toInteger(id(a)) == 100 RETURN a +----------------------------------------------+ | a | +----------------------------------------------+ | (\"100\" :player{age: 42, name: \"Tim Duncan\"}) | +----------------------------------------------+ nebula> MATCH (n:player) WITH n LIMIT toInteger(ceil(1.8)) RETURN count(*) AS count +-------+ | count | +-------+ | 2 | +-------+","title":"Type conversion"},{"location":"3.ngql-guide/3.data-types/9.type-conversion/#type_conversiontype_coercions","text":"Converting an expression of a given type to another type is known as type conversion.","title":"Type Conversion/Type coercions"},{"location":"3.ngql-guide/3.data-types/9.type-conversion/#legacy_version_compatibility","text":"NGQL 1.0 adopted the C -style of type conversion (implicitly or explicitly). (type_name)expression . For example, The results of YIELD (int)(TRUE) is 1 . But it is error-prone to users who are not familiar with C language. NGQL 2.0 chooses the openCypher way of type coercions.","title":"Legacy version compatibility"},{"location":"3.ngql-guide/3.data-types/9.type-conversion/#type_coercions_functions","text":"Function Description toBoolean() Converts a string value to a boolean value. toFloat() Converts an integer or string value to a floating point number. toInteger() Converts a floating point or string value to an integer value. type() Returns the string representation of the relationship type.","title":"Type coercions functions"},{"location":"3.ngql-guide/3.data-types/9.type-conversion/#examples","text":"nebula> UNWIND [true, false, 'true', 'false', NULL] AS b RETURN toBoolean(b) AS b +----------+ | b | +----------+ | true | +----------+ | false | +----------+ | true | +----------+ | false | +----------+ | __NULL__ | +----------+ nebula> RETURN toFloat(1), toFloat('1.3'), toFloat('1e3'), toFloat('not a number') +------------+----------------+----------------+-------------------------+ | toFloat(1) | toFloat(\"1.3\") | toFloat(\"1e3\") | toFloat(\"not a number\") | +------------+----------------+----------------+-------------------------+ | 1.0 | 1.3 | 1000.0 | __NULL__ | +------------+----------------+----------------+-------------------------+ nebula> RETURN toInteger(1), toInteger('1'), toInteger('1e3'), toInteger('not a number') +--------------+----------------+------------------+---------------------------+ | toInteger(1) | toInteger(\"1\") | toInteger(\"1e3\") | toInteger(\"not a number\") | +--------------+----------------+------------------+---------------------------+ | 1 | 1 | 1000 | __NULL__ | +--------------+----------------+------------------+---------------------------+ nebula> MATCH (a:player)-[e]-() RETURN type(e) +----------+ | type(e) | +----------+ | \"follow\" | +----------+ | \"follow\" | nebula> MATCH (a:player {name: \"Tim Duncan\"}) WHERE toInteger(id(a)) == 100 RETURN a +----------------------------------------------+ | a | +----------------------------------------------+ | (\"100\" :player{age: 42, name: \"Tim Duncan\"}) | +----------------------------------------------+ nebula> MATCH (n:player) WITH n LIMIT toInteger(ceil(1.8)) RETURN count(*) AS count +-------+ | count | +-------+ | 2 | +-------+","title":"Examples"},{"location":"3.ngql-guide/4.variable-and-composite-queries/1.composite-queries/","text":"Composite queries (clause structure) \u00b6 Composite queries put data from different queries together. They then use filters, group-bys, or sorting before returning the combined return results. A composite query retrieves multiple levels of related information on existing queries and presents data as a single return result. Nebula Graph supports three methods to compose queries (or sub-queries): (OpenCypher style) Clauses are chained together, and they feed intermediate result sets between each other. (nGQL extension) More than one queries can be batched together, separated by semicolons (;). The result of the last query is returned as the result of the batch. (nGQL extension) Queries can be piped together by using the pipe operator ( | ). The result of the previous query can be used as the input of the next query. OpenCypher compatibility \u00b6 In a composite query, choose the openCypher-style or nGQL-extension. NOT BOTH . For example, if you're in the openCypher way ( MATCH , RETURN , WITH , etc), don't introduce any pipe or semicolons to combine the sub-clauses. If you're in the nGQL-extension way ( FETCH , GO , LOOKUP , etc), you must use pipe or semicolons to combine the sub-clauses. Further more, don't put together openCypher and nGQL-extension clauses in one statement. E.g., This statement is undefined: MATCH ... | GO ... | YIELD ... . Composite queries are not transactional queries (as in SQL/Cypher) \u00b6 For example, a query composed of three sub-queries: A B C , A | B | C or A; B; C . In that A is a read operation, B is a computation operation, and C is a write operation. If any part fails in the execution, the whole result is undefined. There is no rollback. What is written depends on the query executor. NOTE : openCypher has no requirement of transaction . Examples \u00b6 OpenCypher style nebula> MATCH p=(v:player{name:\"Tim Duncan\"})--() \\ WITH nodes(p) AS n \\ UNWIND n AS n1 \\ RETURN DISTINCT n1; Semicolon queries nebula> SHOW TAGS; SHOW EDGES; // Only edges are shown. nebula> INSERT VERTEX player(name, age) VALUES \"player100\":(\"Tim Duncan\", 42); \\ INSERT VERTEX player(name, age) VALUES \"player101\":(\"Tony Parker\", 36); \\ INSERT VERTEX player(name, age) VALUES \"player102\":(\"LaMarcus Aldridge\", 33); // Multiple vertices are inserted in a composite statement. Pipe queries nebula> GO FROM \"player100\" OVER follow YIELD follow._dst AS id | \\ GO FROM $-.id OVER serve YIELD $$.team.name AS Team, \\ $^.player.name AS Player; +---------+-------------+ | Team | Player | +---------+-------------+ | Nuggets | Tony Parker | +---------+-------------+","title":"Composite queries"},{"location":"3.ngql-guide/4.variable-and-composite-queries/1.composite-queries/#composite_queries_clause_structure","text":"Composite queries put data from different queries together. They then use filters, group-bys, or sorting before returning the combined return results. A composite query retrieves multiple levels of related information on existing queries and presents data as a single return result. Nebula Graph supports three methods to compose queries (or sub-queries): (OpenCypher style) Clauses are chained together, and they feed intermediate result sets between each other. (nGQL extension) More than one queries can be batched together, separated by semicolons (;). The result of the last query is returned as the result of the batch. (nGQL extension) Queries can be piped together by using the pipe operator ( | ). The result of the previous query can be used as the input of the next query.","title":"Composite queries (clause structure)"},{"location":"3.ngql-guide/4.variable-and-composite-queries/1.composite-queries/#opencypher_compatibility","text":"In a composite query, choose the openCypher-style or nGQL-extension. NOT BOTH . For example, if you're in the openCypher way ( MATCH , RETURN , WITH , etc), don't introduce any pipe or semicolons to combine the sub-clauses. If you're in the nGQL-extension way ( FETCH , GO , LOOKUP , etc), you must use pipe or semicolons to combine the sub-clauses. Further more, don't put together openCypher and nGQL-extension clauses in one statement. E.g., This statement is undefined: MATCH ... | GO ... | YIELD ... .","title":"OpenCypher compatibility"},{"location":"3.ngql-guide/4.variable-and-composite-queries/1.composite-queries/#composite_queries_are_not_transactional_queries_as_in_sqlcypher","text":"For example, a query composed of three sub-queries: A B C , A | B | C or A; B; C . In that A is a read operation, B is a computation operation, and C is a write operation. If any part fails in the execution, the whole result is undefined. There is no rollback. What is written depends on the query executor. NOTE : openCypher has no requirement of transaction .","title":"Composite queries are not transactional queries (as in SQL/Cypher)"},{"location":"3.ngql-guide/4.variable-and-composite-queries/1.composite-queries/#examples","text":"OpenCypher style nebula> MATCH p=(v:player{name:\"Tim Duncan\"})--() \\ WITH nodes(p) AS n \\ UNWIND n AS n1 \\ RETURN DISTINCT n1; Semicolon queries nebula> SHOW TAGS; SHOW EDGES; // Only edges are shown. nebula> INSERT VERTEX player(name, age) VALUES \"player100\":(\"Tim Duncan\", 42); \\ INSERT VERTEX player(name, age) VALUES \"player101\":(\"Tony Parker\", 36); \\ INSERT VERTEX player(name, age) VALUES \"player102\":(\"LaMarcus Aldridge\", 33); // Multiple vertices are inserted in a composite statement. Pipe queries nebula> GO FROM \"player100\" OVER follow YIELD follow._dst AS id | \\ GO FROM $-.id OVER serve YIELD $$.team.name AS Team, \\ $^.player.name AS Player; +---------+-------------+ | Team | Player | +---------+-------------+ | Nuggets | Tony Parker | +---------+-------------+","title":"Examples"},{"location":"3.ngql-guide/4.variable-and-composite-queries/2.user-defined-variables/","text":"User-defined variables \u00b6 User-defined variables allows passing the result of one statement to another. OpenCypher variables \u00b6 In openCypher, when you refer to a variable of vertex, edge or path, you need to name it first. The name you give to the pattern is a variable. For example: nebula> MATCH (v:player{name:\"Tim Duncan\"}) RETURN v; +----------------------------------------------------+ | v | +----------------------------------------------------+ | (\"player100\" :player{name: \"Tim Duncan\", age: 42}) | +----------------------------------------------------+ The user-defined variable in the preceding query is v . nGQL extensions \u00b6 User-defined variables are written as $var_name . The var_name consists of alphanumeric characters. Any other characters are not permitted. User-defined variables can only be used in one execution. For example, you can use user-defined variables in composite queries separated by semicolon ; or pipe | . Details about composite queries, see Composite queries . NOTE : A user-defined variable is valid only at the current session and execution. A user-defined variable in one statement CANNOT be used in either other clients or other executions. The statement that defines the user-defined variable and the statement that uses it must be submitted together. When this session ends, the user-defined variable is automatically expired. NOTE : User-defined variables are case-sensitive. Example \u00b6 nebula> $var = GO FROM \"player100\" OVER follow YIELD follow._dst AS id; \\ GO FROM $var.id OVER serve YIELD $$.team.name AS Team, \\ $^.player.name AS Player; +---------+-------------+ | Team | Player | +---------+-------------+ | Nuggets | Tony Parker | +---------+-------------+","title":"User-defined variables"},{"location":"3.ngql-guide/4.variable-and-composite-queries/2.user-defined-variables/#user-defined_variables","text":"User-defined variables allows passing the result of one statement to another.","title":"User-defined variables"},{"location":"3.ngql-guide/4.variable-and-composite-queries/2.user-defined-variables/#opencypher_variables","text":"In openCypher, when you refer to a variable of vertex, edge or path, you need to name it first. The name you give to the pattern is a variable. For example: nebula> MATCH (v:player{name:\"Tim Duncan\"}) RETURN v; +----------------------------------------------------+ | v | +----------------------------------------------------+ | (\"player100\" :player{name: \"Tim Duncan\", age: 42}) | +----------------------------------------------------+ The user-defined variable in the preceding query is v .","title":"OpenCypher variables"},{"location":"3.ngql-guide/4.variable-and-composite-queries/2.user-defined-variables/#ngql_extensions","text":"User-defined variables are written as $var_name . The var_name consists of alphanumeric characters. Any other characters are not permitted. User-defined variables can only be used in one execution. For example, you can use user-defined variables in composite queries separated by semicolon ; or pipe | . Details about composite queries, see Composite queries . NOTE : A user-defined variable is valid only at the current session and execution. A user-defined variable in one statement CANNOT be used in either other clients or other executions. The statement that defines the user-defined variable and the statement that uses it must be submitted together. When this session ends, the user-defined variable is automatically expired. NOTE : User-defined variables are case-sensitive.","title":"nGQL extensions"},{"location":"3.ngql-guide/4.variable-and-composite-queries/2.user-defined-variables/#example","text":"nebula> $var = GO FROM \"player100\" OVER follow YIELD follow._dst AS id; \\ GO FROM $var.id OVER serve YIELD $$.team.name AS Team, \\ $^.player.name AS Player; +---------+-------------+ | Team | Player | +---------+-------------+ | Nuggets | Tony Parker | +---------+-------------+","title":"Example"},{"location":"3.ngql-guide/4.variable-and-composite-queries/3.property-reference/","text":"Property reference \u00b6 This page applies to nGQL extensions only. You can refer to the properties of a vertex or an edge in WHERE or YIELD syntax. Property reference for vertex \u00b6 For source vertex \u00b6 $^.tag_name.prop_name $^ is used to get the property of the source vertex, tag_name is the tag of the vertex, and prop_name specifies the property name. For destination vertex \u00b6 $$.tag_name.prop_name $$ is used to get the property of the destination vertex, tag_name is the tag of the vertex, and prop_name specifies the property name. Property reference for edge \u00b6 For property \u00b6 Use the following syntax to get the property of an edge. edge_type.prop_name edge_type is the edge type of the edge, and prop_name specifies the property name. For built-in properties \u00b6 There are four built-in properties in each edge: _src : source vertex ID of the edge _dst : destination vertex ID of the edge _type : edge type _rank : the rank value for the edge You can use _src and _dst to get the starting and ending vertices' ID, and they are very commonly used to show a graph path. Examples \u00b6 nebula> GO FROM \"player100\" OVER follow YIELD $^.player.name AS startName, $$.player.age AS endAge; +--------------+--------+ | startName | endAge | +--------------+--------+ | \"Tim Duncan\" | 36 | +--------------+--------+ | \"Tim Duncan\" | 33 | +--------------+--------+ The preceding query returns the name property of the source vertex and the age property of the destination vertex. nebula> GO FROM \"player100\" OVER follow YIELD follow.degree; +---------------+ | follow.degree | +---------------+ | 95 | +---------------+ | 90 | +---------------+ The preceding query returns the degree property of the edge. nebula> GO FROM \"player100\" OVER follow YIELD follow._src, follow._dst, follow._type, follow._rank; +-------------+-------------+--------------+--------------+ | follow._src | follow._dst | follow._type | follow._rank | +-------------+-------------+--------------+--------------+ | \"player100\" | \"player101\" | 136 | 0 | +-------------+-------------+--------------+--------------+ | \"player100\" | \"player102\" | 136 | 0 | +-------------+-------------+--------------+--------------+ The preceding query returns all the neighbors of vertex \"player100\" over the follow edges, by referencing follow._src as the source vertex ID (which is \"player100\" ) and follow._dst as the destination vertex ID.","title":"Property reference"},{"location":"3.ngql-guide/4.variable-and-composite-queries/3.property-reference/#property_reference","text":"This page applies to nGQL extensions only. You can refer to the properties of a vertex or an edge in WHERE or YIELD syntax.","title":"Property reference"},{"location":"3.ngql-guide/4.variable-and-composite-queries/3.property-reference/#property_reference_for_vertex","text":"","title":"Property reference for vertex"},{"location":"3.ngql-guide/4.variable-and-composite-queries/3.property-reference/#for_source_vertex","text":"$^.tag_name.prop_name $^ is used to get the property of the source vertex, tag_name is the tag of the vertex, and prop_name specifies the property name.","title":"For source vertex"},{"location":"3.ngql-guide/4.variable-and-composite-queries/3.property-reference/#for_destination_vertex","text":"$$.tag_name.prop_name $$ is used to get the property of the destination vertex, tag_name is the tag of the vertex, and prop_name specifies the property name.","title":"For destination vertex"},{"location":"3.ngql-guide/4.variable-and-composite-queries/3.property-reference/#property_reference_for_edge","text":"","title":"Property reference for edge"},{"location":"3.ngql-guide/4.variable-and-composite-queries/3.property-reference/#for_property","text":"Use the following syntax to get the property of an edge. edge_type.prop_name edge_type is the edge type of the edge, and prop_name specifies the property name.","title":"For property"},{"location":"3.ngql-guide/4.variable-and-composite-queries/3.property-reference/#for_built-in_properties","text":"There are four built-in properties in each edge: _src : source vertex ID of the edge _dst : destination vertex ID of the edge _type : edge type _rank : the rank value for the edge You can use _src and _dst to get the starting and ending vertices' ID, and they are very commonly used to show a graph path.","title":"For built-in properties"},{"location":"3.ngql-guide/4.variable-and-composite-queries/3.property-reference/#examples","text":"nebula> GO FROM \"player100\" OVER follow YIELD $^.player.name AS startName, $$.player.age AS endAge; +--------------+--------+ | startName | endAge | +--------------+--------+ | \"Tim Duncan\" | 36 | +--------------+--------+ | \"Tim Duncan\" | 33 | +--------------+--------+ The preceding query returns the name property of the source vertex and the age property of the destination vertex. nebula> GO FROM \"player100\" OVER follow YIELD follow.degree; +---------------+ | follow.degree | +---------------+ | 95 | +---------------+ | 90 | +---------------+ The preceding query returns the degree property of the edge. nebula> GO FROM \"player100\" OVER follow YIELD follow._src, follow._dst, follow._type, follow._rank; +-------------+-------------+--------------+--------------+ | follow._src | follow._dst | follow._type | follow._rank | +-------------+-------------+--------------+--------------+ | \"player100\" | \"player101\" | 136 | 0 | +-------------+-------------+--------------+--------------+ | \"player100\" | \"player102\" | 136 | 0 | +-------------+-------------+--------------+--------------+ The preceding query returns all the neighbors of vertex \"player100\" over the follow edges, by referencing follow._src as the source vertex ID (which is \"player100\" ) and follow._dst as the destination vertex ID.","title":"Examples"},{"location":"3.ngql-guide/5.operators/1.comparison/","text":"Comparison operators \u00b6 Name Description = Assign a value / Division operator == Equal operator != , <> Not equal operator < Less than operator <= Less than or equal operator - Minus operator % Modulo operator + Addition operator * Multiplication operator - Change the sign of the argument IS NULL NULL check IS NOT NULL not NULL check Comparison operations result in a value of true and false . NOTE: Comparability between values of different types is often undefined. The result could be NULL or others. OpenCypher compatibility : Comparing with NULL is different from openCypher. The behavior may change. IS [NOT] NULL is often used with OPTIONAL MATCH . But OPTIONAL MATCH is not support in nGQL. == Equal. String comparisons are case-sensitive. Values of different types are not equal. NOTE: The equality operator is == in nGQL and is = in openCypher. nebula> RETURN 'A' == 'a', toUpper('A') == toUpper('a'), toLower('A') == toLower('a') +------------+------------------------------+------------------------------+ | (\"A\"==\"a\") | (toUpper(\"A\")==toUpper(\"a\")) | (toLower(\"A\")==toLower(\"a\")) | +------------+------------------------------+------------------------------+ | false | true | true | +------------+------------------------------+------------------------------+ nebula> RETURN '2' == 2, toInteger('2') == 2; +----------+---------------------+ | (\"2\"==2) | (toInteger(\"2\")==2) | +----------+---------------------+ | false | true | +----------+---------------------+ > Greater than: nebula> RETURN 3 > 2; +-------+ | (3>2) | +-------+ | true | +-------+ nebula> WITH 4 AS one, 3 AS two RETURN one > two AS result; +--------+ | result | +--------+ | true | +--------+ >= Greater than or equal to: nebula> RETURN 2 >= \"2\", 2 >= 2 +----------+--------+ | (2>=\"2\") | (2>=2) | +----------+--------+ | __NULL__ | true | +----------+--------+ < Less than: nebula> YIELD 2.0 < 1.9; +---------+ | (2<1.9) | +---------+ | false | +---------+ <= Less than or equal to: nebula> YIELD 0.11 <= 0.11; +--------------+ | (0.11<=0.11) | +--------------+ | true | +--------------+ != Not equal: nebula> YIELD 1 != '1'; +--------+ | (1!=1) | +--------+ | true | +--------+ IS [NOT] NULL nebula> RETURN null IS NULL AS value1, null == null AS value2, null != null AS value3 +--------+----------+----------+ | value1 | value2 | value3 | +--------+----------+----------+ | true | __NULL__ | __NULL__ | +--------+----------+----------+ nebula> RETURN length(NULL), size(NULL), count(NULL), NULL IS NULL, NULL IS NOT NULL, sin(NULL), NULL + NULL, [1, NULL] IS NULL +--------------+------------+-------------+--------------+------------------+-----------+-------------+------------------+ | length(NULL) | size(NULL) | COUNT(NULL) | NULL IS NULL | NULL IS NOT NULL | sin(NULL) | (NULL+NULL) | [1,NULL] IS NULL | +--------------+------------+-------------+--------------+------------------+-----------+-------------+------------------+ | BAD_TYPE | __NULL__ | 0 | true | false | BAD_TYPE | __NULL__ | false | +--------------+------------+-------------+--------------+------------------+-----------+-------------+------------------+ nebula> WITH {name: null} AS map RETURN map.name IS NOT NULL +----------------------+ | map.name IS NOT NULL | +----------------------+ | false | +----------------------+ nebula> WITH {name: 'Mats', name2: 'Pontus'} AS map1, \\ {name: null} AS map2, {notName: 0, notName2: null } AS map3 \\ RETURN map1.name IS NULL, map2.name IS NOT NULL, map3.name IS NULL +-------------------+-----------------------+-------------------+ | map1.name IS NULL | map2.name IS NOT NULL | map3.name IS NULL | +-------------------+-----------------------+-------------------+ | false | false | true | +-------------------+-----------------------+-------------------+ nebula> MATCH (n:player) RETURN n.age IS NULL, n.name IS NOT NULL, n.empty IS NULL +---------------+--------------------+-----------------+ | n.age IS NULL | n.name IS NOT NULL | n.empty IS NULL | +---------------+--------------------+-----------------+ | false | true | true | +---------------+--------------------+-----------------+ | false | true | true | +---------------+--------------------+-----------------+ | false | true | true | +---------------+--------------------+-----------------+ ...","title":"Comparison"},{"location":"3.ngql-guide/5.operators/1.comparison/#comparison_operators","text":"Name Description = Assign a value / Division operator == Equal operator != , <> Not equal operator < Less than operator <= Less than or equal operator - Minus operator % Modulo operator + Addition operator * Multiplication operator - Change the sign of the argument IS NULL NULL check IS NOT NULL not NULL check Comparison operations result in a value of true and false . NOTE: Comparability between values of different types is often undefined. The result could be NULL or others. OpenCypher compatibility : Comparing with NULL is different from openCypher. The behavior may change. IS [NOT] NULL is often used with OPTIONAL MATCH . But OPTIONAL MATCH is not support in nGQL. == Equal. String comparisons are case-sensitive. Values of different types are not equal. NOTE: The equality operator is == in nGQL and is = in openCypher. nebula> RETURN 'A' == 'a', toUpper('A') == toUpper('a'), toLower('A') == toLower('a') +------------+------------------------------+------------------------------+ | (\"A\"==\"a\") | (toUpper(\"A\")==toUpper(\"a\")) | (toLower(\"A\")==toLower(\"a\")) | +------------+------------------------------+------------------------------+ | false | true | true | +------------+------------------------------+------------------------------+ nebula> RETURN '2' == 2, toInteger('2') == 2; +----------+---------------------+ | (\"2\"==2) | (toInteger(\"2\")==2) | +----------+---------------------+ | false | true | +----------+---------------------+ > Greater than: nebula> RETURN 3 > 2; +-------+ | (3>2) | +-------+ | true | +-------+ nebula> WITH 4 AS one, 3 AS two RETURN one > two AS result; +--------+ | result | +--------+ | true | +--------+ >= Greater than or equal to: nebula> RETURN 2 >= \"2\", 2 >= 2 +----------+--------+ | (2>=\"2\") | (2>=2) | +----------+--------+ | __NULL__ | true | +----------+--------+ < Less than: nebula> YIELD 2.0 < 1.9; +---------+ | (2<1.9) | +---------+ | false | +---------+ <= Less than or equal to: nebula> YIELD 0.11 <= 0.11; +--------------+ | (0.11<=0.11) | +--------------+ | true | +--------------+ != Not equal: nebula> YIELD 1 != '1'; +--------+ | (1!=1) | +--------+ | true | +--------+ IS [NOT] NULL nebula> RETURN null IS NULL AS value1, null == null AS value2, null != null AS value3 +--------+----------+----------+ | value1 | value2 | value3 | +--------+----------+----------+ | true | __NULL__ | __NULL__ | +--------+----------+----------+ nebula> RETURN length(NULL), size(NULL), count(NULL), NULL IS NULL, NULL IS NOT NULL, sin(NULL), NULL + NULL, [1, NULL] IS NULL +--------------+------------+-------------+--------------+------------------+-----------+-------------+------------------+ | length(NULL) | size(NULL) | COUNT(NULL) | NULL IS NULL | NULL IS NOT NULL | sin(NULL) | (NULL+NULL) | [1,NULL] IS NULL | +--------------+------------+-------------+--------------+------------------+-----------+-------------+------------------+ | BAD_TYPE | __NULL__ | 0 | true | false | BAD_TYPE | __NULL__ | false | +--------------+------------+-------------+--------------+------------------+-----------+-------------+------------------+ nebula> WITH {name: null} AS map RETURN map.name IS NOT NULL +----------------------+ | map.name IS NOT NULL | +----------------------+ | false | +----------------------+ nebula> WITH {name: 'Mats', name2: 'Pontus'} AS map1, \\ {name: null} AS map2, {notName: 0, notName2: null } AS map3 \\ RETURN map1.name IS NULL, map2.name IS NOT NULL, map3.name IS NULL +-------------------+-----------------------+-------------------+ | map1.name IS NULL | map2.name IS NOT NULL | map3.name IS NULL | +-------------------+-----------------------+-------------------+ | false | false | true | +-------------------+-----------------------+-------------------+ nebula> MATCH (n:player) RETURN n.age IS NULL, n.name IS NOT NULL, n.empty IS NULL +---------------+--------------------+-----------------+ | n.age IS NULL | n.name IS NOT NULL | n.empty IS NULL | +---------------+--------------------+-----------------+ | false | true | true | +---------------+--------------------+-----------------+ | false | true | true | +---------------+--------------------+-----------------+ | false | true | true | +---------------+--------------------+-----------------+ ...","title":"Comparison operators"},{"location":"3.ngql-guide/5.operators/2.boolean/","text":"Boolean operators \u00b6 Name Description AND Logical AND NOT Logical NOT OR Logical OR XOR Logical XOR For the precedence of the operators, refer to Operator Precedence . For the logical operations with NULL, refer to NULL . Legacy version compatibility \u00b6 In Nebula Graph 1.0, non-zero numbers are evaluated to true like c-language. In Nebula Graph 2.0, non-zero numbers can't be converted to boolean values.","title":"Boolean"},{"location":"3.ngql-guide/5.operators/2.boolean/#boolean_operators","text":"Name Description AND Logical AND NOT Logical NOT OR Logical OR XOR Logical XOR For the precedence of the operators, refer to Operator Precedence . For the logical operations with NULL, refer to NULL .","title":"Boolean operators"},{"location":"3.ngql-guide/5.operators/2.boolean/#legacy_version_compatibility","text":"In Nebula Graph 1.0, non-zero numbers are evaluated to true like c-language. In Nebula Graph 2.0, non-zero numbers can't be converted to boolean values.","title":"Legacy version compatibility"},{"location":"3.ngql-guide/5.operators/4.pipe/","text":"Pipe operator \u00b6 OpenCypher compatibility \u00b6 This page applies to nGQL extensions only. Syntax \u00b6 One major difference between nGQL and SQL is how sub-queries are composed. In SQL, to form a statement, sub-queries are nested (embedded). In nGQL the shell style PIPE (|) is introduced. Examples \u00b6 nebula> GO FROM \"player100\" OVER follow \\ YIELD follow._dst AS dstid, $$.player.name AS Name |\\ GO FROM $-.dstid OVER follow; +-------------+ | follow._dst | +-------------+ | \"player101\" | +-------------+ If there is no YIELD clause to define the output, the destination vertex ID is returned by default. If a YIELD clause is applied, the output is defined by the YIELD clause. You must define aliases in the YIELD clause for the reference operator $- to use, just like $-.dstid in the preceding example.","title":"Pipe"},{"location":"3.ngql-guide/5.operators/4.pipe/#pipe_operator","text":"","title":"Pipe operator"},{"location":"3.ngql-guide/5.operators/4.pipe/#opencypher_compatibility","text":"This page applies to nGQL extensions only.","title":"OpenCypher compatibility"},{"location":"3.ngql-guide/5.operators/4.pipe/#syntax","text":"One major difference between nGQL and SQL is how sub-queries are composed. In SQL, to form a statement, sub-queries are nested (embedded). In nGQL the shell style PIPE (|) is introduced.","title":"Syntax"},{"location":"3.ngql-guide/5.operators/4.pipe/#examples","text":"nebula> GO FROM \"player100\" OVER follow \\ YIELD follow._dst AS dstid, $$.player.name AS Name |\\ GO FROM $-.dstid OVER follow; +-------------+ | follow._dst | +-------------+ | \"player101\" | +-------------+ If there is no YIELD clause to define the output, the destination vertex ID is returned by default. If a YIELD clause is applied, the output is defined by the YIELD clause. You must define aliases in the YIELD clause for the reference operator $- to use, just like $-.dstid in the preceding example.","title":"Examples"},{"location":"3.ngql-guide/5.operators/5.property-reference/","text":"Reference operators \u00b6 NGQL provides reference operators to represent a property in a WHERE or YIELD clause, or the output of the statement before the pipe symbol in a composite query. OpenCypher compatibility \u00b6 This page applies to nGQL extensions only. Reference operator List \u00b6 Reference operator Description $^ Refers to a source vertex property. For more information, see Property reference . $$ Refers to a destination vertex property. For more information, see Property reference . $- Refers to the output of the statement before the pipe symbol in a composite query. For more information, see Pipe . Examples \u00b6 The following example returns the age of the source vertex and the destination vertex. nebula> GO FROM \"player100\" OVER follow \\ YIELD $^.player.age AS SrcAge, $$.player.age AS DestAge; +--------+---------+ | SrcAge | DestAge | +--------+---------+ | 42 | 36 | +--------+---------+ | 42 | 41 | +--------+---------+ The following example returns the name and team of the players that \"player100\" follows. nebula> GO FROM \"player100\" OVER follow \\ YIELD follow._dst AS id | \\ GO FROM $-.id OVER serve \\ YIELD $^.player.name AS Player, $$.team.name AS Team; +-----------------+-----------+ | Player | Team | +-----------------+-----------+ | \"Tony Parker\" | \"Spurs\" | +-----------------+-----------+ | \"Tony Parker\" | \"Hornets\" | +-----------------+-----------+ | \"Manu Ginobili\" | \"Spurs\" | +-----------------+-----------+","title":"Property reference"},{"location":"3.ngql-guide/5.operators/5.property-reference/#reference_operators","text":"NGQL provides reference operators to represent a property in a WHERE or YIELD clause, or the output of the statement before the pipe symbol in a composite query.","title":"Reference operators"},{"location":"3.ngql-guide/5.operators/5.property-reference/#opencypher_compatibility","text":"This page applies to nGQL extensions only.","title":"OpenCypher compatibility"},{"location":"3.ngql-guide/5.operators/5.property-reference/#reference_operator_list","text":"Reference operator Description $^ Refers to a source vertex property. For more information, see Property reference . $$ Refers to a destination vertex property. For more information, see Property reference . $- Refers to the output of the statement before the pipe symbol in a composite query. For more information, see Pipe .","title":"Reference operator List"},{"location":"3.ngql-guide/5.operators/5.property-reference/#examples","text":"The following example returns the age of the source vertex and the destination vertex. nebula> GO FROM \"player100\" OVER follow \\ YIELD $^.player.age AS SrcAge, $$.player.age AS DestAge; +--------+---------+ | SrcAge | DestAge | +--------+---------+ | 42 | 36 | +--------+---------+ | 42 | 41 | +--------+---------+ The following example returns the name and team of the players that \"player100\" follows. nebula> GO FROM \"player100\" OVER follow \\ YIELD follow._dst AS id | \\ GO FROM $-.id OVER serve \\ YIELD $^.player.name AS Player, $$.team.name AS Team; +-----------------+-----------+ | Player | Team | +-----------------+-----------+ | \"Tony Parker\" | \"Spurs\" | +-----------------+-----------+ | \"Tony Parker\" | \"Hornets\" | +-----------------+-----------+ | \"Manu Ginobili\" | \"Spurs\" | +-----------------+-----------+","title":"Examples"},{"location":"3.ngql-guide/5.operators/6.set/","text":"Set operations \u00b6 OpenCypher compatibility \u00b6 This page applies to nGQL extensions only. Syntax \u00b6 This document descriptions the set operations, including UNION , UNION ALL , INTERSECT , and MINUS . To combine multiple queries, use the set operators. All set operators have equal precedence. If a nGQL statement contains multiple set operators, Nebula Graph evaluates them from the left to right unless parentheses explicitly specify another order. To use the set operators, always match the return results of the GO clause with the same number and data type. UNION, UNION DISTINCT, and UNION ALL \u00b6 <left> UNION [DISTINCT | ALL] <right> [ UNION [DISTINCT | ALL] <right> ...] Operator UNION DISTINCT (or by short UNION ) returns the union of two sets A and B without the duplicate elements. Operator UNION ALL returns the union of two sets A and B with duplicated elements. The <left> and <right> must have the same number of columns and data types. Different data types are converted according to the Type Conversion . Example \u00b6 The following statement nebula> GO FROM \"player102\" OVER follow \\ UNION \\ GO FROM \"player100\" OVER follow; +-------------+ | follow._dst | +-------------+ | \"player101\" | +-------------+ | \"player102\" | +-------------+ returns the neighbors' id of vertex \"player102\" and \"player100 (along with edge follow ) without duplication. While nebula> GO FROM \"player102\" OVER follow \\ UNION ALL \\ GO FROM \"player100\" OVER follow; +-------------+ | follow._dst | +-------------+ | \"player101\" | +-------------+ | \"player101\" | +-------------+ | \"player102\" | +-------------+ returns all the neighbors of vertex \"player102\" and \"player100 , with all possible duplications. UNION can also work with the YIELD statement. For example, let's suppose the results of the following two queries. nebula> GO FROM \"player102\" OVER follow YIELD follow._dst AS id, follow.degree AS Degree, $$.player.age AS Age; -- query 1 +-------------+--------+-----+ | id | Degree | Age | +-------------+--------+-----+ | \"player101\" | 75 | 36 | -- line 1 +-------------+--------+-----+ nebula> GO FROM \"player100\" OVER follow YIELD follow._dst AS id, follow.degree AS Degree, $$.player.age AS Age; -- query 2 +-------------+--------+-----+ | id | Degree | Age | +-------------+--------+-----+ | \"player101\" | 96 | 36 | -- line 2 +-------------+--------+-----+ | \"player102\" | 90 | 33 | -- line 3 +-------------+--------+-----+ And the following statement nebula> GO FROM \"player102\" OVER follow YIELD follow._dst AS id, follow.degree AS Degree, $$.player.age AS Age \\ UNION /* DISTINCT */ \\ GO FROM \"player100\" OVER follow YIELD follow._dst AS id, follow.degree AS Degree, $$.player.age AS Age; returns the follows: +-------------+--------+-----+ | id | Degree | Age | +-------------+--------+-----+ | \"player101\" | 75 | 36 | -- line 1 +-------------+--------+-----+ | \"player101\" | 96 | 36 | -- line 2 +-------------+--------+-----+ | \"player102\" | 90 | 33 | -- line 3 +-------------+--------+-----+ The DISTINCT check duplication by all the columns for every line. So line 1 and line 2 are different. INTERSECT \u00b6 <left> INTERSECT <right> Operator INTERSECT returns the intersection of two sets A and B (denoted by A \u22c2 B). Similar to UNION , the <left> and <right> must have the same number of columns and data types. Only the INTERSECT columns of <left> and <right> are returned. For example, the following query nebula> GO FROM \"player102\" OVER follow YIELD follow._dst AS id, follow.degree AS Degree, $$.player.age AS Age \\ INTERSECT \\ GO FROM \"player100\" OVER follow YIELD follow._dst AS id, follow.degree AS Degree, $$.player.age AS Age; returns Empty set (time spent 5194/6264 us) MINUS \u00b6 <left> MINUS <right> Operator MINUS returns the subtraction (or difference) of two sets A and B (denoted by A - B). Always pay attention to the order of the <left> and <right> . The set A - B consists of elements that are in A but not in B. For example, the following query nebula> GO FROM \"player100\" OVER follow \\ MINUS \\ GO FROM \"player102\" OVER follow; returns +-------------+ | follow._dst | +-------------+ | \"player102\" | +-------------+ If you reverse the MINUS order, the query nebula> GO FROM \"player102\" OVER follow \\ MINUS \\ GO FROM \"player100\" OVER follow; returns Empty set (time spent 2243/3259 us) Precedence of the SET Operations and Pipe \u00b6 Please note that when a query contains pipe | and set operations, pipe takes precedence. Refer to the Pipe Doc for details. Query GO FROM 1 UNION GO FROM 2 | GO FROM 3 is the same as query GO FROM 1 UNION (GO FROM 2 | GO FROM 3) . For example: nebula> GO FROM \"player102\" OVER follow YIELD follow._dst AS play_dst \\ UNION \\ GO FROM \"team200\" OVER serve REVERSELY YIELD serve._dst AS play_dst \\ | GO FROM $-.play_dst OVER follow YIELD follow._dst AS play_dst; +-------------+ | play_dst | +-------------+ | \"player101\" | +-------------+ | \"player102\" | +-------------+ The statements in the red bar are executed first. And then the statement in the green box is executed. nebula> (GO FROM \"player102\" OVER follow YIELD follow._dst AS play_dst \\ UNION \\ GO FROM \"team200\" OVER serve REVERSELY YIELD serve._dst AS play_dst) \\ | GO FROM $-.play_dst OVER follow YIELD follow._dst AS play_dst; In the above query, the parentheses change the execution priority, and the statements within the parentheses take the precedence.","title":"Set"},{"location":"3.ngql-guide/5.operators/6.set/#set_operations","text":"","title":"Set operations"},{"location":"3.ngql-guide/5.operators/6.set/#opencypher_compatibility","text":"This page applies to nGQL extensions only.","title":"OpenCypher compatibility"},{"location":"3.ngql-guide/5.operators/6.set/#syntax","text":"This document descriptions the set operations, including UNION , UNION ALL , INTERSECT , and MINUS . To combine multiple queries, use the set operators. All set operators have equal precedence. If a nGQL statement contains multiple set operators, Nebula Graph evaluates them from the left to right unless parentheses explicitly specify another order. To use the set operators, always match the return results of the GO clause with the same number and data type.","title":"Syntax"},{"location":"3.ngql-guide/5.operators/6.set/#union_union_distinct_and_union_all","text":"<left> UNION [DISTINCT | ALL] <right> [ UNION [DISTINCT | ALL] <right> ...] Operator UNION DISTINCT (or by short UNION ) returns the union of two sets A and B without the duplicate elements. Operator UNION ALL returns the union of two sets A and B with duplicated elements. The <left> and <right> must have the same number of columns and data types. Different data types are converted according to the Type Conversion .","title":"UNION, UNION DISTINCT, and UNION ALL"},{"location":"3.ngql-guide/5.operators/6.set/#example","text":"The following statement nebula> GO FROM \"player102\" OVER follow \\ UNION \\ GO FROM \"player100\" OVER follow; +-------------+ | follow._dst | +-------------+ | \"player101\" | +-------------+ | \"player102\" | +-------------+ returns the neighbors' id of vertex \"player102\" and \"player100 (along with edge follow ) without duplication. While nebula> GO FROM \"player102\" OVER follow \\ UNION ALL \\ GO FROM \"player100\" OVER follow; +-------------+ | follow._dst | +-------------+ | \"player101\" | +-------------+ | \"player101\" | +-------------+ | \"player102\" | +-------------+ returns all the neighbors of vertex \"player102\" and \"player100 , with all possible duplications. UNION can also work with the YIELD statement. For example, let's suppose the results of the following two queries. nebula> GO FROM \"player102\" OVER follow YIELD follow._dst AS id, follow.degree AS Degree, $$.player.age AS Age; -- query 1 +-------------+--------+-----+ | id | Degree | Age | +-------------+--------+-----+ | \"player101\" | 75 | 36 | -- line 1 +-------------+--------+-----+ nebula> GO FROM \"player100\" OVER follow YIELD follow._dst AS id, follow.degree AS Degree, $$.player.age AS Age; -- query 2 +-------------+--------+-----+ | id | Degree | Age | +-------------+--------+-----+ | \"player101\" | 96 | 36 | -- line 2 +-------------+--------+-----+ | \"player102\" | 90 | 33 | -- line 3 +-------------+--------+-----+ And the following statement nebula> GO FROM \"player102\" OVER follow YIELD follow._dst AS id, follow.degree AS Degree, $$.player.age AS Age \\ UNION /* DISTINCT */ \\ GO FROM \"player100\" OVER follow YIELD follow._dst AS id, follow.degree AS Degree, $$.player.age AS Age; returns the follows: +-------------+--------+-----+ | id | Degree | Age | +-------------+--------+-----+ | \"player101\" | 75 | 36 | -- line 1 +-------------+--------+-----+ | \"player101\" | 96 | 36 | -- line 2 +-------------+--------+-----+ | \"player102\" | 90 | 33 | -- line 3 +-------------+--------+-----+ The DISTINCT check duplication by all the columns for every line. So line 1 and line 2 are different.","title":"Example"},{"location":"3.ngql-guide/5.operators/6.set/#intersect","text":"<left> INTERSECT <right> Operator INTERSECT returns the intersection of two sets A and B (denoted by A \u22c2 B). Similar to UNION , the <left> and <right> must have the same number of columns and data types. Only the INTERSECT columns of <left> and <right> are returned. For example, the following query nebula> GO FROM \"player102\" OVER follow YIELD follow._dst AS id, follow.degree AS Degree, $$.player.age AS Age \\ INTERSECT \\ GO FROM \"player100\" OVER follow YIELD follow._dst AS id, follow.degree AS Degree, $$.player.age AS Age; returns Empty set (time spent 5194/6264 us)","title":"INTERSECT"},{"location":"3.ngql-guide/5.operators/6.set/#minus","text":"<left> MINUS <right> Operator MINUS returns the subtraction (or difference) of two sets A and B (denoted by A - B). Always pay attention to the order of the <left> and <right> . The set A - B consists of elements that are in A but not in B. For example, the following query nebula> GO FROM \"player100\" OVER follow \\ MINUS \\ GO FROM \"player102\" OVER follow; returns +-------------+ | follow._dst | +-------------+ | \"player102\" | +-------------+ If you reverse the MINUS order, the query nebula> GO FROM \"player102\" OVER follow \\ MINUS \\ GO FROM \"player100\" OVER follow; returns Empty set (time spent 2243/3259 us)","title":"MINUS"},{"location":"3.ngql-guide/5.operators/6.set/#precedence_of_the_set_operations_and_pipe","text":"Please note that when a query contains pipe | and set operations, pipe takes precedence. Refer to the Pipe Doc for details. Query GO FROM 1 UNION GO FROM 2 | GO FROM 3 is the same as query GO FROM 1 UNION (GO FROM 2 | GO FROM 3) . For example: nebula> GO FROM \"player102\" OVER follow YIELD follow._dst AS play_dst \\ UNION \\ GO FROM \"team200\" OVER serve REVERSELY YIELD serve._dst AS play_dst \\ | GO FROM $-.play_dst OVER follow YIELD follow._dst AS play_dst; +-------------+ | play_dst | +-------------+ | \"player101\" | +-------------+ | \"player102\" | +-------------+ The statements in the red bar are executed first. And then the statement in the green box is executed. nebula> (GO FROM \"player102\" OVER follow YIELD follow._dst AS play_dst \\ UNION \\ GO FROM \"team200\" OVER serve REVERSELY YIELD serve._dst AS play_dst) \\ | GO FROM $-.play_dst OVER follow YIELD follow._dst AS play_dst; In the above query, the parentheses change the execution priority, and the statements within the parentheses take the precedence.","title":"Precedence of the SET Operations and Pipe"},{"location":"3.ngql-guide/5.operators/7.string/","text":"String operators \u00b6 Name Description + concatenating strings CONTAINS Perform case-sensitive inclusion searching in strings (NOT) IN Whether a value is within a set of values (NOT) STARTS WITH Perform case-sensitive matching on the beginning of a string (NOT) ENDS WITH Perform case-sensitive matching on the ending of a string Regular expressions Perform regular expression matching on a string NOTE : All the string matchings are case-sensitive. Examples \u00b6 concatenation (+) nebula> RETURN 'a' + 'b'; +-------+ | (a+b) | +-------+ | \"ab\" | +-------+ nebula> UNWIND 'a' AS a UNWIND 'b' AS b RETURN a + b; +-------+ | (a+b) | +-------+ | \"ab\" | +-------+ CONTAINS The CONTAINS operator requires string type in both left and right side. nebula> MATCH (s:player)-[e:serve]->(t:team) WHERE id(s) == \"player101\" \\ AND t.name CONTAINS \"ets\" RETURN s.name, e.start_year, e.end_year, t.name; +---------------+--------------+------------+-----------+ | s.name | e.start_year | e.end_year | t.name | +---------------+--------------+------------+-----------+ | \"Tony Parker\" | 2018 | 2019 | \"Hornets\" | +---------------+--------------+------------+-----------+ nebula> GO FROM \"player101\" OVER serve WHERE (STRING)serve.start_year CONTAINS \"19\" AND \\ $^.player.name CONTAINS \"ny\" \\ YIELD $^.player.name, serve.start_year, serve.end_year, $$.team.name; +----------------+------------------+----------------+--------------+ | $^.player.name | serve.start_year | serve.end_year | $$.team.name | +----------------+------------------+----------------+--------------+ | \"Tony Parker\" | 1999 | 2018 | \"Spurs\" | +----------------+------------------+----------------+--------------+ nebula> GO FROM \"player101\" OVER serve WHERE !($$.team.name CONTAINS \"ets\") \\ YIELD $^.player.name, serve.start_year, serve.end_year, $$.team.name; +----------------+------------------+----------------+--------------+ | $^.player.name | serve.start_year | serve.end_year | $$.team.name | +----------------+------------------+----------------+--------------+ | \"Tony Parker\" | 1999 | 2018 | \"Spurs\" | +----------------+------------------+----------------+--------------+ IN nebula> RETURN 1 IN [1,2,3], \"Yao\" IN [\"Yi\", \"Tim\", \"Kobe\"], NULL in [\"Yi\", \"Tim\", \"Kobe\"] +----------------+--------------------------------+-------------------------------+ | (1 IN [1,2,3]) | (\"Yao\" IN [\"Yi\",\"Tim\",\"Kobe\"]) | (NULL IN [\"Yi\",\"Tim\",\"Kobe\"]) | +----------------+--------------------------------+-------------------------------+ | true | false | false | +----------------+--------------------------------+-------------------------------+ (NOT) STARTS WITH nebula> RETURN 'apple' STARTS WITH 'app', 'apple' STARTS WITH 'a', 'apple' STARTS WITH toUpper('a') +-----------------------------+---------------------------+------------------------------------+ | (\"apple\" STARTS WITH \"app\") | (\"apple\" STARTS WITH \"a\") | (\"apple\" STARTS WITH toUpper(\"a\")) | +-----------------------------+---------------------------+------------------------------------+ | true | true | false | +-----------------------------+---------------------------+------------------------------------+ nebula> RETURN 'apple' STARTS WITH 'b','apple' NOT STARTS WITH 'app' +---------------------------+---------------------------------+ | (\"apple\" STARTS WITH \"b\") | (\"apple\" NOT STARTS WITH \"app\") | +---------------------------+---------------------------------+ | false | false | +---------------------------+---------------------------------+ (NOT) ENDS WITH nebula> RETURN 'apple' ENDS WITH 'app', 'apple' ENDS WITH 'e', 'apple' ENDS WITH 'E', 'apple' ENDS WITH 'b' +---------------------------+-------------------------+-------------------------+-------------------------+ | (\"apple\" ENDS WITH \"app\") | (\"apple\" ENDS WITH \"e\") | (\"apple\" ENDS WITH \"E\") | (\"apple\" ENDS WITH \"b\") | +---------------------------+-------------------------+-------------------------+-------------------------+ | false | true | false | false | +---------------------------+-------------------------+-------------------------+-------------------------+ Regular expressions Nebula Graph supports filtering by using regular expressions. The regular expression syntax is inherited from std::regex . You can match on regular expressions by using =~ 'regexp' . For example: nebula> RETURN \"384748.39\" =~ \"\\\\d+(\\\\.\\\\d{2})?\"; +----------------------------+ | (384748.39=~\\d+(\\.\\d{2})?) | +----------------------------+ | true | +----------------------------+ nebula> MATCH (v:player) WHERE v.name =~ 'Tony.*' RETURN v.name; +---------------+ | v.name | +---------------+ | \"Tony Parker\" | +---------------+ NOTE : Regular expressions CAN NOT work with nGQL-extensions (GO/FETCH clause will return syntax error). Use it in openCypher only (e.g., in MATCH-WHERE clause).","title":"String"},{"location":"3.ngql-guide/5.operators/7.string/#string_operators","text":"Name Description + concatenating strings CONTAINS Perform case-sensitive inclusion searching in strings (NOT) IN Whether a value is within a set of values (NOT) STARTS WITH Perform case-sensitive matching on the beginning of a string (NOT) ENDS WITH Perform case-sensitive matching on the ending of a string Regular expressions Perform regular expression matching on a string NOTE : All the string matchings are case-sensitive.","title":"String operators"},{"location":"3.ngql-guide/5.operators/7.string/#examples","text":"concatenation (+) nebula> RETURN 'a' + 'b'; +-------+ | (a+b) | +-------+ | \"ab\" | +-------+ nebula> UNWIND 'a' AS a UNWIND 'b' AS b RETURN a + b; +-------+ | (a+b) | +-------+ | \"ab\" | +-------+ CONTAINS The CONTAINS operator requires string type in both left and right side. nebula> MATCH (s:player)-[e:serve]->(t:team) WHERE id(s) == \"player101\" \\ AND t.name CONTAINS \"ets\" RETURN s.name, e.start_year, e.end_year, t.name; +---------------+--------------+------------+-----------+ | s.name | e.start_year | e.end_year | t.name | +---------------+--------------+------------+-----------+ | \"Tony Parker\" | 2018 | 2019 | \"Hornets\" | +---------------+--------------+------------+-----------+ nebula> GO FROM \"player101\" OVER serve WHERE (STRING)serve.start_year CONTAINS \"19\" AND \\ $^.player.name CONTAINS \"ny\" \\ YIELD $^.player.name, serve.start_year, serve.end_year, $$.team.name; +----------------+------------------+----------------+--------------+ | $^.player.name | serve.start_year | serve.end_year | $$.team.name | +----------------+------------------+----------------+--------------+ | \"Tony Parker\" | 1999 | 2018 | \"Spurs\" | +----------------+------------------+----------------+--------------+ nebula> GO FROM \"player101\" OVER serve WHERE !($$.team.name CONTAINS \"ets\") \\ YIELD $^.player.name, serve.start_year, serve.end_year, $$.team.name; +----------------+------------------+----------------+--------------+ | $^.player.name | serve.start_year | serve.end_year | $$.team.name | +----------------+------------------+----------------+--------------+ | \"Tony Parker\" | 1999 | 2018 | \"Spurs\" | +----------------+------------------+----------------+--------------+ IN nebula> RETURN 1 IN [1,2,3], \"Yao\" IN [\"Yi\", \"Tim\", \"Kobe\"], NULL in [\"Yi\", \"Tim\", \"Kobe\"] +----------------+--------------------------------+-------------------------------+ | (1 IN [1,2,3]) | (\"Yao\" IN [\"Yi\",\"Tim\",\"Kobe\"]) | (NULL IN [\"Yi\",\"Tim\",\"Kobe\"]) | +----------------+--------------------------------+-------------------------------+ | true | false | false | +----------------+--------------------------------+-------------------------------+ (NOT) STARTS WITH nebula> RETURN 'apple' STARTS WITH 'app', 'apple' STARTS WITH 'a', 'apple' STARTS WITH toUpper('a') +-----------------------------+---------------------------+------------------------------------+ | (\"apple\" STARTS WITH \"app\") | (\"apple\" STARTS WITH \"a\") | (\"apple\" STARTS WITH toUpper(\"a\")) | +-----------------------------+---------------------------+------------------------------------+ | true | true | false | +-----------------------------+---------------------------+------------------------------------+ nebula> RETURN 'apple' STARTS WITH 'b','apple' NOT STARTS WITH 'app' +---------------------------+---------------------------------+ | (\"apple\" STARTS WITH \"b\") | (\"apple\" NOT STARTS WITH \"app\") | +---------------------------+---------------------------------+ | false | false | +---------------------------+---------------------------------+ (NOT) ENDS WITH nebula> RETURN 'apple' ENDS WITH 'app', 'apple' ENDS WITH 'e', 'apple' ENDS WITH 'E', 'apple' ENDS WITH 'b' +---------------------------+-------------------------+-------------------------+-------------------------+ | (\"apple\" ENDS WITH \"app\") | (\"apple\" ENDS WITH \"e\") | (\"apple\" ENDS WITH \"E\") | (\"apple\" ENDS WITH \"b\") | +---------------------------+-------------------------+-------------------------+-------------------------+ | false | true | false | false | +---------------------------+-------------------------+-------------------------+-------------------------+ Regular expressions Nebula Graph supports filtering by using regular expressions. The regular expression syntax is inherited from std::regex . You can match on regular expressions by using =~ 'regexp' . For example: nebula> RETURN \"384748.39\" =~ \"\\\\d+(\\\\.\\\\d{2})?\"; +----------------------------+ | (384748.39=~\\d+(\\.\\d{2})?) | +----------------------------+ | true | +----------------------------+ nebula> MATCH (v:player) WHERE v.name =~ 'Tony.*' RETURN v.name; +---------------+ | v.name | +---------------+ | \"Tony Parker\" | +---------------+ NOTE : Regular expressions CAN NOT work with nGQL-extensions (GO/FETCH clause will return syntax error). Use it in openCypher only (e.g., in MATCH-WHERE clause).","title":"Examples"},{"location":"3.ngql-guide/5.operators/8.list/","text":"List operators \u00b6 List operators are: concatenating lists: + checking if an element exists in a list: IN accessing an element(s) in a list using the subscript operator: [] Examples \u00b6 nebula> YIELD [1,2,3,4,5]+[6,7] AS myList +-----------------------+ | myList | +-----------------------+ | [1, 2, 3, 4, 5, 6, 7] | +-----------------------+ nebula> RETURN size([NULL, 1, 2]) +------------------+ | size([NULL,1,2]) | +------------------+ | 3 | +------------------+ nebula> RETURN NULL IN [NULL, 1] +--------------------+ | (NULL IN [NULL,1]) | +--------------------+ | true | +--------------------+ nebula> WITH [2, 3, 4, 5] AS numberlist \\ UNWIND numberlist AS number \\ WITH number \\ WHERE number IN [2, 3, 8] \\ RETURN number +--------+ | number | +--------+ | 2 | +--------+ | 3 | +--------+","title":"List"},{"location":"3.ngql-guide/5.operators/8.list/#list_operators","text":"List operators are: concatenating lists: + checking if an element exists in a list: IN accessing an element(s) in a list using the subscript operator: []","title":"List operators"},{"location":"3.ngql-guide/5.operators/8.list/#examples","text":"nebula> YIELD [1,2,3,4,5]+[6,7] AS myList +-----------------------+ | myList | +-----------------------+ | [1, 2, 3, 4, 5, 6, 7] | +-----------------------+ nebula> RETURN size([NULL, 1, 2]) +------------------+ | size([NULL,1,2]) | +------------------+ | 3 | +------------------+ nebula> RETURN NULL IN [NULL, 1] +--------------------+ | (NULL IN [NULL,1]) | +--------------------+ | true | +--------------------+ nebula> WITH [2, 3, 4, 5] AS numberlist \\ UNWIND numberlist AS number \\ WITH number \\ WHERE number IN [2, 3, 8] \\ RETURN number +--------+ | number | +--------+ | 2 | +--------+ | 3 | +--------+","title":"Examples"},{"location":"3.ngql-guide/5.operators/9.precedence/","text":"Operator precedence \u00b6 The following list shows the precedence of nGQL operators in descending order. Operators that are shown together on a line have the same precedence. - (negative number) !, NOT *, /, % -, + == , >=, >, <=, <, <>, != AND OR, XOR = (assignment) For operators that occur at the same precedence level within an expression, evaluation proceeds left to right, with the exception that assignments evaluate right to left. The precedence of operators determines the order of evaluation of terms in an expression. To override this order and group terms explicitly, use parentheses. Examples \u00b6 nebula> RETURN 2+3*5; +-----------+ | (2+(3*5)) | +-----------+ | 17 | +-----------+ nebula> RETURN (2+3)*5; +-----------+ | ((2+3)*5) | +-----------+ | 25 | +-----------+ OpenCypher compatibility \u00b6 In openCypher, comparisons can be chained arbitrarily, e.g., x < y <= z is equivalent to x < y AND y <= z in openCypher. But in nGQL, it is equivalent to (x < y) <= z , which is a boolean (x < y) compare again an integer (z). And the result is NULL.","title":"Precedence"},{"location":"3.ngql-guide/5.operators/9.precedence/#operator_precedence","text":"The following list shows the precedence of nGQL operators in descending order. Operators that are shown together on a line have the same precedence. - (negative number) !, NOT *, /, % -, + == , >=, >, <=, <, <>, != AND OR, XOR = (assignment) For operators that occur at the same precedence level within an expression, evaluation proceeds left to right, with the exception that assignments evaluate right to left. The precedence of operators determines the order of evaluation of terms in an expression. To override this order and group terms explicitly, use parentheses.","title":"Operator precedence"},{"location":"3.ngql-guide/5.operators/9.precedence/#examples","text":"nebula> RETURN 2+3*5; +-----------+ | (2+(3*5)) | +-----------+ | 17 | +-----------+ nebula> RETURN (2+3)*5; +-----------+ | ((2+3)*5) | +-----------+ | 25 | +-----------+","title":"Examples"},{"location":"3.ngql-guide/5.operators/9.precedence/#opencypher_compatibility","text":"In openCypher, comparisons can be chained arbitrarily, e.g., x < y <= z is equivalent to x < y AND y <= z in openCypher. But in nGQL, it is equivalent to (x < y) <= z , which is a boolean (x < y) compare again an integer (z). And the result is NULL.","title":"OpenCypher compatibility"},{"location":"3.ngql-guide/6.functions-and-expressions/1.math/","text":"Built-in math functions \u00b6 Nebula Graph supports the following built-in math functions: Function Description double abs(double x) Returns absolute value of the argument. double floor(double x) Returns the largest integer value smaller than or equal to the argument. (Rounds down) double ceil(double x) Returns the smallest integer greater than or equal to the argument. (Rounds up) double round(double x) Returns the integer value nearest to the argument. Returns a number farther away from 0 if the argument is in the middle. double sqrt(double x) Returns the square root of the argument. double cbrt(double x) Returns the cubic root of the argument. double hypot(double x, double y) Returns the hypotenuse of a right-angled triangle. double pow(double x, double y) Returns the result of x raised by the y th power. double exp(double x) Returns the value of e raised to the x power. double exp2(double x) Returns 2 raised to the argument. double log(double x) Returns natural logarithm of the argument. double log2(double x) Returns the base-2 logarithm of the argument. double log10(double x) Returns the base-10 logarithm of the argument. double sin(double x) Returns sine of the argument. double asin(double x) Returns inverse sine of the argument. double cos(double x) Returns cosine of the argument. double acos(double x) Returns inverse cosine of the argument. double tan(double x) Returns tangent of the argument. double atan(double x) Returns inverse tangent the argument. double rand() Returns a random floating point number in the range from 0 (inclusive) to 1 (exclusive); i.e.[0,1). int rand32(int min, int max) Returns a random 32-bit integer in [min, max). If you set only one argument, it is parsed as max and min is default to 0 . If you set no argument, the system returns a random signed 32-bit integer. int rand64(int min, int max) Returns a random 64-bit integer in [min, max). If you set only one argument, it is parsed as max and min is default to 0 . If you set no argument, the system returns a random signed 64-bit integer. collect() Puts all the collected values to a list. avg() Returns the average value of the argument. count() Returns the number of records. max() Returns the maximum value. min() Returns the minimum value. std() Returns the population standard deviation. sum() Returns the sum value. bit_and() Bitwise AND. bit_or() Bitwise OR. bit_xor() Bitwise exclusive OR (XOR). int size() Returns the number of elements in a list or a map. map(fun, iter) Returns a map object after applying the given function to each item of a given iterable. int range(int start, int end, int step) Returns a list of integers from start (inclusive) to end (inclusive) in the specified steps. step is optional and default to 1. int sign(double x) Returns the signum of the given number: 0 if the number is 0, -1 for any negative number, and 1 for any positive number. double e() Returns the base of the natural logarithm, e (2.718281828459045). double pi() Returns the mathematical constant pi (3.141592653589793). double radians() Converts degrees to radians. radians(180) returns 3.141592653589793. NOTE : If the argument is set to NULL , the output is undefined.","title":"Math"},{"location":"3.ngql-guide/6.functions-and-expressions/1.math/#built-in_math_functions","text":"Nebula Graph supports the following built-in math functions: Function Description double abs(double x) Returns absolute value of the argument. double floor(double x) Returns the largest integer value smaller than or equal to the argument. (Rounds down) double ceil(double x) Returns the smallest integer greater than or equal to the argument. (Rounds up) double round(double x) Returns the integer value nearest to the argument. Returns a number farther away from 0 if the argument is in the middle. double sqrt(double x) Returns the square root of the argument. double cbrt(double x) Returns the cubic root of the argument. double hypot(double x, double y) Returns the hypotenuse of a right-angled triangle. double pow(double x, double y) Returns the result of x raised by the y th power. double exp(double x) Returns the value of e raised to the x power. double exp2(double x) Returns 2 raised to the argument. double log(double x) Returns natural logarithm of the argument. double log2(double x) Returns the base-2 logarithm of the argument. double log10(double x) Returns the base-10 logarithm of the argument. double sin(double x) Returns sine of the argument. double asin(double x) Returns inverse sine of the argument. double cos(double x) Returns cosine of the argument. double acos(double x) Returns inverse cosine of the argument. double tan(double x) Returns tangent of the argument. double atan(double x) Returns inverse tangent the argument. double rand() Returns a random floating point number in the range from 0 (inclusive) to 1 (exclusive); i.e.[0,1). int rand32(int min, int max) Returns a random 32-bit integer in [min, max). If you set only one argument, it is parsed as max and min is default to 0 . If you set no argument, the system returns a random signed 32-bit integer. int rand64(int min, int max) Returns a random 64-bit integer in [min, max). If you set only one argument, it is parsed as max and min is default to 0 . If you set no argument, the system returns a random signed 64-bit integer. collect() Puts all the collected values to a list. avg() Returns the average value of the argument. count() Returns the number of records. max() Returns the maximum value. min() Returns the minimum value. std() Returns the population standard deviation. sum() Returns the sum value. bit_and() Bitwise AND. bit_or() Bitwise OR. bit_xor() Bitwise exclusive OR (XOR). int size() Returns the number of elements in a list or a map. map(fun, iter) Returns a map object after applying the given function to each item of a given iterable. int range(int start, int end, int step) Returns a list of integers from start (inclusive) to end (inclusive) in the specified steps. step is optional and default to 1. int sign(double x) Returns the signum of the given number: 0 if the number is 0, -1 for any negative number, and 1 for any positive number. double e() Returns the base of the natural logarithm, e (2.718281828459045). double pi() Returns the mathematical constant pi (3.141592653589793). double radians() Converts degrees to radians. radians(180) returns 3.141592653589793. NOTE : If the argument is set to NULL , the output is undefined.","title":"Built-in math functions"},{"location":"3.ngql-guide/6.functions-and-expressions/10.collect/","text":"collect() \u00b6 collect() returns a list containing the values returned by an expression. Using this function aggregates data by amalgamating multiple records or values into a single list. collect() is an aggregation function. Like GROUP BY in SQL. Examples \u00b6 This example works like GROUP BY . nebula> UNWIND [1, 2, 1] AS a RETURN a; +---+ | a | +---+ | 1 | +---+ | 2 | +---+ | 1 | +---+ nebula> UNWIND [1, 2, 1] AS a RETURN collect(a); +------------+ | COLLECT(a) | +------------+ | [1, 2, 1] | +------------+ nebula> UNWIND [1, 2, 1] AS a RETURN a, collect(a), size(collect(a)) +---+------------+------------------+ | a | COLLECT(a) | size(COLLECT(a)) | +---+------------+------------------+ | 2 | [2] | 1 | +---+------------+------------------+ | 1 | [1, 1] | 2 | +---+------------+------------------+ You can sort reversely, limit output rows to 3, and collect the output into a list. nebula> UNWIND [\"c\", \"b\", \"a\", \"d\" ] AS p \\ WITH p AS q \\ ORDER BY q DESC LIMIT 3 \\ RETURN collect(q); +-----------------+ | COLLECT(q) | +-----------------+ | [\"d\", \"c\", \"b\"] | +-----------------+ nebula> WITH [1, 1, 2, 2] AS coll \\ UNWIND coll AS x \\ WITH DISTINCT x \\ RETURN collect(x) AS ss +--------+ | ss | +--------+ | [1, 2] | +--------+ This example aggregates all players' names by their ages. ```ngql nebula> MATCH (n:player) RETURN collect(n.age); +---------------------------------------------------------------+ | COLLECT(n.age) | ----------------------------------------------------------------+ | [32, 32, 34, 29, 41, 40, 33, 25, 40, 37, ... ... nebula> MATCH (n:player) RETURN n.age AS age, collect(n.name); ... +-----+--------------------------------------------------------------------------+ | 27 | [\"Cory Joseph\"] | +-----+--------------------------------------------------------------------------+ | 28 | [\"Damian Lillard\", \"Paul George\", \"Ricky Rubio\"] | +-----+--------------------------------------------------------------------------+ | 29 | [\"Dejounte Murray\", \"James Harden\", \"Klay Thompson\", \"Jonathon Simmons\"] | +-----+--------------------------------------------------------------------------+ ...","title":"Collect"},{"location":"3.ngql-guide/6.functions-and-expressions/10.collect/#collect","text":"collect() returns a list containing the values returned by an expression. Using this function aggregates data by amalgamating multiple records or values into a single list. collect() is an aggregation function. Like GROUP BY in SQL.","title":"collect()"},{"location":"3.ngql-guide/6.functions-and-expressions/10.collect/#examples","text":"This example works like GROUP BY . nebula> UNWIND [1, 2, 1] AS a RETURN a; +---+ | a | +---+ | 1 | +---+ | 2 | +---+ | 1 | +---+ nebula> UNWIND [1, 2, 1] AS a RETURN collect(a); +------------+ | COLLECT(a) | +------------+ | [1, 2, 1] | +------------+ nebula> UNWIND [1, 2, 1] AS a RETURN a, collect(a), size(collect(a)) +---+------------+------------------+ | a | COLLECT(a) | size(COLLECT(a)) | +---+------------+------------------+ | 2 | [2] | 1 | +---+------------+------------------+ | 1 | [1, 1] | 2 | +---+------------+------------------+ You can sort reversely, limit output rows to 3, and collect the output into a list. nebula> UNWIND [\"c\", \"b\", \"a\", \"d\" ] AS p \\ WITH p AS q \\ ORDER BY q DESC LIMIT 3 \\ RETURN collect(q); +-----------------+ | COLLECT(q) | +-----------------+ | [\"d\", \"c\", \"b\"] | +-----------------+ nebula> WITH [1, 1, 2, 2] AS coll \\ UNWIND coll AS x \\ WITH DISTINCT x \\ RETURN collect(x) AS ss +--------+ | ss | +--------+ | [1, 2] | +--------+ This example aggregates all players' names by their ages. ```ngql nebula> MATCH (n:player) RETURN collect(n.age); +---------------------------------------------------------------+ | COLLECT(n.age) | ----------------------------------------------------------------+ | [32, 32, 34, 29, 41, 40, 33, 25, 40, 37, ... ... nebula> MATCH (n:player) RETURN n.age AS age, collect(n.name); ... +-----+--------------------------------------------------------------------------+ | 27 | [\"Cory Joseph\"] | +-----+--------------------------------------------------------------------------+ | 28 | [\"Damian Lillard\", \"Paul George\", \"Ricky Rubio\"] | +-----+--------------------------------------------------------------------------+ | 29 | [\"Dejounte Murray\", \"James Harden\", \"Klay Thompson\", \"Jonathon Simmons\"] | +-----+--------------------------------------------------------------------------+ ...","title":"Examples"},{"location":"3.ngql-guide/6.functions-and-expressions/11.reduce/","text":"reduce() function \u00b6 OpenCypher Compatibility \u00b6 In openCypher, the function reduce() is not defined. nGQL implements reduce() function as the Cypher way. Syntax \u00b6 reduce() returns the value resulting from the application of an expression on each successive element in a list in conjunction with the result of the computation thus far. This function will iterate through each element e in the given list, run the expression on e \u2009\u2014\u2009taking into account the current partial result\u2009\u2014\u2009and store the new partial result in the accumulator. This function is analogous to the fold or reduce method in functional languages such as Lisp and Scala. reduce(accumulator = initial, variable IN list | expression) Arguments: Name Description accumulator A variable that will hold the result and the partial results as the list is iterated. initial An expression that runs once to give a starting value to the accumulator. list An expression that returns a list. variable The closure will have a variable introduced in its context. We decide here which variable to use. expression This expression will run once per value in the list, and produce the result value. Returns: The type of the value returned depends on the arguments provided, along with the semantics of expression. Example \u00b6 nebula> RETURN reduce(totalNum = 10, n IN range(1, 3) | totalNum + n) AS r; +----+ | r | +----+ | 16 | +----+ nebula> RETURN reduce(totalNum = -4 * 5, n IN [1, 2] | totalNum + n * 2) AS r; +-----+ | r | +-----+ | -14 | +-----+ nebula> MATCH p = (n:player{name:\"LeBron James\"})<-[:follow]-(m) \\ RETURN nodes(p)[0].age AS src1, \\ nodes(p)[1].age AS dst2, \\ reduce(totalAge = 100, n IN nodes(p) | totalAge + n.age) AS sum +------+------+-----+ | src1 | dst2 | sum | +------+------+-----+ | 34 | 31 | 165 | +------+------+-----+ | 34 | 29 | 163 | +------+------+-----+ | 34 | 33 | 167 | +------+------+-----+ | 34 | 26 | 160 | +------+------+-----+ | 34 | 34 | 168 | +------+------+-----+ | 34 | 37 | 171 | +------+------+-----+ nebula> LOOKUP ON player WHERE player.name == \"Tony Parker\" | GO FROM $-.VertexID over follow WHERE follow.degree != reduce(totalNum = 5, n IN range(1, 3) | $$.player.age + totalNum + n) YIELD $$.player.name AS id, $$.player.age AS age, follow.degree AS degree +---------------------+-----+--------+ | id | age | degree | +---------------------+-----+--------+ | \"Tim Duncan\" | 42 | 95 | +---------------------+-----+--------+ | \"LaMarcus Aldridge\" | 33 | 90 | +---------------------+-----+--------+ | \"Manu Ginobili\" | 41 | 95 | +---------------------+-----+--------+","title":"Reduce"},{"location":"3.ngql-guide/6.functions-and-expressions/11.reduce/#reduce_function","text":"","title":"reduce() function"},{"location":"3.ngql-guide/6.functions-and-expressions/11.reduce/#opencypher_compatibility","text":"In openCypher, the function reduce() is not defined. nGQL implements reduce() function as the Cypher way.","title":"OpenCypher Compatibility"},{"location":"3.ngql-guide/6.functions-and-expressions/11.reduce/#syntax","text":"reduce() returns the value resulting from the application of an expression on each successive element in a list in conjunction with the result of the computation thus far. This function will iterate through each element e in the given list, run the expression on e \u2009\u2014\u2009taking into account the current partial result\u2009\u2014\u2009and store the new partial result in the accumulator. This function is analogous to the fold or reduce method in functional languages such as Lisp and Scala. reduce(accumulator = initial, variable IN list | expression) Arguments: Name Description accumulator A variable that will hold the result and the partial results as the list is iterated. initial An expression that runs once to give a starting value to the accumulator. list An expression that returns a list. variable The closure will have a variable introduced in its context. We decide here which variable to use. expression This expression will run once per value in the list, and produce the result value. Returns: The type of the value returned depends on the arguments provided, along with the semantics of expression.","title":"Syntax"},{"location":"3.ngql-guide/6.functions-and-expressions/11.reduce/#example","text":"nebula> RETURN reduce(totalNum = 10, n IN range(1, 3) | totalNum + n) AS r; +----+ | r | +----+ | 16 | +----+ nebula> RETURN reduce(totalNum = -4 * 5, n IN [1, 2] | totalNum + n * 2) AS r; +-----+ | r | +-----+ | -14 | +-----+ nebula> MATCH p = (n:player{name:\"LeBron James\"})<-[:follow]-(m) \\ RETURN nodes(p)[0].age AS src1, \\ nodes(p)[1].age AS dst2, \\ reduce(totalAge = 100, n IN nodes(p) | totalAge + n.age) AS sum +------+------+-----+ | src1 | dst2 | sum | +------+------+-----+ | 34 | 31 | 165 | +------+------+-----+ | 34 | 29 | 163 | +------+------+-----+ | 34 | 33 | 167 | +------+------+-----+ | 34 | 26 | 160 | +------+------+-----+ | 34 | 34 | 168 | +------+------+-----+ | 34 | 37 | 171 | +------+------+-----+ nebula> LOOKUP ON player WHERE player.name == \"Tony Parker\" | GO FROM $-.VertexID over follow WHERE follow.degree != reduce(totalNum = 5, n IN range(1, 3) | $$.player.age + totalNum + n) YIELD $$.player.name AS id, $$.player.age AS age, follow.degree AS degree +---------------------+-----+--------+ | id | age | degree | +---------------------+-----+--------+ | \"Tim Duncan\" | 42 | 95 | +---------------------+-----+--------+ | \"LaMarcus Aldridge\" | 33 | 90 | +---------------------+-----+--------+ | \"Manu Ginobili\" | 41 | 95 | +---------------------+-----+--------+","title":"Example"},{"location":"3.ngql-guide/6.functions-and-expressions/2.string/","text":"Built-in string functions \u00b6 Nebula Graph supports the following built-in string functions: Function Description int strcasecmp(string a, string b) Compares strings without case sensitivity, when a = b, Returns 0, when a > b Returnsed value is greater than 0, otherwise less than 0. string lower(string a) Returns the argument in lowercase. string toLower(string a) The same as lower(). string upper(string a) Returns the argument in uppercase. string toUpper(string a) The same as upper(). int length(string a) Returns the length of given string in bytes. string trim(string a) Removes leading and trailing spaces. string ltrim(string a) Removes leading spaces. string rtrim(string a) Removes trailing spaces. string left(string a, int count) Returns the substring in [1, count], if length a is less than count, Returns a. string right(string a, int count) Returns the substring in [size - count + 1, size], if length a is less than count, Returns a. string lpad(string a, int size, string letters) Left-pads a string with another string to a certain length. string rpad(string a, int size, string letters) Reft-pads a string with another string to a certain length. string substr(string a, int pos, int count) Returns a substring from a string, starting at the specified position pos , extract count characters. string substring(string a, int pos, int count) The same as substr(). string reverse(string) Returns the reverse of a string. string replace(string a, string b, string c) Replaces string b in string a with string c. list split(string a, string b) Splits string a at string b and returns a list of strings. string toString() Takes in any data type and converts it into a string. int hash() Takes in any data type and encodes it into an integer value. NOTE : If the argument is NULL , the return is undefined. Explanations for the return of substr() and substring() \u00b6 pos uses a 0-based index. If pos is 0, the whole string a is returned. If pos is greater than the maximum string index, an empty string is returned. If pos is a negative number, BAD_DATA is returned. If count is omitted, the function returns the substring starting at the position given by pos and extending to the end of string a . Using NULL as any of the argument of substr() causes an issue . If count is 0, an empty string is returned. OpenCypher Compatibility: In openCypher, if a is null , null is returned. In openCypher, if pos is 0, the returned substring starts from the first character, and extend to count characters. In openCypher, if either pos or count is null or a negative integer, an error is raised.","title":"String"},{"location":"3.ngql-guide/6.functions-and-expressions/2.string/#built-in_string_functions","text":"Nebula Graph supports the following built-in string functions: Function Description int strcasecmp(string a, string b) Compares strings without case sensitivity, when a = b, Returns 0, when a > b Returnsed value is greater than 0, otherwise less than 0. string lower(string a) Returns the argument in lowercase. string toLower(string a) The same as lower(). string upper(string a) Returns the argument in uppercase. string toUpper(string a) The same as upper(). int length(string a) Returns the length of given string in bytes. string trim(string a) Removes leading and trailing spaces. string ltrim(string a) Removes leading spaces. string rtrim(string a) Removes trailing spaces. string left(string a, int count) Returns the substring in [1, count], if length a is less than count, Returns a. string right(string a, int count) Returns the substring in [size - count + 1, size], if length a is less than count, Returns a. string lpad(string a, int size, string letters) Left-pads a string with another string to a certain length. string rpad(string a, int size, string letters) Reft-pads a string with another string to a certain length. string substr(string a, int pos, int count) Returns a substring from a string, starting at the specified position pos , extract count characters. string substring(string a, int pos, int count) The same as substr(). string reverse(string) Returns the reverse of a string. string replace(string a, string b, string c) Replaces string b in string a with string c. list split(string a, string b) Splits string a at string b and returns a list of strings. string toString() Takes in any data type and converts it into a string. int hash() Takes in any data type and encodes it into an integer value. NOTE : If the argument is NULL , the return is undefined.","title":"Built-in string functions"},{"location":"3.ngql-guide/6.functions-and-expressions/2.string/#explanations_for_the_return_of_substr_and_substring","text":"pos uses a 0-based index. If pos is 0, the whole string a is returned. If pos is greater than the maximum string index, an empty string is returned. If pos is a negative number, BAD_DATA is returned. If count is omitted, the function returns the substring starting at the position given by pos and extending to the end of string a . Using NULL as any of the argument of substr() causes an issue . If count is 0, an empty string is returned. OpenCypher Compatibility: In openCypher, if a is null , null is returned. In openCypher, if pos is 0, the returned substring starts from the first character, and extend to count characters. In openCypher, if either pos or count is null or a negative integer, an error is raised.","title":"Explanations for the return of substr() and substring()"},{"location":"3.ngql-guide/6.functions-and-expressions/3.date-and-time/","text":"Built-in date and time functions \u00b6 Nebula Graph supports the following built-in date and time functions: Function Description int now() Return the current date and time of the system time zone. date date() Return the current UTC date based on the current system. time time() Return the current UTC calendar time of the current time zone. datetime datetime() Return the current UTC datetime based on the current time. The date(), time(), and datetime() functions accept three kind of parameters, namely empty, string, and map. Examples \u00b6 > RETURN now(), date(), time(), datetime(); +------------+------------+--------------+-------------------------+ | now() | date() | time() | datetime() | +------------+------------+--------------+-------------------------+ | 1611907165 | 2021-01-29 | 07:59:22.000 | 2021-01-29T07:59:22.000 | +------------+------------+--------------+-------------------------+ OpenCypher compatibility \u00b6 Time in openCypher is measured in milliseconds. Time in nGQL is measured in seconds. The milliseconds are displayed in 000 .","title":"Date and time"},{"location":"3.ngql-guide/6.functions-and-expressions/3.date-and-time/#built-in_date_and_time_functions","text":"Nebula Graph supports the following built-in date and time functions: Function Description int now() Return the current date and time of the system time zone. date date() Return the current UTC date based on the current system. time time() Return the current UTC calendar time of the current time zone. datetime datetime() Return the current UTC datetime based on the current time. The date(), time(), and datetime() functions accept three kind of parameters, namely empty, string, and map.","title":"Built-in date and time functions"},{"location":"3.ngql-guide/6.functions-and-expressions/3.date-and-time/#examples","text":"> RETURN now(), date(), time(), datetime(); +------------+------------+--------------+-------------------------+ | now() | date() | time() | datetime() | +------------+------------+--------------+-------------------------+ | 1611907165 | 2021-01-29 | 07:59:22.000 | 2021-01-29T07:59:22.000 | +------------+------------+--------------+-------------------------+","title":"Examples"},{"location":"3.ngql-guide/6.functions-and-expressions/3.date-and-time/#opencypher_compatibility","text":"Time in openCypher is measured in milliseconds. Time in nGQL is measured in seconds. The milliseconds are displayed in 000 .","title":"OpenCypher compatibility"},{"location":"3.ngql-guide/6.functions-and-expressions/4.schema/","text":"Schema functions \u00b6 Nebula Graph supports the following built-in schema functions: Function Description id(vertex) Returns the id of a vertex. The data type of the result is the same as the vertex ID. list tags(vertex) Returns the tags of a vertex. list labels(vertex) Returns the tags of a vertex. map properties(vertex_or_edge) Takes in a vertex or an edge and returns its properties. string type(edge) Returns the edge type of an edge. vertex startNode(path) Takes in an edge or a path and returns its source vertex ID. string endNode(path) Takes in an edge or a path and returns its destination vertex ID. int rank(edge) Returns the rank value of an edge. Examples \u00b6 nebula> MATCH (a:player) WHERE id(a) == \"player100\" RETURN tags(a), labels(a), properties(a) +------------+------------+-------------------------------+ | tags(a) | labels(a) | properties(a) | +------------+------------+-------------------------------+ | [\"player\"] | [\"player\"] | {age: 42, name: \"Tim Duncan\"} | +------------+------------+-------------------------------+ nebula> MATCH p = (a :player {name : \"Tim Duncan\"})-[r:serve]-(t) RETURN type(r), rank(r) +---------+---------+ | type(r) | rank(r) | +---------+---------+ | \"serve\" | 0 | +---------+---------+ nebula> MATCH p = (a :player {name : \"Tim Duncan\"})-[r:serve]-(t) RETURN startNode(p), endNode(p) +----------------------------------------------------+----------------------------------+ | startNode(p) | endNode(p) | +----------------------------------------------------+----------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | (\"team204\" :team{name: \"Spurs\"}) | +----------------------------------------------------+----------------------------------+","title":"Schema"},{"location":"3.ngql-guide/6.functions-and-expressions/4.schema/#schema_functions","text":"Nebula Graph supports the following built-in schema functions: Function Description id(vertex) Returns the id of a vertex. The data type of the result is the same as the vertex ID. list tags(vertex) Returns the tags of a vertex. list labels(vertex) Returns the tags of a vertex. map properties(vertex_or_edge) Takes in a vertex or an edge and returns its properties. string type(edge) Returns the edge type of an edge. vertex startNode(path) Takes in an edge or a path and returns its source vertex ID. string endNode(path) Takes in an edge or a path and returns its destination vertex ID. int rank(edge) Returns the rank value of an edge.","title":"Schema functions"},{"location":"3.ngql-guide/6.functions-and-expressions/4.schema/#examples","text":"nebula> MATCH (a:player) WHERE id(a) == \"player100\" RETURN tags(a), labels(a), properties(a) +------------+------------+-------------------------------+ | tags(a) | labels(a) | properties(a) | +------------+------------+-------------------------------+ | [\"player\"] | [\"player\"] | {age: 42, name: \"Tim Duncan\"} | +------------+------------+-------------------------------+ nebula> MATCH p = (a :player {name : \"Tim Duncan\"})-[r:serve]-(t) RETURN type(r), rank(r) +---------+---------+ | type(r) | rank(r) | +---------+---------+ | \"serve\" | 0 | +---------+---------+ nebula> MATCH p = (a :player {name : \"Tim Duncan\"})-[r:serve]-(t) RETURN startNode(p), endNode(p) +----------------------------------------------------+----------------------------------+ | startNode(p) | endNode(p) | +----------------------------------------------------+----------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | (\"team204\" :team{name: \"Spurs\"}) | +----------------------------------------------------+----------------------------------+","title":"Examples"},{"location":"3.ngql-guide/6.functions-and-expressions/5.case-expressions/","text":"CASE expressions \u00b6 The CASE expression uses conditions to filter the result of an nGQL query statement. It is usually used in the YIELD or RETURN clause. nGQL provides two forms of CASE expressions just like openCypher: the simple form and the generic form. The CASE expression goes through conditions and returns a result when the first condition is met. Then the CASE expression stops reading the conditions and returns the result. If no conditions are met, it returns the result in the ELSE clause. If there is no ELSE clause and no conditions are met, it returns NULL . The following graph is used for the examples in this topic. The simple form of CASE expressions \u00b6 Syntax \u00b6 CASE <comparer> WHEN <value> THEN <result> [WHEN ...] [ELSE <default>] END CAUTION: Always remember to end a CASE expression with END . Parameters Description comparer A value or a valid expression that outputs a value. This value is used to compare with value . value It will be compared with comparer . If they match, then this condition is met. result It is returned by the CASE expression if value matches comparer . default It is returned by the CASE expression if no conditions are met. Examples \u00b6 nebula> RETURN \\ CASE 2+3 \\ WHEN 4 THEN 0 \\ WHEN 5 THEN 1 \\ ELSE -1 \\ END \\ AS result; +--------+ | result | +--------+ | 1 | +--------+ nebula> GO FROM \"player100\" OVER follow \\ YIELD $$.player.name AS Name, \\ CASE $$.player.age > 35 \\ WHEN true THEN \"Yes\" \\ WHEN false THEN \"No\" \\ ELSE \"Nah\" \\ END \\ AS Age_above_35; +---------------------+--------------+ | Name | Age_above_35 | +---------------------+--------------+ | \"Tony Parker\" | \"Yes\" | +---------------------+--------------+ | \"LaMarcus Aldridge\" | \"No\" | +---------------------+--------------+ The generic form of CASE expressions \u00b6 Syntax \u00b6 CASE WHEN <condition> THEN <result> [WHEN ...] [ELSE <default>] END Parameters Description condition If condition is evaluated as true, result is returned by the CASE expression. result It is returned by the CASE expression if condition is evaluated as true. default It is returned by the CASE expression if no conditions are met. Examples \u00b6 nebula> YIELD \\ CASE WHEN 4 > 5 THEN 0 \\ WHEN 3+4==7 THEN 1 \\ ELSE 2 \\ END \\ AS result; +--------+ | result | +--------+ | 1 | +--------+ nebula> MATCH (v:player) WHERE v.age > 30 \\ RETURN v.name AS Name, \\ CASE \\ WHEN v.name STARTS WITH \"T\" THEN \"Yes\" \\ ELSE \"No\" \\ END \\ AS Starts_with_T; +---------------------+---------------+ | Name | Starts_with_T | +---------------------+---------------+ | \"Tim\" | \"Yes\" | +---------------------+---------------+ | \"LaMarcus Aldridge\" | \"No\" | +---------------------+---------------+ | \"Tony Parker\" | \"Yes\" | +---------------------+---------------+ Differences between the simple form and the generic form \u00b6 To avoid the misuse of the simple form and the generic form, it is important to understand their differences. The following example can help explain them. nebula> GO FROM \"player100\" OVER follow \\ YIELD $$.player.name AS Name, $$.player.age AS Age, \\ CASE $$.player.age \\ WHEN $$.player.age > 35 THEN \"Yes\" \\ ELSE \"No\" \\ END \\ AS Age_above_35; +---------------------+-----+--------------+ | Name | Age | Age_above_35 | +---------------------+-----+--------------+ | \"Tony Parker\" | 36 | \"No\" | +---------------------+-----+--------------+ | \"LaMarcus Aldridge\" | 33 | \"No\" | +---------------------+-----+--------------+ The preceding GO query is intended to output \"Yes\" when the player age is above 35. However, in this example, when the player age is 36, the actual output is not as expected: It is \"No\" instead of \"Yes\". This is because the query uses the CASE expression in the simple form, and a comparison between the values of $$.player.age and $$.player.age > 35 is made. When the player age is 36: The value of $$.player.age is 36 . It is an integer. $$.player.age > 35 is evaluated to true . It is a boolean. The values of $$.player.age and $$.player.age > 35 do not match. This condition is not met and \"No\" is returned.","title":"Case expressions"},{"location":"3.ngql-guide/6.functions-and-expressions/5.case-expressions/#case_expressions","text":"The CASE expression uses conditions to filter the result of an nGQL query statement. It is usually used in the YIELD or RETURN clause. nGQL provides two forms of CASE expressions just like openCypher: the simple form and the generic form. The CASE expression goes through conditions and returns a result when the first condition is met. Then the CASE expression stops reading the conditions and returns the result. If no conditions are met, it returns the result in the ELSE clause. If there is no ELSE clause and no conditions are met, it returns NULL . The following graph is used for the examples in this topic.","title":"CASE expressions"},{"location":"3.ngql-guide/6.functions-and-expressions/5.case-expressions/#the_simple_form_of_case_expressions","text":"","title":"The simple form of CASE expressions"},{"location":"3.ngql-guide/6.functions-and-expressions/5.case-expressions/#syntax","text":"CASE <comparer> WHEN <value> THEN <result> [WHEN ...] [ELSE <default>] END CAUTION: Always remember to end a CASE expression with END . Parameters Description comparer A value or a valid expression that outputs a value. This value is used to compare with value . value It will be compared with comparer . If they match, then this condition is met. result It is returned by the CASE expression if value matches comparer . default It is returned by the CASE expression if no conditions are met.","title":"Syntax"},{"location":"3.ngql-guide/6.functions-and-expressions/5.case-expressions/#examples","text":"nebula> RETURN \\ CASE 2+3 \\ WHEN 4 THEN 0 \\ WHEN 5 THEN 1 \\ ELSE -1 \\ END \\ AS result; +--------+ | result | +--------+ | 1 | +--------+ nebula> GO FROM \"player100\" OVER follow \\ YIELD $$.player.name AS Name, \\ CASE $$.player.age > 35 \\ WHEN true THEN \"Yes\" \\ WHEN false THEN \"No\" \\ ELSE \"Nah\" \\ END \\ AS Age_above_35; +---------------------+--------------+ | Name | Age_above_35 | +---------------------+--------------+ | \"Tony Parker\" | \"Yes\" | +---------------------+--------------+ | \"LaMarcus Aldridge\" | \"No\" | +---------------------+--------------+","title":"Examples"},{"location":"3.ngql-guide/6.functions-and-expressions/5.case-expressions/#the_generic_form_of_case_expressions","text":"","title":"The generic form of CASE expressions"},{"location":"3.ngql-guide/6.functions-and-expressions/5.case-expressions/#syntax_1","text":"CASE WHEN <condition> THEN <result> [WHEN ...] [ELSE <default>] END Parameters Description condition If condition is evaluated as true, result is returned by the CASE expression. result It is returned by the CASE expression if condition is evaluated as true. default It is returned by the CASE expression if no conditions are met.","title":"Syntax"},{"location":"3.ngql-guide/6.functions-and-expressions/5.case-expressions/#examples_1","text":"nebula> YIELD \\ CASE WHEN 4 > 5 THEN 0 \\ WHEN 3+4==7 THEN 1 \\ ELSE 2 \\ END \\ AS result; +--------+ | result | +--------+ | 1 | +--------+ nebula> MATCH (v:player) WHERE v.age > 30 \\ RETURN v.name AS Name, \\ CASE \\ WHEN v.name STARTS WITH \"T\" THEN \"Yes\" \\ ELSE \"No\" \\ END \\ AS Starts_with_T; +---------------------+---------------+ | Name | Starts_with_T | +---------------------+---------------+ | \"Tim\" | \"Yes\" | +---------------------+---------------+ | \"LaMarcus Aldridge\" | \"No\" | +---------------------+---------------+ | \"Tony Parker\" | \"Yes\" | +---------------------+---------------+","title":"Examples"},{"location":"3.ngql-guide/6.functions-and-expressions/5.case-expressions/#differences_between_the_simple_form_and_the_generic_form","text":"To avoid the misuse of the simple form and the generic form, it is important to understand their differences. The following example can help explain them. nebula> GO FROM \"player100\" OVER follow \\ YIELD $$.player.name AS Name, $$.player.age AS Age, \\ CASE $$.player.age \\ WHEN $$.player.age > 35 THEN \"Yes\" \\ ELSE \"No\" \\ END \\ AS Age_above_35; +---------------------+-----+--------------+ | Name | Age | Age_above_35 | +---------------------+-----+--------------+ | \"Tony Parker\" | 36 | \"No\" | +---------------------+-----+--------------+ | \"LaMarcus Aldridge\" | 33 | \"No\" | +---------------------+-----+--------------+ The preceding GO query is intended to output \"Yes\" when the player age is above 35. However, in this example, when the player age is 36, the actual output is not as expected: It is \"No\" instead of \"Yes\". This is because the query uses the CASE expression in the simple form, and a comparison between the values of $$.player.age and $$.player.age > 35 is made. When the player age is 36: The value of $$.player.age is 36 . It is an integer. $$.player.age > 35 is evaluated to true . It is a boolean. The values of $$.player.age and $$.player.age > 35 do not match. This condition is not met and \"No\" is returned.","title":"Differences between the simple form and the generic form"},{"location":"3.ngql-guide/6.functions-and-expressions/6.list/","text":"List functions \u00b6 Function Description keys(expr) Returns a list containing the string representations for all the property names of a vertex, edge, or map. labels(vertex) Returns the tags of a vertex. nodes(path) Returns a list containing all the nodes in a path. range(start, end [, step]) A list of Integer elements. relationships(path) Returns a list containing all the relationships in a path. reverse(list) returns a list in which the order of all elements in the original list have been reversed. tail(list) returns all the elements, excluding the first one. head(list) Returns the first element of a list. last(list) Returns the last element of a list. coalesce(list) Returns the first not null value in a list. reduce() See reduce() function . NOTE : If the parameter is NULL , the output is undefined. Examples \u00b6 nebula> WITH [NULL, 4923, 'abc', 521, 487] AS ids RETURN reverse(ids), tail(ids), head(ids), last(ids), coalesce(ids) +-----------------------------------+-------------------------+-----------+-----------+---------------+ | reverse(ids) | tail(ids) | head(ids) | last(ids) | coalesce(ids) | +-----------------------------------+-------------------------+-----------+-----------+---------------+ | [487, 521, \"abc\", 4923, __NULL__] | [4923, \"abc\", 521, 487] | __NULL__ | 487 | 4923 | +-----------------------------------+-------------------------+-----------+-----------+---------------+ nebula> MATCH (a:player)-[r]->() WHERE id(a) == \"player100\" RETURN labels(a), keys(r) +------------+----------------------------+ | labels(a) | keys(r) | +------------+----------------------------+ | [\"player\"] | [\"degree\"] | +------------+----------------------------+ | [\"player\"] | [\"degree\"] | +------------+----------------------------+ | [\"player\"] | [\"end_year\", \"start_year\"] | +------------+----------------------------+ nebula> MATCH p = (a:player)-[]->(b)-[]->(c:team) WHERE a.name == \"Tim Duncan\" AND c.name == \"Spurs\" RETURN nodes(p) +-----------------------------------------------------------------------------------------------------------------------------------------------+ | nodes(p) | +-----------------------------------------------------------------------------------------------------------------------------------------------+ | [(\"player100\" :player{age: 42, name: \"Tim Duncan\"}), (\"player101\" :player{age: 36, name: \"Tony Parker\"}), (\"team204\" :team{name: \"Spurs\"})] | +-----------------------------------------------------------------------------------------------------------------------------------------------+ | [(\"player100\" :player{age: 42, name: \"Tim Duncan\"}), (\"player125\" :player{age: 41, name: \"Manu Ginobili\"}), (\"team204\" :team{name: \"Spurs\"})] | +-----------------------------------------------------------------------------------------------------------------------------------------------+ nebula> MATCH p = (a:player)-[]->(b)-[]->(c:team) WHERE a.name == \"Tim Duncan\" AND c.name == \"Spurs\" RETURN relationships(p) +-----------------------------------------------------------------------------------------------------------------------------+ | relationships(p) | +-----------------------------------------------------------------------------------------------------------------------------+ | [[:follow \"player100\"->\"player101\" @0 {degree: 95}], [:serve \"player101\"->\"team204\" @0 {end_year: 2018, start_year: 1999}]] | +-----------------------------------------------------------------------------------------------------------------------------+ | [[:follow \"player100\"->\"player125\" @0 {degree: 95}], [:serve \"player125\"->\"team204\" @0 {end_year: 2018, start_year: 2002}]] | +-----------------------------------------------------------------------------------------------------------------------------+","title":"List"},{"location":"3.ngql-guide/6.functions-and-expressions/6.list/#list_functions","text":"Function Description keys(expr) Returns a list containing the string representations for all the property names of a vertex, edge, or map. labels(vertex) Returns the tags of a vertex. nodes(path) Returns a list containing all the nodes in a path. range(start, end [, step]) A list of Integer elements. relationships(path) Returns a list containing all the relationships in a path. reverse(list) returns a list in which the order of all elements in the original list have been reversed. tail(list) returns all the elements, excluding the first one. head(list) Returns the first element of a list. last(list) Returns the last element of a list. coalesce(list) Returns the first not null value in a list. reduce() See reduce() function . NOTE : If the parameter is NULL , the output is undefined.","title":"List functions"},{"location":"3.ngql-guide/6.functions-and-expressions/6.list/#examples","text":"nebula> WITH [NULL, 4923, 'abc', 521, 487] AS ids RETURN reverse(ids), tail(ids), head(ids), last(ids), coalesce(ids) +-----------------------------------+-------------------------+-----------+-----------+---------------+ | reverse(ids) | tail(ids) | head(ids) | last(ids) | coalesce(ids) | +-----------------------------------+-------------------------+-----------+-----------+---------------+ | [487, 521, \"abc\", 4923, __NULL__] | [4923, \"abc\", 521, 487] | __NULL__ | 487 | 4923 | +-----------------------------------+-------------------------+-----------+-----------+---------------+ nebula> MATCH (a:player)-[r]->() WHERE id(a) == \"player100\" RETURN labels(a), keys(r) +------------+----------------------------+ | labels(a) | keys(r) | +------------+----------------------------+ | [\"player\"] | [\"degree\"] | +------------+----------------------------+ | [\"player\"] | [\"degree\"] | +------------+----------------------------+ | [\"player\"] | [\"end_year\", \"start_year\"] | +------------+----------------------------+ nebula> MATCH p = (a:player)-[]->(b)-[]->(c:team) WHERE a.name == \"Tim Duncan\" AND c.name == \"Spurs\" RETURN nodes(p) +-----------------------------------------------------------------------------------------------------------------------------------------------+ | nodes(p) | +-----------------------------------------------------------------------------------------------------------------------------------------------+ | [(\"player100\" :player{age: 42, name: \"Tim Duncan\"}), (\"player101\" :player{age: 36, name: \"Tony Parker\"}), (\"team204\" :team{name: \"Spurs\"})] | +-----------------------------------------------------------------------------------------------------------------------------------------------+ | [(\"player100\" :player{age: 42, name: \"Tim Duncan\"}), (\"player125\" :player{age: 41, name: \"Manu Ginobili\"}), (\"team204\" :team{name: \"Spurs\"})] | +-----------------------------------------------------------------------------------------------------------------------------------------------+ nebula> MATCH p = (a:player)-[]->(b)-[]->(c:team) WHERE a.name == \"Tim Duncan\" AND c.name == \"Spurs\" RETURN relationships(p) +-----------------------------------------------------------------------------------------------------------------------------+ | relationships(p) | +-----------------------------------------------------------------------------------------------------------------------------+ | [[:follow \"player100\"->\"player101\" @0 {degree: 95}], [:serve \"player101\"->\"team204\" @0 {end_year: 2018, start_year: 1999}]] | +-----------------------------------------------------------------------------------------------------------------------------+ | [[:follow \"player100\"->\"player125\" @0 {degree: 95}], [:serve \"player125\"->\"team204\" @0 {end_year: 2018, start_year: 2002}]] | +-----------------------------------------------------------------------------------------------------------------------------+","title":"Examples"},{"location":"3.ngql-guide/6.functions-and-expressions/7.count/","text":"The count() function \u00b6 The count() function calculates the number of the specified values or rows. (nGQL-extension) You can use count() and GROUP BY together to group and count the number of specific values. Use YIELD to return. (OpenCypher style) You can use count() and RETURN . GROUP BY is not necessary. Syntax \u00b6 count({expr | *}) count(*) returns the number of rows (including NULL). count(expr) return non-NULL values return by an expression. count() and size() are different. Examples \u00b6 nebula> WITH [NULL, 1, 1, 2, 2] As a UNWIND a AS b RETURN count(b), count(*), count(DISTINCT b) +----------+----------+-------------------+ | COUNT(b) | COUNT(*) | COUNT(distinct b) | +----------+----------+-------------------+ | 4 | 5 | 2 | +----------+----------+-------------------+ nebula> GO FROM \"player101\" OVER follow BIDIRECT YIELD $$.player.name AS Name | \\ GROUP BY $-.Name YIELD $-.Name, count(*); +---------------------+----------+ | $-.Name | COUNT(*) | +---------------------+----------+ | \"Dejounte Murray\" | 1 | +---------------------+----------+ | \"LaMarcus Aldridge\" | 2 | +---------------------+----------+ | \"Tim Duncan\" | 2 | +---------------------+----------+ | \"Marco Belinelli\" | 1 | +---------------------+----------+ | \"Manu Ginobili\" | 1 | +---------------------+----------+ | \"Boris Diaw\" | 1 | +---------------------+----------+ The statement in the preceding example searches for: People whom player101 follows. People who follow player101 . And retrieves two columns: $-.Name , the names of the people. COUNT(*) , how many times the names show up. Because there are no duplicate names in the nba dataset, the number 2 in the result shows that the person in that row and player101 have followed each other. nebula> LOOKUP ON player YIELD player.age As playerage \\| GROUP BY $-.playerage YIELD $-.playerage as age, count(*) AS number | ORDER BY number DESC, age DESC +-----+--------+ | age | number | +-----+--------+ | 34 | 4 | +-----+--------+ | 33 | 4 | +-----+--------+ | 30 | 4 | +-----+--------+ | 29 | 4 | +-----+--------+ | 38 | 3 | +-----+--------+ ... nebula> MATCH (n:player) RETURN n.age as age, count(*) as number ORDER BY number DESC, age DESC +-----+--------+ | age | number | +-----+--------+ | 34 | 4 | +-----+--------+ | 33 | 4 | +-----+--------+ | 30 | 4 | +-----+--------+ | 29 | 4 | +-----+--------+ | 38 | 3 | +-----+--------+ The two statements in the preceding examples retrieves the age distribution of the players in the dataset. nebula> MATCH (v:player{name:\"Tim Duncan\"}) -- (v2) RETURN count(DISTINCT v2) +--------------------+ | COUNT(distinct v2) | +--------------------+ | 11 | +--------------------+ nebula> MATCH (n:player {name : \"Tim Duncan\"})-[]->(friend:player)-[]->(fof:player) RETURN count(fof), count(DISTINCT fof) +------------+---------------------+ | COUNT(fof) | COUNT(distinct fof) | +------------+---------------------+ | 4 | 3 | +------------+---------------------+ count(NULL) \u00b6 nebula> RETURN count(NULL), size(NULL) +-------------+------------+ | COUNT(NULL) | size(NULL) | +-------------+------------+ | 0 | __NULL__ | +-------------+------------+","title":"count()"},{"location":"3.ngql-guide/6.functions-and-expressions/7.count/#the_count_function","text":"The count() function calculates the number of the specified values or rows. (nGQL-extension) You can use count() and GROUP BY together to group and count the number of specific values. Use YIELD to return. (OpenCypher style) You can use count() and RETURN . GROUP BY is not necessary.","title":"The count() function"},{"location":"3.ngql-guide/6.functions-and-expressions/7.count/#syntax","text":"count({expr | *}) count(*) returns the number of rows (including NULL). count(expr) return non-NULL values return by an expression. count() and size() are different.","title":"Syntax"},{"location":"3.ngql-guide/6.functions-and-expressions/7.count/#examples","text":"nebula> WITH [NULL, 1, 1, 2, 2] As a UNWIND a AS b RETURN count(b), count(*), count(DISTINCT b) +----------+----------+-------------------+ | COUNT(b) | COUNT(*) | COUNT(distinct b) | +----------+----------+-------------------+ | 4 | 5 | 2 | +----------+----------+-------------------+ nebula> GO FROM \"player101\" OVER follow BIDIRECT YIELD $$.player.name AS Name | \\ GROUP BY $-.Name YIELD $-.Name, count(*); +---------------------+----------+ | $-.Name | COUNT(*) | +---------------------+----------+ | \"Dejounte Murray\" | 1 | +---------------------+----------+ | \"LaMarcus Aldridge\" | 2 | +---------------------+----------+ | \"Tim Duncan\" | 2 | +---------------------+----------+ | \"Marco Belinelli\" | 1 | +---------------------+----------+ | \"Manu Ginobili\" | 1 | +---------------------+----------+ | \"Boris Diaw\" | 1 | +---------------------+----------+ The statement in the preceding example searches for: People whom player101 follows. People who follow player101 . And retrieves two columns: $-.Name , the names of the people. COUNT(*) , how many times the names show up. Because there are no duplicate names in the nba dataset, the number 2 in the result shows that the person in that row and player101 have followed each other. nebula> LOOKUP ON player YIELD player.age As playerage \\| GROUP BY $-.playerage YIELD $-.playerage as age, count(*) AS number | ORDER BY number DESC, age DESC +-----+--------+ | age | number | +-----+--------+ | 34 | 4 | +-----+--------+ | 33 | 4 | +-----+--------+ | 30 | 4 | +-----+--------+ | 29 | 4 | +-----+--------+ | 38 | 3 | +-----+--------+ ... nebula> MATCH (n:player) RETURN n.age as age, count(*) as number ORDER BY number DESC, age DESC +-----+--------+ | age | number | +-----+--------+ | 34 | 4 | +-----+--------+ | 33 | 4 | +-----+--------+ | 30 | 4 | +-----+--------+ | 29 | 4 | +-----+--------+ | 38 | 3 | +-----+--------+ The two statements in the preceding examples retrieves the age distribution of the players in the dataset. nebula> MATCH (v:player{name:\"Tim Duncan\"}) -- (v2) RETURN count(DISTINCT v2) +--------------------+ | COUNT(distinct v2) | +--------------------+ | 11 | +--------------------+ nebula> MATCH (n:player {name : \"Tim Duncan\"})-[]->(friend:player)-[]->(fof:player) RETURN count(fof), count(DISTINCT fof) +------------+---------------------+ | COUNT(fof) | COUNT(distinct fof) | +------------+---------------------+ | 4 | 3 | +------------+---------------------+","title":"Examples"},{"location":"3.ngql-guide/6.functions-and-expressions/7.count/#countnull","text":"nebula> RETURN count(NULL), size(NULL) +-------------+------------+ | COUNT(NULL) | size(NULL) | +-------------+------------+ | 0 | __NULL__ | +-------------+------------+","title":"count(NULL)"},{"location":"3.ngql-guide/6.functions-and-expressions/8.predicate/","text":"Predicate functions \u00b6 Predicate functions return true or false. They are most commonly used in WHERE . Functions Description exists() returns true if the specified property exists in the vertex, edge or map. any() returns true if the predicate holds for at least one element in the given list. all() returns true if the predicate holds for all elements in the given list. none() returns true if the predicate holds for no element in the given list. single() returns true if the predicate holds for exactly one of the elements in the given list. NOTE : NULL is returned if the list is NULL or all of its elements are NULL. OpenCypher compatibility \u00b6 In openCypher, only function exists() is defined and specified. The other functions are implement-dependent. Syntax \u00b6 <predicate>(<variable> IN <list> WHERE <condition>) Examples \u00b6 nebula> RETURN any(n IN [1, 2, 3, 4, 5, NULL] WHERE n > 2) AS r +------+ | r | +------+ | true | +------+ nebula> RETURN single(n IN range(1, 5) WHERE n == 3) AS r +------+ | r | +------+ | true | +------+ nebula> RETURN none(n IN range(1, 3) WHERE n == 0) AS r +------+ | r | +------+ | true | +------+ nebula> WITH [1, 2, 3, 4, 5, NULL] AS a RETURN any(n IN a WHERE n > 2) +-------------------------+ | any(n IN a WHERE (n>2)) | +-------------------------+ | true | +-------------------------+ nebula> MATCH p = (n:player{name:\"LeBron James\"})<-[:follow]-(m) \\ RETURN nodes(p)[0].name AS n1, nodes(p)[1].name AS n2, \\ all(n IN nodes(p) WHERE n.name NOT STARTS WITH \"D\") AS b +----------------+-------------------+-------+ | n1 | n2 | b | +----------------+-------------------+-------+ | \"LeBron James\" | \"Danny Green\" | false | +----------------+-------------------+-------+ | \"LeBron James\" | \"Dejounte Murray\" | false | +----------------+-------------------+-------+ | \"LeBron James\" | \"Chris Paul\" | true | +----------------+-------------------+-------+ | \"LeBron James\" | \"Kyrie Irving\" | true | +----------------+-------------------+-------+ | \"LeBron James\" | \"Carmelo Anthony\" | true | +----------------+-------------------+-------+ | \"LeBron James\" | \"Dwyane Wade\" | false | +----------------+-------------------+-------+ nebula> MATCH p = (n:player{name:\"LeBron James\"})-[:follow]->(m) \\ RETURN single(n IN nodes(p) WHERE n.age > 40) AS b +------+ | b | +------+ | true | +------+ nebula> MATCH (n:player) RETURN exists(n.id), n IS NOT NULL +--------------+---------------+ | exists(n.id) | n IS NOT NULL | +--------------+---------------+ | false | true | +--------------+---------------+ ...","title":"Predicate"},{"location":"3.ngql-guide/6.functions-and-expressions/8.predicate/#predicate_functions","text":"Predicate functions return true or false. They are most commonly used in WHERE . Functions Description exists() returns true if the specified property exists in the vertex, edge or map. any() returns true if the predicate holds for at least one element in the given list. all() returns true if the predicate holds for all elements in the given list. none() returns true if the predicate holds for no element in the given list. single() returns true if the predicate holds for exactly one of the elements in the given list. NOTE : NULL is returned if the list is NULL or all of its elements are NULL.","title":"Predicate functions"},{"location":"3.ngql-guide/6.functions-and-expressions/8.predicate/#opencypher_compatibility","text":"In openCypher, only function exists() is defined and specified. The other functions are implement-dependent.","title":"OpenCypher compatibility"},{"location":"3.ngql-guide/6.functions-and-expressions/8.predicate/#syntax","text":"<predicate>(<variable> IN <list> WHERE <condition>)","title":"Syntax"},{"location":"3.ngql-guide/6.functions-and-expressions/8.predicate/#examples","text":"nebula> RETURN any(n IN [1, 2, 3, 4, 5, NULL] WHERE n > 2) AS r +------+ | r | +------+ | true | +------+ nebula> RETURN single(n IN range(1, 5) WHERE n == 3) AS r +------+ | r | +------+ | true | +------+ nebula> RETURN none(n IN range(1, 3) WHERE n == 0) AS r +------+ | r | +------+ | true | +------+ nebula> WITH [1, 2, 3, 4, 5, NULL] AS a RETURN any(n IN a WHERE n > 2) +-------------------------+ | any(n IN a WHERE (n>2)) | +-------------------------+ | true | +-------------------------+ nebula> MATCH p = (n:player{name:\"LeBron James\"})<-[:follow]-(m) \\ RETURN nodes(p)[0].name AS n1, nodes(p)[1].name AS n2, \\ all(n IN nodes(p) WHERE n.name NOT STARTS WITH \"D\") AS b +----------------+-------------------+-------+ | n1 | n2 | b | +----------------+-------------------+-------+ | \"LeBron James\" | \"Danny Green\" | false | +----------------+-------------------+-------+ | \"LeBron James\" | \"Dejounte Murray\" | false | +----------------+-------------------+-------+ | \"LeBron James\" | \"Chris Paul\" | true | +----------------+-------------------+-------+ | \"LeBron James\" | \"Kyrie Irving\" | true | +----------------+-------------------+-------+ | \"LeBron James\" | \"Carmelo Anthony\" | true | +----------------+-------------------+-------+ | \"LeBron James\" | \"Dwyane Wade\" | false | +----------------+-------------------+-------+ nebula> MATCH p = (n:player{name:\"LeBron James\"})-[:follow]->(m) \\ RETURN single(n IN nodes(p) WHERE n.age > 40) AS b +------+ | b | +------+ | true | +------+ nebula> MATCH (n:player) RETURN exists(n.id), n IS NOT NULL +--------------+---------------+ | exists(n.id) | n IS NOT NULL | +--------------+---------------+ | false | true | +--------------+---------------+ ...","title":"Examples"},{"location":"3.ngql-guide/6.functions-and-expressions/9.user-defined-functions/","text":"User-defined functions \u00b6 OpenCypher compatibility \u00b6 User-defined functions are not yet supported nor designed in Nebula Graph 2.x.","title":"User-defined functions"},{"location":"3.ngql-guide/6.functions-and-expressions/9.user-defined-functions/#user-defined_functions","text":"","title":"User-defined functions"},{"location":"3.ngql-guide/6.functions-and-expressions/9.user-defined-functions/#opencypher_compatibility","text":"User-defined functions are not yet supported nor designed in Nebula Graph 2.x.","title":"OpenCypher compatibility"},{"location":"3.ngql-guide/7.general-query-statements/2.match/","text":"MATCH \u00b6 The MATCH statement provides the searching ability based on pattern matching. A MATCH statement defines a search pattern and uses it to match data stored in Nebula Graph and to retrieve them in the form defined in the RETURN clause. A WHERE clause is often used together with the pattern as a filter to the search result. The examples in this topic use the nba dataset as the sample dataset. Syntax \u00b6 The syntax of MATCH is relatively more flexible compared with that of other query statements such as GO or LOOKUP . But generally, it can be summarized as follows. MATCH <pattern> [<WHERE clause>] RETURN <output> The workflow of MATCH \u00b6 The MATCH statement uses a native index to locate a source vertex. The vertex can be in any position in a pattern. In other words, in a valid MATCH statement, there must be an indexed property or tag, or a specific VID. For how to index a property, see Create native index . NOTE: The native index for VID is created by default, so you don't need to create an extra index if you want to match on VID. The MATCH statement searches through the pattern to match edges and other vertices. The MATCH statement retrieves data according to the RETURN clause. OpenCypher compatibility: For now, nGQL DOES NOT support scanning all vertices and edges with MATCH . For example, MATCH (v) RETURN v . Use patterns in MATCH statements \u00b6 Make sure there is at least one index for the MATCH statement to use. If you want to create an index, but there are already vertices or edges related to the tag, edge type, or property that you want to create the index for, you have to rebuild the index after creation to make it take effect on existing vertices or edges. CAUTION: Correct use of indexes can speed up queries, but indexes can dramatically reduce the write performance. The performance reduction can be as much as 90% or even more. DO NOT use indexes in production environments unless you are fully aware of their influences on your service. nebula> CREATE TAG INDEX name ON player(name(20)); // Create an index on the name property. Execution succeeded (time spent 2957/3986 us) nebula> REBUILD TAG INDEX name; // Rebuild the index. +------------+ | New Job Id | +------------+ | 121 | +------------+ Got 1 rows (time spent 2676/3990 us) nebula> SHOW JOB 121; // Make sure the rebuild job succeeded. +----------------+---------------------+------------+------------+------------+ | Job Id(TaskId) | Command(Dest) | Status | Start Time | Stop Time | +----------------+---------------------+------------+------------+------------+ | 121 | \"REBUILD_TAG_INDEX\" | \"FINISHED\" | 1607073046 | 1607073046 | +----------------+---------------------+------------+------------+------------+ | 0 | \"storaged2\" | \"FINISHED\" | 1607073046 | 1607073046 | +----------------+---------------------+------------+------------+------------+ | 1 | \"storaged0\" | \"FINISHED\" | 1607073046 | 1607073046 | +----------------+---------------------+------------+------------+------------+ | 2 | \"storaged1\" | \"FINISHED\" | 1607073046 | 1607073046 | +----------------+---------------------+------------+------------+------------+ Got 4 rows (time spent 1186/2998 us) Match a vertex \u00b6 You can use a user-defined variable in a pair of parentheses to represent a vertex in a pattern. For example: (v) . Match on tag \u00b6 To match on a tag, make sure there is an applicable tag index . For how to create a tag index, see Create tag indexes . NOTE: Tag indexes are different from property indexes. If there is an index for a property of a tag, but no index for the tag, you cannot match on the tag. A vertex tag is specified with :<tag_name> in a pattern. nebula> MATCH (v:player) RETURN v +---------------------------------------------------------------+ | v | +---------------------------------------------------------------+ | (\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"}) | +---------------------------------------------------------------+ | (\"player106\" :player{age: 25, name: \"Kyle Anderson\"}) | +---------------------------------------------------------------+ | (\"player115\" :player{age: 40, name: \"Kobe Bryant\"}) | +---------------------------------------------------------------+ ... Match on vertex property \u00b6 Tag properties are specified with {<prop_name>: <prop_value>} in a pattern after a tag. The following example uses the name property to match a vertex. nebula> MATCH (v:player{name:\"Tim Duncan\"}) RETURN v; +----------------------------------------------------+ | v | +----------------------------------------------------+ | (\"player100\" :player{name: \"Tim Duncan\", age: 42}) | +----------------------------------------------------+ The WHERE clause can do the same thing: nebula> MATCH (v:player) WHERE v.name == \"Tim Duncan\" RETURN v; +----------------------------------------------------+ | v | +----------------------------------------------------+ | (\"player100\" :player{name: \"Tim Duncan\", age: 42}) | +----------------------------------------------------+ OpenCypher compatibility In nGQL, == is the equality operator and = is the assignment operator (as in C++ or Java). In openCypher 9, = is the equality operator. Match on VID \u00b6 You can use the VID to match a vertex. The id() function can retrieve the VID of a vertex. nebula> MATCH (v) WHERE id(v) == 'player101' RETURN v; +---------------------------------------------------+ | v | +---------------------------------------------------+ | (player101) player.name:Tony Parker,player.age:36 | +---------------------------------------------------+ Got 1 rows (time spent 1710/2406 us) To match on multiple VIDs, use WHERE id(v) IN [vid_list] . nebula> MATCH (v:player { name: 'Tim Duncan' })--(v2) \\ WHERE id(v2) IN [\"player101\", \"player102\"] RETURN v2; +-----------------------------------------------------------+ | v2 | +-----------------------------------------------------------+ | (\"player101\" :player{name: \"Tony Parker\", age: 36}) | +-----------------------------------------------------------+ | (\"player102\" :player{name: \"LaMarcus Aldridge\", age: 33}) | +-----------------------------------------------------------+ | (\"player101\" :player{name: \"Tony Parker\", age: 36}) | +-----------------------------------------------------------+ Got 3 rows (time spent 3107/3683 us) Match connected vertices \u00b6 You can use the -- symbol to represent edges of both directions and match vertices connected by these edges. Legacy version compatibility: In nGQL 1.x, the -- symbol is used for inline comments. Starting from nGQL 2.0, the -- symbol represents an incoming or outgoing edge. nebula> MATCH (v:player{name:\"Tim Duncan\"})--(v2) RETURN v2.name AS Name; +---------------------+ | Name | +---------------------+ | \"Tony Parker\" | +---------------------+ | \"LaMarcus Aldridge\" | +---------------------+ | \"Marco Belinelli\" | +---------------------+ | \"Danny Green\" | +---------------------+ | \"Aron Baynes\" | +---------------------+ ... Got 13 rows (time spent 6029/8976 us) And you can add a > or < to the -- symbol to specify the direction of an edge. nebula> MATCH (v:player{name:\"Tim Duncan\"})-->(v2) RETURN v2.name AS Name; +-----------------+ | Name | +-----------------+ | \"Spurs\" | +-----------------+ | \"Tony Parker\" | +-----------------+ | \"Manu Ginobili\" | +-----------------+ Got 3 rows (time spent 2897/5993 us) In the preceding example, --> represents an edge that starts from v and points to v2 . To v , this is an outgoing edge, and to v2 this is an incoming edge. To extend the pattern, add more edges and vertices. nebula> MATCH (v:player{name:\"Tim Duncan\"})-->(v2)<--(v3) RETURN v3.name AS Name; +---------------------+ | Name | +---------------------+ | \"Tony Parker\" | +---------------------+ | \"Tiago Splitter\" | +---------------------+ | \"Dejounte Murray\" | +---------------------+ | \"Tony Parker\" | +---------------------+ | \"LaMarcus Aldridge\" | +---------------------+ ... If you don't need to refer to a vertex, you can omit the variable representing it in the parentheses. nebula> MATCH (v:player{name:\"Tim Duncan\"})-->()<--(v3) RETURN v3.name AS Name; +---------------------+ | Name | +---------------------+ | \"Tony Parker\" | +---------------------+ | \"LaMarcus Aldridge\" | +---------------------+ | \"Rudy Gay\" | +---------------------+ | \"Danny Green\" | +---------------------+ | \"Kyle Anderson\" | +---------------------+ ... Match paths \u00b6 Connected vertices and edges form a path. You can use a user-defined variable as follows to name a path. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-->(v2) RETURN p; +-------------------------------------------+ | p | +-------------------------------------------+ | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player101\" :player{age: 36, name: \"Tony Parker\"})> | +-------------------------------------------+ | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player125\" :player{age: 41, name: \"Manu Ginobili\"})> | +-------------------------------------------+ | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:serve@0 {end_year: 2016, start_year: 1997}]->(\"team204\" :team{name: \"Spurs\"})> | +-------------------------------------------+ Got 3 rows (time spent 3717/4573 us) OpenCypher compatibility: In nGQL, the @ symbol represents the rank of an edge, but openCypher has no such a concept. Match edges \u00b6 Besides using -- , --> , or <-- to indicate a nameless edge, you can use a variable in a pair of square brackets to represent a named edge. For example: -[e]- . nebula> MATCH (v:player{name:\"Tim Duncan\"})-[e]-(v2) RETURN e; +---------------------------------------------------------------------------+ | e | +---------------------------------------------------------------------------+ | [:follow \"player101\"->\"player100\" @0 {degree: 95}] | +---------------------------------------------------------------------------+ | [:follow \"player102\"->\"player100\" @0 {degree: 75}] | +---------------------------------------------------------------------------+ | [:serve \"player100\"->\"team204\" @0 {end_year: 2016, start_year: 1997}] | +---------------------------------------------------------------------------+ ... Match on edge types and properties \u00b6 Just like tags, edge types are specified with :<edge_type> . For example: -[e:serve]- . nebula> MATCH (v:player{name:\"Tim Duncan\"})-[e:serve]-(v2) RETURN e; +---------------------------------------------------------------------------+ | e | +---------------------------------------------------------------------------+ | [:serve \"player100\"->\"team204\" @0 {end_year: 2016, start_year: 1997}] | +---------------------------------------------------------------------------+ Got 1 rows (time spent 5041/5630 us) And edge type properties are specified with {<prop_name>: <prop_value>} after the :<edge_type> . For example: [e:follow{likeness:95}] . nebula> MATCH (v:player{name:\"Tim Duncan\"})-[e:follow{degree:95}]->(v2) RETURN e; +--------------------------------------------------------+ | e | +--------------------------------------------------------+ | [:follow \"player100\"->\"player101\" @0 {degree: 95}] | +--------------------------------------------------------+ | [:follow \"player100\"->\"player125\" @0 {degree: 95}] | +--------------------------------------------------------+ Got 2 rows (time spent 6080/6728 us) Match on multiple edge types \u00b6 The | symbol can help matching on multiple edge types. For example: [e:follow|:serve] . nebula> MATCH (v:player{name:\"Tim Duncan\"})-[e:follow|:serve]->(v2) RETURN e; +---------------------------------------------------------------------------+ | e | +---------------------------------------------------------------------------+ | [:follow \"player100\"->\"player101\" @0 {degree: 95}] | +---------------------------------------------------------------------------+ | [:follow \"player100\"->\"player125\" @0 {degree: 95}] | +---------------------------------------------------------------------------+ | [:serve \"player100\"->\"team204\" @0 {end_year: 2016, start_year: 1997}] | +---------------------------------------------------------------------------+ Got 3 rows (time spent 4264/4976 us) Match multiple edges \u00b6 You can expand a pattern to match multiple edges in a path. nebula> MATCH (v:player{name:\"Tim Duncan\"})-[]->(v2)<-[e:serve]-(v3) RETURN v2, v3; +------------------------------------+-----------------------------------------------------------+ | v2 | v3 | +------------------------------------+-----------------------------------------------------------+ | (\"player204\" :team{name: \"Spurs\"}) | (\"player101\" :player{name: \"Tony Parker\", age: 36}) | +------------------------------------+-----------------------------------------------------------+ | (\"player204\" :team{name: \"Spurs\"}) | (\"player102\" :player{name: \"LaMarcus Aldridge\", age: 33}) | +------------------------------------+-----------------------------------------------------------+ | (\"player204\" :team{name: \"Spurs\"}) | (\"player103\" :player{age: 32, name: \"Rudy Gay\"}) | +------------------------------------+-----------------------------------------------------------+ ... Match fixed-length paths \u00b6 To match a fixed-length path, use the :<edge_type>*<hop> pattern. hop must be a non-negative integer. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[e:follow*2]->(v2) RETURN DISTINCT v2 AS Friends; +-----------------------------------------------------------+ | Friends | +-----------------------------------------------------------+ | (\"player100\" :player{name: \"Tim Duncan\", age: 42}) | +-----------------------------------------------------------+ | (\"player102\" :player{name: \"LaMarcus Aldridge\", age: 33}) | +-----------------------------------------------------------+ | (\"player125\" :player{name: \"Manu Ginobili\", age: 41}) | +-----------------------------------------------------------+ Got 3 rows (time spent 4863/5591 us) If hop is 0, the pattern matches the source vertex on the path. nebula> MATCH (v:player{name:\"Tim Duncan\"}) -[*0]-> (v2) RETURN v2; +----------------------------------------------------+ | v2 | +----------------------------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | +----------------------------------------------------+ Got 1 rows (time spent 2785/3377 us) Match variable-length paths \u00b6 You can use the :<edge_type>*[minHop]..<maxHop> pattern to match variable-length paths. Parameter Description minHop Optional. Represents the minimum length of the path. minHop must be a non-negative integer. The default value is 1. maxHop Required. Represents the maximum length of the path. maxHop must be a non-negative integer. It has no default value. OpenCypher compatibility In nGQL, maxHop is required. And .. cannot be omitted after minHop . In openCypher, maxHop is optional and default to infinity. When no bounds are given, .. can be omitted. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[e:follow*1..3]->(v2) \\ RETURN v2 AS Friends; +-----------------------------------------------------------+ | Friends | +-----------------------------------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | +-----------------------------------------------------------+ | (\"player101\" :player{age: 36, name: \"Tony Parker\"}) | +-----------------------------------------------------------+ | (\"player125\" :player{age: 41, name: \"Manu Ginobili\"}) | +-----------------------------------------------------------+ | (\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"}) | +-----------------------------------------------------------+ Got 4 rows (time spent 6166/6887 us) You can use the DISTINCT keyword to aggregate duplicate results. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[e:follow*1..3]->(v2:player) \\ RETURN DISTINCT v2 AS Friends, count(v2); +-----------------------------------------------------------+-----------+ | Friends | COUNT(v2) | +-----------------------------------------------------------+-----------+ | (\"player125\" :player{age: 41, name: \"Manu Ginobili\"}) | 3 | +-----------------------------------------------------------+-----------+ | (\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"}) | 1 | +-----------------------------------------------------------+-----------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | 4 | +-----------------------------------------------------------+-----------+ | (\"player101\" :player{age: 36, name: \"Tony Parker\"}) | 3 | +-----------------------------------------------------------+-----------+ Got 4 rows (time spent 5502/6556 us) If minHop is 0, the pattern matches the source vertex. Compared to the preceding statement, the following statement uses 0 as the minHop , so in the following result set \"Tim Duncan\" is counted one more time than it is in the preceding result set because it is the source vertex. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[e:follow*0..3]->(v2:player) \\ RETURN DISTINCT v2 AS Friends, count(v2); +-----------------------------------------------------------+-----------+ | Friends | COUNT(v2) | +-----------------------------------------------------------+-----------+ | (\"player125\" :player{age: 41, name: \"Manu Ginobili\"}) | 3 | +-----------------------------------------------------------+-----------+ | (\"player101\" :player{age: 36, name: \"Tony Parker\"}) | 3 | +-----------------------------------------------------------+-----------+ | (\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"}) | 1 | +-----------------------------------------------------------+-----------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | 5 | +-----------------------------------------------------------+-----------+ Got 4 rows (time spent 5553/6275 us) Match variable-length paths with multiple edge types \u00b6 You can specify multiple edge types in a fixed-length or variable-length pattern. In this case, hop , minHop , and maxHop take effect on all edge types. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[e:follow|serve*2]->(v2) \\ RETURN DISTINCT v2; +-----------------------------------------------------------+ | v2 | +-----------------------------------------------------------+ | (\"player100\" :player{name: \"Tim Duncan\", age: 42}) | +-----------------------------------------------------------+ | (\"player102\" :player{name: \"LaMarcus Aldridge\", age: 33}) | +-----------------------------------------------------------+ | (\"player125\" :player{name: \"Manu Ginobili\", age: 41}) | +-----------------------------------------------------------+ | (\"player204\" :team{name: \"Spurs\"}) | +-----------------------------------------------------------+ | (\"player215\" :team{name: \"Hornets\"}) | +-----------------------------------------------------------+ Got 5 rows (time spent 3834/4571 us) Common retrieving operations \u00b6 This section shows how to retrieve commonly used items with MATCH statements. Retrieve vertex or edge information \u00b6 Use RETURN {<vertex_name> | <edge_name>} to retrieve all the information of a vertex or an edge. nebula> MATCH (v:player{name:\"Tim Duncan\"}) RETURN v; +----------------------------------------------------+ | v | +----------------------------------------------------+ | (\"player100\" :player{name: \"Tim Duncan\", age: 42}) | +----------------------------------------------------+ Got 1 rows (time spent 1863/2545 us) nebula> MATCH (v:player{name:\"Tim Duncan\"})-[e]->(v2) RETURN e; +---------------------------------------------------------------------------+ | e | +---------------------------------------------------------------------------+ | [:follow \"player100\"->\"player101\" @0 {degree: 95}] | +---------------------------------------------------------------------------+ | [:follow \"player100\"->\"player125\" @0 {degree: 95}] | +---------------------------------------------------------------------------+ | [:serve \"player100\"->\"team204\" @0 {end_year: 2016, start_year: 1997}] | +---------------------------------------------------------------------------+ Got 3 rows (time spent 3139/3773 us) Retrieve VIDs \u00b6 Use the id() function to retrieve VIDs. nebula> MATCH (v:player{name:\"Tim Duncan\"}) RETURN id(v); +-------------+ | id(v) | +-------------+ | \"player100\" | +-------------+ Got 1 rows (time spent 2070/2747 us) Retrieve tags \u00b6 Use the labels() function to retrieve the list of tags on a vertex. nebula> MATCH (v:player{name:\"Tim Duncan\"}) RETURN labels(v); +------------+ | labels(v) | +------------+ | [\"player\"] | +------------+ Got 1 rows (time spent 2198/2941 us) To retrieve the nth element in the labels(v) list, use labels(v)[n-1] . The following example shows how to use labels(v)[0] to retrieve the first tag in the list. nebula> MATCH (v:player{name:\"Tim Duncan\"}) RETURN labels(v)[0]; +--------------+ | labels(v)[0] | +--------------+ | \"player\" | +--------------+ Got 1 rows (time spent 2609/3481 us) Retrieve a single property on a vertex or an edge \u00b6 Use RETURN {<vertex_name> | <edge_name>}.<property> to retrieve a single property. nebula> MATCH (v:player{name:\"Tim Duncan\"}) RETURN v.age; +-------+ | v.age | +-------+ | 42 | +-------+ Got 1 rows (time spent 2261/2973 us) Use AS to specify an alias for a property. nebula> MATCH (v:player{name:\"Tim Duncan\"}) RETURN v.age AS Age; +-----+ | Age | +-----+ | 42 | +-----+ Got 1 rows (time spent 1762/2321 us) Retrieve all properties on a vertex or an edge \u00b6 Use the properties() function to retrieve all properties on a vertex or an edge. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[]->(v2) RETURN properties(v2); +------------------------------------+ | properties(v2) | +------------------------------------+ | {\"name\":\"Spurs\"} | +------------------------------------+ | {\"name\":\"Tony Parker\", \"age\":36} | +------------------------------------+ | {\"age\":41, \"name\":\"Manu Ginobili\"} | +------------------------------------+ Got 3 rows (time spent 2943/3541 us) Retrieve edge types \u00b6 Use the type() function to retrieve the types of the matched edges. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[e]->() RETURN DISTINCT type(e); +----------+ | type(e) | +----------+ | \"follow\" | +----------+ | \"serve\" | +----------+ Got 3 rows (time spent 3776/4660 us) Retrieve paths \u00b6 Use RETURN <path_name> to retrieve all the information of the matched paths. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[*3]->() RETURN p; +-------------------------------------------------------------------------------------------------+ | p | +-------------------------------------------------------------------------------------------------+ | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player101\" :player{age: 36, name: \"Tony Parker\"})-[:follow@0 {degree: 90}]->(\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"})-[:serve@0 {end_year: 2019, start_year: 2015}]->(\"team204\" :team{name: \"Spurs\"})> | +-------------------------------------------------------------------------------------------------+ | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player101\" :player{age: 36, name: \"Tony Parker\"})-[:follow@0 {degree: 90}]->(\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"})-[:serve@0 {end_year: 2015, start_year: 2006}]->(\"team203\" :team{name: \"Trail Blazers\"})> | +-------------------------------------------------------------------------------------------------+ | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player101\" :player{age: 36, name: \"Tony Parker\"})-[:follow@0 {degree: 90}]->(\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"})-[:follow@0 {degree: 75}]->(\"player101\" :player{age: 36, name: \"Tony Parker\"})> | +-------------------------------------------------------------------------------------------------+ ... Retrieve vertices in a path \u00b6 Use the nodes() function to retrieve all vertices in a path. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[]->(v2) RETURN nodes(p); +---------------------------------------------------------------------------------------------------------------------+ | nodes(p) | +---------------------------------------------------------------------------------------------------------------------+ | [(\"player100\" :star{} :player{age: 42, name: \"Tim Duncan\"}), (\"player204\" :team{name: \"Spurs\"})] | +---------------------------------------------------------------------------------------------------------------------+ | [(\"player100\" :star{} :player{age: 42, name: \"Tim Duncan\"}), (\"player101\" :player{name: \"Tony Parker\", age: 36})] | +---------------------------------------------------------------------------------------------------------------------+ | [(\"player100\" :star{} :player{age: 42, name: \"Tim Duncan\"}), (\"player125\" :player{name: \"Manu Ginobili\", age: 41})] | +---------------------------------------------------------------------------------------------------------------------+ Got 3 rows (time spent 2529/3128 us) Retrieve edges in a path \u00b6 Use the relationships() function to retrieve all edges in a path. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[]->(v2) RETURN relationships(p); +-----------------------------------------------------------------------------+ | relationships(p) | +-----------------------------------------------------------------------------+ | [[:follow \"player100\"->\"player101\" @0 {degree: 95}]] | +-----------------------------------------------------------------------------+ | [[:follow \"player100\"->\"player125\" @0 {degree: 95}]] | +-----------------------------------------------------------------------------+ | [[:serve \"player100\"->\"team204\" @0 {end_year: 2016, start_year: 1997}]] | +-----------------------------------------------------------------------------+ Got 3 rows (time spent 2715/3363 us) Retrieve path length \u00b6 Use the length() function to retrieve the length of a path. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[*..2]->(v2) \\ RETURN p AS Paths, length(p) AS Length; +----------------------------------------------------------------------+--------+ | Paths | Length | +----------------------------------------------------------------------+--------+ | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player125\" :player{age: 41, name: \"Manu Ginobili\"})-[:serve@0 {end_year: 2018, start_year: 2002}]->(\"team204\" :team{name: \"Spurs\"})> | 2 | +----------------------------------------------------------------------+--------+ | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player125\" :player{age: 41, name: \"Manu Ginobili\"})-[:follow@0 {degree: 90}]->(\"player100\" :player{age: 42, name: \"Tim Duncan\"})> | 2 | +----------------------------------------------------------------------+--------+ | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player101\" :player{age: 36, name: \"Tony Parker\"})-[:serve@0 {end_year: 2019, start_year: 2018}]->(\"team215\" :team{name: \"Hornets\"})> | 2 | +----------------------------------------------------------------------+--------+ | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player101\" :player{age: 36, name: \"Tony Parker\"})-[:serve@0 {end_year: 2018, start_year: 1999}]->(\"team204\" :team{name: \"Spurs\"})> | 2 | +----------------------------------------------------------------------+--------+ | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player101\" :player{age: 36, name: \"Tony Parker\"})-[:follow@0 {degree: 95}]->(\"player125\" :player{age: 41, name: \"Manu Ginobili\"})> | 2 | +----------------------------------------------------------------------+--------+ | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player101\" :player{age: 36, name: \"Tony Parker\"})-[:follow@0 {degree: 90}]->(\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"})> | 2 | +----------------------------------------------------------------------+--------+ | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player101\" :player{age: 36, name: \"Tony Parker\"})-[:follow@0 {degree: 95}]->(\"player100\" :player{age: 42, name: \"Tim Duncan\"})> | 2 | +----------------------------------------------------------------------+--------+ | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:serve@0 {end_year: 2016, start_year: 1997}]->(\"team204\" :team{name: \"Spurs\"})> | 1 | +----------------------------------------------------------------------+--------+ | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player125\" :player{age: 41, name: \"Manu Ginobili\"})> | 1 | +----------------------------------------------------------------------+--------+","title":"Match"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match","text":"The MATCH statement provides the searching ability based on pattern matching. A MATCH statement defines a search pattern and uses it to match data stored in Nebula Graph and to retrieve them in the form defined in the RETURN clause. A WHERE clause is often used together with the pattern as a filter to the search result. The examples in this topic use the nba dataset as the sample dataset.","title":"MATCH"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#syntax","text":"The syntax of MATCH is relatively more flexible compared with that of other query statements such as GO or LOOKUP . But generally, it can be summarized as follows. MATCH <pattern> [<WHERE clause>] RETURN <output>","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#the_workflow_of_match","text":"The MATCH statement uses a native index to locate a source vertex. The vertex can be in any position in a pattern. In other words, in a valid MATCH statement, there must be an indexed property or tag, or a specific VID. For how to index a property, see Create native index . NOTE: The native index for VID is created by default, so you don't need to create an extra index if you want to match on VID. The MATCH statement searches through the pattern to match edges and other vertices. The MATCH statement retrieves data according to the RETURN clause. OpenCypher compatibility: For now, nGQL DOES NOT support scanning all vertices and edges with MATCH . For example, MATCH (v) RETURN v .","title":"The workflow of MATCH"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#use_patterns_in_match_statements","text":"Make sure there is at least one index for the MATCH statement to use. If you want to create an index, but there are already vertices or edges related to the tag, edge type, or property that you want to create the index for, you have to rebuild the index after creation to make it take effect on existing vertices or edges. CAUTION: Correct use of indexes can speed up queries, but indexes can dramatically reduce the write performance. The performance reduction can be as much as 90% or even more. DO NOT use indexes in production environments unless you are fully aware of their influences on your service. nebula> CREATE TAG INDEX name ON player(name(20)); // Create an index on the name property. Execution succeeded (time spent 2957/3986 us) nebula> REBUILD TAG INDEX name; // Rebuild the index. +------------+ | New Job Id | +------------+ | 121 | +------------+ Got 1 rows (time spent 2676/3990 us) nebula> SHOW JOB 121; // Make sure the rebuild job succeeded. +----------------+---------------------+------------+------------+------------+ | Job Id(TaskId) | Command(Dest) | Status | Start Time | Stop Time | +----------------+---------------------+------------+------------+------------+ | 121 | \"REBUILD_TAG_INDEX\" | \"FINISHED\" | 1607073046 | 1607073046 | +----------------+---------------------+------------+------------+------------+ | 0 | \"storaged2\" | \"FINISHED\" | 1607073046 | 1607073046 | +----------------+---------------------+------------+------------+------------+ | 1 | \"storaged0\" | \"FINISHED\" | 1607073046 | 1607073046 | +----------------+---------------------+------------+------------+------------+ | 2 | \"storaged1\" | \"FINISHED\" | 1607073046 | 1607073046 | +----------------+---------------------+------------+------------+------------+ Got 4 rows (time spent 1186/2998 us)","title":"Use patterns in MATCH statements"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_a_vertex","text":"You can use a user-defined variable in a pair of parentheses to represent a vertex in a pattern. For example: (v) .","title":"Match a vertex"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_on_tag","text":"To match on a tag, make sure there is an applicable tag index . For how to create a tag index, see Create tag indexes . NOTE: Tag indexes are different from property indexes. If there is an index for a property of a tag, but no index for the tag, you cannot match on the tag. A vertex tag is specified with :<tag_name> in a pattern. nebula> MATCH (v:player) RETURN v +---------------------------------------------------------------+ | v | +---------------------------------------------------------------+ | (\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"}) | +---------------------------------------------------------------+ | (\"player106\" :player{age: 25, name: \"Kyle Anderson\"}) | +---------------------------------------------------------------+ | (\"player115\" :player{age: 40, name: \"Kobe Bryant\"}) | +---------------------------------------------------------------+ ...","title":"Match on tag"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_on_vertex_property","text":"Tag properties are specified with {<prop_name>: <prop_value>} in a pattern after a tag. The following example uses the name property to match a vertex. nebula> MATCH (v:player{name:\"Tim Duncan\"}) RETURN v; +----------------------------------------------------+ | v | +----------------------------------------------------+ | (\"player100\" :player{name: \"Tim Duncan\", age: 42}) | +----------------------------------------------------+ The WHERE clause can do the same thing: nebula> MATCH (v:player) WHERE v.name == \"Tim Duncan\" RETURN v; +----------------------------------------------------+ | v | +----------------------------------------------------+ | (\"player100\" :player{name: \"Tim Duncan\", age: 42}) | +----------------------------------------------------+ OpenCypher compatibility In nGQL, == is the equality operator and = is the assignment operator (as in C++ or Java). In openCypher 9, = is the equality operator.","title":"Match on vertex property"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_on_vid","text":"You can use the VID to match a vertex. The id() function can retrieve the VID of a vertex. nebula> MATCH (v) WHERE id(v) == 'player101' RETURN v; +---------------------------------------------------+ | v | +---------------------------------------------------+ | (player101) player.name:Tony Parker,player.age:36 | +---------------------------------------------------+ Got 1 rows (time spent 1710/2406 us) To match on multiple VIDs, use WHERE id(v) IN [vid_list] . nebula> MATCH (v:player { name: 'Tim Duncan' })--(v2) \\ WHERE id(v2) IN [\"player101\", \"player102\"] RETURN v2; +-----------------------------------------------------------+ | v2 | +-----------------------------------------------------------+ | (\"player101\" :player{name: \"Tony Parker\", age: 36}) | +-----------------------------------------------------------+ | (\"player102\" :player{name: \"LaMarcus Aldridge\", age: 33}) | +-----------------------------------------------------------+ | (\"player101\" :player{name: \"Tony Parker\", age: 36}) | +-----------------------------------------------------------+ Got 3 rows (time spent 3107/3683 us)","title":"Match on VID"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_connected_vertices","text":"You can use the -- symbol to represent edges of both directions and match vertices connected by these edges. Legacy version compatibility: In nGQL 1.x, the -- symbol is used for inline comments. Starting from nGQL 2.0, the -- symbol represents an incoming or outgoing edge. nebula> MATCH (v:player{name:\"Tim Duncan\"})--(v2) RETURN v2.name AS Name; +---------------------+ | Name | +---------------------+ | \"Tony Parker\" | +---------------------+ | \"LaMarcus Aldridge\" | +---------------------+ | \"Marco Belinelli\" | +---------------------+ | \"Danny Green\" | +---------------------+ | \"Aron Baynes\" | +---------------------+ ... Got 13 rows (time spent 6029/8976 us) And you can add a > or < to the -- symbol to specify the direction of an edge. nebula> MATCH (v:player{name:\"Tim Duncan\"})-->(v2) RETURN v2.name AS Name; +-----------------+ | Name | +-----------------+ | \"Spurs\" | +-----------------+ | \"Tony Parker\" | +-----------------+ | \"Manu Ginobili\" | +-----------------+ Got 3 rows (time spent 2897/5993 us) In the preceding example, --> represents an edge that starts from v and points to v2 . To v , this is an outgoing edge, and to v2 this is an incoming edge. To extend the pattern, add more edges and vertices. nebula> MATCH (v:player{name:\"Tim Duncan\"})-->(v2)<--(v3) RETURN v3.name AS Name; +---------------------+ | Name | +---------------------+ | \"Tony Parker\" | +---------------------+ | \"Tiago Splitter\" | +---------------------+ | \"Dejounte Murray\" | +---------------------+ | \"Tony Parker\" | +---------------------+ | \"LaMarcus Aldridge\" | +---------------------+ ... If you don't need to refer to a vertex, you can omit the variable representing it in the parentheses. nebula> MATCH (v:player{name:\"Tim Duncan\"})-->()<--(v3) RETURN v3.name AS Name; +---------------------+ | Name | +---------------------+ | \"Tony Parker\" | +---------------------+ | \"LaMarcus Aldridge\" | +---------------------+ | \"Rudy Gay\" | +---------------------+ | \"Danny Green\" | +---------------------+ | \"Kyle Anderson\" | +---------------------+ ...","title":"Match connected vertices"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_paths","text":"Connected vertices and edges form a path. You can use a user-defined variable as follows to name a path. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-->(v2) RETURN p; +-------------------------------------------+ | p | +-------------------------------------------+ | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player101\" :player{age: 36, name: \"Tony Parker\"})> | +-------------------------------------------+ | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player125\" :player{age: 41, name: \"Manu Ginobili\"})> | +-------------------------------------------+ | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:serve@0 {end_year: 2016, start_year: 1997}]->(\"team204\" :team{name: \"Spurs\"})> | +-------------------------------------------+ Got 3 rows (time spent 3717/4573 us) OpenCypher compatibility: In nGQL, the @ symbol represents the rank of an edge, but openCypher has no such a concept.","title":"Match paths"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_edges","text":"Besides using -- , --> , or <-- to indicate a nameless edge, you can use a variable in a pair of square brackets to represent a named edge. For example: -[e]- . nebula> MATCH (v:player{name:\"Tim Duncan\"})-[e]-(v2) RETURN e; +---------------------------------------------------------------------------+ | e | +---------------------------------------------------------------------------+ | [:follow \"player101\"->\"player100\" @0 {degree: 95}] | +---------------------------------------------------------------------------+ | [:follow \"player102\"->\"player100\" @0 {degree: 75}] | +---------------------------------------------------------------------------+ | [:serve \"player100\"->\"team204\" @0 {end_year: 2016, start_year: 1997}] | +---------------------------------------------------------------------------+ ...","title":"Match edges"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_on_edge_types_and_properties","text":"Just like tags, edge types are specified with :<edge_type> . For example: -[e:serve]- . nebula> MATCH (v:player{name:\"Tim Duncan\"})-[e:serve]-(v2) RETURN e; +---------------------------------------------------------------------------+ | e | +---------------------------------------------------------------------------+ | [:serve \"player100\"->\"team204\" @0 {end_year: 2016, start_year: 1997}] | +---------------------------------------------------------------------------+ Got 1 rows (time spent 5041/5630 us) And edge type properties are specified with {<prop_name>: <prop_value>} after the :<edge_type> . For example: [e:follow{likeness:95}] . nebula> MATCH (v:player{name:\"Tim Duncan\"})-[e:follow{degree:95}]->(v2) RETURN e; +--------------------------------------------------------+ | e | +--------------------------------------------------------+ | [:follow \"player100\"->\"player101\" @0 {degree: 95}] | +--------------------------------------------------------+ | [:follow \"player100\"->\"player125\" @0 {degree: 95}] | +--------------------------------------------------------+ Got 2 rows (time spent 6080/6728 us)","title":"Match on edge types and properties"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_on_multiple_edge_types","text":"The | symbol can help matching on multiple edge types. For example: [e:follow|:serve] . nebula> MATCH (v:player{name:\"Tim Duncan\"})-[e:follow|:serve]->(v2) RETURN e; +---------------------------------------------------------------------------+ | e | +---------------------------------------------------------------------------+ | [:follow \"player100\"->\"player101\" @0 {degree: 95}] | +---------------------------------------------------------------------------+ | [:follow \"player100\"->\"player125\" @0 {degree: 95}] | +---------------------------------------------------------------------------+ | [:serve \"player100\"->\"team204\" @0 {end_year: 2016, start_year: 1997}] | +---------------------------------------------------------------------------+ Got 3 rows (time spent 4264/4976 us)","title":"Match on multiple edge types"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_multiple_edges","text":"You can expand a pattern to match multiple edges in a path. nebula> MATCH (v:player{name:\"Tim Duncan\"})-[]->(v2)<-[e:serve]-(v3) RETURN v2, v3; +------------------------------------+-----------------------------------------------------------+ | v2 | v3 | +------------------------------------+-----------------------------------------------------------+ | (\"player204\" :team{name: \"Spurs\"}) | (\"player101\" :player{name: \"Tony Parker\", age: 36}) | +------------------------------------+-----------------------------------------------------------+ | (\"player204\" :team{name: \"Spurs\"}) | (\"player102\" :player{name: \"LaMarcus Aldridge\", age: 33}) | +------------------------------------+-----------------------------------------------------------+ | (\"player204\" :team{name: \"Spurs\"}) | (\"player103\" :player{age: 32, name: \"Rudy Gay\"}) | +------------------------------------+-----------------------------------------------------------+ ...","title":"Match multiple edges"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_fixed-length_paths","text":"To match a fixed-length path, use the :<edge_type>*<hop> pattern. hop must be a non-negative integer. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[e:follow*2]->(v2) RETURN DISTINCT v2 AS Friends; +-----------------------------------------------------------+ | Friends | +-----------------------------------------------------------+ | (\"player100\" :player{name: \"Tim Duncan\", age: 42}) | +-----------------------------------------------------------+ | (\"player102\" :player{name: \"LaMarcus Aldridge\", age: 33}) | +-----------------------------------------------------------+ | (\"player125\" :player{name: \"Manu Ginobili\", age: 41}) | +-----------------------------------------------------------+ Got 3 rows (time spent 4863/5591 us) If hop is 0, the pattern matches the source vertex on the path. nebula> MATCH (v:player{name:\"Tim Duncan\"}) -[*0]-> (v2) RETURN v2; +----------------------------------------------------+ | v2 | +----------------------------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | +----------------------------------------------------+ Got 1 rows (time spent 2785/3377 us)","title":"Match fixed-length paths"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_variable-length_paths","text":"You can use the :<edge_type>*[minHop]..<maxHop> pattern to match variable-length paths. Parameter Description minHop Optional. Represents the minimum length of the path. minHop must be a non-negative integer. The default value is 1. maxHop Required. Represents the maximum length of the path. maxHop must be a non-negative integer. It has no default value. OpenCypher compatibility In nGQL, maxHop is required. And .. cannot be omitted after minHop . In openCypher, maxHop is optional and default to infinity. When no bounds are given, .. can be omitted. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[e:follow*1..3]->(v2) \\ RETURN v2 AS Friends; +-----------------------------------------------------------+ | Friends | +-----------------------------------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | +-----------------------------------------------------------+ | (\"player101\" :player{age: 36, name: \"Tony Parker\"}) | +-----------------------------------------------------------+ | (\"player125\" :player{age: 41, name: \"Manu Ginobili\"}) | +-----------------------------------------------------------+ | (\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"}) | +-----------------------------------------------------------+ Got 4 rows (time spent 6166/6887 us) You can use the DISTINCT keyword to aggregate duplicate results. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[e:follow*1..3]->(v2:player) \\ RETURN DISTINCT v2 AS Friends, count(v2); +-----------------------------------------------------------+-----------+ | Friends | COUNT(v2) | +-----------------------------------------------------------+-----------+ | (\"player125\" :player{age: 41, name: \"Manu Ginobili\"}) | 3 | +-----------------------------------------------------------+-----------+ | (\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"}) | 1 | +-----------------------------------------------------------+-----------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | 4 | +-----------------------------------------------------------+-----------+ | (\"player101\" :player{age: 36, name: \"Tony Parker\"}) | 3 | +-----------------------------------------------------------+-----------+ Got 4 rows (time spent 5502/6556 us) If minHop is 0, the pattern matches the source vertex. Compared to the preceding statement, the following statement uses 0 as the minHop , so in the following result set \"Tim Duncan\" is counted one more time than it is in the preceding result set because it is the source vertex. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[e:follow*0..3]->(v2:player) \\ RETURN DISTINCT v2 AS Friends, count(v2); +-----------------------------------------------------------+-----------+ | Friends | COUNT(v2) | +-----------------------------------------------------------+-----------+ | (\"player125\" :player{age: 41, name: \"Manu Ginobili\"}) | 3 | +-----------------------------------------------------------+-----------+ | (\"player101\" :player{age: 36, name: \"Tony Parker\"}) | 3 | +-----------------------------------------------------------+-----------+ | (\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"}) | 1 | +-----------------------------------------------------------+-----------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | 5 | +-----------------------------------------------------------+-----------+ Got 4 rows (time spent 5553/6275 us)","title":"Match variable-length paths"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_variable-length_paths_with_multiple_edge_types","text":"You can specify multiple edge types in a fixed-length or variable-length pattern. In this case, hop , minHop , and maxHop take effect on all edge types. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[e:follow|serve*2]->(v2) \\ RETURN DISTINCT v2; +-----------------------------------------------------------+ | v2 | +-----------------------------------------------------------+ | (\"player100\" :player{name: \"Tim Duncan\", age: 42}) | +-----------------------------------------------------------+ | (\"player102\" :player{name: \"LaMarcus Aldridge\", age: 33}) | +-----------------------------------------------------------+ | (\"player125\" :player{name: \"Manu Ginobili\", age: 41}) | +-----------------------------------------------------------+ | (\"player204\" :team{name: \"Spurs\"}) | +-----------------------------------------------------------+ | (\"player215\" :team{name: \"Hornets\"}) | +-----------------------------------------------------------+ Got 5 rows (time spent 3834/4571 us)","title":"Match variable-length paths with multiple edge types"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#common_retrieving_operations","text":"This section shows how to retrieve commonly used items with MATCH statements.","title":"Common retrieving operations"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#retrieve_vertex_or_edge_information","text":"Use RETURN {<vertex_name> | <edge_name>} to retrieve all the information of a vertex or an edge. nebula> MATCH (v:player{name:\"Tim Duncan\"}) RETURN v; +----------------------------------------------------+ | v | +----------------------------------------------------+ | (\"player100\" :player{name: \"Tim Duncan\", age: 42}) | +----------------------------------------------------+ Got 1 rows (time spent 1863/2545 us) nebula> MATCH (v:player{name:\"Tim Duncan\"})-[e]->(v2) RETURN e; +---------------------------------------------------------------------------+ | e | +---------------------------------------------------------------------------+ | [:follow \"player100\"->\"player101\" @0 {degree: 95}] | +---------------------------------------------------------------------------+ | [:follow \"player100\"->\"player125\" @0 {degree: 95}] | +---------------------------------------------------------------------------+ | [:serve \"player100\"->\"team204\" @0 {end_year: 2016, start_year: 1997}] | +---------------------------------------------------------------------------+ Got 3 rows (time spent 3139/3773 us)","title":"Retrieve vertex or edge information"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#retrieve_vids","text":"Use the id() function to retrieve VIDs. nebula> MATCH (v:player{name:\"Tim Duncan\"}) RETURN id(v); +-------------+ | id(v) | +-------------+ | \"player100\" | +-------------+ Got 1 rows (time spent 2070/2747 us)","title":"Retrieve VIDs"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#retrieve_tags","text":"Use the labels() function to retrieve the list of tags on a vertex. nebula> MATCH (v:player{name:\"Tim Duncan\"}) RETURN labels(v); +------------+ | labels(v) | +------------+ | [\"player\"] | +------------+ Got 1 rows (time spent 2198/2941 us) To retrieve the nth element in the labels(v) list, use labels(v)[n-1] . The following example shows how to use labels(v)[0] to retrieve the first tag in the list. nebula> MATCH (v:player{name:\"Tim Duncan\"}) RETURN labels(v)[0]; +--------------+ | labels(v)[0] | +--------------+ | \"player\" | +--------------+ Got 1 rows (time spent 2609/3481 us)","title":"Retrieve tags"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#retrieve_a_single_property_on_a_vertex_or_an_edge","text":"Use RETURN {<vertex_name> | <edge_name>}.<property> to retrieve a single property. nebula> MATCH (v:player{name:\"Tim Duncan\"}) RETURN v.age; +-------+ | v.age | +-------+ | 42 | +-------+ Got 1 rows (time spent 2261/2973 us) Use AS to specify an alias for a property. nebula> MATCH (v:player{name:\"Tim Duncan\"}) RETURN v.age AS Age; +-----+ | Age | +-----+ | 42 | +-----+ Got 1 rows (time spent 1762/2321 us)","title":"Retrieve a single property on a vertex or an edge"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#retrieve_all_properties_on_a_vertex_or_an_edge","text":"Use the properties() function to retrieve all properties on a vertex or an edge. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[]->(v2) RETURN properties(v2); +------------------------------------+ | properties(v2) | +------------------------------------+ | {\"name\":\"Spurs\"} | +------------------------------------+ | {\"name\":\"Tony Parker\", \"age\":36} | +------------------------------------+ | {\"age\":41, \"name\":\"Manu Ginobili\"} | +------------------------------------+ Got 3 rows (time spent 2943/3541 us)","title":"Retrieve all properties on a vertex or an edge"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#retrieve_edge_types","text":"Use the type() function to retrieve the types of the matched edges. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[e]->() RETURN DISTINCT type(e); +----------+ | type(e) | +----------+ | \"follow\" | +----------+ | \"serve\" | +----------+ Got 3 rows (time spent 3776/4660 us)","title":"Retrieve edge types"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#retrieve_paths","text":"Use RETURN <path_name> to retrieve all the information of the matched paths. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[*3]->() RETURN p; +-------------------------------------------------------------------------------------------------+ | p | +-------------------------------------------------------------------------------------------------+ | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player101\" :player{age: 36, name: \"Tony Parker\"})-[:follow@0 {degree: 90}]->(\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"})-[:serve@0 {end_year: 2019, start_year: 2015}]->(\"team204\" :team{name: \"Spurs\"})> | +-------------------------------------------------------------------------------------------------+ | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player101\" :player{age: 36, name: \"Tony Parker\"})-[:follow@0 {degree: 90}]->(\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"})-[:serve@0 {end_year: 2015, start_year: 2006}]->(\"team203\" :team{name: \"Trail Blazers\"})> | +-------------------------------------------------------------------------------------------------+ | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player101\" :player{age: 36, name: \"Tony Parker\"})-[:follow@0 {degree: 90}]->(\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"})-[:follow@0 {degree: 75}]->(\"player101\" :player{age: 36, name: \"Tony Parker\"})> | +-------------------------------------------------------------------------------------------------+ ...","title":"Retrieve paths"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#retrieve_vertices_in_a_path","text":"Use the nodes() function to retrieve all vertices in a path. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[]->(v2) RETURN nodes(p); +---------------------------------------------------------------------------------------------------------------------+ | nodes(p) | +---------------------------------------------------------------------------------------------------------------------+ | [(\"player100\" :star{} :player{age: 42, name: \"Tim Duncan\"}), (\"player204\" :team{name: \"Spurs\"})] | +---------------------------------------------------------------------------------------------------------------------+ | [(\"player100\" :star{} :player{age: 42, name: \"Tim Duncan\"}), (\"player101\" :player{name: \"Tony Parker\", age: 36})] | +---------------------------------------------------------------------------------------------------------------------+ | [(\"player100\" :star{} :player{age: 42, name: \"Tim Duncan\"}), (\"player125\" :player{name: \"Manu Ginobili\", age: 41})] | +---------------------------------------------------------------------------------------------------------------------+ Got 3 rows (time spent 2529/3128 us)","title":"Retrieve vertices in a path"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#retrieve_edges_in_a_path","text":"Use the relationships() function to retrieve all edges in a path. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[]->(v2) RETURN relationships(p); +-----------------------------------------------------------------------------+ | relationships(p) | +-----------------------------------------------------------------------------+ | [[:follow \"player100\"->\"player101\" @0 {degree: 95}]] | +-----------------------------------------------------------------------------+ | [[:follow \"player100\"->\"player125\" @0 {degree: 95}]] | +-----------------------------------------------------------------------------+ | [[:serve \"player100\"->\"team204\" @0 {end_year: 2016, start_year: 1997}]] | +-----------------------------------------------------------------------------+ Got 3 rows (time spent 2715/3363 us)","title":"Retrieve edges in a path"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#retrieve_path_length","text":"Use the length() function to retrieve the length of a path. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[*..2]->(v2) \\ RETURN p AS Paths, length(p) AS Length; +----------------------------------------------------------------------+--------+ | Paths | Length | +----------------------------------------------------------------------+--------+ | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player125\" :player{age: 41, name: \"Manu Ginobili\"})-[:serve@0 {end_year: 2018, start_year: 2002}]->(\"team204\" :team{name: \"Spurs\"})> | 2 | +----------------------------------------------------------------------+--------+ | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player125\" :player{age: 41, name: \"Manu Ginobili\"})-[:follow@0 {degree: 90}]->(\"player100\" :player{age: 42, name: \"Tim Duncan\"})> | 2 | +----------------------------------------------------------------------+--------+ | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player101\" :player{age: 36, name: \"Tony Parker\"})-[:serve@0 {end_year: 2019, start_year: 2018}]->(\"team215\" :team{name: \"Hornets\"})> | 2 | +----------------------------------------------------------------------+--------+ | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player101\" :player{age: 36, name: \"Tony Parker\"})-[:serve@0 {end_year: 2018, start_year: 1999}]->(\"team204\" :team{name: \"Spurs\"})> | 2 | +----------------------------------------------------------------------+--------+ | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player101\" :player{age: 36, name: \"Tony Parker\"})-[:follow@0 {degree: 95}]->(\"player125\" :player{age: 41, name: \"Manu Ginobili\"})> | 2 | +----------------------------------------------------------------------+--------+ | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player101\" :player{age: 36, name: \"Tony Parker\"})-[:follow@0 {degree: 90}]->(\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"})> | 2 | +----------------------------------------------------------------------+--------+ | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player101\" :player{age: 36, name: \"Tony Parker\"})-[:follow@0 {degree: 95}]->(\"player100\" :player{age: 42, name: \"Tim Duncan\"})> | 2 | +----------------------------------------------------------------------+--------+ | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:serve@0 {end_year: 2016, start_year: 1997}]->(\"team204\" :team{name: \"Spurs\"})> | 1 | +----------------------------------------------------------------------+--------+ | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player125\" :player{age: 41, name: \"Manu Ginobili\"})> | 1 | +----------------------------------------------------------------------+--------+","title":"Retrieve path length"},{"location":"3.ngql-guide/7.general-query-statements/3.go/","text":"GO \u00b6 OpenCypher compatibility \u00b6 This page applies to nGQL extensions only. Syntax \u00b6 GO [[<M> TO] <N> STEPS ] FROM <vertex_list> OVER <edge_type_list> [{REVERSELY | BIDIRECT}] [ WHERE <expression> [ {AND | OR} expression ...]) ] [YIELD [DISTINCT] <return_list>] [| ORDER BY <expression> [{ASC | DESC}]] [| LIMIT [<offset_value>,] <number_rows>] GO [[<M> TO] <N> STEPS ] FROM <vertex_list> OVER <edge_type_list> [{REVERSELY | BIDIRECT}] [ WHERE <expression> [ {AND | OR} expression ...]) ] [| GROUP BY {col_name | expr | position} YIELD <col_name>] <vertex_list> ::= <vid> [, <vid> ...] <edge_type_list> ::= edge_type [, edge_type ...] | * <return_list> ::= <col_name> [AS <col_alias>] [, <col_name> [AS <col_alias>] ...] GO traverses in a graph with specified filters and returns results. <N> STEPS specifies the hop number. If not specified, the default value for N is one. When N is zero, Nebula Graph does not traverse any edges and returns nothing. M TO N STEPS traverses from M to N hops. When M is zero, the output is the same as that of M is one. That is, the output of GO 0 TO 2 and GO 1 TO 2 are the same. <vertex_list> is a list of vertex IDs separated by commas, or a special place holder $-.id . For more information, see Pipe . <edge_type_list> is a list of edge types which the traversal can go through. REVERSELY | BIDIRECT defines the direction of the query. By default, GO statements searches for outgoing edges. If REVERSELY is set, GO searches for incoming edges. If BIDIRECT is set, GO searches for edges of both directions. WHERE <expression> specifies the traversal filters. You can use WHERE for the source vertices, the edges, and the destination vertices. You can use WHERE together with AND , OR , and NOT . For more information, see WHERE . NOTE : There are some restrictions for the WHERE clause when you traverse along with multiple edge types. For example, WHERE edge1.prop1 > edge2.prop2 is not supported. YIELD [DISTINCT] <return_list> specifies the desired output. For more information, see YIELD . When not specified, the destination vertex IDs are returned by default. ORDER BY sorts the outputs with the specified orders. For more information, see ORDER BY . NOTE : When the sorting method is not specified, the output orders can be different for the same query. LIMIT limits the row numbers for the output. For more information, see LIMIT . GROUP BY groups outputs into subgroups based on values of the specified properties. For more information, see GROUP BY . Examples \u00b6 // Returns teams that player 102 serves. nebula> GO FROM \"player102\" OVER serve; +------------+ | serve._dst | +------------+ | \"team203\" | +------------+ | \"team204\" | +------------+ // Returns the 2 hop friends of the player 102. nebula> GO 2 STEPS FROM \"player102\" OVER follow; +-------------+ | follow._dst | +-------------+ | \"player101\" | +-------------+ | \"player125\" | +-------------+ ... // Adds a filter for the traversal then duplicates the output. nebula> GO FROM \"player100\", \"player102\" OVER serve \\ WHERE serve.start_year > 1995 \\ YIELD DISTINCT $$.team.name AS team_name, serve.start_year AS start_year, $^.player.name AS player_name; +-----------------+------------+---------------------+ | team_name | start_year | player_name | +-----------------+------------+---------------------+ | \"Spurs\" | 1997 | \"Tim Duncan\" | +-----------------+------------+---------------------+ | \"Trail Blazers\" | 2006 | \"LaMarcus Aldridge\" | +-----------------+------------+---------------------+ | \"Spurs\" | 2015 | \"LaMarcus Aldridge\" | +-----------------+------------+---------------------+ // Traverses along with multiple edge types. nebula> GO FROM \"player100\" OVER follow, serve YIELD follow.degree, serve.start_year; +---------------+------------------+ | follow.degree | serve.start_year | +---------------+------------------+ | 95 | __EMPTY__ | +---------------+------------------+ | 95 | __EMPTY__ | +---------------+------------------+ | __EMPTY__ | 1997 | +---------------+------------------+ Nebula Graph displays different properties by columns. If there is no value for a property, the output is __EMPTY__ . // Returns player 100. nebula> GO FROM \"player100\" OVER follow REVERSELY YIELD follow._dst AS destination; +-------------+ | destination | +-------------+ | \"player101\" | +-------------+ | \"player102\" | +-------------+ ... // This MATCH query shares the same semantics with the preceding GO query. nebula> MATCH (v)<-[e:follow]- (v2) WHERE id(v) == 'player100' RETURN id(v2) AS destination; +-------------+ | destination | +-------------+ | \"player101\" | +-------------+ | \"player102\" | +-------------+ ... // Finds player 100's friends and the teams that they serve. nebula> GO FROM \"player100\" OVER follow REVERSELY \\ YIELD follow._dst AS id | \\ GO FROM $-.id OVER serve \\ WHERE $^.player.age > 20 \\ YIELD $^.player.name AS FriendOf, $$.team.name AS Team; +---------------------+-----------------+ | FriendOf | Team | +---------------------+-----------------+ | \"Tony Parker\" | \"Spurs\" | +---------------------+-----------------+ | \"Tony Parker\" | \"Hornets\" | +---------------------+-----------------+ ... // This MATCH query shares the same semantics with the preceding GO query. nebula> MATCH (v)<-[e:follow]- (v2)-[e2:serve]->(v3) WHERE id(v) == 'player100' RETURN v2.name AS FriendOf, v3.name AS Team; +---------------------+-----------------+ | FriendOf | Team | +---------------------+-----------------+ | \"Tony Parker\" | \"Spurs\" | +---------------------+-----------------+ | \"Tony Parker\" | \"Hornets\" | +---------------------+-----------------+ ... nebula> GO FROM \"player102\" OVER follow BIDIRECT YIELD follow._dst AS both; +-------------+ | both | +-------------+ | \"player100\" | +-------------+ | \"player101\" | +-------------+ ... // This MATCH query shares the same semantics with the preceding GO query. nebula> MATCH (v) -[e:follow]-(v2) WHERE id(v)== \"player102\" RETURN id(v2) AS both; +-------------+ | both | +-------------+ | \"player101\" | +-------------+ | \"player103\" | +-------------+ ... nebula> GO 1 TO 2 STEPS FROM \"player100\" OVER follow YIELD follow._dst AS destination; +-------------+ | destination | +-------------+ | \"player101\" | +-------------+ | \"player125\" | +-------------+ ... // This MATCH query shares the same semantics with the preceding GO query. nebula> MATCH (v) -[e:follow*1..2]->(v2) WHERE id(v) == \"player100\" RETURN id(v2) AS destination; +-------------+ | destination | +-------------+ | \"player100\" | +-------------+ | \"player102\" | +-------------+ nebula> GO 2 STEPS FROM \"player100\" OVER follow \\ YIELD follow._src AS src, follow._dst AS dst, $$.player.age AS age \\ | GROUP BY $-.dst YIELD $-.dst AS dst, collect_set($-.src) AS src, collect($-.age) AS age +-------------+----------------------------+----------+ | dst | src | age | +-------------+----------------------------+----------+ | \"player125\" | [\"player101\"] | [41] | +-------------+----------------------------+----------+ | \"player100\" | [\"player125\", \"player101\"] | [42, 42] | +-------------+----------------------------+----------+ | \"player102\" | [\"player101\"] | [33] | +-------------+----------------------------+----------+ nebula> $a = GO FROM \"player100\" OVER follow YIELD follow._src AS src, follow._dst AS dst; \\ GO 2 STEPS FROM $a.dst OVER follow YIELD $a.src AS src, $a.dst, follow._src, follow._dst \\ | ORDER BY $-.src | OFFSET 1 LIMIT 2; +-------------+-------------+-------------+-------------+ | src | $a.dst | follow._src | follow._dst | +-------------+-------------+-------------+-------------+ | \"player100\" | \"player125\" | \"player100\" | \"player101\" | +-------------+-------------+-------------+-------------+ | \"player100\" | \"player101\" | \"player100\" | \"player125\" | +-------------+-------------+-------------+-------------+","title":"GO"},{"location":"3.ngql-guide/7.general-query-statements/3.go/#go","text":"","title":"GO"},{"location":"3.ngql-guide/7.general-query-statements/3.go/#opencypher_compatibility","text":"This page applies to nGQL extensions only.","title":"OpenCypher compatibility"},{"location":"3.ngql-guide/7.general-query-statements/3.go/#syntax","text":"GO [[<M> TO] <N> STEPS ] FROM <vertex_list> OVER <edge_type_list> [{REVERSELY | BIDIRECT}] [ WHERE <expression> [ {AND | OR} expression ...]) ] [YIELD [DISTINCT] <return_list>] [| ORDER BY <expression> [{ASC | DESC}]] [| LIMIT [<offset_value>,] <number_rows>] GO [[<M> TO] <N> STEPS ] FROM <vertex_list> OVER <edge_type_list> [{REVERSELY | BIDIRECT}] [ WHERE <expression> [ {AND | OR} expression ...]) ] [| GROUP BY {col_name | expr | position} YIELD <col_name>] <vertex_list> ::= <vid> [, <vid> ...] <edge_type_list> ::= edge_type [, edge_type ...] | * <return_list> ::= <col_name> [AS <col_alias>] [, <col_name> [AS <col_alias>] ...] GO traverses in a graph with specified filters and returns results. <N> STEPS specifies the hop number. If not specified, the default value for N is one. When N is zero, Nebula Graph does not traverse any edges and returns nothing. M TO N STEPS traverses from M to N hops. When M is zero, the output is the same as that of M is one. That is, the output of GO 0 TO 2 and GO 1 TO 2 are the same. <vertex_list> is a list of vertex IDs separated by commas, or a special place holder $-.id . For more information, see Pipe . <edge_type_list> is a list of edge types which the traversal can go through. REVERSELY | BIDIRECT defines the direction of the query. By default, GO statements searches for outgoing edges. If REVERSELY is set, GO searches for incoming edges. If BIDIRECT is set, GO searches for edges of both directions. WHERE <expression> specifies the traversal filters. You can use WHERE for the source vertices, the edges, and the destination vertices. You can use WHERE together with AND , OR , and NOT . For more information, see WHERE . NOTE : There are some restrictions for the WHERE clause when you traverse along with multiple edge types. For example, WHERE edge1.prop1 > edge2.prop2 is not supported. YIELD [DISTINCT] <return_list> specifies the desired output. For more information, see YIELD . When not specified, the destination vertex IDs are returned by default. ORDER BY sorts the outputs with the specified orders. For more information, see ORDER BY . NOTE : When the sorting method is not specified, the output orders can be different for the same query. LIMIT limits the row numbers for the output. For more information, see LIMIT . GROUP BY groups outputs into subgroups based on values of the specified properties. For more information, see GROUP BY .","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/3.go/#examples","text":"// Returns teams that player 102 serves. nebula> GO FROM \"player102\" OVER serve; +------------+ | serve._dst | +------------+ | \"team203\" | +------------+ | \"team204\" | +------------+ // Returns the 2 hop friends of the player 102. nebula> GO 2 STEPS FROM \"player102\" OVER follow; +-------------+ | follow._dst | +-------------+ | \"player101\" | +-------------+ | \"player125\" | +-------------+ ... // Adds a filter for the traversal then duplicates the output. nebula> GO FROM \"player100\", \"player102\" OVER serve \\ WHERE serve.start_year > 1995 \\ YIELD DISTINCT $$.team.name AS team_name, serve.start_year AS start_year, $^.player.name AS player_name; +-----------------+------------+---------------------+ | team_name | start_year | player_name | +-----------------+------------+---------------------+ | \"Spurs\" | 1997 | \"Tim Duncan\" | +-----------------+------------+---------------------+ | \"Trail Blazers\" | 2006 | \"LaMarcus Aldridge\" | +-----------------+------------+---------------------+ | \"Spurs\" | 2015 | \"LaMarcus Aldridge\" | +-----------------+------------+---------------------+ // Traverses along with multiple edge types. nebula> GO FROM \"player100\" OVER follow, serve YIELD follow.degree, serve.start_year; +---------------+------------------+ | follow.degree | serve.start_year | +---------------+------------------+ | 95 | __EMPTY__ | +---------------+------------------+ | 95 | __EMPTY__ | +---------------+------------------+ | __EMPTY__ | 1997 | +---------------+------------------+ Nebula Graph displays different properties by columns. If there is no value for a property, the output is __EMPTY__ . // Returns player 100. nebula> GO FROM \"player100\" OVER follow REVERSELY YIELD follow._dst AS destination; +-------------+ | destination | +-------------+ | \"player101\" | +-------------+ | \"player102\" | +-------------+ ... // This MATCH query shares the same semantics with the preceding GO query. nebula> MATCH (v)<-[e:follow]- (v2) WHERE id(v) == 'player100' RETURN id(v2) AS destination; +-------------+ | destination | +-------------+ | \"player101\" | +-------------+ | \"player102\" | +-------------+ ... // Finds player 100's friends and the teams that they serve. nebula> GO FROM \"player100\" OVER follow REVERSELY \\ YIELD follow._dst AS id | \\ GO FROM $-.id OVER serve \\ WHERE $^.player.age > 20 \\ YIELD $^.player.name AS FriendOf, $$.team.name AS Team; +---------------------+-----------------+ | FriendOf | Team | +---------------------+-----------------+ | \"Tony Parker\" | \"Spurs\" | +---------------------+-----------------+ | \"Tony Parker\" | \"Hornets\" | +---------------------+-----------------+ ... // This MATCH query shares the same semantics with the preceding GO query. nebula> MATCH (v)<-[e:follow]- (v2)-[e2:serve]->(v3) WHERE id(v) == 'player100' RETURN v2.name AS FriendOf, v3.name AS Team; +---------------------+-----------------+ | FriendOf | Team | +---------------------+-----------------+ | \"Tony Parker\" | \"Spurs\" | +---------------------+-----------------+ | \"Tony Parker\" | \"Hornets\" | +---------------------+-----------------+ ... nebula> GO FROM \"player102\" OVER follow BIDIRECT YIELD follow._dst AS both; +-------------+ | both | +-------------+ | \"player100\" | +-------------+ | \"player101\" | +-------------+ ... // This MATCH query shares the same semantics with the preceding GO query. nebula> MATCH (v) -[e:follow]-(v2) WHERE id(v)== \"player102\" RETURN id(v2) AS both; +-------------+ | both | +-------------+ | \"player101\" | +-------------+ | \"player103\" | +-------------+ ... nebula> GO 1 TO 2 STEPS FROM \"player100\" OVER follow YIELD follow._dst AS destination; +-------------+ | destination | +-------------+ | \"player101\" | +-------------+ | \"player125\" | +-------------+ ... // This MATCH query shares the same semantics with the preceding GO query. nebula> MATCH (v) -[e:follow*1..2]->(v2) WHERE id(v) == \"player100\" RETURN id(v2) AS destination; +-------------+ | destination | +-------------+ | \"player100\" | +-------------+ | \"player102\" | +-------------+ nebula> GO 2 STEPS FROM \"player100\" OVER follow \\ YIELD follow._src AS src, follow._dst AS dst, $$.player.age AS age \\ | GROUP BY $-.dst YIELD $-.dst AS dst, collect_set($-.src) AS src, collect($-.age) AS age +-------------+----------------------------+----------+ | dst | src | age | +-------------+----------------------------+----------+ | \"player125\" | [\"player101\"] | [41] | +-------------+----------------------------+----------+ | \"player100\" | [\"player125\", \"player101\"] | [42, 42] | +-------------+----------------------------+----------+ | \"player102\" | [\"player101\"] | [33] | +-------------+----------------------------+----------+ nebula> $a = GO FROM \"player100\" OVER follow YIELD follow._src AS src, follow._dst AS dst; \\ GO 2 STEPS FROM $a.dst OVER follow YIELD $a.src AS src, $a.dst, follow._src, follow._dst \\ | ORDER BY $-.src | OFFSET 1 LIMIT 2; +-------------+-------------+-------------+-------------+ | src | $a.dst | follow._src | follow._dst | +-------------+-------------+-------------+-------------+ | \"player100\" | \"player125\" | \"player100\" | \"player101\" | +-------------+-------------+-------------+-------------+ | \"player100\" | \"player101\" | \"player100\" | \"player125\" | +-------------+-------------+-------------+-------------+","title":"Examples"},{"location":"3.ngql-guide/7.general-query-statements/4.fetch/","text":"FETCH \u00b6 The FETCH statement retrieves the properties of the specified vertices or edges. OpenCypher Compatibility \u00b6 This topic applies to nGQL extensions only. Fetch vertex properties \u00b6 Syntax \u00b6 FETCH PROP ON {<tag_name>[, tag_name ...] | *} <vid> [, vid ...] [YIELD <output>] The descriptions of the fields are as follows. Field Description tag_name The name of the tag. * Represents all the tags in the current graph space. vid The vertex ID. output Specifies the information to be returned. For more information, see YIELD . If there is no YIELD clause, FETCH returns all the matched information. Fetch vertex properties by one tag \u00b6 Specify a tag in the FETCH statement to fetch the vertex properties by that tag. nebula> FETCH PROP ON player \"player100\"; +----------------------------------------------------+ | vertices_ | +----------------------------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | +----------------------------------------------------+ Got 1 rows (time spent 913/1629 us) Fetch specific properties of a vertex \u00b6 Use a YIELD clause to specify the properties to be returned. nebula> FETCH PROP ON player \"player100\" \\ YIELD player.name; +-------------+--------------+ | VertexID | player.name | +-------------+--------------+ | \"player100\" | \"Tim Duncan\" | +-------------+--------------+ Got 1 rows (time spent 2933/5931 us) Fetch properties of multiple vertices \u00b6 Specify multiple VIDs (vertex IDs) to fetch properties of multiple vertices. Separate the VIDs with commas. nebula> FETCH PROP ON player \"player101\", \"player102\", \"player103\"; +-----------------------------------------------------------+ | vertices_ | +-----------------------------------------------------------+ | (\"player101\" :player{age: 36, name: \"Tony Parker\"}) | +-----------------------------------------------------------+ | (\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"}) | +-----------------------------------------------------------+ | (\"player103\" :player{age: 32, name: \"Rudy Gay\"}) | +-----------------------------------------------------------+ Got 3 rows (time spent 1786/3135 us) Fetch vertex properties by multiple tags \u00b6 Specify multiple tags in the FETCH statement to fetch the vertex properties by the tags. Separate the tags with commas. // Create a new tag t1. nebula> CREATE TAG t1(a string, b int); Execution succeeded (time spent 4153/5296 us) // Attach t1 to vertex \"player100\". nebula> INSERT VERTEX t1(a, b) VALUE \"player100\":(\"Hello\", 100); Execution succeeded (time spent 1703/2321 us) // Fetch the properties of vertex \"player100\" by the tags player and t1. nebula> FETCH PROP ON player, t1 \"player100\"; +----------------------------------------------------------------------------+ | vertices_ | +----------------------------------------------------------------------------+ | (\"player100\" :t1{a: \"Hello\", b: 100} :player{age: 42, name: \"Tim Duncan\"}) | +----------------------------------------------------------------------------+ Got 1 rows (time spent 1788/2560 us) You can combine multiple tags with multiple VIDs in a FETCH statement. nebula> FETCH PROP ON player, t1 \"player100\", \"player103\"; +----------------------------------------------------------------------------+ | vertices_ | +----------------------------------------------------------------------------+ | (\"player100\" :t1{a: \"Hello\", b: 100} :player{age: 42, name: \"Tim Duncan\"}) | +----------------------------------------------------------------------------+ | (\"player103\" :player{age: 32, name: \"Rudy Gay\"}) | +----------------------------------------------------------------------------+ Got 2 rows (time spent 2971/3748 us) Fetch vertex properties by all tags \u00b6 Set an asterisk symbol (*) to fetch properties by all tags in the current graph space. nebula> FETCH PROP ON * \"player100\", \"player106\", \"team200\"; +----------------------------------------------------------------------------+ | vertices_ | +----------------------------------------------------------------------------+ | (\"player106\" :player{age: 25, name: \"Kyle Anderson\"}) | +----------------------------------------------------------------------------+ | (\"team200\" :team{name: \"Warriors\"}) | +----------------------------------------------------------------------------+ | (\"player100\" :t1{a: \"Hello\", b: 100} :player{age: 42, name: \"Tim Duncan\"}) | +----------------------------------------------------------------------------+ Got 3 rows (time spent 2620/4863 us) Fetch edge properties \u00b6 Syntax \u00b6 FETCH PROP ON <edge_type> <src_vid> -> <dst_vid>[@<rank>] [, <src_vid> -> <dst_vid> ...] [YIELD <output>] The descriptions of the fields are as follows. Field Description edge_type The name of the edge type. src_vid The VID of the source vertex. It specifies the start of an edge. dst_vid The VID of the destination vertex. It specifies the end of an edge. rank The rank of the edge. It is optional and defaults to 0. It distinguishes an edge from other edges with the same edge type, source vertex, and destination vertex. output Specifies the information to be returned. For more information, see YIELD . If there is no YIELD clause, FETCH returns all the matched information. Fetch all properties of an edge \u00b6 The following statement fetches all the properties of the serve edge that connects vertex \"player100\" and vertex \"team204\" . nebula> FETCH PROP ON serve \"player100\" -> \"team204\"; +-----------------------------------------------------------------------+ | edges_ | +-----------------------------------------------------------------------+ | [:serve \"player100\"->\"team204\" @0 {end_year: 2016, start_year: 1997}] | +-----------------------------------------------------------------------+ Got 1 rows (time spent 1048/1632 us) Fetch specific properties of an edge \u00b6 Use a YIELD clause to fetch specific properties of an edge. nebula> FETCH PROP ON serve \"player100\" -> \"team204\" YIELD serve.start_year; +-------------+------------+-------------+------------------+ | serve._src | serve._dst | serve._rank | serve.start_year | +-------------+------------+-------------+------------------+ | \"player100\" | \"team204\" | 0 | 1997 | +-------------+------------+-------------+------------------+ Got 1 rows (time spent 1834/2863 us) Fetch properties of multiple edges \u00b6 Specify multiple edge patterns ( <src_vid> -> <dst_vid>[@<rank>] ) to fetch properties of multiple edges. Separate the edge patterns with commas. nebula> FETCH PROP ON serve \"player100\" -> \"team204\", \"player133\" -> \"team202\"; +-----------------------------------------------------------------------+ | edges_ | +-----------------------------------------------------------------------+ | [:serve \"player100\"->\"team204\" @0 {end_year: 2016, start_year: 1997}] | +-----------------------------------------------------------------------+ | [:serve \"player133\"->\"team202\" @0 {end_year: 2011, start_year: 2002}] | +-----------------------------------------------------------------------+ Got 2 rows (time spent 1466/2441 us) Fetch properties based on edge rank \u00b6 If there are multiple edges that have different ranks but the same edge type, source vertex, destination vertex, specify the rank to fetch the properties on the correct edge. // Insert edges with different ranks and property values. nebula> insert edge serve(start_year,end_year) \\ values \"player100\"->\"team204\"@1:(1998, 2017); Execution succeeded (time spent 1679/3192 us) nebula> insert edge serve(start_year,end_year) \\ values \"player100\"->\"team204\"@2:(1990, 2018); Execution succeeded (time spent 1091/1608 us) // By default, FETCH returns the edge with rank 0. nebula> FETCH PROP ON serve \"player100\" -> \"team204\"; +-----------------------------------------------------------------------+ | edges_ | +-----------------------------------------------------------------------+ | [:serve \"player100\"->\"team204\" @0 {end_year: 2016, start_year: 1997}] | +-----------------------------------------------------------------------+ Got 1 rows (time spent 2031/2739 us) // To fetch on an edge with rank other than 0, set its rank in FETCH. nebula> FETCH PROP ON serve \"player100\" -> \"team204\"@1; +-----------------------------------------------------------------------+ | edges_ | +-----------------------------------------------------------------------+ | [:serve \"player100\"->\"team204\" @1 {end_year: 2017, start_year: 1998}] | +-----------------------------------------------------------------------+ Got 1 rows (time spent 1049/1711 us) Use FETCH in composite queries \u00b6 A common way to use FETCH is to combine it with nGQL extensions such as GO. The following statement returns the degree values of outgoing follow edges that start from vertex \"player101\" . nebula> GO FROM \"player101\" OVER follow \\ YIELD follow._src AS s, follow._dst AS d | \\ FETCH PROP ON follow $-.s -> $-.d \\ YIELD follow.degree; +-------------+-------------+--------------+---------------+ | follow._src | follow._dst | follow._rank | follow.degree | +-------------+-------------+--------------+---------------+ | \"player101\" | \"player100\" | 0 | 95 | +-------------+-------------+--------------+---------------+ | \"player101\" | \"player102\" | 0 | 90 | +-------------+-------------+--------------+---------------+ | \"player101\" | \"player125\" | 0 | 95 | +-------------+-------------+--------------+---------------+ Got 3 rows (time spent 3047/3880 us) Or you can use user-defined variables to construct similar queries. nebula> $var = GO FROM \"player101\" OVER follow \\ YIELD follow._src AS s, follow._dst AS d; \\ FETCH PROP ON follow $var.s -> $var.d \\ YIELD follow.degree; +-------------+-------------+--------------+---------------+ | follow._src | follow._dst | follow._rank | follow.degree | +-------------+-------------+--------------+---------------+ | \"player101\" | \"player100\" | 0 | 95 | +-------------+-------------+--------------+---------------+ | \"player101\" | \"player102\" | 0 | 90 | +-------------+-------------+--------------+---------------+ | \"player101\" | \"player125\" | 0 | 95 | +-------------+-------------+--------------+---------------+ Got 3 rows (time spent 1891/2509 us) For more information about composite queries, see Composite queries (clause structure) .","title":"FETCH"},{"location":"3.ngql-guide/7.general-query-statements/4.fetch/#fetch","text":"The FETCH statement retrieves the properties of the specified vertices or edges.","title":"FETCH"},{"location":"3.ngql-guide/7.general-query-statements/4.fetch/#opencypher_compatibility","text":"This topic applies to nGQL extensions only.","title":"OpenCypher Compatibility"},{"location":"3.ngql-guide/7.general-query-statements/4.fetch/#fetch_vertex_properties","text":"","title":"Fetch vertex properties"},{"location":"3.ngql-guide/7.general-query-statements/4.fetch/#syntax","text":"FETCH PROP ON {<tag_name>[, tag_name ...] | *} <vid> [, vid ...] [YIELD <output>] The descriptions of the fields are as follows. Field Description tag_name The name of the tag. * Represents all the tags in the current graph space. vid The vertex ID. output Specifies the information to be returned. For more information, see YIELD . If there is no YIELD clause, FETCH returns all the matched information.","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/4.fetch/#fetch_vertex_properties_by_one_tag","text":"Specify a tag in the FETCH statement to fetch the vertex properties by that tag. nebula> FETCH PROP ON player \"player100\"; +----------------------------------------------------+ | vertices_ | +----------------------------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | +----------------------------------------------------+ Got 1 rows (time spent 913/1629 us)","title":"Fetch vertex properties by one tag"},{"location":"3.ngql-guide/7.general-query-statements/4.fetch/#fetch_specific_properties_of_a_vertex","text":"Use a YIELD clause to specify the properties to be returned. nebula> FETCH PROP ON player \"player100\" \\ YIELD player.name; +-------------+--------------+ | VertexID | player.name | +-------------+--------------+ | \"player100\" | \"Tim Duncan\" | +-------------+--------------+ Got 1 rows (time spent 2933/5931 us)","title":"Fetch specific properties of a vertex"},{"location":"3.ngql-guide/7.general-query-statements/4.fetch/#fetch_properties_of_multiple_vertices","text":"Specify multiple VIDs (vertex IDs) to fetch properties of multiple vertices. Separate the VIDs with commas. nebula> FETCH PROP ON player \"player101\", \"player102\", \"player103\"; +-----------------------------------------------------------+ | vertices_ | +-----------------------------------------------------------+ | (\"player101\" :player{age: 36, name: \"Tony Parker\"}) | +-----------------------------------------------------------+ | (\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"}) | +-----------------------------------------------------------+ | (\"player103\" :player{age: 32, name: \"Rudy Gay\"}) | +-----------------------------------------------------------+ Got 3 rows (time spent 1786/3135 us)","title":"Fetch properties of multiple vertices"},{"location":"3.ngql-guide/7.general-query-statements/4.fetch/#fetch_vertex_properties_by_multiple_tags","text":"Specify multiple tags in the FETCH statement to fetch the vertex properties by the tags. Separate the tags with commas. // Create a new tag t1. nebula> CREATE TAG t1(a string, b int); Execution succeeded (time spent 4153/5296 us) // Attach t1 to vertex \"player100\". nebula> INSERT VERTEX t1(a, b) VALUE \"player100\":(\"Hello\", 100); Execution succeeded (time spent 1703/2321 us) // Fetch the properties of vertex \"player100\" by the tags player and t1. nebula> FETCH PROP ON player, t1 \"player100\"; +----------------------------------------------------------------------------+ | vertices_ | +----------------------------------------------------------------------------+ | (\"player100\" :t1{a: \"Hello\", b: 100} :player{age: 42, name: \"Tim Duncan\"}) | +----------------------------------------------------------------------------+ Got 1 rows (time spent 1788/2560 us) You can combine multiple tags with multiple VIDs in a FETCH statement. nebula> FETCH PROP ON player, t1 \"player100\", \"player103\"; +----------------------------------------------------------------------------+ | vertices_ | +----------------------------------------------------------------------------+ | (\"player100\" :t1{a: \"Hello\", b: 100} :player{age: 42, name: \"Tim Duncan\"}) | +----------------------------------------------------------------------------+ | (\"player103\" :player{age: 32, name: \"Rudy Gay\"}) | +----------------------------------------------------------------------------+ Got 2 rows (time spent 2971/3748 us)","title":"Fetch vertex properties by multiple tags"},{"location":"3.ngql-guide/7.general-query-statements/4.fetch/#fetch_vertex_properties_by_all_tags","text":"Set an asterisk symbol (*) to fetch properties by all tags in the current graph space. nebula> FETCH PROP ON * \"player100\", \"player106\", \"team200\"; +----------------------------------------------------------------------------+ | vertices_ | +----------------------------------------------------------------------------+ | (\"player106\" :player{age: 25, name: \"Kyle Anderson\"}) | +----------------------------------------------------------------------------+ | (\"team200\" :team{name: \"Warriors\"}) | +----------------------------------------------------------------------------+ | (\"player100\" :t1{a: \"Hello\", b: 100} :player{age: 42, name: \"Tim Duncan\"}) | +----------------------------------------------------------------------------+ Got 3 rows (time spent 2620/4863 us)","title":"Fetch vertex properties by all tags"},{"location":"3.ngql-guide/7.general-query-statements/4.fetch/#fetch_edge_properties","text":"","title":"Fetch edge properties"},{"location":"3.ngql-guide/7.general-query-statements/4.fetch/#syntax_1","text":"FETCH PROP ON <edge_type> <src_vid> -> <dst_vid>[@<rank>] [, <src_vid> -> <dst_vid> ...] [YIELD <output>] The descriptions of the fields are as follows. Field Description edge_type The name of the edge type. src_vid The VID of the source vertex. It specifies the start of an edge. dst_vid The VID of the destination vertex. It specifies the end of an edge. rank The rank of the edge. It is optional and defaults to 0. It distinguishes an edge from other edges with the same edge type, source vertex, and destination vertex. output Specifies the information to be returned. For more information, see YIELD . If there is no YIELD clause, FETCH returns all the matched information.","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/4.fetch/#fetch_all_properties_of_an_edge","text":"The following statement fetches all the properties of the serve edge that connects vertex \"player100\" and vertex \"team204\" . nebula> FETCH PROP ON serve \"player100\" -> \"team204\"; +-----------------------------------------------------------------------+ | edges_ | +-----------------------------------------------------------------------+ | [:serve \"player100\"->\"team204\" @0 {end_year: 2016, start_year: 1997}] | +-----------------------------------------------------------------------+ Got 1 rows (time spent 1048/1632 us)","title":"Fetch all properties of an edge"},{"location":"3.ngql-guide/7.general-query-statements/4.fetch/#fetch_specific_properties_of_an_edge","text":"Use a YIELD clause to fetch specific properties of an edge. nebula> FETCH PROP ON serve \"player100\" -> \"team204\" YIELD serve.start_year; +-------------+------------+-------------+------------------+ | serve._src | serve._dst | serve._rank | serve.start_year | +-------------+------------+-------------+------------------+ | \"player100\" | \"team204\" | 0 | 1997 | +-------------+------------+-------------+------------------+ Got 1 rows (time spent 1834/2863 us)","title":"Fetch specific properties of an edge"},{"location":"3.ngql-guide/7.general-query-statements/4.fetch/#fetch_properties_of_multiple_edges","text":"Specify multiple edge patterns ( <src_vid> -> <dst_vid>[@<rank>] ) to fetch properties of multiple edges. Separate the edge patterns with commas. nebula> FETCH PROP ON serve \"player100\" -> \"team204\", \"player133\" -> \"team202\"; +-----------------------------------------------------------------------+ | edges_ | +-----------------------------------------------------------------------+ | [:serve \"player100\"->\"team204\" @0 {end_year: 2016, start_year: 1997}] | +-----------------------------------------------------------------------+ | [:serve \"player133\"->\"team202\" @0 {end_year: 2011, start_year: 2002}] | +-----------------------------------------------------------------------+ Got 2 rows (time spent 1466/2441 us)","title":"Fetch properties of multiple edges"},{"location":"3.ngql-guide/7.general-query-statements/4.fetch/#fetch_properties_based_on_edge_rank","text":"If there are multiple edges that have different ranks but the same edge type, source vertex, destination vertex, specify the rank to fetch the properties on the correct edge. // Insert edges with different ranks and property values. nebula> insert edge serve(start_year,end_year) \\ values \"player100\"->\"team204\"@1:(1998, 2017); Execution succeeded (time spent 1679/3192 us) nebula> insert edge serve(start_year,end_year) \\ values \"player100\"->\"team204\"@2:(1990, 2018); Execution succeeded (time spent 1091/1608 us) // By default, FETCH returns the edge with rank 0. nebula> FETCH PROP ON serve \"player100\" -> \"team204\"; +-----------------------------------------------------------------------+ | edges_ | +-----------------------------------------------------------------------+ | [:serve \"player100\"->\"team204\" @0 {end_year: 2016, start_year: 1997}] | +-----------------------------------------------------------------------+ Got 1 rows (time spent 2031/2739 us) // To fetch on an edge with rank other than 0, set its rank in FETCH. nebula> FETCH PROP ON serve \"player100\" -> \"team204\"@1; +-----------------------------------------------------------------------+ | edges_ | +-----------------------------------------------------------------------+ | [:serve \"player100\"->\"team204\" @1 {end_year: 2017, start_year: 1998}] | +-----------------------------------------------------------------------+ Got 1 rows (time spent 1049/1711 us)","title":"Fetch properties based on edge rank"},{"location":"3.ngql-guide/7.general-query-statements/4.fetch/#use_fetch_in_composite_queries","text":"A common way to use FETCH is to combine it with nGQL extensions such as GO. The following statement returns the degree values of outgoing follow edges that start from vertex \"player101\" . nebula> GO FROM \"player101\" OVER follow \\ YIELD follow._src AS s, follow._dst AS d | \\ FETCH PROP ON follow $-.s -> $-.d \\ YIELD follow.degree; +-------------+-------------+--------------+---------------+ | follow._src | follow._dst | follow._rank | follow.degree | +-------------+-------------+--------------+---------------+ | \"player101\" | \"player100\" | 0 | 95 | +-------------+-------------+--------------+---------------+ | \"player101\" | \"player102\" | 0 | 90 | +-------------+-------------+--------------+---------------+ | \"player101\" | \"player125\" | 0 | 95 | +-------------+-------------+--------------+---------------+ Got 3 rows (time spent 3047/3880 us) Or you can use user-defined variables to construct similar queries. nebula> $var = GO FROM \"player101\" OVER follow \\ YIELD follow._src AS s, follow._dst AS d; \\ FETCH PROP ON follow $var.s -> $var.d \\ YIELD follow.degree; +-------------+-------------+--------------+---------------+ | follow._src | follow._dst | follow._rank | follow.degree | +-------------+-------------+--------------+---------------+ | \"player101\" | \"player100\" | 0 | 95 | +-------------+-------------+--------------+---------------+ | \"player101\" | \"player102\" | 0 | 90 | +-------------+-------------+--------------+---------------+ | \"player101\" | \"player125\" | 0 | 95 | +-------------+-------------+--------------+---------------+ Got 3 rows (time spent 1891/2509 us) For more information about composite queries, see Composite queries (clause structure) .","title":"Use FETCH in composite queries"},{"location":"3.ngql-guide/7.general-query-statements/5.lookup/","text":"LOOKUP \u00b6 OpenCypher compatibility \u00b6 This page applies to nGQL extensions only. Syntax \u00b6 The LOOKUP statement retrieves data based on indexes. You can use LOOKUP for the following purposes: Search for the specific data based on conditions defined by the WHERE clause. List vertices with a tag: retrieve the VID of all vertices with a tag. List edges with an edge type: retrieve the source Vertex IDs, destination vertex IDs, and ranks of all edges with an edge type. Count the number of vertices or edges with a tag or an edge type. Prerequisites \u00b6 Before using the LOOKUP statement, make sure that relative indexes are created. For how to create indexes, see CREATE INDEX . Syntax \u00b6 LOOKUP ON {<vertex_tag> | <edge_type>} [WHERE <expression> [AND <expression> ...]] [YIELD <return_list>] <return_list> <prop_name> [AS <col_alias>] [, <prop_name> [AS <prop_alias>] ...] The WHERE clause filters data with the specified conditions. Both AND and OR are supported between different expressions. For more information, see WHERE . The YIELD clause specifies the results to be returned and the format of the results. If there is a WHERE clause but no YIELD clause: The Vertex ID is returned when LOOKUP a tag. The source vertex ID, destination vertex ID, and rank of the edge is returned when LOOKUP an edge type. Limitations of using WHERE in LOOKUP \u00b6 The WHERE clause in a LOOKUP statement does not support the following operations: $- and $^ . In relational expressions, expressions with field names on both sides of the operator are not supported, such as tagName.prop1> tagName.prop2 . Nested AliasProp expressions in operation expressions and function expressions are not supported. Range scan is not supported in the string-type index. The OR and XOR operations are not supported. Retrieve Vertices \u00b6 The following example returns vertices whose name is Tony Parker and tagged with player . nebula> CREATE TAG INDEX index_player ON player(name(30), age); nebula> REBUILD TAG INDEX index_player; +------------+ | New Job Id | +------------+ | 15 | +------------+ nebula> LOOKUP ON player WHERE player.name == \"Tony Parker\"; ============ | VertexID | ============ | 101 | ------------ nebula> LOOKUP ON player WHERE player.name == \"Tony Parker\" \\ YIELD player.name, player.age; ======================================= | VertexID | player.name | player.age | ======================================= | 101 | Tony Parker | 36 | --------------------------------------- nebula> LOOKUP ON player WHERE player.name == \"Kobe Bryant\" YIELD player.name AS name \\ | GO FROM $-.VertexID OVER serve \\ YIELD $-.name, serve.start_year, serve.end_year, $$.team.name; ================================================================== | $-.name | serve.start_year | serve.end_year | $$.team.name | ================================================================== | Kobe Bryant | 1996 | 2016 | Lakers | ------------------------------------------------------------------ Retrieve Edges \u00b6 The following example returns edges whose degree is 90 and the edge type is follow . nebula> CREATE EDGE INDEX index_follow ON follow(degree); nebula> REBUILD EDGE INDEX index_follow; +------------+ | New Job Id | +------------+ | 62 | +------------+ nebula> LOOKUP ON follow WHERE follow.degree == 90; ============================= | SrcVID | DstVID | Ranking | ============================= | 100 | 106 | 0 | ----------------------------- nebula> LOOKUP ON follow WHERE follow.degree == 90 YIELD follow.degree; ============================================= | SrcVID | DstVID | Ranking | follow.degree | ============================================= | 100 | 106 | 0 | 90 | --------------------------------------------- nebula> LOOKUP ON follow WHERE follow.degree == 60 YIELD follow.degree AS Degree \\ | GO FROM $-.DstVID OVER serve \\ YIELD $-.DstVID, serve.start_year, serve.end_year, $$.team.name; ================================================================ | $-.DstVID | serve.start_year | serve.end_year | $$.team.name | ================================================================ | 105 | 2010 | 2018 | Spurs | ---------------------------------------------------------------- | 105 | 2009 | 2010 | Cavaliers | ---------------------------------------------------------------- | 105 | 2018 | 2019 | Raptors | ---------------------------------------------------------------- List vertices or edges with a tag or an edge type \u00b6 To list vertices or edges with a tag or an edge type, at least one index must exist on the tag or the edge type, or its property. For example, if there is a player tag with a name property and an age property, to retrieve the VID of all vertices tagged with player , there has to be an index on the player tag itself, the name property, or the age property. The following example shows how to retrieve the VID of all vertices tagged with player . nebula> CREATE TAG player(name string,age int); Execution succeeded (time spent 3235/3865 us) nebula> CREATE TAG INDEX player_index on player(); Execution succeeded (time spent 3486/4124 us) nebula> REBUILD TAG INDEX player_index; +------------+ | New Job Id | +------------+ | 66 | +------------+ nebula> INSERT VERTEX player(name,age) VALUES \"player100\":(\"Tim Duncan\", 42), \"player101\":(\"Tony Parker\", 36); Execution succeeded (time spent 1695/2268 us) nebula> LOOKUP ON player; +-------------+ | _vid | +-------------+ | \"player100\" | +-------------+ | \"player101\" | +-------------+ Got 2 rows (time spent 1514/2070 us) The following example shows how to retrieve the source Vertex IDs, destination vertex IDs, and ranks of all edges of the like edge type. nebula)> CREATE EDGE like(likeness int); Execution succeeded (time spent 3710/4483 us) nebula)> CREATE EDGE INDEX like_index on like(); Execution succeeded (time spent 3422/4026 us) nebula> REBUILD EDGE INDEX like_index; +------------+ | New Job Id | +------------+ | 88 | +------------+ nebula)> INSERT EDGE like(likeness) values \"player100\"->\"player101\":(95); Execution succeeded (time spent 1638/2351 us) nebula)> LOOKUP ON like; +-------------+----------+-------------+ | _src | _ranking | _dst | +-------------+----------+-------------+ | \"player100\" | 0 | \"player101\" | +-------------+----------+-------------+ Got 1 rows (time spent 1163/1748 us) Count the numbers of vertices or edges \u00b6 The following example shows how to count the number of vertices tagged with player and edges of the like edge type. nebula> LOOKUP ON player | YIELD COUNT(*) AS Player_Number; +---------------+ | Player_Number | +---------------+ | 2 | +---------------+ Got 1 rows (time spent 1158/1864 us) nebula> LOOKUP ON like | YIELD COUNT(*) AS Like_Number; +-------------+ | Like_Number | +-------------+ | 1 | +-------------+ Got 1 rows (time spent 1190/1970 us) FAQ \u00b6 Error code 411 \u00b6 [ ERROR ( -8 )] : Unknown error ( 411 ) : Error code 411 shows there is no valid index for the current WHERE filter. Nebula Graph uses the left matching mode to select indexes. That is, columns in the WHERE filter must be in the first N columns of the index. For example: nebula> CREATE TAG INDEX example_index ON TAG t(p1, p2, p3); -- Create an index for the first 3 properties of tag t nebula> LOOKUP ON t WHERE p2 == 1 and p3 == 1; -- Not supported nebula> LOOKUP ON t WHERE p1 == 1; -- Supported nebula> LOOKUP ON t WHERE p1 == 1 and p2 == 1; -- Supported nebula> LOOKUP ON t WHERE p1 == 1 and p2 == 1 and p3 == 1; -- Supported No valid index found \u00b6 No valid index found If your query filter contains a string type field, Nebula Graph selects the index that matches all the fields. For example: nebula> CREATE TAG t1 (c1 string, c2 int); nebula> CREATE TAG INDEX i1 ON t1 (c1, c2); nebula> LOOKUP ON t1 WHERE t1.c1 == \"a\"; -- Index i1 is invalid nebula> LOOKUP ON t1 WHERE t1.c1 == \"a\" and t1.c2 == 1; -- Index i1 is valid","title":"LOOKUP"},{"location":"3.ngql-guide/7.general-query-statements/5.lookup/#lookup","text":"","title":"LOOKUP"},{"location":"3.ngql-guide/7.general-query-statements/5.lookup/#opencypher_compatibility","text":"This page applies to nGQL extensions only.","title":"OpenCypher compatibility"},{"location":"3.ngql-guide/7.general-query-statements/5.lookup/#syntax","text":"The LOOKUP statement retrieves data based on indexes. You can use LOOKUP for the following purposes: Search for the specific data based on conditions defined by the WHERE clause. List vertices with a tag: retrieve the VID of all vertices with a tag. List edges with an edge type: retrieve the source Vertex IDs, destination vertex IDs, and ranks of all edges with an edge type. Count the number of vertices or edges with a tag or an edge type.","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/5.lookup/#prerequisites","text":"Before using the LOOKUP statement, make sure that relative indexes are created. For how to create indexes, see CREATE INDEX .","title":"Prerequisites"},{"location":"3.ngql-guide/7.general-query-statements/5.lookup/#syntax_1","text":"LOOKUP ON {<vertex_tag> | <edge_type>} [WHERE <expression> [AND <expression> ...]] [YIELD <return_list>] <return_list> <prop_name> [AS <col_alias>] [, <prop_name> [AS <prop_alias>] ...] The WHERE clause filters data with the specified conditions. Both AND and OR are supported between different expressions. For more information, see WHERE . The YIELD clause specifies the results to be returned and the format of the results. If there is a WHERE clause but no YIELD clause: The Vertex ID is returned when LOOKUP a tag. The source vertex ID, destination vertex ID, and rank of the edge is returned when LOOKUP an edge type.","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/5.lookup/#limitations_of_using_where_in_lookup","text":"The WHERE clause in a LOOKUP statement does not support the following operations: $- and $^ . In relational expressions, expressions with field names on both sides of the operator are not supported, such as tagName.prop1> tagName.prop2 . Nested AliasProp expressions in operation expressions and function expressions are not supported. Range scan is not supported in the string-type index. The OR and XOR operations are not supported.","title":"Limitations of using WHERE in LOOKUP"},{"location":"3.ngql-guide/7.general-query-statements/5.lookup/#retrieve_vertices","text":"The following example returns vertices whose name is Tony Parker and tagged with player . nebula> CREATE TAG INDEX index_player ON player(name(30), age); nebula> REBUILD TAG INDEX index_player; +------------+ | New Job Id | +------------+ | 15 | +------------+ nebula> LOOKUP ON player WHERE player.name == \"Tony Parker\"; ============ | VertexID | ============ | 101 | ------------ nebula> LOOKUP ON player WHERE player.name == \"Tony Parker\" \\ YIELD player.name, player.age; ======================================= | VertexID | player.name | player.age | ======================================= | 101 | Tony Parker | 36 | --------------------------------------- nebula> LOOKUP ON player WHERE player.name == \"Kobe Bryant\" YIELD player.name AS name \\ | GO FROM $-.VertexID OVER serve \\ YIELD $-.name, serve.start_year, serve.end_year, $$.team.name; ================================================================== | $-.name | serve.start_year | serve.end_year | $$.team.name | ================================================================== | Kobe Bryant | 1996 | 2016 | Lakers | ------------------------------------------------------------------","title":"Retrieve Vertices"},{"location":"3.ngql-guide/7.general-query-statements/5.lookup/#retrieve_edges","text":"The following example returns edges whose degree is 90 and the edge type is follow . nebula> CREATE EDGE INDEX index_follow ON follow(degree); nebula> REBUILD EDGE INDEX index_follow; +------------+ | New Job Id | +------------+ | 62 | +------------+ nebula> LOOKUP ON follow WHERE follow.degree == 90; ============================= | SrcVID | DstVID | Ranking | ============================= | 100 | 106 | 0 | ----------------------------- nebula> LOOKUP ON follow WHERE follow.degree == 90 YIELD follow.degree; ============================================= | SrcVID | DstVID | Ranking | follow.degree | ============================================= | 100 | 106 | 0 | 90 | --------------------------------------------- nebula> LOOKUP ON follow WHERE follow.degree == 60 YIELD follow.degree AS Degree \\ | GO FROM $-.DstVID OVER serve \\ YIELD $-.DstVID, serve.start_year, serve.end_year, $$.team.name; ================================================================ | $-.DstVID | serve.start_year | serve.end_year | $$.team.name | ================================================================ | 105 | 2010 | 2018 | Spurs | ---------------------------------------------------------------- | 105 | 2009 | 2010 | Cavaliers | ---------------------------------------------------------------- | 105 | 2018 | 2019 | Raptors | ----------------------------------------------------------------","title":"Retrieve Edges"},{"location":"3.ngql-guide/7.general-query-statements/5.lookup/#list_vertices_or_edges_with_a_tag_or_an_edge_type","text":"To list vertices or edges with a tag or an edge type, at least one index must exist on the tag or the edge type, or its property. For example, if there is a player tag with a name property and an age property, to retrieve the VID of all vertices tagged with player , there has to be an index on the player tag itself, the name property, or the age property. The following example shows how to retrieve the VID of all vertices tagged with player . nebula> CREATE TAG player(name string,age int); Execution succeeded (time spent 3235/3865 us) nebula> CREATE TAG INDEX player_index on player(); Execution succeeded (time spent 3486/4124 us) nebula> REBUILD TAG INDEX player_index; +------------+ | New Job Id | +------------+ | 66 | +------------+ nebula> INSERT VERTEX player(name,age) VALUES \"player100\":(\"Tim Duncan\", 42), \"player101\":(\"Tony Parker\", 36); Execution succeeded (time spent 1695/2268 us) nebula> LOOKUP ON player; +-------------+ | _vid | +-------------+ | \"player100\" | +-------------+ | \"player101\" | +-------------+ Got 2 rows (time spent 1514/2070 us) The following example shows how to retrieve the source Vertex IDs, destination vertex IDs, and ranks of all edges of the like edge type. nebula)> CREATE EDGE like(likeness int); Execution succeeded (time spent 3710/4483 us) nebula)> CREATE EDGE INDEX like_index on like(); Execution succeeded (time spent 3422/4026 us) nebula> REBUILD EDGE INDEX like_index; +------------+ | New Job Id | +------------+ | 88 | +------------+ nebula)> INSERT EDGE like(likeness) values \"player100\"->\"player101\":(95); Execution succeeded (time spent 1638/2351 us) nebula)> LOOKUP ON like; +-------------+----------+-------------+ | _src | _ranking | _dst | +-------------+----------+-------------+ | \"player100\" | 0 | \"player101\" | +-------------+----------+-------------+ Got 1 rows (time spent 1163/1748 us)","title":"List vertices or edges with a tag or an edge type"},{"location":"3.ngql-guide/7.general-query-statements/5.lookup/#count_the_numbers_of_vertices_or_edges","text":"The following example shows how to count the number of vertices tagged with player and edges of the like edge type. nebula> LOOKUP ON player | YIELD COUNT(*) AS Player_Number; +---------------+ | Player_Number | +---------------+ | 2 | +---------------+ Got 1 rows (time spent 1158/1864 us) nebula> LOOKUP ON like | YIELD COUNT(*) AS Like_Number; +-------------+ | Like_Number | +-------------+ | 1 | +-------------+ Got 1 rows (time spent 1190/1970 us)","title":"Count the numbers of vertices or edges"},{"location":"3.ngql-guide/7.general-query-statements/5.lookup/#faq","text":"","title":"FAQ"},{"location":"3.ngql-guide/7.general-query-statements/5.lookup/#error_code_411","text":"[ ERROR ( -8 )] : Unknown error ( 411 ) : Error code 411 shows there is no valid index for the current WHERE filter. Nebula Graph uses the left matching mode to select indexes. That is, columns in the WHERE filter must be in the first N columns of the index. For example: nebula> CREATE TAG INDEX example_index ON TAG t(p1, p2, p3); -- Create an index for the first 3 properties of tag t nebula> LOOKUP ON t WHERE p2 == 1 and p3 == 1; -- Not supported nebula> LOOKUP ON t WHERE p1 == 1; -- Supported nebula> LOOKUP ON t WHERE p1 == 1 and p2 == 1; -- Supported nebula> LOOKUP ON t WHERE p1 == 1 and p2 == 1 and p3 == 1; -- Supported","title":"Error code 411"},{"location":"3.ngql-guide/7.general-query-statements/5.lookup/#no_valid_index_found","text":"No valid index found If your query filter contains a string type field, Nebula Graph selects the index that matches all the fields. For example: nebula> CREATE TAG t1 (c1 string, c2 int); nebula> CREATE TAG INDEX i1 ON t1 (c1, c2); nebula> LOOKUP ON t1 WHERE t1.c1 == \"a\"; -- Index i1 is invalid nebula> LOOKUP ON t1 WHERE t1.c1 == \"a\" and t1.c2 == 1; -- Index i1 is valid","title":"No valid index found"},{"location":"3.ngql-guide/7.general-query-statements/7.unwind/","text":"UNWIND \u00b6 The UNWIND statement splits a list into separated rows. UNWIND can function as an individual statement or a clause in a statement. Syntax \u00b6 UNWIND <list> AS <alias> <RETURN clause> Split a list \u00b6 The following example splits the list [1,2,3] into three rows. nebula) [nba]> UNWIND [1,2,3] AS n RETURN n; +---+ | n | +---+ | 1 | +---+ | 2 | +---+ | 3 | +---+ Got 3 rows (time spent 806/2126 us) Return a list with distinct items \u00b6 Use UNWIND and WITH DISTINCT together to return a list with distinct items. Example 1 \u00b6 The following statement: Splits the list [1,1,2,2,3,3] into rows. Removes duplicated rows. Sorts the rows. Transforms the rows to a list. nebula> WITH [1,1,2,2,3,3] AS n \\ UNWIND n AS r \\ WITH DISTINCT r AS r \\ ORDER BY r \\ RETURN collect(r); +------------+ | COLLECT(r) | +------------+ | [1, 2, 3] | +------------+ Got 1 rows (time spent 307/1043 us) Example 2 \u00b6 The following statement: Outputs the vertices on the matched path into a list. Splits the list into rows. Removes duplicated rows. Transforms the rows to a list. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})--(v2) \\ WITH nodes(p) AS n \\ UNWIND n AS r \\ WITH DISTINCT r AS r \\ RETURN collect(r); +----------------------------------------------------------------------------------------------------------------------+ | COLLECT(r) | +----------------------------------------------------------------------------------------------------------------------+ | [(\"player100\" :player{age: 42, name: \"Tim Duncan\"}), (\"player101\" :player{age: 36, name: \"Tony Parker\"}), (\"team204\" :team{name: \"Spurs\"}), (\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"}), (\"player125\" :player{age: 41, name: \"Manu Ginobili\"}), (\"player104\" :player{age: 32, name: \"Marco Belinelli\"}), (\"player144\" :player{age: 47, name: \"Shaquile O'Neal\"}), (\"player105\" :player{age: 31, name: \"Danny Green\"}), (\"player113\" :player{age: 29, name: \"Dejounte Murray\"}), (\"player107\" :player{age: 32, name: \"Aron Baynes\"}), (\"player109\" :player{age: 34, name: \"Tiago Splitter\"}), (\"player108\" :player{age: 36, name: \"Boris Diaw\"})] | +----------------------------------------------------------------------------------------------------------------------+ Got 1 rows (time spent 6157/6833 us)","title":"UNWIND"},{"location":"3.ngql-guide/7.general-query-statements/7.unwind/#unwind","text":"The UNWIND statement splits a list into separated rows. UNWIND can function as an individual statement or a clause in a statement.","title":"UNWIND"},{"location":"3.ngql-guide/7.general-query-statements/7.unwind/#syntax","text":"UNWIND <list> AS <alias> <RETURN clause>","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/7.unwind/#split_a_list","text":"The following example splits the list [1,2,3] into three rows. nebula) [nba]> UNWIND [1,2,3] AS n RETURN n; +---+ | n | +---+ | 1 | +---+ | 2 | +---+ | 3 | +---+ Got 3 rows (time spent 806/2126 us)","title":"Split a list"},{"location":"3.ngql-guide/7.general-query-statements/7.unwind/#return_a_list_with_distinct_items","text":"Use UNWIND and WITH DISTINCT together to return a list with distinct items.","title":"Return a list with distinct items"},{"location":"3.ngql-guide/7.general-query-statements/7.unwind/#example_1","text":"The following statement: Splits the list [1,1,2,2,3,3] into rows. Removes duplicated rows. Sorts the rows. Transforms the rows to a list. nebula> WITH [1,1,2,2,3,3] AS n \\ UNWIND n AS r \\ WITH DISTINCT r AS r \\ ORDER BY r \\ RETURN collect(r); +------------+ | COLLECT(r) | +------------+ | [1, 2, 3] | +------------+ Got 1 rows (time spent 307/1043 us)","title":"Example 1"},{"location":"3.ngql-guide/7.general-query-statements/7.unwind/#example_2","text":"The following statement: Outputs the vertices on the matched path into a list. Splits the list into rows. Removes duplicated rows. Transforms the rows to a list. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})--(v2) \\ WITH nodes(p) AS n \\ UNWIND n AS r \\ WITH DISTINCT r AS r \\ RETURN collect(r); +----------------------------------------------------------------------------------------------------------------------+ | COLLECT(r) | +----------------------------------------------------------------------------------------------------------------------+ | [(\"player100\" :player{age: 42, name: \"Tim Duncan\"}), (\"player101\" :player{age: 36, name: \"Tony Parker\"}), (\"team204\" :team{name: \"Spurs\"}), (\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"}), (\"player125\" :player{age: 41, name: \"Manu Ginobili\"}), (\"player104\" :player{age: 32, name: \"Marco Belinelli\"}), (\"player144\" :player{age: 47, name: \"Shaquile O'Neal\"}), (\"player105\" :player{age: 31, name: \"Danny Green\"}), (\"player113\" :player{age: 29, name: \"Dejounte Murray\"}), (\"player107\" :player{age: 32, name: \"Aron Baynes\"}), (\"player109\" :player{age: 34, name: \"Tiago Splitter\"}), (\"player108\" :player{age: 36, name: \"Boris Diaw\"})] | +----------------------------------------------------------------------------------------------------------------------+ Got 1 rows (time spent 6157/6833 us)","title":"Example 2"},{"location":"3.ngql-guide/7.general-query-statements/6.show/1.show-charset/","text":"SHOW CHARSET \u00b6 The SHOW CHARSET statement shows the available character sets. Currently available types are utf8 and utf8mb4. The default charset type is utf8. Nebula Graph extends the uft8 to support four-byte characters. Therefore utf8 and utf8mb4 are equivalent. Syntax \u00b6 SHOW CHARSET Example \u00b6 nebula> SHOW CHARSET; +---------+-----------------+-------------------+--------+ | Charset | Description | Default collation | Maxlen | +---------+-----------------+-------------------+--------+ | \"utf8\" | \"UTF-8 Unicode\" | \"utf8_bin\" | 4 | +---------+-----------------+-------------------+--------+ Got 1 rows (time spent 527/1269 us) The output of SHOW CHARSET is explained as follows: Column Description Charset The character set name. Description A description of the character set. Default collation The default collation for the character set. Maxlen The maximum number of bytes required to store one character.","title":"SHOW CHARSET"},{"location":"3.ngql-guide/7.general-query-statements/6.show/1.show-charset/#show_charset","text":"The SHOW CHARSET statement shows the available character sets. Currently available types are utf8 and utf8mb4. The default charset type is utf8. Nebula Graph extends the uft8 to support four-byte characters. Therefore utf8 and utf8mb4 are equivalent.","title":"SHOW CHARSET"},{"location":"3.ngql-guide/7.general-query-statements/6.show/1.show-charset/#syntax","text":"SHOW CHARSET","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/6.show/1.show-charset/#example","text":"nebula> SHOW CHARSET; +---------+-----------------+-------------------+--------+ | Charset | Description | Default collation | Maxlen | +---------+-----------------+-------------------+--------+ | \"utf8\" | \"UTF-8 Unicode\" | \"utf8_bin\" | 4 | +---------+-----------------+-------------------+--------+ Got 1 rows (time spent 527/1269 us) The output of SHOW CHARSET is explained as follows: Column Description Charset The character set name. Description A description of the character set. Default collation The default collation for the character set. Maxlen The maximum number of bytes required to store one character.","title":"Example"},{"location":"3.ngql-guide/7.general-query-statements/6.show/10.show-roles/","text":"SHOW ROLES \u00b6 The SHOW ROLES statement shows the roles that are assigned to a user account. The return message differs according to the role of the user who is running this statement: If the user is a GOD or ADMIN and is granted access to the specified graph space, Nebula Graph shows all roles in this graph space except for GOD . If the user is a DBA , USER , or GUEST and is granted access to the specified graph space, Nebula Graph shows the user's own role in this graph space. If the user doesn't have a role, PermissionError is returned. For more information about user roles, see Roles and privileges . Syntax \u00b6 SHOW ROLES IN <space_name> Example \u00b6 nebula> SHOW ROLES in nba; +---------+-----------+ | Account | Role Type | +---------+-----------+ | \"user1\" | \"ADMIN\" | +---------+-----------+ Got 1 rows (time spent 789/1594 us)","title":"SHOW ROLES"},{"location":"3.ngql-guide/7.general-query-statements/6.show/10.show-roles/#show_roles","text":"The SHOW ROLES statement shows the roles that are assigned to a user account. The return message differs according to the role of the user who is running this statement: If the user is a GOD or ADMIN and is granted access to the specified graph space, Nebula Graph shows all roles in this graph space except for GOD . If the user is a DBA , USER , or GUEST and is granted access to the specified graph space, Nebula Graph shows the user's own role in this graph space. If the user doesn't have a role, PermissionError is returned. For more information about user roles, see Roles and privileges .","title":"SHOW ROLES"},{"location":"3.ngql-guide/7.general-query-statements/6.show/10.show-roles/#syntax","text":"SHOW ROLES IN <space_name>","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/6.show/10.show-roles/#example","text":"nebula> SHOW ROLES in nba; +---------+-----------+ | Account | Role Type | +---------+-----------+ | \"user1\" | \"ADMIN\" | +---------+-----------+ Got 1 rows (time spent 789/1594 us)","title":"Example"},{"location":"3.ngql-guide/7.general-query-statements/6.show/11.show-snapshots/","text":"SHOW SNAPSHOTS \u00b6 The SHOW SNAPSHOTS statement shows all the snapshots. For how to create a snapshot and backup data, see Snapshot . Role requirement \u00b6 Only the root user who has the GOD role can use this statement. Syntax \u00b6 SHOW SNAPSHOTS Example \u00b6 nebula> SHOW SNAPSHOTS; +--------------------------------+---------+-----------------------------------------------------+ | Name | Status | Hosts | +--------------------------------+---------+-----------------------------------------------------+ | \"SNAPSHOT_2020_12_16_11_13_55\" | \"VALID\" | \"storaged0:9779, storaged1:9779, storaged2:9779\" | +--------------------------------+---------+-----------------------------------------------------+ | \"SNAPSHOT_2020_12_16_11_14_10\" | \"VALID\" | \"storaged0:9779, storaged1:9779, storaged2:9779\" | +--------------------------------+---------+-----------------------------------------------------+ Got 2 rows (time spent 762/1434 us)","title":"SHOW SNAPSHOTS"},{"location":"3.ngql-guide/7.general-query-statements/6.show/11.show-snapshots/#show_snapshots","text":"The SHOW SNAPSHOTS statement shows all the snapshots. For how to create a snapshot and backup data, see Snapshot .","title":"SHOW SNAPSHOTS"},{"location":"3.ngql-guide/7.general-query-statements/6.show/11.show-snapshots/#role_requirement","text":"Only the root user who has the GOD role can use this statement.","title":"Role requirement"},{"location":"3.ngql-guide/7.general-query-statements/6.show/11.show-snapshots/#syntax","text":"SHOW SNAPSHOTS","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/6.show/11.show-snapshots/#example","text":"nebula> SHOW SNAPSHOTS; +--------------------------------+---------+-----------------------------------------------------+ | Name | Status | Hosts | +--------------------------------+---------+-----------------------------------------------------+ | \"SNAPSHOT_2020_12_16_11_13_55\" | \"VALID\" | \"storaged0:9779, storaged1:9779, storaged2:9779\" | +--------------------------------+---------+-----------------------------------------------------+ | \"SNAPSHOT_2020_12_16_11_14_10\" | \"VALID\" | \"storaged0:9779, storaged1:9779, storaged2:9779\" | +--------------------------------+---------+-----------------------------------------------------+ Got 2 rows (time spent 762/1434 us)","title":"Example"},{"location":"3.ngql-guide/7.general-query-statements/6.show/12.show-spaces/","text":"SHOW SPACES \u00b6 The SHOW SPACES statement shows the graph spaces in Nebula Graph. For how to create a graph space, see CREATE SPACE . Syntax \u00b6 SHOW SPACES Example \u00b6 nebula> SHOW SPACES; +--------+ | Name | +--------+ | \"docs\" | +--------+ | \"nba\" | +--------+ Got 2 rows (time spent 968/1893 us)","title":"SHOW SPACES"},{"location":"3.ngql-guide/7.general-query-statements/6.show/12.show-spaces/#show_spaces","text":"The SHOW SPACES statement shows the graph spaces in Nebula Graph. For how to create a graph space, see CREATE SPACE .","title":"SHOW SPACES"},{"location":"3.ngql-guide/7.general-query-statements/6.show/12.show-spaces/#syntax","text":"SHOW SPACES","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/6.show/12.show-spaces/#example","text":"nebula> SHOW SPACES; +--------+ | Name | +--------+ | \"docs\" | +--------+ | \"nba\" | +--------+ Got 2 rows (time spent 968/1893 us)","title":"Example"},{"location":"3.ngql-guide/7.general-query-statements/6.show/14.show-stats/","text":"SHOW STATS \u00b6 The SHOW STATS statement shows the statistics of the graph space collected by the latest STATS job. The statistics list the following information: The number of vertices and edges in the graph space The number of vertices with each tag The number of edges of each edge type Prerequisites \u00b6 You have successfully run the SUBMIT JOB STATS statement in the graph space you want to collect statistics. For more information, see SUBMIT JOB STATS . NOTE: The result of the SHOW STATS statement is based on the last executed SUBMIT JOB STATS statement. If you want to update the result, run SUBMIT JOB STATS again. Syntax \u00b6 SHOW STATS Example \u00b6 nebula> USE nba; Execution succeeded (time spent 1075/1646 us) --Start a `STATS` job. nebula> SUBMIT JOB STATS; +------------+ | New Job Id | +------------+ | 98 | +------------+ Got 1 rows (time spent 2058/2609 us) --Make sure the job is finished. nebula> SHOW JOB 98; +----------------+---------------+------------+------------+------------+ | Job Id(TaskId) | Command(Dest) | Status | Start Time | Stop Time | +----------------+---------------+------------+------------+------------+ | 98 | \"STATS\" | \"FINISHED\" | 1606552675 | 1606552675 | +----------------+---------------+------------+------------+------------+ | 0 | \"storaged2\" | \"FINISHED\" | 1606552675 | 1606552675 | +----------------+---------------+------------+------------+------------+ | 1 | \"storaged0\" | \"FINISHED\" | 1606552675 | 1606552675 | +----------------+---------------+------------+------------+------------+ | 2 | \"storaged1\" | \"FINISHED\" | 1606552675 | 1606552675 | +----------------+---------------+------------+------------+------------+ Got 4 rows (time spent 1233/1924 us) --Check the statistics. nebula> SHOW STATS; +---------+------------+-------+ | Type | Name | Count | +---------+------------+-------+ | \"Tag\" | \"player\" | 51 | +---------+------------+-------+ | \"Tag\" | \"team\" | 30 | +---------+------------+-------+ | \"Edge\" | \"like\" | 81 | +---------+------------+-------+ | \"Edge\" | \"serve\" | 152 | +---------+------------+-------+ | \"Space\" | \"vertices\" | 81 | +---------+------------+-------+ | \"Space\" | \"edges\" | 233 | +---------+------------+-------+ Got 6 rows (time spent 996/1637 us)","title":"SHOW STATS"},{"location":"3.ngql-guide/7.general-query-statements/6.show/14.show-stats/#show_stats","text":"The SHOW STATS statement shows the statistics of the graph space collected by the latest STATS job. The statistics list the following information: The number of vertices and edges in the graph space The number of vertices with each tag The number of edges of each edge type","title":"SHOW STATS"},{"location":"3.ngql-guide/7.general-query-statements/6.show/14.show-stats/#prerequisites","text":"You have successfully run the SUBMIT JOB STATS statement in the graph space you want to collect statistics. For more information, see SUBMIT JOB STATS . NOTE: The result of the SHOW STATS statement is based on the last executed SUBMIT JOB STATS statement. If you want to update the result, run SUBMIT JOB STATS again.","title":"Prerequisites"},{"location":"3.ngql-guide/7.general-query-statements/6.show/14.show-stats/#syntax","text":"SHOW STATS","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/6.show/14.show-stats/#example","text":"nebula> USE nba; Execution succeeded (time spent 1075/1646 us) --Start a `STATS` job. nebula> SUBMIT JOB STATS; +------------+ | New Job Id | +------------+ | 98 | +------------+ Got 1 rows (time spent 2058/2609 us) --Make sure the job is finished. nebula> SHOW JOB 98; +----------------+---------------+------------+------------+------------+ | Job Id(TaskId) | Command(Dest) | Status | Start Time | Stop Time | +----------------+---------------+------------+------------+------------+ | 98 | \"STATS\" | \"FINISHED\" | 1606552675 | 1606552675 | +----------------+---------------+------------+------------+------------+ | 0 | \"storaged2\" | \"FINISHED\" | 1606552675 | 1606552675 | +----------------+---------------+------------+------------+------------+ | 1 | \"storaged0\" | \"FINISHED\" | 1606552675 | 1606552675 | +----------------+---------------+------------+------------+------------+ | 2 | \"storaged1\" | \"FINISHED\" | 1606552675 | 1606552675 | +----------------+---------------+------------+------------+------------+ Got 4 rows (time spent 1233/1924 us) --Check the statistics. nebula> SHOW STATS; +---------+------------+-------+ | Type | Name | Count | +---------+------------+-------+ | \"Tag\" | \"player\" | 51 | +---------+------------+-------+ | \"Tag\" | \"team\" | 30 | +---------+------------+-------+ | \"Edge\" | \"like\" | 81 | +---------+------------+-------+ | \"Edge\" | \"serve\" | 152 | +---------+------------+-------+ | \"Space\" | \"vertices\" | 81 | +---------+------------+-------+ | \"Space\" | \"edges\" | 233 | +---------+------------+-------+ Got 6 rows (time spent 996/1637 us)","title":"Example"},{"location":"3.ngql-guide/7.general-query-statements/6.show/15.show-tags-edges/","text":"SHOW TAGS/EDGES \u00b6 The SHOW TAGS or SHOW EDGES statement shows all tags or edge types in the current graph space. Syntax \u00b6 SHOW {TAGS | EDGES} Examples \u00b6 Show tags: nebula> SHOW TAGS; +----------+ | Name | +----------+ | \"player\" | +----------+ | \"star\" | +----------+ | \"team\" | +----------+ Got 3 rows (time spent 1461/2114 us) Show edge types\uff1a nebula> SHOW EDGES; +---------+ | Name | +---------+ | \"like\" | +---------+ | \"serve\" | +---------+ Got 2 rows (time spent 1039/1687 us)","title":"SHOW TAGS/EDGES"},{"location":"3.ngql-guide/7.general-query-statements/6.show/15.show-tags-edges/#show_tagsedges","text":"The SHOW TAGS or SHOW EDGES statement shows all tags or edge types in the current graph space.","title":"SHOW TAGS/EDGES"},{"location":"3.ngql-guide/7.general-query-statements/6.show/15.show-tags-edges/#syntax","text":"SHOW {TAGS | EDGES}","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/6.show/15.show-tags-edges/#examples","text":"Show tags: nebula> SHOW TAGS; +----------+ | Name | +----------+ | \"player\" | +----------+ | \"star\" | +----------+ | \"team\" | +----------+ Got 3 rows (time spent 1461/2114 us) Show edge types\uff1a nebula> SHOW EDGES; +---------+ | Name | +---------+ | \"like\" | +---------+ | \"serve\" | +---------+ Got 2 rows (time spent 1039/1687 us)","title":"Examples"},{"location":"3.ngql-guide/7.general-query-statements/6.show/16.show-users/","text":"SHOW USERS \u00b6 The SHOW USERS statement shows the user information. Role requirement \u00b6 Only the root user who has the GOD role can use this statement. Syntax \u00b6 SHOW USERS Example \u00b6 nebula> SHOW USERS; +---------+ | Account | +---------+ | \"root\" | +---------+ | \"user1\" | +---------+ Got 2 rows (time spent 964/1691 us)","title":"SHOW USERS"},{"location":"3.ngql-guide/7.general-query-statements/6.show/16.show-users/#show_users","text":"The SHOW USERS statement shows the user information.","title":"SHOW USERS"},{"location":"3.ngql-guide/7.general-query-statements/6.show/16.show-users/#role_requirement","text":"Only the root user who has the GOD role can use this statement.","title":"Role requirement"},{"location":"3.ngql-guide/7.general-query-statements/6.show/16.show-users/#syntax","text":"SHOW USERS","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/6.show/16.show-users/#example","text":"nebula> SHOW USERS; +---------+ | Account | +---------+ | \"root\" | +---------+ | \"user1\" | +---------+ Got 2 rows (time spent 964/1691 us)","title":"Example"},{"location":"3.ngql-guide/7.general-query-statements/6.show/2.show-collation/","text":"SHOW COLLATION \u00b6 The SHOW COLLATION statement shows the collations supported by Nebula Graph. Currently available types are: utf8_bin, utf8_general_ci, utf8mb4_bin, and utf8mb4_general_ci. When the character set is utf8, the default collate is utf8_bin; when the character set is utf8mb4, the default collate is utf8mb4_bin. Both utf8_general_ci and utf8mb4_general_ci are case-insensitive. Syntax \u00b6 SHOW COLLATION Example \u00b6 nebula> SHOW COLLATION; +------------+---------+ | Collation | Charset | +------------+---------+ | \"utf8_bin\" | \"utf8\" | +------------+---------+ Got 1 rows (time spent 413/1034 us) The output of SHOW CHARSET is described as follows: Column Description Collation The collation name. Charset The name of the character set with which the collation is associated.","title":"SHOW COLLATION"},{"location":"3.ngql-guide/7.general-query-statements/6.show/2.show-collation/#show_collation","text":"The SHOW COLLATION statement shows the collations supported by Nebula Graph. Currently available types are: utf8_bin, utf8_general_ci, utf8mb4_bin, and utf8mb4_general_ci. When the character set is utf8, the default collate is utf8_bin; when the character set is utf8mb4, the default collate is utf8mb4_bin. Both utf8_general_ci and utf8mb4_general_ci are case-insensitive.","title":"SHOW COLLATION"},{"location":"3.ngql-guide/7.general-query-statements/6.show/2.show-collation/#syntax","text":"SHOW COLLATION","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/6.show/2.show-collation/#example","text":"nebula> SHOW COLLATION; +------------+---------+ | Collation | Charset | +------------+---------+ | \"utf8_bin\" | \"utf8\" | +------------+---------+ Got 1 rows (time spent 413/1034 us) The output of SHOW CHARSET is described as follows: Column Description Collation The collation name. Charset The name of the character set with which the collation is associated.","title":"Example"},{"location":"3.ngql-guide/7.general-query-statements/6.show/4.show-create-space/","text":"SHOW CREATE SPACE \u00b6 The SHOW CREATE SPACE statement shows the basic information of the specified graph space, such as the nGQL for creating the graph space, the partition number, the replica number. For details about the graph space information, see CREATE SPACE . Syntax \u00b6 SHOW CREATE SPACE <space_name> Example \u00b6 nebula> SHOW CREATE SPACE nba; +-------+--------------------------------------------------------------------------------------------------------------------------------+ | Space | Create Space | +-------+--------------------------------------------------------------------------------------------------------------------------------+ | \"nba\" | \"CREATE SPACE `nba` (partition_num = 10, replica_factor = 1, charset = utf8, collate = utf8_bin, vid_type = FIXED_STRING(32))\" | +-------+--------------------------------------------------------------------------------------------------------------------------------+ Got 1 rows (time spent 1747/2562 us)","title":"SHOW CREATE SPACE"},{"location":"3.ngql-guide/7.general-query-statements/6.show/4.show-create-space/#show_create_space","text":"The SHOW CREATE SPACE statement shows the basic information of the specified graph space, such as the nGQL for creating the graph space, the partition number, the replica number. For details about the graph space information, see CREATE SPACE .","title":"SHOW CREATE SPACE"},{"location":"3.ngql-guide/7.general-query-statements/6.show/4.show-create-space/#syntax","text":"SHOW CREATE SPACE <space_name>","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/6.show/4.show-create-space/#example","text":"nebula> SHOW CREATE SPACE nba; +-------+--------------------------------------------------------------------------------------------------------------------------------+ | Space | Create Space | +-------+--------------------------------------------------------------------------------------------------------------------------------+ | \"nba\" | \"CREATE SPACE `nba` (partition_num = 10, replica_factor = 1, charset = utf8, collate = utf8_bin, vid_type = FIXED_STRING(32))\" | +-------+--------------------------------------------------------------------------------------------------------------------------------+ Got 1 rows (time spent 1747/2562 us)","title":"Example"},{"location":"3.ngql-guide/7.general-query-statements/6.show/5.show-create-tags-edges/","text":"SHOW CREATE TAG/EDGE \u00b6 The SHOW CREATE TAG or SHOW CREATE EDGE statement shows the basic information of the specified tag or edge type. For details about the tag or edge type information, see CREATE TAG and CREATE EDGE . Syntax \u00b6 SHOW CREATE {TAG <tag_name> | EDGE <edge_name>} Example \u00b6 nebula> SHOW CREATE TAG player; +----------+-----------------------------------+ | Tag | Create Tag | +----------+-----------------------------------+ | \"player\" | \"CREATE TAG `player` ( | | | `name` string NULL, | | | `age` int64 NULL | | | ) ttl_duration = 0, ttl_col = \"\" | +----------+-----------------------------------+","title":"SHOW CREATE TAGS/EDGES"},{"location":"3.ngql-guide/7.general-query-statements/6.show/5.show-create-tags-edges/#show_create_tagedge","text":"The SHOW CREATE TAG or SHOW CREATE EDGE statement shows the basic information of the specified tag or edge type. For details about the tag or edge type information, see CREATE TAG and CREATE EDGE .","title":"SHOW CREATE TAG/EDGE"},{"location":"3.ngql-guide/7.general-query-statements/6.show/5.show-create-tags-edges/#syntax","text":"SHOW CREATE {TAG <tag_name> | EDGE <edge_name>}","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/6.show/5.show-create-tags-edges/#example","text":"nebula> SHOW CREATE TAG player; +----------+-----------------------------------+ | Tag | Create Tag | +----------+-----------------------------------+ | \"player\" | \"CREATE TAG `player` ( | | | `name` string NULL, | | | `age` int64 NULL | | | ) ttl_duration = 0, ttl_col = \"\" | +----------+-----------------------------------+","title":"Example"},{"location":"3.ngql-guide/7.general-query-statements/6.show/6.show-hosts/","text":"SHOW HOSTS \u00b6 The SHOW HOSTS statement lists graph/storage/meta hosts registered by the Meta Service. Syntax \u00b6 SHOW HOSTS [GRAPH/STORAGE/META] Example \u00b6 nebula> SHOW HOSTS; +-------------+-------+----------+--------------+---------------------+------------------------+ | Host | Port | Status | Leader count | Leader distribution | Partition distribution | +-------------+-------+----------+--------------+---------------------+------------------------+ | \"storaged0\" | 9779 | \"ONLINE\" | 8 | \"docs:5, nba:3\" | \"docs:5, nba:3\" | +-------------+-------+----------+--------------+---------------------+------------------------+ | \"storaged1\" | 9779 | \"ONLINE\" | 9 | \"nba:4, docs:5\" | \"docs:5, nba:4\" | +-------------+-------+----------+--------------+---------------------+------------------------+ | \"storaged2\" | 9779 | \"ONLINE\" | 8 | \"nba:3, docs:5\" | \"docs:5, nba:3\" | +-------------+-------+----------+--------------+---------------------+------------------------+ Got 3 rows (time spent 866/1411 us) nebula> SHOW HOSTS GRAPH; +-------------+------+----------+---------+--------------+ | Host | Port | Status | Role | Git Info Sha | +-------------+------+----------+---------+--------------+ | \"12.16.2.3\" | 9669 | \"ONLINE\" | \"GRAPH\" | \"761f22b\" | nebula> SHOW HOSTS STORAGE; +-------------+------+----------+-----------+--------------+ | Host | Port | Status | Role | Git Info Sha | +-------------+------+----------+-----------+--------------+ | \"12.16.2.3\" | 9779 | \"ONLINE\" | \"STORAGE\" | \"761f22b\" | nebula> SHOW HOSTS META; +-------------+------+----------+--------+--------------+ | Host | Port | Status | Role | Git Info Sha | +-------------+------+----------+--------+--------------+ | \"12.16.2.3\" | 9559 | \"ONLINE\" | \"META\" | \"761f22b\" |","title":"SHOW HOSTS"},{"location":"3.ngql-guide/7.general-query-statements/6.show/6.show-hosts/#show_hosts","text":"The SHOW HOSTS statement lists graph/storage/meta hosts registered by the Meta Service.","title":"SHOW HOSTS"},{"location":"3.ngql-guide/7.general-query-statements/6.show/6.show-hosts/#syntax","text":"SHOW HOSTS [GRAPH/STORAGE/META]","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/6.show/6.show-hosts/#example","text":"nebula> SHOW HOSTS; +-------------+-------+----------+--------------+---------------------+------------------------+ | Host | Port | Status | Leader count | Leader distribution | Partition distribution | +-------------+-------+----------+--------------+---------------------+------------------------+ | \"storaged0\" | 9779 | \"ONLINE\" | 8 | \"docs:5, nba:3\" | \"docs:5, nba:3\" | +-------------+-------+----------+--------------+---------------------+------------------------+ | \"storaged1\" | 9779 | \"ONLINE\" | 9 | \"nba:4, docs:5\" | \"docs:5, nba:4\" | +-------------+-------+----------+--------------+---------------------+------------------------+ | \"storaged2\" | 9779 | \"ONLINE\" | 8 | \"nba:3, docs:5\" | \"docs:5, nba:3\" | +-------------+-------+----------+--------------+---------------------+------------------------+ Got 3 rows (time spent 866/1411 us) nebula> SHOW HOSTS GRAPH; +-------------+------+----------+---------+--------------+ | Host | Port | Status | Role | Git Info Sha | +-------------+------+----------+---------+--------------+ | \"12.16.2.3\" | 9669 | \"ONLINE\" | \"GRAPH\" | \"761f22b\" | nebula> SHOW HOSTS STORAGE; +-------------+------+----------+-----------+--------------+ | Host | Port | Status | Role | Git Info Sha | +-------------+------+----------+-----------+--------------+ | \"12.16.2.3\" | 9779 | \"ONLINE\" | \"STORAGE\" | \"761f22b\" | nebula> SHOW HOSTS META; +-------------+------+----------+--------+--------------+ | Host | Port | Status | Role | Git Info Sha | +-------------+------+----------+--------+--------------+ | \"12.16.2.3\" | 9559 | \"ONLINE\" | \"META\" | \"761f22b\" |","title":"Example"},{"location":"3.ngql-guide/7.general-query-statements/6.show/7.show-index-status/","text":"SHOW INDEX STATUS \u00b6 The SHOW INDEX STATUS statement shows the status of jobs that rebuild native indexes. You can find out whether a native index is successfully rebuilt or not. Syntax \u00b6 SHOW {TAG | EDGE} INDEX STATUS Example \u00b6 nebula> SHOW TAG INDEX STATUS; +----------------+--------------+ | Name | Index Status | +----------------+--------------+ | \"like_index_0\" | \"FINISHED\" | +----------------+--------------+ | \"like1\" | \"FINISHED\" | +----------------+--------------+ Got 2 rows (time spent 1456/2122 us) Related topics \u00b6 Job manager and the JOB statements REBUILD NATIVE INDEX","title":"SHOW INDEX STATUS"},{"location":"3.ngql-guide/7.general-query-statements/6.show/7.show-index-status/#show_index_status","text":"The SHOW INDEX STATUS statement shows the status of jobs that rebuild native indexes. You can find out whether a native index is successfully rebuilt or not.","title":"SHOW INDEX STATUS"},{"location":"3.ngql-guide/7.general-query-statements/6.show/7.show-index-status/#syntax","text":"SHOW {TAG | EDGE} INDEX STATUS","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/6.show/7.show-index-status/#example","text":"nebula> SHOW TAG INDEX STATUS; +----------------+--------------+ | Name | Index Status | +----------------+--------------+ | \"like_index_0\" | \"FINISHED\" | +----------------+--------------+ | \"like1\" | \"FINISHED\" | +----------------+--------------+ Got 2 rows (time spent 1456/2122 us)","title":"Example"},{"location":"3.ngql-guide/7.general-query-statements/6.show/7.show-index-status/#related_topics","text":"Job manager and the JOB statements REBUILD NATIVE INDEX","title":"Related topics"},{"location":"3.ngql-guide/7.general-query-statements/6.show/8.show-indexes/","text":"SHOW INDEXES \u00b6 The SHOW INDEXES statement shows the names of existing native indexes. Syntax \u00b6 SHOW {TAG | EDGE} INDEXES Example \u00b6 nebula> SHOW TAG INDEXES; +------------------+ | Names | +------------------+ | \"play_age_0\" | +------------------+ | \"player_index_0\" | +------------------+ | \"player_index_1\" | +------------------+ | \"star\" | +------------------+ Got 4 rows (time spent 1450/2087 us)","title":"SHOW INDEXES"},{"location":"3.ngql-guide/7.general-query-statements/6.show/8.show-indexes/#show_indexes","text":"The SHOW INDEXES statement shows the names of existing native indexes.","title":"SHOW INDEXES"},{"location":"3.ngql-guide/7.general-query-statements/6.show/8.show-indexes/#syntax","text":"SHOW {TAG | EDGE} INDEXES","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/6.show/8.show-indexes/#example","text":"nebula> SHOW TAG INDEXES; +------------------+ | Names | +------------------+ | \"play_age_0\" | +------------------+ | \"player_index_0\" | +------------------+ | \"player_index_1\" | +------------------+ | \"star\" | +------------------+ Got 4 rows (time spent 1450/2087 us)","title":"Example"},{"location":"3.ngql-guide/7.general-query-statements/6.show/9.show-parts/","text":"SHOW PARTS \u00b6 The SHOW PARTS statement shows the information of a specified partition or all partitions in a graph space. Syntax \u00b6 SHOW PARTS [<part_id>] Examples \u00b6 Show the information of all partitions: nebula> SHOW PARTS; +--------------+-------------------+-------------------+-------+ | Partition ID | Leader | Peers | Losts | +--------------+-------------------+-------------------+-------+ | 1 | \"storaged1:44500\" | \"storaged1:44500\" | \"\" | +--------------+-------------------+-------------------+-------+ | 2 | \"storaged2:44500\" | \"storaged2:44500\" | \"\" | +--------------+-------------------+-------------------+-------+ | 3 | \"storaged0:44500\" | \"storaged0:44500\" | \"\" | +--------------+-------------------+-------------------+-------+ | 4 | \"storaged1:44500\" | \"storaged1:44500\" | \"\" | +--------------+-------------------+-------------------+-------+ | 5 | \"storaged2:44500\" | \"storaged2:44500\" | \"\" | +--------------+-------------------+-------------------+-------+ | 6 | \"storaged0:44500\" | \"storaged0:44500\" | \"\" | +--------------+-------------------+-------------------+-------+ | 7 | \"storaged1:44500\" | \"storaged1:44500\" | \"\" | +--------------+-------------------+-------------------+-------+ | 8 | \"storaged2:44500\" | \"storaged2:44500\" | \"\" | +--------------+-------------------+-------------------+-------+ | 9 | \"storaged0:44500\" | \"storaged0:44500\" | \"\" | +--------------+-------------------+-------------------+-------+ | 10 | \"storaged1:44500\" | \"storaged1:44500\" | \"\" | +--------------+-------------------+-------------------+-------+ Got 10 rows (time spent 2317/3512 us) Show the information of partition 1: nebula> SHOW PARTS 1; +--------------+-------------------+-------------------+-------+ | Partition ID | Leader | Peers | Losts | +--------------+-------------------+-------------------+-------+ | 1 | \"storaged1:44500\" | \"storaged1:44500\" | \"\" | +--------------+-------------------+-------------------+-------+ Got 1 rows (time spent 1055/1678 us)","title":"SHOW PARTS"},{"location":"3.ngql-guide/7.general-query-statements/6.show/9.show-parts/#show_parts","text":"The SHOW PARTS statement shows the information of a specified partition or all partitions in a graph space.","title":"SHOW PARTS"},{"location":"3.ngql-guide/7.general-query-statements/6.show/9.show-parts/#syntax","text":"SHOW PARTS [<part_id>]","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/6.show/9.show-parts/#examples","text":"Show the information of all partitions: nebula> SHOW PARTS; +--------------+-------------------+-------------------+-------+ | Partition ID | Leader | Peers | Losts | +--------------+-------------------+-------------------+-------+ | 1 | \"storaged1:44500\" | \"storaged1:44500\" | \"\" | +--------------+-------------------+-------------------+-------+ | 2 | \"storaged2:44500\" | \"storaged2:44500\" | \"\" | +--------------+-------------------+-------------------+-------+ | 3 | \"storaged0:44500\" | \"storaged0:44500\" | \"\" | +--------------+-------------------+-------------------+-------+ | 4 | \"storaged1:44500\" | \"storaged1:44500\" | \"\" | +--------------+-------------------+-------------------+-------+ | 5 | \"storaged2:44500\" | \"storaged2:44500\" | \"\" | +--------------+-------------------+-------------------+-------+ | 6 | \"storaged0:44500\" | \"storaged0:44500\" | \"\" | +--------------+-------------------+-------------------+-------+ | 7 | \"storaged1:44500\" | \"storaged1:44500\" | \"\" | +--------------+-------------------+-------------------+-------+ | 8 | \"storaged2:44500\" | \"storaged2:44500\" | \"\" | +--------------+-------------------+-------------------+-------+ | 9 | \"storaged0:44500\" | \"storaged0:44500\" | \"\" | +--------------+-------------------+-------------------+-------+ | 10 | \"storaged1:44500\" | \"storaged1:44500\" | \"\" | +--------------+-------------------+-------------------+-------+ Got 10 rows (time spent 2317/3512 us) Show the information of partition 1: nebula> SHOW PARTS 1; +--------------+-------------------+-------------------+-------+ | Partition ID | Leader | Peers | Losts | +--------------+-------------------+-------------------+-------+ | 1 | \"storaged1:44500\" | \"storaged1:44500\" | \"\" | +--------------+-------------------+-------------------+-------+ Got 1 rows (time spent 1055/1678 us)","title":"Examples"},{"location":"3.ngql-guide/8.clauses-and-options/group-by/","text":"GROUP BY \u00b6 OpenCypher Compatibility \u00b6 This page applies to nGQL extensions only. Use GROUP BY in nGQL-extensions ONLY to aggregate data. OpenCypher uses the count() function to aggregate data. nebula> MATCH (v:player)<-[:follow]-(:player) RETURN v.name AS Name, count(*) as cnt ORDER BY cnt DESC +----------------------+--------------+ | Name | Follower_Num | +----------------------+--------------+ | \"Tim Duncan\" | 10 | +----------------------+--------------+ | \"LeBron James\" | 6 | +----------------------+--------------+ | \"Tony Parker\" | 5 | +----------------------+--------------+ | \"Manu Ginobili\" | 4 | +----------------------+--------------+ | \"Chris Paul\" | 4 | +----------------------+--------------+ | \"Tracy McGrady\" | 3 | +----------------------+--------------+ | \"Dwyane Wade\" | 3 | +----------------------+--------------+ ... Syntax \u00b6 The GROUP BY clause groups the rows with the same value into summary rows. Then operations such as counting, sorting, and calculation can be applied. GROUP BY works after the pipe symbol and before a YIELD clause. | GROUP BY <var> YIELD <var>, <aggregation_function(var)> aggregation_function can be avg(), sum(), max(), min(), count(), collect(), std() . Examples \u00b6 The following statement finds all the vertices connected directly to vertex \"player100\" , groups the result set by player names, and counts the times that the names show up in the result set. nebula> GO FROM \"player100\" \\ OVER follow BIDIRECT \\ YIELD $$.player.name as Name | \\ GROUP BY $-.Name \\ YIELD $-.Name as Player, count(*) AS Name_Count; +---------------------+------------+ | Player | Name_Count | +---------------------+------------+ | \"Tiago Splitter\" | 1 | +---------------------+------------+ | \"Aron Baynes\" | 1 | +---------------------+------------+ | \"Boris Diaw\" | 1 | +---------------------+------------+ | \"Manu Ginobili\" | 2 | +---------------------+------------+ | \"Dejounte Murray\" | 1 | +---------------------+------------+ | \"Danny Green\" | 1 | +---------------------+------------+ | \"Tony Parker\" | 2 | +---------------------+------------+ | \"Shaquille O'Neal\" | 1 | +---------------------+------------+ | \"LaMarcus Aldridge\" | 1 | +---------------------+------------+ | \"Marco Belinelli\" | 1 | +---------------------+------------+ Got 10 rows (time spent 3527/4423 us) Group and calculate with functions \u00b6 The following statement finds all the players followed by \"player100\" , returns these players as player and the property of the follow edge as degree . These players are grouped and the sum of their degree values is returned. nebula> GO FROM \"player100\" OVER follow YIELD follow._src AS player, follow.degree AS degree | GROUP BY $-.player YIELD sum($-.degree); +----------------+ | sum($-.degree) | +----------------+ | 190 | +----------------+ Got 1 rows (time spent 2851/3624 us) For more information about functions, see Functions .","title":"GROUP BY"},{"location":"3.ngql-guide/8.clauses-and-options/group-by/#group_by","text":"","title":"GROUP BY"},{"location":"3.ngql-guide/8.clauses-and-options/group-by/#opencypher_compatibility","text":"This page applies to nGQL extensions only. Use GROUP BY in nGQL-extensions ONLY to aggregate data. OpenCypher uses the count() function to aggregate data. nebula> MATCH (v:player)<-[:follow]-(:player) RETURN v.name AS Name, count(*) as cnt ORDER BY cnt DESC +----------------------+--------------+ | Name | Follower_Num | +----------------------+--------------+ | \"Tim Duncan\" | 10 | +----------------------+--------------+ | \"LeBron James\" | 6 | +----------------------+--------------+ | \"Tony Parker\" | 5 | +----------------------+--------------+ | \"Manu Ginobili\" | 4 | +----------------------+--------------+ | \"Chris Paul\" | 4 | +----------------------+--------------+ | \"Tracy McGrady\" | 3 | +----------------------+--------------+ | \"Dwyane Wade\" | 3 | +----------------------+--------------+ ...","title":"OpenCypher Compatibility"},{"location":"3.ngql-guide/8.clauses-and-options/group-by/#syntax","text":"The GROUP BY clause groups the rows with the same value into summary rows. Then operations such as counting, sorting, and calculation can be applied. GROUP BY works after the pipe symbol and before a YIELD clause. | GROUP BY <var> YIELD <var>, <aggregation_function(var)> aggregation_function can be avg(), sum(), max(), min(), count(), collect(), std() .","title":"Syntax"},{"location":"3.ngql-guide/8.clauses-and-options/group-by/#examples","text":"The following statement finds all the vertices connected directly to vertex \"player100\" , groups the result set by player names, and counts the times that the names show up in the result set. nebula> GO FROM \"player100\" \\ OVER follow BIDIRECT \\ YIELD $$.player.name as Name | \\ GROUP BY $-.Name \\ YIELD $-.Name as Player, count(*) AS Name_Count; +---------------------+------------+ | Player | Name_Count | +---------------------+------------+ | \"Tiago Splitter\" | 1 | +---------------------+------------+ | \"Aron Baynes\" | 1 | +---------------------+------------+ | \"Boris Diaw\" | 1 | +---------------------+------------+ | \"Manu Ginobili\" | 2 | +---------------------+------------+ | \"Dejounte Murray\" | 1 | +---------------------+------------+ | \"Danny Green\" | 1 | +---------------------+------------+ | \"Tony Parker\" | 2 | +---------------------+------------+ | \"Shaquille O'Neal\" | 1 | +---------------------+------------+ | \"LaMarcus Aldridge\" | 1 | +---------------------+------------+ | \"Marco Belinelli\" | 1 | +---------------------+------------+ Got 10 rows (time spent 3527/4423 us)","title":"Examples"},{"location":"3.ngql-guide/8.clauses-and-options/group-by/#group_and_calculate_with_functions","text":"The following statement finds all the players followed by \"player100\" , returns these players as player and the property of the follow edge as degree . These players are grouped and the sum of their degree values is returned. nebula> GO FROM \"player100\" OVER follow YIELD follow._src AS player, follow.degree AS degree | GROUP BY $-.player YIELD sum($-.degree); +----------------+ | sum($-.degree) | +----------------+ | 190 | +----------------+ Got 1 rows (time spent 2851/3624 us) For more information about functions, see Functions .","title":"Group and calculate with functions"},{"location":"3.ngql-guide/8.clauses-and-options/limit/","text":"LIMIT AND SKIP \u00b6 The LIMIT clause constrains the number of rows in the output. The Syntax in openCypher and nGQL-extension are different. NGQL-extension: A pipe | must be used. And an offset can be ignored. OpenCypher style: No pipes are permitted. Use Skip to indicate offset. NOTE : When using LIMIT (in either syntax above), it is important to use an ORDER BY clause that constrains the output into a unique order. Otherwise, you will get an unpredictable subset of the output. nGQL-extension syntax \u00b6 In nGQL-extension, LIMIT works the same as in SQL , and must be used with pipe | . The LIMIT clause accepts one or two arguments. The values of both arguments must be non-negative integers. YIELD <var> [| LIMIT [<offset_value>,] <number_rows>] var: The columns or calculations that you wish to sort. number_rows: It constrains the number of rows to return. For example, LIMIT 10 would return the first 10 rows. offset_value(Optional): It defines from which row to start including the rows in the output. The offset starts from zero. Examples \u00b6 nebula> GO FROM \"player100\" OVER follow REVERSELY YIELD $$.player.name AS Friend, $$.player.age AS Age | ORDER BY Age,Friend | LIMIT 1, 3; +-------------------+-----+ | Friend | Age | +-------------------+-----+ | \"Danny Green\" | 31 | +-------------------+-----+ | \"Aron Baynes\" | 32 | +-------------------+-----+ | \"Marco Belinelli\" | 32 | +-------------------+-----+ OpenCypher Syntax \u00b6 RETURN <var> [SKIP <offset>] [LIMIT <number_rows>] Parameter Description offset Optional. It specifies the number of rows to be skipped. The offset starts from zero. number_rows It specifies the number of rows to be returned. It can be a non-negative integer or an expression that outputs a non-negative integer. Either offset or number_rows can accept an expression, which value must be a non-negative integer. NOTE: Fraction expressions composed of two integers are automatically floored to integers. For example, 8/6 is floored to 1. Examples \u00b6 Return a specific number of rows. To return the top N rows from the result, use LIMIT <N> as follows: nebula> MATCH (v:player) RETURN v.name AS Name, v.age AS Age \\ ORDER BY Age LIMIT 5; +-------------------------+-----+ | Name | Age | +-------------------------+-----+ | \"Luka Doncic\" | 20 | +-------------------------+-----+ | \"Ben Simmons\" | 22 | +-------------------------+-----+ | \"Kristaps Porzingis\" | 23 | +-------------------------+-----+ | \"Giannis Antetokounmpo\" | 24 | +-------------------------+-----+ | \"Kyle Anderson\" | 25 | +-------------------------+-----+ nebula> MATCH (v:player) RETURN v.name AS Name, v.age AS Age \\ ORDER BY Age LIMIT rand32(5); +-------------------------+-----+ | Name | Age | +-------------------------+-----+ | \"Luka Doncic\" | 20 | +-------------------------+-----+ | \"Ben Simmons\" | 22 | +-------------------------+-----+ | \"Kristaps Porzingis\" | 23 | +-------------------------+-----+ | \"Giannis Antetokounmpo\" | 24 | +-------------------------+-----+ SKIP-syntax \u00b6 You can use SKIP <N> to skip the top N rows from the result and return the rest of the result. nebula> MATCH (v:player{name:\"Tim Duncan\"}) --> (v2) \\ RETURN v2.name AS Name, v2.age AS Age \\ ORDER BY Age DESC SKIP 1; +-----------------+-----+ | Name | Age | +-----------------+-----+ | \"Manu Ginobili\" | 41 | +-----------------+-----+ | \"Tony Parker\" | 36 | +-----------------+-----+ nebula> MATCH (v:player{name:\"Tim Duncan\"}) --> (v2) \\ RETURN v2.name AS Name, v2.age AS Age \\ ORDER BY Age DESC SKIP 1+1; +---------------+-----+ | Name | Age | +---------------+-----+ | \"Tony Parker\" | 36 | +---------------+-----+ You can use SKIP and LIMIT together to return the middle N rows. nebula> MATCH (v:player{name:\"Tim Duncan\"}) --> (v2) \\ RETURN v2.name AS Name, v2.age AS Age \\ ORDER BY Age DESC SKIP 1 LIMIT 1; +-----------------+-----+ | Name | Age | +-----------------+-----+ | \"Manu Ginobili\" | 41 | +-----------------+-----+","title":"LIMIT and SKIP"},{"location":"3.ngql-guide/8.clauses-and-options/limit/#limit_and_skip","text":"The LIMIT clause constrains the number of rows in the output. The Syntax in openCypher and nGQL-extension are different. NGQL-extension: A pipe | must be used. And an offset can be ignored. OpenCypher style: No pipes are permitted. Use Skip to indicate offset. NOTE : When using LIMIT (in either syntax above), it is important to use an ORDER BY clause that constrains the output into a unique order. Otherwise, you will get an unpredictable subset of the output.","title":"LIMIT AND SKIP"},{"location":"3.ngql-guide/8.clauses-and-options/limit/#ngql-extension_syntax","text":"In nGQL-extension, LIMIT works the same as in SQL , and must be used with pipe | . The LIMIT clause accepts one or two arguments. The values of both arguments must be non-negative integers. YIELD <var> [| LIMIT [<offset_value>,] <number_rows>] var: The columns or calculations that you wish to sort. number_rows: It constrains the number of rows to return. For example, LIMIT 10 would return the first 10 rows. offset_value(Optional): It defines from which row to start including the rows in the output. The offset starts from zero.","title":"nGQL-extension syntax"},{"location":"3.ngql-guide/8.clauses-and-options/limit/#examples","text":"nebula> GO FROM \"player100\" OVER follow REVERSELY YIELD $$.player.name AS Friend, $$.player.age AS Age | ORDER BY Age,Friend | LIMIT 1, 3; +-------------------+-----+ | Friend | Age | +-------------------+-----+ | \"Danny Green\" | 31 | +-------------------+-----+ | \"Aron Baynes\" | 32 | +-------------------+-----+ | \"Marco Belinelli\" | 32 | +-------------------+-----+","title":"Examples"},{"location":"3.ngql-guide/8.clauses-and-options/limit/#opencypher_syntax","text":"RETURN <var> [SKIP <offset>] [LIMIT <number_rows>] Parameter Description offset Optional. It specifies the number of rows to be skipped. The offset starts from zero. number_rows It specifies the number of rows to be returned. It can be a non-negative integer or an expression that outputs a non-negative integer. Either offset or number_rows can accept an expression, which value must be a non-negative integer. NOTE: Fraction expressions composed of two integers are automatically floored to integers. For example, 8/6 is floored to 1.","title":"OpenCypher Syntax"},{"location":"3.ngql-guide/8.clauses-and-options/limit/#examples_1","text":"Return a specific number of rows. To return the top N rows from the result, use LIMIT <N> as follows: nebula> MATCH (v:player) RETURN v.name AS Name, v.age AS Age \\ ORDER BY Age LIMIT 5; +-------------------------+-----+ | Name | Age | +-------------------------+-----+ | \"Luka Doncic\" | 20 | +-------------------------+-----+ | \"Ben Simmons\" | 22 | +-------------------------+-----+ | \"Kristaps Porzingis\" | 23 | +-------------------------+-----+ | \"Giannis Antetokounmpo\" | 24 | +-------------------------+-----+ | \"Kyle Anderson\" | 25 | +-------------------------+-----+ nebula> MATCH (v:player) RETURN v.name AS Name, v.age AS Age \\ ORDER BY Age LIMIT rand32(5); +-------------------------+-----+ | Name | Age | +-------------------------+-----+ | \"Luka Doncic\" | 20 | +-------------------------+-----+ | \"Ben Simmons\" | 22 | +-------------------------+-----+ | \"Kristaps Porzingis\" | 23 | +-------------------------+-----+ | \"Giannis Antetokounmpo\" | 24 | +-------------------------+-----+","title":"Examples"},{"location":"3.ngql-guide/8.clauses-and-options/limit/#skip-syntax","text":"You can use SKIP <N> to skip the top N rows from the result and return the rest of the result. nebula> MATCH (v:player{name:\"Tim Duncan\"}) --> (v2) \\ RETURN v2.name AS Name, v2.age AS Age \\ ORDER BY Age DESC SKIP 1; +-----------------+-----+ | Name | Age | +-----------------+-----+ | \"Manu Ginobili\" | 41 | +-----------------+-----+ | \"Tony Parker\" | 36 | +-----------------+-----+ nebula> MATCH (v:player{name:\"Tim Duncan\"}) --> (v2) \\ RETURN v2.name AS Name, v2.age AS Age \\ ORDER BY Age DESC SKIP 1+1; +---------------+-----+ | Name | Age | +---------------+-----+ | \"Tony Parker\" | 36 | +---------------+-----+ You can use SKIP and LIMIT together to return the middle N rows. nebula> MATCH (v:player{name:\"Tim Duncan\"}) --> (v2) \\ RETURN v2.name AS Name, v2.age AS Age \\ ORDER BY Age DESC SKIP 1 LIMIT 1; +-----------------+-----+ | Name | Age | +-----------------+-----+ | \"Manu Ginobili\" | 41 | +-----------------+-----+","title":"SKIP-syntax"},{"location":"3.ngql-guide/8.clauses-and-options/order-by/","text":"ORDER BY \u00b6 The ORDER BY clause specifies the order of the rows in the output. NGQL-extension: You must use a pipe ( | ) and an ORDER BY clause after YIELD clause. OpenCypher style: no pipe is permitted. ORDER BY follows a RETURN clause. There are two order options: ASC : Ascending. ASC is the default order. DESC : Descending. An order option takes effect only when the expression before it is used for sorting the results. nGQL-extension Syntax \u00b6 <YIELD clause> ORDER BY <expression> [ASC | DESC] [, <expression> [ASC | DESC] ...] Examples \u00b6 nebula> FETCH PROP ON player \"player100\", \"player101\", \"player102\", \"player103\" YIELD player.age AS age, player.name AS name \\ | ORDER BY age ASC, name DESC; +-------------+-----+---------------------+ | VertexID | age | name | +-------------+-----+---------------------+ | \"player103\" | 32 | \"Rudy Gay\" | +-------------+-----+---------------------+ | \"player102\" | 33 | \"LaMarcus Aldridge\" | +-------------+-----+---------------------+ | \"player101\" | 36 | \"Tony Parker\" | +-------------+-----+---------------------+ | \"player100\" | 42 | \"Tim Duncan\" | +-------------+-----+---------------------+ OpenCypher Syntax \u00b6 <RETURN clause> ORDER BY <expression> [ASC | DESC] [, <expression> [ASC | DESC] ...] An order option takes effect only when the expression before it is used for sorting the results. Examples \u00b6 nebula> MATCH (v:player) RETURN v.name AS Name, v.age AS Age ORDER BY Name DESC; +-----------------+-----+ | Name | Age | +-----------------+-----+ | \"Yao Ming\" | 38 | +-----------------+-----+ | \"Vince Carter\" | 42 | +-----------------+-----+ | \"Tracy McGrady\" | 39 | +-----------------+-----+ | \"Tony Parker\" | 36 | +-----------------+-----+ | \"Tim Duncan\" | 42 | +-----------------+-----+ ... nebula> MATCH (v:player) RETURN v.age AS Age, v.name AS Name ORDER BY Age DESC, Name ASC +-----+-------------------+ | Age | Name | +-----+-------------------+ | 47 | \"Shaquille O'Neal\" | +-----+-------------------+ | 46 | \"Grant Hill\" | +-----+-------------------+ | 45 | \"Jason Kidd\" | +-----+-------------------+ | 45 | \"Steve Nash\" | +-----+-------------------+ ... In the preceding example, nGQL sorts the rows by Age first. If multiple people are of the same age, nGQL sorts them by Name . Order by NULL values \u00b6 nGQL lists NULL values at the end of the output for ascending sorting, and at the start for descending sorting. nebula> MATCH (v:player{name:\"Tim Duncan\"}) --> (v2) \\ RETURN v2.name AS Name, v2.age AS Age \\ ORDER BY Age; +-----------------+----------+ | Name | Age | +-----------------+----------+ | \"Tony Parker\" | 36 | +-----------------+----------+ | \"Manu Ginobili\" | 41 | +-----------------+----------+ | \"Spurs\" | __NULL__ | +-----------------+----------+ Got 3 rows (time spent 3089/3719 us) nebula> MATCH (v:player{name:\"Tim Duncan\"}) --> (v2) \\ RETURN v2.name AS Name, v2.age AS Age \\ ORDER BY Age DESC; +-----------------+----------+ | Name | Age | +-----------------+----------+ | \"Spurs\" | __NULL__ | +-----------------+----------+ | \"Manu Ginobili\" | 41 | +-----------------+----------+ | \"Tony Parker\" | 36 | +-----------------+----------+ Got 3 rows (time spent 2851/3360 us)","title":"ORDER BY"},{"location":"3.ngql-guide/8.clauses-and-options/order-by/#order_by","text":"The ORDER BY clause specifies the order of the rows in the output. NGQL-extension: You must use a pipe ( | ) and an ORDER BY clause after YIELD clause. OpenCypher style: no pipe is permitted. ORDER BY follows a RETURN clause. There are two order options: ASC : Ascending. ASC is the default order. DESC : Descending. An order option takes effect only when the expression before it is used for sorting the results.","title":"ORDER BY"},{"location":"3.ngql-guide/8.clauses-and-options/order-by/#ngql-extension_syntax","text":"<YIELD clause> ORDER BY <expression> [ASC | DESC] [, <expression> [ASC | DESC] ...]","title":"nGQL-extension Syntax"},{"location":"3.ngql-guide/8.clauses-and-options/order-by/#examples","text":"nebula> FETCH PROP ON player \"player100\", \"player101\", \"player102\", \"player103\" YIELD player.age AS age, player.name AS name \\ | ORDER BY age ASC, name DESC; +-------------+-----+---------------------+ | VertexID | age | name | +-------------+-----+---------------------+ | \"player103\" | 32 | \"Rudy Gay\" | +-------------+-----+---------------------+ | \"player102\" | 33 | \"LaMarcus Aldridge\" | +-------------+-----+---------------------+ | \"player101\" | 36 | \"Tony Parker\" | +-------------+-----+---------------------+ | \"player100\" | 42 | \"Tim Duncan\" | +-------------+-----+---------------------+","title":"Examples"},{"location":"3.ngql-guide/8.clauses-and-options/order-by/#opencypher_syntax","text":"<RETURN clause> ORDER BY <expression> [ASC | DESC] [, <expression> [ASC | DESC] ...] An order option takes effect only when the expression before it is used for sorting the results.","title":"OpenCypher Syntax"},{"location":"3.ngql-guide/8.clauses-and-options/order-by/#examples_1","text":"nebula> MATCH (v:player) RETURN v.name AS Name, v.age AS Age ORDER BY Name DESC; +-----------------+-----+ | Name | Age | +-----------------+-----+ | \"Yao Ming\" | 38 | +-----------------+-----+ | \"Vince Carter\" | 42 | +-----------------+-----+ | \"Tracy McGrady\" | 39 | +-----------------+-----+ | \"Tony Parker\" | 36 | +-----------------+-----+ | \"Tim Duncan\" | 42 | +-----------------+-----+ ... nebula> MATCH (v:player) RETURN v.age AS Age, v.name AS Name ORDER BY Age DESC, Name ASC +-----+-------------------+ | Age | Name | +-----+-------------------+ | 47 | \"Shaquille O'Neal\" | +-----+-------------------+ | 46 | \"Grant Hill\" | +-----+-------------------+ | 45 | \"Jason Kidd\" | +-----+-------------------+ | 45 | \"Steve Nash\" | +-----+-------------------+ ... In the preceding example, nGQL sorts the rows by Age first. If multiple people are of the same age, nGQL sorts them by Name .","title":"Examples"},{"location":"3.ngql-guide/8.clauses-and-options/order-by/#order_by_null_values","text":"nGQL lists NULL values at the end of the output for ascending sorting, and at the start for descending sorting. nebula> MATCH (v:player{name:\"Tim Duncan\"}) --> (v2) \\ RETURN v2.name AS Name, v2.age AS Age \\ ORDER BY Age; +-----------------+----------+ | Name | Age | +-----------------+----------+ | \"Tony Parker\" | 36 | +-----------------+----------+ | \"Manu Ginobili\" | 41 | +-----------------+----------+ | \"Spurs\" | __NULL__ | +-----------------+----------+ Got 3 rows (time spent 3089/3719 us) nebula> MATCH (v:player{name:\"Tim Duncan\"}) --> (v2) \\ RETURN v2.name AS Name, v2.age AS Age \\ ORDER BY Age DESC; +-----------------+----------+ | Name | Age | +-----------------+----------+ | \"Spurs\" | __NULL__ | +-----------------+----------+ | \"Manu Ginobili\" | 41 | +-----------------+----------+ | \"Tony Parker\" | 36 | +-----------------+----------+ Got 3 rows (time spent 2851/3360 us)","title":"Order by NULL values"},{"location":"3.ngql-guide/8.clauses-and-options/return/","text":"RETURN \u00b6 RETURN defines the output of an nGQL query. To return multiple fields, separate them with commas. RETURN can lead a clause or a statement: A RETURN clause works in openCypher statements in nGQL, such as MATCH or UNWIND . A RETURN statement works independently to output the result of an expression. OpenCypher compatibility \u00b6 This topic applies to the openCypher syntax in nGQL only. For nGQL extensions, use YIELD . RETURN does not support the following openCypher features yet. Return variables with uncommon characters, for example: MATCH (`non-english_characters`:player) \\ RETURN `non-english_characters`; Set a pattern in the RETURN clause and return all elements that this pattern matches, for example: MATCH (v:player) \\ RETURN (v)-[e]->(v2); NGQL compatibility \u00b6 In nGQL 1.0, RETURN works with nGQL extensions with the syntax RETURN <var_ref> IF <var_ref> IS NOT NULL . In nGQL 2.0, RETURN does not work with nGQL extensions. Return vertices \u00b6 Set a vertex in the RETURN clause to return it. nebula> MATCH (v:player) \\ RETURN v; +---------------------------------------------------------------+ | v | +---------------------------------------------------------------+ | (\"player104\" :player{age: 32, name: \"Marco Belinelli\"}) | +---------------------------------------------------------------+ | (\"player107\" :player{age: 32, name: \"Aron Baynes\"}) | +---------------------------------------------------------------+ | (\"player116\" :player{age: 34, name: \"LeBron James\"}) | +---------------------------------------------------------------+ | (\"player120\" :player{age: 29, name: \"James Harden\"}) | +---------------------------------------------------------------+ | (\"player125\" :player{age: 41, name: \"Manu Ginobili\"}) | +---------------------------------------------------------------+ ... Got 51 rows (time spent 7322/8244 us) Return edges \u00b6 Set an edge in the RETURN clause to return it. nebula> MATCH (v:player)-[e]->() \\ RETURN e; +------------------------------------------------------------------------------+ | e | +------------------------------------------------------------------------------+ | [:follow \"player104\"->\"player100\" @0 {degree: 55}] | +------------------------------------------------------------------------------+ | [:follow \"player104\"->\"player101\" @0 {degree: 50}] | +------------------------------------------------------------------------------+ | [:follow \"player104\"->\"player105\" @0 {degree: 60}] | +------------------------------------------------------------------------------+ | [:serve \"player104\"->\"team200\" @0 {end_year: 2009, start_year: 2007}] | +------------------------------------------------------------------------------+ | [:serve \"player104\"->\"team208\" @0 {end_year: 2016, start_year: 2015}] | +------------------------------------------------------------------------------+ ... Got 233 rows (time spent 14013/16136 us) Return properties \u00b6 To return a vertex or edge property, use the {<vertex_name>|<edge_name>}.<property> syntax. nebula> MATCH (v:player) \\ RETURN v.name, v.age \\ LIMIT 3; +-------------------+-------+ | v.name | v.age | +-------------------+-------+ | \"Rajon Rondo\" | 33 | +-------------------+-------+ | \"Rudy Gay\" | 32 | +-------------------+-------+ | \"Dejounte Murray\" | 29 | +-------------------+-------+ Got 3 rows (time spent 2663/3260 us) Return all elements \u00b6 To return all the elements matched on a pattern, use an asterisk (*). nebula> MATCH (v:player{name:\"Tim Duncan\"}) \\ RETURN *; +----------------------------------------------------+ | v | +----------------------------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | +----------------------------------------------------+ Got 1 rows (time spent 3332/3954 us) nebula> MATCH (v:player{name:\"Tim Duncan\"})-[e]->(v2) \\ RETURN *; +----------------------------------------------------+-----------------------------------------------------------------------+-------------------------------------------------------+ | v | e | v2 | +----------------------------------------------------+-----------------------------------------------------------------------+-------------------------------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | [:follow \"player100\"->\"player101\" @0 {degree: 95}] | (\"player101\" :player{age: 36, name: \"Tony Parker\"}) | +----------------------------------------------------+-----------------------------------------------------------------------+-------------------------------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | [:follow \"player100\"->\"player125\" @0 {degree: 95}] | (\"player125\" :player{age: 41, name: \"Manu Ginobili\"}) | +----------------------------------------------------+-----------------------------------------------------------------------+-------------------------------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | [:serve \"player100\"->\"team204\" @0 {end_year: 2016, start_year: 1997}] | (\"team204\" :team{name: \"Spurs\"}) | +----------------------------------------------------+-----------------------------------------------------------------------+-------------------------------------------------------+ Got 3 rows (time spent 3957/4696 us) Rename a field \u00b6 Use the AS <alias> syntax to rename a field in the output. nebula> MATCH (v:player{name:\"Tim Duncan\"})-[:serve]->(v2) \\ RETURN v2.name AS Team; +---------+ | Team | +---------+ | \"Spurs\" | +---------+ Got 1 rows (time spent 2370/3017 us) nebula> RETURN \"Amber\" AS Name; +---------+ | Name | +---------+ | \"Amber\" | +---------+ Got 1 rows (time spent 380/1097 us) Return a non-existing property \u00b6 If a property matched does not exist, NULL is returned. nebula> MATCH (v:player{name:\"Tim Duncan\"})-[e]->(v2) \\ RETURN v2.name, type(e), v2.age; +-----------------+----------+----------+ | v2.name | type(e) | v2.age | +-----------------+----------+----------+ | \"Tony Parker\" | \"follow\" | 36 | +-----------------+----------+----------+ | \"Manu Ginobili\" | \"follow\" | 41 | +-----------------+----------+----------+ | \"Spurs\" | \"serve\" | __NULL__ | +-----------------+----------+----------+ Got 3 rows (time spent 2976/3658 us) Return expression results \u00b6 To return the results of expressions such as literals, functions, or predicates, set them in a RETURN clause. nebula> MATCH (v:player{name:\"Tony Parker\"})-->(v2:player) \\ RETURN DISTINCT v2.name, \"Hello\"+\" graphs!\", v2.age > 35; +---------------------+------------------+-------------+ | v2.name | (Hello+ graphs!) | (v2.age>35) | +---------------------+------------------+-------------+ | \"Tim Duncan\" | \"Hello graphs!\" | true | +---------------------+------------------+-------------+ | \"LaMarcus Aldridge\" | \"Hello graphs!\" | false | +---------------------+------------------+-------------+ | \"Manu Ginobili\" | \"Hello graphs!\" | true | +---------------------+------------------+-------------+ Got 3 rows (time spent 2645/3237 us) nebula> RETURN 1+1; +-------+ | (1+1) | +-------+ | 2 | +-------+ Got 1 rows (time spent 319/1238 us) nebula> RETURN 3 > 1; +-------+ | (3>1) | +-------+ | true | +-------+ Got 1 rows (time spent 205/751 us) RETURN 1+1, rand32(1, 5); +-------+-------------+ | (1+1) | rand32(1,5) | +-------+-------------+ | 2 | 1 | +-------+-------------+ Got 1 rows (time spent 258/1098 us) Return unique fields \u00b6 Use DISTINCT to remove duplicate fields in the result set. // Before using DISTINCT nebula> MATCH (v:player{name:\"Tony Parker\"})--(v2:player) \\ RETURN v2.name, v2.age; +---------------------+--------+ | v2.name | v2.age | +---------------------+--------+ | \"Tim Duncan\" | 42 | +---------------------+--------+ | \"LaMarcus Aldridge\" | 33 | +---------------------+--------+ | \"Marco Belinelli\" | 32 | +---------------------+--------+ | \"Boris Diaw\" | 36 | +---------------------+--------+ | \"Dejounte Murray\" | 29 | +---------------------+--------+ | \"Tim Duncan\" | 42 | +---------------------+--------+ | \"LaMarcus Aldridge\" | 33 | +---------------------+--------+ | \"Manu Ginobili\" | 41 | +---------------------+--------+ Got 8 rows (time spent 3273/3893 us) // After using DISTINCT MATCH (v:player{name:\"Tony Parker\"})--(v2:player) RETURN DISTINCT v2.name, v2.age; +---------------------+--------+ | v2.name | v2.age | +---------------------+--------+ | \"Tim Duncan\" | 42 | +---------------------+--------+ | \"LaMarcus Aldridge\" | 33 | +---------------------+--------+ | \"Marco Belinelli\" | 32 | +---------------------+--------+ | \"Boris Diaw\" | 36 | +---------------------+--------+ | \"Dejounte Murray\" | 29 | +---------------------+--------+ | \"Manu Ginobili\" | 41 | +---------------------+--------+ Got 6 rows (time spent 3314/3897 us)","title":"RETURN"},{"location":"3.ngql-guide/8.clauses-and-options/return/#return","text":"RETURN defines the output of an nGQL query. To return multiple fields, separate them with commas. RETURN can lead a clause or a statement: A RETURN clause works in openCypher statements in nGQL, such as MATCH or UNWIND . A RETURN statement works independently to output the result of an expression.","title":"RETURN"},{"location":"3.ngql-guide/8.clauses-and-options/return/#opencypher_compatibility","text":"This topic applies to the openCypher syntax in nGQL only. For nGQL extensions, use YIELD . RETURN does not support the following openCypher features yet. Return variables with uncommon characters, for example: MATCH (`non-english_characters`:player) \\ RETURN `non-english_characters`; Set a pattern in the RETURN clause and return all elements that this pattern matches, for example: MATCH (v:player) \\ RETURN (v)-[e]->(v2);","title":"OpenCypher compatibility"},{"location":"3.ngql-guide/8.clauses-and-options/return/#ngql_compatibility","text":"In nGQL 1.0, RETURN works with nGQL extensions with the syntax RETURN <var_ref> IF <var_ref> IS NOT NULL . In nGQL 2.0, RETURN does not work with nGQL extensions.","title":"NGQL compatibility"},{"location":"3.ngql-guide/8.clauses-and-options/return/#return_vertices","text":"Set a vertex in the RETURN clause to return it. nebula> MATCH (v:player) \\ RETURN v; +---------------------------------------------------------------+ | v | +---------------------------------------------------------------+ | (\"player104\" :player{age: 32, name: \"Marco Belinelli\"}) | +---------------------------------------------------------------+ | (\"player107\" :player{age: 32, name: \"Aron Baynes\"}) | +---------------------------------------------------------------+ | (\"player116\" :player{age: 34, name: \"LeBron James\"}) | +---------------------------------------------------------------+ | (\"player120\" :player{age: 29, name: \"James Harden\"}) | +---------------------------------------------------------------+ | (\"player125\" :player{age: 41, name: \"Manu Ginobili\"}) | +---------------------------------------------------------------+ ... Got 51 rows (time spent 7322/8244 us)","title":"Return vertices"},{"location":"3.ngql-guide/8.clauses-and-options/return/#return_edges","text":"Set an edge in the RETURN clause to return it. nebula> MATCH (v:player)-[e]->() \\ RETURN e; +------------------------------------------------------------------------------+ | e | +------------------------------------------------------------------------------+ | [:follow \"player104\"->\"player100\" @0 {degree: 55}] | +------------------------------------------------------------------------------+ | [:follow \"player104\"->\"player101\" @0 {degree: 50}] | +------------------------------------------------------------------------------+ | [:follow \"player104\"->\"player105\" @0 {degree: 60}] | +------------------------------------------------------------------------------+ | [:serve \"player104\"->\"team200\" @0 {end_year: 2009, start_year: 2007}] | +------------------------------------------------------------------------------+ | [:serve \"player104\"->\"team208\" @0 {end_year: 2016, start_year: 2015}] | +------------------------------------------------------------------------------+ ... Got 233 rows (time spent 14013/16136 us)","title":"Return edges"},{"location":"3.ngql-guide/8.clauses-and-options/return/#return_properties","text":"To return a vertex or edge property, use the {<vertex_name>|<edge_name>}.<property> syntax. nebula> MATCH (v:player) \\ RETURN v.name, v.age \\ LIMIT 3; +-------------------+-------+ | v.name | v.age | +-------------------+-------+ | \"Rajon Rondo\" | 33 | +-------------------+-------+ | \"Rudy Gay\" | 32 | +-------------------+-------+ | \"Dejounte Murray\" | 29 | +-------------------+-------+ Got 3 rows (time spent 2663/3260 us)","title":"Return properties"},{"location":"3.ngql-guide/8.clauses-and-options/return/#return_all_elements","text":"To return all the elements matched on a pattern, use an asterisk (*). nebula> MATCH (v:player{name:\"Tim Duncan\"}) \\ RETURN *; +----------------------------------------------------+ | v | +----------------------------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | +----------------------------------------------------+ Got 1 rows (time spent 3332/3954 us) nebula> MATCH (v:player{name:\"Tim Duncan\"})-[e]->(v2) \\ RETURN *; +----------------------------------------------------+-----------------------------------------------------------------------+-------------------------------------------------------+ | v | e | v2 | +----------------------------------------------------+-----------------------------------------------------------------------+-------------------------------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | [:follow \"player100\"->\"player101\" @0 {degree: 95}] | (\"player101\" :player{age: 36, name: \"Tony Parker\"}) | +----------------------------------------------------+-----------------------------------------------------------------------+-------------------------------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | [:follow \"player100\"->\"player125\" @0 {degree: 95}] | (\"player125\" :player{age: 41, name: \"Manu Ginobili\"}) | +----------------------------------------------------+-----------------------------------------------------------------------+-------------------------------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | [:serve \"player100\"->\"team204\" @0 {end_year: 2016, start_year: 1997}] | (\"team204\" :team{name: \"Spurs\"}) | +----------------------------------------------------+-----------------------------------------------------------------------+-------------------------------------------------------+ Got 3 rows (time spent 3957/4696 us)","title":"Return all elements"},{"location":"3.ngql-guide/8.clauses-and-options/return/#rename_a_field","text":"Use the AS <alias> syntax to rename a field in the output. nebula> MATCH (v:player{name:\"Tim Duncan\"})-[:serve]->(v2) \\ RETURN v2.name AS Team; +---------+ | Team | +---------+ | \"Spurs\" | +---------+ Got 1 rows (time spent 2370/3017 us) nebula> RETURN \"Amber\" AS Name; +---------+ | Name | +---------+ | \"Amber\" | +---------+ Got 1 rows (time spent 380/1097 us)","title":"Rename a field"},{"location":"3.ngql-guide/8.clauses-and-options/return/#return_a_non-existing_property","text":"If a property matched does not exist, NULL is returned. nebula> MATCH (v:player{name:\"Tim Duncan\"})-[e]->(v2) \\ RETURN v2.name, type(e), v2.age; +-----------------+----------+----------+ | v2.name | type(e) | v2.age | +-----------------+----------+----------+ | \"Tony Parker\" | \"follow\" | 36 | +-----------------+----------+----------+ | \"Manu Ginobili\" | \"follow\" | 41 | +-----------------+----------+----------+ | \"Spurs\" | \"serve\" | __NULL__ | +-----------------+----------+----------+ Got 3 rows (time spent 2976/3658 us)","title":"Return a non-existing property"},{"location":"3.ngql-guide/8.clauses-and-options/return/#return_expression_results","text":"To return the results of expressions such as literals, functions, or predicates, set them in a RETURN clause. nebula> MATCH (v:player{name:\"Tony Parker\"})-->(v2:player) \\ RETURN DISTINCT v2.name, \"Hello\"+\" graphs!\", v2.age > 35; +---------------------+------------------+-------------+ | v2.name | (Hello+ graphs!) | (v2.age>35) | +---------------------+------------------+-------------+ | \"Tim Duncan\" | \"Hello graphs!\" | true | +---------------------+------------------+-------------+ | \"LaMarcus Aldridge\" | \"Hello graphs!\" | false | +---------------------+------------------+-------------+ | \"Manu Ginobili\" | \"Hello graphs!\" | true | +---------------------+------------------+-------------+ Got 3 rows (time spent 2645/3237 us) nebula> RETURN 1+1; +-------+ | (1+1) | +-------+ | 2 | +-------+ Got 1 rows (time spent 319/1238 us) nebula> RETURN 3 > 1; +-------+ | (3>1) | +-------+ | true | +-------+ Got 1 rows (time spent 205/751 us) RETURN 1+1, rand32(1, 5); +-------+-------------+ | (1+1) | rand32(1,5) | +-------+-------------+ | 2 | 1 | +-------+-------------+ Got 1 rows (time spent 258/1098 us)","title":"Return expression results"},{"location":"3.ngql-guide/8.clauses-and-options/return/#return_unique_fields","text":"Use DISTINCT to remove duplicate fields in the result set. // Before using DISTINCT nebula> MATCH (v:player{name:\"Tony Parker\"})--(v2:player) \\ RETURN v2.name, v2.age; +---------------------+--------+ | v2.name | v2.age | +---------------------+--------+ | \"Tim Duncan\" | 42 | +---------------------+--------+ | \"LaMarcus Aldridge\" | 33 | +---------------------+--------+ | \"Marco Belinelli\" | 32 | +---------------------+--------+ | \"Boris Diaw\" | 36 | +---------------------+--------+ | \"Dejounte Murray\" | 29 | +---------------------+--------+ | \"Tim Duncan\" | 42 | +---------------------+--------+ | \"LaMarcus Aldridge\" | 33 | +---------------------+--------+ | \"Manu Ginobili\" | 41 | +---------------------+--------+ Got 8 rows (time spent 3273/3893 us) // After using DISTINCT MATCH (v:player{name:\"Tony Parker\"})--(v2:player) RETURN DISTINCT v2.name, v2.age; +---------------------+--------+ | v2.name | v2.age | +---------------------+--------+ | \"Tim Duncan\" | 42 | +---------------------+--------+ | \"LaMarcus Aldridge\" | 33 | +---------------------+--------+ | \"Marco Belinelli\" | 32 | +---------------------+--------+ | \"Boris Diaw\" | 36 | +---------------------+--------+ | \"Dejounte Murray\" | 29 | +---------------------+--------+ | \"Manu Ginobili\" | 41 | +---------------------+--------+ Got 6 rows (time spent 3314/3897 us)","title":"Return unique fields"},{"location":"3.ngql-guide/8.clauses-and-options/ttl-options/","text":"TTL \u00b6 TTL indicates time to live. Use the TTL options to specify a timeout for a property. Once timed out, the property expires. OpenCypher Compatibility \u00b6 This topic applies to nGQL extensions only. Precautions \u00b6 You CANNOT modify a property with TTL options on it. TTL options and indexes CANNOT coexist on a tag or an edge type. Not even if you try to set them on different properties. Data expiration and deletion \u00b6 Vertex property expiration \u00b6 Vertex property expiration has the following impact. If a vertex has only one tag, once a property of the vertex expires, the vertex expires. If a vertex has multiple tags, once a property of the vertex expires, properties bound to the same tag with the expired property also expires, but the vertex does not expire and other properties of it remain untouched. Edge property expiration \u00b6 Since an edge can have only one edge type, once an edge property expires, the edge expires. Data deletion \u00b6 The expired data are still stored on the disk, but queries will filter them out. Nebula Graph automatically deletes the expired data and reclaims the disk space during the next compaction . TTL options \u00b6 The nGQL TTL feature has the following options. Option Description ttl_col Specifies the property to set a timeout on. The data type of the property must be int or timestamp. ttl_duration Specifies the timeout adds-on value in seconds. The value must be a non-negative int64 number. A property expires if the sum of its value and the ttl_duration value is greater than the current timestamp. If the ttl_duration value is 0, the property never expires. Use TTL options \u00b6 You must use the TTL options together to set a valid timeout on a property. Set a timeout if a tag or an edge type exists \u00b6 If a tag or an edge type is already created, to set a timeout on a property bound to the tag or edge type, use ALTER to update the tag or edge type. // Create a tag. nebula> CREATE TAG t1 (a timestamp); Execution succeeded (time spent 4172/5377 us) // Use ALTER to update the tag and set the TTL options. nebula> ALTER TAG t1 ttl_col = \"a\", ttl_duration = 5; Execution succeeded (time spent 2975/3700 us) // Insert a vertex with tag t1. The vertex expires 5 seconds after the insertion. nebula> INSERT VERTEX t1(a) values \"101\":(now()); Execution succeeded (time spent 1902/2642 us) Set a timeout when creating a tag or an edge type \u00b6 Use TTL options in the CREATE statement to set a timeout when creating a tag or an edge type. For more information, see CREATE TAG or CREATE EDGE . // Create a tag and set the TTL options. nebula> CREATE TAG t2(a int, b int, c string) ttl_duration= 100, ttl_col = \"a\"; Execution succeeded (time spent 3173/3753 us) // Insert a vertex with tag t2. // The timeout timestamp is 1612778164774 (1612778164674 + 100). nebula> INSERT VERTEX t2(a, b, c) values \"102\":(1612778164674, 30, \"Hello\"); Execution succeeded (time spent 1254/1921 us) Remove a timeout \u00b6 To disable TTL and remove the timeout on a property, use the following approaches. Set ttl_col to an empty string. nebula> ALTER TAG t1 ttl_col = \"\"; Drop the property with the timeout. nebula> ALTER TAG t1 DROP (a); Set ttl_duration to 0. This operation keeps the TTL options and prevents the property from expiring. nebula> ALTER TAG t1 ttl_duration = 0; CAUTION: Even when ttl_duration is 0, you CANNOT alter the property because it still has TTL options.","title":"TTL"},{"location":"3.ngql-guide/8.clauses-and-options/ttl-options/#ttl","text":"TTL indicates time to live. Use the TTL options to specify a timeout for a property. Once timed out, the property expires.","title":"TTL"},{"location":"3.ngql-guide/8.clauses-and-options/ttl-options/#opencypher_compatibility","text":"This topic applies to nGQL extensions only.","title":"OpenCypher Compatibility"},{"location":"3.ngql-guide/8.clauses-and-options/ttl-options/#precautions","text":"You CANNOT modify a property with TTL options on it. TTL options and indexes CANNOT coexist on a tag or an edge type. Not even if you try to set them on different properties.","title":"Precautions"},{"location":"3.ngql-guide/8.clauses-and-options/ttl-options/#data_expiration_and_deletion","text":"","title":"Data expiration and deletion"},{"location":"3.ngql-guide/8.clauses-and-options/ttl-options/#vertex_property_expiration","text":"Vertex property expiration has the following impact. If a vertex has only one tag, once a property of the vertex expires, the vertex expires. If a vertex has multiple tags, once a property of the vertex expires, properties bound to the same tag with the expired property also expires, but the vertex does not expire and other properties of it remain untouched.","title":"Vertex property expiration"},{"location":"3.ngql-guide/8.clauses-and-options/ttl-options/#edge_property_expiration","text":"Since an edge can have only one edge type, once an edge property expires, the edge expires.","title":"Edge property expiration"},{"location":"3.ngql-guide/8.clauses-and-options/ttl-options/#data_deletion","text":"The expired data are still stored on the disk, but queries will filter them out. Nebula Graph automatically deletes the expired data and reclaims the disk space during the next compaction .","title":"Data deletion"},{"location":"3.ngql-guide/8.clauses-and-options/ttl-options/#ttl_options","text":"The nGQL TTL feature has the following options. Option Description ttl_col Specifies the property to set a timeout on. The data type of the property must be int or timestamp. ttl_duration Specifies the timeout adds-on value in seconds. The value must be a non-negative int64 number. A property expires if the sum of its value and the ttl_duration value is greater than the current timestamp. If the ttl_duration value is 0, the property never expires.","title":"TTL options"},{"location":"3.ngql-guide/8.clauses-and-options/ttl-options/#use_ttl_options","text":"You must use the TTL options together to set a valid timeout on a property.","title":"Use TTL options"},{"location":"3.ngql-guide/8.clauses-and-options/ttl-options/#set_a_timeout_if_a_tag_or_an_edge_type_exists","text":"If a tag or an edge type is already created, to set a timeout on a property bound to the tag or edge type, use ALTER to update the tag or edge type. // Create a tag. nebula> CREATE TAG t1 (a timestamp); Execution succeeded (time spent 4172/5377 us) // Use ALTER to update the tag and set the TTL options. nebula> ALTER TAG t1 ttl_col = \"a\", ttl_duration = 5; Execution succeeded (time spent 2975/3700 us) // Insert a vertex with tag t1. The vertex expires 5 seconds after the insertion. nebula> INSERT VERTEX t1(a) values \"101\":(now()); Execution succeeded (time spent 1902/2642 us)","title":"Set a timeout if a tag or an edge type exists"},{"location":"3.ngql-guide/8.clauses-and-options/ttl-options/#set_a_timeout_when_creating_a_tag_or_an_edge_type","text":"Use TTL options in the CREATE statement to set a timeout when creating a tag or an edge type. For more information, see CREATE TAG or CREATE EDGE . // Create a tag and set the TTL options. nebula> CREATE TAG t2(a int, b int, c string) ttl_duration= 100, ttl_col = \"a\"; Execution succeeded (time spent 3173/3753 us) // Insert a vertex with tag t2. // The timeout timestamp is 1612778164774 (1612778164674 + 100). nebula> INSERT VERTEX t2(a, b, c) values \"102\":(1612778164674, 30, \"Hello\"); Execution succeeded (time spent 1254/1921 us)","title":"Set a timeout when creating a tag or an edge type"},{"location":"3.ngql-guide/8.clauses-and-options/ttl-options/#remove_a_timeout","text":"To disable TTL and remove the timeout on a property, use the following approaches. Set ttl_col to an empty string. nebula> ALTER TAG t1 ttl_col = \"\"; Drop the property with the timeout. nebula> ALTER TAG t1 DROP (a); Set ttl_duration to 0. This operation keeps the TTL options and prevents the property from expiring. nebula> ALTER TAG t1 ttl_duration = 0; CAUTION: Even when ttl_duration is 0, you CANNOT alter the property because it still has TTL options.","title":"Remove a timeout"},{"location":"3.ngql-guide/8.clauses-and-options/where/","text":"WHERE \u00b6 The WHERE clause filters the outputs by conditions. WHERE works in the following queries: nGQL extensions such as GO and LOOKUP . OpenCypher syntax such as MATCH and WITH . Syntax \u00b6 WHERE <expression> [ AND|OR|XOR <expression> ...]) OpenCypher compatibility \u00b6 Using patterns in WHERE is not supported (TODO: planning), for example WHERE (v)-->(v2) . Filtering on edge rank is a native nGQL feature. It only applies to nGQL extensions such as GO and LOOKUP because the concept edge rank does not exist in openCypher. Basic usage \u00b6 Define conditions with boolean operators \u00b6 Use the boolean operators NOT , AND , OR , and XOR to define conditions in WHERE clauses. For the precedence of the operators, see Precedence . nebula> MATCH (v:player) \\ WHERE v.name == \"Tim Duncan\" \\ XOR (v.age < 30 AND v.name == \"Yao Ming\") \\ OR NOT (v.name == \"Yao Ming\" OR v.name == \"Tim Duncan\") \\ RETURN v.name, v.age; +-------------------------+-------+ | v.name | v.age | +-------------------------+-------+ | \"Marco Belinelli\" | 32 | +-------------------------+-------+ | \"Aron Baynes\" | 32 | +-------------------------+-------+ | \"LeBron James\" | 34 | +-------------------------+-------+ | \"James Harden\" | 29 | +-------------------------+-------+ | \"Manu Ginobili\" | 41 | +-------------------------+-------+ ... Got 50 rows (time spent 6152/6994 us) nebula> GO FROM \"player100\" \\ OVER follow \\ WHERE follow.degree > 90 \\ OR $$.player.age != 33 \\ AND $$.player.name != \"Tony Parker\"; +-------------+ | follow._dst | +-------------+ | \"player101\" | +-------------+ | \"player125\" | +-------------+ Got 2 rows (time spent 3198/3877 us) Filter on properties \u00b6 Use vertex or edge properties to define conditions in WHERE clauses. Filter on a vertex property: nebula> MATCH (v:player)-[e]->(v2) \\ WHERE v2.age < 25 \\ RETURN v2.name, v2.age; +----------------------+--------+ | v2.name | v2.age | +----------------------+--------+ | \"Luka Doncic\" | 20 | +----------------------+--------+ | \"Kristaps Porzingis\" | 23 | +----------------------+--------+ | \"Ben Simmons\" | 22 | +----------------------+--------+ Got 3 rows (time spent 7382/8080 us) nebula> GO FROM \"player100\" \\ OVER follow \\ WHERE $^.player.age >= 42; +-------------+ | follow._dst | +-------------+ | \"player101\" | +-------------+ | \"player125\" | +-------------+ Got 2 rows (time spent 1051/1668 us) Filter on an edge property: nebula> MATCH (v:player)-[e]->() \\ WHERE e.start_year < 2000 \\ RETURN DISTINCT v.name, v.age; +--------------------+-------+ | v.name | v.age | +--------------------+-------+ | \"Shaquille O'Neal\" | 47 | +--------------------+-------+ | \"Steve Nash\" | 45 | +--------------------+-------+ | \"Ray Allen\" | 43 | +--------------------+-------+ | \"Grant Hill\" | 46 | +--------------------+-------+ | \"Tony Parker\" | 36 | +--------------------+-------+ ... Got 11 rows (time spent 7585/8154 us) nebula> GO FROM \"player100\" \\ OVER follow \\ WHERE follow.degree > 90; +-------------+ | follow._dst | +-------------+ | \"player101\" | +-------------+ | \"player125\" | +-------------+ Got 2 rows (time spent 2815/3571 us) Filter on dynamically-calculated property \u00b6 nebula> MATCH (v:player) \\ WHERE v[toLower(\"AGE\")] < 21 \\ RETURN v.name, v.age; +---------------+-------+ | v.name | v.age | +---------------+-------+ | \"Luka Doncic\" | 20 | +---------------+-------+ Filter on the existence of a property \u00b6 nebula> MATCH (v:player) \\ WHERE exists(v.age) \\ RETURN v.name, v.age; +-------------------------+-------+ | v.name | v.age | +-------------------------+-------+ | \"Boris Diaw\" | 36 | +-------------------------+-------+ | \"DeAndre Jordan\" | 30 | +-------------------------+-------+ Filter on edge rank \u00b6 In nGQL, if a group of edges has the same source vertex, destination vertex, and properties, the only thing that distinguishes them is the rank. Use rank conditions in WHERE to filter such edges. The following example creates a group of edges. The differences among the edges are their ranks and properties. Then the example uses a GO statement with a WHERE clause to filter the edges on ranks. nebula> CREATE SPACE test; nebula> USE test; nebula> CREATE EDGE e1(p1 int); nebula> CREATE TAG person(p1 int); nebula> INSERT VERTEX person(p1) VALUES \"1\":(1); nebula> INSERT VERTEX person(p1) VALUES \"2\":(2); nebula> INSERT EDGE e1(p1) VALUES \"1\"->\"2\"@0:(10); nebula> INSERT EDGE e1(p1) VALUES \"1\"->\"2\"@1:(11); nebula> INSERT EDGE e1(p1) VALUES \"1\"->\"2\"@2:(12); nebula> INSERT EDGE e1(p1) VALUES \"1\"->\"2\"@3:(13); nebula> INSERT EDGE e1(p1) VALUES \"1\"->\"2\"@4:(14); nebula> INSERT EDGE e1(p1) VALUES \"1\"->\"2\"@5:(15); nebula> INSERT EDGE e1(p1) VALUES \"1\"->\"2\"@6:(16); // The return messages of the preceding statements are omitted in this example. nebula> GO FROM \"1\" \\ OVER e1 \\ WHERE e1._rank>2 \\ YIELD e1._src, e1._dst, e1._rank AS Rank, e1.p1 | \\ ORDER BY Rank DESC; ==================================== | e1._src | e1._dst | Rank | e1.p1 | ==================================== | 1 | 2 | 6 | 16 | ------------------------------------ | 1 | 2 | 5 | 15 | ------------------------------------ | 1 | 2 | 4 | 14 | ------------------------------------ | 1 | 2 | 3 | 13 | ------------------------------------ Filter on strings \u00b6 Use STARTS WITH , ENDS WITH , or CONTAINS in WHERE to match a specific part of a string. String matching is case-sensitive. Match the beginning of a string \u00b6 Use STARTS WITH \"T\" to match a player name that starts with T . nebula> MATCH (v:player) \\ WHERE v.name STARTS WITH \"T\" \\ RETURN v.name, v.age; +------------------+-------+ | v.name | v.age | +------------------+-------+ | \"Tracy McGrady\" | 39 | +------------------+-------+ | \"Tony Parker\" | 36 | +------------------+-------+ | \"Tim Duncan\" | 42 | +------------------+-------+ | \"Tiago Splitter\" | 34 | +------------------+-------+ Got 4 rows (time spent 5575/7203 us) If you use STARTS WITH \"t\" in the preceding statement, an empty set is returned because no name in the dataset starts with the lowercase t . nebula> MATCH (v:player) \\ WHERE v.name STARTS WITH \"t\" \\ RETURN v.name, v.age; Empty set (time spent 5080/6474 us) Match the ending of a string \u00b6 Use ENDS WITH \"r\" to match a player name that ends with r . nebula> MATCH (v:player) \\ WHERE v.name ENDS WITH \"r\" \\ RETURN v.name, v.age; +------------------+-------+ | v.name | v.age | +------------------+-------+ | \"Vince Carter\" | 42 | +------------------+-------+ | \"Tony Parker\" | 36 | +------------------+-------+ | \"Tiago Splitter\" | 34 | +------------------+-------+ Got 3 rows (time spent 4934/5832 us) Match any part of a string \u00b6 Use CONTAINS \"Pa\" to match a player name that contains Pa . nebula> MATCH (v:player) \\ WHERE v.name CONTAINS \"Pa\" \\ RETURN v.name, v.age; +---------------+-------+ | v.name | v.age | +---------------+-------+ | \"Paul George\" | 28 | +---------------+-------+ | \"Tony Parker\" | 36 | +---------------+-------+ | \"Paul Gasol\" | 38 | +---------------+-------+ | \"Chris Paul\" | 33 | +---------------+-------+ Got 4 rows (time spent 3265/4113 us) Negative string matching \u00b6 Use the boolean operator NOT to negate a string matching condition. nebula> MATCH (v:player) \\ WHERE NOT v.name ENDS WITH \"R\" \\ RETURN v.name, v.age; +-------------------------+-------+ | v.name | v.age | +-------------------------+-------+ | \"Rajon Rondo\" | 33 | +-------------------------+-------+ | \"Rudy Gay\" | 32 | +-------------------------+-------+ | \"Dejounte Murray\" | 29 | +-------------------------+-------+ | \"Chris Paul\" | 33 | +-------------------------+-------+ | \"Carmelo Anthony\" | 34 | +-------------------------+-------+ ... Got 51 rows (time spent 2622/3463 us) Filter on lists \u00b6 Match values in a list \u00b6 Use the IN operator to check if a value is in a specific list. nebula> MATCH (v:player) \\ WHERE v.age IN range(20,25) \\ RETURN v.name, v.age; +-------------------------+-------+ | v.name | v.age | +-------------------------+-------+ | \"Ben Simmons\" | 22 | +-------------------------+-------+ | \"Kristaps Porzingis\" | 23 | +-------------------------+-------+ | \"Luka Doncic\" | 20 | +-------------------------+-------+ | \"Kyle Anderson\" | 25 | +-------------------------+-------+ | \"Giannis Antetokounmpo\" | 24 | +-------------------------+-------+ | \"Joel Embiid\" | 25 | +-------------------------+-------+ Got 6 rows (time spent 5815/7220 us) Match values not in a list \u00b6 Use NOT before IN to rule out the values in a list. nebula> MATCH (v:player) \\ WHERE v.age NOT IN range(20,25) \\ RETURN v.name AS Name, v.age AS Age \\ ORDER BY Age; +---------------------+-----+ | Name | Age | +---------------------+-----+ | \"Kyrie Irving\" | 26 | +---------------------+-----+ | \"Cory Joseph\" | 27 | +---------------------+-----+ | \"Damian Lillard\" | 28 | +---------------------+-----+ | \"Paul George\" | 28 | +---------------------+-----+ | \"Ricky Rubio\" | 28 | +---------------------+-----+ ... Got 45 rows (time spent 2954/3725 us)","title":"WHERE"},{"location":"3.ngql-guide/8.clauses-and-options/where/#where","text":"The WHERE clause filters the outputs by conditions. WHERE works in the following queries: nGQL extensions such as GO and LOOKUP . OpenCypher syntax such as MATCH and WITH .","title":"WHERE"},{"location":"3.ngql-guide/8.clauses-and-options/where/#syntax","text":"WHERE <expression> [ AND|OR|XOR <expression> ...])","title":"Syntax"},{"location":"3.ngql-guide/8.clauses-and-options/where/#opencypher_compatibility","text":"Using patterns in WHERE is not supported (TODO: planning), for example WHERE (v)-->(v2) . Filtering on edge rank is a native nGQL feature. It only applies to nGQL extensions such as GO and LOOKUP because the concept edge rank does not exist in openCypher.","title":"OpenCypher compatibility"},{"location":"3.ngql-guide/8.clauses-and-options/where/#basic_usage","text":"","title":"Basic usage"},{"location":"3.ngql-guide/8.clauses-and-options/where/#define_conditions_with_boolean_operators","text":"Use the boolean operators NOT , AND , OR , and XOR to define conditions in WHERE clauses. For the precedence of the operators, see Precedence . nebula> MATCH (v:player) \\ WHERE v.name == \"Tim Duncan\" \\ XOR (v.age < 30 AND v.name == \"Yao Ming\") \\ OR NOT (v.name == \"Yao Ming\" OR v.name == \"Tim Duncan\") \\ RETURN v.name, v.age; +-------------------------+-------+ | v.name | v.age | +-------------------------+-------+ | \"Marco Belinelli\" | 32 | +-------------------------+-------+ | \"Aron Baynes\" | 32 | +-------------------------+-------+ | \"LeBron James\" | 34 | +-------------------------+-------+ | \"James Harden\" | 29 | +-------------------------+-------+ | \"Manu Ginobili\" | 41 | +-------------------------+-------+ ... Got 50 rows (time spent 6152/6994 us) nebula> GO FROM \"player100\" \\ OVER follow \\ WHERE follow.degree > 90 \\ OR $$.player.age != 33 \\ AND $$.player.name != \"Tony Parker\"; +-------------+ | follow._dst | +-------------+ | \"player101\" | +-------------+ | \"player125\" | +-------------+ Got 2 rows (time spent 3198/3877 us)","title":"Define conditions with boolean operators"},{"location":"3.ngql-guide/8.clauses-and-options/where/#filter_on_properties","text":"Use vertex or edge properties to define conditions in WHERE clauses. Filter on a vertex property: nebula> MATCH (v:player)-[e]->(v2) \\ WHERE v2.age < 25 \\ RETURN v2.name, v2.age; +----------------------+--------+ | v2.name | v2.age | +----------------------+--------+ | \"Luka Doncic\" | 20 | +----------------------+--------+ | \"Kristaps Porzingis\" | 23 | +----------------------+--------+ | \"Ben Simmons\" | 22 | +----------------------+--------+ Got 3 rows (time spent 7382/8080 us) nebula> GO FROM \"player100\" \\ OVER follow \\ WHERE $^.player.age >= 42; +-------------+ | follow._dst | +-------------+ | \"player101\" | +-------------+ | \"player125\" | +-------------+ Got 2 rows (time spent 1051/1668 us) Filter on an edge property: nebula> MATCH (v:player)-[e]->() \\ WHERE e.start_year < 2000 \\ RETURN DISTINCT v.name, v.age; +--------------------+-------+ | v.name | v.age | +--------------------+-------+ | \"Shaquille O'Neal\" | 47 | +--------------------+-------+ | \"Steve Nash\" | 45 | +--------------------+-------+ | \"Ray Allen\" | 43 | +--------------------+-------+ | \"Grant Hill\" | 46 | +--------------------+-------+ | \"Tony Parker\" | 36 | +--------------------+-------+ ... Got 11 rows (time spent 7585/8154 us) nebula> GO FROM \"player100\" \\ OVER follow \\ WHERE follow.degree > 90; +-------------+ | follow._dst | +-------------+ | \"player101\" | +-------------+ | \"player125\" | +-------------+ Got 2 rows (time spent 2815/3571 us)","title":"Filter on properties"},{"location":"3.ngql-guide/8.clauses-and-options/where/#filter_on_dynamically-calculated_property","text":"nebula> MATCH (v:player) \\ WHERE v[toLower(\"AGE\")] < 21 \\ RETURN v.name, v.age; +---------------+-------+ | v.name | v.age | +---------------+-------+ | \"Luka Doncic\" | 20 | +---------------+-------+","title":"Filter on dynamically-calculated property"},{"location":"3.ngql-guide/8.clauses-and-options/where/#filter_on_the_existence_of_a_property","text":"nebula> MATCH (v:player) \\ WHERE exists(v.age) \\ RETURN v.name, v.age; +-------------------------+-------+ | v.name | v.age | +-------------------------+-------+ | \"Boris Diaw\" | 36 | +-------------------------+-------+ | \"DeAndre Jordan\" | 30 | +-------------------------+-------+","title":"Filter on the existence of a property"},{"location":"3.ngql-guide/8.clauses-and-options/where/#filter_on_edge_rank","text":"In nGQL, if a group of edges has the same source vertex, destination vertex, and properties, the only thing that distinguishes them is the rank. Use rank conditions in WHERE to filter such edges. The following example creates a group of edges. The differences among the edges are their ranks and properties. Then the example uses a GO statement with a WHERE clause to filter the edges on ranks. nebula> CREATE SPACE test; nebula> USE test; nebula> CREATE EDGE e1(p1 int); nebula> CREATE TAG person(p1 int); nebula> INSERT VERTEX person(p1) VALUES \"1\":(1); nebula> INSERT VERTEX person(p1) VALUES \"2\":(2); nebula> INSERT EDGE e1(p1) VALUES \"1\"->\"2\"@0:(10); nebula> INSERT EDGE e1(p1) VALUES \"1\"->\"2\"@1:(11); nebula> INSERT EDGE e1(p1) VALUES \"1\"->\"2\"@2:(12); nebula> INSERT EDGE e1(p1) VALUES \"1\"->\"2\"@3:(13); nebula> INSERT EDGE e1(p1) VALUES \"1\"->\"2\"@4:(14); nebula> INSERT EDGE e1(p1) VALUES \"1\"->\"2\"@5:(15); nebula> INSERT EDGE e1(p1) VALUES \"1\"->\"2\"@6:(16); // The return messages of the preceding statements are omitted in this example. nebula> GO FROM \"1\" \\ OVER e1 \\ WHERE e1._rank>2 \\ YIELD e1._src, e1._dst, e1._rank AS Rank, e1.p1 | \\ ORDER BY Rank DESC; ==================================== | e1._src | e1._dst | Rank | e1.p1 | ==================================== | 1 | 2 | 6 | 16 | ------------------------------------ | 1 | 2 | 5 | 15 | ------------------------------------ | 1 | 2 | 4 | 14 | ------------------------------------ | 1 | 2 | 3 | 13 | ------------------------------------","title":"Filter on edge rank"},{"location":"3.ngql-guide/8.clauses-and-options/where/#filter_on_strings","text":"Use STARTS WITH , ENDS WITH , or CONTAINS in WHERE to match a specific part of a string. String matching is case-sensitive.","title":"Filter on strings"},{"location":"3.ngql-guide/8.clauses-and-options/where/#match_the_beginning_of_a_string","text":"Use STARTS WITH \"T\" to match a player name that starts with T . nebula> MATCH (v:player) \\ WHERE v.name STARTS WITH \"T\" \\ RETURN v.name, v.age; +------------------+-------+ | v.name | v.age | +------------------+-------+ | \"Tracy McGrady\" | 39 | +------------------+-------+ | \"Tony Parker\" | 36 | +------------------+-------+ | \"Tim Duncan\" | 42 | +------------------+-------+ | \"Tiago Splitter\" | 34 | +------------------+-------+ Got 4 rows (time spent 5575/7203 us) If you use STARTS WITH \"t\" in the preceding statement, an empty set is returned because no name in the dataset starts with the lowercase t . nebula> MATCH (v:player) \\ WHERE v.name STARTS WITH \"t\" \\ RETURN v.name, v.age; Empty set (time spent 5080/6474 us)","title":"Match the beginning of a string"},{"location":"3.ngql-guide/8.clauses-and-options/where/#match_the_ending_of_a_string","text":"Use ENDS WITH \"r\" to match a player name that ends with r . nebula> MATCH (v:player) \\ WHERE v.name ENDS WITH \"r\" \\ RETURN v.name, v.age; +------------------+-------+ | v.name | v.age | +------------------+-------+ | \"Vince Carter\" | 42 | +------------------+-------+ | \"Tony Parker\" | 36 | +------------------+-------+ | \"Tiago Splitter\" | 34 | +------------------+-------+ Got 3 rows (time spent 4934/5832 us)","title":"Match the ending of a string"},{"location":"3.ngql-guide/8.clauses-and-options/where/#match_any_part_of_a_string","text":"Use CONTAINS \"Pa\" to match a player name that contains Pa . nebula> MATCH (v:player) \\ WHERE v.name CONTAINS \"Pa\" \\ RETURN v.name, v.age; +---------------+-------+ | v.name | v.age | +---------------+-------+ | \"Paul George\" | 28 | +---------------+-------+ | \"Tony Parker\" | 36 | +---------------+-------+ | \"Paul Gasol\" | 38 | +---------------+-------+ | \"Chris Paul\" | 33 | +---------------+-------+ Got 4 rows (time spent 3265/4113 us)","title":"Match any part of a string"},{"location":"3.ngql-guide/8.clauses-and-options/where/#negative_string_matching","text":"Use the boolean operator NOT to negate a string matching condition. nebula> MATCH (v:player) \\ WHERE NOT v.name ENDS WITH \"R\" \\ RETURN v.name, v.age; +-------------------------+-------+ | v.name | v.age | +-------------------------+-------+ | \"Rajon Rondo\" | 33 | +-------------------------+-------+ | \"Rudy Gay\" | 32 | +-------------------------+-------+ | \"Dejounte Murray\" | 29 | +-------------------------+-------+ | \"Chris Paul\" | 33 | +-------------------------+-------+ | \"Carmelo Anthony\" | 34 | +-------------------------+-------+ ... Got 51 rows (time spent 2622/3463 us)","title":"Negative string matching"},{"location":"3.ngql-guide/8.clauses-and-options/where/#filter_on_lists","text":"","title":"Filter on lists"},{"location":"3.ngql-guide/8.clauses-and-options/where/#match_values_in_a_list","text":"Use the IN operator to check if a value is in a specific list. nebula> MATCH (v:player) \\ WHERE v.age IN range(20,25) \\ RETURN v.name, v.age; +-------------------------+-------+ | v.name | v.age | +-------------------------+-------+ | \"Ben Simmons\" | 22 | +-------------------------+-------+ | \"Kristaps Porzingis\" | 23 | +-------------------------+-------+ | \"Luka Doncic\" | 20 | +-------------------------+-------+ | \"Kyle Anderson\" | 25 | +-------------------------+-------+ | \"Giannis Antetokounmpo\" | 24 | +-------------------------+-------+ | \"Joel Embiid\" | 25 | +-------------------------+-------+ Got 6 rows (time spent 5815/7220 us)","title":"Match values in a list"},{"location":"3.ngql-guide/8.clauses-and-options/where/#match_values_not_in_a_list","text":"Use NOT before IN to rule out the values in a list. nebula> MATCH (v:player) \\ WHERE v.age NOT IN range(20,25) \\ RETURN v.name AS Name, v.age AS Age \\ ORDER BY Age; +---------------------+-----+ | Name | Age | +---------------------+-----+ | \"Kyrie Irving\" | 26 | +---------------------+-----+ | \"Cory Joseph\" | 27 | +---------------------+-----+ | \"Damian Lillard\" | 28 | +---------------------+-----+ | \"Paul George\" | 28 | +---------------------+-----+ | \"Ricky Rubio\" | 28 | +---------------------+-----+ ... Got 45 rows (time spent 2954/3725 us)","title":"Match values not in a list"},{"location":"3.ngql-guide/8.clauses-and-options/with/","text":"WITH \u00b6 OpenCypher compatibility \u00b6 The WITH clause can take the output from a query part, process it, and pass it to the next query part as the input. WITH has a similar function with the pipe symbol in nGQL-extension, but they work in different ways. WITH only works in the openCypher syntax, such as in MATCH or UNWIND . In the nGQL-extensions such as GO or FETCH , use pipe symbols ( | ) instead. DON'T: Don't use pipe symbols in the openCypher syntax or use WITH in the nGQL extensions. Combine statements and form a composite query \u00b6 Use a WITH clause to combine statements and transfer the output of a statement as the input of another statement. Example 1 \u00b6 The following statement: Matches a path. Outputs all the vertices on the path to a list with the nodes() function. Unwinds the list into rows. Removes duplicated vertices and returns a set of distinct vertices. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})--() \\ WITH nodes(p) AS n \\ UNWIND n AS n1 \\ RETURN DISTINCT n1; +----------------------------------------------------------------------+ | n1 | +----------------------------------------------------------------------+ | (\"player100\" :star{} :person{} :player{age: 42, name: \"Tim Duncan\"}) | +----------------------------------------------------------------------+ | (\"player101\" :player{age: 36, name: \"Tony Parker\"}) | +----------------------------------------------------------------------+ | (\"team204\" :team{name: \"Spurs\"}) | +----------------------------------------------------------------------+ | (\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"}) | +----------------------------------------------------------------------+ | (\"player125\" :player{age: 41, name: \"Manu Ginobili\"}) | +----------------------------------------------------------------------+ | (\"player104\" :player{age: 32, name: \"Marco Belinelli\"}) | +----------------------------------------------------------------------+ | (\"player144\" :player{age: 47, name: \"Shaquile O'Neal\"}) | +----------------------------------------------------------------------+ | (\"player105\" :player{age: 31, name: \"Danny Green\"}) | +----------------------------------------------------------------------+ | (\"player113\" :player{age: 29, name: \"Dejounte Murray\"}) | +----------------------------------------------------------------------+ | (\"player107\" :player{age: 32, name: \"Aron Baynes\"}) | +----------------------------------------------------------------------+ | (\"player109\" :player{age: 34, name: \"Tiago Splitter\"}) | +----------------------------------------------------------------------+ | (\"player108\" :player{age: 36, name: \"Boris Diaw\"}) | +----------------------------------------------------------------------+ Got 12 rows (time spent 3795/4487 us) Example 2 \u00b6 The following statement: Matches a vertex with the VID \"player100\". Outputs all the tags of the vertex into a list with the labels() function. Unwinds the list into rows. Returns the rows. nebula> MATCH (v) \\ WHERE id(v)==\"player100\" \\ WITH labels(v) AS tags_unf \\ UNWIND tags_unf AS tags_f \\ RETURN tags_f; +----------+ | tags_f | +----------+ | \"star\" | +----------+ | \"player\" | +----------+ | \"person\" | +----------+ Got 3 rows (time spent 1709/2495 us) Filter aggregated queries \u00b6 WITH can work as a filter in the middle of an aggregated query. nebula> MATCH (v:player)-->(v2:player) \\ WITH DISTINCT v2 AS v2, v2.age AS Age \\ ORDER BY Age \\ WHERE Age<25 \\ RETURN v2.name AS Name, Age; +----------------------+-----+ | Name | Age | +----------------------+-----+ | \"Luka Doncic\" | 20 | +----------------------+-----+ | \"Ben Simmons\" | 22 | +----------------------+-----+ | \"Kristaps Porzingis\" | 23 | +----------------------+-----+ Got 3 rows (time spent 7444/8467 us) Process the output before using collect() on it \u00b6 Use a WITH clause to sort and limit the output before using collect() to transform the output into a list. nebula> MATCH (v:player) \\ WITH v.name AS Name \\ ORDER BY Name DESC \\ LIMIT 3 \\ RETURN collect(Name); +-----------------------------------------------+ | COLLECT(Name) | +-----------------------------------------------+ | [\"Yao Ming\", \"Vince Carter\", \"Tracy McGrady\"] | +-----------------------------------------------+ Got 1 rows (time spent 3498/4222 us)","title":"WITH"},{"location":"3.ngql-guide/8.clauses-and-options/with/#with","text":"","title":"WITH"},{"location":"3.ngql-guide/8.clauses-and-options/with/#opencypher_compatibility","text":"The WITH clause can take the output from a query part, process it, and pass it to the next query part as the input. WITH has a similar function with the pipe symbol in nGQL-extension, but they work in different ways. WITH only works in the openCypher syntax, such as in MATCH or UNWIND . In the nGQL-extensions such as GO or FETCH , use pipe symbols ( | ) instead. DON'T: Don't use pipe symbols in the openCypher syntax or use WITH in the nGQL extensions.","title":"OpenCypher compatibility"},{"location":"3.ngql-guide/8.clauses-and-options/with/#combine_statements_and_form_a_composite_query","text":"Use a WITH clause to combine statements and transfer the output of a statement as the input of another statement.","title":"Combine statements and form a composite query"},{"location":"3.ngql-guide/8.clauses-and-options/with/#example_1","text":"The following statement: Matches a path. Outputs all the vertices on the path to a list with the nodes() function. Unwinds the list into rows. Removes duplicated vertices and returns a set of distinct vertices. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})--() \\ WITH nodes(p) AS n \\ UNWIND n AS n1 \\ RETURN DISTINCT n1; +----------------------------------------------------------------------+ | n1 | +----------------------------------------------------------------------+ | (\"player100\" :star{} :person{} :player{age: 42, name: \"Tim Duncan\"}) | +----------------------------------------------------------------------+ | (\"player101\" :player{age: 36, name: \"Tony Parker\"}) | +----------------------------------------------------------------------+ | (\"team204\" :team{name: \"Spurs\"}) | +----------------------------------------------------------------------+ | (\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"}) | +----------------------------------------------------------------------+ | (\"player125\" :player{age: 41, name: \"Manu Ginobili\"}) | +----------------------------------------------------------------------+ | (\"player104\" :player{age: 32, name: \"Marco Belinelli\"}) | +----------------------------------------------------------------------+ | (\"player144\" :player{age: 47, name: \"Shaquile O'Neal\"}) | +----------------------------------------------------------------------+ | (\"player105\" :player{age: 31, name: \"Danny Green\"}) | +----------------------------------------------------------------------+ | (\"player113\" :player{age: 29, name: \"Dejounte Murray\"}) | +----------------------------------------------------------------------+ | (\"player107\" :player{age: 32, name: \"Aron Baynes\"}) | +----------------------------------------------------------------------+ | (\"player109\" :player{age: 34, name: \"Tiago Splitter\"}) | +----------------------------------------------------------------------+ | (\"player108\" :player{age: 36, name: \"Boris Diaw\"}) | +----------------------------------------------------------------------+ Got 12 rows (time spent 3795/4487 us)","title":"Example 1"},{"location":"3.ngql-guide/8.clauses-and-options/with/#example_2","text":"The following statement: Matches a vertex with the VID \"player100\". Outputs all the tags of the vertex into a list with the labels() function. Unwinds the list into rows. Returns the rows. nebula> MATCH (v) \\ WHERE id(v)==\"player100\" \\ WITH labels(v) AS tags_unf \\ UNWIND tags_unf AS tags_f \\ RETURN tags_f; +----------+ | tags_f | +----------+ | \"star\" | +----------+ | \"player\" | +----------+ | \"person\" | +----------+ Got 3 rows (time spent 1709/2495 us)","title":"Example 2"},{"location":"3.ngql-guide/8.clauses-and-options/with/#filter_aggregated_queries","text":"WITH can work as a filter in the middle of an aggregated query. nebula> MATCH (v:player)-->(v2:player) \\ WITH DISTINCT v2 AS v2, v2.age AS Age \\ ORDER BY Age \\ WHERE Age<25 \\ RETURN v2.name AS Name, Age; +----------------------+-----+ | Name | Age | +----------------------+-----+ | \"Luka Doncic\" | 20 | +----------------------+-----+ | \"Ben Simmons\" | 22 | +----------------------+-----+ | \"Kristaps Porzingis\" | 23 | +----------------------+-----+ Got 3 rows (time spent 7444/8467 us)","title":"Filter aggregated queries"},{"location":"3.ngql-guide/8.clauses-and-options/with/#process_the_output_before_using_collect_on_it","text":"Use a WITH clause to sort and limit the output before using collect() to transform the output into a list. nebula> MATCH (v:player) \\ WITH v.name AS Name \\ ORDER BY Name DESC \\ LIMIT 3 \\ RETURN collect(Name); +-----------------------------------------------+ | COLLECT(Name) | +-----------------------------------------------+ | [\"Yao Ming\", \"Vince Carter\", \"Tracy McGrady\"] | +-----------------------------------------------+ Got 1 rows (time spent 3498/4222 us)","title":"Process the output before using collect() on it"},{"location":"3.ngql-guide/8.clauses-and-options/yield/","text":"YIELD \u00b6 YIELD defines the output of an nGQL query. YIELD can lead a clause or a statement: A YIELD clause works in nGQL statements such as GO , FETCH , or LOOKUP . A YIELD statement works in a composite query or independently. OpenCypher Compatibility \u00b6 This topic applies to nGQL extensions only. For the openCypher syntax, use RETURN . YIELD has different functions in openCypher and nGQL. In openCypher, YIELD is used in the CALL[\u2026YIELD] clause to specify the output of the procedure call. NOTE: NGQL does not support CALL[\u2026YIELD] yet. In nGQL, YIELD works like RETURN in openCypher. YIELD clauses \u00b6 Syntax \u00b6 YIELD [DISTINCT] <col> [AS <alias>] [, <col> [AS <alias>] ...] The syntax is described as follows. Keyword/Field Description DISTINCT Aggregates the output and makes the statement return a distinct result set. col A field to be returned. If no alias is set, col will be a column name in the output. alias An alias for col . It is set after the keyword AS and will be a column name in the output. Use a YIELD clause in a statement \u00b6 Use YIELD with GO : nebula> GO FROM \"player100\" OVER follow \\ YIELD $$.player.name AS Friend, $$.player.age AS Age; +-----------------+-----+ | Friend | Age | +-----------------+-----+ | \"Tony Parker\" | 36 | +-----------------+-----+ | \"Manu Ginobili\" | 41 | +-----------------+-----+ Got 2 rows (time spent 3378/4030 us) Use YIELD with FETCH : nebula> FETCH PROP ON player \"player100\" \\ YIELD player.name; +-------------+--------------+ | VertexID | player.name | +-------------+--------------+ | \"player100\" | \"Tim Duncan\" | +-------------+--------------+ Got 1 rows (time spent 2933/5931 us) Use YIELD with LOOKUP : nebula> LOOKUP ON player WHERE player.name == \"Tony Parker\" \\ YIELD player.name, player.age; ======================================= | VertexID | player.name | player.age | ======================================= | 101 | Tony Parker | 36 | --------------------------------------- Got 1 rows (time spent 2963/3778 us) YIELD Statements \u00b6 Syntax \u00b6 YIELD [DISTINCT] <col> [AS <alias>] [, <col> [AS <alias>] ...] [WHERE <conditions>] The syntax is described as follows. Field Description DISTINCT Aggregates the output and makes the statement return a distinct result set. col A field to be returned. If no alias is set, col will be a column name in the output. alias An alias for col . It is set after the keyword AS and will be a column name in the output. conditions Conditions set in a WHERE clause to filter the output. For more information, see WHERE . Use a YIELD statement in a composite query \u00b6 In a composite query , a YIELD statement accepts, filters, and reforms the result set of the preceding statement, and then outputs it. The following query finds the players that \"player100\" follows and calculates their average age. nebula> GO FROM \"player100\" OVER follow \\ YIELD follow._dst AS ID | \\ FETCH PROP ON player $-.ID \\ YIELD player.age AS Age | \\ YIELD AVG($-.Age) as Avg_age, count(*)as Num_friends; +---------+-------------+ | Avg_age | Num_friends | +---------+-------------+ | 38.5 | 2 | +---------+-------------+ Got 1 rows (time spent 1846/2426 us) The following query finds the players that \"player101\" follows and the follow degrees are greater than 90. nebula> $var1 = GO FROM \"player101\" OVER follow \\ YIELD follow.degree AS Degree, follow._dst as ID; \\ YIELD $var1.ID AS ID \\ WHERE $var1.Degree > 90; +-------------+ | ID | +-------------+ | \"player100\" | +-------------+ | \"player125\" | +-------------+ Got 2 rows (time spent 891/1411 us) Use a standalone YIELD statement \u00b6 A YIELD statement can calculate a valid expression and output the result. nebula> YIELD rand32(1, 6); +-------------+ | rand32(1,6) | +-------------+ | 3 | +-------------+ Got 1 rows (time spent 144/615 us) nebula> YIELD \"Hel\" + \"\\tlo\" AS string1, \", World!\" AS string2; +-------------+------------+ | string1 | string2 | +-------------+------------+ | \"Hel lo\" | \", World!\" | +-------------+------------+ Got 1 rows (time spent 154/692 us) nebula> YIELD hash(\"Tim\") % 100; +-----------------+ | (hash(Tim)%100) | +-----------------+ | 42 | +-----------------+ Got 1 rows (time spent 164/820 us) nebula> YIELD \\ CASE 2+3 \\ WHEN 4 THEN 0 \\ WHEN 5 THEN 1 \\ ELSE -1 \\ END \\ AS result; +--------+ | result | +--------+ | 1 | +--------+ Got 1 rows (time spent 204/935 us)","title":"YIELD"},{"location":"3.ngql-guide/8.clauses-and-options/yield/#yield","text":"YIELD defines the output of an nGQL query. YIELD can lead a clause or a statement: A YIELD clause works in nGQL statements such as GO , FETCH , or LOOKUP . A YIELD statement works in a composite query or independently.","title":"YIELD"},{"location":"3.ngql-guide/8.clauses-and-options/yield/#opencypher_compatibility","text":"This topic applies to nGQL extensions only. For the openCypher syntax, use RETURN . YIELD has different functions in openCypher and nGQL. In openCypher, YIELD is used in the CALL[\u2026YIELD] clause to specify the output of the procedure call. NOTE: NGQL does not support CALL[\u2026YIELD] yet. In nGQL, YIELD works like RETURN in openCypher.","title":"OpenCypher Compatibility"},{"location":"3.ngql-guide/8.clauses-and-options/yield/#yield_clauses","text":"","title":"YIELD clauses"},{"location":"3.ngql-guide/8.clauses-and-options/yield/#syntax","text":"YIELD [DISTINCT] <col> [AS <alias>] [, <col> [AS <alias>] ...] The syntax is described as follows. Keyword/Field Description DISTINCT Aggregates the output and makes the statement return a distinct result set. col A field to be returned. If no alias is set, col will be a column name in the output. alias An alias for col . It is set after the keyword AS and will be a column name in the output.","title":"Syntax"},{"location":"3.ngql-guide/8.clauses-and-options/yield/#use_a_yield_clause_in_a_statement","text":"Use YIELD with GO : nebula> GO FROM \"player100\" OVER follow \\ YIELD $$.player.name AS Friend, $$.player.age AS Age; +-----------------+-----+ | Friend | Age | +-----------------+-----+ | \"Tony Parker\" | 36 | +-----------------+-----+ | \"Manu Ginobili\" | 41 | +-----------------+-----+ Got 2 rows (time spent 3378/4030 us) Use YIELD with FETCH : nebula> FETCH PROP ON player \"player100\" \\ YIELD player.name; +-------------+--------------+ | VertexID | player.name | +-------------+--------------+ | \"player100\" | \"Tim Duncan\" | +-------------+--------------+ Got 1 rows (time spent 2933/5931 us) Use YIELD with LOOKUP : nebula> LOOKUP ON player WHERE player.name == \"Tony Parker\" \\ YIELD player.name, player.age; ======================================= | VertexID | player.name | player.age | ======================================= | 101 | Tony Parker | 36 | --------------------------------------- Got 1 rows (time spent 2963/3778 us)","title":"Use a YIELD clause in a statement"},{"location":"3.ngql-guide/8.clauses-and-options/yield/#yield_statements","text":"","title":"YIELD Statements"},{"location":"3.ngql-guide/8.clauses-and-options/yield/#syntax_1","text":"YIELD [DISTINCT] <col> [AS <alias>] [, <col> [AS <alias>] ...] [WHERE <conditions>] The syntax is described as follows. Field Description DISTINCT Aggregates the output and makes the statement return a distinct result set. col A field to be returned. If no alias is set, col will be a column name in the output. alias An alias for col . It is set after the keyword AS and will be a column name in the output. conditions Conditions set in a WHERE clause to filter the output. For more information, see WHERE .","title":"Syntax"},{"location":"3.ngql-guide/8.clauses-and-options/yield/#use_a_yield_statement_in_a_composite_query","text":"In a composite query , a YIELD statement accepts, filters, and reforms the result set of the preceding statement, and then outputs it. The following query finds the players that \"player100\" follows and calculates their average age. nebula> GO FROM \"player100\" OVER follow \\ YIELD follow._dst AS ID | \\ FETCH PROP ON player $-.ID \\ YIELD player.age AS Age | \\ YIELD AVG($-.Age) as Avg_age, count(*)as Num_friends; +---------+-------------+ | Avg_age | Num_friends | +---------+-------------+ | 38.5 | 2 | +---------+-------------+ Got 1 rows (time spent 1846/2426 us) The following query finds the players that \"player101\" follows and the follow degrees are greater than 90. nebula> $var1 = GO FROM \"player101\" OVER follow \\ YIELD follow.degree AS Degree, follow._dst as ID; \\ YIELD $var1.ID AS ID \\ WHERE $var1.Degree > 90; +-------------+ | ID | +-------------+ | \"player100\" | +-------------+ | \"player125\" | +-------------+ Got 2 rows (time spent 891/1411 us)","title":"Use a YIELD statement in a composite query"},{"location":"3.ngql-guide/8.clauses-and-options/yield/#use_a_standalone_yield_statement","text":"A YIELD statement can calculate a valid expression and output the result. nebula> YIELD rand32(1, 6); +-------------+ | rand32(1,6) | +-------------+ | 3 | +-------------+ Got 1 rows (time spent 144/615 us) nebula> YIELD \"Hel\" + \"\\tlo\" AS string1, \", World!\" AS string2; +-------------+------------+ | string1 | string2 | +-------------+------------+ | \"Hel lo\" | \", World!\" | +-------------+------------+ Got 1 rows (time spent 154/692 us) nebula> YIELD hash(\"Tim\") % 100; +-----------------+ | (hash(Tim)%100) | +-----------------+ | 42 | +-----------------+ Got 1 rows (time spent 164/820 us) nebula> YIELD \\ CASE 2+3 \\ WHEN 4 THEN 0 \\ WHEN 5 THEN 1 \\ ELSE -1 \\ END \\ AS result; +--------+ | result | +--------+ | 1 | +--------+ Got 1 rows (time spent 204/935 us)","title":"Use a standalone YIELD statement"},{"location":"3.ngql-guide/9.space-statements/1.create-space/","text":"CREATE SPACE \u00b6 CREATE SPACE [IF NOT EXISTS] <graph_space_name> [(partition_num = <partition_number>, replica_factor = <replica_number>, vid_type = {FIXED_STRING(<N>) | INT64})]; The CREATE SPACE statement creates a new graph space with the given name. A SPACE is a region that provides physically isolated graphs in Nebula Graph. An error occurs if a graph space with the same name exists if you did not specify IF NOT EXISTS . IF NOT EXISTS \u00b6 You can use the IF NOT EXISTS keywords when creating graph spaces. These keywords automatically detect if the related graph space exists. If it does not exist, a new one is created. Otherwise, no graph space is created. NOTE : The graph space existence detection here only compares the graph space name (excluding properties). Graph space name \u00b6 The graph_space_name uniquely identifies a graph space in a Nebula Graph instance. Customized graph space options \u00b6 You can set four optional options for a new graph space: partition_num Specifies the number of partitions in each replica. The suggested number is five times the number of the hard disks in the cluster. For example, if you have 3 hard disks in the cluster, we recommend that you set 15 partitions. replica_factor Specifies the number of replicas in the cluster. The default replica factor is 1. The suggested number is 3 in a production environment and 1 in a test environment. Always set the replica to an odd number for the need of quorum-based voting. vid_type Specifies the data type of VIDs in a graph space. Available values are FIXED_STRING(N) and INT64 , where N represents the maximum length of the VIDs and it must be a positive integer. The default value is FIXED_STRING(8) . If you set a VID length greater than N , Nebula Graph throws an error. To set the integer VID for vertices, set vid_type to INT64 . If no option is given, Nebula Graph creates the graph space with the default options. Example \u00b6 nebula> CREATE SPACE my_space_1; -- create a graph space with default options nebula> CREATE SPACE my_space_2(partition_num=10); -- create a graph space with customized partition number nebula> CREATE SPACE my_space_3(replica_factor=1); -- create a graph space with customized replica factor nebula> CREATE SPACE my_space_4(vid_type = FIXED_STRING(30)); -- create a graph space with customized VID maximum length Implementation of the operation \u00b6 Trying to use a newly created graph space may fail because the creation is implemented asynchronously. Nebula Graph implements the creation in the next heartbeat cycle. To make sure the creation is successful, take one of the following approaches: Find the new graph space in the result of SHOW SPACES or DESCRIBE SPACE . If you can't, wait a few seconds and try again. Wait for two heartbeat cycles, i.e., 20 seconds. To change the heartbeat interval, modify the heartbeat_interval_secs parameter in the configuration files for all services. Check partition distribution \u00b6 On some large clusters, the partition distribution is possibly unbalanced because of the different startup times. You can run the command to do a check of the machine distribution. nebula> SHOW HOSTS; +-----------+-------+--------+--------------+---------------------+------------------------+ | Host | Port | Status | Leader count | Leader distribution | Partition distribution | +-----------+-------+--------+--------------+---------------------+------------------------+ | storaged0 | 9779 | ONLINE | 1 | nba:5 | nba:5 | +-----------+-------+--------+--------------+---------------------+------------------------+ | storaged1 | 9779 | ONLINE | 2 | test:1, nba:5 | nba:5, test:1 | +-----------+-------+--------+--------------+---------------------+------------------------+ | storaged2 | 9779 | ONLINE | 1 | nba:5 | nba:5 | +-----------+-------+--------+--------------+---------------------+------------------------+ To balance the request loads, use the following command. nebula> BALANCE LEADER;","title":"CREATE SPACE"},{"location":"3.ngql-guide/9.space-statements/1.create-space/#create_space","text":"CREATE SPACE [IF NOT EXISTS] <graph_space_name> [(partition_num = <partition_number>, replica_factor = <replica_number>, vid_type = {FIXED_STRING(<N>) | INT64})]; The CREATE SPACE statement creates a new graph space with the given name. A SPACE is a region that provides physically isolated graphs in Nebula Graph. An error occurs if a graph space with the same name exists if you did not specify IF NOT EXISTS .","title":"CREATE SPACE"},{"location":"3.ngql-guide/9.space-statements/1.create-space/#if_not_exists","text":"You can use the IF NOT EXISTS keywords when creating graph spaces. These keywords automatically detect if the related graph space exists. If it does not exist, a new one is created. Otherwise, no graph space is created. NOTE : The graph space existence detection here only compares the graph space name (excluding properties).","title":"IF NOT EXISTS"},{"location":"3.ngql-guide/9.space-statements/1.create-space/#graph_space_name","text":"The graph_space_name uniquely identifies a graph space in a Nebula Graph instance.","title":"Graph space name"},{"location":"3.ngql-guide/9.space-statements/1.create-space/#customized_graph_space_options","text":"You can set four optional options for a new graph space: partition_num Specifies the number of partitions in each replica. The suggested number is five times the number of the hard disks in the cluster. For example, if you have 3 hard disks in the cluster, we recommend that you set 15 partitions. replica_factor Specifies the number of replicas in the cluster. The default replica factor is 1. The suggested number is 3 in a production environment and 1 in a test environment. Always set the replica to an odd number for the need of quorum-based voting. vid_type Specifies the data type of VIDs in a graph space. Available values are FIXED_STRING(N) and INT64 , where N represents the maximum length of the VIDs and it must be a positive integer. The default value is FIXED_STRING(8) . If you set a VID length greater than N , Nebula Graph throws an error. To set the integer VID for vertices, set vid_type to INT64 . If no option is given, Nebula Graph creates the graph space with the default options.","title":"Customized graph space options"},{"location":"3.ngql-guide/9.space-statements/1.create-space/#example","text":"nebula> CREATE SPACE my_space_1; -- create a graph space with default options nebula> CREATE SPACE my_space_2(partition_num=10); -- create a graph space with customized partition number nebula> CREATE SPACE my_space_3(replica_factor=1); -- create a graph space with customized replica factor nebula> CREATE SPACE my_space_4(vid_type = FIXED_STRING(30)); -- create a graph space with customized VID maximum length","title":"Example"},{"location":"3.ngql-guide/9.space-statements/1.create-space/#implementation_of_the_operation","text":"Trying to use a newly created graph space may fail because the creation is implemented asynchronously. Nebula Graph implements the creation in the next heartbeat cycle. To make sure the creation is successful, take one of the following approaches: Find the new graph space in the result of SHOW SPACES or DESCRIBE SPACE . If you can't, wait a few seconds and try again. Wait for two heartbeat cycles, i.e., 20 seconds. To change the heartbeat interval, modify the heartbeat_interval_secs parameter in the configuration files for all services.","title":"Implementation of the operation"},{"location":"3.ngql-guide/9.space-statements/1.create-space/#check_partition_distribution","text":"On some large clusters, the partition distribution is possibly unbalanced because of the different startup times. You can run the command to do a check of the machine distribution. nebula> SHOW HOSTS; +-----------+-------+--------+--------------+---------------------+------------------------+ | Host | Port | Status | Leader count | Leader distribution | Partition distribution | +-----------+-------+--------+--------------+---------------------+------------------------+ | storaged0 | 9779 | ONLINE | 1 | nba:5 | nba:5 | +-----------+-------+--------+--------------+---------------------+------------------------+ | storaged1 | 9779 | ONLINE | 2 | test:1, nba:5 | nba:5, test:1 | +-----------+-------+--------+--------------+---------------------+------------------------+ | storaged2 | 9779 | ONLINE | 1 | nba:5 | nba:5 | +-----------+-------+--------+--------------+---------------------+------------------------+ To balance the request loads, use the following command. nebula> BALANCE LEADER;","title":"Check partition distribution"},{"location":"3.ngql-guide/9.space-statements/2.use-space/","text":"USE \u00b6 USE <graph_space_name> The USE statement specifies a graph space as the current working space for subsequent queries. To manage multiple graph spaces, use the USE statement. The USE statement requires some privilege . The graph space remains the same unless another USE statement is executed. nebula> USE space1; -- Traverse in graph space1. nebula> GO FROM 1 OVER edge1; nebula> USE space2; -- Traverse in graph space2. These vertices and edges have no relevance with space1. nebula> GO FROM 2 OVER edge2; -- Now you are back to space1. Hereafter, you can not read any data from space2. nebula> USE space1; NOTE : You can't use two spaces in one statement. Different from SQL or Fabric Cypher, making a graph space as the working graph space prevents you from accessing other spaces. The only way to traverse in a new graph space is to switch by the USE statement. Graph spaces are FULLY ISOLATED from each other. Unlike Fabric Cypher, you can only use one graph space at a time in Nebula Graph. But in Fabric Cypher, you can use two (graph) spaces in one statement.","title":"USE SPACE"},{"location":"3.ngql-guide/9.space-statements/2.use-space/#use","text":"USE <graph_space_name> The USE statement specifies a graph space as the current working space for subsequent queries. To manage multiple graph spaces, use the USE statement. The USE statement requires some privilege . The graph space remains the same unless another USE statement is executed. nebula> USE space1; -- Traverse in graph space1. nebula> GO FROM 1 OVER edge1; nebula> USE space2; -- Traverse in graph space2. These vertices and edges have no relevance with space1. nebula> GO FROM 2 OVER edge2; -- Now you are back to space1. Hereafter, you can not read any data from space2. nebula> USE space1; NOTE : You can't use two spaces in one statement. Different from SQL or Fabric Cypher, making a graph space as the working graph space prevents you from accessing other spaces. The only way to traverse in a new graph space is to switch by the USE statement. Graph spaces are FULLY ISOLATED from each other. Unlike Fabric Cypher, you can only use one graph space at a time in Nebula Graph. But in Fabric Cypher, you can use two (graph) spaces in one statement.","title":"USE"},{"location":"3.ngql-guide/9.space-statements/3.show-spaces/","text":"SHOW SPACES \u00b6 SHOW SPACES The SHOW SPACES statement lists the all the graph spaces in a Nebula Graph instance. For example: nebula> SHOW SPACES; +--------+ | Name | +--------+ | \"nba\" | +--------+ To create graph spaces, see Create Space document .","title":"SHOW SPACES"},{"location":"3.ngql-guide/9.space-statements/3.show-spaces/#show_spaces","text":"SHOW SPACES The SHOW SPACES statement lists the all the graph spaces in a Nebula Graph instance. For example: nebula> SHOW SPACES; +--------+ | Name | +--------+ | \"nba\" | +--------+ To create graph spaces, see Create Space document .","title":"SHOW SPACES"},{"location":"3.ngql-guide/9.space-statements/4.describe-space/","text":"DESCRIBE SPACE \u00b6 DESC[RIBE] SPACE <graph_space_name> The DESCRIBE SPACE statement returns information about a graph space. The DESCRIBE SPACE statement is different from the SHOW SPACES statement. For details about SHOW SPACES , see SHOW SPACES . You can use DESC instead of DESCRIBE for short. Example \u00b6 Get information about a graph space. nebula> DESCRIBE SPACE nba; +----+-------+------------------+----------------+---------+------------+--------------------+-------------+-----------+ | ID | Name | Partition Number | Replica Factor | Charset | Collate | Vid Type | Atomic Edge | Group | +----+-------+------------------+----------------+---------+------------+--------------------+-------------+-----------+ | 1 | \"nba\" | 10 | 1 | \"utf8\" | \"utf8_bin\" | \"FIXED_STRING(32)\" | \"false\" | \"default\" | +----+-------+------------------+----------------+---------+------------+--------------------+-------------+-----------+","title":"DESCRIBE SPACE"},{"location":"3.ngql-guide/9.space-statements/4.describe-space/#describe_space","text":"DESC[RIBE] SPACE <graph_space_name> The DESCRIBE SPACE statement returns information about a graph space. The DESCRIBE SPACE statement is different from the SHOW SPACES statement. For details about SHOW SPACES , see SHOW SPACES . You can use DESC instead of DESCRIBE for short.","title":"DESCRIBE SPACE"},{"location":"3.ngql-guide/9.space-statements/4.describe-space/#example","text":"Get information about a graph space. nebula> DESCRIBE SPACE nba; +----+-------+------------------+----------------+---------+------------+--------------------+-------------+-----------+ | ID | Name | Partition Number | Replica Factor | Charset | Collate | Vid Type | Atomic Edge | Group | +----+-------+------------------+----------------+---------+------------+--------------------+-------------+-----------+ | 1 | \"nba\" | 10 | 1 | \"utf8\" | \"utf8_bin\" | \"FIXED_STRING(32)\" | \"false\" | \"default\" | +----+-------+------------------+----------------+---------+------------+--------------------+-------------+-----------+","title":"Example"},{"location":"3.ngql-guide/9.space-statements/5.drop-space/","text":"DROP SPACE \u00b6 DROP SPACE [IF EXISTS] <graph_space_name> The DROP SPACE statement deletes everything in the related graph space. You must have the DROP privilege for the related graph space. You can use the IF EXISTS keywords when dropping spaces. These keywords automatically detects if the related graph space exists. If it exists, it is deleted. Otherwise, no graph space is deleted. Other graph spaces stay unchanged. The DROP SPACE statement does not immediately remove all the files and directories from disk. Use another space , and submit job compact . NOTE: Be very careful with this statement.","title":"DROP SPACE"},{"location":"3.ngql-guide/9.space-statements/5.drop-space/#drop_space","text":"DROP SPACE [IF EXISTS] <graph_space_name> The DROP SPACE statement deletes everything in the related graph space. You must have the DROP privilege for the related graph space. You can use the IF EXISTS keywords when dropping spaces. These keywords automatically detects if the related graph space exists. If it exists, it is deleted. Otherwise, no graph space is deleted. Other graph spaces stay unchanged. The DROP SPACE statement does not immediately remove all the files and directories from disk. Use another space , and submit job compact . NOTE: Be very careful with this statement.","title":"DROP SPACE"},{"location":"4.deployment-and-installation/1.resource-preparations/","text":"Prepare resources for compiling, installing, and running Nebula Graph \u00b6 This topic describes the requirements and suggestions for compiling and installing Nebula Graph, as well as how to estimate the resource you need to reserve for running a Nebula Graph cluster. Reading guide \u00b6 If you are reading this topic with the questions listed below, click them to jump to their answers. What do I need to compile Nebula Graph? What do I need to run Nebula Graph in a test environment? What do I need to run Nebula Graph in a production environment? How much memory and disk space do I need to reserve for my Nebula Graph cluster? Requirements for compiling the Nebula Graph source code \u00b6 Hardware requirements for compiling Nebula Graph \u00b6 Item Requirement CPU architecture x86_64 Memory 4 GB Disk 10 GB, SSD Supported operating systems for compiling Nebula Graph \u00b6 For now, we can only compile Nebula Graph in the Linux system. We recommend that you use any Linux system with kernel version 2.6.32 or above. Software requirements for compiling Nebula Graph \u00b6 You must have the correct version of the software listed below to compile Nebula Graph. If they are not as required or you are not sure, follow the steps in Prepare software for compiling Nebula Graph to get them ready. Software Version Note glibc 2.12 or above You can run ldd --version to check the glibc version. make Any stable version - m4 Any stable version - git Any stable version - wget Any stable version - unzip Any stable version - xz Any stable version - readline-devel Any stable version - ncurses-devel Any stable version - zlid-devel Any stable version - gcc 7.1.0 or above You can run gcc -v to check the gcc version. gcc-c++ Any stable version - cmake 3.5.0 or above You can run cmake --version to check the cmake version. gettext Any stable version - curl Any stable version - redhat-lsb-core Any stable version - libstdc++-static Any stable version Only needed in CentOS 8+, RedHat 8+, and Fedora systems. libasan Any stable version Only needed in CentOS 8+, RedHat 8+, and Fedora systems. Other third-party software will be automatically downloaded and installed to the build directory at the configure (cmake) stage. Prepare software for compiling Nebula Graph \u00b6 This section guides you through the downloading and installation of software required for compiling Nebula Graph. Install dependencies. For CentOS, RedHat, and Fedora users, run the following commands. ```bash $ yum update $ yum install -y make \\ m4 \\ git \\ wget \\ unzip \\ xz \\ readline-devel \\ ncurses-devel \\ zlib-devel \\ gcc \\ gcc-c++ \\ cmake \\ gettext \\ curl \\ redhat-lsb-core // For CentOS 8+, RedHat 8+, and Fedora, install libstdc++-static, libasan as well $ yum install -y libstdc++-static libasan ``` For Debian and Ubuntu users, run the following commands. ```bash $ apt-get update $ apt-get install -y make \\ m4 \\ git \\ wget \\ unzip \\ xz-utils \\ curl \\ lsb-core \\ build-essential \\ libreadline-dev \\ ncurses-dev \\ cmake \\ gettext ``` Check if the GCC and cmake on your host are in the right version. See Software requirements for compiling Nebula Graph for the required versions. $ g++ --version $ cmake --version If your GCC and CMake are in the right version, then you are all set. If they are not, follow the sub-steps as follows. 1. Clone the nebula-common repository to your host. ```bash $ git clone https://github.com/vesoft-inc/nebula-common.git ``` The source code of Nebula Graph versions such as v2.0.0 is stored in particular branches. You can use the `--branch` or `-b` option to specify the branch to be cloned. For example, for 2.0.0, run the following command. ```bash $ git clone --branch v2.0.0 https://github.com/vesoft-inc/nebula-common.git ``` 2. Make nebula-common the current working directory. ```bash $ cd nebula-common ``` 3. Run the following commands to install and enable CMake and GCC. ```bash // Install CMake. $ ./third-party/install-cmake.sh cmake-install // Enable CMake $ source cmake-install/bin/enable-cmake.sh // Install GCC. Installing GCC to /opt requires root privilege, you can change it to other locations. $ sudo ./third-party/install-gcc.sh --prefix=/opt // Enable GCC. $ source /opt/vesoft/toolset/gcc/7.5.0/enable ``` Requirements and suggestions for installing Nebula Graph in test environments \u00b6 Hardware requirements for test environments \u00b6 Item Requirement CPU architecture x86_64 Number of CPU core 4 Memory 8 GB Disk 100 GB, SSD Supported operating systems for test environments \u00b6 For now, we can only install Nebula Graph in the Linux system. To install Nebula Graph in a test environment, we recommend that you use any Linux system with kernel version 3.9 or above. Suggested service architecture for test environments \u00b6 Process Suggested number metad (the metadata service process) 1 storaged (the storage service process) 1 or more graphd (the query engine service process) 1 or more For example, for a single-machine environment, you can deploy 1 metad, 1 storaged, and 1 graphd processes in the machine. For a more common environment, such as a cluster of 3 machines (named as A, B, and C), you can deploy Nebula Graph as follows: Machine name Number of metad Number of storaged Number of graphd A 1 1 1 B None 1 1 C None 1 1 Requirements and suggestions for installing Nebula Graph in production environments \u00b6 Hardware requirements for production environments \u00b6 Item Requirement CPU architecture x86_64 Number of CPU core 48 Memory 96 GB Disk 2 * 900 GB, NVMe SSD Supported operating systems for production environments \u00b6 For now, we can only install Nebula Graph in the Linux system. To install Nebula Graph in a production environment, we recommend that you use any Linux system with kernel version 3.9 or above. You can adjust some of the kernel parameters to better accommodate the need for running Nebula Graph. For more information, see kernel configuration . Suggested service architecture for production environments \u00b6 Process Suggested number metad (the metadata service process) 3 storaged (the storage service process) 3 or more graphd (the query engine service process) 3 or more Each metad process automatically creates and maintains a copy of the metadata. Usually, you only need 3 metad processes. The number of storaged processes does not affect the number of graph space copies. You can deploy multiple processes on a single machine. For example, on a cluster of 5 machines (named as A, B, C, D, and E), you can deploy Nebula Graph as follows: WARNING : Do not deploy a cluster across IDCs. Machine name Number of metad Number of storaged Number of graphd A 1 1 1 B 1 1 1 C 1 1 1 D None 1 1 E None 1 1 Capacity requirements for running a Nebula Graph cluster \u00b6 You can estimate the memory, disk space, and partition number needed for a Nebula Graph cluster of 3 replicas as follows. Resource Unit How to estimate Description Disk space for a cluster Bytes the_sum_of_edge_number_and_vertex_number * average_bytes_of_attributes * 6 * 120% - Memory for a cluster Bytes [ the_sum_of_edge_number_and_vertex_number * 15 + the_number_of_RocksDB_instances * ( write_buffer_size * max_write_buffer_number ) + rocksdb_block_cache ] * 120% write_buffer_size and max_write_buffer_number are RocksDB parameters, for more information, see MemTable . For details about rocksdb_block_cache , see Memory usage in RocksDB . Number of partitions for a graph space - the_number_of_disks_in_the_cluster * disk_partition_num_multiplier disk_partition_num_multiplier is an integer between 2 and 10 (both including). It's value depends on the disk performance. Use 2 for HDD. Question 1: Why do we multiply the disk space and memory by 120%? Answer: The extra 20% is for buffer. Question 2: How to get the number of RocksDB instances? Answer: Each directory in the --data_path item in the etc/nebula-storaged.conf file corresponds to a RocksDB instance. Count the number of directories to get the RocksDB instance number. NOTE : You can decrease the memory size occupied by the bloom filter by adding --enable_partitioned_index_filter=true in etc/nebula-storaged.conf . But it may decrease the read performance in some random-seek cases. About storage devices \u00b6 Nebula Graph is designed and implemented for NVMe SSD. All default parameters are optimized for the SSD devices. Due to the poor IOPS capability and long random seek latency, HDD is not recommended. You may encounter many problems when using HDD. And remote storage devices, such as NAS or SAN, are not recommended/tested as well. Use local SSD device.","title":"Resource preparations"},{"location":"4.deployment-and-installation/1.resource-preparations/#prepare_resources_for_compiling_installing_and_running_nebula_graph","text":"This topic describes the requirements and suggestions for compiling and installing Nebula Graph, as well as how to estimate the resource you need to reserve for running a Nebula Graph cluster.","title":"Prepare resources for compiling, installing, and running Nebula Graph"},{"location":"4.deployment-and-installation/1.resource-preparations/#reading_guide","text":"If you are reading this topic with the questions listed below, click them to jump to their answers. What do I need to compile Nebula Graph? What do I need to run Nebula Graph in a test environment? What do I need to run Nebula Graph in a production environment? How much memory and disk space do I need to reserve for my Nebula Graph cluster?","title":"Reading guide"},{"location":"4.deployment-and-installation/1.resource-preparations/#requirements_for_compiling_the_nebula_graph_source_code","text":"","title":"Requirements for compiling the Nebula Graph source code"},{"location":"4.deployment-and-installation/1.resource-preparations/#hardware_requirements_for_compiling_nebula_graph","text":"Item Requirement CPU architecture x86_64 Memory 4 GB Disk 10 GB, SSD","title":"Hardware requirements for compiling Nebula Graph"},{"location":"4.deployment-and-installation/1.resource-preparations/#supported_operating_systems_for_compiling_nebula_graph","text":"For now, we can only compile Nebula Graph in the Linux system. We recommend that you use any Linux system with kernel version 2.6.32 or above.","title":"Supported operating systems for compiling Nebula Graph"},{"location":"4.deployment-and-installation/1.resource-preparations/#software_requirements_for_compiling_nebula_graph","text":"You must have the correct version of the software listed below to compile Nebula Graph. If they are not as required or you are not sure, follow the steps in Prepare software for compiling Nebula Graph to get them ready. Software Version Note glibc 2.12 or above You can run ldd --version to check the glibc version. make Any stable version - m4 Any stable version - git Any stable version - wget Any stable version - unzip Any stable version - xz Any stable version - readline-devel Any stable version - ncurses-devel Any stable version - zlid-devel Any stable version - gcc 7.1.0 or above You can run gcc -v to check the gcc version. gcc-c++ Any stable version - cmake 3.5.0 or above You can run cmake --version to check the cmake version. gettext Any stable version - curl Any stable version - redhat-lsb-core Any stable version - libstdc++-static Any stable version Only needed in CentOS 8+, RedHat 8+, and Fedora systems. libasan Any stable version Only needed in CentOS 8+, RedHat 8+, and Fedora systems. Other third-party software will be automatically downloaded and installed to the build directory at the configure (cmake) stage.","title":"Software requirements for compiling Nebula Graph"},{"location":"4.deployment-and-installation/1.resource-preparations/#prepare_software_for_compiling_nebula_graph","text":"This section guides you through the downloading and installation of software required for compiling Nebula Graph. Install dependencies. For CentOS, RedHat, and Fedora users, run the following commands. ```bash $ yum update $ yum install -y make \\ m4 \\ git \\ wget \\ unzip \\ xz \\ readline-devel \\ ncurses-devel \\ zlib-devel \\ gcc \\ gcc-c++ \\ cmake \\ gettext \\ curl \\ redhat-lsb-core // For CentOS 8+, RedHat 8+, and Fedora, install libstdc++-static, libasan as well $ yum install -y libstdc++-static libasan ``` For Debian and Ubuntu users, run the following commands. ```bash $ apt-get update $ apt-get install -y make \\ m4 \\ git \\ wget \\ unzip \\ xz-utils \\ curl \\ lsb-core \\ build-essential \\ libreadline-dev \\ ncurses-dev \\ cmake \\ gettext ``` Check if the GCC and cmake on your host are in the right version. See Software requirements for compiling Nebula Graph for the required versions. $ g++ --version $ cmake --version If your GCC and CMake are in the right version, then you are all set. If they are not, follow the sub-steps as follows. 1. Clone the nebula-common repository to your host. ```bash $ git clone https://github.com/vesoft-inc/nebula-common.git ``` The source code of Nebula Graph versions such as v2.0.0 is stored in particular branches. You can use the `--branch` or `-b` option to specify the branch to be cloned. For example, for 2.0.0, run the following command. ```bash $ git clone --branch v2.0.0 https://github.com/vesoft-inc/nebula-common.git ``` 2. Make nebula-common the current working directory. ```bash $ cd nebula-common ``` 3. Run the following commands to install and enable CMake and GCC. ```bash // Install CMake. $ ./third-party/install-cmake.sh cmake-install // Enable CMake $ source cmake-install/bin/enable-cmake.sh // Install GCC. Installing GCC to /opt requires root privilege, you can change it to other locations. $ sudo ./third-party/install-gcc.sh --prefix=/opt // Enable GCC. $ source /opt/vesoft/toolset/gcc/7.5.0/enable ```","title":"Prepare software for compiling Nebula Graph"},{"location":"4.deployment-and-installation/1.resource-preparations/#requirements_and_suggestions_for_installing_nebula_graph_in_test_environments","text":"","title":"Requirements and suggestions for installing Nebula Graph in test environments"},{"location":"4.deployment-and-installation/1.resource-preparations/#hardware_requirements_for_test_environments","text":"Item Requirement CPU architecture x86_64 Number of CPU core 4 Memory 8 GB Disk 100 GB, SSD","title":"Hardware requirements for test environments"},{"location":"4.deployment-and-installation/1.resource-preparations/#supported_operating_systems_for_test_environments","text":"For now, we can only install Nebula Graph in the Linux system. To install Nebula Graph in a test environment, we recommend that you use any Linux system with kernel version 3.9 or above.","title":"Supported operating systems for test environments"},{"location":"4.deployment-and-installation/1.resource-preparations/#suggested_service_architecture_for_test_environments","text":"Process Suggested number metad (the metadata service process) 1 storaged (the storage service process) 1 or more graphd (the query engine service process) 1 or more For example, for a single-machine environment, you can deploy 1 metad, 1 storaged, and 1 graphd processes in the machine. For a more common environment, such as a cluster of 3 machines (named as A, B, and C), you can deploy Nebula Graph as follows: Machine name Number of metad Number of storaged Number of graphd A 1 1 1 B None 1 1 C None 1 1","title":"Suggested service architecture for test environments"},{"location":"4.deployment-and-installation/1.resource-preparations/#requirements_and_suggestions_for_installing_nebula_graph_in_production_environments","text":"","title":"Requirements and suggestions for installing Nebula Graph in production environments"},{"location":"4.deployment-and-installation/1.resource-preparations/#hardware_requirements_for_production_environments","text":"Item Requirement CPU architecture x86_64 Number of CPU core 48 Memory 96 GB Disk 2 * 900 GB, NVMe SSD","title":"Hardware requirements for production environments"},{"location":"4.deployment-and-installation/1.resource-preparations/#supported_operating_systems_for_production_environments","text":"For now, we can only install Nebula Graph in the Linux system. To install Nebula Graph in a production environment, we recommend that you use any Linux system with kernel version 3.9 or above. You can adjust some of the kernel parameters to better accommodate the need for running Nebula Graph. For more information, see kernel configuration .","title":"Supported operating systems for production environments"},{"location":"4.deployment-and-installation/1.resource-preparations/#suggested_service_architecture_for_production_environments","text":"Process Suggested number metad (the metadata service process) 3 storaged (the storage service process) 3 or more graphd (the query engine service process) 3 or more Each metad process automatically creates and maintains a copy of the metadata. Usually, you only need 3 metad processes. The number of storaged processes does not affect the number of graph space copies. You can deploy multiple processes on a single machine. For example, on a cluster of 5 machines (named as A, B, C, D, and E), you can deploy Nebula Graph as follows: WARNING : Do not deploy a cluster across IDCs. Machine name Number of metad Number of storaged Number of graphd A 1 1 1 B 1 1 1 C 1 1 1 D None 1 1 E None 1 1","title":"Suggested service architecture for production environments"},{"location":"4.deployment-and-installation/1.resource-preparations/#capacity_requirements_for_running_a_nebula_graph_cluster","text":"You can estimate the memory, disk space, and partition number needed for a Nebula Graph cluster of 3 replicas as follows. Resource Unit How to estimate Description Disk space for a cluster Bytes the_sum_of_edge_number_and_vertex_number * average_bytes_of_attributes * 6 * 120% - Memory for a cluster Bytes [ the_sum_of_edge_number_and_vertex_number * 15 + the_number_of_RocksDB_instances * ( write_buffer_size * max_write_buffer_number ) + rocksdb_block_cache ] * 120% write_buffer_size and max_write_buffer_number are RocksDB parameters, for more information, see MemTable . For details about rocksdb_block_cache , see Memory usage in RocksDB . Number of partitions for a graph space - the_number_of_disks_in_the_cluster * disk_partition_num_multiplier disk_partition_num_multiplier is an integer between 2 and 10 (both including). It's value depends on the disk performance. Use 2 for HDD. Question 1: Why do we multiply the disk space and memory by 120%? Answer: The extra 20% is for buffer. Question 2: How to get the number of RocksDB instances? Answer: Each directory in the --data_path item in the etc/nebula-storaged.conf file corresponds to a RocksDB instance. Count the number of directories to get the RocksDB instance number. NOTE : You can decrease the memory size occupied by the bloom filter by adding --enable_partitioned_index_filter=true in etc/nebula-storaged.conf . But it may decrease the read performance in some random-seek cases.","title":"Capacity requirements for running a Nebula Graph cluster"},{"location":"4.deployment-and-installation/1.resource-preparations/#about_storage_devices","text":"Nebula Graph is designed and implemented for NVMe SSD. All default parameters are optimized for the SSD devices. Due to the poor IOPS capability and long random seek latency, HDD is not recommended. You may encounter many problems when using HDD. And remote storage devices, such as NAS or SAN, are not recommended/tested as well. Use local SSD device.","title":"About storage devices"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/","text":"Upgrade Nebula Graph to v2.0.0 \u00b6 This topic describes how to upgrade Nebula Graph to v2.0.0. Limitations \u00b6 Rolling Upgrade is not supported. You must stop the Nebula Graph services before the upgrade. There is no upgrade script. You have to manually upgrade each server in the cluster. Supported versions: From Nebula Graph v1.2.0 to Nebula Graph v2.0.0 . From Nebula Graph v2.0.0-RC1 to Nebula Graph 2.0.0. This topic does not apply to scenarios where Nebula Graph is deployed with Docker, including Docker Swarm, Docker Compose, and Kubernetes. You must upgrade the old Nebula Graph services on the same machines they are deployed. DO NOT change the IP addresses, configuration files of the machines, and DO NOT change the cluster topology. The hard disk space of each machine should be three times as much as the space taken by the original data directories. Known issues that could cause data loss are listed on GitHub known issues . The issues are all related to altering schema or default values. To connect to Nebula Graph 2.0.0, you must upgrade all the Nebula Graph clients. The communication protocols of the old versions and the latest versions are not compatible. The upgrade takes about 30 minutes in this test environment . DO NOT use soft links to switch the data directories. You must have the sudo privileges to complete the steps in this topic. Installation paths \u00b6 Old installation path \u00b6 By default, old versions of Nebula Graph are installed in /usr/local/nebula/ , hereinafter referred to as ${nebula-old} . The default configuration file path is ${nebula-old}/etc/ . The data of the old Nebula Graph are stored by the Storage Service and the Meta Service. You can find the data paths as follows. Storage data path is defined by the --data_path option in the ${nebula-old}/etc/nebula-storaged.conf file. The default path is data/storage . Meta data path is defined by the --data_path option in the ${nebula-old}/etc/nebula-metad.conf file. The default path is data/meta . NOTE: The actual paths in your environment may be different from those described in this topic. You can run the Linux command ps -ef | grep nebula to locate them. New installation path \u00b6 ${nebula-new} represents the installation path of the new Nebula Graph version. An example for ${nebula-new} is /usr/local/nebula-new/ . Steps \u00b6 Stop all client connections. You can run the following commands on each Graph server to turn off the Graph Service and avoid dirty write. > ${ nebula -old } /scripts/nebula.service stop graphd [ INFO ] Stopping nebula-graphd... [ INFO ] Done Run the following commands to stop all services of the old version Nebula Graph. > ${ nebula -old } /scripts/nebula.service stop all [ INFO ] Stopping nebula-metad... [ INFO ] Done [ INFO ] Stopping nebula-graphd... [ INFO ] Done [ INFO ] Stopping nebula-storaged... [ INFO ] Done The Storage Service needs about 1 minute to flush data. Wait 1 minute and then run ps -ef | grep nebula to check and make sure that all the Nebula Graph services are stopped. NOTE: If the services are not fully stopped in 20 minutes, stop upgrading and go to the Nebula Graph community for help. Install the new version of Nebula Graph on each machine. * To install with RPM/DEB packages, run the following command. For detailed steps, see Install Nebula Graph with RPM or DEB package . ```bash > sudo rpm --force -i --prefix=${nebula-new} ${nebula-package-name.rpm} # for CentOS/RedHat > sudo dpkg -i --instdir==${nebula-new} ${nebula-package-name.deb} # for Ubuntu ``` * To install with the source code, follow the substeps. For detailed steps, see Install Nebula Graph by compiling the source code 1. Clone the source code. ```bash > git clone --branch v2.0.0 https://github.com/vesoft-inc/nebula-graph.git ``` 2. Configure CMake. ```bash > cmake -DCMAKE_INSTALL_PREFIX=${nebula-new} -DENABLE_BUILD_STORAGE=on -DENABLE_TESTING=OFF -DCMAKE_BUILD_TYPE=Release -DNEBULA_COMMON_REPO_TAG=v2.0.0 -DNEBULA_STORAGE_REPO_TAG=v2.0.0 .. ``` Copy the configuration files from the old path to the new path. > cp -rf ${ nebula -old } /etc ${ nebula -new } / Follow the substeps to prepare the Meta servers (usually 3 of them in a cluster). NOTE: You must make sure that this step is applied on every Meta server. 1. Locate the old Meta data path and copy the data files to the new path. ```bash > mkdir -p ${nebula-new}/data/meta/ > cp -r ${nebula-old}/data/meta/* ${nebula-new}/data/meta/ ``` 2. Modify the new Meta configuration files: ```bash > vim ${nebula-new}/nebula-metad.conf ``` [Optional] Add the following parameters in the Meta configuration files if you need them. * --null_type=false : Disables the support for using NULL as schema properties after the upgrade. The default value is true . When set to false , you must specify a default value when altering tags or edge types, otherwise, data reading fails. * --string_index_limit=32 : Specifies the index length for string values as 32. The default length is 64. Prepare the Storage configuration files on each Storage server. * If the old Storage data path is not the default setting --data_path=data/storage , Modify the Storage configuration file and change the value of --data_path as the new data path. ```bash > vim ${nebula-new}/nebula-storaged.conf ``` * Create the new Storage data directories. ```bash > mkdir -p ${nebula-new}/data/storage/ ``` > **NOTE:** If the `--data_path` default value has been modified, create the Storage data directories according to the modification. Start the new Meta Service. 1. Run the following command on each Meta server. ```bash $ sudo ${nebula-new}/scripts/nebula.service start metad [INFO] Starting nebula-metad... [INFO] Done ``` 2. Check if every nebula-metad process is started normally. ```bash $ ps -ef |grep nebula-metad ``` 3. Check if there is any error information in the Meta logs in ${nebula-new}/logs . If any nebula-metad process cannot start normally, stop upgrading, start the Nebula Graph services from the old directories, and take the error logs to the Nebula Graph community for help. Run the following commands to upgrade the Storage data format. $ sudo ${ nebula -new } /bin/db_upgrader \\ --src_db_path = <old_storage_directory_path> \\ --dst_db_path = <new_storage_directory_path> \\ --upgrade_meta_server = <meta_server_ip1>:<port1> [ ,<meta_server_ip2>:<port2>,... ] \\ --upgrade_version = <old_nebula_version> \\ The parameters are described as follows. * --src_db_path : Specifies the absolute path of the OLD Storage data directories. Separate multiple paths with commas, without spaces. * --dst_db_path : Specifies the absolute path of the NEW Storage data directories. Separate multiple paths with commas, without spaces. The paths must correspond to the paths set in --src_db_path one by one. DON'T: Don't mix up the preceding two parameters, otherwise, the old data will be damaged during the upgrade. * --upgrade_meta_server : Specifies the addresses of the new Meta servers that you started in step 7. * --upgrade_version : If the old Nebula Graph version is v1.2.0, set the parameter value to 1 . If the old version is v2.0.0-RC1, set the value to 2. DON'T: Don't set the value to other numbers. Example of upgrading from v1.2.0: $ sudo /usr/local/nebula_new/bin/db_upgrader \\ --src_db_path = /usr/local/nebula/data/storage/data1/,/usr/local/nebula/data/storage/data2/ \\ --dst_db_path = /usr/local/nebula_new/data/storage/data1/,/usr/local/nebula_new/data/storage/data2/ \\ --upgrade_meta_server = 192 .168.8.14:45500,192.168.8.15:45500,192.168.8.16:45500 \\ --upgrade_version = 1 Example of upgrading from v2.0.0-RC1: $ sudo /usr/local/nebula_new/bin/db_upgrader \\ --src_db_path = /usr/local/nebula/data/storage/ \\ --dst_db_path = /usr/local/nebula_new/data/storage/ \\ --upgrade_meta_server = 192 .168.8.14:9559,192.168.8.15:9559,192.168.8.16:9559 \\ --upgrade_version = 2 NOTE: Make sure that all the Storage servers have finished the upgrade. If anything goes wrong: Stop upgrading. Stop all the Meta servers. Start the Nebula Graph services from the old directories. Go to the Nebula Graph community for help. Start the new Storage Service on each Storage server. $ sudo ${ nebula -new } /scripts/nebula.service start storaged $ sudo ${ nebula -new } /scripts/nebula.service status storaged NOTE: If this step goes wrong on any server: Stop upgrading. Stop all the Meta servers and Storage servers. Start the Nebula Graph services from the old directories. Take the logs in ${nebula-new}/logs/ to the Nebula Graph community for help. Start the new Graph Service on each Graph server. $ sudo ${ nebula -new } /scripts/nebula.service start graphd $ sudo ${ nebula -new } /scripts/nebula.service status graphd NOTE: If this step goes wrong on any server: Stop upgrading. Stop all the Meta servers, Storage servers, and Graph servers. Start the Nebula Graph services from the old directories. Take the logs in ${nebula-new}/logs/ to the Nebula Graph community for help. Connect to Nebula Graph with the new version (v2.0.0 or later) of Nebula Console . Verify if the Nebula Graph services are available and if the data can be accessed normally. The command for connection, including the IP address and port of the Graph Service, is the same as the old one. The following statements may help in this step. nebula> SHOW HOSTS; nebula> SHOW SPACES; nebula> USE <space_name> nebula> SHOW PARTS; nebula> SUBMIT JOB STATS; nebula> SHOW STATS; DON'T: Don't use Nebula Console versions prior to v2.0.0. Upgrade other Nebula Graph clients. You must upgrade all other clients to corresponding v2.0.0 versions. The clients include but are not limited to the following ones. Find the v2.0.0 branch for each client. * studio * python * java * go * c++ * flink-connector * spark-util * benchmark NOTE: Communication protocols of the v2.0.0 versions are not compatible with that of the historical versions. To upgrade the clients, you must compile the v2.0.0 source code of the clients or download corresponding binaries. Tip for maintenance: The data path after the upgrade is ${nebula-new}/ . Modify relative paths for hard disk monitor systems or log ELK. Upgrade failure and rollback \u00b6 If the upgrade fails, stop all Nebula Graph services of the new version, and start the services of the old version. All Nebula Graph clients in use must be switched to the old version. Appendix 1: Test Environment \u00b6 The test environment for this topic is as follows: Machine specifications: 32 CPU cores, 62 GB memory, and SSD. Data size: 100 GB of Nebula Graph 1.2.0 LDBC test data, with 1 graph space, 24 partitions, and 92 GB of data directory size. Concurrent configuration: Parameter Default value Applied value in the Tests --max_concurrent 5 5 --max_concurrent_parts 10 24 --write_batch_num 100 100 The upgrade cost 21 minutes in all, including 21 minutes of compaction. Appendix 2: Nebula Graph V2.0.0 code address and commit ID \u00b6 Code address Commit ID Graph Service 7923a45 Storage and Meta Services 761f22b Common b2512aa FAQ \u00b6 Can I write through the client during the upgrade? \u00b6 A: No. The state of the data written during this process is undefined. Can I upgrade other old versions except for v1.2.0 or v2.0.0-RC1 to v2.0.0? \u00b6 A: Upgrading from other old versions is not tested. Theoretically, versions between v1.0.0 and v1.2.0 could adopt the upgrade approach for v1.2.0. V2.x nightly versions cannot apply the solutions in this topic. How to upgrade clients after the server upgrade? \u00b6 A: See step 12 in this topic. How to upgrade if a machine has only the Graph Service, but not the Storage Service? \u00b6 A: Upgrade the Graph Service with the corresponding binary or rpm package. How to resolve the error Permission denied ? \u00b6 A: Try again with the sudo privileges. Is there any change in gflags? \u00b6 A: Yes. For more information, see known gflags changes . What are the differences between deleting data then installing the new version and upgrading according to this topic? \u00b6 A: The default configurations for v2.x and v1.x are different, including the ports used. The upgrade solution keeps the old configurations, and the delete-and-install solution uses the new configurations. Is there a tool or solution for verifying data consistency after the upgrade? \u00b6 A: No.","title":"Upgrade Nebula Graph"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/#upgrade_nebula_graph_to_v200","text":"This topic describes how to upgrade Nebula Graph to v2.0.0.","title":"Upgrade Nebula Graph to v2.0.0"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/#limitations","text":"Rolling Upgrade is not supported. You must stop the Nebula Graph services before the upgrade. There is no upgrade script. You have to manually upgrade each server in the cluster. Supported versions: From Nebula Graph v1.2.0 to Nebula Graph v2.0.0 . From Nebula Graph v2.0.0-RC1 to Nebula Graph 2.0.0. This topic does not apply to scenarios where Nebula Graph is deployed with Docker, including Docker Swarm, Docker Compose, and Kubernetes. You must upgrade the old Nebula Graph services on the same machines they are deployed. DO NOT change the IP addresses, configuration files of the machines, and DO NOT change the cluster topology. The hard disk space of each machine should be three times as much as the space taken by the original data directories. Known issues that could cause data loss are listed on GitHub known issues . The issues are all related to altering schema or default values. To connect to Nebula Graph 2.0.0, you must upgrade all the Nebula Graph clients. The communication protocols of the old versions and the latest versions are not compatible. The upgrade takes about 30 minutes in this test environment . DO NOT use soft links to switch the data directories. You must have the sudo privileges to complete the steps in this topic.","title":"Limitations"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/#installation_paths","text":"","title":"Installation paths"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/#old_installation_path","text":"By default, old versions of Nebula Graph are installed in /usr/local/nebula/ , hereinafter referred to as ${nebula-old} . The default configuration file path is ${nebula-old}/etc/ . The data of the old Nebula Graph are stored by the Storage Service and the Meta Service. You can find the data paths as follows. Storage data path is defined by the --data_path option in the ${nebula-old}/etc/nebula-storaged.conf file. The default path is data/storage . Meta data path is defined by the --data_path option in the ${nebula-old}/etc/nebula-metad.conf file. The default path is data/meta . NOTE: The actual paths in your environment may be different from those described in this topic. You can run the Linux command ps -ef | grep nebula to locate them.","title":"Old installation path"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/#new_installation_path","text":"${nebula-new} represents the installation path of the new Nebula Graph version. An example for ${nebula-new} is /usr/local/nebula-new/ .","title":"New installation path"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/#steps","text":"Stop all client connections. You can run the following commands on each Graph server to turn off the Graph Service and avoid dirty write. > ${ nebula -old } /scripts/nebula.service stop graphd [ INFO ] Stopping nebula-graphd... [ INFO ] Done Run the following commands to stop all services of the old version Nebula Graph. > ${ nebula -old } /scripts/nebula.service stop all [ INFO ] Stopping nebula-metad... [ INFO ] Done [ INFO ] Stopping nebula-graphd... [ INFO ] Done [ INFO ] Stopping nebula-storaged... [ INFO ] Done The Storage Service needs about 1 minute to flush data. Wait 1 minute and then run ps -ef | grep nebula to check and make sure that all the Nebula Graph services are stopped. NOTE: If the services are not fully stopped in 20 minutes, stop upgrading and go to the Nebula Graph community for help. Install the new version of Nebula Graph on each machine. * To install with RPM/DEB packages, run the following command. For detailed steps, see Install Nebula Graph with RPM or DEB package . ```bash > sudo rpm --force -i --prefix=${nebula-new} ${nebula-package-name.rpm} # for CentOS/RedHat > sudo dpkg -i --instdir==${nebula-new} ${nebula-package-name.deb} # for Ubuntu ``` * To install with the source code, follow the substeps. For detailed steps, see Install Nebula Graph by compiling the source code 1. Clone the source code. ```bash > git clone --branch v2.0.0 https://github.com/vesoft-inc/nebula-graph.git ``` 2. Configure CMake. ```bash > cmake -DCMAKE_INSTALL_PREFIX=${nebula-new} -DENABLE_BUILD_STORAGE=on -DENABLE_TESTING=OFF -DCMAKE_BUILD_TYPE=Release -DNEBULA_COMMON_REPO_TAG=v2.0.0 -DNEBULA_STORAGE_REPO_TAG=v2.0.0 .. ``` Copy the configuration files from the old path to the new path. > cp -rf ${ nebula -old } /etc ${ nebula -new } / Follow the substeps to prepare the Meta servers (usually 3 of them in a cluster). NOTE: You must make sure that this step is applied on every Meta server. 1. Locate the old Meta data path and copy the data files to the new path. ```bash > mkdir -p ${nebula-new}/data/meta/ > cp -r ${nebula-old}/data/meta/* ${nebula-new}/data/meta/ ``` 2. Modify the new Meta configuration files: ```bash > vim ${nebula-new}/nebula-metad.conf ``` [Optional] Add the following parameters in the Meta configuration files if you need them. * --null_type=false : Disables the support for using NULL as schema properties after the upgrade. The default value is true . When set to false , you must specify a default value when altering tags or edge types, otherwise, data reading fails. * --string_index_limit=32 : Specifies the index length for string values as 32. The default length is 64. Prepare the Storage configuration files on each Storage server. * If the old Storage data path is not the default setting --data_path=data/storage , Modify the Storage configuration file and change the value of --data_path as the new data path. ```bash > vim ${nebula-new}/nebula-storaged.conf ``` * Create the new Storage data directories. ```bash > mkdir -p ${nebula-new}/data/storage/ ``` > **NOTE:** If the `--data_path` default value has been modified, create the Storage data directories according to the modification. Start the new Meta Service. 1. Run the following command on each Meta server. ```bash $ sudo ${nebula-new}/scripts/nebula.service start metad [INFO] Starting nebula-metad... [INFO] Done ``` 2. Check if every nebula-metad process is started normally. ```bash $ ps -ef |grep nebula-metad ``` 3. Check if there is any error information in the Meta logs in ${nebula-new}/logs . If any nebula-metad process cannot start normally, stop upgrading, start the Nebula Graph services from the old directories, and take the error logs to the Nebula Graph community for help. Run the following commands to upgrade the Storage data format. $ sudo ${ nebula -new } /bin/db_upgrader \\ --src_db_path = <old_storage_directory_path> \\ --dst_db_path = <new_storage_directory_path> \\ --upgrade_meta_server = <meta_server_ip1>:<port1> [ ,<meta_server_ip2>:<port2>,... ] \\ --upgrade_version = <old_nebula_version> \\ The parameters are described as follows. * --src_db_path : Specifies the absolute path of the OLD Storage data directories. Separate multiple paths with commas, without spaces. * --dst_db_path : Specifies the absolute path of the NEW Storage data directories. Separate multiple paths with commas, without spaces. The paths must correspond to the paths set in --src_db_path one by one. DON'T: Don't mix up the preceding two parameters, otherwise, the old data will be damaged during the upgrade. * --upgrade_meta_server : Specifies the addresses of the new Meta servers that you started in step 7. * --upgrade_version : If the old Nebula Graph version is v1.2.0, set the parameter value to 1 . If the old version is v2.0.0-RC1, set the value to 2. DON'T: Don't set the value to other numbers. Example of upgrading from v1.2.0: $ sudo /usr/local/nebula_new/bin/db_upgrader \\ --src_db_path = /usr/local/nebula/data/storage/data1/,/usr/local/nebula/data/storage/data2/ \\ --dst_db_path = /usr/local/nebula_new/data/storage/data1/,/usr/local/nebula_new/data/storage/data2/ \\ --upgrade_meta_server = 192 .168.8.14:45500,192.168.8.15:45500,192.168.8.16:45500 \\ --upgrade_version = 1 Example of upgrading from v2.0.0-RC1: $ sudo /usr/local/nebula_new/bin/db_upgrader \\ --src_db_path = /usr/local/nebula/data/storage/ \\ --dst_db_path = /usr/local/nebula_new/data/storage/ \\ --upgrade_meta_server = 192 .168.8.14:9559,192.168.8.15:9559,192.168.8.16:9559 \\ --upgrade_version = 2 NOTE: Make sure that all the Storage servers have finished the upgrade. If anything goes wrong: Stop upgrading. Stop all the Meta servers. Start the Nebula Graph services from the old directories. Go to the Nebula Graph community for help. Start the new Storage Service on each Storage server. $ sudo ${ nebula -new } /scripts/nebula.service start storaged $ sudo ${ nebula -new } /scripts/nebula.service status storaged NOTE: If this step goes wrong on any server: Stop upgrading. Stop all the Meta servers and Storage servers. Start the Nebula Graph services from the old directories. Take the logs in ${nebula-new}/logs/ to the Nebula Graph community for help. Start the new Graph Service on each Graph server. $ sudo ${ nebula -new } /scripts/nebula.service start graphd $ sudo ${ nebula -new } /scripts/nebula.service status graphd NOTE: If this step goes wrong on any server: Stop upgrading. Stop all the Meta servers, Storage servers, and Graph servers. Start the Nebula Graph services from the old directories. Take the logs in ${nebula-new}/logs/ to the Nebula Graph community for help. Connect to Nebula Graph with the new version (v2.0.0 or later) of Nebula Console . Verify if the Nebula Graph services are available and if the data can be accessed normally. The command for connection, including the IP address and port of the Graph Service, is the same as the old one. The following statements may help in this step. nebula> SHOW HOSTS; nebula> SHOW SPACES; nebula> USE <space_name> nebula> SHOW PARTS; nebula> SUBMIT JOB STATS; nebula> SHOW STATS; DON'T: Don't use Nebula Console versions prior to v2.0.0. Upgrade other Nebula Graph clients. You must upgrade all other clients to corresponding v2.0.0 versions. The clients include but are not limited to the following ones. Find the v2.0.0 branch for each client. * studio * python * java * go * c++ * flink-connector * spark-util * benchmark NOTE: Communication protocols of the v2.0.0 versions are not compatible with that of the historical versions. To upgrade the clients, you must compile the v2.0.0 source code of the clients or download corresponding binaries. Tip for maintenance: The data path after the upgrade is ${nebula-new}/ . Modify relative paths for hard disk monitor systems or log ELK.","title":"Steps"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/#upgrade_failure_and_rollback","text":"If the upgrade fails, stop all Nebula Graph services of the new version, and start the services of the old version. All Nebula Graph clients in use must be switched to the old version.","title":"Upgrade failure and rollback"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/#appendix_1_test_environment","text":"The test environment for this topic is as follows: Machine specifications: 32 CPU cores, 62 GB memory, and SSD. Data size: 100 GB of Nebula Graph 1.2.0 LDBC test data, with 1 graph space, 24 partitions, and 92 GB of data directory size. Concurrent configuration: Parameter Default value Applied value in the Tests --max_concurrent 5 5 --max_concurrent_parts 10 24 --write_batch_num 100 100 The upgrade cost 21 minutes in all, including 21 minutes of compaction.","title":"Appendix 1: Test Environment"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/#appendix_2_nebula_graph_v200_code_address_and_commit_id","text":"Code address Commit ID Graph Service 7923a45 Storage and Meta Services 761f22b Common b2512aa","title":"Appendix 2: Nebula Graph V2.0.0 code address and commit ID"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/#faq","text":"","title":"FAQ"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/#can_i_write_through_the_client_during_the_upgrade","text":"A: No. The state of the data written during this process is undefined.","title":"Can I write through the client during the upgrade?"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/#can_i_upgrade_other_old_versions_except_for_v120_or_v200-rc1_to_v200","text":"A: Upgrading from other old versions is not tested. Theoretically, versions between v1.0.0 and v1.2.0 could adopt the upgrade approach for v1.2.0. V2.x nightly versions cannot apply the solutions in this topic.","title":"Can I upgrade other old versions except for v1.2.0 or v2.0.0-RC1 to v2.0.0?"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/#how_to_upgrade_clients_after_the_server_upgrade","text":"A: See step 12 in this topic.","title":"How to upgrade clients after the server upgrade?"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/#how_to_upgrade_if_a_machine_has_only_the_graph_service_but_not_the_storage_service","text":"A: Upgrade the Graph Service with the corresponding binary or rpm package.","title":"How to upgrade if a machine has only the Graph Service, but not the Storage Service?"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/#how_to_resolve_the_error_permission_denied","text":"A: Try again with the sudo privileges.","title":"How to resolve the error Permission denied?"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/#is_there_any_change_in_gflags","text":"A: Yes. For more information, see known gflags changes .","title":"Is there any change in gflags?"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/#what_are_the_differences_between_deleting_data_then_installing_the_new_version_and_upgrading_according_to_this_topic","text":"A: The default configurations for v2.x and v1.x are different, including the ports used. The upgrade solution keeps the old configurations, and the delete-and-install solution uses the new configurations.","title":"What are the differences between deleting data then installing the new version and upgrading according to this topic?"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/#is_there_a_tool_or_solution_for_verifying_data_consistency_after_the_upgrade","text":"A: No.","title":"Is there a tool or solution for verifying data consistency after the upgrade?"},{"location":"4.deployment-and-installation/deploy-nebula-graph-cluster/","text":"Deploy Nebula Graph cluster \u00b6 This topic describes how to manually deploy a Nebula Graph cluster. NOTE: For now, Nebula Graph does not have an official deployment tool. Prerequisites \u00b6 Prepare hardware for deploying the cluster. Step 1: Install Nebula Graph \u00b6 Install Nebula Graph on each machine in the cluster. Available approaches of installation are as follows. Install Nebula Graph with RPM or DEB package Install Nebula Graph by compiling the source code Step 2: Modify the configurations \u00b6 To deploy Nebula Graph according to your requirements, you have to modify the configuration files. All the configuration files for Nebula Graph, including nebula-graphd.conf , nebula-metad.conf , and nebula-storaged.conf , are stored in the etc directory in the installation path. You only need to modify the configuration for the corresponding service on the machines. For example, modify nebula-graphd.conf on the machines where you want to deploy the Graph Service. For how to prepare the configuration files, see: Meta Service configurations Graph Service configurations Storage Service configurations Step 3: Start the cluster \u00b6 Start the corresponding service on each machine. The command to start the Nebula Graph services is as follows. sudo /usr/local/nebula/scripts/nebula.service start <metad|graphd|storaged|all> /usr/local/nebula is the default installation path for Nebula Graph. Use the actual path if you have customized the path. For more information about how to start and stop the services, see Manage Nebula Graph services . Connect to the cluster \u00b6 Connect to the Graph Service with a Nebula Graph client, such as Nebula Console. For more information, see Connect to Nebula Graph . Check the cluster status \u00b6 After connecting to the Nebula Graph cluster, run SHOW HOSTS to check the cluster status.","title":"Deploy Nebula Graph cluster"},{"location":"4.deployment-and-installation/deploy-nebula-graph-cluster/#deploy_nebula_graph_cluster","text":"This topic describes how to manually deploy a Nebula Graph cluster. NOTE: For now, Nebula Graph does not have an official deployment tool.","title":"Deploy Nebula Graph cluster"},{"location":"4.deployment-and-installation/deploy-nebula-graph-cluster/#prerequisites","text":"Prepare hardware for deploying the cluster.","title":"Prerequisites"},{"location":"4.deployment-and-installation/deploy-nebula-graph-cluster/#step_1_install_nebula_graph","text":"Install Nebula Graph on each machine in the cluster. Available approaches of installation are as follows. Install Nebula Graph with RPM or DEB package Install Nebula Graph by compiling the source code","title":"Step 1: Install Nebula Graph"},{"location":"4.deployment-and-installation/deploy-nebula-graph-cluster/#step_2_modify_the_configurations","text":"To deploy Nebula Graph according to your requirements, you have to modify the configuration files. All the configuration files for Nebula Graph, including nebula-graphd.conf , nebula-metad.conf , and nebula-storaged.conf , are stored in the etc directory in the installation path. You only need to modify the configuration for the corresponding service on the machines. For example, modify nebula-graphd.conf on the machines where you want to deploy the Graph Service. For how to prepare the configuration files, see: Meta Service configurations Graph Service configurations Storage Service configurations","title":"Step 2: Modify the configurations"},{"location":"4.deployment-and-installation/deploy-nebula-graph-cluster/#step_3_start_the_cluster","text":"Start the corresponding service on each machine. The command to start the Nebula Graph services is as follows. sudo /usr/local/nebula/scripts/nebula.service start <metad|graphd|storaged|all> /usr/local/nebula is the default installation path for Nebula Graph. Use the actual path if you have customized the path. For more information about how to start and stop the services, see Manage Nebula Graph services .","title":"Step 3: Start the cluster"},{"location":"4.deployment-and-installation/deploy-nebula-graph-cluster/#connect_to_the_cluster","text":"Connect to the Graph Service with a Nebula Graph client, such as Nebula Console. For more information, see Connect to Nebula Graph .","title":"Connect to the cluster"},{"location":"4.deployment-and-installation/deploy-nebula-graph-cluster/#check_the_cluster_status","text":"After connecting to the Nebula Graph cluster, run SHOW HOSTS to check the cluster status.","title":"Check the cluster status"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/","text":"Install Nebula Graph by compiling the source code \u00b6 Installing Nebula Graph from the source code allows you to customize the compiling and installation settings and test the latest features. Prerequisites \u00b6 You have prepared the necessary resources described in Prepare resources for compiling, installing, and running Nebula Graph . You can access the Internet from the host you plan to install Nebula Graph. The console is not complied or packaged with Nebula Graph server binaries. You can install nebula-console by yourself. How to install \u00b6 Use Git to clone the source code of Nebula Graph to your host. To install the latest developing version, run the following command to download the source code from the master branch. $ git clone https://github.com/vesoft-inc/nebula-graph.git To install a specific release version, use the --branch option to specify the correct branch. For example, to install 2.0.0, run the following command. $ git clone --branch v2.0.0 https://github.com/vesoft-inc/nebula-graph.git Make the nebula-graph directory the current working directory. $ cd nebula-graph Create a build directory and make it the current working directory. $ mkdir build && cd build Generate the Makefile with CMake. NOTE : The installation path is /user/local/nebula by default. To customize it, add the -DCMAKE_INSTALL_PREFIX=/your/install/path/ CMake variable in the following command. For more information about CMake variables, see CMake variables . If you are installing the latest developing version and has cloned the master branch in step 1, run the following command. $ cmake -DENABLE_BUILD_STORAGE = on -DENABLE_TESTING = OFF -DCMAKE_BUILD_TYPE = Release .. If you are installing a specific release version and has cloned the corresponding branch in step 1, use the -DNEBULA_COMMON_REPO_TAG and -DNEBULA_STORAGE_REPO_TAG options to specify the correct branches of the nebula-common and nebula-storage repositories. For example, to install release version 2.0.0, run the following command. $ cmake -DENABLE_BUILD_STORAGE = on -DENABLE_TESTING = OFF -DCMAKE_BUILD_TYPE = Release \\ -DNEBULA_COMMON_REPO_TAG = v2.0.0 -DNEBULA_STORAGE_REPO_TAG = v2.0.0 .. Compile Nebula Graph. To speed up the compiling, use the -j option to set a concurrent number N . It should be min(MEM/2, CPU) , where MEM is the memory size in GB, and CPU is the core number. $ make -j { N } # E.g., make -j4 This step will take about 20 minutes on a VM with four cores of Intel(R) Xeon(R) Platinum 8260M CPU @ 2.30GHz . Install Nebula Graph. $ sudo make install-all [Optional] Update the source code of the master branch. (It changes frequently.) 1. In the nebula-graph/ directory, you can use git pull upstream master to update the source code. 2. In nebula-graph/modules/common/ and nebula-graph/modules/storage/ , run git pull upstream master separately. 3. In nebula-graph/build/ , make and make install[-all] again. CMake variables \u00b6 Usage of CMake variables: $ cmake -D<variable> = <value> ... The following CMake variables can be used at the configure (cmake) stage to adjust the compiling settings. ENABLE_BUILD_STORAGE \u00b6 Starting from the 2.0 pre-release, Nebula Graph uses two separated github repositories of compute and storage. The ENABLE_BUILD_STORAGE variable is set to OFF by default so that the storage service is not installed together with the graph service. If you are deploying Nebula Graph on a single host for testing, you can set ENABLE_BUILD_STORAGE to ON to download and install the storage service automatically. CMAKE_INSTALL_PREFIX \u00b6 CMAKE_INSTALL_PREFIX specifies the path where the service modules, scripts, configuration files are installed. The default path is /usr/local/nebula . ENABLE_WERROR \u00b6 ENABLE_WERROR is ON by default and it makes all warnings into errors. You can set it to OFF if needed. ENABLE_TESTING \u00b6 ENABLE_TESTING is ON by default and unit tests are built with the Nebula Graph services. If you just need the service modules, set it to OFF . ENABLE_ASAN \u00b6 ENABLE_ASAN is OFF by default and the building of ASan (AddressSanitizer), a memory error detector, is disabled. To enable it, set ENABLE_ASAN to ON . This variable is intended for Nebula Graph developers. CMAKE_BUILD_TYPE \u00b6 Nebula Graph supports the following building types: Debug , the default value of CMAKE_BUILD_TYPE , indicates building Nebula Graph with the debug info but not the optimization options. Release , indicates building Nebula Graph with the optimization options but not the debug info. RelWithDebInfo , indicates building Nebula Graph with the optimization options and the debug info. MinSizeRel , indicates building Nebula Graph with the optimization options for controlling the code size but not the debug info. CMAKE_C_COMPILER/CMAKE_CXX_COMPILER \u00b6 Usually, CMake locates and uses a C/C++ compiler installed in the host automatically. But if your compiler is not installed at the standard path, or if you want to use a different one, run the command as follows to specify the installation path of the target compiler: $ cmake -DCMAKE_C_COMPILER = <path_to_gcc/bin/gcc> -DCMAKE_CXX_COMPILER = <path_to_gcc/bin/g++> .. $ cmake -DCMAKE_C_COMPILER = <path_to_clang/bin/clang> -DCMAKE_CXX_COMPILER = <path_to_clang/bin/clang++> .. ENABLE_CCACHE \u00b6 ENABLE_CCACHE is ON by default and ccache is used to speed up the compiling of Nebula Graph. To disable ccache , set ENABLE_CCACHE to OFF . On some platforms, the ccache installation hooks up or precedes the compiler. In such a case, you have to set an environment variable export CCACHE_DISABLE=true or add a line disable=true in ~/.ccache/ccache.conf as well. For more information, see the ccache official documentation . NEBULA_THIRDPARTY_ROOT \u00b6 NEBULA_THIRDPARTY_ROOT specifies the path where the third party software is installed. By default it is /opt/vesoft/third-party . What to do next \u00b6 Manage Nebula Graph services Connect to Nebula Graph Try Nebula Graph CRUD","title":"Install Nebula\u00a0Graph by compiling the source code"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#install_nebula_graph_by_compiling_the_source_code","text":"Installing Nebula Graph from the source code allows you to customize the compiling and installation settings and test the latest features.","title":"Install Nebula Graph by compiling the source code"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#prerequisites","text":"You have prepared the necessary resources described in Prepare resources for compiling, installing, and running Nebula Graph . You can access the Internet from the host you plan to install Nebula Graph. The console is not complied or packaged with Nebula Graph server binaries. You can install nebula-console by yourself.","title":"Prerequisites"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#how_to_install","text":"Use Git to clone the source code of Nebula Graph to your host. To install the latest developing version, run the following command to download the source code from the master branch. $ git clone https://github.com/vesoft-inc/nebula-graph.git To install a specific release version, use the --branch option to specify the correct branch. For example, to install 2.0.0, run the following command. $ git clone --branch v2.0.0 https://github.com/vesoft-inc/nebula-graph.git Make the nebula-graph directory the current working directory. $ cd nebula-graph Create a build directory and make it the current working directory. $ mkdir build && cd build Generate the Makefile with CMake. NOTE : The installation path is /user/local/nebula by default. To customize it, add the -DCMAKE_INSTALL_PREFIX=/your/install/path/ CMake variable in the following command. For more information about CMake variables, see CMake variables . If you are installing the latest developing version and has cloned the master branch in step 1, run the following command. $ cmake -DENABLE_BUILD_STORAGE = on -DENABLE_TESTING = OFF -DCMAKE_BUILD_TYPE = Release .. If you are installing a specific release version and has cloned the corresponding branch in step 1, use the -DNEBULA_COMMON_REPO_TAG and -DNEBULA_STORAGE_REPO_TAG options to specify the correct branches of the nebula-common and nebula-storage repositories. For example, to install release version 2.0.0, run the following command. $ cmake -DENABLE_BUILD_STORAGE = on -DENABLE_TESTING = OFF -DCMAKE_BUILD_TYPE = Release \\ -DNEBULA_COMMON_REPO_TAG = v2.0.0 -DNEBULA_STORAGE_REPO_TAG = v2.0.0 .. Compile Nebula Graph. To speed up the compiling, use the -j option to set a concurrent number N . It should be min(MEM/2, CPU) , where MEM is the memory size in GB, and CPU is the core number. $ make -j { N } # E.g., make -j4 This step will take about 20 minutes on a VM with four cores of Intel(R) Xeon(R) Platinum 8260M CPU @ 2.30GHz . Install Nebula Graph. $ sudo make install-all [Optional] Update the source code of the master branch. (It changes frequently.) 1. In the nebula-graph/ directory, you can use git pull upstream master to update the source code. 2. In nebula-graph/modules/common/ and nebula-graph/modules/storage/ , run git pull upstream master separately. 3. In nebula-graph/build/ , make and make install[-all] again.","title":"How to install"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#cmake_variables","text":"Usage of CMake variables: $ cmake -D<variable> = <value> ... The following CMake variables can be used at the configure (cmake) stage to adjust the compiling settings.","title":"CMake variables"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#enable_build_storage","text":"Starting from the 2.0 pre-release, Nebula Graph uses two separated github repositories of compute and storage. The ENABLE_BUILD_STORAGE variable is set to OFF by default so that the storage service is not installed together with the graph service. If you are deploying Nebula Graph on a single host for testing, you can set ENABLE_BUILD_STORAGE to ON to download and install the storage service automatically.","title":"ENABLE_BUILD_STORAGE"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#cmake_install_prefix","text":"CMAKE_INSTALL_PREFIX specifies the path where the service modules, scripts, configuration files are installed. The default path is /usr/local/nebula .","title":"CMAKE_INSTALL_PREFIX"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#enable_werror","text":"ENABLE_WERROR is ON by default and it makes all warnings into errors. You can set it to OFF if needed.","title":"ENABLE_WERROR"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#enable_testing","text":"ENABLE_TESTING is ON by default and unit tests are built with the Nebula Graph services. If you just need the service modules, set it to OFF .","title":"ENABLE_TESTING"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#enable_asan","text":"ENABLE_ASAN is OFF by default and the building of ASan (AddressSanitizer), a memory error detector, is disabled. To enable it, set ENABLE_ASAN to ON . This variable is intended for Nebula Graph developers.","title":"ENABLE_ASAN"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#cmake_build_type","text":"Nebula Graph supports the following building types: Debug , the default value of CMAKE_BUILD_TYPE , indicates building Nebula Graph with the debug info but not the optimization options. Release , indicates building Nebula Graph with the optimization options but not the debug info. RelWithDebInfo , indicates building Nebula Graph with the optimization options and the debug info. MinSizeRel , indicates building Nebula Graph with the optimization options for controlling the code size but not the debug info.","title":"CMAKE_BUILD_TYPE"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#cmake_c_compilercmake_cxx_compiler","text":"Usually, CMake locates and uses a C/C++ compiler installed in the host automatically. But if your compiler is not installed at the standard path, or if you want to use a different one, run the command as follows to specify the installation path of the target compiler: $ cmake -DCMAKE_C_COMPILER = <path_to_gcc/bin/gcc> -DCMAKE_CXX_COMPILER = <path_to_gcc/bin/g++> .. $ cmake -DCMAKE_C_COMPILER = <path_to_clang/bin/clang> -DCMAKE_CXX_COMPILER = <path_to_clang/bin/clang++> ..","title":"CMAKE_C_COMPILER/CMAKE_CXX_COMPILER"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#enable_ccache","text":"ENABLE_CCACHE is ON by default and ccache is used to speed up the compiling of Nebula Graph. To disable ccache , set ENABLE_CCACHE to OFF . On some platforms, the ccache installation hooks up or precedes the compiler. In such a case, you have to set an environment variable export CCACHE_DISABLE=true or add a line disable=true in ~/.ccache/ccache.conf as well. For more information, see the ccache official documentation .","title":"ENABLE_CCACHE"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#nebula_thirdparty_root","text":"NEBULA_THIRDPARTY_ROOT specifies the path where the third party software is installed. By default it is /opt/vesoft/third-party .","title":"NEBULA_THIRDPARTY_ROOT"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#what_to_do_next","text":"Manage Nebula Graph services Connect to Nebula Graph Try Nebula Graph CRUD","title":"What to do next"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/2.install-nebula-graph-by-rpm-or-deb/","text":"Install Nebula Graph with RPM or DEB package \u00b6 RPM and DEB are common package formats on Linux systems. This topic shows how to quickly install Nebula Graph with the RPM or DEB package. Prerequisites \u00b6 Prepare the right resources . NOTE: The console is not complied or packaged with Nebula Graph server binaries. You can install nebula-console by yourself. Steps \u00b6 Download the package from cloud service \u00b6 Download the release version. URL\uff1a //Centos 6 https://oss-cdn.nebula-graph.io/package/<release_version>/nebula-graph-<release_version>.el6.x86_64.rpm //Centos 7 https://oss-cdn.nebula-graph.io/package/<release_version>/nebula-graph-<release_version>.el7.x86_64.rpm //Centos 8 https://oss-cdn.nebula-graph.io/package/<release_version>/nebula-graph-<release_version>.el8.x86_64.rpm //Ubuntu 1604 https://oss-cdn.nebula-graph.io/package/<release_version>/nebula-graph-<release_version>.ubuntu1604.amd64.deb //Ubuntu 1804 https://oss-cdn.nebula-graph.io/package/<release_version>/nebula-graph-<release_version>.ubuntu1804.amd64.deb //Ubuntu 2004 https://oss-cdn.nebula-graph.io/package/<release_version>/nebula-graph-<release_version>.ubuntu2004.amd64.deb For example, download release package 2.0.0 for Centos 7.5 \uff1a wget https://oss-cdn.nebula-graph.io/package/2.0.0/nebula-graph-2.0.0.el7.x86_64.rpm wget https://oss-cdn.nebula-graph.io/package/2.0.0/nebula-graph-2.0.0.el7.x86_64.rpm.sha256sum.txt download release package 2.0.0 for Ubuntu 1804 \uff1a wget https://oss-cdn.nebula-graph.io/package/2.0.0/nebula-graph-2.0.0.ubuntu1804.amd64.deb wget https://oss-cdn.nebula-graph.io/package/2.0.0/nebula-graph-2.0.0.ubuntu1804.amd64.deb.sha256sum.txt Download the nightly version. DON'T: Nightly versions are usually used to test new features. Don't use it for production. URL\uff1a //Centos 6 https://oss-cdn.nebula-graph.io/package/v2-nightly/<yyyy.mm.dd>/nebula-graph-<yyyy.mm.dd>-nightly.el6.x86_64.rpm //Centos 7 https://oss-cdn.nebula-graph.io/package/v2-nightly/<yyyy.mm.dd>/nebula-graph-<yyyy.mm.dd>-nightly.el7.x86_64.rpm //Centos 8 https://oss-cdn.nebula-graph.io/package/v2-nightly/<yyyy.mm.dd>/nebula-graph-<yyyy.mm.dd>-nightly.el8.x86_64.rpm //Ubuntu 1604 https://oss-cdn.nebula-graph.io/package/v2-nightly/<yyyy.mm.dd>/nebula-graph-<yyyy.mm.dd>-nightly.ubuntu1604.amd64.deb //Ubuntu 1804 https://oss-cdn.nebula-graph.io/package/v2-nightly/<yyyy.mm.dd>/nebula-graph-<yyyy.mm.dd>-nightly.ubuntu1804.amd64.deb //Ubuntu 2004 https://oss-cdn.nebula-graph.io/package/v2-nightly/<yyyy.mm.dd>/nebula-graph-<yyyy.mm.dd>-nightly.ubuntu2004.amd64.deb For example, download the Centos 7.5 package developed and built in 2021.03.28 \uff1a wget https://oss-cdn.nebula-graph.io/package/v2-nightly/2021.03.28/nebula-graph-2021.03.28-nightly.el7.x86_64.rpm wget https://oss-cdn.nebula-graph.io/package/v2-nightly/2021.03.28/nebula-graph-2021.03.28-nightly.el7.x86_64.rpm.sha256sum.txt For example, download the Ubuntu 1804 package developed and built in 2021.03.28 \uff1a wget https://oss-cdn.nebula-graph.io/package/v2-nightly/2021.03.28/nebula-graph-2021.03.28-nightly.ubuntu1804.amd64.deb wget https://oss-cdn.nebula-graph.io/package/v2-nightly/2021.03.28/nebula-graph-2021.03.28-nightly.ubuntu1804.amd64.deb.sha256sum.txt Or, download the package from GitHub. \u00b6 Download the release version. + On the Nebula Graph Releases page, find the required version and click Assets . + In the Assets area, click the package to download it. Download the nightly version. DON'T: Nightly versions are usually used to test new features. Don't use it for production. + On the Nebula Graph package page, click the latest package on the top of the package list. + In the Artifacts area, click the package to download it. Install Nebula Graph. \u00b6 Use the following syntax to install with an RPM package. sudo rpm -ivh --prefix = <installation_path> <package_name> Use the following syntax to install with a DEB package. sudo dpkg -i --instdir == <installation_path> <package_name> NOTE: The default installation path is /usr/local/nebula/ . What to do next \u00b6 Start Nebula Graph","title":"Install Nebula Graph with RPM or DEB package"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/2.install-nebula-graph-by-rpm-or-deb/#install_nebula_graph_with_rpm_or_deb_package","text":"RPM and DEB are common package formats on Linux systems. This topic shows how to quickly install Nebula Graph with the RPM or DEB package.","title":"Install Nebula Graph with RPM or DEB package"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/2.install-nebula-graph-by-rpm-or-deb/#prerequisites","text":"Prepare the right resources . NOTE: The console is not complied or packaged with Nebula Graph server binaries. You can install nebula-console by yourself.","title":"Prerequisites"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/2.install-nebula-graph-by-rpm-or-deb/#steps","text":"","title":"Steps"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/2.install-nebula-graph-by-rpm-or-deb/#download_the_package_from_cloud_service","text":"Download the release version. URL\uff1a //Centos 6 https://oss-cdn.nebula-graph.io/package/<release_version>/nebula-graph-<release_version>.el6.x86_64.rpm //Centos 7 https://oss-cdn.nebula-graph.io/package/<release_version>/nebula-graph-<release_version>.el7.x86_64.rpm //Centos 8 https://oss-cdn.nebula-graph.io/package/<release_version>/nebula-graph-<release_version>.el8.x86_64.rpm //Ubuntu 1604 https://oss-cdn.nebula-graph.io/package/<release_version>/nebula-graph-<release_version>.ubuntu1604.amd64.deb //Ubuntu 1804 https://oss-cdn.nebula-graph.io/package/<release_version>/nebula-graph-<release_version>.ubuntu1804.amd64.deb //Ubuntu 2004 https://oss-cdn.nebula-graph.io/package/<release_version>/nebula-graph-<release_version>.ubuntu2004.amd64.deb For example, download release package 2.0.0 for Centos 7.5 \uff1a wget https://oss-cdn.nebula-graph.io/package/2.0.0/nebula-graph-2.0.0.el7.x86_64.rpm wget https://oss-cdn.nebula-graph.io/package/2.0.0/nebula-graph-2.0.0.el7.x86_64.rpm.sha256sum.txt download release package 2.0.0 for Ubuntu 1804 \uff1a wget https://oss-cdn.nebula-graph.io/package/2.0.0/nebula-graph-2.0.0.ubuntu1804.amd64.deb wget https://oss-cdn.nebula-graph.io/package/2.0.0/nebula-graph-2.0.0.ubuntu1804.amd64.deb.sha256sum.txt Download the nightly version. DON'T: Nightly versions are usually used to test new features. Don't use it for production. URL\uff1a //Centos 6 https://oss-cdn.nebula-graph.io/package/v2-nightly/<yyyy.mm.dd>/nebula-graph-<yyyy.mm.dd>-nightly.el6.x86_64.rpm //Centos 7 https://oss-cdn.nebula-graph.io/package/v2-nightly/<yyyy.mm.dd>/nebula-graph-<yyyy.mm.dd>-nightly.el7.x86_64.rpm //Centos 8 https://oss-cdn.nebula-graph.io/package/v2-nightly/<yyyy.mm.dd>/nebula-graph-<yyyy.mm.dd>-nightly.el8.x86_64.rpm //Ubuntu 1604 https://oss-cdn.nebula-graph.io/package/v2-nightly/<yyyy.mm.dd>/nebula-graph-<yyyy.mm.dd>-nightly.ubuntu1604.amd64.deb //Ubuntu 1804 https://oss-cdn.nebula-graph.io/package/v2-nightly/<yyyy.mm.dd>/nebula-graph-<yyyy.mm.dd>-nightly.ubuntu1804.amd64.deb //Ubuntu 2004 https://oss-cdn.nebula-graph.io/package/v2-nightly/<yyyy.mm.dd>/nebula-graph-<yyyy.mm.dd>-nightly.ubuntu2004.amd64.deb For example, download the Centos 7.5 package developed and built in 2021.03.28 \uff1a wget https://oss-cdn.nebula-graph.io/package/v2-nightly/2021.03.28/nebula-graph-2021.03.28-nightly.el7.x86_64.rpm wget https://oss-cdn.nebula-graph.io/package/v2-nightly/2021.03.28/nebula-graph-2021.03.28-nightly.el7.x86_64.rpm.sha256sum.txt For example, download the Ubuntu 1804 package developed and built in 2021.03.28 \uff1a wget https://oss-cdn.nebula-graph.io/package/v2-nightly/2021.03.28/nebula-graph-2021.03.28-nightly.ubuntu1804.amd64.deb wget https://oss-cdn.nebula-graph.io/package/v2-nightly/2021.03.28/nebula-graph-2021.03.28-nightly.ubuntu1804.amd64.deb.sha256sum.txt","title":"Download the package from cloud service"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/2.install-nebula-graph-by-rpm-or-deb/#or_download_the_package_from_github","text":"Download the release version. + On the Nebula Graph Releases page, find the required version and click Assets . + In the Assets area, click the package to download it. Download the nightly version. DON'T: Nightly versions are usually used to test new features. Don't use it for production. + On the Nebula Graph package page, click the latest package on the top of the package list. + In the Artifacts area, click the package to download it.","title":"Or, download the package from GitHub."},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/2.install-nebula-graph-by-rpm-or-deb/#install_nebula_graph","text":"Use the following syntax to install with an RPM package. sudo rpm -ivh --prefix = <installation_path> <package_name> Use the following syntax to install with a DEB package. sudo dpkg -i --instdir == <installation_path> <package_name> NOTE: The default installation path is /usr/local/nebula/ .","title":"Install Nebula Graph."},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/2.install-nebula-graph-by-rpm-or-deb/#what_to_do_next","text":"Start Nebula Graph","title":"What to do next"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/3.install-nebula-console/","text":"Install Nebula Console \u00b6 The nebula-console is not complied or packaged with Nebula Graph server binaries. You can install nebula-console by yourself.","title":"Install Nebula Console"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/3.install-nebula-console/#install_nebula_console","text":"The nebula-console is not complied or packaged with Nebula Graph server binaries. You can install nebula-console by yourself.","title":"Install Nebula Console"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/1.text-based-index-restrictions/","text":"Full-text index restrictions \u00b6 This document holds the restrictions for full-text indexes. Please read the restrictions very carefully before using the full-text indexes. For now, full-text search has the following limitations: The maximum indexing string length is 256 bytes. To index data that is longer than 256 bytes, store your data in a reverse order. Full-text index can not be applied to more than one property at a time (similar to a composite index). The WHERE clause in full-text search statement LOOKUP does not support logical expressions AND and OR . Full-text index can not be applied to multiple tags search. Ideographic language Chinese does not have word delimiters. Therefore, the built-in full-text parser cannot determine where words begin and end in Chinese. Install the elasticsearch-analysis-ik to parse Chinese. Sorting for the returned results of the full-text search is not supported. Data is returned in the order of data insertion. Full-text index can not search the null properties. Rebuilding or altering Elasticsearch indexes is not supported at this time. Pipe is not supported in the LOOKUP statement, excluding the examples in our document. Full-text search only works on single terms. Full-text indexes are not deleted together with the graph space. Make sure that you start the Elasticsearch cluster and Nebula Graph at the same time. If not, the data writing on the Elasticsearch cluster can be incomplete. Do not contain ' or \\ in the vertex or edge values. If not, a error is caused in the Elasticsearch cluster storage. It may take a while for Elasticsearch to create indexes. If Nebula Graph warns no index is found, wait for the index to take effect.","title":"Full-text restrictions"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/1.text-based-index-restrictions/#full-text_index_restrictions","text":"This document holds the restrictions for full-text indexes. Please read the restrictions very carefully before using the full-text indexes. For now, full-text search has the following limitations: The maximum indexing string length is 256 bytes. To index data that is longer than 256 bytes, store your data in a reverse order. Full-text index can not be applied to more than one property at a time (similar to a composite index). The WHERE clause in full-text search statement LOOKUP does not support logical expressions AND and OR . Full-text index can not be applied to multiple tags search. Ideographic language Chinese does not have word delimiters. Therefore, the built-in full-text parser cannot determine where words begin and end in Chinese. Install the elasticsearch-analysis-ik to parse Chinese. Sorting for the returned results of the full-text search is not supported. Data is returned in the order of data insertion. Full-text index can not search the null properties. Rebuilding or altering Elasticsearch indexes is not supported at this time. Pipe is not supported in the LOOKUP statement, excluding the examples in our document. Full-text search only works on single terms. Full-text indexes are not deleted together with the graph space. Make sure that you start the Elasticsearch cluster and Nebula Graph at the same time. If not, the data writing on the Elasticsearch cluster can be incomplete. Do not contain ' or \\ in the vertex or edge values. If not, a error is caused in the Elasticsearch cluster storage. It may take a while for Elasticsearch to create indexes. If Nebula Graph warns no index is found, wait for the index to take effect.","title":"Full-text index restrictions"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/2.deploy-es/","text":"Deploy full-text index \u00b6 Nebula Graph full-text indexes are powered by Elasticsearch . This means that you can use Elasticsearch full-text query language to retrieve what you want. Full-text indexes are managed through built-in procedures. They can be created only for variable STRING and FIXED_STRING properties when the listener cluster and the Elasticsearch cluster are deployed. Before you start \u00b6 Before you start using the full-text index, please make sure that you know the restrictions . Deploy Elasticsearch cluster \u00b6 To deploy an Elasticsearch cluster, see the Elasticsearch documentation . When the Elasticsearch cluster is started, add the template file for the Nebula Graph full-text index. Take the following sample template for example: { \"template\" : \"nebula*\" , \"settings\" : { \"index\" : { \"number_of_shards\" : 3 , \"number_of_replicas\" : 1 } }, \"mappings\" : { \"properties\" : { \"tag_id\" : { \"type\" : \"long\" }, \"column_id\" : { \"type\" : \"text\" }, \"value\" :{ \"type\" : \"keyword\" } } } } Make sure that you specify the following fields in strict accordance with the preceding template format: \"template\" : \"nebula*\" \"tag_id\" : { \"type\" : \"long\" }, \"column_id\" : { \"type\" : \"text\" }, \"value\" :{ \"type\" : \"keyword\" } You can configure the Elasticsearch to meet your business needs. To customize the Elasticsearch, see Elasticsearch Document . Sign in to the text search clients \u00b6 SIGN IN TEXT SERVICE [(<elastic_ip:port> [,<username>, <password>]), (<elastic_ip:port>), ...] When the Elasticsearch cluster is deployed, use the SIGN IN statement to sign in to the Elasticsearch clients. Multiple elastic_ip:port pairs are separated with commas. You must use the IPs and the port number in the configuration file for the Elasticsearch. For example: nebula> SIGN IN TEXT SERVICE (127.0.0.1:9200); Elasticsearch does not have username or password by default. If you configured a username and password, you need to specify in the SIGN IN statement. Show text search clients \u00b6 SHOW TEXT SEARCH CLIENTS Use the SHOW TEXT SEARCH CLIENTS statement to list the text search clients. For example: nebula> SHOW TEXT SEARCH CLIENTS; +-------------+------+ | Host | Port | +-------------+------+ | \"127.0.0.1\" | 9200 | +-------------+------+ | \"127.0.0.1\" | 9200 | +-------------+------+ | \"127.0.0.1\" | 9200 | +-------------+------+ Sign out to the text search clients \u00b6 SIGN OUT TEXT SERVICE Use the SIGN OUT TEXT SERVICE to sign out all the text search clients. For example: nebula> SIGN OUT TEXT SERVICE;","title":"Deploy Elasticsearch cluster"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/2.deploy-es/#deploy_full-text_index","text":"Nebula Graph full-text indexes are powered by Elasticsearch . This means that you can use Elasticsearch full-text query language to retrieve what you want. Full-text indexes are managed through built-in procedures. They can be created only for variable STRING and FIXED_STRING properties when the listener cluster and the Elasticsearch cluster are deployed.","title":"Deploy full-text index"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/2.deploy-es/#before_you_start","text":"Before you start using the full-text index, please make sure that you know the restrictions .","title":"Before you start"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/2.deploy-es/#deploy_elasticsearch_cluster","text":"To deploy an Elasticsearch cluster, see the Elasticsearch documentation . When the Elasticsearch cluster is started, add the template file for the Nebula Graph full-text index. Take the following sample template for example: { \"template\" : \"nebula*\" , \"settings\" : { \"index\" : { \"number_of_shards\" : 3 , \"number_of_replicas\" : 1 } }, \"mappings\" : { \"properties\" : { \"tag_id\" : { \"type\" : \"long\" }, \"column_id\" : { \"type\" : \"text\" }, \"value\" :{ \"type\" : \"keyword\" } } } } Make sure that you specify the following fields in strict accordance with the preceding template format: \"template\" : \"nebula*\" \"tag_id\" : { \"type\" : \"long\" }, \"column_id\" : { \"type\" : \"text\" }, \"value\" :{ \"type\" : \"keyword\" } You can configure the Elasticsearch to meet your business needs. To customize the Elasticsearch, see Elasticsearch Document .","title":"Deploy Elasticsearch cluster"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/2.deploy-es/#sign_in_to_the_text_search_clients","text":"SIGN IN TEXT SERVICE [(<elastic_ip:port> [,<username>, <password>]), (<elastic_ip:port>), ...] When the Elasticsearch cluster is deployed, use the SIGN IN statement to sign in to the Elasticsearch clients. Multiple elastic_ip:port pairs are separated with commas. You must use the IPs and the port number in the configuration file for the Elasticsearch. For example: nebula> SIGN IN TEXT SERVICE (127.0.0.1:9200); Elasticsearch does not have username or password by default. If you configured a username and password, you need to specify in the SIGN IN statement.","title":"Sign in to the text search clients"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/2.deploy-es/#show_text_search_clients","text":"SHOW TEXT SEARCH CLIENTS Use the SHOW TEXT SEARCH CLIENTS statement to list the text search clients. For example: nebula> SHOW TEXT SEARCH CLIENTS; +-------------+------+ | Host | Port | +-------------+------+ | \"127.0.0.1\" | 9200 | +-------------+------+ | \"127.0.0.1\" | 9200 | +-------------+------+ | \"127.0.0.1\" | 9200 | +-------------+------+","title":"Show text search clients"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/2.deploy-es/#sign_out_to_the_text_search_clients","text":"SIGN OUT TEXT SERVICE Use the SIGN OUT TEXT SERVICE to sign out all the text search clients. For example: nebula> SIGN OUT TEXT SERVICE;","title":"Sign out to the text search clients"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/","text":"Deploy Raft Listener for Nebula Storage service \u00b6 Full-Text index data is written to the Elasticsearch cluster asynchronously. The Raft Listener (hereinafter shortened as Listener) is a separate process that fetches data from the Storage Service and writes them into the Elasticsearch cluster. Prerequisites \u00b6 You have read and fully understand the restrictions for using Full-Text indexes. You have deployed a Nebula Graph cluster . You have prepared at least one extra Storage Server. To use the Full-Text search, you must run one or more Storage Server as the Raft Listener. Precautions \u00b6 The Storage Service that you want to run as a Listener must have the same or later version with all the other Nebula Graph services in the cluster. For now, you can only add Listeners to a graph space once and for all. Trying to add listeners to a graph space that already has a listener will fail. To add multiple listeners, set them in one statement . Step 1: Prepare the configuration file for the Listeners \u00b6 You have to prepare a Listener configuration file as follows on the machine that you want to deploy the Listeners. The file name must be nebula-storaged-listener.conf . A template configuration file is provided for your reference. To use the template configuration file, rename it as nebula-storaged-listener.conf . NOTE: Use real IP addresses in the configuration file instead of domain names or loopback IP addresses such as 127.0.0.1. ########## basics ########## # Whether to run as a daemon process --daemonize=true # The file to host the process id --pid_file=pids_listener/nebula-storaged.pid ########## logging ########## # The directory to host logging files, which must already exist --log_dir=logs_listener # Log level, 0, 1, 2, 3 for INFO, WARNING, ERROR, FATAL respectively --minloglevel=0 # Verbose log level, 1, 2, 3, 4, the higher of the level, the more verbose of the logging --v=0 # Maximum seconds to buffer the log messages --logbufsecs=0 # Whether to redirect stdout and stderr to separate output files --redirect_stdout=true # Destination filename of stdout and stderr, which will also reside in log_dir. --stdout_log_file=storaged-stdout.log --stderr_log_file=storaged-stderr.log # Copy log messages at or above this level to stderr in addition to logfiles. The numbers of severity levels INFO, WARNING, ERROR, and FATAL are 0, 1, 2, and 3, respectively. --stderrthreshold=2 ########## networking ########## # Meta server address --meta_server_addrs=192.168.2.1:45500 # Local ip --local_ip=192.168.2.4 # Storage daemon listening port --port=44510 # HTTP service ip --ws_ip=192.168.2.4 # HTTP service port --ws_http_port=12021 # HTTP2 service port --ws_h2_port=12031 # heartbeat with meta service --heartbeat_interval_secs=10 ########## storage ########## # Listener wal directory. only one path is allowed. --listener_path=data/listener # This parameter can be ignored for compatibility. let's fill A default value of \"data\" --data_path=data # The type of part manager, [memory | meta] --part_man_type=memory # The default reserved bytes for one batch operation --rocksdb_batch_size=4096 # The default block cache size used in BlockBasedTable. # The unit is MB. --rocksdb_block_cache=4 # The type of storage engine, `rocksdb', `memory', etc. --engine_type=rocksdb # The type of part, `simple', `consensus'... --part_type=simple Step 2: Start the Listeners \u00b6 Run the following command to start the Listeners. ./bin/nebula-storaged --flagfile ${ listener_config_path } /nebula-storaged-listener.conf ${listener_config_path} is the path where you store the Listener configuration file. Step 3: Add Listeners to Nebula Graph \u00b6 Connect to Nebula Graph and run USE <space> to enter the graph space that you want to create Full-Text indexes for. Then run the following statement to add the Listener into Nebula Graph. NOTE: You must use real IPs for the listeners. ADD LISTENER ELASTICSEARCH <listener_ip:port> [,<listener_ip:port>, ...] Multiple listener_ip:port pairs are separated with commas. For example: nebula> ADD LISTENER ELASTICSEARCH 192.168.8.5:46780,192.168.8.6:46780; Show Listeners \u00b6 Run the SHOW LISTENER statement to list the Listeners. For example: nebula> SHOW LISTENER; +--------+-----------------+-----------------------+----------+ | PartId | Type | Host | Status | +--------+-----------------+-----------------------+----------+ | 1 | \"ELASTICSEARCH\" | \"[192.168.8.5:46780]\" | \"ONLINE\" | +--------+-----------------+-----------------------+----------+ | 2 | \"ELASTICSEARCH\" | \"[192.168.8.5:46780]\" | \"ONLINE\" | +--------+-----------------+-----------------------+----------+ | 3 | \"ELASTICSEARCH\" | \"[192.168.8.5:46780]\" | \"ONLINE\" | +--------+-----------------+-----------------------+----------+ Remove Listeners \u00b6 Run the REMOVE LISTENER ELASTICSEARCH statement to remove all the Elasticsearch Listeners for a graph space. For example: nebula> REMOVE LISTENER ELASTICSEARCH; What to do next \u00b6 After deploying the Elasticsearch cluster and the Listeners, Full-Text indexes are created automatically on the Elasticsearch cluster. You can do Full-Text search now. For more information, see Full-Text search .","title":"Deploy Raft Listener cluster"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/#deploy_raft_listener_for_nebula_storage_service","text":"Full-Text index data is written to the Elasticsearch cluster asynchronously. The Raft Listener (hereinafter shortened as Listener) is a separate process that fetches data from the Storage Service and writes them into the Elasticsearch cluster.","title":"Deploy Raft Listener for Nebula Storage service"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/#prerequisites","text":"You have read and fully understand the restrictions for using Full-Text indexes. You have deployed a Nebula Graph cluster . You have prepared at least one extra Storage Server. To use the Full-Text search, you must run one or more Storage Server as the Raft Listener.","title":"Prerequisites"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/#precautions","text":"The Storage Service that you want to run as a Listener must have the same or later version with all the other Nebula Graph services in the cluster. For now, you can only add Listeners to a graph space once and for all. Trying to add listeners to a graph space that already has a listener will fail. To add multiple listeners, set them in one statement .","title":"Precautions"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/#step_1_prepare_the_configuration_file_for_the_listeners","text":"You have to prepare a Listener configuration file as follows on the machine that you want to deploy the Listeners. The file name must be nebula-storaged-listener.conf . A template configuration file is provided for your reference. To use the template configuration file, rename it as nebula-storaged-listener.conf . NOTE: Use real IP addresses in the configuration file instead of domain names or loopback IP addresses such as 127.0.0.1. ########## basics ########## # Whether to run as a daemon process --daemonize=true # The file to host the process id --pid_file=pids_listener/nebula-storaged.pid ########## logging ########## # The directory to host logging files, which must already exist --log_dir=logs_listener # Log level, 0, 1, 2, 3 for INFO, WARNING, ERROR, FATAL respectively --minloglevel=0 # Verbose log level, 1, 2, 3, 4, the higher of the level, the more verbose of the logging --v=0 # Maximum seconds to buffer the log messages --logbufsecs=0 # Whether to redirect stdout and stderr to separate output files --redirect_stdout=true # Destination filename of stdout and stderr, which will also reside in log_dir. --stdout_log_file=storaged-stdout.log --stderr_log_file=storaged-stderr.log # Copy log messages at or above this level to stderr in addition to logfiles. The numbers of severity levels INFO, WARNING, ERROR, and FATAL are 0, 1, 2, and 3, respectively. --stderrthreshold=2 ########## networking ########## # Meta server address --meta_server_addrs=192.168.2.1:45500 # Local ip --local_ip=192.168.2.4 # Storage daemon listening port --port=44510 # HTTP service ip --ws_ip=192.168.2.4 # HTTP service port --ws_http_port=12021 # HTTP2 service port --ws_h2_port=12031 # heartbeat with meta service --heartbeat_interval_secs=10 ########## storage ########## # Listener wal directory. only one path is allowed. --listener_path=data/listener # This parameter can be ignored for compatibility. let's fill A default value of \"data\" --data_path=data # The type of part manager, [memory | meta] --part_man_type=memory # The default reserved bytes for one batch operation --rocksdb_batch_size=4096 # The default block cache size used in BlockBasedTable. # The unit is MB. --rocksdb_block_cache=4 # The type of storage engine, `rocksdb', `memory', etc. --engine_type=rocksdb # The type of part, `simple', `consensus'... --part_type=simple","title":"Step 1: Prepare the configuration file for the Listeners"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/#step_2_start_the_listeners","text":"Run the following command to start the Listeners. ./bin/nebula-storaged --flagfile ${ listener_config_path } /nebula-storaged-listener.conf ${listener_config_path} is the path where you store the Listener configuration file.","title":"Step 2: Start the Listeners"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/#step_3_add_listeners_to_nebula_graph","text":"Connect to Nebula Graph and run USE <space> to enter the graph space that you want to create Full-Text indexes for. Then run the following statement to add the Listener into Nebula Graph. NOTE: You must use real IPs for the listeners. ADD LISTENER ELASTICSEARCH <listener_ip:port> [,<listener_ip:port>, ...] Multiple listener_ip:port pairs are separated with commas. For example: nebula> ADD LISTENER ELASTICSEARCH 192.168.8.5:46780,192.168.8.6:46780;","title":"Step 3: Add Listeners to Nebula Graph"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/#show_listeners","text":"Run the SHOW LISTENER statement to list the Listeners. For example: nebula> SHOW LISTENER; +--------+-----------------+-----------------------+----------+ | PartId | Type | Host | Status | +--------+-----------------+-----------------------+----------+ | 1 | \"ELASTICSEARCH\" | \"[192.168.8.5:46780]\" | \"ONLINE\" | +--------+-----------------+-----------------------+----------+ | 2 | \"ELASTICSEARCH\" | \"[192.168.8.5:46780]\" | \"ONLINE\" | +--------+-----------------+-----------------------+----------+ | 3 | \"ELASTICSEARCH\" | \"[192.168.8.5:46780]\" | \"ONLINE\" | +--------+-----------------+-----------------------+----------+","title":"Show Listeners"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/#remove_listeners","text":"Run the REMOVE LISTENER ELASTICSEARCH statement to remove all the Elasticsearch Listeners for a graph space. For example: nebula> REMOVE LISTENER ELASTICSEARCH;","title":"Remove Listeners"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/#what_to_do_next","text":"After deploying the Elasticsearch cluster and the Listeners, Full-Text indexes are created automatically on the Elasticsearch cluster. You can do Full-Text search now. For more information, see Full-Text search .","title":"What to do next"},{"location":"5.configurations-and-logs/1.configurations/1.configurations/","text":"Configurations \u00b6 This document gives some introduction to configurations in Nebula Graph. For the path and usage of local configuration files for Nebula Graph services, see: Meta configuration Graph configuration Storage configuration Get configurations \u00b6 Most configurations are gflags. You can get all the gflags and the explanations by the following command. <binary> --help For example: $ ./nebula-metad --help $ ./nebula-graphd --help $ ./nebula-storaged --help $ ./nebula-console --help Besides, you can get the values of running flags by curl -ing from the services. For example: $ curl 127 .0.0.1:19559/flags # From Meta $ curl 127 .0.0.1:19669/flags # From Graph $ curl 127 .0.0.1:19779/flags # From Storage Modify configurations \u00b6 We suggest that you change configurations from local configure files. To change configurations from local files, follow these steps: Add --local_config=true to each configuration file. The configuration files are stored in /usr/local/nebula/etc/ by default. If you have customized your Nebula Graph installation directory, the path to your configuration files is $pwd/nebula/etc/ . Save your modification to the files. Restart the Nebula Graph services. NOTE : Remember to add --local_config=true to each configuration file. To make your modifications take effect, restart all the Nebula Graph services. Legacy Issues \u00b6 The curl commands and parameters in Nebula Graph v2.x. are different from Nebula Graph v1.x. Those curl commands in v1.x are deprecated now.","title":"Configurations"},{"location":"5.configurations-and-logs/1.configurations/1.configurations/#configurations","text":"This document gives some introduction to configurations in Nebula Graph. For the path and usage of local configuration files for Nebula Graph services, see: Meta configuration Graph configuration Storage configuration","title":"Configurations"},{"location":"5.configurations-and-logs/1.configurations/1.configurations/#get_configurations","text":"Most configurations are gflags. You can get all the gflags and the explanations by the following command. <binary> --help For example: $ ./nebula-metad --help $ ./nebula-graphd --help $ ./nebula-storaged --help $ ./nebula-console --help Besides, you can get the values of running flags by curl -ing from the services. For example: $ curl 127 .0.0.1:19559/flags # From Meta $ curl 127 .0.0.1:19669/flags # From Graph $ curl 127 .0.0.1:19779/flags # From Storage","title":"Get configurations"},{"location":"5.configurations-and-logs/1.configurations/1.configurations/#modify_configurations","text":"We suggest that you change configurations from local configure files. To change configurations from local files, follow these steps: Add --local_config=true to each configuration file. The configuration files are stored in /usr/local/nebula/etc/ by default. If you have customized your Nebula Graph installation directory, the path to your configuration files is $pwd/nebula/etc/ . Save your modification to the files. Restart the Nebula Graph services. NOTE : Remember to add --local_config=true to each configuration file. To make your modifications take effect, restart all the Nebula Graph services.","title":"Modify configurations"},{"location":"5.configurations-and-logs/1.configurations/1.configurations/#legacy_issues","text":"The curl commands and parameters in Nebula Graph v2.x. are different from Nebula Graph v1.x. Those curl commands in v1.x are deprecated now.","title":"Legacy Issues"},{"location":"5.configurations-and-logs/1.configurations/2.meta-config/","text":"Meta Service configuration \u00b6 Nebula Graph provides two initial configuration files for the Meta Service: nebula-metad.conf.default and nebula-metad.conf.production . You can use them in different scenarios. The default file path is /usr/local/nebula/etc/ . How to use the configuration files \u00b6 The Meta Service gets its configuration from the nebula-metad.conf file. You have to remove the suffix .default or .production from an initial configuration file for the Meta Service to apply the configuration defined in it. If you have modified the configuration in the file and want new configuration to take effect, add --local_conf=true at the top of the file. Otherwise, Nebula Graph reads the cached configuration. About parameter values \u00b6 If a parameter is not set in the configuration file, Nebula Graph uses the default value. NOTE: The default value of a parameter in Nebula Graph may be different from the predefined value in the .default and .production files. The predefined parameters in nebula-metad.conf.default and nebula-metad.conf.production are different. And not all parameters are predefined. This topic uses the parameters in nebula-metad.conf.default . Nebula Graph provides two initial configuration files for the Meta Service: nebula-metad.conf.default and nebula-metad.conf.production . You can use them in different scenarios. The default file path is /usr/local/nebula/etc/ . Basic configurations \u00b6 Name Predefine Value Descriptions daemonize true When set to true , the process is a daemon process. pid_file pids/nebula-metad.pid File to host the process ID. Logging configurations \u00b6 Name Predefine Value Descriptions log_dir logs Directory to the Meta Service log. We recommend that you put logs on a different hard disk from the data_path . minloglevel 0 Specifies the minimum log level. Available values are 0 (INFO), 1 (WARNING), 2 (ERROR), and 3 (FATAL). We suggest that you set minloglevel to 0 for debugging and 1 for production. When you set it to 4 , Nebula Graph does not print any logs. v 0 Specifies the verbose log level. Available values are 0-4. The larger the value, the more verbose the log. logbufsecs 0 Specifies the maximum time to buffer the logs. The configuration is measured in seconds. stdout_log_file metad-stdout.log Specifies the filename for the stdout log. stderr_log_file metad-stderr.log Specifies the filename for the stderr log. stderrthreshold 2 Specifies the minimum level to copy the log messages to stderr. Networking configurations \u00b6 Name Predefine Value Descriptions meta_server_addrs 127.0.0.1:9559 Specifies the IP addresses for the Meta Services. Separate multiple addresses with commas. local_ip 127.0.0.1 Specifies the local IP for the Meta Service. port 9559 Specifies RPC daemon listening port. The external port for Meta Service is 9559 . The internal port is external_port + 1 , i.e., 9560 . Nebula Graph uses the internal port for multi-replica interactions. ws_ip 0.0.0.0 Specifies the IP address for the HTTP service. ws_http_port 19559 Specifies the port for the HTTP service. ws_h2_port 19560 Specifies the port for the HTTP2 service. heartbeat_interval_secs 10 Specifies the default heartbeat interval in seconds. Make sure the heartbeat_interval_secs values for all services are the same, otherwise Nebula Graph CANNOT work normally. NOTE : We recommend that you use the real IP address in your configuration because sometimes 127.0.0.1 can not be parsed correctly. Storage configurations \u00b6 Name Predefine Value Descriptions data_path data/meta (i.e. /usr/local/nebula/data/meta/) Directory for cluster metadata persistence Misc configurations \u00b6 Name Predefine Value Descriptions default_parts_num 100 Specifies the default partition number when you create a new graph space. default_replica_factor 1 Specifies the default replica factor number when you create a new graph space. RocksDB options \u00b6 Name Predefine Value Descriptions rocksdb_wal_sync true Enable or disable RocksDB WAL synchronization. Available values are true (enable) and false (disable).","title":"Meta Service configurations"},{"location":"5.configurations-and-logs/1.configurations/2.meta-config/#meta_service_configuration","text":"Nebula Graph provides two initial configuration files for the Meta Service: nebula-metad.conf.default and nebula-metad.conf.production . You can use them in different scenarios. The default file path is /usr/local/nebula/etc/ .","title":"Meta Service configuration"},{"location":"5.configurations-and-logs/1.configurations/2.meta-config/#how_to_use_the_configuration_files","text":"The Meta Service gets its configuration from the nebula-metad.conf file. You have to remove the suffix .default or .production from an initial configuration file for the Meta Service to apply the configuration defined in it. If you have modified the configuration in the file and want new configuration to take effect, add --local_conf=true at the top of the file. Otherwise, Nebula Graph reads the cached configuration.","title":"How to use the configuration files"},{"location":"5.configurations-and-logs/1.configurations/2.meta-config/#about_parameter_values","text":"If a parameter is not set in the configuration file, Nebula Graph uses the default value. NOTE: The default value of a parameter in Nebula Graph may be different from the predefined value in the .default and .production files. The predefined parameters in nebula-metad.conf.default and nebula-metad.conf.production are different. And not all parameters are predefined. This topic uses the parameters in nebula-metad.conf.default . Nebula Graph provides two initial configuration files for the Meta Service: nebula-metad.conf.default and nebula-metad.conf.production . You can use them in different scenarios. The default file path is /usr/local/nebula/etc/ .","title":"About parameter values"},{"location":"5.configurations-and-logs/1.configurations/2.meta-config/#basic_configurations","text":"Name Predefine Value Descriptions daemonize true When set to true , the process is a daemon process. pid_file pids/nebula-metad.pid File to host the process ID.","title":"Basic configurations"},{"location":"5.configurations-and-logs/1.configurations/2.meta-config/#logging_configurations","text":"Name Predefine Value Descriptions log_dir logs Directory to the Meta Service log. We recommend that you put logs on a different hard disk from the data_path . minloglevel 0 Specifies the minimum log level. Available values are 0 (INFO), 1 (WARNING), 2 (ERROR), and 3 (FATAL). We suggest that you set minloglevel to 0 for debugging and 1 for production. When you set it to 4 , Nebula Graph does not print any logs. v 0 Specifies the verbose log level. Available values are 0-4. The larger the value, the more verbose the log. logbufsecs 0 Specifies the maximum time to buffer the logs. The configuration is measured in seconds. stdout_log_file metad-stdout.log Specifies the filename for the stdout log. stderr_log_file metad-stderr.log Specifies the filename for the stderr log. stderrthreshold 2 Specifies the minimum level to copy the log messages to stderr.","title":"Logging configurations"},{"location":"5.configurations-and-logs/1.configurations/2.meta-config/#networking_configurations","text":"Name Predefine Value Descriptions meta_server_addrs 127.0.0.1:9559 Specifies the IP addresses for the Meta Services. Separate multiple addresses with commas. local_ip 127.0.0.1 Specifies the local IP for the Meta Service. port 9559 Specifies RPC daemon listening port. The external port for Meta Service is 9559 . The internal port is external_port + 1 , i.e., 9560 . Nebula Graph uses the internal port for multi-replica interactions. ws_ip 0.0.0.0 Specifies the IP address for the HTTP service. ws_http_port 19559 Specifies the port for the HTTP service. ws_h2_port 19560 Specifies the port for the HTTP2 service. heartbeat_interval_secs 10 Specifies the default heartbeat interval in seconds. Make sure the heartbeat_interval_secs values for all services are the same, otherwise Nebula Graph CANNOT work normally. NOTE : We recommend that you use the real IP address in your configuration because sometimes 127.0.0.1 can not be parsed correctly.","title":"Networking configurations"},{"location":"5.configurations-and-logs/1.configurations/2.meta-config/#storage_configurations","text":"Name Predefine Value Descriptions data_path data/meta (i.e. /usr/local/nebula/data/meta/) Directory for cluster metadata persistence","title":"Storage configurations"},{"location":"5.configurations-and-logs/1.configurations/2.meta-config/#misc_configurations","text":"Name Predefine Value Descriptions default_parts_num 100 Specifies the default partition number when you create a new graph space. default_replica_factor 1 Specifies the default replica factor number when you create a new graph space.","title":"Misc configurations"},{"location":"5.configurations-and-logs/1.configurations/2.meta-config/#rocksdb_options","text":"Name Predefine Value Descriptions rocksdb_wal_sync true Enable or disable RocksDB WAL synchronization. Available values are true (enable) and false (disable).","title":"RocksDB options"},{"location":"5.configurations-and-logs/1.configurations/3.graph-config/","text":"Graph Service configuration \u00b6 Nebula Graph provides two initial configuration files for the Graph Service: nebula-graphd.conf.default and nebula-graphd.conf.production . You can use them in different scenarios. The default file path is /usr/local/nebula/etc/ . How to use the configuration files \u00b6 The Graph Service gets its configuration from the nebula-graphd.conf file. You have to remove the suffix .default or .production from an initial configuration file for the Graph Service to apply the configuration defined in it. If you have modified the configuration in the file and want new configuration to take effect, add --local_conf=true at the top of the file. Otherwise, Nebula Graph reads the cached configuration. About parameter values \u00b6 If a parameter is not set in the configuration file, Nebula Graph uses its default value. NOTE: The default value of a parameter in Nebula Graph may be different from the predefined value in the .default and .production files. The predefined parameters in nebula-graphd.conf.default and nebula-graphd.conf.production are different. And not all parameters are predefined. This topic uses the parameters in nebula-graphd.conf.default . Basic configurations \u00b6 Name Predefine Value Descriptions daemonize true When set to true , the process is a daemon process. pid_file pids/nebula-graphd.pid File to host the process ID. enable_optimizer true When set to true , the optimizer is enabled. Logging configurations \u00b6 Name Predefine Value Descriptions log_dir logs Directory to the Graph Service log. We recommend that you put logs on a different hard disk from the data_path . minloglevel 0 Specifies the minimum log level. Available values are 0 (INFO), 1 (WARNING), 2 (ERROR), and 3 (FATAL). We suggest that you set minloglevel to 0 for debugging and 1 for production. When you set it to 4 , Nebula Graph does not print any logs. v 0 Specifies the verbose log level. Available values are 0-4. The larger the value, the more verbose the log. logbufsecs 0 Specifies the maximum time to buffer the logs. The configuration is measured in seconds. redirect_stdout true When set to true , stdout and stderr are redirected. stdout_log_file graphd-stdout.log Specifies the filename for the stdout log. stderr_log_file graphd-stderr.log Specifies the filename for the stderr log. stderrthreshold 2 Specifies the minimum level to copy the log messages to stderr. Networking configurations \u00b6 Name Predefine Value Descriptions meta_server_addrs 127.0.0.1:9559 Specifies the IP addresses for the Graph Services. Separate multiple addresses with commas. local_ip 127.0.0.1 Specifies the local IP for the Graph Service. listen_netdev any Specifies the network device to listen on. port 9669 Specifies RPC daemon listening port. The external port for the Graph Service is 9669 . The internal port is port+1 , namely 9670 . Nebula Graph uses the internal port for multi-replica interactions. reuse_port false When set to false , the SO_REUSEPORT is closed. listen_backlog 1024 Specifies the backlog for the listen socket. You must modify this configuration together with the net.core.somaxconn . client_idle_timeout_secs 0 Specifies the time to close an idle connection. This configuration is measured in seconds. session_idle_timeout_secs 0 Specifies the time to expire an idle session. This configuration is measured in seconds. num_accept_threads 1 Specifies the thread number to accept incoming connections. num_netio_threads 0 Specifies the networking IO threads number. 0 is the number of CPU cores. num_worker_threads 0 Specifies the thread number to execute user queries. 0 is the number of CPU cores. ws_ip 0.0.0.0 Specifies the IP address for the HTTP service. ws_http_port 19669 Specifies the port for the HTTP service. ws_h2_port 19670 Specifies the port for the HTTP2 service. heartbeat_interval_secs 10 Specifies the default heartbeat interval in seconds. Make sure the heartbeat_interval_secs values for all services are the same, otherwise Nebula Graph CANNOT work normally. storage_client_timeout_ms - Specifies the RPC connection timeout threshold between the Graph Service and the Storage Service. This parameter is not predefined in the initial configuration files. You can manually set it if you need it. The system default value is 60000 ms. NOTE : We recommend that you use the real IP address in your configuration because sometimes 127.0.0.1 can not be parsed correctly. Charset and collate configurations \u00b6 Name Predefine Value Descriptions default_charset utf8 Specifies the default charset when you create a new graph space. default_collate utf8_bin Specifies the default collate when you create a new graph space. Authorization and authentication configurations \u00b6 Name Predefine Value Descriptions enable_authorize false When set to false , the system authentication is not enabled. For more information, see Authentication . auth_type password Specifies the login method. Available values are password , ldap , and cloud . If you have set enable_authorize to true , you can only log in with the root account. For example: /usr/local/nebula/bin/nebula -u root -p nebula --addr = 127 .0.0.1 --port = 9669 If you have set enable_authorize to false , you can log in with any account and password. For example: /usr/local/nebula/bin/nebula -u any -p 123 --addr = 127 .0.0.1 --port = 9669","title":"Graph Service configurations"},{"location":"5.configurations-and-logs/1.configurations/3.graph-config/#graph_service_configuration","text":"Nebula Graph provides two initial configuration files for the Graph Service: nebula-graphd.conf.default and nebula-graphd.conf.production . You can use them in different scenarios. The default file path is /usr/local/nebula/etc/ .","title":"Graph Service configuration"},{"location":"5.configurations-and-logs/1.configurations/3.graph-config/#how_to_use_the_configuration_files","text":"The Graph Service gets its configuration from the nebula-graphd.conf file. You have to remove the suffix .default or .production from an initial configuration file for the Graph Service to apply the configuration defined in it. If you have modified the configuration in the file and want new configuration to take effect, add --local_conf=true at the top of the file. Otherwise, Nebula Graph reads the cached configuration.","title":"How to use the configuration files"},{"location":"5.configurations-and-logs/1.configurations/3.graph-config/#about_parameter_values","text":"If a parameter is not set in the configuration file, Nebula Graph uses its default value. NOTE: The default value of a parameter in Nebula Graph may be different from the predefined value in the .default and .production files. The predefined parameters in nebula-graphd.conf.default and nebula-graphd.conf.production are different. And not all parameters are predefined. This topic uses the parameters in nebula-graphd.conf.default .","title":"About parameter values"},{"location":"5.configurations-and-logs/1.configurations/3.graph-config/#basic_configurations","text":"Name Predefine Value Descriptions daemonize true When set to true , the process is a daemon process. pid_file pids/nebula-graphd.pid File to host the process ID. enable_optimizer true When set to true , the optimizer is enabled.","title":"Basic configurations"},{"location":"5.configurations-and-logs/1.configurations/3.graph-config/#logging_configurations","text":"Name Predefine Value Descriptions log_dir logs Directory to the Graph Service log. We recommend that you put logs on a different hard disk from the data_path . minloglevel 0 Specifies the minimum log level. Available values are 0 (INFO), 1 (WARNING), 2 (ERROR), and 3 (FATAL). We suggest that you set minloglevel to 0 for debugging and 1 for production. When you set it to 4 , Nebula Graph does not print any logs. v 0 Specifies the verbose log level. Available values are 0-4. The larger the value, the more verbose the log. logbufsecs 0 Specifies the maximum time to buffer the logs. The configuration is measured in seconds. redirect_stdout true When set to true , stdout and stderr are redirected. stdout_log_file graphd-stdout.log Specifies the filename for the stdout log. stderr_log_file graphd-stderr.log Specifies the filename for the stderr log. stderrthreshold 2 Specifies the minimum level to copy the log messages to stderr.","title":"Logging configurations"},{"location":"5.configurations-and-logs/1.configurations/3.graph-config/#networking_configurations","text":"Name Predefine Value Descriptions meta_server_addrs 127.0.0.1:9559 Specifies the IP addresses for the Graph Services. Separate multiple addresses with commas. local_ip 127.0.0.1 Specifies the local IP for the Graph Service. listen_netdev any Specifies the network device to listen on. port 9669 Specifies RPC daemon listening port. The external port for the Graph Service is 9669 . The internal port is port+1 , namely 9670 . Nebula Graph uses the internal port for multi-replica interactions. reuse_port false When set to false , the SO_REUSEPORT is closed. listen_backlog 1024 Specifies the backlog for the listen socket. You must modify this configuration together with the net.core.somaxconn . client_idle_timeout_secs 0 Specifies the time to close an idle connection. This configuration is measured in seconds. session_idle_timeout_secs 0 Specifies the time to expire an idle session. This configuration is measured in seconds. num_accept_threads 1 Specifies the thread number to accept incoming connections. num_netio_threads 0 Specifies the networking IO threads number. 0 is the number of CPU cores. num_worker_threads 0 Specifies the thread number to execute user queries. 0 is the number of CPU cores. ws_ip 0.0.0.0 Specifies the IP address for the HTTP service. ws_http_port 19669 Specifies the port for the HTTP service. ws_h2_port 19670 Specifies the port for the HTTP2 service. heartbeat_interval_secs 10 Specifies the default heartbeat interval in seconds. Make sure the heartbeat_interval_secs values for all services are the same, otherwise Nebula Graph CANNOT work normally. storage_client_timeout_ms - Specifies the RPC connection timeout threshold between the Graph Service and the Storage Service. This parameter is not predefined in the initial configuration files. You can manually set it if you need it. The system default value is 60000 ms. NOTE : We recommend that you use the real IP address in your configuration because sometimes 127.0.0.1 can not be parsed correctly.","title":"Networking configurations"},{"location":"5.configurations-and-logs/1.configurations/3.graph-config/#charset_and_collate_configurations","text":"Name Predefine Value Descriptions default_charset utf8 Specifies the default charset when you create a new graph space. default_collate utf8_bin Specifies the default collate when you create a new graph space.","title":"Charset and collate configurations"},{"location":"5.configurations-and-logs/1.configurations/3.graph-config/#authorization_and_authentication_configurations","text":"Name Predefine Value Descriptions enable_authorize false When set to false , the system authentication is not enabled. For more information, see Authentication . auth_type password Specifies the login method. Available values are password , ldap , and cloud . If you have set enable_authorize to true , you can only log in with the root account. For example: /usr/local/nebula/bin/nebula -u root -p nebula --addr = 127 .0.0.1 --port = 9669 If you have set enable_authorize to false , you can log in with any account and password. For example: /usr/local/nebula/bin/nebula -u any -p 123 --addr = 127 .0.0.1 --port = 9669","title":"Authorization and authentication configurations"},{"location":"5.configurations-and-logs/1.configurations/4.storage-config/","text":"Storage Service configurations \u00b6 Nebula Graph provides two initial configuration files for the Storage Service: nebula-storaged.conf.default and nebula-storaged.conf.production . You can use them in different scenarios. The default file path is /usr/local/nebula/etc/ . NOTE: Raft Listener is different from the Storage Service. For more information, see Raft Listener . How to use the configuration files \u00b6 The Storage Service gets its configuration from the nebula-storaged.conf file. You have to remove the suffix .default or .production from an initial configuration file for the Storage Service to apply the configuration defined in it. If you have modified the configuration in the file and want the new configuration to take effect, add --local_conf=true at the top of the file. Otherwise, Nebula Graph reads the cached configuration. About parameter values \u00b6 If a parameter is not set in the configuration file, Nebula Graph uses its default value. NOTE: The default value of a parameter in Nebula Graph may be different from the predefined value in the .default and .production files. The predefined parameter in nebula-storaged.conf.default and nebula-storaged.conf.production are different. And not all parameters are predefined. This topic uses the parameters in nebula-storaged.conf.default . Basic configurations \u00b6 Name Predefine Value Descriptions daemonize true When set to true , the process is a daemon process. pid_file pids/nebula-storaged.pid File to host the process ID. Logging configurations \u00b6 Name Predefine Value Descriptions log_dir logs Directory to the Storage Service log. We recommend that you put logs on a different hard disk from the data_path . minloglevel 0 Specifies the minimum log level. Available values are 0-3. 0 , 1 , 2 , and 3 are INFO , WARNING , ERROR , and FATAL . We suggest that you set minloglevel to 0 for debug, 1 for production. When you set it to 4 , Nebula Graph does not print any logs. v 0 Specifies the verbose log level. Available values are 0-4. The larger the value, the more verbose the log. logbufsecs 0 Specifies the maximum time to buffer the logs. The configuration is measured in seconds. redirect_stdout true When set to true , stdout and stderr are redirected. stdout_log_file storaged-stdout.log Specifies the filename for the stdout log. stderr_log_file storaged-stderr.log Specifies the filename for the stderr log. stderrthreshold 2 Specifies the minimum level to copy the log messages to stderr. Available values are 0-3. 0 , 1 , 2 , and 3 are INFO , WARNING , ERROR , and FATAL . Networking configurations \u00b6 Name Predefine Value Descriptions meta_server_addrs 127.0.0.1:9559 Specifies the IP addresses for the Meta Services. Separate multiple addresses with commas. local_ip 127.0.0.1 Specifies the local IP for the Storage Service. port 9779 Specifies RPC daemon listening port. The external port for Storage Service is 9779 . The internal port is port+1 , namely 9780 . Nebula Graph uses the internal port for multi-replica interactions. ws_ip 0.0.0.0 Specifies the IP address for the HTTP service. ws_http_port 19779 Specifies the port for the HTTP service. ws_h2_port 19780 Specifies the port for the HTTP2 service. heartbeat_interval_secs 10 Specifies the default heartbeat interval in seconds. Make sure the heartbeat_interval_secs values for all services are the same, otherwise Nebula Graph CANNOT work normally. NOTE : We recommend that you use the real IP address in your configuration because sometimes 127.0.0.1 can not be parsed correctly. Raft configurations \u00b6 Name Predefine Value Descriptions raft_heartbeat_interval_secs 30 Specifies the timeout for the Raft election. The configuration is measured in seconds. raft_rpc_timeout_ms 500 Specifies the timeout for the Raft RPC. The configuration is measured in milliseconds. wal_ttl 14400 Specifies the recycle RAFT wal time. The configuration is measured in seconds. Disk configurations \u00b6 Name Predefine Value Descriptions data_path data/storage Specifies the root data path. Separate multiple paths with commas. rocksdb_batch_size 4096 Specifies the block cache for a batch operation. The configuration is measured in bytes. rocksdb_block_cache 4 Specifies the block cache for BlockBasedTable. The configuration is measured in megabytes. engine_type rocksdb Specifies the engine type. rocksdb_compression lz4 Specifies the compression algorithm for RocksDB. Available values are no , snappy , lz4 , lz4hc , zlib , bzip2 , and zstd . rocksdb_compression_per_level \\ Specifies compression for each level. enable_rocksdb_statistics false When set to false , RocksDB statistics is disabled. rocksdb_stats_level kExceptHistogramOrTimers Specifies the stats level for RocksDB. Available values are kExceptHistogramOrTimers , kExceptTimers , kExceptDetailedTimers , kExceptTimeForMutex , and kAll . enable_rocksdb_prefix_filtering false When set to true , the prefix bloom filter for RocksDB is enabled. Enabling prefix bloom filter reduces memory usage. rocksdb_filtering_prefix_length 12 Specifies the prefix length for each key. Available values are 12 and 16 . RocksDB options \u00b6 The format of the RocksDB options is {\"<option_name>\":\"<option_value>\"} . Multiple options are separated with commas. Name Predefine Value Descriptions rocksdb_db_options {} Specifies the RocksDB options. rocksdb_column_family_options {\"write_buffer_size\":\"67108864\", \"max_write_buffer_number\":\"4\", \"max_bytes_for_level_base\":\"268435456\"} Specifies the RocksDB column family options. rocksdb_block_based_table_options {\"block_size\":\"8192\"} Specifies the RocksDB block based table options. Available rocksdb_db_options and rocksdb_column_family_options are listed as follows. rocksdb_db_options max_total_wal_size delete_obsolete_files_period_micros max_background_jobs stats_dump_period_sec compaction_readahead_size writable_file_max_buffer_size bytes_per_sync wal_bytes_per_sync delayed_write_rate avoid_flush_during_shutdown max_open_files stats_persist_period_sec stats_history_buffer_size strict_bytes_per_sync enable_rocksdb_prefix_filtering enable_rocksdb_whole_key_filtering rocksdb_filtering_prefix_length num_compaction_threads rate_limit rocksdb_column_family_options write_buffer_size max_write_buffer_number level0_file_num_compaction_trigger level0_slowdown_writes_trigger level0_stop_writes_trigger target_file_size_base target_file_size_multiplier max_bytes_for_level_base max_bytes_for_level_multiplier disable_auto_compactions For more information about RocksDB configuration, see RocksDB official documentation \u3002 For super-Large vertices \u00b6 For super vertex with a large number of edges, currently there are two truncation strategies: Truncate directly. Set the enable_reservoir_sampling parameter to false . A certain number of edges specified in the Max_edge_returned_per_vertex parameter are truncated by default. Truncate with the reservoir sampling algorithm. Based on the algorithm, a certain number of edges specified in the Max_edge_returned_per_vertex parameter are truncated with equal probability from the total n edges. Equal probability sampling is useful in some business scenarios. However, the performance is affected compared to direct truncation due to the probability calculation. Storage configuration for large dataset \u00b6 When you have a large dataset (in the RocksDB directory) and your memory is tight, we suggest that you set the enable_partitioned_index_filter parameter to true . For example, 100 vertices + 100 edges require 300 key-values. Each key takes 10bit in memory. Then you can calculate your own memory usage.","title":"Storage Service configurations"},{"location":"5.configurations-and-logs/1.configurations/4.storage-config/#storage_service_configurations","text":"Nebula Graph provides two initial configuration files for the Storage Service: nebula-storaged.conf.default and nebula-storaged.conf.production . You can use them in different scenarios. The default file path is /usr/local/nebula/etc/ . NOTE: Raft Listener is different from the Storage Service. For more information, see Raft Listener .","title":"Storage Service configurations"},{"location":"5.configurations-and-logs/1.configurations/4.storage-config/#how_to_use_the_configuration_files","text":"The Storage Service gets its configuration from the nebula-storaged.conf file. You have to remove the suffix .default or .production from an initial configuration file for the Storage Service to apply the configuration defined in it. If you have modified the configuration in the file and want the new configuration to take effect, add --local_conf=true at the top of the file. Otherwise, Nebula Graph reads the cached configuration.","title":"How to use the configuration files"},{"location":"5.configurations-and-logs/1.configurations/4.storage-config/#about_parameter_values","text":"If a parameter is not set in the configuration file, Nebula Graph uses its default value. NOTE: The default value of a parameter in Nebula Graph may be different from the predefined value in the .default and .production files. The predefined parameter in nebula-storaged.conf.default and nebula-storaged.conf.production are different. And not all parameters are predefined. This topic uses the parameters in nebula-storaged.conf.default .","title":"About parameter values"},{"location":"5.configurations-and-logs/1.configurations/4.storage-config/#basic_configurations","text":"Name Predefine Value Descriptions daemonize true When set to true , the process is a daemon process. pid_file pids/nebula-storaged.pid File to host the process ID.","title":"Basic configurations"},{"location":"5.configurations-and-logs/1.configurations/4.storage-config/#logging_configurations","text":"Name Predefine Value Descriptions log_dir logs Directory to the Storage Service log. We recommend that you put logs on a different hard disk from the data_path . minloglevel 0 Specifies the minimum log level. Available values are 0-3. 0 , 1 , 2 , and 3 are INFO , WARNING , ERROR , and FATAL . We suggest that you set minloglevel to 0 for debug, 1 for production. When you set it to 4 , Nebula Graph does not print any logs. v 0 Specifies the verbose log level. Available values are 0-4. The larger the value, the more verbose the log. logbufsecs 0 Specifies the maximum time to buffer the logs. The configuration is measured in seconds. redirect_stdout true When set to true , stdout and stderr are redirected. stdout_log_file storaged-stdout.log Specifies the filename for the stdout log. stderr_log_file storaged-stderr.log Specifies the filename for the stderr log. stderrthreshold 2 Specifies the minimum level to copy the log messages to stderr. Available values are 0-3. 0 , 1 , 2 , and 3 are INFO , WARNING , ERROR , and FATAL .","title":"Logging configurations"},{"location":"5.configurations-and-logs/1.configurations/4.storage-config/#networking_configurations","text":"Name Predefine Value Descriptions meta_server_addrs 127.0.0.1:9559 Specifies the IP addresses for the Meta Services. Separate multiple addresses with commas. local_ip 127.0.0.1 Specifies the local IP for the Storage Service. port 9779 Specifies RPC daemon listening port. The external port for Storage Service is 9779 . The internal port is port+1 , namely 9780 . Nebula Graph uses the internal port for multi-replica interactions. ws_ip 0.0.0.0 Specifies the IP address for the HTTP service. ws_http_port 19779 Specifies the port for the HTTP service. ws_h2_port 19780 Specifies the port for the HTTP2 service. heartbeat_interval_secs 10 Specifies the default heartbeat interval in seconds. Make sure the heartbeat_interval_secs values for all services are the same, otherwise Nebula Graph CANNOT work normally. NOTE : We recommend that you use the real IP address in your configuration because sometimes 127.0.0.1 can not be parsed correctly.","title":"Networking configurations"},{"location":"5.configurations-and-logs/1.configurations/4.storage-config/#raft_configurations","text":"Name Predefine Value Descriptions raft_heartbeat_interval_secs 30 Specifies the timeout for the Raft election. The configuration is measured in seconds. raft_rpc_timeout_ms 500 Specifies the timeout for the Raft RPC. The configuration is measured in milliseconds. wal_ttl 14400 Specifies the recycle RAFT wal time. The configuration is measured in seconds.","title":"Raft configurations"},{"location":"5.configurations-and-logs/1.configurations/4.storage-config/#disk_configurations","text":"Name Predefine Value Descriptions data_path data/storage Specifies the root data path. Separate multiple paths with commas. rocksdb_batch_size 4096 Specifies the block cache for a batch operation. The configuration is measured in bytes. rocksdb_block_cache 4 Specifies the block cache for BlockBasedTable. The configuration is measured in megabytes. engine_type rocksdb Specifies the engine type. rocksdb_compression lz4 Specifies the compression algorithm for RocksDB. Available values are no , snappy , lz4 , lz4hc , zlib , bzip2 , and zstd . rocksdb_compression_per_level \\ Specifies compression for each level. enable_rocksdb_statistics false When set to false , RocksDB statistics is disabled. rocksdb_stats_level kExceptHistogramOrTimers Specifies the stats level for RocksDB. Available values are kExceptHistogramOrTimers , kExceptTimers , kExceptDetailedTimers , kExceptTimeForMutex , and kAll . enable_rocksdb_prefix_filtering false When set to true , the prefix bloom filter for RocksDB is enabled. Enabling prefix bloom filter reduces memory usage. rocksdb_filtering_prefix_length 12 Specifies the prefix length for each key. Available values are 12 and 16 .","title":"Disk configurations"},{"location":"5.configurations-and-logs/1.configurations/4.storage-config/#rocksdb_options","text":"The format of the RocksDB options is {\"<option_name>\":\"<option_value>\"} . Multiple options are separated with commas. Name Predefine Value Descriptions rocksdb_db_options {} Specifies the RocksDB options. rocksdb_column_family_options {\"write_buffer_size\":\"67108864\", \"max_write_buffer_number\":\"4\", \"max_bytes_for_level_base\":\"268435456\"} Specifies the RocksDB column family options. rocksdb_block_based_table_options {\"block_size\":\"8192\"} Specifies the RocksDB block based table options. Available rocksdb_db_options and rocksdb_column_family_options are listed as follows. rocksdb_db_options max_total_wal_size delete_obsolete_files_period_micros max_background_jobs stats_dump_period_sec compaction_readahead_size writable_file_max_buffer_size bytes_per_sync wal_bytes_per_sync delayed_write_rate avoid_flush_during_shutdown max_open_files stats_persist_period_sec stats_history_buffer_size strict_bytes_per_sync enable_rocksdb_prefix_filtering enable_rocksdb_whole_key_filtering rocksdb_filtering_prefix_length num_compaction_threads rate_limit rocksdb_column_family_options write_buffer_size max_write_buffer_number level0_file_num_compaction_trigger level0_slowdown_writes_trigger level0_stop_writes_trigger target_file_size_base target_file_size_multiplier max_bytes_for_level_base max_bytes_for_level_multiplier disable_auto_compactions For more information about RocksDB configuration, see RocksDB official documentation \u3002","title":"RocksDB options"},{"location":"5.configurations-and-logs/1.configurations/4.storage-config/#for_super-large_vertices","text":"For super vertex with a large number of edges, currently there are two truncation strategies: Truncate directly. Set the enable_reservoir_sampling parameter to false . A certain number of edges specified in the Max_edge_returned_per_vertex parameter are truncated by default. Truncate with the reservoir sampling algorithm. Based on the algorithm, a certain number of edges specified in the Max_edge_returned_per_vertex parameter are truncated with equal probability from the total n edges. Equal probability sampling is useful in some business scenarios. However, the performance is affected compared to direct truncation due to the probability calculation.","title":"For super-Large vertices"},{"location":"5.configurations-and-logs/1.configurations/4.storage-config/#storage_configuration_for_large_dataset","text":"When you have a large dataset (in the RocksDB directory) and your memory is tight, we suggest that you set the enable_partitioned_index_filter parameter to true . For example, 100 vertices + 100 edges require 300 key-values. Each key takes 10bit in memory. Then you can calculate your own memory usage.","title":"Storage configuration for large dataset"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/","text":"Kernel configurations \u00b6 This document gives some introductions to the Kernel configurations in Nebula Graph. ulimit \u00b6 ulimit -c \u00b6 ulimit -c limits the size of the core dumps. We recommend that you set it to unlimited . The command is: ulimit -c unlimited ulimit -n \u00b6 ulimit -n limits the number of open files. We recommend that you set it to more than 100,000. For example: ulimit -n 130000 Memory \u00b6 vm.swappiness \u00b6 vm.swappiness is the percentage of the free memory before starting swap. The greater the value, the more likely the swap occurs. We recommend that you set it to 0. When set to 0, the page cache is removed first. Note that when vm.swappiness is 0, it does not mean that there is no swap. vm.min_free_kbytes \u00b6 vm.min_free_kbytes is used to force the Linux VM to keep a minimum number of kilobytes free. If you have a large system memory, we recommend that you increase this value. For example, if your physical memory 128GB, set it to 5GB. If the value is not big enough, the system cannot apply for enough continuous physical memory. vm.max_map_count \u00b6 vm.max_map_count limits the maximum number of vma (virtual memory area) for a process. The default value is 65530 . It is enough for most applications. If your memory application fails because the memory consumption is large, increase the vm.max_map_count value. vm.overcommit_memory \u00b6 vm.overcommit_memory contains a flag that enables memory overcommitment. We recommend that you set the default value 0 or 1. DO NOT set it to 2. vm.dirty_* \u00b6 These values control the aggressiveness of the dirty page cache for the system. For write-intensive scenarios, you can make adjustments based on your needs (throughput priority or delay priority). We recommend that you use the system default value. Transparent huge page \u00b6 For better delay performance, you must delete the transparent huge pages (THP). The options are /sys/kernel/mm/transparent_hugepage/enabled and /sys/kernel/mm/transparent_hugepage/defrag . For example: echo never > /sys/kernel/mm/transparent_hugepage/enabled echo never > /sys/kernel/mm/transparent_hugepage/defrag swapoff -a && swapon -a Networking \u00b6 net.ipv4.tcp_slow_start_after_idle \u00b6 The default value for this parameter is 1 . If set, the congestion window is timed out after an idle period. We recommend that you set it to 0, especially for long fat links (high latency and large bandwidth). net.core.somaxconn \u00b6 net.core.somaxconn is the maximum number of the backlogged sockets. The default value is 128 . For scenarios with a large number of burst connections, we recommend that you set it to greater than 1024 . net.ipv4.tcp_max_syn_backlog \u00b6 The maximum number of remembered connection requests. The setting rule for this parameter is the same as that of net.core.somaxconn . net.core.netdev_max_backlog \u00b6 It determines the maximum number of packets. We recommend that you increase it to greater than 10,000, especially for 10G network adapters. The default value is 1000 . net.ipv4.tcp_keepalive_* \u00b6 Keep alive parameters for the TCP connections. For applications that use a 4-layer transparent load balancer, if the idle connection is disconnected unexpectedly, decrease tcp_keepalive_time and tcp_keepalive_intvl . net.ipv4.tcp_rmem/wmem \u00b6 The minimum, default, and maximum size of the TCP socket receive buffer. For long fat links, we recommend that you increase the default value to bandwidth * RTT. scheduler \u00b6 For SSD devices, we recommend that you set /sys/block/DEV_NAME/queue/scheduler to noop or none. Other parameters \u00b6 kernel.core_pattern \u00b6 we recommend that you set it to core and set kernel.core_uses_pid to 1. Parameter usage guide \u00b6 sysctl \u00b6 sysctl conf_name checks the current parameter value. sysctl -w conf_name=value modifies the parameter value. And your modification takes effect immediately. sysctl -p loads parameter values \u200b\u200bfrom related configuration files. Introduction to ulimit \u00b6 ulimit sets the resource threshold for the current shell session. Please note that: Changes made by the ulimit command are valid only for the current session (and child processes). ulimit cannot adjust the (soft) threshold of a resource to a value greater than the current hard value. Ordinary users cannot adjust the hard threshold (even by using sudo ) through this command. To modify on the system level, or adjust the hard threshold, edit the /etc/security/limits.conf file. But this method needs to re-log in to take effect. prlimit \u00b6 prlimit gets and sets process resource limits. You can modify the hard threshold by using it and the sudo command. Together with the sudo command, the hard threshold can be modified. For example, prlimit --nofile = 130000 --pid = $$ adjusts the maximum number of open files permitted by the current process to 14000 . And the modification takes effect immediately. Note that this command is only available in RedHat 7u or later OS versions.","title":"Kernel configurations"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#kernel_configurations","text":"This document gives some introductions to the Kernel configurations in Nebula Graph.","title":"Kernel configurations"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#ulimit","text":"","title":"ulimit"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#ulimit_-c","text":"ulimit -c limits the size of the core dumps. We recommend that you set it to unlimited . The command is: ulimit -c unlimited","title":"ulimit -c"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#ulimit_-n","text":"ulimit -n limits the number of open files. We recommend that you set it to more than 100,000. For example: ulimit -n 130000","title":"ulimit -n"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#memory","text":"","title":"Memory"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#vmswappiness","text":"vm.swappiness is the percentage of the free memory before starting swap. The greater the value, the more likely the swap occurs. We recommend that you set it to 0. When set to 0, the page cache is removed first. Note that when vm.swappiness is 0, it does not mean that there is no swap.","title":"vm.swappiness"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#vmmin_free_kbytes","text":"vm.min_free_kbytes is used to force the Linux VM to keep a minimum number of kilobytes free. If you have a large system memory, we recommend that you increase this value. For example, if your physical memory 128GB, set it to 5GB. If the value is not big enough, the system cannot apply for enough continuous physical memory.","title":"vm.min_free_kbytes"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#vmmax_map_count","text":"vm.max_map_count limits the maximum number of vma (virtual memory area) for a process. The default value is 65530 . It is enough for most applications. If your memory application fails because the memory consumption is large, increase the vm.max_map_count value.","title":"vm.max_map_count"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#vmovercommit_memory","text":"vm.overcommit_memory contains a flag that enables memory overcommitment. We recommend that you set the default value 0 or 1. DO NOT set it to 2.","title":"vm.overcommit_memory"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#vmdirty_","text":"These values control the aggressiveness of the dirty page cache for the system. For write-intensive scenarios, you can make adjustments based on your needs (throughput priority or delay priority). We recommend that you use the system default value.","title":"vm.dirty_*"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#transparent_huge_page","text":"For better delay performance, you must delete the transparent huge pages (THP). The options are /sys/kernel/mm/transparent_hugepage/enabled and /sys/kernel/mm/transparent_hugepage/defrag . For example: echo never > /sys/kernel/mm/transparent_hugepage/enabled echo never > /sys/kernel/mm/transparent_hugepage/defrag swapoff -a && swapon -a","title":"Transparent huge page"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#networking","text":"","title":"Networking"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#netipv4tcp_slow_start_after_idle","text":"The default value for this parameter is 1 . If set, the congestion window is timed out after an idle period. We recommend that you set it to 0, especially for long fat links (high latency and large bandwidth).","title":"net.ipv4.tcp_slow_start_after_idle"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#netcoresomaxconn","text":"net.core.somaxconn is the maximum number of the backlogged sockets. The default value is 128 . For scenarios with a large number of burst connections, we recommend that you set it to greater than 1024 .","title":"net.core.somaxconn"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#netipv4tcp_max_syn_backlog","text":"The maximum number of remembered connection requests. The setting rule for this parameter is the same as that of net.core.somaxconn .","title":"net.ipv4.tcp_max_syn_backlog"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#netcorenetdev_max_backlog","text":"It determines the maximum number of packets. We recommend that you increase it to greater than 10,000, especially for 10G network adapters. The default value is 1000 .","title":"net.core.netdev_max_backlog"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#netipv4tcp_keepalive_","text":"Keep alive parameters for the TCP connections. For applications that use a 4-layer transparent load balancer, if the idle connection is disconnected unexpectedly, decrease tcp_keepalive_time and tcp_keepalive_intvl .","title":"net.ipv4.tcp_keepalive_*"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#netipv4tcp_rmemwmem","text":"The minimum, default, and maximum size of the TCP socket receive buffer. For long fat links, we recommend that you increase the default value to bandwidth * RTT.","title":"net.ipv4.tcp_rmem/wmem"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#scheduler","text":"For SSD devices, we recommend that you set /sys/block/DEV_NAME/queue/scheduler to noop or none.","title":"scheduler"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#other_parameters","text":"","title":"Other parameters"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#kernelcore_pattern","text":"we recommend that you set it to core and set kernel.core_uses_pid to 1.","title":"kernel.core_pattern"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#parameter_usage_guide","text":"","title":"Parameter usage guide"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#sysctl","text":"sysctl conf_name checks the current parameter value. sysctl -w conf_name=value modifies the parameter value. And your modification takes effect immediately. sysctl -p loads parameter values \u200b\u200bfrom related configuration files.","title":"sysctl"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#introduction_to_ulimit","text":"ulimit sets the resource threshold for the current shell session. Please note that: Changes made by the ulimit command are valid only for the current session (and child processes). ulimit cannot adjust the (soft) threshold of a resource to a value greater than the current hard value. Ordinary users cannot adjust the hard threshold (even by using sudo ) through this command. To modify on the system level, or adjust the hard threshold, edit the /etc/security/limits.conf file. But this method needs to re-log in to take effect.","title":"Introduction to ulimit"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#prlimit","text":"prlimit gets and sets process resource limits. You can modify the hard threshold by using it and the sudo command. Together with the sudo command, the hard threshold can be modified. For example, prlimit --nofile = 130000 --pid = $$ adjusts the maximum number of open files permitted by the current process to 14000 . And the modification takes effect immediately. Note that this command is only available in RedHat 7u or later OS versions.","title":"prlimit"},{"location":"5.configurations-and-logs/2.log-management/logs/","text":"Logs \u00b6 Nebula Graph uses glog to print logs, uses gflag to control the severity level of the log, and provides an HTTP interface to dynamically change the log level at runtime to facilitate tracking. Log Directory \u00b6 The default log directory is /usr/local/nebula/logs/ . NOTE : If you deleted the log directory during runtime, the runtime log would not continue to be printed. However, this operation will not affect the services. Restart the services to recover the logs. Parameter Description \u00b6 Two most commonly used flags in glog \u00b6 minloglevel: The scale of minloglevel is 0-4. The numbers of severity levels INFO(DEBUG), WARNING, ERROR, and FATAL are 0, 1, 2, and 3, respectively. Usually specified as 0 for debug, 1 for production. If you set the minloglevel to 4, no logs are printed. v: The scale of v is 0-3. When the value is set to 0, you can further set the severity level of the debug log. The greater the value is, the more detailed the log is. Configuration Files \u00b6 The default severity level for the metad, graphd, and storaged logs can be found in the configuration files (usually in /usr/local/nebula/etc/ ). Check and Change the Severity Levels Dynamically \u00b6 Check all the flag values (log values included) of the current gflags with the following command. Not all flags are listed because changing some flags can be dangerous. Read the response explanation and the source code before you change these not documented parameters. To get all the available flags for a process, use this command: > curl ${ ws_ip } : ${ ws_port } /flags In the command: ws_ip is the IP address for the HTTP service, which can be found in the configuration files above. The default value is 127.0.0.1 . ws_port is the port for the HTTP service, the default values for metad , storaged , and graphd are 19559 , 19779 , and 19669 , respectively. NOTE: If you changed the runtime log level, then restart the services, the log level changes to the configuration file specifications. For more information, see Storage Service configurations . For example, check the minloglevel for the storaged service: > curl 127 .0.0.1:19559/flags | grep minloglevel To change the log level for a process, use these commands. For example, you can change the log severity level the the most detailed . $ curl -X PUT -H \"Content-Type: application/json\" -d '{\"minloglevel\":0,\"v\":3}' \"127.0.0.1:19779/flags\" # storaged $ curl -X PUT -H \"Content-Type: application/json\" -d '{\"minloglevel\":0,\"v\":3}' \"127.0.0.1:19669/flags\" # graphd $ curl -X PUT -H \"Content-Type: application/json\" -d '{\"minloglevel\":0,\"v\":3}' \"127.0.0.1:19559/flags\" # metad To change the severity of the storage log, replace the port in the preceding command with storage port. NOTE : Nebula Graph only supports modifying the graph and storage log severity by using the console. And the severity level of meta logs can only be modified with the curl command. Close all logs print (FATAL only) with the following command. $ curl -X PUT -H \"Content-Type: application/json\" -d '{\"minloglevel\":3,\"v\":0}' \"127.0.0.1:19779/flags\" # storaged $ curl -X PUT -H \"Content-Type: application/json\" -d '{\"minloglevel\":3,\"v\":0}' \"127.0.0.1:19669/flags\" # graphd $ curl -X PUT -H \"Content-Type: application/json\" -d '{\"minloglevel\":3,\"v\":0}' \"127.0.0.1:19559/flags\" # metad","title":"Logs"},{"location":"5.configurations-and-logs/2.log-management/logs/#logs","text":"Nebula Graph uses glog to print logs, uses gflag to control the severity level of the log, and provides an HTTP interface to dynamically change the log level at runtime to facilitate tracking.","title":"Logs"},{"location":"5.configurations-and-logs/2.log-management/logs/#log_directory","text":"The default log directory is /usr/local/nebula/logs/ . NOTE : If you deleted the log directory during runtime, the runtime log would not continue to be printed. However, this operation will not affect the services. Restart the services to recover the logs.","title":"Log Directory"},{"location":"5.configurations-and-logs/2.log-management/logs/#parameter_description","text":"","title":"Parameter Description"},{"location":"5.configurations-and-logs/2.log-management/logs/#two_most_commonly_used_flags_in_glog","text":"minloglevel: The scale of minloglevel is 0-4. The numbers of severity levels INFO(DEBUG), WARNING, ERROR, and FATAL are 0, 1, 2, and 3, respectively. Usually specified as 0 for debug, 1 for production. If you set the minloglevel to 4, no logs are printed. v: The scale of v is 0-3. When the value is set to 0, you can further set the severity level of the debug log. The greater the value is, the more detailed the log is.","title":"Two most commonly used flags in glog"},{"location":"5.configurations-and-logs/2.log-management/logs/#configuration_files","text":"The default severity level for the metad, graphd, and storaged logs can be found in the configuration files (usually in /usr/local/nebula/etc/ ).","title":"Configuration Files"},{"location":"5.configurations-and-logs/2.log-management/logs/#check_and_change_the_severity_levels_dynamically","text":"Check all the flag values (log values included) of the current gflags with the following command. Not all flags are listed because changing some flags can be dangerous. Read the response explanation and the source code before you change these not documented parameters. To get all the available flags for a process, use this command: > curl ${ ws_ip } : ${ ws_port } /flags In the command: ws_ip is the IP address for the HTTP service, which can be found in the configuration files above. The default value is 127.0.0.1 . ws_port is the port for the HTTP service, the default values for metad , storaged , and graphd are 19559 , 19779 , and 19669 , respectively. NOTE: If you changed the runtime log level, then restart the services, the log level changes to the configuration file specifications. For more information, see Storage Service configurations . For example, check the minloglevel for the storaged service: > curl 127 .0.0.1:19559/flags | grep minloglevel To change the log level for a process, use these commands. For example, you can change the log severity level the the most detailed . $ curl -X PUT -H \"Content-Type: application/json\" -d '{\"minloglevel\":0,\"v\":3}' \"127.0.0.1:19779/flags\" # storaged $ curl -X PUT -H \"Content-Type: application/json\" -d '{\"minloglevel\":0,\"v\":3}' \"127.0.0.1:19669/flags\" # graphd $ curl -X PUT -H \"Content-Type: application/json\" -d '{\"minloglevel\":0,\"v\":3}' \"127.0.0.1:19559/flags\" # metad To change the severity of the storage log, replace the port in the preceding command with storage port. NOTE : Nebula Graph only supports modifying the graph and storage log severity by using the console. And the severity level of meta logs can only be modified with the curl command. Close all logs print (FATAL only) with the following command. $ curl -X PUT -H \"Content-Type: application/json\" -d '{\"minloglevel\":3,\"v\":0}' \"127.0.0.1:19779/flags\" # storaged $ curl -X PUT -H \"Content-Type: application/json\" -d '{\"minloglevel\":3,\"v\":0}' \"127.0.0.1:19669/flags\" # graphd $ curl -X PUT -H \"Content-Type: application/json\" -d '{\"minloglevel\":3,\"v\":0}' \"127.0.0.1:19559/flags\" # metad","title":"Check and Change the Severity Levels Dynamically"},{"location":"6.monitor-and-metrics/1.query-performance-metrics/","text":"Query Nebula Graph metrics \u00b6 Nebula Graph supports querying the monitoring metrics through HTTP ports. Metrics \u00b6 Each metric of Nebula Graph consists of three fields: name, type, and time range. The fields are separated by periods, for example, num_queries.sum.600 . The detailed description is as follows. Field Example Description Metric name num_queries Indicates the function of the metric. Metric type sum Indicates how the metrics are collected. Supported types are SUM, COUNT, AVG, RATE, and the P-th sample quantiles such as P75, P95, P99, and P99.9. Time range 600 The time range in seconds for the metric collection. Supported values are 5, 60, 600, and 3600, representing the last 5 seconds, 1 minute, 10 minutes, and 1 hour. Different Nebula Graph services (Graph, Storage, or Meta) support different metrics, for more information, see Metric list (TODO: doc). Query metrics over HTTP \u00b6 Syntax \u00b6 $ curl -G \"http://<ip>:<port>/stats?stats=<metric_name_list>[&format=json]\" Parameter Description ip The IP address of the server. You can find it in the configuration file in the installation directory. port The HTTP port of the server. You can find it in the configuration file in the installation directory. The default ports are 19559 (Meta), 19669 (Graph), and 19779 (Storage). metric_name_list The metrics names. Multiple metrics are separated by commas (,). &format=json Optional. Returns the result in the JSON format. NOTE: If Nebula Graph is deployed with Docker Compose , run docker-compose ps to check the ports that are mapped from the service ports inside of the container and then query through them. Example \u00b6 Query a single metric Query the query number in the last 10 minutes in the Graph Service. $ curl -G \"http://192.168.8.40:19669/stats?stats=num_queries.sum.600\" num_queries.sum.600 = 400 Query multiple metrics Query the following metrics together: * The average heartbeat latency in the last 1 minute. * The average latency of the slowest 1% heartbeats, i.e., the P99 heartbeats, in the last 10 minutes. $ curl -G \"http://192.168.8.40:19559/stats?stats=heartbeat_latency_us.avg.60,heartbeat_latency_us.p99.600\" heartbeat_latency_us.avg.60 = 281 heartbeat_latency_us.p99.600 = 985 Return a JSON result. Query the number of new vertices in the Storage Service in the last 10 minutes and return the result in the JSON format. $ curl -G \"http://192.168.8.40:19779/stats?stats=num_add_vertices.sum.600&format=json\" [{ \"value\" :1, \"name\" : \"num_add_vertices.sum.600\" }] Query all metrics in a service. If no metric is specified in the query, Nebula Graph returns all metrics in the service. $ curl -G \"http://192.168.8.40:19559/stats\" heartbeat_latency_us.avg.5 = 304 heartbeat_latency_us.avg.60 = 308 heartbeat_latency_us.avg.600 = 299 heartbeat_latency_us.avg.3600 = 285 heartbeat_latency_us.p75.5 = 652 heartbeat_latency_us.p75.60 = 669 heartbeat_latency_us.p75.600 = 651 heartbeat_latency_us.p75.3600 = 642 heartbeat_latency_us.p95.5 = 930 heartbeat_latency_us.p95.60 = 963 heartbeat_latency_us.p95.600 = 933 heartbeat_latency_us.p95.3600 = 929 heartbeat_latency_us.p99.5 = 986 heartbeat_latency_us.p99.60 = 1409 heartbeat_latency_us.p99.600 = 989 heartbeat_latency_us.p99.3600 = 986 num_heartbeats.rate.5 = 0 num_heartbeats.rate.60 = 0 num_heartbeats.rate.600 = 0 num_heartbeats.rate.3600 = 0 num_heartbeats.sum.5 = 2 num_heartbeats.sum.60 = 40 num_heartbeats.sum.600 = 394 num_heartbeats.sum.3600 = 2364","title":"Query Nebula Graph metrics"},{"location":"6.monitor-and-metrics/1.query-performance-metrics/#query_nebula_graph_metrics","text":"Nebula Graph supports querying the monitoring metrics through HTTP ports.","title":"Query Nebula Graph metrics"},{"location":"6.monitor-and-metrics/1.query-performance-metrics/#metrics","text":"Each metric of Nebula Graph consists of three fields: name, type, and time range. The fields are separated by periods, for example, num_queries.sum.600 . The detailed description is as follows. Field Example Description Metric name num_queries Indicates the function of the metric. Metric type sum Indicates how the metrics are collected. Supported types are SUM, COUNT, AVG, RATE, and the P-th sample quantiles such as P75, P95, P99, and P99.9. Time range 600 The time range in seconds for the metric collection. Supported values are 5, 60, 600, and 3600, representing the last 5 seconds, 1 minute, 10 minutes, and 1 hour. Different Nebula Graph services (Graph, Storage, or Meta) support different metrics, for more information, see Metric list (TODO: doc).","title":"Metrics"},{"location":"6.monitor-and-metrics/1.query-performance-metrics/#query_metrics_over_http","text":"","title":"Query metrics over HTTP"},{"location":"6.monitor-and-metrics/1.query-performance-metrics/#syntax","text":"$ curl -G \"http://<ip>:<port>/stats?stats=<metric_name_list>[&format=json]\" Parameter Description ip The IP address of the server. You can find it in the configuration file in the installation directory. port The HTTP port of the server. You can find it in the configuration file in the installation directory. The default ports are 19559 (Meta), 19669 (Graph), and 19779 (Storage). metric_name_list The metrics names. Multiple metrics are separated by commas (,). &format=json Optional. Returns the result in the JSON format. NOTE: If Nebula Graph is deployed with Docker Compose , run docker-compose ps to check the ports that are mapped from the service ports inside of the container and then query through them.","title":"Syntax"},{"location":"6.monitor-and-metrics/1.query-performance-metrics/#example","text":"Query a single metric Query the query number in the last 10 minutes in the Graph Service. $ curl -G \"http://192.168.8.40:19669/stats?stats=num_queries.sum.600\" num_queries.sum.600 = 400 Query multiple metrics Query the following metrics together: * The average heartbeat latency in the last 1 minute. * The average latency of the slowest 1% heartbeats, i.e., the P99 heartbeats, in the last 10 minutes. $ curl -G \"http://192.168.8.40:19559/stats?stats=heartbeat_latency_us.avg.60,heartbeat_latency_us.p99.600\" heartbeat_latency_us.avg.60 = 281 heartbeat_latency_us.p99.600 = 985 Return a JSON result. Query the number of new vertices in the Storage Service in the last 10 minutes and return the result in the JSON format. $ curl -G \"http://192.168.8.40:19779/stats?stats=num_add_vertices.sum.600&format=json\" [{ \"value\" :1, \"name\" : \"num_add_vertices.sum.600\" }] Query all metrics in a service. If no metric is specified in the query, Nebula Graph returns all metrics in the service. $ curl -G \"http://192.168.8.40:19559/stats\" heartbeat_latency_us.avg.5 = 304 heartbeat_latency_us.avg.60 = 308 heartbeat_latency_us.avg.600 = 299 heartbeat_latency_us.avg.3600 = 285 heartbeat_latency_us.p75.5 = 652 heartbeat_latency_us.p75.60 = 669 heartbeat_latency_us.p75.600 = 651 heartbeat_latency_us.p75.3600 = 642 heartbeat_latency_us.p95.5 = 930 heartbeat_latency_us.p95.60 = 963 heartbeat_latency_us.p95.600 = 933 heartbeat_latency_us.p95.3600 = 929 heartbeat_latency_us.p99.5 = 986 heartbeat_latency_us.p99.60 = 1409 heartbeat_latency_us.p99.600 = 989 heartbeat_latency_us.p99.3600 = 986 num_heartbeats.rate.5 = 0 num_heartbeats.rate.60 = 0 num_heartbeats.rate.600 = 0 num_heartbeats.rate.3600 = 0 num_heartbeats.sum.5 = 2 num_heartbeats.sum.60 = 40 num_heartbeats.sum.600 = 394 num_heartbeats.sum.3600 = 2364","title":"Example"},{"location":"7.data-security/3.manage-snapshot/","text":"Backup and restore data with snapshots \u00b6 Nebula Graph supports using snapshots to backup and restore data. Authentication and snapshots \u00b6 Nebula Graph authentication is disabled by default. In this case, All users can use the snapshot feature. If authentication is enabled, only the GOD-role user can use the snapshot function. For more information about roles, see Roles and privileges . Precautions \u00b6 To prevent data loss, create a snapshot as soon as the system structure changes, for example, after operations such as ADD HOST , DROP HOST , CREATE SPACE , DROP SPACE , and BALANCE are performed. Nebula Graph cannot automatically delete the invalid files created by a failed snapshot task, you have to manually delete them by using DROP SNAPSHOT . Customizing the storage path for the snapshots is not supported for now. Snapshot form and path \u00b6 Nebula Graph snapshots are in the form of directories with names like SNAPSHOT_2021_03_09_08_43_12 . The suffix 2021_03_09_08_43_12 is generated automatically based on the creation time. When a snapshot is created, snapshot directories will be automatically created in the checkpoints directory on the leader Meta server and each Storage server. To fast locate the path where the snapshots are stored, you can use the Linux command find . For example: $ find | grep 'SNAPSHOT_2021_03_11_07_30_36' ./data/meta2/nebula/0/checkpoints/SNAPSHOT_2021_03_11_07_30_36 ./data/meta2/nebula/0/checkpoints/SNAPSHOT_2021_03_11_07_30_36/data ./data/meta2/nebula/0/checkpoints/SNAPSHOT_2021_03_11_07_30_36/data/000081.sst ... NOTE: For how to get the snapshot name, see View snapshots . Create a snapshot \u00b6 Run CREATE SNAPSHOT to create a snapshot for all the graph spaces based on the current time for Nebula Graph. NOTE: Creating a snapshot for a specific graph space is not supported yet. If the creation fails, delete the snapshot and try again. If it still fails, go to the Nebula Graph community for help. View snapshots \u00b6 To view all existing snapshots, run SHOW SNAPSHOTS . nebula> SHOW SNAPSHOTS; +--------------------------------+---------+------------------+ | Name | Status | Hosts | +--------------------------------+---------+------------------+ | \"SNAPSHOT_2021_03_09_08_43_12\" | \"VALID\" | \"127.0.0.1:9779\" | +--------------------------------+---------+------------------+ | \"SNAPSHOT_2021_03_09_09_10_52\" | \"VALID\" | \"127.0.0.1:9779\" | +--------------------------------+---------+------------------+ The parameters in the return information are described as follows. Parameter Description Name Name of the snapshot directory. Status Status of the snapshot. VALID indicates that the creation succeeded and INVALID indicates that it failed. Hosts IP addresses and ports of all Storage servers at the time the snapshot was created. Delete a snapshot \u00b6 To delete a snapshot, use the following syntax: DROP SNAPSHOT <snapshot_name>; Example: nebula> DROP SNAPSHOT SNAPSHOT_2021_03_09_08_43_12; nebula> SHOW SNAPSHOTS; +--------------------------------+---------+------------------+ | Name | Status | Hosts | +--------------------------------+---------+------------------+ | \"SNAPSHOT_2021_03_09_09_10_52\" | \"VALID\" | \"127.0.0.1:9779\" | +--------------------------------+---------+------------------+ Restore data with a snapshot \u00b6 Find the snapshot directories you want to use for data restoration. Choose an approach to restore the data files: Change the data_path in the Meta configuration and Storage configuration to the snapshot path. Copy the snapshot directories to other locations, and change the data_path to these locations. Copy all the content in the snapshot directories into the directories where the checkpoints directories are located, and cover the existing files that have duplicate names with them. For example, cover /usr/local/nebula/data/meta/nebula/0/data with /usr/local/nebula/data/meta/nebula/0/checkpoints/SNAPSHOT_2021_03_09_09_10_52/data . Restart Nebula Graph . Another way to backup and restore data \u00b6 You can also use Backup&Restore to backup and restore Nebula Graph data. (TODO: coding)","title":"Manage snapshots"},{"location":"7.data-security/3.manage-snapshot/#backup_and_restore_data_with_snapshots","text":"Nebula Graph supports using snapshots to backup and restore data.","title":"Backup and restore data with snapshots"},{"location":"7.data-security/3.manage-snapshot/#authentication_and_snapshots","text":"Nebula Graph authentication is disabled by default. In this case, All users can use the snapshot feature. If authentication is enabled, only the GOD-role user can use the snapshot function. For more information about roles, see Roles and privileges .","title":"Authentication and snapshots"},{"location":"7.data-security/3.manage-snapshot/#precautions","text":"To prevent data loss, create a snapshot as soon as the system structure changes, for example, after operations such as ADD HOST , DROP HOST , CREATE SPACE , DROP SPACE , and BALANCE are performed. Nebula Graph cannot automatically delete the invalid files created by a failed snapshot task, you have to manually delete them by using DROP SNAPSHOT . Customizing the storage path for the snapshots is not supported for now.","title":"Precautions"},{"location":"7.data-security/3.manage-snapshot/#snapshot_form_and_path","text":"Nebula Graph snapshots are in the form of directories with names like SNAPSHOT_2021_03_09_08_43_12 . The suffix 2021_03_09_08_43_12 is generated automatically based on the creation time. When a snapshot is created, snapshot directories will be automatically created in the checkpoints directory on the leader Meta server and each Storage server. To fast locate the path where the snapshots are stored, you can use the Linux command find . For example: $ find | grep 'SNAPSHOT_2021_03_11_07_30_36' ./data/meta2/nebula/0/checkpoints/SNAPSHOT_2021_03_11_07_30_36 ./data/meta2/nebula/0/checkpoints/SNAPSHOT_2021_03_11_07_30_36/data ./data/meta2/nebula/0/checkpoints/SNAPSHOT_2021_03_11_07_30_36/data/000081.sst ... NOTE: For how to get the snapshot name, see View snapshots .","title":"Snapshot form and path"},{"location":"7.data-security/3.manage-snapshot/#create_a_snapshot","text":"Run CREATE SNAPSHOT to create a snapshot for all the graph spaces based on the current time for Nebula Graph. NOTE: Creating a snapshot for a specific graph space is not supported yet. If the creation fails, delete the snapshot and try again. If it still fails, go to the Nebula Graph community for help.","title":"Create a snapshot"},{"location":"7.data-security/3.manage-snapshot/#view_snapshots","text":"To view all existing snapshots, run SHOW SNAPSHOTS . nebula> SHOW SNAPSHOTS; +--------------------------------+---------+------------------+ | Name | Status | Hosts | +--------------------------------+---------+------------------+ | \"SNAPSHOT_2021_03_09_08_43_12\" | \"VALID\" | \"127.0.0.1:9779\" | +--------------------------------+---------+------------------+ | \"SNAPSHOT_2021_03_09_09_10_52\" | \"VALID\" | \"127.0.0.1:9779\" | +--------------------------------+---------+------------------+ The parameters in the return information are described as follows. Parameter Description Name Name of the snapshot directory. Status Status of the snapshot. VALID indicates that the creation succeeded and INVALID indicates that it failed. Hosts IP addresses and ports of all Storage servers at the time the snapshot was created.","title":"View snapshots"},{"location":"7.data-security/3.manage-snapshot/#delete_a_snapshot","text":"To delete a snapshot, use the following syntax: DROP SNAPSHOT <snapshot_name>; Example: nebula> DROP SNAPSHOT SNAPSHOT_2021_03_09_08_43_12; nebula> SHOW SNAPSHOTS; +--------------------------------+---------+------------------+ | Name | Status | Hosts | +--------------------------------+---------+------------------+ | \"SNAPSHOT_2021_03_09_09_10_52\" | \"VALID\" | \"127.0.0.1:9779\" | +--------------------------------+---------+------------------+","title":"Delete a snapshot"},{"location":"7.data-security/3.manage-snapshot/#restore_data_with_a_snapshot","text":"Find the snapshot directories you want to use for data restoration. Choose an approach to restore the data files: Change the data_path in the Meta configuration and Storage configuration to the snapshot path. Copy the snapshot directories to other locations, and change the data_path to these locations. Copy all the content in the snapshot directories into the directories where the checkpoints directories are located, and cover the existing files that have duplicate names with them. For example, cover /usr/local/nebula/data/meta/nebula/0/data with /usr/local/nebula/data/meta/nebula/0/checkpoints/SNAPSHOT_2021_03_09_09_10_52/data . Restart Nebula Graph .","title":"Restore data with a snapshot"},{"location":"7.data-security/3.manage-snapshot/#another_way_to_backup_and_restore_data","text":"You can also use Backup&Restore to backup and restore Nebula Graph data. (TODO: coding)","title":"Another way to backup and restore data"},{"location":"7.data-security/1.authentication/1.authentication/","text":"Authentication \u00b6 Nebula Graph replies on local authentication or LDAP authentication to implement access control. Nebula Graph creates a session when a client connects to it. The session stores information about the connection, including the user information. By default, authentication is disabled and Nebula Graph allows connections with any username and password. If the authentication system is enabled, Nebula Graph checks a session according to the authentication configuration, and decides whether the session should be allowed or denied. Local authentication \u00b6 Local authentication indicates that usernames and passwords are stored locally on the server, with the passwords encrypted. Enable local authentication \u00b6 In the /usr/local/nebula/etc/nebula-graphd.conf file, set --enable_authorize=true and save the modification. NOTE: /usr/local/nebula/ is the default installation path for Nebula Graph. If you have changed it, use the actual path. Restart the Nebula Graph services. For how to restart, see Manage Nebula Graph services . NOTE: You can use the username root and password nebula to log into Nebula Graph after enabling local authentication. This account has the build-in God role. For more information about roles, see Roles and privileges . LDAP authentication \u00b6 Lightweight Directory Access Protocol (LDAP), is a lightweight client-server protocol for accessing directories and building a centralized account management system. LDAP authentication and local authentication can be enabled at the same time, but LDAP authentication has a higher priority. If the local authentication server and the LDAP server both have the information of user Amber , Nebula Graph reads from the LDAP server first. Enable LDAP authentication \u00b6 The Nebula Graph Enterprise Edition supports LDAP authentication. For how to enable LDAP, see Authenticate with an LDAP server (TODO: doc).","title":"Authentication"},{"location":"7.data-security/1.authentication/1.authentication/#authentication","text":"Nebula Graph replies on local authentication or LDAP authentication to implement access control. Nebula Graph creates a session when a client connects to it. The session stores information about the connection, including the user information. By default, authentication is disabled and Nebula Graph allows connections with any username and password. If the authentication system is enabled, Nebula Graph checks a session according to the authentication configuration, and decides whether the session should be allowed or denied.","title":"Authentication"},{"location":"7.data-security/1.authentication/1.authentication/#local_authentication","text":"Local authentication indicates that usernames and passwords are stored locally on the server, with the passwords encrypted.","title":"Local authentication"},{"location":"7.data-security/1.authentication/1.authentication/#enable_local_authentication","text":"In the /usr/local/nebula/etc/nebula-graphd.conf file, set --enable_authorize=true and save the modification. NOTE: /usr/local/nebula/ is the default installation path for Nebula Graph. If you have changed it, use the actual path. Restart the Nebula Graph services. For how to restart, see Manage Nebula Graph services . NOTE: You can use the username root and password nebula to log into Nebula Graph after enabling local authentication. This account has the build-in God role. For more information about roles, see Roles and privileges .","title":"Enable local authentication"},{"location":"7.data-security/1.authentication/1.authentication/#ldap_authentication","text":"Lightweight Directory Access Protocol (LDAP), is a lightweight client-server protocol for accessing directories and building a centralized account management system. LDAP authentication and local authentication can be enabled at the same time, but LDAP authentication has a higher priority. If the local authentication server and the LDAP server both have the information of user Amber , Nebula Graph reads from the LDAP server first.","title":"LDAP authentication"},{"location":"7.data-security/1.authentication/1.authentication/#enable_ldap_authentication","text":"The Nebula Graph Enterprise Edition supports LDAP authentication. For how to enable LDAP, see Authenticate with an LDAP server (TODO: doc).","title":"Enable LDAP authentication"},{"location":"7.data-security/1.authentication/2.management-user/","text":"User management \u00b6 This topic describes how to manage users and roles. By default, Nebula Graph allows connections with any username and password. After enabling authentication , only valid users can connect to Nebula Graph and access the resources according to the user roles . CREATE USER \u00b6 The root user with the GOD role can run CREATE USER to create a new user. Syntax CREATE USER [IF NOT EXISTS] <user_name> [WITH PASSWORD '<password>']; Example nebula> CREATE USER user1 WITH PASSWORD 'nebula'; GRANT ROLE \u00b6 Users with the GOD role or the ADMIN role can run GRANT ROLE to assign a built-in role in a graph space to a user. For more information about Nebula Graph built-in roles, see Roles and privileges NOTE: If the target user is connected to Nebula Graph when running GRANT ROLE , the new role takes effect when the user logs out and logs in again. Syntax GRANT ROLE <role_type> ON <space_name> TO <user_name>; Example nebula> GRANT ROLE USER ON nba TO user1; REVOKE ROLE \u00b6 Users with the GOD role or the ADMIN role can run REVOKE ROLE to revoke a user's role in a graph space. NOTE: If the target user is connected to Nebula Graph when running REVOKE ROLE , the old role still takes effect until the user logs out. Syntax REVOKE ROLE <role_type> ON <space_name> FROM <user_name>; Example nebula> REVOKE ROLE USER ON nba FROM user1; CHANGE PASSWORD \u00b6 With the correct username and password, users can run CHANGE PASSWORD to set a new password for a user. Syntax CHANGE PASSWORD <user_name> FROM '<old_password>' TO '<new_password>'; Example nebula> CHANGE PASSWORD user1 FROM 'nebula' TO 'nebula123'; ALTER USER \u00b6 The root user with the GOD role can run ALTER USER to set a new password for a user. Syntax ALTER USER <user_name> WITH PASSWORD '<password>'; Example nebula> ALTER USER user1 WITH PASSWORD 'nebula'; DROP USER \u00b6 The root user with the GOD role can run DROP USER to remove a user. NOTE: Removing a user does not close the user's current session, and the user role still takes effect in the session until the session is closed. Syntax DROP USER [IF EXISTS] <user_name>; Example nebula> DROP USER user1; SHOW USERS \u00b6 The root user with the GOD role can run SHOW USERS to list all the users. Syntax SHOW USERS; Example nebula> SHOW USERS; +-----------+ | Account | +-----------+ | \"test1\" | +-----------+ | \"test2\" | +-----------+ | \"test3\" | +-----------+","title":"User management"},{"location":"7.data-security/1.authentication/2.management-user/#user_management","text":"This topic describes how to manage users and roles. By default, Nebula Graph allows connections with any username and password. After enabling authentication , only valid users can connect to Nebula Graph and access the resources according to the user roles .","title":"User management"},{"location":"7.data-security/1.authentication/2.management-user/#create_user","text":"The root user with the GOD role can run CREATE USER to create a new user. Syntax CREATE USER [IF NOT EXISTS] <user_name> [WITH PASSWORD '<password>']; Example nebula> CREATE USER user1 WITH PASSWORD 'nebula';","title":"CREATE USER"},{"location":"7.data-security/1.authentication/2.management-user/#grant_role","text":"Users with the GOD role or the ADMIN role can run GRANT ROLE to assign a built-in role in a graph space to a user. For more information about Nebula Graph built-in roles, see Roles and privileges NOTE: If the target user is connected to Nebula Graph when running GRANT ROLE , the new role takes effect when the user logs out and logs in again. Syntax GRANT ROLE <role_type> ON <space_name> TO <user_name>; Example nebula> GRANT ROLE USER ON nba TO user1;","title":"GRANT ROLE"},{"location":"7.data-security/1.authentication/2.management-user/#revoke_role","text":"Users with the GOD role or the ADMIN role can run REVOKE ROLE to revoke a user's role in a graph space. NOTE: If the target user is connected to Nebula Graph when running REVOKE ROLE , the old role still takes effect until the user logs out. Syntax REVOKE ROLE <role_type> ON <space_name> FROM <user_name>; Example nebula> REVOKE ROLE USER ON nba FROM user1;","title":"REVOKE ROLE"},{"location":"7.data-security/1.authentication/2.management-user/#change_password","text":"With the correct username and password, users can run CHANGE PASSWORD to set a new password for a user. Syntax CHANGE PASSWORD <user_name> FROM '<old_password>' TO '<new_password>'; Example nebula> CHANGE PASSWORD user1 FROM 'nebula' TO 'nebula123';","title":"CHANGE PASSWORD"},{"location":"7.data-security/1.authentication/2.management-user/#alter_user","text":"The root user with the GOD role can run ALTER USER to set a new password for a user. Syntax ALTER USER <user_name> WITH PASSWORD '<password>'; Example nebula> ALTER USER user1 WITH PASSWORD 'nebula';","title":"ALTER USER"},{"location":"7.data-security/1.authentication/2.management-user/#drop_user","text":"The root user with the GOD role can run DROP USER to remove a user. NOTE: Removing a user does not close the user's current session, and the user role still takes effect in the session until the session is closed. Syntax DROP USER [IF EXISTS] <user_name>; Example nebula> DROP USER user1;","title":"DROP USER"},{"location":"7.data-security/1.authentication/2.management-user/#show_users","text":"The root user with the GOD role can run SHOW USERS to list all the users. Syntax SHOW USERS; Example nebula> SHOW USERS; +-----------+ | Account | +-----------+ | \"test1\" | +-----------+ | \"test2\" | +-----------+ | \"test3\" | +-----------+","title":"SHOW USERS"},{"location":"7.data-security/1.authentication/3.role-list/","text":"Roles and privileges \u00b6 A role is a collection of privileges. You can assign a role to a user for access control. Built-in roles \u00b6 Nebula Graph does not support custom roles, but it has multiple built-in roles: GOD GOD is the original role with all privileges not limited to graph spaces. It is similar to root in Linux and administrator in Windows. When the Meta Service is initialized, the one and only GOD role user root is automatically created with the password nebula . CAUTION: Modify the password for root as soon as possible for security. The default username root is immutable. If authentication is disabled, you can use any username and password to connect to Nebula Graph. This user is regarded as the GOD role. ADMIN An ADMIN role can read and write both the Schema and the data in a specific graph space. An ADMIN role of a graph space can grant DBA, USER, and GUEST roles in the graph space to other users. DBA A DBA role can read and write both the Schema and the data in a specific graph space. A DBA role of a graph space CANNOT grant roles to other users. USER A USER role can read and write data in a specific graph space. The Schema information is read-only to the USER roles in a graph space. GUEST A GUEST role can only read the Schema and the data in a specific graph space. NOTE: A user can have only one role in a graph space. Role privileges and allowed nGQL \u00b6 The privileges of roles and the nGQL statements that each role can use are listed as follows. Privilege God Admin DBA User Guest Allowed nGQL Read space Y Y Y Y Y USE , DESCRIBE SPACE Write space Y CREATE SPACE , DROP SPACE , CREATE SNAPSHOT , DROP SNAPSHOT , BALANCE , ADMIN , CONFIG , INGEST , DOWNLOAD Read schema Y Y Y Y Y DESCRIBE TAG , DESCRIBE EDGE , DESCRIBE TAG INDEX , DESCRIBE EDGE INDEX Write schema Y Y Y CREATE TAG , ALTER TAG , CREATE EDGE , ALTER EDGE , DROP TAG , DROP EDGE , CREATE TAG INDEX , CREATE EDGE INDEX , DROP TAG INDEX , DROP EDGE INDEX Write user Y CREATE USER , DROP USER , ALTER USER Write role Y Y GRANT , REVOKE Read data Y Y Y Y Y GO , SET , PIPE , MATCH , ASSIGNMENT , LOOKUP , YIELD , ORDER BY , FETCH VERTICES , Find , FETCH EDGES , FIND PATH , LIMIT , GROUP BY , RETURN Write data Y Y Y Y BUILD TAG INDEX , BUILD EDGE INDEX , INSERT VERTEX , UPDATE VERTEX , INSERT EDGE , UPDATE EDGE , DELETE VERTEX , DELETE EDGES Show operations Y Y Y Y Y SHOW , CHANGE PASSWORD NOTE: The results of SHOW operations are limited to the role of a user. For example, all users can run SHOW SPACES , but the results only include the graph spaces that the users have privileges. Only the GOD role can run SHOW USERS and SHOW SNAPSHOTS .","title":"Roles and privileges"},{"location":"7.data-security/1.authentication/3.role-list/#roles_and_privileges","text":"A role is a collection of privileges. You can assign a role to a user for access control.","title":"Roles and privileges"},{"location":"7.data-security/1.authentication/3.role-list/#built-in_roles","text":"Nebula Graph does not support custom roles, but it has multiple built-in roles: GOD GOD is the original role with all privileges not limited to graph spaces. It is similar to root in Linux and administrator in Windows. When the Meta Service is initialized, the one and only GOD role user root is automatically created with the password nebula . CAUTION: Modify the password for root as soon as possible for security. The default username root is immutable. If authentication is disabled, you can use any username and password to connect to Nebula Graph. This user is regarded as the GOD role. ADMIN An ADMIN role can read and write both the Schema and the data in a specific graph space. An ADMIN role of a graph space can grant DBA, USER, and GUEST roles in the graph space to other users. DBA A DBA role can read and write both the Schema and the data in a specific graph space. A DBA role of a graph space CANNOT grant roles to other users. USER A USER role can read and write data in a specific graph space. The Schema information is read-only to the USER roles in a graph space. GUEST A GUEST role can only read the Schema and the data in a specific graph space. NOTE: A user can have only one role in a graph space.","title":"Built-in roles"},{"location":"7.data-security/1.authentication/3.role-list/#role_privileges_and_allowed_ngql","text":"The privileges of roles and the nGQL statements that each role can use are listed as follows. Privilege God Admin DBA User Guest Allowed nGQL Read space Y Y Y Y Y USE , DESCRIBE SPACE Write space Y CREATE SPACE , DROP SPACE , CREATE SNAPSHOT , DROP SNAPSHOT , BALANCE , ADMIN , CONFIG , INGEST , DOWNLOAD Read schema Y Y Y Y Y DESCRIBE TAG , DESCRIBE EDGE , DESCRIBE TAG INDEX , DESCRIBE EDGE INDEX Write schema Y Y Y CREATE TAG , ALTER TAG , CREATE EDGE , ALTER EDGE , DROP TAG , DROP EDGE , CREATE TAG INDEX , CREATE EDGE INDEX , DROP TAG INDEX , DROP EDGE INDEX Write user Y CREATE USER , DROP USER , ALTER USER Write role Y Y GRANT , REVOKE Read data Y Y Y Y Y GO , SET , PIPE , MATCH , ASSIGNMENT , LOOKUP , YIELD , ORDER BY , FETCH VERTICES , Find , FETCH EDGES , FIND PATH , LIMIT , GROUP BY , RETURN Write data Y Y Y Y BUILD TAG INDEX , BUILD EDGE INDEX , INSERT VERTEX , UPDATE VERTEX , INSERT EDGE , UPDATE EDGE , DELETE VERTEX , DELETE EDGES Show operations Y Y Y Y Y SHOW , CHANGE PASSWORD NOTE: The results of SHOW operations are limited to the role of a user. For example, all users can run SHOW SPACES , but the results only include the graph spaces that the users have privileges. Only the GOD role can run SHOW USERS and SHOW SNAPSHOTS .","title":"Role privileges and allowed nGQL"},{"location":"7.data-security/2.backup-restore/1.what-is-br/","text":"What is Backup & Restore \u00b6 Backup & Restore (BR in short) is a Command-Line Interface (CLI) tool for you to back up data of graph spaces of Nebula Graph and to restore data from the backup files. Features \u00b6 Supports storing backup files in local disks (SSD or HDD), Alibaba Cloud OSS, and Amazon S3. Supports backing up data of one or multiple graph spaces. Limitations \u00b6 Supports Nebula Graph v2.0.0-RC and later versions only. Supports full backup, but not incremental backup. Supports restoration of data on clusters of the same topologies only, which means both clusters must have exactly the same number of hosts. SSH login is required for backup and restoration. Does not support the Nebula Graph services deployed with Docker Compose. During the backup process, both DDL and DML statements in the specified graph spaces are blocked. We recommend that you do the operation within the low peak period of the business, for example, from 2:00 AM to 5:00 AM. The restoration process is performed OFFLINE. Implementation \u00b6 You can use the BR to do these: Backing up a cluster and storing its data in a local or cloud storage system. Restoring data to a cluster from a local or cloud storage system. This section introduces how backup and restoration are implemented in the BR. Backup \u00b6 To back up data, the BR sends a backup request to the leader metad process to trigger the backup process as follows: The SSH login from the BR machine to the meta and the storage servers is verified. Besides, if a remote storage system such as Amazon S3 or Alibaba Cloud OSS is necessary, their client installation and configuration are verified. The BR sends a request to create backup files. The leader metad process is locked. NOTE : From now on, you cannot run any DDL statement of nGQL until Step 9. The leader metad process blocks writing to the specified graph spaces. NOTE : From now on, you cannot run any DML statement of nGQL in the specified graph spaces until Step 7. But this process has no effect on the DQL statements in these graph spaces, and you can do whatever you want in other graph spaces. The leader metad process sends a request to the storaged processes for the snapshot file names. The leader metad process scans local RocksDB files and output SST files. The leader metad process cancels blocking writing to the specified graph spaces. NOTE : From now on, you can run DML statements in the specified graph spaces. The leader metad process sends responses to the BR with the metadata including: the thrift format, partition information of the graph spaces, and the Raft log commit ID of each partition, and the snapshot information including the catalog of the snapshots of each storaged process, their SST file names of the meta server, and the backup file names. The leader metad process is unlocked. > NOTE : From now on, you can run any DDL statement in the specified graph spaces. The account on the BR machine logs on via SSH to the meta server where the leader locates and to all the storage servers, and backs up files. If Amazon S3 or Alibaba Cloud OSS is used, the BR calls commands to upload the files to the cloud storage system. > NOTE : This step causes massive disk reads. We recommend that a 10 Gigabit Network is applied. If a networking error occurs during this step, the backup process fails and you must do the backup operation again. For now, the backup process cannot be resumed from the broken point. The BR sends a request to clean the snapshots from meta server and storage servers, and the backup process is done. This figure shows how the backup is implemented. When backup files are generated, the file names are generated automatically. A folder name is in the format of BACKUP_YY_MM_DD_HH_mm_SS , of which, BACKUP indicates the files are backup files. YY_MM_DD_HH_mm_SS indicates the timestamp when the files are generated. Restore \u00b6 CAUTION : During the restoration process, the data on the target cluster is removed and then is replaced with the data from the backup files. If necessary, back up the data on the target cluster. The restoration process is implemented as follows: The SSH login from the BR to the meta and the storage servers is verified. Besides, if a cloud storage system such as Amazon S3 or Alibaba Cloud OSS is necessary, their client installation and configuration are verified. The BR downloads the metadata (but not data) of the backup files from the remote storage system or other external storage systems. The BR verifies the topology of the clusters. The BR stops the Meta Service and the Storage Service remotely. The account on the BR machine logs on via SSH to the meta and storage servers to remove the existing data files. When data files are removed, the account on the BR machine logs on via SSH to the meta and storage servers and downloads the backup files from the cloud storage system or other external storage systems. When the backup files are downloaded, the BR starts the Meta Service. The BR calls the br restore command to change the partition information of the specified metad processes. The BR starts the Storage Service, and the restoration process is done. This figure shows how the restoration process is implemented. How to use \u00b6 To use the BR, follow these steps: Compile BR . Use BR to back up data . Use BR to restore data from backup files .","title":"What is Backup & Restore"},{"location":"7.data-security/2.backup-restore/1.what-is-br/#what_is_backup_restore","text":"Backup & Restore (BR in short) is a Command-Line Interface (CLI) tool for you to back up data of graph spaces of Nebula Graph and to restore data from the backup files.","title":"What is Backup &amp; Restore"},{"location":"7.data-security/2.backup-restore/1.what-is-br/#features","text":"Supports storing backup files in local disks (SSD or HDD), Alibaba Cloud OSS, and Amazon S3. Supports backing up data of one or multiple graph spaces.","title":"Features"},{"location":"7.data-security/2.backup-restore/1.what-is-br/#limitations","text":"Supports Nebula Graph v2.0.0-RC and later versions only. Supports full backup, but not incremental backup. Supports restoration of data on clusters of the same topologies only, which means both clusters must have exactly the same number of hosts. SSH login is required for backup and restoration. Does not support the Nebula Graph services deployed with Docker Compose. During the backup process, both DDL and DML statements in the specified graph spaces are blocked. We recommend that you do the operation within the low peak period of the business, for example, from 2:00 AM to 5:00 AM. The restoration process is performed OFFLINE.","title":"Limitations"},{"location":"7.data-security/2.backup-restore/1.what-is-br/#implementation","text":"You can use the BR to do these: Backing up a cluster and storing its data in a local or cloud storage system. Restoring data to a cluster from a local or cloud storage system. This section introduces how backup and restoration are implemented in the BR.","title":"Implementation"},{"location":"7.data-security/2.backup-restore/1.what-is-br/#backup","text":"To back up data, the BR sends a backup request to the leader metad process to trigger the backup process as follows: The SSH login from the BR machine to the meta and the storage servers is verified. Besides, if a remote storage system such as Amazon S3 or Alibaba Cloud OSS is necessary, their client installation and configuration are verified. The BR sends a request to create backup files. The leader metad process is locked. NOTE : From now on, you cannot run any DDL statement of nGQL until Step 9. The leader metad process blocks writing to the specified graph spaces. NOTE : From now on, you cannot run any DML statement of nGQL in the specified graph spaces until Step 7. But this process has no effect on the DQL statements in these graph spaces, and you can do whatever you want in other graph spaces. The leader metad process sends a request to the storaged processes for the snapshot file names. The leader metad process scans local RocksDB files and output SST files. The leader metad process cancels blocking writing to the specified graph spaces. NOTE : From now on, you can run DML statements in the specified graph spaces. The leader metad process sends responses to the BR with the metadata including: the thrift format, partition information of the graph spaces, and the Raft log commit ID of each partition, and the snapshot information including the catalog of the snapshots of each storaged process, their SST file names of the meta server, and the backup file names. The leader metad process is unlocked. > NOTE : From now on, you can run any DDL statement in the specified graph spaces. The account on the BR machine logs on via SSH to the meta server where the leader locates and to all the storage servers, and backs up files. If Amazon S3 or Alibaba Cloud OSS is used, the BR calls commands to upload the files to the cloud storage system. > NOTE : This step causes massive disk reads. We recommend that a 10 Gigabit Network is applied. If a networking error occurs during this step, the backup process fails and you must do the backup operation again. For now, the backup process cannot be resumed from the broken point. The BR sends a request to clean the snapshots from meta server and storage servers, and the backup process is done. This figure shows how the backup is implemented. When backup files are generated, the file names are generated automatically. A folder name is in the format of BACKUP_YY_MM_DD_HH_mm_SS , of which, BACKUP indicates the files are backup files. YY_MM_DD_HH_mm_SS indicates the timestamp when the files are generated.","title":"Backup"},{"location":"7.data-security/2.backup-restore/1.what-is-br/#restore","text":"CAUTION : During the restoration process, the data on the target cluster is removed and then is replaced with the data from the backup files. If necessary, back up the data on the target cluster. The restoration process is implemented as follows: The SSH login from the BR to the meta and the storage servers is verified. Besides, if a cloud storage system such as Amazon S3 or Alibaba Cloud OSS is necessary, their client installation and configuration are verified. The BR downloads the metadata (but not data) of the backup files from the remote storage system or other external storage systems. The BR verifies the topology of the clusters. The BR stops the Meta Service and the Storage Service remotely. The account on the BR machine logs on via SSH to the meta and storage servers to remove the existing data files. When data files are removed, the account on the BR machine logs on via SSH to the meta and storage servers and downloads the backup files from the cloud storage system or other external storage systems. When the backup files are downloaded, the BR starts the Meta Service. The BR calls the br restore command to change the partition information of the specified metad processes. The BR starts the Storage Service, and the restoration process is done. This figure shows how the restoration process is implemented.","title":"Restore"},{"location":"7.data-security/2.backup-restore/1.what-is-br/#how_to_use","text":"To use the BR, follow these steps: Compile BR . Use BR to back up data . Use BR to restore data from backup files .","title":"How to use"},{"location":"7.data-security/2.backup-restore/2.compile-br/","text":"Compile BR \u00b6 For now, the BR is not provided as a package. You can compile the BR. NOTE : If you want to store the backup files locally, we recommend that you compile the BR on one meta server of the target Nebula Graph cluster where you will perform data restoration. For more information, see Restore data from backup files . Prerequisites \u00b6 To compile the BR, do a check of these: Go 1.14 or a later version is installed. make is installed. Procedure \u00b6 To compile the BR, follow these steps: Clone the nebula-storage repository to your machine. git clone https://github.com/vesoft-inc/nebula-storage.git Change to the br diretory. cd nebula-storage/util/br Compile the BR. make build && make test When the BR is compiled successfully, you can find the br binary file under the nebula-storage/util/br/bin/ directory.","title":"Compile BR"},{"location":"7.data-security/2.backup-restore/2.compile-br/#compile_br","text":"For now, the BR is not provided as a package. You can compile the BR. NOTE : If you want to store the backup files locally, we recommend that you compile the BR on one meta server of the target Nebula Graph cluster where you will perform data restoration. For more information, see Restore data from backup files .","title":"Compile BR"},{"location":"7.data-security/2.backup-restore/2.compile-br/#prerequisites","text":"To compile the BR, do a check of these: Go 1.14 or a later version is installed. make is installed.","title":"Prerequisites"},{"location":"7.data-security/2.backup-restore/2.compile-br/#procedure","text":"To compile the BR, follow these steps: Clone the nebula-storage repository to your machine. git clone https://github.com/vesoft-inc/nebula-storage.git Change to the br diretory. cd nebula-storage/util/br Compile the BR. make build && make test When the BR is compiled successfully, you can find the br binary file under the nebula-storage/util/br/bin/ directory.","title":"Procedure"},{"location":"7.data-security/2.backup-restore/3.br-backup-data/","text":"Use BR to back up data \u00b6 After the BR is compiled, you can back up data of specified graph spaces. This article introduces how to use the BR to back up data. Prerequisites \u00b6 To back up data with the BR, do a check of these: The BR is compiled. For more information, see Compile BR . NOTE : If you want to store the backup files locally, we recommend that you compile the BR on one meta server of the target Nebula Graph cluster where you will perform data restoration. For more information, see Restore data from backup files . The Nebula Graph services are running and we recommend that the backup is performed when the application request traffic is very low. Get the names of the specified graph spaces. In this example, nba is used. Get the Nebula Graph installation directory. In this example, /usr/local/nebula/ is used. From the nebula-metad.conf and nebula-storaged.conf files, get the IP addresses and ports of the meta and the storage servers. Both files are in the <nebula_installation_path>/nebula/etc directory. In this example, For the meta server: 192.168.8.161:9559 For the storage server: 192.168.8.161:9779 NOTE : Make sure that the actual IP addresses instead of 127.0.0.1 are used in the configuration files. Your account on the BR machine can log on via SSH to the meta and the storage servers without a password. Here is a configuration reference . In this example, such an account named nebula on the BR machine is used. If you use Amazon S3 or Alibaba Cloud OSS to store the backup files, make sure that the S3 CLI client or ossutil is installed and configured on the meta servers, the storage servers, and the BR machine. For more information, see Amazon S3 CLI Documentation and Alibaba Cloud ossutil Documentation . NOTE : Run ln -s /<ossutil_tool_installation_path>/<ossutil64 or ossutil> /usr/local/bin/ossutil to make the ossutil command effective. If you store the backup files locally, create a directory with the same absolute path on the meta and the storage servers and the BR machine for the backup files and get the absolute path. In this example, /home/user/backup/ is used. NOTE : In the production environment, we recommend that you mount Network File System (NFS) storage to the meta and the storage servers and the BR machine for local backup, or use S3 or OSS for remote backup. When you restore the data from local files, you must manually move these backup files to a specified directory, which causes redundant data and troubles. For more information, see Restore data from backup files . Procedure \u00b6 To back up data of the specified graph spaces: Edit the configuration file as follows. You can find an example configuration in the nebula-storage/util/br/ directory. meta_nodes : - # Set the IP address and the port of one meta server addrs : \"192.168.8.161:9559\" # Set the absolute path of the Nebula Graph installation directory root : \"/usr/local/nebula/\" # Set the absolute path of the data directory of this metad process data : \"/usr/local/nebula/data/meta\" # Set the account of the BR machine that is authorized to log on to the meta server via SSH without a password user : \"nebula\" #- # If more than one metad process runs, refer to the preceding configuration to add more # addrs: \"192.168.8.161:9559\" # root: \"/usr/local/nebula/\" # data: \"/usr/local/nebula/data/meta\" # user: \"nebula\" #- addrs: \"192.168.8.161:9559\" # root: \"/usr/local/nebula/\" # data: \"/usr/local/nebula/data/meta\" # user: \"nebula\" storage_nodes : - # Set the IP address and the port of one storage server addrs : \"192.168.8.161:9779\" # Set the absolute path of the Nebula Graph installation directory root : \"/usr/local/nebula/\" # Set the absolute path of the data directory of the storaged process data : \"/usr/local/nebula/data/storage\" # Set the account on the BR machine that is authorized to log on to the storage server via SSH without a password user : \"nebula\" #- If more than one storaged processes run, refer to the preceding configuration to add more # addrs: \"192.168.8.161:9779\" # root: \"/usr/local/nebula/\" # data: \"/usr/local/nebula/data/storage\" # user: \"nebula\" #- addrs: \"192.168.8.161:9779\" # root: \"/usr/local/nebula/\" # data: \"/usr/local/nebula/data/storage\" # user: \"nebula\" # Set the store directory for the backup files. # If the backup files are stored locally, set backend : \"local:///absolute/path/to/the/store/directory\" # If Alibaba Cloud OSS is used, set # backend: \"oss://nebulabackup\" # If Amazon S3 is used, set # backend: \"s3://nebulabackup\" # Set the graph spaces to be backed up. # If more than one graph spaces are necessary, set # space_names: [\"space_name1\", \"space_name2\", ..., \"space_name\"] space_name : [ \"nba\" ] Run the command to change to the nebula-storage/util/br/bin/ directory. cd nebula-storage/util/util/br/bin/ Run the command to back up data. ./br backup full --config \"/absolute/path/to/the/backup/configuration/file.yaml\" In this command: backup full : Backs up data. --config \"/absolute/path/to/the/backup/configuration/file.yaml\" : Sets the absolute path of the configuration file. NOTE : During the backup process, if the leader changes, an error occurs. You can clean the temporary files with the br cleanup command as in Step 4, and then run the br backup command again. When the backup is successful, you can find a backup folder with a name in the BACKUP_YY_MM_DD_HH_mm_SS format in the backup store directory on the BR machine and all the servers of the cluster. In this example, in the /home/user/backup/ directory, you can find a folder named BACKUP_2020_11_30_20_47_44 . All these backup files on all the machines are required for data restoration. (Optional) By default, all the snapshots will be deleted when the backup is done. If errors occur during the deletion of these files, run this command to delete them. ./br cleanup --backup_name [ backup file name ] --meta 192 .168.8.161:9559 In this command: - cleanup : Deletes all the temporary files on the meta and the storage servers. - --backup_name BACKUP_YY_MM_DD_HH_mm_SS : Sets a backup folder name, indicating the command is run to delete the temporary files that were generated when this backup folder was generated. - --meta <IP address:port> : Sets the IP address and the port of a meta server. Next to do \u00b6 After the backup files are generated, you can use the BR to restore them for Nebula Graph. For more information, see Use BR to restore data .","title":"Use BR to back up data"},{"location":"7.data-security/2.backup-restore/3.br-backup-data/#use_br_to_back_up_data","text":"After the BR is compiled, you can back up data of specified graph spaces. This article introduces how to use the BR to back up data.","title":"Use BR to back up data"},{"location":"7.data-security/2.backup-restore/3.br-backup-data/#prerequisites","text":"To back up data with the BR, do a check of these: The BR is compiled. For more information, see Compile BR . NOTE : If you want to store the backup files locally, we recommend that you compile the BR on one meta server of the target Nebula Graph cluster where you will perform data restoration. For more information, see Restore data from backup files . The Nebula Graph services are running and we recommend that the backup is performed when the application request traffic is very low. Get the names of the specified graph spaces. In this example, nba is used. Get the Nebula Graph installation directory. In this example, /usr/local/nebula/ is used. From the nebula-metad.conf and nebula-storaged.conf files, get the IP addresses and ports of the meta and the storage servers. Both files are in the <nebula_installation_path>/nebula/etc directory. In this example, For the meta server: 192.168.8.161:9559 For the storage server: 192.168.8.161:9779 NOTE : Make sure that the actual IP addresses instead of 127.0.0.1 are used in the configuration files. Your account on the BR machine can log on via SSH to the meta and the storage servers without a password. Here is a configuration reference . In this example, such an account named nebula on the BR machine is used. If you use Amazon S3 or Alibaba Cloud OSS to store the backup files, make sure that the S3 CLI client or ossutil is installed and configured on the meta servers, the storage servers, and the BR machine. For more information, see Amazon S3 CLI Documentation and Alibaba Cloud ossutil Documentation . NOTE : Run ln -s /<ossutil_tool_installation_path>/<ossutil64 or ossutil> /usr/local/bin/ossutil to make the ossutil command effective. If you store the backup files locally, create a directory with the same absolute path on the meta and the storage servers and the BR machine for the backup files and get the absolute path. In this example, /home/user/backup/ is used. NOTE : In the production environment, we recommend that you mount Network File System (NFS) storage to the meta and the storage servers and the BR machine for local backup, or use S3 or OSS for remote backup. When you restore the data from local files, you must manually move these backup files to a specified directory, which causes redundant data and troubles. For more information, see Restore data from backup files .","title":"Prerequisites"},{"location":"7.data-security/2.backup-restore/3.br-backup-data/#procedure","text":"To back up data of the specified graph spaces: Edit the configuration file as follows. You can find an example configuration in the nebula-storage/util/br/ directory. meta_nodes : - # Set the IP address and the port of one meta server addrs : \"192.168.8.161:9559\" # Set the absolute path of the Nebula Graph installation directory root : \"/usr/local/nebula/\" # Set the absolute path of the data directory of this metad process data : \"/usr/local/nebula/data/meta\" # Set the account of the BR machine that is authorized to log on to the meta server via SSH without a password user : \"nebula\" #- # If more than one metad process runs, refer to the preceding configuration to add more # addrs: \"192.168.8.161:9559\" # root: \"/usr/local/nebula/\" # data: \"/usr/local/nebula/data/meta\" # user: \"nebula\" #- addrs: \"192.168.8.161:9559\" # root: \"/usr/local/nebula/\" # data: \"/usr/local/nebula/data/meta\" # user: \"nebula\" storage_nodes : - # Set the IP address and the port of one storage server addrs : \"192.168.8.161:9779\" # Set the absolute path of the Nebula Graph installation directory root : \"/usr/local/nebula/\" # Set the absolute path of the data directory of the storaged process data : \"/usr/local/nebula/data/storage\" # Set the account on the BR machine that is authorized to log on to the storage server via SSH without a password user : \"nebula\" #- If more than one storaged processes run, refer to the preceding configuration to add more # addrs: \"192.168.8.161:9779\" # root: \"/usr/local/nebula/\" # data: \"/usr/local/nebula/data/storage\" # user: \"nebula\" #- addrs: \"192.168.8.161:9779\" # root: \"/usr/local/nebula/\" # data: \"/usr/local/nebula/data/storage\" # user: \"nebula\" # Set the store directory for the backup files. # If the backup files are stored locally, set backend : \"local:///absolute/path/to/the/store/directory\" # If Alibaba Cloud OSS is used, set # backend: \"oss://nebulabackup\" # If Amazon S3 is used, set # backend: \"s3://nebulabackup\" # Set the graph spaces to be backed up. # If more than one graph spaces are necessary, set # space_names: [\"space_name1\", \"space_name2\", ..., \"space_name\"] space_name : [ \"nba\" ] Run the command to change to the nebula-storage/util/br/bin/ directory. cd nebula-storage/util/util/br/bin/ Run the command to back up data. ./br backup full --config \"/absolute/path/to/the/backup/configuration/file.yaml\" In this command: backup full : Backs up data. --config \"/absolute/path/to/the/backup/configuration/file.yaml\" : Sets the absolute path of the configuration file. NOTE : During the backup process, if the leader changes, an error occurs. You can clean the temporary files with the br cleanup command as in Step 4, and then run the br backup command again. When the backup is successful, you can find a backup folder with a name in the BACKUP_YY_MM_DD_HH_mm_SS format in the backup store directory on the BR machine and all the servers of the cluster. In this example, in the /home/user/backup/ directory, you can find a folder named BACKUP_2020_11_30_20_47_44 . All these backup files on all the machines are required for data restoration. (Optional) By default, all the snapshots will be deleted when the backup is done. If errors occur during the deletion of these files, run this command to delete them. ./br cleanup --backup_name [ backup file name ] --meta 192 .168.8.161:9559 In this command: - cleanup : Deletes all the temporary files on the meta and the storage servers. - --backup_name BACKUP_YY_MM_DD_HH_mm_SS : Sets a backup folder name, indicating the command is run to delete the temporary files that were generated when this backup folder was generated. - --meta <IP address:port> : Sets the IP address and the port of a meta server.","title":"Procedure"},{"location":"7.data-security/2.backup-restore/3.br-backup-data/#next_to_do","text":"After the backup files are generated, you can use the BR to restore them for Nebula Graph. For more information, see Use BR to restore data .","title":"Next to do"},{"location":"7.data-security/2.backup-restore/4.br-restore-data/","text":"Use BR to restore data \u00b6 If you use the BR to back up data, you can use it to restore the data to Nebula Graph. This article introduces how to use the BR to restore data from backup files. NOTE : The restoration process is performed OFFLINE. CAUTION : During the restoration process, the data on the target Nebula Graph cluster is removed and then is replaced with the data from the backup files. If necessary, back up the data on the target cluster. Prerequisites \u00b6 To restore data with the BR, do a check of these: The BR is compiled. For more information, see Compile BR . No application is connected to the target Nebula Graph cluster. Make sure that the target and the source Nebula Graph clusters have the same topology, which means that they have exactly the same number of hosts. Get the backup folder names to do the restoration. In this example, BACKUP_2020_12_21_01_17_53 is used. From the nebula-metad.conf and nebula-storaged.conf files, get the IP addresses and ports of the meta and the storage servers. Both files are in the <nebula_installation_path>/nebula/etc directory. In this example, For the meta server: 192.168.8.161:9559 For the storage server: 192.168.8.161:9779 NOTE : Make sure that the actual IP addresses instead of 127.0.0.1 are used in the configuration file. Your account on the BR machine can log on to the meta and the storage servers via SSH without a password. Here is a configuration reference . This account must have the write permission to the installation directory of Nebula Graph. In this example, such an account named nebula on the BR machine is used. If the backup files are stored on Alibaba Cloud OSS or Amazon S3, make sure that the S3 CLI client or ossutil is installed and configured on the meta servers, the storage servers, and the BR machine. For more information, see Amazon S3 CLI Documentation and Alibaba Cloud ossutil Documentation . NOTE : Run ln -s /<ossutil_tool_installation_path>/<ossutil64 or ossutil> /usr/local/bin/ossutil to make the ossutil command effective. If the backup files are stored locally on the servers, create a directory with the same absolute path on the BR machine and all the servers of the target Nebula Graph cluster, and then manually move these backup files to this directory. Such file movement causes redundant data and troubles. Procedure \u00b6 To restore data from some backup files: Edit the configuration file as follows. You can find an example configuration in the nebula-storage/util/br/ directory. meta_nodes : - # Set the IP address and the port of one meta server addrs : \"192.168.8.161:9559\" # Set the absolute path of the Nebula Graph installation directory root : \"/usr/local/nebula/\" # Set the absolute path of the data directory of the metad process data : \"/usr/local/nebula/data/meta\" # Set the account of the BR machine that is authorized to log on to the meta server via SSH user : \"nebula\" #- # If more than one metad processes run, refer to the preceding configuration to add more #- addrs: \"192.168.8.161:9559\" # root: \"/usr/local/nebula/\" # data: \"/usr/local/nebula/data/meta\" # user: \"nebula\" #- addrs: \"192.168.8.161:9559\" # root: \"/usr/local/nebula/\" # data: \"/usr/local/nebula/data/meta\" # user: \"nebula\" storage_nodes : - # Set the IP address and the port of one storage server addrs : \"192.168.8.161:9779\" # Set the absolute path of the Nebula Graph installation directory root : \"/usr/local/nebula/\" # Set the absolute path of the data directory of the storaged process data : \"/usr/local/nebula/data/storage\" # Set the account of the BR machine that is authorized to log on to the storage server via SSH user : \"nebula\" #- If more than one storaged processes run, refer to the preceding configuration to add more #- addrs: \"192.168.8.161:9779\" # root: \"/usr/local/nebula/\" # data: \"/usr/local/nebula/data/storage\" # user: \"nebula\" #- addrs: \"192.168.8.161:9779\" # root: \"/usr/local/nebula/\" # data: \"/usr/local/nebula/data/storage\" # user: \"nebula\" # Set the directory where the backup files are located. # If the backup files are stored locally backend : \"local:///absolute/path/to/the/store/directory\" # If Alibaba Cloud OSS is used # backend: \"oss://nebulabackup\" # If Amazon S3 is used # backend: \"s3://nebulabackup\" # Set the backup files to be restored backup_name : \"BACKUP_2020_12_21_01_17_53\" Run the command to change to the nebula-storage/util/br/bin/ directory. cd nebula-storage/util/util/br/bin/ Run the command to restore data. ./br restore full --config \"/absolute/path/to/the/restore/configuration/file.yaml\" In this command: - restore full : Restores data. - --config \"/absolute/path/to/the/restore/configuration/file.yaml\" : Sets the absolute path of the configuration file. NOTE : During the restoration process, if the leader changes, an error occurs. To prevent data corruption, when an error occurs, you must run the br restore command to perform the restoration again. When the restoration is successful, you can find the data in the <nebula_installation_path>/data/storage directory under the Nebula Graph installation directory. Wait about several seconds until the metadata and the schema are synchronized, and then verify the data. For example, on the nebula-console, run SHOW STATS to verify the number of vertices and edges in the restored graph space. NOTE : After restoration, if no records are returned for the USE <space_name> statement, we recommend that you restart the Graph Service. if the Storage Error: part: 2, error code: -3. error occurs when you query the restored data, do a check of the status of the Storage Service. If necessary, restart the Storage Service.","title":"Use BR to restore data"},{"location":"7.data-security/2.backup-restore/4.br-restore-data/#use_br_to_restore_data","text":"If you use the BR to back up data, you can use it to restore the data to Nebula Graph. This article introduces how to use the BR to restore data from backup files. NOTE : The restoration process is performed OFFLINE. CAUTION : During the restoration process, the data on the target Nebula Graph cluster is removed and then is replaced with the data from the backup files. If necessary, back up the data on the target cluster.","title":"Use BR to restore data"},{"location":"7.data-security/2.backup-restore/4.br-restore-data/#prerequisites","text":"To restore data with the BR, do a check of these: The BR is compiled. For more information, see Compile BR . No application is connected to the target Nebula Graph cluster. Make sure that the target and the source Nebula Graph clusters have the same topology, which means that they have exactly the same number of hosts. Get the backup folder names to do the restoration. In this example, BACKUP_2020_12_21_01_17_53 is used. From the nebula-metad.conf and nebula-storaged.conf files, get the IP addresses and ports of the meta and the storage servers. Both files are in the <nebula_installation_path>/nebula/etc directory. In this example, For the meta server: 192.168.8.161:9559 For the storage server: 192.168.8.161:9779 NOTE : Make sure that the actual IP addresses instead of 127.0.0.1 are used in the configuration file. Your account on the BR machine can log on to the meta and the storage servers via SSH without a password. Here is a configuration reference . This account must have the write permission to the installation directory of Nebula Graph. In this example, such an account named nebula on the BR machine is used. If the backup files are stored on Alibaba Cloud OSS or Amazon S3, make sure that the S3 CLI client or ossutil is installed and configured on the meta servers, the storage servers, and the BR machine. For more information, see Amazon S3 CLI Documentation and Alibaba Cloud ossutil Documentation . NOTE : Run ln -s /<ossutil_tool_installation_path>/<ossutil64 or ossutil> /usr/local/bin/ossutil to make the ossutil command effective. If the backup files are stored locally on the servers, create a directory with the same absolute path on the BR machine and all the servers of the target Nebula Graph cluster, and then manually move these backup files to this directory. Such file movement causes redundant data and troubles.","title":"Prerequisites"},{"location":"7.data-security/2.backup-restore/4.br-restore-data/#procedure","text":"To restore data from some backup files: Edit the configuration file as follows. You can find an example configuration in the nebula-storage/util/br/ directory. meta_nodes : - # Set the IP address and the port of one meta server addrs : \"192.168.8.161:9559\" # Set the absolute path of the Nebula Graph installation directory root : \"/usr/local/nebula/\" # Set the absolute path of the data directory of the metad process data : \"/usr/local/nebula/data/meta\" # Set the account of the BR machine that is authorized to log on to the meta server via SSH user : \"nebula\" #- # If more than one metad processes run, refer to the preceding configuration to add more #- addrs: \"192.168.8.161:9559\" # root: \"/usr/local/nebula/\" # data: \"/usr/local/nebula/data/meta\" # user: \"nebula\" #- addrs: \"192.168.8.161:9559\" # root: \"/usr/local/nebula/\" # data: \"/usr/local/nebula/data/meta\" # user: \"nebula\" storage_nodes : - # Set the IP address and the port of one storage server addrs : \"192.168.8.161:9779\" # Set the absolute path of the Nebula Graph installation directory root : \"/usr/local/nebula/\" # Set the absolute path of the data directory of the storaged process data : \"/usr/local/nebula/data/storage\" # Set the account of the BR machine that is authorized to log on to the storage server via SSH user : \"nebula\" #- If more than one storaged processes run, refer to the preceding configuration to add more #- addrs: \"192.168.8.161:9779\" # root: \"/usr/local/nebula/\" # data: \"/usr/local/nebula/data/storage\" # user: \"nebula\" #- addrs: \"192.168.8.161:9779\" # root: \"/usr/local/nebula/\" # data: \"/usr/local/nebula/data/storage\" # user: \"nebula\" # Set the directory where the backup files are located. # If the backup files are stored locally backend : \"local:///absolute/path/to/the/store/directory\" # If Alibaba Cloud OSS is used # backend: \"oss://nebulabackup\" # If Amazon S3 is used # backend: \"s3://nebulabackup\" # Set the backup files to be restored backup_name : \"BACKUP_2020_12_21_01_17_53\" Run the command to change to the nebula-storage/util/br/bin/ directory. cd nebula-storage/util/util/br/bin/ Run the command to restore data. ./br restore full --config \"/absolute/path/to/the/restore/configuration/file.yaml\" In this command: - restore full : Restores data. - --config \"/absolute/path/to/the/restore/configuration/file.yaml\" : Sets the absolute path of the configuration file. NOTE : During the restoration process, if the leader changes, an error occurs. To prevent data corruption, when an error occurs, you must run the br restore command to perform the restoration again. When the restoration is successful, you can find the data in the <nebula_installation_path>/data/storage directory under the Nebula Graph installation directory. Wait about several seconds until the metadata and the schema are synchronized, and then verify the data. For example, on the nebula-console, run SHOW STATS to verify the number of vertices and edges in the restored graph space. NOTE : After restoration, if no records are returned for the USE <space_name> statement, we recommend that you restart the Graph Service. if the Storage Error: part: 2, error code: -3. error occurs when you query the restored data, do a check of the status of the Storage Service. If necessary, restart the Storage Service.","title":"Procedure"},{"location":"8.service-tuning/compaction/","text":"Compaction \u00b6 This document gives some information about compaction. Introduction to compaction \u00b6 In Nebula Graph, compaction is the most important background process. Compaction has an important effect on performance. Compaction reads the data that is written on the hard disk, then re-organizes the data structure and the indexes to make the data easier to read. The read performance can increase by times after compaction. Thus, to get high read performance, trigger compaction manually when writing a large amount of data into Nebula Graph. Note that compaction leads to long time hard disk IO, we suggest that you do compaction during off-peak hours (for example, early morning). Nebula Graph has two types of compaction: automatic compaction and full compaction. Automatic compaction \u00b6 Automatic compaction is done when the system reads data, writes data, or the system restarts. The automatic compaction is enabled by default. But once triggered during peak hours, it can cause unexpected IO occupancy that has an unwanted effect on the performance. To disable automatic compaction, use this statement: nebula> UPDATE CONFIGS storage:rocksdb_column_family_options = {disable_auto_compactions = true}; Full compaction \u00b6 Full compaction enables large scale background operations for a graph space such as merging files, deleting the data expired by TTL. Use these statements to enable full compaction: nebula> USE <your_graph_space>; nebula> SUBMIT JOB COMPACT; The preceding statement returns a job_id. To show the compaction progress, use this statement: nebula> SHOW JOB <job_id>; NOTE: Do the full compaction during off-peak hours because full compaction has a lot of IO operations. Operation suggestions \u00b6 These are some operation suggestions to keep Nebula Graph performing well. To avoid unwanted IO waste during data writing, set disable_auto_compactions to true before large amounts of data writing. After data import is done, run SUBMIT JOB COMPACT . Run SUBMIT JOB COMPACT periodically during off-peak hours, for example, early morning. Set disable_auto_compactions to false during day time. To control the read and write traffic limitation for compactions, set these two parameters in the nebula-storaged.conf configuration file. # read from the local configuration file and start --local-config = true --rate_limit = 20 ( in MB/s ) FAQ \u00b6 Q: Can I do full compactions for multiple graph spaces at the same time? A: Yes, you can. But the IO is much larger at this time. Q: How much time does it take for full compactions? A: When rate_limit is set to 20 , you can estimate the full compaction time by dividing the hard disk usage by the rate_limit . If you do not set the rate_limit value, the empirical value is around 50 MB/s. Q: Can I modify --rate_limit dynamically? A: No, you cannot. Q: Can I stop a full compaction after it starts? A: No you cannot. When you start a full compaction, you have to wait till it is done. This is the limitation of RocksDB.","title":"Compaction"},{"location":"8.service-tuning/compaction/#compaction","text":"This document gives some information about compaction.","title":"Compaction"},{"location":"8.service-tuning/compaction/#introduction_to_compaction","text":"In Nebula Graph, compaction is the most important background process. Compaction has an important effect on performance. Compaction reads the data that is written on the hard disk, then re-organizes the data structure and the indexes to make the data easier to read. The read performance can increase by times after compaction. Thus, to get high read performance, trigger compaction manually when writing a large amount of data into Nebula Graph. Note that compaction leads to long time hard disk IO, we suggest that you do compaction during off-peak hours (for example, early morning). Nebula Graph has two types of compaction: automatic compaction and full compaction.","title":"Introduction to compaction"},{"location":"8.service-tuning/compaction/#automatic_compaction","text":"Automatic compaction is done when the system reads data, writes data, or the system restarts. The automatic compaction is enabled by default. But once triggered during peak hours, it can cause unexpected IO occupancy that has an unwanted effect on the performance. To disable automatic compaction, use this statement: nebula> UPDATE CONFIGS storage:rocksdb_column_family_options = {disable_auto_compactions = true};","title":"Automatic compaction"},{"location":"8.service-tuning/compaction/#full_compaction","text":"Full compaction enables large scale background operations for a graph space such as merging files, deleting the data expired by TTL. Use these statements to enable full compaction: nebula> USE <your_graph_space>; nebula> SUBMIT JOB COMPACT; The preceding statement returns a job_id. To show the compaction progress, use this statement: nebula> SHOW JOB <job_id>; NOTE: Do the full compaction during off-peak hours because full compaction has a lot of IO operations.","title":"Full compaction"},{"location":"8.service-tuning/compaction/#operation_suggestions","text":"These are some operation suggestions to keep Nebula Graph performing well. To avoid unwanted IO waste during data writing, set disable_auto_compactions to true before large amounts of data writing. After data import is done, run SUBMIT JOB COMPACT . Run SUBMIT JOB COMPACT periodically during off-peak hours, for example, early morning. Set disable_auto_compactions to false during day time. To control the read and write traffic limitation for compactions, set these two parameters in the nebula-storaged.conf configuration file. # read from the local configuration file and start --local-config = true --rate_limit = 20 ( in MB/s )","title":"Operation suggestions"},{"location":"8.service-tuning/compaction/#faq","text":"Q: Can I do full compactions for multiple graph spaces at the same time? A: Yes, you can. But the IO is much larger at this time. Q: How much time does it take for full compactions? A: When rate_limit is set to 20 , you can estimate the full compaction time by dividing the hard disk usage by the rate_limit . If you do not set the rate_limit value, the empirical value is around 50 MB/s. Q: Can I modify --rate_limit dynamically? A: No, you cannot. Q: Can I stop a full compaction after it starts? A: No you cannot. When you start a full compaction, you have to wait till it is done. This is the limitation of RocksDB.","title":"FAQ"},{"location":"8.service-tuning/load-balance/","text":"Storage load balance \u00b6 To scale in or scale out the Nebula Graph Storage Service, use the BALANCE statements. Balance partition distribution \u00b6 BALANCE DATA starts a task to equally distribute the storage partitions in a Nebula Graph cluster. A group of subtasks will be created and implemented to migrate data and balance the partition distribution. DON'T: DON'T stop any machine in the cluster or change its IP address until all the subtasks finish. Otherwise, the follow-up subtasks fail. Take scaling out Nebula Graph for an example. After you add new storage hosts into the cluster, no partition is deployed on the new hosts. You can run SHOW HOSTS to check the partition distribution. nebual> SHOW HOSTS; +-------------+------+----------+--------------+----------------------+------------------------+ | Host | Port | Status | Leader count | Leader distribution | Partition distribution | +-------------+------+----------+--------------+----------------------+------------------------+ | \"storaged0\" | 9779 | \"ONLINE\" | 4 | \"nba:4\" | \"nba:15\" | +-------------+------+----------+--------------+----------------------+------------------------+ | \"storaged1\" | 9779 | \"ONLINE\" | 8 | \"nba:8\" | \"nba:15\" | +-------------+------+----------+--------------+----------------------+------------------------+ | \"storaged2\" | 9779 | \"ONLINE\" | 3 | \"nba:3\" | \"nba:15\" | +-------------+------+----------+--------------+----------------------+------------------------+ | \"storaged3\" | 9779 | \"ONLINE\" | 0 | \"No valid partition\" | \"No valid partition\" | +-------------+------+----------+--------------+----------------------+------------------------+ | \"storaged4\" | 9779 | \"ONLINE\" | 0 | \"No valid partition\" | \"No valid partition\" | +-------------+------+----------+--------------+----------------------+------------------------+ | \"Total\" | | | 15 | \"nba:15\" | \"nba:45\" | +-------------+------+----------+--------------+----------------------+------------------------+ Got 6 rows (time spent 1002/1780 us) Run BALANCE DATA to start balancing the storage partitions. If the partitions are already balanced, BALANCE DATA fails. nebula> BALANCE DATA; +------------+ | ID | +------------+ | 1614237867 | +------------+ Got 1 rows (time spent 3783/4533 us) A BALANCE task ID is returned after running BALANCE DATA . Run BALANCE DATA <balance_id> to check the status of the BALANCE task. nebula> BALANCE DATA 1614237867; +--------------------------------------------------------------+-------------------+ | balanceId, spaceId:partId, src->dst | status | +--------------------------------------------------------------+-------------------+ | \"[1614237867, 11:1, storaged1:9779->storaged3:9779]\" | \"SUCCEEDED\" | +--------------------------------------------------------------+-------------------+ | \"[1614237867, 11:1, storaged2:9779->storaged4:9779]\" | \"SUCCEEDED\" | +--------------------------------------------------------------+-------------------+ | \"[1614237867, 11:2, storaged1:9779->storaged3:9779]\" | \"SUCCEEDED\" | +--------------------------------------------------------------+-------------------+ ... +--------------------------------------------------------------+-------------------+ | \"Total:22, Succeeded:22, Failed:0, In Progress:0, Invalid:0\" | 100 | +--------------------------------------------------------------+-------------------+ Got 23 rows (time spent 916/1528 us) When all the subtasks succeed, the load balancing process finishes. Run SHOW HOSTS again to make sure the partition distribution is balanced. NOTE: BALANCE DATA does not balance the leader distribution. nebula> SHOW HOSTS; +-------------+------+----------+--------------+----------------------+------------------------+ | Host | Port | Status | Leader count | Leader distribution | Partition distribution | +-------------+------+----------+--------------+----------------------+------------------------+ | \"storaged0\" | 9779 | \"ONLINE\" | 4 | \"nba:4\" | \"nba:9\" | +-------------+------+----------+--------------+----------------------+------------------------+ | \"storaged1\" | 9779 | \"ONLINE\" | 8 | \"nba:8\" | \"nba:9\" | +-------------+------+----------+--------------+----------------------+------------------------+ | \"storaged2\" | 9779 | \"ONLINE\" | 3 | \"nba:3\" | \"nba:9\" | +-------------+------+----------+--------------+----------------------+------------------------+ | \"storaged3\" | 9779 | \"ONLINE\" | 0 | \"No valid partition\" | \"nba:9\" | +-------------+------+----------+--------------+----------------------+------------------------+ | \"storaged4\" | 9779 | \"ONLINE\" | 0 | \"No valid partition\" | \"nba:9\" | +-------------+------+----------+--------------+----------------------+------------------------+ | \"Total\" | | | 15 | \"nba:15\" | \"nba:45\" | +-------------+------+----------+--------------+----------------------+------------------------+ Got 6 rows (time spent 849/1420 us) If any subtask fails, run BALANCE DATA again to restart the balancing. If redoing load balancing does not solve the problem, ask for help in the Nebula Graph community . Stop data balancing \u00b6 To stop a balance task, run BALANCE DATA STOP . If no balance task is running, an error is returned. If a balance task is running, the task ID is returned. BALANCE DATA STOP does not stop the running subtasks but cancels all follow-up subtasks. The running subtasks continue. To check the status of the stopped balance task, run BALANCE DATA <balance_id> . Once all the subtasks are finished or stopped, you can run BALANCE DATA again to balance the partitions again. If any subtask of the preceding balance task failed, Nebula Graph restarts the preceding balance task. If no subtask of the preceding balance task failed, Nebula Graph starts a new balance task. Remove storage servers \u00b6 To remove specific storage servers and scale in the Storage Service, use the BALANCE DATA REMOVE <host_list> syntax. For example, to remove the following storage servers: Server name IP Port storage3 192.168.0.8 19779 storage4 192.168.0.9 19779 Run the following statement: BALANCE DATA REMOVE 192.168.0.8:19779,192.168.0.9:19779; Nebula Graph will start a balance task, migrate the storage partitions in storage3 and storage4, and then remove them from the cluster. Balance leader distribution \u00b6 BALANCE DATA only balances the partition distribution. If the raft leader distribution is not balanced, some of the leaders may overload. To load balance the raft leaders, run BALANCE LEADER . nebula> BALANCE LEADER; Execution succeeded (time spent 7576/8657 us) Run SHOW HOSTS to check the balance result. nebula> SHOW HOSTS; +-------------+------+----------+--------------+----------------------+------------------------+ | Host | Port | Status | Leader count | Leader distribution | Partition distribution | +-------------+------+----------+--------------+----------------------+------------------------+ | \"storaged0\" | 9779 | \"ONLINE\" | 3 | \"nba:3\" | \"nba:9\" | +-------------+------+----------+--------------+----------------------+------------------------+ | \"storaged1\" | 9779 | \"ONLINE\" | 3 | \"nba:3\" | \"nba:9\" | +-------------+------+----------+--------------+----------------------+------------------------+ | \"storaged2\" | 9779 | \"ONLINE\" | 3 | \"nba:3\" | \"nba:9\" | +-------------+------+----------+--------------+----------------------+------------------------+ | \"storaged3\" | 9779 | \"ONLINE\" | 3 | \"nba:3\" | \"nba:9\" | +-------------+------+----------+--------------+----------------------+------------------------+ | \"storaged4\" | 9779 | \"ONLINE\" | 3 | \"nba:3\" | \"nba:9\" | +-------------+------+----------+--------------+----------------------+------------------------+ | \"Total\" | | | 15 | \"nba:15\" | \"nba:45\" | +-------------+------+----------+--------------+----------------------+------------------------+","title":"Storage load balance"},{"location":"8.service-tuning/load-balance/#storage_load_balance","text":"To scale in or scale out the Nebula Graph Storage Service, use the BALANCE statements.","title":"Storage load balance"},{"location":"8.service-tuning/load-balance/#balance_partition_distribution","text":"BALANCE DATA starts a task to equally distribute the storage partitions in a Nebula Graph cluster. A group of subtasks will be created and implemented to migrate data and balance the partition distribution. DON'T: DON'T stop any machine in the cluster or change its IP address until all the subtasks finish. Otherwise, the follow-up subtasks fail. Take scaling out Nebula Graph for an example. After you add new storage hosts into the cluster, no partition is deployed on the new hosts. You can run SHOW HOSTS to check the partition distribution. nebual> SHOW HOSTS; +-------------+------+----------+--------------+----------------------+------------------------+ | Host | Port | Status | Leader count | Leader distribution | Partition distribution | +-------------+------+----------+--------------+----------------------+------------------------+ | \"storaged0\" | 9779 | \"ONLINE\" | 4 | \"nba:4\" | \"nba:15\" | +-------------+------+----------+--------------+----------------------+------------------------+ | \"storaged1\" | 9779 | \"ONLINE\" | 8 | \"nba:8\" | \"nba:15\" | +-------------+------+----------+--------------+----------------------+------------------------+ | \"storaged2\" | 9779 | \"ONLINE\" | 3 | \"nba:3\" | \"nba:15\" | +-------------+------+----------+--------------+----------------------+------------------------+ | \"storaged3\" | 9779 | \"ONLINE\" | 0 | \"No valid partition\" | \"No valid partition\" | +-------------+------+----------+--------------+----------------------+------------------------+ | \"storaged4\" | 9779 | \"ONLINE\" | 0 | \"No valid partition\" | \"No valid partition\" | +-------------+------+----------+--------------+----------------------+------------------------+ | \"Total\" | | | 15 | \"nba:15\" | \"nba:45\" | +-------------+------+----------+--------------+----------------------+------------------------+ Got 6 rows (time spent 1002/1780 us) Run BALANCE DATA to start balancing the storage partitions. If the partitions are already balanced, BALANCE DATA fails. nebula> BALANCE DATA; +------------+ | ID | +------------+ | 1614237867 | +------------+ Got 1 rows (time spent 3783/4533 us) A BALANCE task ID is returned after running BALANCE DATA . Run BALANCE DATA <balance_id> to check the status of the BALANCE task. nebula> BALANCE DATA 1614237867; +--------------------------------------------------------------+-------------------+ | balanceId, spaceId:partId, src->dst | status | +--------------------------------------------------------------+-------------------+ | \"[1614237867, 11:1, storaged1:9779->storaged3:9779]\" | \"SUCCEEDED\" | +--------------------------------------------------------------+-------------------+ | \"[1614237867, 11:1, storaged2:9779->storaged4:9779]\" | \"SUCCEEDED\" | +--------------------------------------------------------------+-------------------+ | \"[1614237867, 11:2, storaged1:9779->storaged3:9779]\" | \"SUCCEEDED\" | +--------------------------------------------------------------+-------------------+ ... +--------------------------------------------------------------+-------------------+ | \"Total:22, Succeeded:22, Failed:0, In Progress:0, Invalid:0\" | 100 | +--------------------------------------------------------------+-------------------+ Got 23 rows (time spent 916/1528 us) When all the subtasks succeed, the load balancing process finishes. Run SHOW HOSTS again to make sure the partition distribution is balanced. NOTE: BALANCE DATA does not balance the leader distribution. nebula> SHOW HOSTS; +-------------+------+----------+--------------+----------------------+------------------------+ | Host | Port | Status | Leader count | Leader distribution | Partition distribution | +-------------+------+----------+--------------+----------------------+------------------------+ | \"storaged0\" | 9779 | \"ONLINE\" | 4 | \"nba:4\" | \"nba:9\" | +-------------+------+----------+--------------+----------------------+------------------------+ | \"storaged1\" | 9779 | \"ONLINE\" | 8 | \"nba:8\" | \"nba:9\" | +-------------+------+----------+--------------+----------------------+------------------------+ | \"storaged2\" | 9779 | \"ONLINE\" | 3 | \"nba:3\" | \"nba:9\" | +-------------+------+----------+--------------+----------------------+------------------------+ | \"storaged3\" | 9779 | \"ONLINE\" | 0 | \"No valid partition\" | \"nba:9\" | +-------------+------+----------+--------------+----------------------+------------------------+ | \"storaged4\" | 9779 | \"ONLINE\" | 0 | \"No valid partition\" | \"nba:9\" | +-------------+------+----------+--------------+----------------------+------------------------+ | \"Total\" | | | 15 | \"nba:15\" | \"nba:45\" | +-------------+------+----------+--------------+----------------------+------------------------+ Got 6 rows (time spent 849/1420 us) If any subtask fails, run BALANCE DATA again to restart the balancing. If redoing load balancing does not solve the problem, ask for help in the Nebula Graph community .","title":"Balance partition distribution"},{"location":"8.service-tuning/load-balance/#stop_data_balancing","text":"To stop a balance task, run BALANCE DATA STOP . If no balance task is running, an error is returned. If a balance task is running, the task ID is returned. BALANCE DATA STOP does not stop the running subtasks but cancels all follow-up subtasks. The running subtasks continue. To check the status of the stopped balance task, run BALANCE DATA <balance_id> . Once all the subtasks are finished or stopped, you can run BALANCE DATA again to balance the partitions again. If any subtask of the preceding balance task failed, Nebula Graph restarts the preceding balance task. If no subtask of the preceding balance task failed, Nebula Graph starts a new balance task.","title":"Stop data balancing"},{"location":"8.service-tuning/load-balance/#remove_storage_servers","text":"To remove specific storage servers and scale in the Storage Service, use the BALANCE DATA REMOVE <host_list> syntax. For example, to remove the following storage servers: Server name IP Port storage3 192.168.0.8 19779 storage4 192.168.0.9 19779 Run the following statement: BALANCE DATA REMOVE 192.168.0.8:19779,192.168.0.9:19779; Nebula Graph will start a balance task, migrate the storage partitions in storage3 and storage4, and then remove them from the cluster.","title":"Remove storage servers"},{"location":"8.service-tuning/load-balance/#balance_leader_distribution","text":"BALANCE DATA only balances the partition distribution. If the raft leader distribution is not balanced, some of the leaders may overload. To load balance the raft leaders, run BALANCE LEADER . nebula> BALANCE LEADER; Execution succeeded (time spent 7576/8657 us) Run SHOW HOSTS to check the balance result. nebula> SHOW HOSTS; +-------------+------+----------+--------------+----------------------+------------------------+ | Host | Port | Status | Leader count | Leader distribution | Partition distribution | +-------------+------+----------+--------------+----------------------+------------------------+ | \"storaged0\" | 9779 | \"ONLINE\" | 3 | \"nba:3\" | \"nba:9\" | +-------------+------+----------+--------------+----------------------+------------------------+ | \"storaged1\" | 9779 | \"ONLINE\" | 3 | \"nba:3\" | \"nba:9\" | +-------------+------+----------+--------------+----------------------+------------------------+ | \"storaged2\" | 9779 | \"ONLINE\" | 3 | \"nba:3\" | \"nba:9\" | +-------------+------+----------+--------------+----------------------+------------------------+ | \"storaged3\" | 9779 | \"ONLINE\" | 3 | \"nba:3\" | \"nba:9\" | +-------------+------+----------+--------------+----------------------+------------------------+ | \"storaged4\" | 9779 | \"ONLINE\" | 3 | \"nba:3\" | \"nba:9\" | +-------------+------+----------+--------------+----------------------+------------------------+ | \"Total\" | | | 15 | \"nba:15\" | \"nba:45\" | +-------------+------+----------+--------------+----------------------+------------------------+","title":"Balance leader distribution"},{"location":"nebula-exchange/ex-ug-compile/","text":"Compile Exchange \u00b6 To compile Exchange, follow these steps: Run these commands to install Nebula Java Client v2.x. $ git clone https://github.com/vesoft-inc/nebula-java.git $ cd nebula-java $ mvn clean install -Dmaven.test.skip = true -Dgpg.skip -Dmaven.javadoc.skip = true NOTE : After the installation, you can see the /com/vesoft/client/2.0.0-beta/client-2.0.0-beta.jar in your local Maven repository. Run these commands to package Nebula Exchange v2.x. $ git clone https://github.com/vesoft-inc/nebula-spark-utils.git $ cd nebula-spark-utils/nebula-exchange $ mvn clean package -Dmaven.test.skip = true -Dgpg.skip -Dmaven.javadoc.skip = true After the compiling, you can see the structure of the Exchange directory as follows. . \u251c\u2500\u2500 README-CN.md \u251c\u2500\u2500 README.md \u251c\u2500\u2500 pom.xml \u251c\u2500\u2500 src \u2502 \u251c\u2500\u2500 main \u2502 \u2514\u2500\u2500 test \u2514\u2500\u2500 target \u251c\u2500\u2500 classes \u251c\u2500\u2500 classes.timestamp \u251c\u2500\u2500 maven-archiver \u251c\u2500\u2500 nebula-exchange-2.x.y-javadoc.jar \u251c\u2500\u2500 nebula-exchange-2.x.y-sources.jar \u251c\u2500\u2500 nebula-exchange-2.x.y.jar \u251c\u2500\u2500 original-nebula-exchange-2.x.y.jar \u2514\u2500\u2500 site In the target directory, you can see the exchange-2.x.y.jar file. NOTE : The version of the JAR file depends on the releases of Nebula Java Client. You can find the latest versions on the Releases page of the nebula-spark-utils repository . To import data, you can refer to the example configuration in the target/classes/application.conf , target/classes/server_application.conf , and target/classes/stream_application.conf files.","title":"Compile Exchange"},{"location":"nebula-exchange/ex-ug-compile/#compile_exchange","text":"To compile Exchange, follow these steps: Run these commands to install Nebula Java Client v2.x. $ git clone https://github.com/vesoft-inc/nebula-java.git $ cd nebula-java $ mvn clean install -Dmaven.test.skip = true -Dgpg.skip -Dmaven.javadoc.skip = true NOTE : After the installation, you can see the /com/vesoft/client/2.0.0-beta/client-2.0.0-beta.jar in your local Maven repository. Run these commands to package Nebula Exchange v2.x. $ git clone https://github.com/vesoft-inc/nebula-spark-utils.git $ cd nebula-spark-utils/nebula-exchange $ mvn clean package -Dmaven.test.skip = true -Dgpg.skip -Dmaven.javadoc.skip = true After the compiling, you can see the structure of the Exchange directory as follows. . \u251c\u2500\u2500 README-CN.md \u251c\u2500\u2500 README.md \u251c\u2500\u2500 pom.xml \u251c\u2500\u2500 src \u2502 \u251c\u2500\u2500 main \u2502 \u2514\u2500\u2500 test \u2514\u2500\u2500 target \u251c\u2500\u2500 classes \u251c\u2500\u2500 classes.timestamp \u251c\u2500\u2500 maven-archiver \u251c\u2500\u2500 nebula-exchange-2.x.y-javadoc.jar \u251c\u2500\u2500 nebula-exchange-2.x.y-sources.jar \u251c\u2500\u2500 nebula-exchange-2.x.y.jar \u251c\u2500\u2500 original-nebula-exchange-2.x.y.jar \u2514\u2500\u2500 site In the target directory, you can see the exchange-2.x.y.jar file. NOTE : The version of the JAR file depends on the releases of Nebula Java Client. You can find the latest versions on the Releases page of the nebula-spark-utils repository . To import data, you can refer to the example configuration in the target/classes/application.conf , target/classes/server_application.conf , and target/classes/stream_application.conf files.","title":"Compile Exchange"},{"location":"nebula-exchange/ex-ug-toc/","text":"Nebula Exchange v2.x User Guide \u00b6 About Nebula Exchange What is Nebula Exchange Limitations Glossary FAQ Compile Exchange Use Exchange Import data from CSV files Import data from JSON files Import data from HIVE [Import data from Apache Parquet][DOC_TO_DO] [Import data from Apache ORC][DOC_TO_DO] [Import data from Neo4j][DOC_TO_DO] [Import data from HBase][DOC_TO_DO] [Import data from MySQL][DOC_TO_DO] [Import data from Kafka][DOC_TO_DO] [Import data from Pulsar][DOC_TO_DO] [Import SST files][DOC_TO_DO] Parameter reference Spark related parameters Nebula Graph related parameters Import command parameters","title":"Nebula Exchange v2.x User Guide"},{"location":"nebula-exchange/ex-ug-toc/#nebula_exchange_v2x_user_guide","text":"About Nebula Exchange What is Nebula Exchange Limitations Glossary FAQ Compile Exchange Use Exchange Import data from CSV files Import data from JSON files Import data from HIVE [Import data from Apache Parquet][DOC_TO_DO] [Import data from Apache ORC][DOC_TO_DO] [Import data from Neo4j][DOC_TO_DO] [Import data from HBase][DOC_TO_DO] [Import data from MySQL][DOC_TO_DO] [Import data from Kafka][DOC_TO_DO] [Import data from Pulsar][DOC_TO_DO] [Import SST files][DOC_TO_DO] Parameter reference Spark related parameters Nebula Graph related parameters Import command parameters","title":"Nebula Exchange v2.x User Guide"},{"location":"nebula-exchange/about-exchange/ex-ug-faq/","text":"FAQ \u00b6 What version of Nebula Graph does Exchange v2.x support? Read Limitations to get the latest information about supported Nebula Graph versions. What are the differences between Exchange v1.x and Exchange v2.x? Compared with Exchange v1.x, Exchange v2.x has these new features: Importing vertex data with String type IDs. Importing data of the Null, Date, DateTime, and Time types. Importing data from other Hive sources besides Hive on Spark. Recording and retrying the INSERT statement after failures during data import. For more information, see Exchange README . What is the difference between Exchange and Spark Writer? Both are Spark applications, and Exchange is based on Spark Writer. Both of them are designed for the migration of data into a Nebula Graph cluster in a distributed environment, but the later maintenance work will focus on Exchange. Compared with Spark Writer, Exchange has the following improvements: Supporting more data sources, such as MySQL, Neo4j, HIVE, HBase, Kafka, and Pulsar. Some problems with Spark Writer were fixed. For example, by default Spark reads source data from HDFS as strings, which is probably different from your graph schema defined in Nebula Graph. Exchange supports automatically matching and converting data types. With it, when a non-string data type is defined in Nebula Graph, Exchange converts the strings into data of the required data type.","title":"FAQ"},{"location":"nebula-exchange/about-exchange/ex-ug-faq/#faq","text":"What version of Nebula Graph does Exchange v2.x support? Read Limitations to get the latest information about supported Nebula Graph versions. What are the differences between Exchange v1.x and Exchange v2.x? Compared with Exchange v1.x, Exchange v2.x has these new features: Importing vertex data with String type IDs. Importing data of the Null, Date, DateTime, and Time types. Importing data from other Hive sources besides Hive on Spark. Recording and retrying the INSERT statement after failures during data import. For more information, see Exchange README . What is the difference between Exchange and Spark Writer? Both are Spark applications, and Exchange is based on Spark Writer. Both of them are designed for the migration of data into a Nebula Graph cluster in a distributed environment, but the later maintenance work will focus on Exchange. Compared with Spark Writer, Exchange has the following improvements: Supporting more data sources, such as MySQL, Neo4j, HIVE, HBase, Kafka, and Pulsar. Some problems with Spark Writer were fixed. For example, by default Spark reads source data from HDFS as strings, which is probably different from your graph schema defined in Nebula Graph. Exchange supports automatically matching and converting data types. With it, when a non-string data type is defined in Nebula Graph, Exchange converts the strings into data of the required data type.","title":"FAQ"},{"location":"nebula-exchange/about-exchange/ex-ug-limitations/","text":"Limitations \u00b6 This article introduces the limitations of Exchange v2.x. Supported Nebula Graph versions \u00b6 Exchange v2.x supports Nebula Graph v2.x only. If you are using Nebula Graph v1.x, please use Nebula Exchange v1.x . Supported operation systems \u00b6 You can use Exchange v2.x in these operation systems: CentOS 7 macOS NOTE : Importing SST files with Exchange v2.x is supported in Linux only. Software dependencies \u00b6 To make sure that Exchange v2.x works properly, make sure that these software applications are installed in your machine: Apache Spark: 2.3.0 or later versions Java: 1.8 Scala: 2.10.7, 2.11.12, or 2.12.10 In these scenarios, Hadoop Distributed File System (HDFS) must be deployed: Importing data from HDFS to Nebula Graph Importing SST files into Nebula Graph","title":"Limitations"},{"location":"nebula-exchange/about-exchange/ex-ug-limitations/#limitations","text":"This article introduces the limitations of Exchange v2.x.","title":"Limitations"},{"location":"nebula-exchange/about-exchange/ex-ug-limitations/#supported_nebula_graph_versions","text":"Exchange v2.x supports Nebula Graph v2.x only. If you are using Nebula Graph v1.x, please use Nebula Exchange v1.x .","title":"Supported Nebula Graph versions"},{"location":"nebula-exchange/about-exchange/ex-ug-limitations/#supported_operation_systems","text":"You can use Exchange v2.x in these operation systems: CentOS 7 macOS NOTE : Importing SST files with Exchange v2.x is supported in Linux only.","title":"Supported operation systems"},{"location":"nebula-exchange/about-exchange/ex-ug-limitations/#software_dependencies","text":"To make sure that Exchange v2.x works properly, make sure that these software applications are installed in your machine: Apache Spark: 2.3.0 or later versions Java: 1.8 Scala: 2.10.7, 2.11.12, or 2.12.10 In these scenarios, Hadoop Distributed File System (HDFS) must be deployed: Importing data from HDFS to Nebula Graph Importing SST files into Nebula Graph","title":"Software dependencies"},{"location":"nebula-exchange/about-exchange/ex-ug-terms/","text":"Glossary \u00b6 This article gives explanations of some necessary terminologies in this user guide. Nebula Exchange: Referred to as Exchange v2.x or Exchange in this user guide. It is a Spark application based on Apache Spark\u2122 for batch or stream processing data migration. It supports converting data from different sources into vertex and edge data that can be recognized by Nebula Graph, and then concurrently importing data into Nebula Graph. Apache Spark\u2122: A fast and general computing engine designed for large-scale data processing. It is an open-source project of Apache Software Foundation. Driver Program: Referred to as driver in this user guide. It is a program that runs the main function of an application and creates a new SparkContext instance.","title":"Glossary"},{"location":"nebula-exchange/about-exchange/ex-ug-terms/#glossary","text":"This article gives explanations of some necessary terminologies in this user guide. Nebula Exchange: Referred to as Exchange v2.x or Exchange in this user guide. It is a Spark application based on Apache Spark\u2122 for batch or stream processing data migration. It supports converting data from different sources into vertex and edge data that can be recognized by Nebula Graph, and then concurrently importing data into Nebula Graph. Apache Spark\u2122: A fast and general computing engine designed for large-scale data processing. It is an open-source project of Apache Software Foundation. Driver Program: Referred to as driver in this user guide. It is a program that runs the main function of an application and creates a new SparkContext instance.","title":"Glossary"},{"location":"nebula-exchange/about-exchange/ex-ug-what-is-exchange/","text":"What is Nebula Exchange \u00b6 Nebula Exchange v2.x (Exchange v2.x or Exchange in short) is an Apache Spark\u2122 application. It can be used to migrate data from a cluster in a distributed environment to a Nebula Graph v2.x cluster. It supports processing different formats of batch data and streaming data. Exchange is composed of Reader, Processor, and Writer. Reader reads data of different sources and creates DataFrame. Processor traverses every row of the DataFrame and obtains the values for each column according to the mapping of the fields in the configuration file. And then after the specified rows of data to be batch processed are traversed, Writer writes the obtained data into Nebula Graph concurrently. This figure shows how the data is transformed and transferred in Exchange. Scenarios \u00b6 You can use Exchange in these scenarios: Converting streaming data from Kafka or Pulsar platforms to vertex or edge data of property graphs and importing them into Nebula Graph. For example, log files, online shopping data, in-game player activities, social networking information, financial trading services, geospatial services, or telemetry data from connected devices or instruments in the data center. Converting batch data (such as data in a certain period of time) from a relational database (such as MySQL) or a distributed file system (such as HDFS) into vertex or edge data of property graphs, and importing them into Nebula Graph. Converting a large amount of data into SST files and then importing them into Nebula Graph. Features \u00b6 Exchange has these features: Adaptable: Exchange supports importing data from different sources into Nebula Graph, which is convenient for you to migrate data. SST files supported: Exchange supports converting data from different sources into SST files for data import. NOTE : Importing SST files with Exchange v2.x is supported in Linux only. Resuming broken transfer: Exchange supports resuming an interrupted transfer from a broken point during the data import process, which saves your time and improves efficiency. NOTE : Exchange v2.x supports resuming broken transfer for Neo4j only. Asynchronous: Exchange enables you to set an insertion statement for the source and sends it to the Graph Service of Nebula Graph for data insertion. Flexible: Exchange supports importing multiple types of vertices and edges of different sources or formats simultaneously. Statistics: Exchange uses the accumulator in Apache Spark\u2122 to count the successes and failures during the insertion process. Easy to use and user-friendly: Exchange supports HOCON (Human-Optimized Config Object Notation) configuration file format, which is object-oriented, and easy to understand and operate. Supported data sources \u00b6 You can use Exchange v2.x to convert data of these sources into vertex and edge data and then import them to Nebula Graph v2.x: Data of different formats stored on HDFS, including: Apache Parquet Apache ORC JSON CSV Apache HBase\u2122 Data warehouses: HIVE Graph databases: Neo4j 2.4.5-M1. Resuming transfer from a broken point is supported for Neo4j data only. Relational databases: MySQL Stream processing platforms: Apache Kafka\u00ae Messaging and streaming platforms: Apache Pulsar 2.4.5","title":"What is Nebula Exchange"},{"location":"nebula-exchange/about-exchange/ex-ug-what-is-exchange/#what_is_nebula_exchange","text":"Nebula Exchange v2.x (Exchange v2.x or Exchange in short) is an Apache Spark\u2122 application. It can be used to migrate data from a cluster in a distributed environment to a Nebula Graph v2.x cluster. It supports processing different formats of batch data and streaming data. Exchange is composed of Reader, Processor, and Writer. Reader reads data of different sources and creates DataFrame. Processor traverses every row of the DataFrame and obtains the values for each column according to the mapping of the fields in the configuration file. And then after the specified rows of data to be batch processed are traversed, Writer writes the obtained data into Nebula Graph concurrently. This figure shows how the data is transformed and transferred in Exchange.","title":"What is Nebula Exchange"},{"location":"nebula-exchange/about-exchange/ex-ug-what-is-exchange/#scenarios","text":"You can use Exchange in these scenarios: Converting streaming data from Kafka or Pulsar platforms to vertex or edge data of property graphs and importing them into Nebula Graph. For example, log files, online shopping data, in-game player activities, social networking information, financial trading services, geospatial services, or telemetry data from connected devices or instruments in the data center. Converting batch data (such as data in a certain period of time) from a relational database (such as MySQL) or a distributed file system (such as HDFS) into vertex or edge data of property graphs, and importing them into Nebula Graph. Converting a large amount of data into SST files and then importing them into Nebula Graph.","title":"Scenarios"},{"location":"nebula-exchange/about-exchange/ex-ug-what-is-exchange/#features","text":"Exchange has these features: Adaptable: Exchange supports importing data from different sources into Nebula Graph, which is convenient for you to migrate data. SST files supported: Exchange supports converting data from different sources into SST files for data import. NOTE : Importing SST files with Exchange v2.x is supported in Linux only. Resuming broken transfer: Exchange supports resuming an interrupted transfer from a broken point during the data import process, which saves your time and improves efficiency. NOTE : Exchange v2.x supports resuming broken transfer for Neo4j only. Asynchronous: Exchange enables you to set an insertion statement for the source and sends it to the Graph Service of Nebula Graph for data insertion. Flexible: Exchange supports importing multiple types of vertices and edges of different sources or formats simultaneously. Statistics: Exchange uses the accumulator in Apache Spark\u2122 to count the successes and failures during the insertion process. Easy to use and user-friendly: Exchange supports HOCON (Human-Optimized Config Object Notation) configuration file format, which is object-oriented, and easy to understand and operate.","title":"Features"},{"location":"nebula-exchange/about-exchange/ex-ug-what-is-exchange/#supported_data_sources","text":"You can use Exchange v2.x to convert data of these sources into vertex and edge data and then import them to Nebula Graph v2.x: Data of different formats stored on HDFS, including: Apache Parquet Apache ORC JSON CSV Apache HBase\u2122 Data warehouses: HIVE Graph databases: Neo4j 2.4.5-M1. Resuming transfer from a broken point is supported for Neo4j data only. Relational databases: MySQL Stream processing platforms: Apache Kafka\u00ae Messaging and streaming platforms: Apache Pulsar 2.4.5","title":"Supported data sources"},{"location":"nebula-exchange/parameter-reference/ex-ug-para-import-command/","text":"Import command parameters \u00b6 When the configuration file is ready, replace master-node-url and exchange-2.x.y.jar in this command and run it to import the data from the specified source into Nebula Graph. $SPARK_HOME /bin/spark-submit --master \"master-node-url\" --class com.vesoft.nebula.exchange.Exchange target/exchange-2.x.y.jar -c /path/to/conf/application.conf This table lists all the parameters in the preceding command. Parameters Required? Default Description --master Yes None Specifies the URL of the Master node of the specified Spark cluster. For more information, see master-urls in Spark Documentation \u3002 --class Yes None Specifies the entry point of Exchange. -c / --config Yes None Specifies the path of the Exchange configuration file. -h / --hive No false If you want to import data from HIVE, add this parameter. -D / --dry No false Before data import, add this parameter to do a check of the format of the configuration file, but not the configuration of tags and edges . Do not use this parameter when you import data.","title":"Import command parameters"},{"location":"nebula-exchange/parameter-reference/ex-ug-para-import-command/#import_command_parameters","text":"When the configuration file is ready, replace master-node-url and exchange-2.x.y.jar in this command and run it to import the data from the specified source into Nebula Graph. $SPARK_HOME /bin/spark-submit --master \"master-node-url\" --class com.vesoft.nebula.exchange.Exchange target/exchange-2.x.y.jar -c /path/to/conf/application.conf This table lists all the parameters in the preceding command. Parameters Required? Default Description --master Yes None Specifies the URL of the Master node of the specified Spark cluster. For more information, see master-urls in Spark Documentation \u3002 --class Yes None Specifies the entry point of Exchange. -c / --config Yes None Specifies the path of the Exchange configuration file. -h / --hive No false If you want to import data from HIVE, add this parameter. -D / --dry No false Before data import, add this parameter to do a check of the format of the configuration file, but not the configuration of tags and edges . Do not use this parameter when you import data.","title":"Import command parameters"},{"location":"nebula-exchange/parameter-reference/ex-ug-paras-nebulagraph/","text":"Nebula Graph related parameters \u00b6 To import data, you must set parameters for Nebula Graph. This table lists all the Nebula Graph related parameters. For more information, see the examples . Parameters Default Data Type Required? Description nebula.address.graph None list[string] Yes Specifies the addresses and ports used by the Graph Service of Nebula Graph. Multiple addresses must be separated with commas. In the format of \"ip1:port\",\"ip2:port\",\"ip3:port\" . nebula.address.meta None list[string] Yes Specifies the addresses and ports used by the Meta Service of Nebula Graph. Multiple addresses must be separated with commas. In the format of \"ip1:port\",\"ip2:port\",\"ip3:port\" . nebula.user user string Yes Specifies an account of Nebula Graph. The default value is user . If authentication is enabled in Nebula Graph: - If no account is created, use root . - If a specified account is created and given the write permission to a graph space, you can use this account. nebula.pswd password string Yes Specifies the password of the specified account. The default password for the user account is password . If authentication is enabled in Nebula Graph: - For the root account, use nebula . - For another account, use the specified password. nebula.space None string Yes Specifies the name of the graph space to import data. nebula.connection.timeout 3000 int No Specifies the period of timeout for Thrift connection. Unit: ms. nebula.connection.retry 3 int No Specifies the number of retries for Thrift connection. nebula.execution.retry 3 int No Specifies the number of execution retries of an nGQL statements nebula.error.max 32 int No Specifies the maximum number of failures during the import process. When the specified number of failures occur, the submitted Spark job stops automatically. nebula.error.output None string Yes Specifies a logging directory on the Nebula Graph cluster for the error message.","title":"Nebula Graph related parameters"},{"location":"nebula-exchange/parameter-reference/ex-ug-paras-nebulagraph/#nebula_graph_related_parameters","text":"To import data, you must set parameters for Nebula Graph. This table lists all the Nebula Graph related parameters. For more information, see the examples . Parameters Default Data Type Required? Description nebula.address.graph None list[string] Yes Specifies the addresses and ports used by the Graph Service of Nebula Graph. Multiple addresses must be separated with commas. In the format of \"ip1:port\",\"ip2:port\",\"ip3:port\" . nebula.address.meta None list[string] Yes Specifies the addresses and ports used by the Meta Service of Nebula Graph. Multiple addresses must be separated with commas. In the format of \"ip1:port\",\"ip2:port\",\"ip3:port\" . nebula.user user string Yes Specifies an account of Nebula Graph. The default value is user . If authentication is enabled in Nebula Graph: - If no account is created, use root . - If a specified account is created and given the write permission to a graph space, you can use this account. nebula.pswd password string Yes Specifies the password of the specified account. The default password for the user account is password . If authentication is enabled in Nebula Graph: - For the root account, use nebula . - For another account, use the specified password. nebula.space None string Yes Specifies the name of the graph space to import data. nebula.connection.timeout 3000 int No Specifies the period of timeout for Thrift connection. Unit: ms. nebula.connection.retry 3 int No Specifies the number of retries for Thrift connection. nebula.execution.retry 3 int No Specifies the number of execution retries of an nGQL statements nebula.error.max 32 int No Specifies the maximum number of failures during the import process. When the specified number of failures occur, the submitted Spark job stops automatically. nebula.error.output None string Yes Specifies a logging directory on the Nebula Graph cluster for the error message.","title":"Nebula Graph related parameters"},{"location":"nebula-exchange/parameter-reference/ex-ug-paras-spark/","text":"Spark related parameters \u00b6 To import data, you must set parameters for Spark. This table lists some generally-used parameters. For more Spark-related parameters, see Apache Spark documentation . For more information, see the examples . Parameters Default Data type Required? Description spark.app.name Nebula Exchange 2.0 string No Specifies the name of the Spark Driver Program. spark.driver.cores 1 int No Specifies the number of cores to use for the driver process, only in cluster mode. spark.driver.maxResultSize 1G string No Specifies the limit of the total size of serialized results of all partitions for each Spark action (e.g. collect) in bytes. Should be at least 1M, or 0 for unlimited. spark.cores.max None int No When the driver program runs on a standalone deployed cluster or a Mesos cluster in \"coarse-grained\" sharing mode, the maximum amount of CPU cores to request for the application from across the cluster (not from each machine). If not set, the default will be spark.deploy.defaultCores on the standalone cluster manager of Spark, or infinite (all available cores) on Mesos.","title":"Spark related parameters"},{"location":"nebula-exchange/parameter-reference/ex-ug-paras-spark/#spark_related_parameters","text":"To import data, you must set parameters for Spark. This table lists some generally-used parameters. For more Spark-related parameters, see Apache Spark documentation . For more information, see the examples . Parameters Default Data type Required? Description spark.app.name Nebula Exchange 2.0 string No Specifies the name of the Spark Driver Program. spark.driver.cores 1 int No Specifies the number of cores to use for the driver process, only in cluster mode. spark.driver.maxResultSize 1G string No Specifies the limit of the total size of serialized results of all partitions for each Spark action (e.g. collect) in bytes. Should be at least 1M, or 0 for unlimited. spark.cores.max None int No When the driver program runs on a standalone deployed cluster or a Mesos cluster in \"coarse-grained\" sharing mode, the maximum amount of CPU cores to request for the application from across the cluster (not from each machine). If not set, the default will be spark.deploy.defaultCores on the standalone cluster manager of Spark, or infinite (all available cores) on Mesos.","title":"Spark related parameters"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-csv/","text":"Import data from CSV files \u00b6 This article uses an example to show how to use Exchange to import data from CSV files stored on HDFS into Nebula Graph. If you want to import data from local CSV files into Nebula Graph v2.x, see Nebula Importer . Dataset \u00b6 In this article, the Social Network: MOOC User Action Dataset provided by Stanford Network Analysis Platform (SNAP) and 97 unique course names obtained from the public network are used as the sample dataset. The dataset contains: Two vertex types ( user and course ), 7,144 vertices in total. One edge type ( action ), 411,749 edges in total. You can download the example dataset from the nebula-web-docker repository. Environment \u00b6 The practice is done in macOS. Here is the environment information: Hardware specifications: CPU: 1.7 GHz Quad-Core Intel Core i7 Memory: 16 GB Spark 2.4.7, deployed in the Standalone mode Hadoop 2.9.2, deployed in the Pseudo-Distributed mode Nebula Graph v2-nightly, deployed with Docker Compose. For more information, see Deploy Nebula Graph with Docker Compose . Prerequisites \u00b6 To import data from CSV files on HDFS with Exchange v2.x, do a check of these: Exchange v2.x is compiled. For more information, see Compile Exchange v2.x . Exchange 2.0.0 is used in this example. Spark is installed. Hadoop is installed and started. Nebula Graph is deployed and started. Get the information: IP addresses and ports of the Graph Service and the Meta Service. A Nebula Graph account with the privilege of writing data and its password. Get the necessary information for schema creation in Nebula Graph, including tags and edge types. Procedure \u00b6 Step 1. Create a schema in Nebula Graph \u00b6 Analyze the data in the CSV files and follow these steps to create a schema in Nebula Graph: Confirm the essential elements of the schema. Elements Names Properties Tag user userId string Tag course courseId int, courseName string Edge Type action actionId int, duration double, label bool, feature0 double, feature1 double, feature2 double, feature3 double In Nebula Graph, create a graph space named csv and create a schema. -- Create a graph space named csv CREATE SPACE [IF NOT EXISTS] <graph_space_name> [(partition_num = <partition_number>, replica_factor = <replica_number>, vid_type = {FIXED_STRING(<N>)) | INT64}]; -- Choose the csv graph space USE csv; -- Create the user tag CREATE TAG user(userId string); -- Create the course tag CREATE TAG course(courseId int, courseName string); -- Create the action edge type CREATE EDGE action (actionId int, duration double, label bool, feature0 double, feature1 double, feature2 double, feature3 double); For more information, see Quick Start of Nebula Graph Database . Step 2. Prepare CSV files \u00b6 Do a check of these: The CSV files are processed to meet the requirements of the schema. For more information, see Quick Start of Nebula Graph Studio . > NOTE : Exchange supports importing CSV files with or without headers. The CSV files must be stored in HDFS and get the file storage path. Step 3. Edit configuration file \u00b6 After compiling of Exchange, copy the target/classes/application.conf file and edit the configuration for CSV files. In this example, a new configuration file is named csv_ application.conf . In this file, the vertex and edge related configuration is introduced in the comments and all the items that are not used in this example are commented out. For more information about the Spark and Nebula related parameters, see Spark related parameters and Nebula Graph related parameters . { # Spark related configuration spark: { app: { name: Nebula Exchange 2.0 } driver: { cores: 1 maxResultSize: 1G } executor: { memory:1G } cores { max: 16 } } # Nebula Graph related configuration nebula: { address:{ # Specifies the IP addresses and ports of the Graph Service and the Meta Service of Nebula Graph. # If multiple servers are used, separate the addresses with commas. # Format: \"ip1:port\",\"ip2:port\",\"ip3:port\". graph:[\"127.0.0.1:9669\"] meta:[\"127.0.0.1:9559\"] } # Specifies an account that has the WriteData privilege in Nebula Graph and its password. user: user pswd: password # Specifies a graph space name space: csv connection { timeout: 3000 retry: 3 } execution { retry: 3 } error: { max: 32 output: /tmp/errors } rate: { limit: 1024 timeout: 1000 } } # Process vertices tags: [ # Sets for the course tag { # Specifies a tag name defined in Nebula Graph. name: course type: { # Specifies the data source. csv is used. source: csv # Specifies how to import vertex data into Nebula Graph: client or sst. # For more information about importing sst files, see Import SST files (doc_to_do). sink: client } # Specifies the HDFS path of the CSV file. # Enclose the path with double quotes and start the path with hdfs://. path: \"hdfs://namenode_ip:port/path/to/course.csv\" # If the CSV file has no header, use [_c0, _c1, _c2, ..., _cn] to # represent its header and to indicate columns as the source of the property values. fields: [_c0, _c1] # If the CSV file has a header, use the actual column names. # Specifies property names defined in Nebula Graph. # fields for the CSV file and nebula.fields for Nebula Graph must # have the one-to-one correspondence relationship. nebula.fields: [courseId, courseName] # Specifies a column as the source of VIDs. # The value of vertex must be one column of the CSV file. vertex: _c1 # For now, only string type VIDs are supported in Nebula Graph v2.x. # Do not use vertex.policy for mapping. # vertex: { # field: _c1, # policy: \"hash\" # } # Specifies the separator. The default value is commas. separator: \",\" # If the CSV file has a header, set header to true. # If the CSV file has no header, set header to false (default value). header: false # Specifies the maximum number of vertex data to be written into # Nebula Graph in a single batch. batch: 256 # Specifies the partition number of Spark. partition: 32 } # Sets for the user tag { name: user type: { source: csv sink: client } path: \"hdfs://namenode_ip:port/path/to/user.csv\" # fields for the CSV file and nebula.fields for Nebula Graph must # have the one-to-one correspondence relationship. fields: [userId] # Specifies property names defined in Nebula Graph. # fields for the CSV file and nebula.fields for Nebula Graph must # have the one-to-one correspondence relationship. nebula.fields: [userId] # The value of vertex.field must be one column of the CSV file. vertex: userId separator: \",\" header: true batch: 256 partition: 32 } # If more tags are necessary, refer to the preceding configuration to add more. ] # Process edges edges: [ # Sets for the action edge type { # Specifies an edge type name defined in Nebula Graph name: action type: { # Specifies the data source. csv is used. source: csv # Specifies how to import vertex data into Nebula Graph: client or sst. # For more information about importing sst files, see Import SST files (doc_to_do). sink: client } # Specifies the HDFS path of the CSV file. # Enclose the path with double quotes and start the path with hdfs://. path: \"hdfs://namenode_ip:port/path/to/actions.csv\" # If the CSV file has no header, use [_c0, _c1, _c2, ..., _cn] to # represent its header and to indicate columns as the source of the property values. fields: [_c0, _c3, _c4, _c5, _c6, _c7, _c8] # If the CSV file has a header, use the actual column names. # Specifies property names defined in Nebula Graph. # fields for the CSV file and nebula.fields for Nebula Graph must # have the one-to-one correspondence relationship. nebula.fields: [actionId, duration, feature0, feature1, feature2, feature3, label] # Specifies the columns as the source of the IDs of the source and target vertices. source: _c1 target: _c2 # For now, only string type VIDs are supported in Nebula Graph v2.x. # Do not use source.policy or target.policy for mapping. #target: { # field: _c2 # policy: \"hash\" #} # Specifies the separator. The default value is commas. separator: \",\" # If the CSV file has a header, set header to true. # If the CSV file has no header, set header to false (default value). header: false # Specifies the maximum number of vertex data to be written into # Nebula Graph in a single batch. batch: 256 # Specifies the partition number of Spark. partition: 32 } ] # If more edge types are necessary, refer to the preceding configuration to add more. } Step 4. (Optional) Verify the configuration \u00b6 After the configuration, run the import command with the -D parameter to verify the configuration file. For more information about the parameters, see Import command parameters . $SPARK_HOME /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange /path/to/nebula-exchange-2.0.0.jar -c /path/to/conf/csv_application.conf -D Step 5. Import data into Nebula Graph \u00b6 When the configuration is ready, run this command to import data from CSV files into Nebula Graph. For more information about the parameters, see Import command parameters . $SPARK_HOME /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange /path/to/nebula-exchange-2.0.0.jar -c /path/to/conf/csv_application.conf Step 6. (Optional) Verify data in Nebula Graph \u00b6 You can use a Nebula Graph client, such as Nebula Graph Studio, to verify the imported data. For example, in Nebula Graph Studio, run this statement. GO FROM \"1\" OVER action; If the queried destination vertices return, the data are imported into Nebula Graph. You can run the SHOW STATS statement to count the data. Step 7. (Optional) Create and rebuild indexes in Nebula Graph \u00b6 After the data is imported, you can create and rebuild indexes in Nebula Graph. For more information, see nGQL User Guide .","title":"Import data from CSV files"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-csv/#import_data_from_csv_files","text":"This article uses an example to show how to use Exchange to import data from CSV files stored on HDFS into Nebula Graph. If you want to import data from local CSV files into Nebula Graph v2.x, see Nebula Importer .","title":"Import data from CSV files"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-csv/#dataset","text":"In this article, the Social Network: MOOC User Action Dataset provided by Stanford Network Analysis Platform (SNAP) and 97 unique course names obtained from the public network are used as the sample dataset. The dataset contains: Two vertex types ( user and course ), 7,144 vertices in total. One edge type ( action ), 411,749 edges in total. You can download the example dataset from the nebula-web-docker repository.","title":"Dataset"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-csv/#environment","text":"The practice is done in macOS. Here is the environment information: Hardware specifications: CPU: 1.7 GHz Quad-Core Intel Core i7 Memory: 16 GB Spark 2.4.7, deployed in the Standalone mode Hadoop 2.9.2, deployed in the Pseudo-Distributed mode Nebula Graph v2-nightly, deployed with Docker Compose. For more information, see Deploy Nebula Graph with Docker Compose .","title":"Environment"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-csv/#prerequisites","text":"To import data from CSV files on HDFS with Exchange v2.x, do a check of these: Exchange v2.x is compiled. For more information, see Compile Exchange v2.x . Exchange 2.0.0 is used in this example. Spark is installed. Hadoop is installed and started. Nebula Graph is deployed and started. Get the information: IP addresses and ports of the Graph Service and the Meta Service. A Nebula Graph account with the privilege of writing data and its password. Get the necessary information for schema creation in Nebula Graph, including tags and edge types.","title":"Prerequisites"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-csv/#procedure","text":"","title":"Procedure"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-csv/#step_1_create_a_schema_in_nebula_graph","text":"Analyze the data in the CSV files and follow these steps to create a schema in Nebula Graph: Confirm the essential elements of the schema. Elements Names Properties Tag user userId string Tag course courseId int, courseName string Edge Type action actionId int, duration double, label bool, feature0 double, feature1 double, feature2 double, feature3 double In Nebula Graph, create a graph space named csv and create a schema. -- Create a graph space named csv CREATE SPACE [IF NOT EXISTS] <graph_space_name> [(partition_num = <partition_number>, replica_factor = <replica_number>, vid_type = {FIXED_STRING(<N>)) | INT64}]; -- Choose the csv graph space USE csv; -- Create the user tag CREATE TAG user(userId string); -- Create the course tag CREATE TAG course(courseId int, courseName string); -- Create the action edge type CREATE EDGE action (actionId int, duration double, label bool, feature0 double, feature1 double, feature2 double, feature3 double); For more information, see Quick Start of Nebula Graph Database .","title":"Step 1. Create a schema in Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-csv/#step_2_prepare_csv_files","text":"Do a check of these: The CSV files are processed to meet the requirements of the schema. For more information, see Quick Start of Nebula Graph Studio . > NOTE : Exchange supports importing CSV files with or without headers. The CSV files must be stored in HDFS and get the file storage path.","title":"Step 2. Prepare CSV files"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-csv/#step_3_edit_configuration_file","text":"After compiling of Exchange, copy the target/classes/application.conf file and edit the configuration for CSV files. In this example, a new configuration file is named csv_ application.conf . In this file, the vertex and edge related configuration is introduced in the comments and all the items that are not used in this example are commented out. For more information about the Spark and Nebula related parameters, see Spark related parameters and Nebula Graph related parameters . { # Spark related configuration spark: { app: { name: Nebula Exchange 2.0 } driver: { cores: 1 maxResultSize: 1G } executor: { memory:1G } cores { max: 16 } } # Nebula Graph related configuration nebula: { address:{ # Specifies the IP addresses and ports of the Graph Service and the Meta Service of Nebula Graph. # If multiple servers are used, separate the addresses with commas. # Format: \"ip1:port\",\"ip2:port\",\"ip3:port\". graph:[\"127.0.0.1:9669\"] meta:[\"127.0.0.1:9559\"] } # Specifies an account that has the WriteData privilege in Nebula Graph and its password. user: user pswd: password # Specifies a graph space name space: csv connection { timeout: 3000 retry: 3 } execution { retry: 3 } error: { max: 32 output: /tmp/errors } rate: { limit: 1024 timeout: 1000 } } # Process vertices tags: [ # Sets for the course tag { # Specifies a tag name defined in Nebula Graph. name: course type: { # Specifies the data source. csv is used. source: csv # Specifies how to import vertex data into Nebula Graph: client or sst. # For more information about importing sst files, see Import SST files (doc_to_do). sink: client } # Specifies the HDFS path of the CSV file. # Enclose the path with double quotes and start the path with hdfs://. path: \"hdfs://namenode_ip:port/path/to/course.csv\" # If the CSV file has no header, use [_c0, _c1, _c2, ..., _cn] to # represent its header and to indicate columns as the source of the property values. fields: [_c0, _c1] # If the CSV file has a header, use the actual column names. # Specifies property names defined in Nebula Graph. # fields for the CSV file and nebula.fields for Nebula Graph must # have the one-to-one correspondence relationship. nebula.fields: [courseId, courseName] # Specifies a column as the source of VIDs. # The value of vertex must be one column of the CSV file. vertex: _c1 # For now, only string type VIDs are supported in Nebula Graph v2.x. # Do not use vertex.policy for mapping. # vertex: { # field: _c1, # policy: \"hash\" # } # Specifies the separator. The default value is commas. separator: \",\" # If the CSV file has a header, set header to true. # If the CSV file has no header, set header to false (default value). header: false # Specifies the maximum number of vertex data to be written into # Nebula Graph in a single batch. batch: 256 # Specifies the partition number of Spark. partition: 32 } # Sets for the user tag { name: user type: { source: csv sink: client } path: \"hdfs://namenode_ip:port/path/to/user.csv\" # fields for the CSV file and nebula.fields for Nebula Graph must # have the one-to-one correspondence relationship. fields: [userId] # Specifies property names defined in Nebula Graph. # fields for the CSV file and nebula.fields for Nebula Graph must # have the one-to-one correspondence relationship. nebula.fields: [userId] # The value of vertex.field must be one column of the CSV file. vertex: userId separator: \",\" header: true batch: 256 partition: 32 } # If more tags are necessary, refer to the preceding configuration to add more. ] # Process edges edges: [ # Sets for the action edge type { # Specifies an edge type name defined in Nebula Graph name: action type: { # Specifies the data source. csv is used. source: csv # Specifies how to import vertex data into Nebula Graph: client or sst. # For more information about importing sst files, see Import SST files (doc_to_do). sink: client } # Specifies the HDFS path of the CSV file. # Enclose the path with double quotes and start the path with hdfs://. path: \"hdfs://namenode_ip:port/path/to/actions.csv\" # If the CSV file has no header, use [_c0, _c1, _c2, ..., _cn] to # represent its header and to indicate columns as the source of the property values. fields: [_c0, _c3, _c4, _c5, _c6, _c7, _c8] # If the CSV file has a header, use the actual column names. # Specifies property names defined in Nebula Graph. # fields for the CSV file and nebula.fields for Nebula Graph must # have the one-to-one correspondence relationship. nebula.fields: [actionId, duration, feature0, feature1, feature2, feature3, label] # Specifies the columns as the source of the IDs of the source and target vertices. source: _c1 target: _c2 # For now, only string type VIDs are supported in Nebula Graph v2.x. # Do not use source.policy or target.policy for mapping. #target: { # field: _c2 # policy: \"hash\" #} # Specifies the separator. The default value is commas. separator: \",\" # If the CSV file has a header, set header to true. # If the CSV file has no header, set header to false (default value). header: false # Specifies the maximum number of vertex data to be written into # Nebula Graph in a single batch. batch: 256 # Specifies the partition number of Spark. partition: 32 } ] # If more edge types are necessary, refer to the preceding configuration to add more. }","title":"Step 3. Edit configuration file"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-csv/#step_4_optional_verify_the_configuration","text":"After the configuration, run the import command with the -D parameter to verify the configuration file. For more information about the parameters, see Import command parameters . $SPARK_HOME /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange /path/to/nebula-exchange-2.0.0.jar -c /path/to/conf/csv_application.conf -D","title":"Step 4. (Optional) Verify the configuration"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-csv/#step_5_import_data_into_nebula_graph","text":"When the configuration is ready, run this command to import data from CSV files into Nebula Graph. For more information about the parameters, see Import command parameters . $SPARK_HOME /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange /path/to/nebula-exchange-2.0.0.jar -c /path/to/conf/csv_application.conf","title":"Step 5. Import data into Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-csv/#step_6_optional_verify_data_in_nebula_graph","text":"You can use a Nebula Graph client, such as Nebula Graph Studio, to verify the imported data. For example, in Nebula Graph Studio, run this statement. GO FROM \"1\" OVER action; If the queried destination vertices return, the data are imported into Nebula Graph. You can run the SHOW STATS statement to count the data.","title":"Step 6. (Optional) Verify data in Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-csv/#step_7_optional_create_and_rebuild_indexes_in_nebula_graph","text":"After the data is imported, you can create and rebuild indexes in Nebula Graph. For more information, see nGQL User Guide .","title":"Step 7. (Optional) Create and rebuild indexes in Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-hive/","text":"Import data from HIVE \u00b6 This article uses an example to show how to use Exchange to import data from HIVE into Nebula Graph. Dataset \u00b6 In this article, the Social Network: MOOC User Action Dataset provided by Stanford Network Analysis Platform (SNAP) and 97 unique course names obtained from the public network are used as the sample dataset. The dataset contains: Two vertex types ( user and course ), 7,144 vertices in total. One edge type ( action ), 411,749 edges in total. You can download the example dataset from the nebula-web-docker repository. In this example, the dataset is stored in a database named mooc in HIVE, and the information of all vertices and edges is stored in the users , courses , and actions tables. Here are the structures of all the tables. scala > sql ( \"describe mooc.users\" ). show + --------+---------+-------+ | col_name | data_type | comment | + --------+---------+-------+ | userid | string | null | + --------+---------+-------+ scala > sql ( \"describe mooc.courses\" ). show + ----------+---------+-------+ | col_name | data_type | comment | + ----------+---------+-------+ | courseid | bigint | null | | coursename | string | null | + ----------+---------+-------+ scala > sql ( \"describe mooc.actions\" ). show + --------+---------+-------+ | col_name | data_type | comment | + --------+---------+-------+ | actionid | bigint | null | | srcid | string | null | | dstid | string | null | | duration | double | null | | feature0 | double | null | | feature1 | double | null | | feature2 | double | null | | feature3 | double | null | | label | boolean | null | + --------+---------+-------+ NOTE : bigint in HIVE equals to int in Nebula Graph. Environment \u00b6 The practice is done in macOS. Here is the environment information: Hardware specifications: CPU: 1.7 GHz Quad-Core Intel Core i7 Memory: 16 GB Spark 2.4.7, deployed in the Standalone mode Hadoop 2.9.2, deployed in the Pseudo-Distributed mode HIVE 2.3.7, with MySQL 8.0.22 Nebula Graph v2-nightly, deployed with Docker Compose. For more information, see Deploy Nebula Graph with Docker Compose . Prerequisites \u00b6 To import data from HIVE with Exchange v2.x, do a check of these: Exchange v2.x is compiled. For more information, see Compile Exchange v2.x . Exchange 2.0.0 is used in this example. Spark is installed. Hadoop is installed and started and the hive metastore database (MySQL is used in this example) is started. Nebula Graph is deployed and started. Get the information: IP addresses and ports of the Graph Service and the Meta Service. A Nebula Graph account with the privilege of writing data and its password. Get the necessary information for schema creation in Nebula Graph, including tags and edge types. Procedure \u00b6 Step 1. Create a schema in Nebula Graph \u00b6 Follow these steps to create a schema in Nebula Graph: Confirm the essential elements of the schema. Elements Names Properties Tag user userId string Tag course courseId int, courseName string Edge Type action actionId int, duration double, label bool, feature0 double, feature1 double, feature2 double, feature3 double In Nebula Graph, create a graph space named hive and create a schema. -- Create a graph space named hive CREATE SPACE hive(partition_num=10, replica_factor=1, vid_type=fixed_string(100)); -- Choose the hive graph space USE hive; -- Create the user tag CREATE TAG user(userId string); -- Create the course tag CREATE TAG course(courseId int, courseName string); -- Create the action edge type CREATE EDGE action (actionId int, duration double, label bool, feature0 double, feature1 double, feature2 double, feature3 double); For more information, see Quick Start of Nebula Graph . Step 2. Verify the HIVE SQL statements \u00b6 When spark-shell starts, run these statements one by one to make sure that Spark can read data from HIVE. scala > sql ( \"select userid from mooc.users\" ). show scala > sql ( \"select courseid, coursename from mooc.courses\" ). show scala > sql ( \"select actionid, srcid, dstid, duration, feature0, feature1, feature2, feature3, label from mooc.actions\" ). show Here is an example of data read from the mooc.actions table. + --------+-----+--------------------+--------+------------+------------+-----------+-----------+-----+ | actionid | srcid | dstid | duration | feature0 | feature1 | feature2 | feature3 | label | + --------+-----+--------------------+--------+------------+------------+-----------+-----------+-----+ | 0 | 0 | Environmental Dis ... | 0 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 1 | 0 | History of Ecology | 6 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 2 | 0 | Women in Islam | 41 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 3 | 0 | History of Ecology | 49 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 4 | 0 | Women in Islam | 51 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 5 | 0 | Legacies of the A ... | 55 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 6 | 0 | ITP Core 2 | 59 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 7 | 0 | The Research Pape ... | 62 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 8 | 0 | Neurobiology | 65 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 9 | 0 | Wikipedia | 113 . 0 |- 0 . 319991479 |- 0 . 435701433 | 1 . 108826104 | 12 . 77723482 | false | | 10 | 0 | Media History and ... | 226 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 607804941 | 149 . 4512115 | false | | 11 | 0 | WIKISOO | 974 . 0 |- 0 . 319991479 |- 0 . 435701433 | 1 . 108826104 | 3 . 344522776 | false | | 12 | 0 | Environmental Dis ... | 1000 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 13 | 0 | WIKISOO | 1172 . 0 |- 0 . 319991479 |- 0 . 435701433 | 1 . 108826104 | 1 . 136866766 | false | | 14 | 0 | Women in Islam | 1182 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 15 | 0 | History of Ecology | 1185 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 16 | 0 | Human Development ... | 1687 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 17 | 1 | Human Development ... | 7262 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 18 | 1 | History of Ecology | 7266 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 19 | 1 | Women in Islam | 7273 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 607804941 | 0 . 936170765 | false | + --------+-----+--------------------+--------+------------+------------+-----------+-----------+-----+ only showing top 20 rows Step 3. Edit configuration file \u00b6 After compiling of Exchange, copy the target/classes/application.conf file and edit the configuration for HIVE. In this example, a new configuration file is named hive_ application.conf . In this file, the vertex and edge related configuration is introduced as comments and all the items that are not used in this example are commented out. For more information about the Spark and Nebula related parameters, see Spark related parameters and Nebula Graph related parameters . { # Spark related configuration spark: { app: { name: Nebula Exchange 2.0 } driver: { cores: 1 maxResultSize: 1G } executor: { memory:1G } cores { max: 16 } } # If Spark and HIVE are deployed in the different clusters, # configure these parameters for HIVE. Otherwise, ignore them. #hive: { # waredir: \"hdfs://NAMENODE_IP:9000/apps/svr/hive-xxx/warehouse/\" # connectionURL: \"jdbc:mysql://your_ip:3306/hive_spark?characterEncoding=UTF-8\" # connectionDriverName: \"com.mysql.jdbc.Driver\" # connectionUserName: \"user\" # connectionPassword: \"password\" #} # Nebula Graph related configuration nebula: { address:{ # Specifies the IP addresses and ports of the Graph Service and the Meta Service of Nebula Graph # If multiple servers are used, separate the addresses with commas. # Format: \"ip1:port\",\"ip2:port\",\"ip3:port\" graph:[\"127.0.0.1:9669\"] meta:[\"127.0.0.1:9559\"] } # Specifies an account that has the WriteData privilege in Nebula Graph and its password user: user pswd: password # Specifies a graph space name space: hive connection { timeout: 3000 retry: 3 } execution { retry: 3 } error: { max: 32 output: /tmp/errors } rate: { limit: 1024 timeout: 1000 } } # Process vertices tags: [ # Sets for the user tag { # Specifies a tag name defined in Nebula Graph name: user type: { # Specifies the data source. hive is used. source: hive # Specifies how to import vertex data into Nebula Graph: client or sst. # For more information about importing sst files, see Import SST files (doc to do). sink: client } # Specifies the SQL statement to read data from the users table in the mooc database exec: \"select userid from mooc.users\" # Specifies the column names from the users table to fields. # Their values are used as the source of the userId (nebula.fields) property defined in Nebula Graph. # If more than one column name is specified, separate them with commas. # fields for the HIVE and nebula.fields for Nebula Graph must have the one-to-one correspondence relationship. fields: [userid] nebula.fields: [userId] # Specifies a column as the source of VIDs. # The value of vertex must be one column name in the exec sentence. # If the values are not of the int type, use vertex.policy to # set the mapping policy. \"hash\" is preferred. # Refer to the configuration of the course tag. vertex: userid # Specifies the maximum number of vertex data to be written into # Nebula Graph in a single batch. batch: 256 # Specifies the partition number of Spark. partition: 32 } # Sets for the course tag { name: course type: { source: hive sink: client } exec: \"select courseid, coursename from mooc.courses\" fields: [courseid, coursename] nebula.fields: [courseId, courseName] # Specifies a column as the source of VIDs. # The value of vertex.field must be one column name in the exec sentence. vertex: coursename # For now, only string type VIDs are supported in Nebula Graph v2.x. # Do not use vertex.policy for mapping. #vertex: { # field: coursename # policy: \"hash\" #} batch: 256 partition: 32 } # If more tags are necessary, refer to the preceding configuration to add more. ] # Process edges edges: [ # Sets for the action edge type { # Specifies an edge type name defined in Nebula Graph name: action type: { # Specifies the data source. hive is used. source: hive # Specifies how to import vertex data into Nebula Graph: client or sst # For more information about importing sst files, # see Import SST files (doc to do). sink: client } # Specifies the SQL statement to read data from the actions table in # the mooc database. exec: \"select actionid, srcid, dstid, duration, feature0, feature1, feature2, feature3, label from mooc.actions\" # Specifies the column names from the actions table to fields. # Their values are used as the source of the properties of # the action edge type defined in Nebula Graph. # If more than one column name is specified, separate them with commas. # fields for the HIVE and nebula.fields for Nebula Graph must # have the one-to-one correspondence relationship. fields: [actionid, duration, feature0, feature1, feature2, feature3, label] nebula.fields: [actionId, duration, feature0, feature1, feature2, feature3, label] # source specifies a column as the source of the IDs of # the source vertex of an edge. # target specifies a column as the source of the IDs of # the target vertex of an edge. # The value of source.field and target.field must be # column names set in the exec sentence. source: srcid target: dstid # For now, only string type VIDs are supported in Nebula Graph v2.x. # Do not use vertex.policy for mapping. #target: { # field: dstid # policy: \"hash\" #} # Specifies the maximum number of vertex data to be # written into Nebula Graph in a single batch. batch: 256 # Specifies the partition number of Spark. partition: 32 } ] } Step 4. (Optional) Verify the configuration \u00b6 After the configuration, run the import command with the -D parameter to verify the configuration file. For more information about the parameters, see Import command parameters . $SPARK_HOME /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange /path/to/nebula-exchange-2.0.0.jar -c /path/to/conf/hive_application.conf -h -D Step 5. Import data into Nebula Graph \u00b6 When the configuration is ready, run this command to import data from HIVE into Nebula Graph. For more information about the parameters, see Import command parameters . $SPARK_HOME /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange /path/to/nebula-exchange-2.0.0.jar -c /path/to/conf/hive_application.conf -h Step 6. (Optional) Verify data in Nebula Graph \u00b6 You can use a Nebula Graph client, such as Nebula Graph Studio, to verify the imported data. For example, in Nebula Graph Studio, run this statement. GO FROM \"1\" OVER action; If the queried destination vertices return, the data are imported into Nebula Graph. You can run the SHOW STATS statement to count the data. Step 7. (Optional) Create and rebuild indexes in Nebula Graph \u00b6 After the data is imported, you can create and rebuild indexes in Nebula Graph. For more information, see nGQL User Guide .","title":"Import data from HIVE"},{"location":"nebula-exchange/use-exchange/ex-ug-import-hive/#import_data_from_hive","text":"This article uses an example to show how to use Exchange to import data from HIVE into Nebula Graph.","title":"Import data from HIVE"},{"location":"nebula-exchange/use-exchange/ex-ug-import-hive/#dataset","text":"In this article, the Social Network: MOOC User Action Dataset provided by Stanford Network Analysis Platform (SNAP) and 97 unique course names obtained from the public network are used as the sample dataset. The dataset contains: Two vertex types ( user and course ), 7,144 vertices in total. One edge type ( action ), 411,749 edges in total. You can download the example dataset from the nebula-web-docker repository. In this example, the dataset is stored in a database named mooc in HIVE, and the information of all vertices and edges is stored in the users , courses , and actions tables. Here are the structures of all the tables. scala > sql ( \"describe mooc.users\" ). show + --------+---------+-------+ | col_name | data_type | comment | + --------+---------+-------+ | userid | string | null | + --------+---------+-------+ scala > sql ( \"describe mooc.courses\" ). show + ----------+---------+-------+ | col_name | data_type | comment | + ----------+---------+-------+ | courseid | bigint | null | | coursename | string | null | + ----------+---------+-------+ scala > sql ( \"describe mooc.actions\" ). show + --------+---------+-------+ | col_name | data_type | comment | + --------+---------+-------+ | actionid | bigint | null | | srcid | string | null | | dstid | string | null | | duration | double | null | | feature0 | double | null | | feature1 | double | null | | feature2 | double | null | | feature3 | double | null | | label | boolean | null | + --------+---------+-------+ NOTE : bigint in HIVE equals to int in Nebula Graph.","title":"Dataset"},{"location":"nebula-exchange/use-exchange/ex-ug-import-hive/#environment","text":"The practice is done in macOS. Here is the environment information: Hardware specifications: CPU: 1.7 GHz Quad-Core Intel Core i7 Memory: 16 GB Spark 2.4.7, deployed in the Standalone mode Hadoop 2.9.2, deployed in the Pseudo-Distributed mode HIVE 2.3.7, with MySQL 8.0.22 Nebula Graph v2-nightly, deployed with Docker Compose. For more information, see Deploy Nebula Graph with Docker Compose .","title":"Environment"},{"location":"nebula-exchange/use-exchange/ex-ug-import-hive/#prerequisites","text":"To import data from HIVE with Exchange v2.x, do a check of these: Exchange v2.x is compiled. For more information, see Compile Exchange v2.x . Exchange 2.0.0 is used in this example. Spark is installed. Hadoop is installed and started and the hive metastore database (MySQL is used in this example) is started. Nebula Graph is deployed and started. Get the information: IP addresses and ports of the Graph Service and the Meta Service. A Nebula Graph account with the privilege of writing data and its password. Get the necessary information for schema creation in Nebula Graph, including tags and edge types.","title":"Prerequisites"},{"location":"nebula-exchange/use-exchange/ex-ug-import-hive/#procedure","text":"","title":"Procedure"},{"location":"nebula-exchange/use-exchange/ex-ug-import-hive/#step_1_create_a_schema_in_nebula_graph","text":"Follow these steps to create a schema in Nebula Graph: Confirm the essential elements of the schema. Elements Names Properties Tag user userId string Tag course courseId int, courseName string Edge Type action actionId int, duration double, label bool, feature0 double, feature1 double, feature2 double, feature3 double In Nebula Graph, create a graph space named hive and create a schema. -- Create a graph space named hive CREATE SPACE hive(partition_num=10, replica_factor=1, vid_type=fixed_string(100)); -- Choose the hive graph space USE hive; -- Create the user tag CREATE TAG user(userId string); -- Create the course tag CREATE TAG course(courseId int, courseName string); -- Create the action edge type CREATE EDGE action (actionId int, duration double, label bool, feature0 double, feature1 double, feature2 double, feature3 double); For more information, see Quick Start of Nebula Graph .","title":"Step 1. Create a schema in Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-hive/#step_2_verify_the_hive_sql_statements","text":"When spark-shell starts, run these statements one by one to make sure that Spark can read data from HIVE. scala > sql ( \"select userid from mooc.users\" ). show scala > sql ( \"select courseid, coursename from mooc.courses\" ). show scala > sql ( \"select actionid, srcid, dstid, duration, feature0, feature1, feature2, feature3, label from mooc.actions\" ). show Here is an example of data read from the mooc.actions table. + --------+-----+--------------------+--------+------------+------------+-----------+-----------+-----+ | actionid | srcid | dstid | duration | feature0 | feature1 | feature2 | feature3 | label | + --------+-----+--------------------+--------+------------+------------+-----------+-----------+-----+ | 0 | 0 | Environmental Dis ... | 0 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 1 | 0 | History of Ecology | 6 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 2 | 0 | Women in Islam | 41 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 3 | 0 | History of Ecology | 49 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 4 | 0 | Women in Islam | 51 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 5 | 0 | Legacies of the A ... | 55 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 6 | 0 | ITP Core 2 | 59 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 7 | 0 | The Research Pape ... | 62 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 8 | 0 | Neurobiology | 65 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 9 | 0 | Wikipedia | 113 . 0 |- 0 . 319991479 |- 0 . 435701433 | 1 . 108826104 | 12 . 77723482 | false | | 10 | 0 | Media History and ... | 226 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 607804941 | 149 . 4512115 | false | | 11 | 0 | WIKISOO | 974 . 0 |- 0 . 319991479 |- 0 . 435701433 | 1 . 108826104 | 3 . 344522776 | false | | 12 | 0 | Environmental Dis ... | 1000 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 13 | 0 | WIKISOO | 1172 . 0 |- 0 . 319991479 |- 0 . 435701433 | 1 . 108826104 | 1 . 136866766 | false | | 14 | 0 | Women in Islam | 1182 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 15 | 0 | History of Ecology | 1185 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 16 | 0 | Human Development ... | 1687 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 17 | 1 | Human Development ... | 7262 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 18 | 1 | History of Ecology | 7266 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 19 | 1 | Women in Islam | 7273 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 607804941 | 0 . 936170765 | false | + --------+-----+--------------------+--------+------------+------------+-----------+-----------+-----+ only showing top 20 rows","title":"Step 2. Verify the HIVE SQL statements"},{"location":"nebula-exchange/use-exchange/ex-ug-import-hive/#step_3_edit_configuration_file","text":"After compiling of Exchange, copy the target/classes/application.conf file and edit the configuration for HIVE. In this example, a new configuration file is named hive_ application.conf . In this file, the vertex and edge related configuration is introduced as comments and all the items that are not used in this example are commented out. For more information about the Spark and Nebula related parameters, see Spark related parameters and Nebula Graph related parameters . { # Spark related configuration spark: { app: { name: Nebula Exchange 2.0 } driver: { cores: 1 maxResultSize: 1G } executor: { memory:1G } cores { max: 16 } } # If Spark and HIVE are deployed in the different clusters, # configure these parameters for HIVE. Otherwise, ignore them. #hive: { # waredir: \"hdfs://NAMENODE_IP:9000/apps/svr/hive-xxx/warehouse/\" # connectionURL: \"jdbc:mysql://your_ip:3306/hive_spark?characterEncoding=UTF-8\" # connectionDriverName: \"com.mysql.jdbc.Driver\" # connectionUserName: \"user\" # connectionPassword: \"password\" #} # Nebula Graph related configuration nebula: { address:{ # Specifies the IP addresses and ports of the Graph Service and the Meta Service of Nebula Graph # If multiple servers are used, separate the addresses with commas. # Format: \"ip1:port\",\"ip2:port\",\"ip3:port\" graph:[\"127.0.0.1:9669\"] meta:[\"127.0.0.1:9559\"] } # Specifies an account that has the WriteData privilege in Nebula Graph and its password user: user pswd: password # Specifies a graph space name space: hive connection { timeout: 3000 retry: 3 } execution { retry: 3 } error: { max: 32 output: /tmp/errors } rate: { limit: 1024 timeout: 1000 } } # Process vertices tags: [ # Sets for the user tag { # Specifies a tag name defined in Nebula Graph name: user type: { # Specifies the data source. hive is used. source: hive # Specifies how to import vertex data into Nebula Graph: client or sst. # For more information about importing sst files, see Import SST files (doc to do). sink: client } # Specifies the SQL statement to read data from the users table in the mooc database exec: \"select userid from mooc.users\" # Specifies the column names from the users table to fields. # Their values are used as the source of the userId (nebula.fields) property defined in Nebula Graph. # If more than one column name is specified, separate them with commas. # fields for the HIVE and nebula.fields for Nebula Graph must have the one-to-one correspondence relationship. fields: [userid] nebula.fields: [userId] # Specifies a column as the source of VIDs. # The value of vertex must be one column name in the exec sentence. # If the values are not of the int type, use vertex.policy to # set the mapping policy. \"hash\" is preferred. # Refer to the configuration of the course tag. vertex: userid # Specifies the maximum number of vertex data to be written into # Nebula Graph in a single batch. batch: 256 # Specifies the partition number of Spark. partition: 32 } # Sets for the course tag { name: course type: { source: hive sink: client } exec: \"select courseid, coursename from mooc.courses\" fields: [courseid, coursename] nebula.fields: [courseId, courseName] # Specifies a column as the source of VIDs. # The value of vertex.field must be one column name in the exec sentence. vertex: coursename # For now, only string type VIDs are supported in Nebula Graph v2.x. # Do not use vertex.policy for mapping. #vertex: { # field: coursename # policy: \"hash\" #} batch: 256 partition: 32 } # If more tags are necessary, refer to the preceding configuration to add more. ] # Process edges edges: [ # Sets for the action edge type { # Specifies an edge type name defined in Nebula Graph name: action type: { # Specifies the data source. hive is used. source: hive # Specifies how to import vertex data into Nebula Graph: client or sst # For more information about importing sst files, # see Import SST files (doc to do). sink: client } # Specifies the SQL statement to read data from the actions table in # the mooc database. exec: \"select actionid, srcid, dstid, duration, feature0, feature1, feature2, feature3, label from mooc.actions\" # Specifies the column names from the actions table to fields. # Their values are used as the source of the properties of # the action edge type defined in Nebula Graph. # If more than one column name is specified, separate them with commas. # fields for the HIVE and nebula.fields for Nebula Graph must # have the one-to-one correspondence relationship. fields: [actionid, duration, feature0, feature1, feature2, feature3, label] nebula.fields: [actionId, duration, feature0, feature1, feature2, feature3, label] # source specifies a column as the source of the IDs of # the source vertex of an edge. # target specifies a column as the source of the IDs of # the target vertex of an edge. # The value of source.field and target.field must be # column names set in the exec sentence. source: srcid target: dstid # For now, only string type VIDs are supported in Nebula Graph v2.x. # Do not use vertex.policy for mapping. #target: { # field: dstid # policy: \"hash\" #} # Specifies the maximum number of vertex data to be # written into Nebula Graph in a single batch. batch: 256 # Specifies the partition number of Spark. partition: 32 } ] }","title":"Step 3. Edit configuration file"},{"location":"nebula-exchange/use-exchange/ex-ug-import-hive/#step_4_optional_verify_the_configuration","text":"After the configuration, run the import command with the -D parameter to verify the configuration file. For more information about the parameters, see Import command parameters . $SPARK_HOME /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange /path/to/nebula-exchange-2.0.0.jar -c /path/to/conf/hive_application.conf -h -D","title":"Step 4. (Optional) Verify the configuration"},{"location":"nebula-exchange/use-exchange/ex-ug-import-hive/#step_5_import_data_into_nebula_graph","text":"When the configuration is ready, run this command to import data from HIVE into Nebula Graph. For more information about the parameters, see Import command parameters . $SPARK_HOME /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange /path/to/nebula-exchange-2.0.0.jar -c /path/to/conf/hive_application.conf -h","title":"Step 5. Import data into Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-hive/#step_6_optional_verify_data_in_nebula_graph","text":"You can use a Nebula Graph client, such as Nebula Graph Studio, to verify the imported data. For example, in Nebula Graph Studio, run this statement. GO FROM \"1\" OVER action; If the queried destination vertices return, the data are imported into Nebula Graph. You can run the SHOW STATS statement to count the data.","title":"Step 6. (Optional) Verify data in Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-hive/#step_7_optional_create_and_rebuild_indexes_in_nebula_graph","text":"After the data is imported, you can create and rebuild indexes in Nebula Graph. For more information, see nGQL User Guide .","title":"Step 7. (Optional) Create and rebuild indexes in Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-json/","text":"Import data from JSON files \u00b6 This article uses an example to show how to use Exchange to import data from JSON files stored on HDFS into Nebula Graph. Dataset \u00b6 The JSON file (test.json) used in this example is like {\"source\":string, \"target\":string, \"likeness\":double} , representing a like relationship between source and target . 21,645 records in total. Here are some sample data: { \"source\" : 53802643 , \"target\" : 87847387 , \"likeness\" : 0.34 } { \"source\" : 29509860 , \"target\" : 57501950 , \"likeness\" : 0.40 } { \"source\" : 97319348 , \"target\" : 50240344 , \"likeness\" : 0.77 } { \"source\" : 94295709 , \"target\" : 8189720 , \"likeness\" : 0.82 } { \"source\" : 78707720 , \"target\" : 53874070 , \"likeness\" : 0.98 } { \"source\" : 23399562 , \"target\" : 20136097 , \"likeness\" : 0.47 } Environment \u00b6 The practice is done in macOS. Here is the environment information: Hardware specifications: CPU: 1.7 GHz Quad-Core Intel Core i7 Memory: 16 GB Spark 2.4.7, deployed in the Standalone mode Hadoop 2.9.2, deployed in the Pseudo-Distributed mode Nebula Graph v2-nightly, deployed with Docker Compose. For more information, see Deploy Nebula Graph with Docker Compose . Prerequisites \u00b6 To import data from JSON files on HDFS with Exchange v2.x, do a check of these: Exchange v2.x is compiled. For more information, see Compile Exchange v2.x . Exchange 2.0.0 is used in this example. Spark is installed. Hadoop is installed and started. Nebula Graph is deployed and started. Get the information: IP addresses and ports of the Graph Service and the Meta Service. A Nebula Graph account with the privilege of writing data and its password. Get the necessary information for schema creation in Nebula Graph, including tags and edge types. Procedure \u00b6 Step 1. Create a schema in Nebula Graph \u00b6 Analyze the data in the JSON files and follow these steps to create a schema in Nebula Graph: Confirm the essential elements of the schema. Elements Names Properties Tag source srcId string Tag target dstId string Edge Type like likeness double In Nebula Graph, create a graph space named json and create a schema. -- Create a graph space named json CREATE SPACE json(partition_num=10, replica_factor=1, vid_type=fixed_string(30)); -- Choose the json graph space USE json; -- Create the source tag CREATE TAG source (srcId string); -- Create the target tag CREATE TAG target (dstId string); -- Create the like edge type CREATE EDGE like (likeness double); For more information, see Quick Start of Nebula Graph . Step 2. Prepare JSON files \u00b6 Create separate JSON files for vertex and edge data. Store the JSON files in HDFS and get the HDFS path of the files. NOTE : In this example, only one JSON file is used to import vertex and edge data at the same time. Some vertex data representing source and target are duplicate. Therefore, during the import process, these vertices are written repeatedly. In Nebula Graph, data is overwritten when repeated insertion occurs, and the last write is read out. In practice, to increase the write speed, creating separate files for vertex and edge data is recommended. Step 3. Edit configuration file \u00b6 After compiling of Exchange, copy the target/classes/application.conf file and edit the configuration for JSON files. In this example, a new configuration file is named json_ application.conf . In this file, the vertex and edge related configuration is introduced as comments and all the items that are not used in this example are commented out. For more information about the Spark and Nebula related parameters, see Spark related parameters and Nebula Graph related parameters . { # Spark related configuration spark: { app: { name: Spark Writer } driver: { cores: 1 maxResultSize: 1G } executor: { memory:1G } cores { max: 16 } } # Nebula Graph related configuration nebula: { address:{ # Specifies the IP addresses and ports of the Graph Service and the Meta Service of Nebula Graph. # If multiple servers are used, separate the addresses with commas. # Format: \"ip1:port\",\"ip2:port\",\"ip3:port\" graph:[\"127.0.0.1:9669\"] meta:[\"127.0.0.1:9559\"] } # Specifies an account that has the WriteData privilege in Nebula Graph and its password. user: user pswd: password # Specifies a graph space name space: json connection { timeout: 3000 retry: 3 } execution { retry: 3 } error: { max: 32 output: /tmp/errors } rate: { limit: 1024 timeout: 1000 } } # Process vertices tags: [ # Sets for the source tag { # Specifies a tag name defined in Nebula Graph name: source type: { # Specifies the data source. json is used. source: json # Specifies how to import vertex data into Nebula Graph: client or sst. # For more information about importing sst files, see Import SST files (doc to do). sink: client } # Specifies the HDFS path of the JSON file. # Enclose the path with double quotes and start the path with hdfs://. path: \"hdfs://namenode_ip:port/path/to/test.json\" # Specifies the keys in the JSON file. # Their values are used as the source of the srcId property # defined in Nebula Graph. # If more than one key is specified, separate them with commas. fields: [\"source\"] nebula.fields: [\"srcId\"] # Specifies the values of a key in the JSON file as # the source of the VID in Nebula Graph. # For now, only string type VIDs are supported in Nebula Graph v2.x. # Do not use vertex.policy for mapping. # vertex: { # field: key_name_in_json # policy: \"hash\" # } vertex: source batch: 256 partition: 32 } # Sets for the target tag { name: target type: { source: json sink: client } path: \"hdfs://namenode_ip:port/path/to/test.json\" fields: [\"target\"] nebula.fields: [\"dstId\"] vertex: \"target\" batch: 256 partition: 32 isImplicit: true } # If more tags are necessary, refer to the preceding configuration to add more. ] # Process edges edges: [ # Sets for the like edge type { # Specifies an edge type name defined in Nebula Graph name: like type: { # Specifies the data source. json is used. source: json # Specifies how to import vertex data into Nebula Graph: client or sst. # For more information about importing sst files, see Import SST files (doc to do). sink: client } # Specifies the HDFS path of the JSON file. # Enclose the path with double quotes and start the path with hdfs://. path: \"hdfs://namenode_ip:port/path/to/test.json\" # Specifies the keys in the JSON file. # Their values are used as the source of the likeness property defined in Nebula Graph. # If more than one key is specified, separate them with commas. fields: [\"likeness\"] nebula.fields: [\"likeness\"] # Specifies the values of two keys in the JSON file as the source # of the IDs of source and destination vertices of the like edges in Nebula Graph. # For now, only string type VIDs are supported in Nebula Graph v2.x. # Do not use vertex.policy for mapping. # source: { # field: key_name_in_json # policy: \"hash\" # } # target: { # field: key_name_in_json # policy: \"hash\" # } source: \"source\" target: \"target\" batch: 256 partition: 32 } # If more edge types are necessary, refer to the preceding configuration to add more. ] } Step 4. (Optional) Verify the configuration \u00b6 After the configuration, run the import command with the -D parameter to verify the configuration file. For more information about the parameters, see Import command parameters . $SPARK_HOME /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange /path/to/nebula-exchange-2.0.0.jar -c /path/to/conf/json_application.conf -D Step 5. Import data into Nebula Graph \u00b6 When the configuration is ready, run this command to import data from JSON files into Nebula Graph. For more information about the parameters, see Import command parameters . $SPARK_HOME /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange /path/to/nebula-exchange-2.0.0.jar -c /path/to/conf/json_application.conf Step 6. (Optional) Verify data in Nebula Graph \u00b6 You can use a Nebula Graph client, such as Nebula Graph Studio, to verify the imported data. For example, in Nebula Graph Studio, run this statement. GO FROM \"53802643\" OVER like; If the queried destination vertices return, the data are imported into Nebula Graph. You can run the SHOW STATS statement to count the data. Step 7. (Optional) Create and rebuild indexes in Nebula Graph \u00b6 After the data is imported, you can create and rebuild indexes in Nebula Graph. For more information, see nGQL User Guide .","title":"Import data from JSON files"},{"location":"nebula-exchange/use-exchange/ex-ug-import-json/#import_data_from_json_files","text":"This article uses an example to show how to use Exchange to import data from JSON files stored on HDFS into Nebula Graph.","title":"Import data from JSON files"},{"location":"nebula-exchange/use-exchange/ex-ug-import-json/#dataset","text":"The JSON file (test.json) used in this example is like {\"source\":string, \"target\":string, \"likeness\":double} , representing a like relationship between source and target . 21,645 records in total. Here are some sample data: { \"source\" : 53802643 , \"target\" : 87847387 , \"likeness\" : 0.34 } { \"source\" : 29509860 , \"target\" : 57501950 , \"likeness\" : 0.40 } { \"source\" : 97319348 , \"target\" : 50240344 , \"likeness\" : 0.77 } { \"source\" : 94295709 , \"target\" : 8189720 , \"likeness\" : 0.82 } { \"source\" : 78707720 , \"target\" : 53874070 , \"likeness\" : 0.98 } { \"source\" : 23399562 , \"target\" : 20136097 , \"likeness\" : 0.47 }","title":"Dataset"},{"location":"nebula-exchange/use-exchange/ex-ug-import-json/#environment","text":"The practice is done in macOS. Here is the environment information: Hardware specifications: CPU: 1.7 GHz Quad-Core Intel Core i7 Memory: 16 GB Spark 2.4.7, deployed in the Standalone mode Hadoop 2.9.2, deployed in the Pseudo-Distributed mode Nebula Graph v2-nightly, deployed with Docker Compose. For more information, see Deploy Nebula Graph with Docker Compose .","title":"Environment"},{"location":"nebula-exchange/use-exchange/ex-ug-import-json/#prerequisites","text":"To import data from JSON files on HDFS with Exchange v2.x, do a check of these: Exchange v2.x is compiled. For more information, see Compile Exchange v2.x . Exchange 2.0.0 is used in this example. Spark is installed. Hadoop is installed and started. Nebula Graph is deployed and started. Get the information: IP addresses and ports of the Graph Service and the Meta Service. A Nebula Graph account with the privilege of writing data and its password. Get the necessary information for schema creation in Nebula Graph, including tags and edge types.","title":"Prerequisites"},{"location":"nebula-exchange/use-exchange/ex-ug-import-json/#procedure","text":"","title":"Procedure"},{"location":"nebula-exchange/use-exchange/ex-ug-import-json/#step_1_create_a_schema_in_nebula_graph","text":"Analyze the data in the JSON files and follow these steps to create a schema in Nebula Graph: Confirm the essential elements of the schema. Elements Names Properties Tag source srcId string Tag target dstId string Edge Type like likeness double In Nebula Graph, create a graph space named json and create a schema. -- Create a graph space named json CREATE SPACE json(partition_num=10, replica_factor=1, vid_type=fixed_string(30)); -- Choose the json graph space USE json; -- Create the source tag CREATE TAG source (srcId string); -- Create the target tag CREATE TAG target (dstId string); -- Create the like edge type CREATE EDGE like (likeness double); For more information, see Quick Start of Nebula Graph .","title":"Step 1. Create a schema in Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-json/#step_2_prepare_json_files","text":"Create separate JSON files for vertex and edge data. Store the JSON files in HDFS and get the HDFS path of the files. NOTE : In this example, only one JSON file is used to import vertex and edge data at the same time. Some vertex data representing source and target are duplicate. Therefore, during the import process, these vertices are written repeatedly. In Nebula Graph, data is overwritten when repeated insertion occurs, and the last write is read out. In practice, to increase the write speed, creating separate files for vertex and edge data is recommended.","title":"Step 2. Prepare JSON files"},{"location":"nebula-exchange/use-exchange/ex-ug-import-json/#step_3_edit_configuration_file","text":"After compiling of Exchange, copy the target/classes/application.conf file and edit the configuration for JSON files. In this example, a new configuration file is named json_ application.conf . In this file, the vertex and edge related configuration is introduced as comments and all the items that are not used in this example are commented out. For more information about the Spark and Nebula related parameters, see Spark related parameters and Nebula Graph related parameters . { # Spark related configuration spark: { app: { name: Spark Writer } driver: { cores: 1 maxResultSize: 1G } executor: { memory:1G } cores { max: 16 } } # Nebula Graph related configuration nebula: { address:{ # Specifies the IP addresses and ports of the Graph Service and the Meta Service of Nebula Graph. # If multiple servers are used, separate the addresses with commas. # Format: \"ip1:port\",\"ip2:port\",\"ip3:port\" graph:[\"127.0.0.1:9669\"] meta:[\"127.0.0.1:9559\"] } # Specifies an account that has the WriteData privilege in Nebula Graph and its password. user: user pswd: password # Specifies a graph space name space: json connection { timeout: 3000 retry: 3 } execution { retry: 3 } error: { max: 32 output: /tmp/errors } rate: { limit: 1024 timeout: 1000 } } # Process vertices tags: [ # Sets for the source tag { # Specifies a tag name defined in Nebula Graph name: source type: { # Specifies the data source. json is used. source: json # Specifies how to import vertex data into Nebula Graph: client or sst. # For more information about importing sst files, see Import SST files (doc to do). sink: client } # Specifies the HDFS path of the JSON file. # Enclose the path with double quotes and start the path with hdfs://. path: \"hdfs://namenode_ip:port/path/to/test.json\" # Specifies the keys in the JSON file. # Their values are used as the source of the srcId property # defined in Nebula Graph. # If more than one key is specified, separate them with commas. fields: [\"source\"] nebula.fields: [\"srcId\"] # Specifies the values of a key in the JSON file as # the source of the VID in Nebula Graph. # For now, only string type VIDs are supported in Nebula Graph v2.x. # Do not use vertex.policy for mapping. # vertex: { # field: key_name_in_json # policy: \"hash\" # } vertex: source batch: 256 partition: 32 } # Sets for the target tag { name: target type: { source: json sink: client } path: \"hdfs://namenode_ip:port/path/to/test.json\" fields: [\"target\"] nebula.fields: [\"dstId\"] vertex: \"target\" batch: 256 partition: 32 isImplicit: true } # If more tags are necessary, refer to the preceding configuration to add more. ] # Process edges edges: [ # Sets for the like edge type { # Specifies an edge type name defined in Nebula Graph name: like type: { # Specifies the data source. json is used. source: json # Specifies how to import vertex data into Nebula Graph: client or sst. # For more information about importing sst files, see Import SST files (doc to do). sink: client } # Specifies the HDFS path of the JSON file. # Enclose the path with double quotes and start the path with hdfs://. path: \"hdfs://namenode_ip:port/path/to/test.json\" # Specifies the keys in the JSON file. # Their values are used as the source of the likeness property defined in Nebula Graph. # If more than one key is specified, separate them with commas. fields: [\"likeness\"] nebula.fields: [\"likeness\"] # Specifies the values of two keys in the JSON file as the source # of the IDs of source and destination vertices of the like edges in Nebula Graph. # For now, only string type VIDs are supported in Nebula Graph v2.x. # Do not use vertex.policy for mapping. # source: { # field: key_name_in_json # policy: \"hash\" # } # target: { # field: key_name_in_json # policy: \"hash\" # } source: \"source\" target: \"target\" batch: 256 partition: 32 } # If more edge types are necessary, refer to the preceding configuration to add more. ] }","title":"Step 3. Edit configuration file"},{"location":"nebula-exchange/use-exchange/ex-ug-import-json/#step_4_optional_verify_the_configuration","text":"After the configuration, run the import command with the -D parameter to verify the configuration file. For more information about the parameters, see Import command parameters . $SPARK_HOME /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange /path/to/nebula-exchange-2.0.0.jar -c /path/to/conf/json_application.conf -D","title":"Step 4. (Optional) Verify the configuration"},{"location":"nebula-exchange/use-exchange/ex-ug-import-json/#step_5_import_data_into_nebula_graph","text":"When the configuration is ready, run this command to import data from JSON files into Nebula Graph. For more information about the parameters, see Import command parameters . $SPARK_HOME /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange /path/to/nebula-exchange-2.0.0.jar -c /path/to/conf/json_application.conf","title":"Step 5. Import data into Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-json/#step_6_optional_verify_data_in_nebula_graph","text":"You can use a Nebula Graph client, such as Nebula Graph Studio, to verify the imported data. For example, in Nebula Graph Studio, run this statement. GO FROM \"53802643\" OVER like; If the queried destination vertices return, the data are imported into Nebula Graph. You can run the SHOW STATS statement to count the data.","title":"Step 6. (Optional) Verify data in Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-json/#step_7_optional_create_and_rebuild_indexes_in_nebula_graph","text":"After the data is imported, you can create and rebuild indexes in Nebula Graph. For more information, see nGQL User Guide .","title":"Step 7. (Optional) Create and rebuild indexes in Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-steps/","text":"Use Exchange \u00b6 This article introduces the generally-used procedure on how to use Exchange to import data from a specified source to Nebula Graph. Prerequisites \u00b6 To import data with Exchange, do a check of these: Nebula Graph is deployed and started. Get the information: IP addresses and ports of the Graph Service and the Meta Service. A Nebula Graph account with the privilege of writing data and its password. Exchange is compiled. For more information, see Compile Exchange . Spark is installed. Get the necessary information for schema creation in Nebula Graph, including tags and edge types. Procedure \u00b6 To import data from a source to Nebula Graph, follow these steps: Create a graph space and a schema in Nebula Graph. (Optional) Process the source data. For example, to import data from a Neo4j database, create indexes for the specified tags in Neo4j to export the data from Neo4j more quickly. Edit the configuration file for Spark, Nebula Graph, vertices, and edges. NOTE : After compiling of Exchange, refer to the example configuration files in the nebula-exchange/target/classes directory for the configuration for different sources. (Optional) Run the import command with the -D parameter to verify the configuration. For more information, see Import command parameters . Run the import command to import data into Nebula Graph. Verify the imported data in Nebula Graph. (Optional) Create and rebuild indexes in Nebula Graph. For more information, see the examples: Import data from HIVE Import data from CSV files Import data from JSON files","title":"Use Exchange"},{"location":"nebula-exchange/use-exchange/ex-ug-import-steps/#use_exchange","text":"This article introduces the generally-used procedure on how to use Exchange to import data from a specified source to Nebula Graph.","title":"Use Exchange"},{"location":"nebula-exchange/use-exchange/ex-ug-import-steps/#prerequisites","text":"To import data with Exchange, do a check of these: Nebula Graph is deployed and started. Get the information: IP addresses and ports of the Graph Service and the Meta Service. A Nebula Graph account with the privilege of writing data and its password. Exchange is compiled. For more information, see Compile Exchange . Spark is installed. Get the necessary information for schema creation in Nebula Graph, including tags and edge types.","title":"Prerequisites"},{"location":"nebula-exchange/use-exchange/ex-ug-import-steps/#procedure","text":"To import data from a source to Nebula Graph, follow these steps: Create a graph space and a schema in Nebula Graph. (Optional) Process the source data. For example, to import data from a Neo4j database, create indexes for the specified tags in Neo4j to export the data from Neo4j more quickly. Edit the configuration file for Spark, Nebula Graph, vertices, and edges. NOTE : After compiling of Exchange, refer to the example configuration files in the nebula-exchange/target/classes directory for the configuration for different sources. (Optional) Run the import command with the -D parameter to verify the configuration. For more information, see Import command parameters . Run the import command to import data into Nebula Graph. Verify the imported data in Nebula Graph. (Optional) Create and rebuild indexes in Nebula Graph. For more information, see the examples: Import data from HIVE Import data from CSV files Import data from JSON files","title":"Procedure"},{"location":"nebula-studio/st-ug-toc/","text":"Nebula Graph Studio User Guide \u00b6 About Nebula Graph Studio What is Nebula Graph Studio Limitations Check updates Deploy and connect Deploy Studio Connect to Nebula Graph Design a schema Create a schema Import data Query graph data --> Operation guide [Use Explore][DOC_TO_DO] Use Console Open in Explore View subgraphs [Export as CSV files][DOC_TO_DO] Best practices [DOC_TO_DO] Troubleshooting [DOC_TO_DO] Connection Error messages","title":"Nebula Graph Studio User Guide"},{"location":"nebula-studio/st-ug-toc/#nebula_graph_studio_user_guide","text":"About Nebula Graph Studio What is Nebula Graph Studio Limitations Check updates Deploy and connect Deploy Studio Connect to Nebula Graph Design a schema Create a schema Import data Query graph data --> Operation guide [Use Explore][DOC_TO_DO] Use Console Open in Explore View subgraphs [Export as CSV files][DOC_TO_DO] Best practices [DOC_TO_DO] Troubleshooting [DOC_TO_DO] Connection Error messages","title":"Nebula Graph Studio User Guide"},{"location":"nebula-studio/about-studio/st-ug-check-updates/","text":"Check updates \u00b6 Studio v2.x is in development. To get updated with its development, visit GitHub and read its Changelog . For Docker-based Studio v2.x, when you get access to Studio, on the upper-right corner of the page, click the version number and then New version , and you will be directed to the Changelog. When new version is released, under the nebula-web-docker directory, run these commands one by one to update the Docker image and start the services: $ git pull origin master $ cd v2 $ docker-compose pull && docker-compose up -d","title":"Check updates"},{"location":"nebula-studio/about-studio/st-ug-check-updates/#check_updates","text":"Studio v2.x is in development. To get updated with its development, visit GitHub and read its Changelog . For Docker-based Studio v2.x, when you get access to Studio, on the upper-right corner of the page, click the version number and then New version , and you will be directed to the Changelog. When new version is released, under the nebula-web-docker directory, run these commands one by one to update the Docker image and start the services: $ git pull origin master $ cd v2 $ docker-compose pull && docker-compose up -d","title":"Check updates"},{"location":"nebula-studio/about-studio/st-ug-limitations/","text":"Limitations \u00b6 This article introduces the limitations of Studio v2.x. Nebula Graph versions \u00b6 Only Nebula Graph v2.x is supported. If you are using Nebula Graph v1.x, please use Studio v1.x. For more information, see Studio v1.x User Guide . Architecture \u00b6 For now, Docker-based Studio v2.x supports x86_64 architecture only. Upload data \u00b6 On Docker-based Studio v2.x, only CSV files with comma-separated data and without headers can be uploaded, but no limitations are applied to the size and store period for a single file. The maximum data volume depends on the storage capacity of your machine. Data backup \u00b6 For now, you can export the queried results in the CSV format on the Console page. No other backup methods are available. nGQL statements \u00b6 On the Console page of Docker-based Studio v2.x, all the nGQL syntaxes except these are supported: USE <space_name> : You cannot run such a statement on the Console page to choose a graph space. As an alternative, you can click a graph space name in the drop-down list of Current Graph Space . You cannot use line breaks (\\). As an alternative, you can use the Enter key to split a line. Browser \u00b6 We recommend that you use the latest version of Chrome to get access to Studio v2.x.","title":"Limitations"},{"location":"nebula-studio/about-studio/st-ug-limitations/#limitations","text":"This article introduces the limitations of Studio v2.x.","title":"Limitations"},{"location":"nebula-studio/about-studio/st-ug-limitations/#nebula_graph_versions","text":"Only Nebula Graph v2.x is supported. If you are using Nebula Graph v1.x, please use Studio v1.x. For more information, see Studio v1.x User Guide .","title":"Nebula Graph versions"},{"location":"nebula-studio/about-studio/st-ug-limitations/#architecture","text":"For now, Docker-based Studio v2.x supports x86_64 architecture only.","title":"Architecture"},{"location":"nebula-studio/about-studio/st-ug-limitations/#upload_data","text":"On Docker-based Studio v2.x, only CSV files with comma-separated data and without headers can be uploaded, but no limitations are applied to the size and store period for a single file. The maximum data volume depends on the storage capacity of your machine.","title":"Upload data"},{"location":"nebula-studio/about-studio/st-ug-limitations/#data_backup","text":"For now, you can export the queried results in the CSV format on the Console page. No other backup methods are available.","title":"Data backup"},{"location":"nebula-studio/about-studio/st-ug-limitations/#ngql_statements","text":"On the Console page of Docker-based Studio v2.x, all the nGQL syntaxes except these are supported: USE <space_name> : You cannot run such a statement on the Console page to choose a graph space. As an alternative, you can click a graph space name in the drop-down list of Current Graph Space . You cannot use line breaks (\\). As an alternative, you can use the Enter key to split a line.","title":"nGQL statements"},{"location":"nebula-studio/about-studio/st-ug-limitations/#browser","text":"We recommend that you use the latest version of Chrome to get access to Studio v2.x.","title":"Browser"},{"location":"nebula-studio/about-studio/st-ug-what-is-graph-studio/","text":"What is Nebula Graph Studio \u00b6 Nebula Graph Studio (Studio in short) is a browser-based visualization tool to manage Nebula Graph. It provides you with a graphical user interface to manipulate graph schemas, import data, explore graph data, and run nGQL statements to retrieve data. With Studio, you can quickly become a graph exploration expert from scratch. Release distributions \u00b6 For now, Studio v2.x has only Docker-based distribution. You can deploy Studio v2.x with Docker and connect it to Nebula Graph v2.x. For more information, see Deploy Studio . Studio on Cloud is not available now. Features \u00b6 Studio provides these features: Graphical Console enables you to run nGQL statements and read the results in a human-friendly way. The Explore function enables you to explore the graph data. It helps you dig the relationships among data and improves the efficiency of data analysis. Scenarios \u00b6 You can use Studio v2.x in one of these scenarios: You have a dataset, and you want to explore and analyze data in a visualized way. You can use Docker Compose or Nebula Graph Cloud Service to deploy Nebula Graph and then use Studio to explore or analyze data in a visualized way. You have deployed Nebula Graph and imported a dataset. You want to use a GUI to run nGQL statements or explore and analyze graph data in a visualized way. You are a beginner of nGQL (Nebula Graph Query Language) and you prefer to use a GUI rather than a command-line interface (CLI) to learn the language.","title":"What is Studio"},{"location":"nebula-studio/about-studio/st-ug-what-is-graph-studio/#what_is_nebula_graph_studio","text":"Nebula Graph Studio (Studio in short) is a browser-based visualization tool to manage Nebula Graph. It provides you with a graphical user interface to manipulate graph schemas, import data, explore graph data, and run nGQL statements to retrieve data. With Studio, you can quickly become a graph exploration expert from scratch.","title":"What is Nebula Graph Studio"},{"location":"nebula-studio/about-studio/st-ug-what-is-graph-studio/#release_distributions","text":"For now, Studio v2.x has only Docker-based distribution. You can deploy Studio v2.x with Docker and connect it to Nebula Graph v2.x. For more information, see Deploy Studio . Studio on Cloud is not available now.","title":"Release distributions"},{"location":"nebula-studio/about-studio/st-ug-what-is-graph-studio/#features","text":"Studio provides these features: Graphical Console enables you to run nGQL statements and read the results in a human-friendly way. The Explore function enables you to explore the graph data. It helps you dig the relationships among data and improves the efficiency of data analysis.","title":"Features"},{"location":"nebula-studio/about-studio/st-ug-what-is-graph-studio/#scenarios","text":"You can use Studio v2.x in one of these scenarios: You have a dataset, and you want to explore and analyze data in a visualized way. You can use Docker Compose or Nebula Graph Cloud Service to deploy Nebula Graph and then use Studio to explore or analyze data in a visualized way. You have deployed Nebula Graph and imported a dataset. You want to use a GUI to run nGQL statements or explore and analyze graph data in a visualized way. You are a beginner of nGQL (Nebula Graph Query Language) and you prefer to use a GUI rather than a command-line interface (CLI) to learn the language.","title":"Scenarios"},{"location":"nebula-studio/install-configure/st-ug-connect/","text":"Connect to Nebula Graph \u00b6 For Docker-based Studio v2.x, when it is started, you must configure it to connect to Nebula Graph v2.x. This article introduces how to connect Docker-based Studio v2.x to Nebula Graph v2.x. Prerequisites \u00b6 Before you connect Docker-based Studio v2.x to Nebula Graph v2.x, you must do a check of these: The Nebula Graph v2.x services and Studio v2.x are started. For more information, see Deploy Studio . You have the IP address and the port used by the Graph service of Nebula Graph v2.x. The default port is 9669 . NOTE : Run ifconfig or ipconfig on the machine to get the IP address. Procedure \u00b6 To connect Docker-based Studio to Nebula Graph, follow these steps: On the Config Server page of Studio, configure these fields: Host : Enter the IP address and the port of the Graph service of Nebula Graph. The valid format is IP:port . The default port is 9669 . NOTE : When Nebula Graph and Studio are deployed on the same machine, you must enter the IP address of the machine, but not 127.0.0.1 or localhost , in the Host field. Username and Password : You can use user and password as the username and its password. After the configuration, click the Connect button. If you can see the Console page, Docker-based Studio is successfully connected to Nebula Graph. One session continues up to 30 minutes. If you do not operate Studio within 30 minutes, the active session will time out and you must connect to Nebula Graph again. Next to do \u00b6 When Studio v2.x is successfully connected to Nebula Graph v2.x, you can learn nGQL v2.x on the Console page or explore and analyze data on the Explore page.","title":"Connect to Nebula Graph"},{"location":"nebula-studio/install-configure/st-ug-connect/#connect_to_nebula_graph","text":"For Docker-based Studio v2.x, when it is started, you must configure it to connect to Nebula Graph v2.x. This article introduces how to connect Docker-based Studio v2.x to Nebula Graph v2.x.","title":"Connect to Nebula Graph"},{"location":"nebula-studio/install-configure/st-ug-connect/#prerequisites","text":"Before you connect Docker-based Studio v2.x to Nebula Graph v2.x, you must do a check of these: The Nebula Graph v2.x services and Studio v2.x are started. For more information, see Deploy Studio . You have the IP address and the port used by the Graph service of Nebula Graph v2.x. The default port is 9669 . NOTE : Run ifconfig or ipconfig on the machine to get the IP address.","title":"Prerequisites"},{"location":"nebula-studio/install-configure/st-ug-connect/#procedure","text":"To connect Docker-based Studio to Nebula Graph, follow these steps: On the Config Server page of Studio, configure these fields: Host : Enter the IP address and the port of the Graph service of Nebula Graph. The valid format is IP:port . The default port is 9669 . NOTE : When Nebula Graph and Studio are deployed on the same machine, you must enter the IP address of the machine, but not 127.0.0.1 or localhost , in the Host field. Username and Password : You can use user and password as the username and its password. After the configuration, click the Connect button. If you can see the Console page, Docker-based Studio is successfully connected to Nebula Graph. One session continues up to 30 minutes. If you do not operate Studio within 30 minutes, the active session will time out and you must connect to Nebula Graph again.","title":"Procedure"},{"location":"nebula-studio/install-configure/st-ug-connect/#next_to_do","text":"When Studio v2.x is successfully connected to Nebula Graph v2.x, you can learn nGQL v2.x on the Console page or explore and analyze data on the Explore page.","title":"Next to do"},{"location":"nebula-studio/install-configure/st-ug-deploy/","text":"Deploy Studio \u00b6 This article introduces how to deploy Docker-based Studio v2.x. Prerequisites \u00b6 Before you deploy Docker-based Studio v2.x, you must do a check of these: The Nebula Graph v2.x services are deployed and started. For more information, see Nebula Graph Database Manual . NOTE : Different methods are available for you to deploy Nebula Graph. If this is your first time to use Nebula Graph, we recommend that you use Docker Compose to deploy Nebula Graph. For more information, see Deploy Nebula Graph with Docker Compose . On the machine where Studio v2.x will run, Docker Compose is installed and started. For more information, see Docker Compose Documentation . Procedure \u00b6 To deploy and start Docker-based Studio v2.x, run these commands one by one: Download the configuration files for the deployment. git clone https://github.com/vesoft-inc/nebula-web-docker.git Change to the nebula-web-docker/v2 directory. cd nebula-web-docker/v2 Pull the Docker image of Studio v2.x. docker-compose pull Build and start Docker-based Studio v2.x. In this command, -d is to run the containers in the background. docker-compose up -d If these lines return, Docker-based Studio v2.x is deployed and started. Creating docker_importer_1 ... done Creating docker_client_1 ... done Creating docker_web_1 ... done Creating docker_nginx_1 ... done When Docker-based Studio v2.x is started, use http://ip address:7001 to get access to Studio v2.x. NOTE : Run ifconfig or ipconfig to get the IP address of the machine where Docker-based Studio is running. On the machine running Docker-based Studio, you can use http://localhost:7001 to get access to Studio. If you can see the Config Server page on the browser, Docker-based Studio is started successfully. Next to do \u00b6 On the Config Server page, connect Docker-based Studio to Nebula Graph. For more information, see Connect to Nebula Graph .","title":"Deploy Studio"},{"location":"nebula-studio/install-configure/st-ug-deploy/#deploy_studio","text":"This article introduces how to deploy Docker-based Studio v2.x.","title":"Deploy Studio"},{"location":"nebula-studio/install-configure/st-ug-deploy/#prerequisites","text":"Before you deploy Docker-based Studio v2.x, you must do a check of these: The Nebula Graph v2.x services are deployed and started. For more information, see Nebula Graph Database Manual . NOTE : Different methods are available for you to deploy Nebula Graph. If this is your first time to use Nebula Graph, we recommend that you use Docker Compose to deploy Nebula Graph. For more information, see Deploy Nebula Graph with Docker Compose . On the machine where Studio v2.x will run, Docker Compose is installed and started. For more information, see Docker Compose Documentation .","title":"Prerequisites"},{"location":"nebula-studio/install-configure/st-ug-deploy/#procedure","text":"To deploy and start Docker-based Studio v2.x, run these commands one by one: Download the configuration files for the deployment. git clone https://github.com/vesoft-inc/nebula-web-docker.git Change to the nebula-web-docker/v2 directory. cd nebula-web-docker/v2 Pull the Docker image of Studio v2.x. docker-compose pull Build and start Docker-based Studio v2.x. In this command, -d is to run the containers in the background. docker-compose up -d If these lines return, Docker-based Studio v2.x is deployed and started. Creating docker_importer_1 ... done Creating docker_client_1 ... done Creating docker_web_1 ... done Creating docker_nginx_1 ... done When Docker-based Studio v2.x is started, use http://ip address:7001 to get access to Studio v2.x. NOTE : Run ifconfig or ipconfig to get the IP address of the machine where Docker-based Studio is running. On the machine running Docker-based Studio, you can use http://localhost:7001 to get access to Studio. If you can see the Config Server page on the browser, Docker-based Studio is started successfully.","title":"Procedure"},{"location":"nebula-studio/install-configure/st-ug-deploy/#next_to_do","text":"On the Config Server page, connect Docker-based Studio to Nebula Graph. For more information, see Connect to Nebula Graph .","title":"Next to do"},{"location":"nebula-studio/manage-schema/st-ug-crud-edge-type/","text":"","title":"St ug crud edge type"},{"location":"nebula-studio/manage-schema/st-ug-crud-index/","text":"","title":"St ug crud index"},{"location":"nebula-studio/manage-schema/st-ug-crud-space/","text":"","title":"St ug crud space"},{"location":"nebula-studio/manage-schema/st-ug-crud-tag/","text":"","title":"St ug crud tag"},{"location":"nebula-studio/quick-start/st-ug-create-schema/","text":"Create a schema \u00b6 To batch import data into Nebula Graph, you must have a graph schema. You can create a schema on the Console page or on the Schema page of Studio. NOTE : You can use nebula-console to create a schema. For more information, see Deploy Nebula Graph with Docker Compose and Get started with Nebula Graph . Prerequisites \u00b6 To create a graph schema on Studio v2.x, you must do a check of these: Studio is connected to Nebula Graph v2.x. Your account has the privilege of GOD, ADMIN, or DBA. The schema is designed. A graph space is created. NOTE : If no graph space exists and your account has the GOD privilege, you can create a graph space on the Console page. For more information, see CREATE SPACE . In this example, we recommend that you set vid_type=FIXED_STRING(100) in the CREATE SPACE statement. Create a schema with Schema \u00b6 To create a schema on the Schema page, follow these steps: Create tags. For more information, see Operate tags . Create edge types. For more information, see Operate edge types . Create a schema with Console \u00b6 To create a schema on the Console page, follow these steps: In the toolbar, click the Console tab. In the Current Graph Space field, choose a graph space name. In this example, mooc_actions is used. In the input box, enter these statements one by one and click the button . // To create a tag named \"user\", with no property nebula> CREATE TAG user (); // To create a tag named \"course\", with one property nebula> CREATE TAG course (courseId int); // To create an edge type named \"action\", with seven properties nebula> CREATE EDGE action (actionId int, duration double, label bool, feature0 double, feature1 double, feature2 double, feature3 double); If the preceding statements are executed successfully, the schema is created. You can run the statements as follows to view the schema. // To list all the tags in the current graph space nebula> SHOW TAGS; // To list all the edge types in the current graph space nebula> SHOW EDGES; // To view the definition of the tags and edge types nebula> DESCRIBE TAG user; nebula> DESCRIBE TAG course; nebula> DESCRIBE EDGE action; If the schema is created successfully, in the result window, you can see the definition of the tags and edge types. Next to do \u00b6 When a schema is created, you can import data .","title":"Create a schema"},{"location":"nebula-studio/quick-start/st-ug-create-schema/#create_a_schema","text":"To batch import data into Nebula Graph, you must have a graph schema. You can create a schema on the Console page or on the Schema page of Studio. NOTE : You can use nebula-console to create a schema. For more information, see Deploy Nebula Graph with Docker Compose and Get started with Nebula Graph .","title":"Create a schema"},{"location":"nebula-studio/quick-start/st-ug-create-schema/#prerequisites","text":"To create a graph schema on Studio v2.x, you must do a check of these: Studio is connected to Nebula Graph v2.x. Your account has the privilege of GOD, ADMIN, or DBA. The schema is designed. A graph space is created. NOTE : If no graph space exists and your account has the GOD privilege, you can create a graph space on the Console page. For more information, see CREATE SPACE . In this example, we recommend that you set vid_type=FIXED_STRING(100) in the CREATE SPACE statement.","title":"Prerequisites"},{"location":"nebula-studio/quick-start/st-ug-create-schema/#create_a_schema_with_schema","text":"To create a schema on the Schema page, follow these steps: Create tags. For more information, see Operate tags . Create edge types. For more information, see Operate edge types .","title":"Create a schema with Schema"},{"location":"nebula-studio/quick-start/st-ug-create-schema/#create_a_schema_with_console","text":"To create a schema on the Console page, follow these steps: In the toolbar, click the Console tab. In the Current Graph Space field, choose a graph space name. In this example, mooc_actions is used. In the input box, enter these statements one by one and click the button . // To create a tag named \"user\", with no property nebula> CREATE TAG user (); // To create a tag named \"course\", with one property nebula> CREATE TAG course (courseId int); // To create an edge type named \"action\", with seven properties nebula> CREATE EDGE action (actionId int, duration double, label bool, feature0 double, feature1 double, feature2 double, feature3 double); If the preceding statements are executed successfully, the schema is created. You can run the statements as follows to view the schema. // To list all the tags in the current graph space nebula> SHOW TAGS; // To list all the edge types in the current graph space nebula> SHOW EDGES; // To view the definition of the tags and edge types nebula> DESCRIBE TAG user; nebula> DESCRIBE TAG course; nebula> DESCRIBE EDGE action; If the schema is created successfully, in the result window, you can see the definition of the tags and edge types.","title":"Create a schema with Console"},{"location":"nebula-studio/quick-start/st-ug-create-schema/#next_to_do","text":"When a schema is created, you can import data .","title":"Next to do"},{"location":"nebula-studio/quick-start/st-ug-explore/","text":"Query graph data \u00b6 When data is imported, you can use the Console page or the Explore page to query graph data. For example, if you want to query the properties of the course named History of Chinese Women Through Time , you can perform these optional operations: On the Console tab: Run FETCH PROP ON * \"History of Chinese Women Through Time\"; . The result window shows all the property information of this vertex. When the result returns, click the Open in Explore button and then you can view the vertex information in a visualized way. On the Explore tab: Click the Start with Vertices button. In the dialog box, enter History of Chinese Women Through Time and then click the Add button. On the board, you can see the vertex. Move your mouse pointer on the vertex to see the vertex details, as shown in the preceding figure.","title":"Query graph data"},{"location":"nebula-studio/quick-start/st-ug-explore/#query_graph_data","text":"When data is imported, you can use the Console page or the Explore page to query graph data. For example, if you want to query the properties of the course named History of Chinese Women Through Time , you can perform these optional operations: On the Console tab: Run FETCH PROP ON * \"History of Chinese Women Through Time\"; . The result window shows all the property information of this vertex. When the result returns, click the Open in Explore button and then you can view the vertex information in a visualized way. On the Explore tab: Click the Start with Vertices button. In the dialog box, enter History of Chinese Women Through Time and then click the Add button. On the board, you can see the vertex. Move your mouse pointer on the vertex to see the vertex details, as shown in the preceding figure.","title":"Query graph data"},{"location":"nebula-studio/quick-start/st-ug-import-data/","text":"Import data \u00b6 After CSV files of data and a schema are created, you can use the Import page to batch import vertex and edge data into Nebula Graph for graph exploration and data analysis. Prerequisites \u00b6 To batch import data, do a check of these: Studio v2.x is connected to Nebula Graph v2.x. A schema is created. CSV files for vertex and edge data separately are created. Your account has privilege of GOD, ADMIN, DBA, or USER. Procedure \u00b6 To batch import data, follow these steps: In the toolbar, click the Import tab. On the Select Space page, choose a graph space name. In this example, mooc_actions is used. And then click the Next button. On the Upload Files page, click the Upload Files button and then choose CSV files. In this example, user.csv , course.csv , and actions.csv are chosen. NOTE : You can choose multiple CSV files at the same time. On the Select Files page, do a check of the file size and click Preview or Delete in the Operations column to make sure that all source data is correct. And then click the Next button. On the Map Vertices page, click the + Bind Datasource button, and in the dialog box, choose a CSV file. In this example, user.csv or course.csv is chosen. In the DataSource X tab, click the + Tag button. In the vertexId section, do these operations: a. In the CSV Index column, click Mapping . b. In the dialog box, choose a column from the CSV file. In this example, the only one cloumn of user.csv is chosen to generate VIDs representing users and the courseName column of course.csv is chosen to generate VIDs representing courses. In the TAG 1 section, do these operations: a. In the TAG drop-down list, choose a tag name. In this example, user is used for the user.csv file, and course is used for the course.csv file. b. In the property list, click Mapping to choose a data column from the CSV file as the value of a property. In this example, no data is chosen for user . For the course tag, choose Column 0 for the courseId property and set its type to int . (Optional) If necessary, repeat Step 5 through Step 8 for more tags. When the configuration is done, click the Next button. When Config validation was successful prompts, data mapping for the vertices is successful. On the Map Edges page, click the + Bind Datasource button, and in the dialog box, choose a CSV file. In this example, the actions.csv file is chosen. In the Type drop-down list, choose an edge type name. In this example, action is chosen. In the property list, click Mapping to choose a column from the actions.csv file as values of a property for the edges. srcId and dstId are the VIDs of the source vertex and destination vertex of an edge. In this example, srcId must be set to the VIDs of the users and dstId must be set to the VIDs of the courses. rank is optional. When the configuration is done, click the Next button. On the Import page, click the Start Import button. On the log window, you can see the import progress. The consumed time depends on the data volume. During data import, you can click the Stop Import button to stop data import. When the log window shows information as follows, the data import is done. Next to do \u00b6 When the data are imported to Nebula Graph v2.x, you can query graph data .","title":"Import data"},{"location":"nebula-studio/quick-start/st-ug-import-data/#import_data","text":"After CSV files of data and a schema are created, you can use the Import page to batch import vertex and edge data into Nebula Graph for graph exploration and data analysis.","title":"Import data"},{"location":"nebula-studio/quick-start/st-ug-import-data/#prerequisites","text":"To batch import data, do a check of these: Studio v2.x is connected to Nebula Graph v2.x. A schema is created. CSV files for vertex and edge data separately are created. Your account has privilege of GOD, ADMIN, DBA, or USER.","title":"Prerequisites"},{"location":"nebula-studio/quick-start/st-ug-import-data/#procedure","text":"To batch import data, follow these steps: In the toolbar, click the Import tab. On the Select Space page, choose a graph space name. In this example, mooc_actions is used. And then click the Next button. On the Upload Files page, click the Upload Files button and then choose CSV files. In this example, user.csv , course.csv , and actions.csv are chosen. NOTE : You can choose multiple CSV files at the same time. On the Select Files page, do a check of the file size and click Preview or Delete in the Operations column to make sure that all source data is correct. And then click the Next button. On the Map Vertices page, click the + Bind Datasource button, and in the dialog box, choose a CSV file. In this example, user.csv or course.csv is chosen. In the DataSource X tab, click the + Tag button. In the vertexId section, do these operations: a. In the CSV Index column, click Mapping . b. In the dialog box, choose a column from the CSV file. In this example, the only one cloumn of user.csv is chosen to generate VIDs representing users and the courseName column of course.csv is chosen to generate VIDs representing courses. In the TAG 1 section, do these operations: a. In the TAG drop-down list, choose a tag name. In this example, user is used for the user.csv file, and course is used for the course.csv file. b. In the property list, click Mapping to choose a data column from the CSV file as the value of a property. In this example, no data is chosen for user . For the course tag, choose Column 0 for the courseId property and set its type to int . (Optional) If necessary, repeat Step 5 through Step 8 for more tags. When the configuration is done, click the Next button. When Config validation was successful prompts, data mapping for the vertices is successful. On the Map Edges page, click the + Bind Datasource button, and in the dialog box, choose a CSV file. In this example, the actions.csv file is chosen. In the Type drop-down list, choose an edge type name. In this example, action is chosen. In the property list, click Mapping to choose a column from the actions.csv file as values of a property for the edges. srcId and dstId are the VIDs of the source vertex and destination vertex of an edge. In this example, srcId must be set to the VIDs of the users and dstId must be set to the VIDs of the courses. rank is optional. When the configuration is done, click the Next button. On the Import page, click the Start Import button. On the log window, you can see the import progress. The consumed time depends on the data volume. During data import, you can click the Stop Import button to stop data import. When the log window shows information as follows, the data import is done.","title":"Procedure"},{"location":"nebula-studio/quick-start/st-ug-import-data/#next_to_do","text":"When the data are imported to Nebula Graph v2.x, you can query graph data .","title":"Next to do"},{"location":"nebula-studio/quick-start/st-ug-plan-schema/","text":"Design a schema \u00b6 To manipulate graph data in Nebula Graph with Studio, you must have a graph schema. This article introduces how to design a graph schema for Nebula Graph. A graph schema for Nebula Graph must have these essential elements: Tags (namely vertex types) and their properties. Edge types and their properties In this article, the Social Network: MOOC User Action Dataset and 97 distinct course names are used to introduce how to design a schema. This table gives all the essential elements of the schema. Element Name Property name (Data type) Description Tag user No property. Represents users of the specified MOOC platform. The userId values are used as VIDs of user vertices. Tag course courseId ( int ). Represents the courses on the specified MOOC platform. The courseName values are used as the VIDs of the course vertices. Edge type action - actionId ( int ) - duration ( double ): Represents the duration of an action measured in seconds from the beginning. Its values are equal to the timestamp values in the data source. - label ( bool ): Represents whether a user drops out after an action. TRUE indicates a drop-out action, FALSE otherwise. - feature0 ( double ) - feature1 ( double ) - feature2 ( double ) - feature3 ( double ) Represents actions taken by users on the specified MOOC platform. An action links a user and a course and the direction is from a user to a course. It has four features. This figure shows the relationship ( action ) between a user and a course on the MOOC platform.","title":"Design a schema"},{"location":"nebula-studio/quick-start/st-ug-plan-schema/#design_a_schema","text":"To manipulate graph data in Nebula Graph with Studio, you must have a graph schema. This article introduces how to design a graph schema for Nebula Graph. A graph schema for Nebula Graph must have these essential elements: Tags (namely vertex types) and their properties. Edge types and their properties In this article, the Social Network: MOOC User Action Dataset and 97 distinct course names are used to introduce how to design a schema. This table gives all the essential elements of the schema. Element Name Property name (Data type) Description Tag user No property. Represents users of the specified MOOC platform. The userId values are used as VIDs of user vertices. Tag course courseId ( int ). Represents the courses on the specified MOOC platform. The courseName values are used as the VIDs of the course vertices. Edge type action - actionId ( int ) - duration ( double ): Represents the duration of an action measured in seconds from the beginning. Its values are equal to the timestamp values in the data source. - label ( bool ): Represents whether a user drops out after an action. TRUE indicates a drop-out action, FALSE otherwise. - feature0 ( double ) - feature1 ( double ) - feature2 ( double ) - feature3 ( double ) Represents actions taken by users on the specified MOOC platform. An action links a user and a course and the direction is from a user to a course. It has four features. This figure shows the relationship ( action ) between a user and a course on the MOOC platform.","title":"Design a schema"},{"location":"nebula-studio/quick-start/st-ug-prepare-csv/","text":"Prepare CSV files \u00b6 With Studio, you can batch import vertex and edge data into Nebula Graph. Currently, only CSV files without headers and comma-separated data are supported. Each file represents vertex or edge data of one type. To create applicable CSV files, process the source data as follows: Generate CSV files for vertex and edge data: user.csv : Contains the vertices representing users with no property. The userId column can be used as the vertex IDs. course.csv : Contains the vertices representing courses with the courseId properties. The courseName column can be used as the vertex IDs. actions.csv contains: The edges representing actions with the actionId , label , duration , feature0 , feature1 , feature2 , and feature3 properties. For the label column, 1 is replaced with TRUE and 0 is replaced with FALSE . The userId column representing the source vertices of the edges. The courseName column representing the destination vertices of the edges. This figure shows an example of a CSV file with the header. Delete all the headers from the CSV files.","title":"Prepare CSV files"},{"location":"nebula-studio/quick-start/st-ug-prepare-csv/#prepare_csv_files","text":"With Studio, you can batch import vertex and edge data into Nebula Graph. Currently, only CSV files without headers and comma-separated data are supported. Each file represents vertex or edge data of one type. To create applicable CSV files, process the source data as follows: Generate CSV files for vertex and edge data: user.csv : Contains the vertices representing users with no property. The userId column can be used as the vertex IDs. course.csv : Contains the vertices representing courses with the courseId properties. The courseName column can be used as the vertex IDs. actions.csv contains: The edges representing actions with the actionId , label , duration , feature0 , feature1 , feature2 , and feature3 properties. For the label column, 1 is replaced with TRUE and 0 is replaced with FALSE . The userId column representing the source vertices of the edges. The courseName column representing the destination vertices of the edges. This figure shows an example of a CSV file with the header. Delete all the headers from the CSV files.","title":"Prepare CSV files"},{"location":"nebula-studio/use-console/st-ug-open-in-explore/","text":"Open in Explore \u00b6 With the Open in Explore function, you can run nGQL statements on the Console page to query vertex or edge data and then view the result on the Explore page in a visualized way. Prerequisites \u00b6 To use the Open in Explore function, you must do a check of these: The version of Studio is v2.0.0 or later. Studio is connected to Nebula Graph v2.x. A dataset exists in the database. Query and visualize edge data \u00b6 To query edge data on the Console page and then view the result on the Explore page, follow these steps: In the toolbar, click the Console tab. In the Current Graph Space field, choose a graph space name. In this example, mooc_actions is chosen. In the input box, enter an nGQL statement and click the button . NOTE : The query result must contain the VIDs of the source vertex and the destination vertex of an edge. Here is an nGQL statement example. nebula> MATCH (u:user {userId: 1}) - [:action] -> (c) RETURN u.userId AS UserID, c.courseName AS Course; NOTE : For more information about the MATCH syntax, see MATCH in nGQL User Guide . The query result gives the edges between User 1 and the courses that he/she takes on the MOOC platform, as shown in this figure. Click the Open in Explore button. In the dialog box, configure as follows: a. Click Edge Type . b. In the Edge Type field, enter an edge type name. In this example, action is used. c. In the Src ID field, choose a column name from the result table representing the VIDs of the source vertices. In this example, UserID is chosen. d. In the Dst ID field, choose a column name from the result table representing the VIDs of the destination vertices. In this example, Course is chosen. e. (Optional) If the result table contains the ranking information of the edges, in the Rank field, choose a column name representing the rank of the edges. If no ranking information exists in the result, leave the Rank field blank. f. When the configuration is done, click the Import button. If some data exists on the board of Explore , choose a method to insert data: Incremental Insertion : Click this button to add the result to the existing data on the board. Insert After Clear : Click this button to clear the existing data from the board and then add the data to the board. When the data is inserted, you can view the visualized representation of the edge data. Query and visualize vertex data \u00b6 To query vertex data on the Console page and then view the result on the Explore page, follow these steps: In the toolbar, click the Console tab. In the Current Graph Space field, choose a graph space name. In this example, mooc_actions is chosen. In the input box, enter an nGQL statement and click the button . NOTE : The query result must contain the VIDs of the vertices. Here is an nGQL statement example. nebula> GO FROM \"1\" OVER action YIELD action._dst AS Course; The query result gives the courses that the specified user took, as shown in this figure. Click the Open in Explore button. In the dialog box, configure as follows: a. Click Vertex . b. In the Vertex ID field, choose a column name from the result table representing the VIDs of the vertices. In this example, Course is chosen. c. When the configuration is done, click the Import button. If some data exists on the board of Explore , choose a method to insert data: Incremental Insertion : Click this button to add the queried result to the existing data on the board. Insert After Clear : Click this button to clear the existing data from the board and then add the data. When the data is inserted, you can view the visualized representation of the vertex data. Next to do \u00b6 On the Explore page, you can expand the board to explore and analyze graph data.","title":"Open in Explore"},{"location":"nebula-studio/use-console/st-ug-open-in-explore/#open_in_explore","text":"With the Open in Explore function, you can run nGQL statements on the Console page to query vertex or edge data and then view the result on the Explore page in a visualized way.","title":"Open in Explore"},{"location":"nebula-studio/use-console/st-ug-open-in-explore/#prerequisites","text":"To use the Open in Explore function, you must do a check of these: The version of Studio is v2.0.0 or later. Studio is connected to Nebula Graph v2.x. A dataset exists in the database.","title":"Prerequisites"},{"location":"nebula-studio/use-console/st-ug-open-in-explore/#query_and_visualize_edge_data","text":"To query edge data on the Console page and then view the result on the Explore page, follow these steps: In the toolbar, click the Console tab. In the Current Graph Space field, choose a graph space name. In this example, mooc_actions is chosen. In the input box, enter an nGQL statement and click the button . NOTE : The query result must contain the VIDs of the source vertex and the destination vertex of an edge. Here is an nGQL statement example. nebula> MATCH (u:user {userId: 1}) - [:action] -> (c) RETURN u.userId AS UserID, c.courseName AS Course; NOTE : For more information about the MATCH syntax, see MATCH in nGQL User Guide . The query result gives the edges between User 1 and the courses that he/she takes on the MOOC platform, as shown in this figure. Click the Open in Explore button. In the dialog box, configure as follows: a. Click Edge Type . b. In the Edge Type field, enter an edge type name. In this example, action is used. c. In the Src ID field, choose a column name from the result table representing the VIDs of the source vertices. In this example, UserID is chosen. d. In the Dst ID field, choose a column name from the result table representing the VIDs of the destination vertices. In this example, Course is chosen. e. (Optional) If the result table contains the ranking information of the edges, in the Rank field, choose a column name representing the rank of the edges. If no ranking information exists in the result, leave the Rank field blank. f. When the configuration is done, click the Import button. If some data exists on the board of Explore , choose a method to insert data: Incremental Insertion : Click this button to add the result to the existing data on the board. Insert After Clear : Click this button to clear the existing data from the board and then add the data to the board. When the data is inserted, you can view the visualized representation of the edge data.","title":"Query and visualize edge data"},{"location":"nebula-studio/use-console/st-ug-open-in-explore/#query_and_visualize_vertex_data","text":"To query vertex data on the Console page and then view the result on the Explore page, follow these steps: In the toolbar, click the Console tab. In the Current Graph Space field, choose a graph space name. In this example, mooc_actions is chosen. In the input box, enter an nGQL statement and click the button . NOTE : The query result must contain the VIDs of the vertices. Here is an nGQL statement example. nebula> GO FROM \"1\" OVER action YIELD action._dst AS Course; The query result gives the courses that the specified user took, as shown in this figure. Click the Open in Explore button. In the dialog box, configure as follows: a. Click Vertex . b. In the Vertex ID field, choose a column name from the result table representing the VIDs of the vertices. In this example, Course is chosen. c. When the configuration is done, click the Import button. If some data exists on the board of Explore , choose a method to insert data: Incremental Insertion : Click this button to add the queried result to the existing data on the board. Insert After Clear : Click this button to clear the existing data from the board and then add the data. When the data is inserted, you can view the visualized representation of the vertex data.","title":"Query and visualize vertex data"},{"location":"nebula-studio/use-console/st-ug-open-in-explore/#next_to_do","text":"On the Explore page, you can expand the board to explore and analyze graph data.","title":"Next to do"},{"location":"nebula-studio/use-console/st-ug-visualize-subgraph/","text":"View subgraphs \u00b6 With the View Subgraphs function, you can run a FIND SHORTEST | ALL PATH or a GET SUBGRAPH statement on the Console page and then view the result on the Explore page. Studio version \u00b6 Studio of v2.0.0 or later versions supports this function. To update the version, see Check updates . Prerequisites \u00b6 To use the View Subgraphs function, you must do a check of these: The version of Studio is v2.0.0 or later. Studio is connected to Nebula Graph v2.x. A dataset exists in the database. In the example of this article, the mooc_actions dataset is used. For more information, see Import data . Procedure \u00b6 To query the paths or subgraph on the Console page and then view them on the Explore page, follow these steps: In the navigation bar, click the Console tab. In the Current Graph Space dropdown list, choose a graph space name. In this example, mooc_actions is chosen. In the input box, enter a FIND SHORTEST PATH , FIND ALL PATH , or GET SUBGRAPH statement and click Run . Here is an nGQL statement example. // Run FIND ALL PATH nebula> FIND ALL PATH FROM \"1\",\"2\",\"4\",\"6\",\"42\" to \"History of Ecology\",\"Neurobiology\" OVER action; // Run FIND SHORTEST PATH nebula> FIND SHORTEST PATH FROM \"1\",\"2\",\"4\",\"6\",\"42\" to \"History of Ecology\",\"Neurobiology\" OVER action; // Run GET SUBGRAPH nebula> GET SUBGRAPH 1 STEPS FROM \"1\"; Take the FIND ALL PATH for example, the queried result gives all the paths from the specified user vertices to the course vertices, as shown in this figure. Click the View Subgraphs button. (Optional) If some data exists on the board of Explore , choose a method to insert data: - Incremental Insertion : Click this button to add the result to the existing data on the board. - Insert After Clear : Click this button to clear the existing data from the board and then add the data to the board. When the data is inserted, you can view the visualized representation of the paths. Next to do \u00b6 On the Explore page, you can expand the graph to explore and analyze graph data.","title":"View subgraphs"},{"location":"nebula-studio/use-console/st-ug-visualize-subgraph/#view_subgraphs","text":"With the View Subgraphs function, you can run a FIND SHORTEST | ALL PATH or a GET SUBGRAPH statement on the Console page and then view the result on the Explore page.","title":"View subgraphs"},{"location":"nebula-studio/use-console/st-ug-visualize-subgraph/#studio_version","text":"Studio of v2.0.0 or later versions supports this function. To update the version, see Check updates .","title":"Studio version"},{"location":"nebula-studio/use-console/st-ug-visualize-subgraph/#prerequisites","text":"To use the View Subgraphs function, you must do a check of these: The version of Studio is v2.0.0 or later. Studio is connected to Nebula Graph v2.x. A dataset exists in the database. In the example of this article, the mooc_actions dataset is used. For more information, see Import data .","title":"Prerequisites"},{"location":"nebula-studio/use-console/st-ug-visualize-subgraph/#procedure","text":"To query the paths or subgraph on the Console page and then view them on the Explore page, follow these steps: In the navigation bar, click the Console tab. In the Current Graph Space dropdown list, choose a graph space name. In this example, mooc_actions is chosen. In the input box, enter a FIND SHORTEST PATH , FIND ALL PATH , or GET SUBGRAPH statement and click Run . Here is an nGQL statement example. // Run FIND ALL PATH nebula> FIND ALL PATH FROM \"1\",\"2\",\"4\",\"6\",\"42\" to \"History of Ecology\",\"Neurobiology\" OVER action; // Run FIND SHORTEST PATH nebula> FIND SHORTEST PATH FROM \"1\",\"2\",\"4\",\"6\",\"42\" to \"History of Ecology\",\"Neurobiology\" OVER action; // Run GET SUBGRAPH nebula> GET SUBGRAPH 1 STEPS FROM \"1\"; Take the FIND ALL PATH for example, the queried result gives all the paths from the specified user vertices to the course vertices, as shown in this figure. Click the View Subgraphs button. (Optional) If some data exists on the board of Explore , choose a method to insert data: - Incremental Insertion : Click this button to add the result to the existing data on the board. - Insert After Clear : Click this button to clear the existing data from the board and then add the data to the board. When the data is inserted, you can view the visualized representation of the paths.","title":"Procedure"},{"location":"nebula-studio/use-console/st-ug-visualize-subgraph/#next_to_do","text":"On the Explore page, you can expand the graph to explore and analyze graph data.","title":"Next to do"}]}