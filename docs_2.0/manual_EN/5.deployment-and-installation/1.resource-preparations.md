# Prepare resources for compiling, installing, and running Nebula Graph

This topic describes the requirements and suggestions for compiling and installing Nebula Graph, as well as how to estimate the resource you need to reserve for running a Nebula Graph cluster.

## Reading guide

If you are reading this topic with the questions listed below, click them to jump to their answers.

* [What do I need to compile Nebula Graph?](#requirements-for-compiling-nebula-graph)
* [What do I need to run Nebula Graph in a test environment?](#requirements-and-suggestions-for-installing-nebula-graph-in-test-environments)
* [What do I need to run Nebula Graph in a production environment?](#requirements-and-suggestions-for-installing-nebula-graph-in-production-environments)
* [How much memory and storage do I need to reserve for my Nebula Graph cluster?](#reserve-resources-for-running-a-nebula-graph-cluster)
* [How to optimize the configuration for HDD and Gigabit Networks?](#optimize-the-configuration-for-hdd-and-gigabit-networks)

## Requirements for compiling Nebula Graph

### Hardware requirements for compiling Nebula Graph

| Item             | Requirement |
| ---------------- | ----------- |
| CPU architecture | x86_64      |
| Memory           | 4 GB        |
| Storage          | 10 GB, SSD  |

### Supported operating systems for compiling Nebula Graph

For now, we can only compile Nebula Graph in the Linux system. We recommend that you use any Linux system with kernel version 2.6.32 or above.

### Software requirements for compiling Nebula Graph

You must have the correct version of the software listed below to compile Nebula Graph.

| Software         | Version                | Note                                                      |
| ---------------- | ---------------------- | --------------------------------------------------------- |
| glibc            | 2.12 or above          | You can run `ldd --version` to check the glibc version.   |
| make             | Any mainstream version | -                                                         |
| m4               | Any mainstream version | -                                                         |
| git              | Any mainstream version | -                                                         |
| wget             | Any mainstream version | -                                                         |
| unzip            | Any mainstream version | -                                                         |
| xz               | Any mainstream version | -                                                         |
| readline-devel   | Any mainstream version | -                                                         |
| ncurses-devel    | Any mainstream version | -                                                         |
| zlid-devel       | Any mainstream version | -                                                         |
| gcc              | 7.1.0 or above         | You can run `gcc -v` to check the gcc version.            |
| gcc-c++          | Any mainstream version | -                                                         |
| cmake            | 3.5.0 or above         | You can run `cmake --version` to check the cmake version. |
| gettext          | Any mainstream version | -                                                         |
| curl             | Any mainstream version | -                                                         |
| redhat-lsb-core  | Any mainstream version | -                                                         |
| libstdc++-static | Any mainstream version | Only needed in CentOS 8+, RedHat 8+, and Fedora systems.  |
| libasan          | Any mainstream version | Only needed in CentOS 8+, RedHat 8+, and Fedora systems.  |

## Requirements and suggestions for installing Nebula Graph in test environments

### Hardware requirements for test environments

| Item               | Requirement |
| ------------------ | ----------- |
| CPU architecture   | x86_64      |
| Number of CPU core | 4           |
| Memory             | 8 GB        |
| Storage            | 100 GB, SSD |

### Supported operating systems for test environments

For now, we can only install Nebula Graph in the Linux system. To install Nebula Graph in a test environment, we recommend that you use any Linux system with kernel version 3.9 or above.

You can adjust some of the kernel parameters to better accommodate the need for running Nebula Graph. For more information, see [Optimize Linux kernel configuration](TODO) [TODO].

### Suggested service architecture for test environments

| Process                                   | Suggested number |
| ----------------------------------------- | ---------------- |
| metad (the metadata service process)      | 1                |
| storaged (the storage service process)    | 1 or more        |
| graphd (the query engine service process) | 1 or more        |

For example, for a single-machine environment, you can deploy 1 metad, 1 storaged, and 1 graphd processes in the machine.

For a more common environment, such as a cluster of 3 machines (named as A, B, and C), you can deploy Nebula Graph as follows:

| Machine name | Number of metad | Number of storaged | Number of graphd |
| ------------ | --------------- | ------------------ | ---------------- |
| A            | 1               | 1                  | 1                |
| B            | None            | 1                  | 1                |
| C            | None            | 1                  | 1                |

## Requirements and suggestions for installing Nebula Graph in production environments

### Hardware requirements for production environments

| Item               | Requirement          |
| ------------------ | -------------------- |
| CPU architecture   | x86_64               |
| Number of CPU core | 48                   |
| Memory             | 96 GB                |
| Storage            | 2 * 900 GB, NVMe SSD |

### Supported operating systems for production environments

For now, we can only install Nebula Graph in the Linux system. To install Nebula Graph in a production environment, we recommend that you use any Linux system with kernel version 3.9 or above.

You can adjust some of the kernel parameters to better accommodate the need for running Nebula Graph. For more information, see [Optimize Linux kernel configuration](TODO) [TODO].

### Suggested service architecture for production environments

| Process                                   | Suggested number |
| ----------------------------------------- | ---------------- |
| metad (the metadata service process)      | 3                |
| storaged (the storage service process)    | 3 or more        |
| graphd (the query engine service process) | 3 or more        |

Each metad process automatically creates and maintains a copy of the metadata. Usually, you only need 3 metad processes. The number of storaged processes does not affect the number of graph space copies.

You can deploy multiple processes on a single machine. For example, on a cluster of 5 machines (named as A, B, C, D, and E), you can deploy Nebula Graph as follows:

> **WARNING**: Do not deploy a cluster across IDCs.

| Machine name | Number of metad | Number of storaged | Number of graphd |
| ------------ | --------------- | ------------------ | ---------------- |
| A            | 1               | 1                  | 1                |
| B            | 1               | 1                  | 1                |
| C            | 1               | 1                  | 1                |
| D            | None            | 1                  | 1                |
| E            | None            | 1                  | 1                |

## Reserve resources for running a Nebula Graph cluster

You can estimate the memory, storage, and partition number needed for a Nebula Graph cluster of 3 replicas as follows.

| Resource                               | How to estimate                                                                                                                                                                |
| -------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| Storage for a cluster                  | _the sum of edge number and vertex number_ * _average bytes of attributes_ * 6 * 120%                                                                                          |
| Memory for a cluster                   | [_the sum of edge number and vertex number_ * 15 bytes + _the number of RocksDB instances_ * (_write_buffer_size_ * _max_write_buffer_number_) + _rocksdb_block_cache_] * 120% |
| Number of partitions for a graph space | _the number of disks in the cluster_ * _disk_partition_num_multiplier_                                                                                                                |

* Question 1: Why do we multiply the storage and memory by 120%?

    Answer: The extra 20% is for buffer.

* Question 2: How to get the number of RocksDB instances?

    Answer: Each directory in the `--data_path` item in the `etc/nebula-storaged.conf` file corresponds to a RocksDB instance. Count the number of directories to get the RocksDB instance number.

    > **NOTE**: You can decrease the memory size occupied by the bloom filter by adding `--enable_partitioned_index_filter=true` in `etc/nebula-storaged.conf`. But it may decrease the read performance in some random-seek cases.

* Question 3: What is the _disk_partition_num_multiplier_?

    Answer: _disk_partition_num_multiplier_ is a value between 2 to 10, the better performance of the hard disk, the larger the value. Use 2 for HDD.

## Optimize the configuration for HDD

[This part might be moved to the configuration doc map later.]

Nebula Graph is intended for NVMe SSD, but if you don't have a choice, optimizing the configuration as follows may better accommodate HDD.

* etc/nebula-storage.conf:

  * `--raft_rpc_timeout_ms` = 5000 ~ 10000
  * `--rocksdb_batch_size` = 4096 ~ 16384
  * `--heartbeat_interval_secs` = 30 ~ 60
  * `--raft_heartbeat_interval_secs` = 30 ~ 60

* etc/nebula-meta.conf:

    ```shell
    --heartbeat_interval_secs is the same as etc/nebula-storage.conf
    ```

* Spark Writer:

    ```JSON
    rate: {
        timeout: 5000 to 10000
        }
    ```

* go-importer:

  * `batchSize`: 10 to 50
  * `concurrency`: 1 to 10
  * `channelBufferSize`: 100 to 500
