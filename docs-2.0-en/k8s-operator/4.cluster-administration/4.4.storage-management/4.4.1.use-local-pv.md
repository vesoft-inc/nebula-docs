
# Use Local Persistent Volumes in a NebulaGraph cluster

Local Persistent Volumes in K8s store container data directly using the node's local disk directory. Compared with network storage, Local Persistent Volumes provide higher IOPS and lower read and write latency, which is suitable for data-intensive applications. This topic describes how to use Local Persistent Volumes in NebulaGraph clusters and how to handle node failures while using Local Persistent Volumes.

While using Local Persistent Volumes can enhance performance, it's essential to note that, unlike network storage, local storage does not support automatic backup. In the event of a node failure, all data in local storage may be lost. Therefore, the utilization of Local Persistent Volumes involves a trade-off among service availability, data persistence, and flexibility.


## Background

In K8s, data persistence mainly relies on [Persistent Volume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/). PVs provided by K8s can be broadly categorized into two types: local storage and network storage. See [Types of Persistent Volumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#types-of-persistent-volumes) for details.


## Prerequisites

NebulaGraph Operator is installed. For details, see [Install NebulaGraph Operator](../../2.get-started/2.1.install-operator.md).

## Use a Local Persistent Volume

Using a Local Persistent Volume involves configuring local storage, creating a Local Persistent Volume, and other steps. The [Local Persistence Volume Static Provisioner](https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner#overview) tool automates the management of Local Persistent Volumes. To use Local Persistent Volumes, storage disks need to be configured manually on each node. After the storage disks are placed in a specific directory, Local Persistence Volume Static Provisioner automatically creates the appropriate PVs for them. When the disk is no longer needed, the PVC can be deleted. The external static provisioner will clean up the disk and make the PV available for use again.


1. Prepare and configure local storage disks.

The preparation and configuration differ depending on how disks are mounted to nodes. For details, see [Prepare and set up local volumes in discovery directory](https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner/blob/master/docs/operations.md#prepare-and-set-up-local-volumes-in-discovery-directory).
  
 
2. Create a StorageClass with the example file `default_example_storageclass.yaml`.

  ```bash
  kubectl create -f deployment/kubernetes/example/default_example_storageclass.yaml
  ```
  
  The configuration of the StorageClass is as follows:

  ```yaml
  # Only create this for K8s 1.9+
  apiVersion: storage.k8s.io/v1
  kind: StorageClass
  metadata:
    name: local-storage
  provisioner: kubernetes.io/no-provisioner
  volumeBindingMode: WaitForFirstConsumer
  # Supported policies: Delete, Retain
  reclaimPolicy: Delete
  ```
  
  !!! note

        To use a Local Persistent Volume, the `volumeBindingMode` must be set to `WaitForFirstConsumer` in the StorageClass configuration. For details, see [Creating a StorageClass](https://kubernetes.io/blog/2019/04/04/kubernetes-1.14-local-persistent-volumes-ga/#how-to-use-a-local-persistent-volume)ã€‚
    

3. [Deploy Local Persistence Volume Static Provisioner](https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner/blob/master/docs/getting-started.md#step-3-creating-local-persistent-volumes).

4. Verify that the Local Persistent Volume is automatically created.
   
  ```bash
  $ kubectl get pv
  NAME                CAPACITY    ACCESSMODES   RECLAIMPOLICY   STATUS      CLAIM     STORAGECLASS    REASON    AGE
  local-pv-ce05be60   1024220Ki   RWO           Delete          Available             local-storage             26s

  $ kubectl describe pv local-pv-ce05be60
  Name:		local-pv-ce05be60
  Labels:		<none>
  Annotations:	pv.kubernetes.io/provisioned-by=local-volume-provisioner-minikube-18f57fb2-a186-11e7-b543-080027d51893
  StorageClass:	local-storage
  Status:		Available
  Claim:
  Reclaim Policy:	Delete
  Access Modes:	RWO
  Capacity:	1024220Ki
  NodeAffinity:
    Required Terms:
        Term 0:  kubernetes.io/hostname in [my-node]
  Message:
  Source:
      Type:	LocalVolume (a persistent volume backed by local storage on a node)
      Path:	/mnt/disks/vol1
  Events:		<none>
  ```

5. Bind the PVC and StorageClass defined by `spec.storaged.dataVolumeClaims` or `spec.metad.dataVolumeClaim` in the cluster configuration file to the created Local Persistent Volume. For specific steps, see [Create a NebulaGraph cluster](../4.1.installation/4.1.1.cluster-install.md).
  
## Failover for Local Persistent Volumes

When using network storage (e.g., AWS EBS, Google Cloud Persistent Disk, Azure Disk Storage, Ceph, NFS, etc.) as a PV, the storage resource is independent of any particular node. Therefore, the storage resource can be mounted and used by Pods regardless of the node to which the Pods are scheduled. However, when using a local storage disk as a PV, the storage resource can only be used by Pods on a specific node due to [nodeAffinity](https://kubernetes.io/blog/2018/04/13/local-persistent-volumes-beta/#creating-a-local-persistent-volume).

The Storage service of NebulaGraph supports data redundancy, which allows you to set multiple odd-numbered partition replicas. When a node fails, the associated partition is automatically transferred to a healthy node. However, Storage Pods using Local Persistent Volumes cannot run on other nodes due to the node affinity setting and must wait for the node to recover. To run on another node, the Pods must be unbound from the associated Local Persistent Volume.

NebulaGraph Operator supports automatic failover in the event of a node failure while using Local Persistent Volumes. By setting `spec.enableAutoFailover` to `true` in the cluster configuration file, Pods are automatically unbound from the Local Persistent Volume, allowing the Pods to run on another node.

Example configuration:

```yaml
...
spec:
  # Enable automatic failover for Local PV.
  enableAutoFailover: true
  # The time to wait for the Storage service to be in the `OFFLINE` status
  # before automatic failover. 
  # The default value is 5 minutes.
  # If the Storage service recovers to the `ONLINE` status during this period,
  # failover will not be triggered.
  failoverPeriod: "2m"
  ...
```