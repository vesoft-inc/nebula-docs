{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to NebulaGraph 3.5.0 Documentation","text":"<p>Note</p> <p>This manual is revised on 2023-11-20, with GitHub commit 39a361ddb1.</p> <p>Compatibility</p> <p>In the version of NebulaGraph 3.2, the vertex without tags is allowed. But since NebulaGraph 3.3.0, the vertex without tags is not supported by default.</p> <p>NebulaGraph is a distributed, scalable, and lightning-fast graph database. It is the optimal solution in the world capable of hosting graphs with dozens of billions of vertices (nodes) and trillions of edges (relationships) with millisecond latency.</p>"},{"location":"#getting_started","title":"Getting started","text":"<ul> <li>Learning path &amp; Get NebulaGraph Certifications</li> <li>What is Nebula\u00a0Graph</li> <li>Quick start</li> <li>Preparations before deployment</li> <li>nGQL cheatsheet</li> <li>FAQ</li> <li>Ecosystem Tools</li> </ul>"},{"location":"#release_notes","title":"Release notes","text":"<ul> <li>NebulaGraph Community Edition 3.5.0</li> </ul> <ul> <li>NebulaGraph Studio</li> <li>NebulaGraph Explorer</li> <li>NebulaGraph Dashboard Community Edition</li> <li>NebulaGraph Dashboard Enterprise Edition</li> </ul>"},{"location":"#other_sources","title":"Other Sources","text":"<ul> <li>To cite NebulaGraph</li> <li>NebulaGraph Homepage</li> <li>Forum</li> <li>Blogs</li> <li>Videos</li> <li>Chinese Docs</li> </ul>"},{"location":"#symbols_used_in_this_manual","title":"Symbols used in this manual","text":"<p>Note</p> <p>Additional information or operation-related notes.</p> <p>Caution</p> <p>Cautions that need strict observation. If not, systematic breakdown, data loss, and security issues may happen.</p> <p>Danger</p> <p>Operations that may cause danger. If not observed, systematic breakdown, data loss, and security issues will happen.</p> <p>Performance</p> <p>Operations that merit attention as for performance enhancement.</p> <p>Faq</p> <p>Frequently asked questions.</p> <p>Compatibility</p> <p>The compatibility notes between nGQL and openCypher, or between the current version of nGQL and its prior ones. </p> <p>Enterpriseonly</p> <p>Differences between the NebulaGraph Community and Enterprise editions.</p>"},{"location":"#modify_errors","title":"Modify errors","text":"<p>This NebulaGraph manual is written in the Markdown language. Users can click the pencil sign on the upper right side of each document title and modify errors.</p>"},{"location":"nebula-bench/","title":"NebulaGraph Bench","text":"<p>NebulaGraph Bench is a performance test tool for NebulaGraph using the LDBC data set.</p>"},{"location":"nebula-bench/#scenario","title":"Scenario","text":"<ul> <li>Generate test data and import NebulaGraph.</li> </ul> <ul> <li>Performance testing in the NebulaGraph cluster.</li> </ul>"},{"location":"nebula-bench/#release_note","title":"Release note","text":"<p>Release</p>"},{"location":"nebula-bench/#test_process","title":"Test process","text":"<p>For detailed usage instructions, see NebulaGraph Bench.</p>"},{"location":"nebula-console/","title":"NebulaGraph Console","text":"<p>NebulaGraph Console is a native CLI client for NebulaGraph. It can be used to connect a NebulaGraph cluster and execute queries. It also supports special commands to manage parameters, export query results, import test datasets, etc.</p>"},{"location":"nebula-console/#compatibility_with_nebulagraph","title":"Compatibility with NebulaGraph","text":"<p>See github.</p>"},{"location":"nebula-console/#obtain_nebulagraph_console","title":"Obtain NebulaGraph Console","text":"<p>You can obtain NebulaGraph Console in the following ways:</p> <ul> <li>Download the binary file from the GitHub releases page.</li> </ul> <ul> <li>Compile the source code to obtain the binary file. For more information, see Install from source code.</li> </ul>"},{"location":"nebula-console/#nebulagraph_console_functions","title":"NebulaGraph Console functions","text":""},{"location":"nebula-console/#connect_to_nebulagraph","title":"Connect to NebulaGraph","text":"<p>To connect to NebulaGraph with the <code>nebula-console</code> file, use the following syntax:</p> <pre><code>&lt;path_of_console&gt; -addr &lt;ip&gt; -port &lt;port&gt; -u &lt;username&gt; -p &lt;password&gt;\n</code></pre> <p><code>path_of_console</code> indicates the storage path of the NebulaGraph Console binary file.</p> <p>Parameter descriptions are as follows:</p> Parameter Description <code>-h/-help</code> Shows the help menu. <code>-addr/-address</code> Sets the IP address of the Graph service. The default address is 127.0.0.1.  <code>-P/-port</code> Sets the port number of the graphd service. The default port number is 9669. <code>-u/-user</code> Sets the username of your NebulaGraph account. Before enabling authentication, you can use any existing username. The default username is <code>root</code>. <code>-p/-password</code> Sets the password of your NebulaGraph account. Before enabling authentication, you can use any characters as the password. <code>-t/-timeout</code> Sets an integer-type timeout threshold of the connection. The unit is millisecond. The default value is 120. <code>-e/-eval</code> Sets a string-type nGQL statement. The nGQL statement is executed once the connection succeeds. The connection stops after the result is returned. <code>-f/-file</code> Sets the path of an nGQL file. The nGQL statements in the file are executed once the connection succeeds. The result will be returned and the connection stops then. <code>-enable_ssl</code> Enables SSL encryption when connecting to NebulaGraph. <code>-ssl_root_ca_path</code> Sets the storage path of the certification authority file. <code>-ssl_cert_path</code> Sets the storage path of the certificate file. <code>-ssl_private_key_path</code> Sets the storage path of the private key file. <p>For information on more parameters, see the project repository.</p> <p>For example, to connect to the Graph Service deployed on 192.168.10.8, run the following command:</p> <pre><code>./nebula-console -addr 192.168.10.8 -port 9669 -u root -p thisisapassword\n</code></pre>"},{"location":"nebula-console/#manage_parameters","title":"Manage parameters","text":"<p>You can save parameters for parameterized queries.</p> <p>Note</p> <ul> <li>Setting a parameter as a VID in a query is not supported.</li> </ul> <ul> <li>Parameters are not supported in <code>SAMPLE</code> clauses.</li> </ul> <ul> <li>Parameters are deleted when their sessions are released.</li> </ul> <ul> <li> <p>The command to save a parameter is as follows:</p> <pre><code>nebula&gt; :param &lt;param_name&gt; =&gt; &lt;param_value&gt;;\n</code></pre> <p>The example is as follows:</p> <pre><code>nebula&gt; :param p1 =&gt; \"Tim Duncan\";\nnebula&gt; MATCH (v:player{name:$p1})-[:follow]-&gt;(n)  RETURN v,n;\n+----------------------------------------------------+-------------------------------------------------------+\n| v                                                  | n                                                     |\n+----------------------------------------------------+-------------------------------------------------------+\n| (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | (\"player125\" :player{age: 41, name: \"Manu Ginobili\"}) |\n| (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | (\"player101\" :player{age: 36, name: \"Tony Parker\"})   |\n+----------------------------------------------------+-------------------------------------------------------+\nnebula&gt; :param p2 =&gt; {\"a\":3,\"b\":false,\"c\":\"Tim Duncan\"};\nnebula&gt; RETURN $p2.b AS b;\n+-------+\n| b     |\n+-------+\n| false |\n+-------+\n</code></pre> </li> </ul> <ul> <li> <p>The command to view the saved parameters is as follows:</p> <pre><code>nebula&gt; :params;\n</code></pre> </li> </ul> <ul> <li> <p>The command to view the specified parameters is as follows:</p> <pre><code>nebula&gt; :params &lt;param_name&gt;;\n</code></pre> </li> </ul> <ul> <li> <p>The command to delete a specified parameter is as follows:</p> <pre><code>nebula&gt; :param &lt;param_name&gt; =&gt;;\n</code></pre> </li> </ul>"},{"location":"nebula-console/#export_query_results","title":"Export query results","text":"<p>Export query results,  which can be saved as a CSV file, DOT file, and a format of Profile or Explain.</p> <p>Note</p> <ul> <li>The exported file is stored in the working directory, i.e., what the linux command <code>pwd</code> shows.</li> </ul> <ul> <li>This command only works for the next query statement.</li> </ul> <ul> <li>You can copy the contents of the DOT file and paste them in GraphvizOnline to generate a visualized execution plan.</li> </ul> <ul> <li> <p>The command to export a csv file is as follows:</p> <pre><code>nebula&gt; :CSV &lt;file_name.csv&gt;;\n</code></pre> </li> </ul> <ul> <li> <p>The command to export a DOT file is as follows:</p> <pre><code>nebula&gt; :dot &lt;file_name.dot&gt;\n</code></pre> <p>The example is as follows:</p> <pre><code>nebula&gt; :dot a.dot\nnebula&gt; PROFILE FORMAT=\"dot\" GO FROM \"player100\" OVER follow;\n</code></pre> </li> </ul> <ul> <li> <p>The command to export a PROFILE or EXPLAIN format is as follows: </p> <p><pre><code>nebula&gt; :profile &lt;file_name&gt;;\n</code></pre> or</p> <pre><code>nebula&gt; :explain &lt;file_name&gt;;\n</code></pre> <p>Note</p> <p>The text file output by the above command is the preferred way to report issues in GitHub and execution plans in forums, and for graph query tuning because it has more information and is more readable than a screenshot or CSV file in Studio.</p> <p>The example is as follows:</p> <pre><code>nebula&gt; :profile profile.log\nnebula&gt; PROFILE GO FROM \"player102\" OVER serve YIELD dst(edge);\nnebula&gt; :profile profile.dot\nnebula&gt; PROFILE FORMAT=\"dot\" GO FROM \"player102\" OVER serve YIELD dst(edge);\nnebula&gt; :explain explain.log\nnebula&gt; EXPLAIN GO FROM \"player102\" OVER serve YIELD dst(edge);\n</code></pre> </li> </ul>"},{"location":"nebula-console/#import_a_testing_dataset","title":"Import a testing dataset","text":"<p>The testing dataset is named <code>basketballplayer</code>. To view details about the schema and data, use the corresponding <code>SHOW</code> command.</p> <p>The command to import a testing dataset is as follows:</p> <pre><code>nebula&gt; :play basketballplayer\n</code></pre>"},{"location":"nebula-console/#run_a_command_multiple_times","title":"Run a command multiple times","text":"<p>To run a command multiple times, use the following command:</p> <pre><code>nebula&gt; :repeat N\n</code></pre> <p>The example is as follows:</p> <pre><code>nebula&gt; :repeat 3\nnebula&gt; GO FROM \"player100\" OVER follow YIELD dst(edge);\n+-------------+\n| dst(EDGE)   |\n+-------------+\n| \"player101\" |\n| \"player125\" |\n+-------------+\nGot 2 rows (time spent 2602/3214 us)\n\nFri, 20 Aug 2021 06:36:05 UTC\n\n+-------------+\n| dst(EDGE)   |\n+-------------+\n| \"player101\" |\n| \"player125\" |\n+-------------+\nGot 2 rows (time spent 583/849 us)\n\nFri, 20 Aug 2021 06:36:05 UTC\n\n+-------------+\n| dst(EDGE)   |\n+-------------+\n| \"player101\" |\n| \"player125\" |\n+-------------+\nGot 2 rows (time spent 496/671 us)\n\nFri, 20 Aug 2021 06:36:05 UTC\n\nExecuted 3 times, (total time spent 3681/4734 us), (average time spent 1227/1578 us)\n</code></pre>"},{"location":"nebula-console/#sleep","title":"Sleep","text":"<p>This command will make NebulaGraph Console sleep for N seconds. The schema is altered in an async way and takes effect in the next heartbeat cycle. Therefore, this command is usually used when altering schema. The command is as follows:</p> <pre><code>nebula&gt; :sleep N\n</code></pre>"},{"location":"nebula-console/#disconnect_nebulagraph_console_from_nebulagraph","title":"Disconnect NebulaGraph Console from NebulaGraph","text":"<p>You can use <code>:EXIT</code> or <code>:QUIT</code> to disconnect from NebulaGraph. For convenience, NebulaGraph Console supports using these commands in lower case without the colon (\":\"), such as <code>quit</code>.</p> <p>The example is as follows:</p> <pre><code>nebula&gt; :QUIT\n\nBye root!\n</code></pre>"},{"location":"nebula-flink-connector/","title":"NebulaGraph Flink Connector","text":"<p>NebulaGraph Flink Connector is a connector that helps Flink users quickly access NebulaGraph. NebulaGraph Flink Connector supports reading data from the NebulaGraph database or writing other external data to the NebulaGraph database.</p> <p>For more information, see NebulaGraph Flink Connector.</p>"},{"location":"nebula-flink-connector/#use_cases","title":"Use cases","text":"<p>NebulaGraph Flink Connector applies to the following scenarios:</p> <ul> <li>Migrate data between different NebulaGraph clusters.</li> </ul> <ul> <li>Migrate data between different graph spaces in the same NebulaGraph cluster.</li> </ul> <ul> <li>Migrate data between NebulaGraph and other data sources.</li> </ul>"},{"location":"nebula-flink-connector/#release_note","title":"Release note","text":"<p>Release</p>"},{"location":"nebula-spark-connector/","title":"NebulaGraph Spark Connector","text":"<p>NebulaGraph Spark Connector is a Spark connector application for reading and writing NebulaGraph data in Spark standard format. NebulaGraph Spark Connector consists of two parts: Reader and Writer.</p> <ul> <li> <p>Reader</p> <p>Provides a Spark SQL interface. This interface can be used to read NebulaGraph data. It reads one vertex or edge type data at a time and assemble the result into a Spark DataFrame.</p> </li> </ul> <ul> <li> <p>Writer</p> <p>Provides a Spark SQL interface. This interface can be used to write DataFrames into NebulaGraph in a row-by-row or batch-import way.</p> </li> </ul> <p>For more information, see NebulaGraph Spark Connector.</p>"},{"location":"nebula-spark-connector/#version_compatibility","title":"Version compatibility","text":"<p>The correspondence between the NebulaGraph Spark Connector version, the NebulaGraph core version and the Spark version is as follows.</p> Spark Connector version NebulaGraph version Spark version nebula-spark-connector_3.0-3.0-SNAPSHOT.jar nightly 3.x nebula-spark-connector_2.2-3.0-SNAPSHOT.jar nightly 2.2.x nebula-spark-connector-3.0-SNAPSHOT.jar nightly 2.4.x nebula-spark-connector_2.2-3.4.0.jar 3.x 2.2.x nebula-spark-connector-3.4.0.jar 3.x 2.4.x nebula-spark-connector_2.2-3.3.0.jar 3.x 2.2.x nebula-spark-connector-3.3.0.jar 3.x 2.4.x nebula-spark-connector-3.0.0.jar 3.x 2.4.x nebula-spark-connector-2.6.1.jar 2.6.0, 2.6.1 2.4.x nebula-spark-connector-2.6.0.jar 2.6.0, 2.6.1 2.4.x nebula-spark-connector-2.5.1.jar 2.5.0, 2.5.1 2.4.x nebula-spark-connector-2.5.0.jar 2.5.0, 2.5.1 2.4.x nebula-spark-connector-2.1.0.jar 2.0.0, 2.0.1 2.4.x nebula-spark-connector-2.0.1.jar 2.0.0, 2.0.1 2.4.x nebula-spark-connector-2.0.0.jar 2.0.0, 2.0.1 2.4.x"},{"location":"nebula-spark-connector/#use_cases","title":"Use cases","text":"<p>NebulaGraph Spark Connector applies to the following scenarios:</p> <ul> <li>Migrate data between different NebulaGraph clusters.</li> </ul> <ul> <li>Migrate data between different graph spaces in the same NebulaGraph cluster.</li> </ul> <ul> <li>Migrate data between NebulaGraph and other data sources.</li> </ul> <ul> <li>Graph computing with NebulaGraph Algorithm.</li> </ul>"},{"location":"nebula-spark-connector/#benefits","title":"Benefits","text":"<p>The features of NebulaGraph Spark Connector 3.4.0 are as follows:</p> <ul> <li>Supports multiple connection settings, such as timeout period, number of connection retries, number of execution retries, etc.</li> </ul> <ul> <li>Supports multiple settings for data writing, such as setting the corresponding column as vertex ID, starting vertex ID, destination vertex ID or attributes.</li> </ul> <ul> <li>Supports non-attribute reading and full attribute reading.</li> </ul> <ul> <li>Supports reading NebulaGraph data into VertexRDD and EdgeRDD, and supports non-Long vertex IDs.</li> </ul> <ul> <li>Unifies the extended data source of SparkSQL, and uses DataSourceV2 to extend NebulaGraph data.</li> </ul> <ul> <li>Three write modes, <code>insert</code>, <code>update</code> and <code>delete</code>, are supported. <code>insert</code> mode will insert (overwrite) data, <code>update</code> mode will only update existing data, and <code>delete</code> mode will only delete data.</li> </ul>"},{"location":"nebula-spark-connector/#release_note","title":"Release note","text":"<p>Release</p>"},{"location":"nebula-spark-connector/#get_nebulagraph_spark_connector","title":"Get NebulaGraph Spark Connector","text":""},{"location":"nebula-spark-connector/#compile_package","title":"Compile package","text":"<ol> <li> <p>Clone repository <code>nebula-spark-connector</code>.</p> <p><pre><code>$ git clone -b release-3.4 https://github.com/vesoft-inc/nebula-spark-connector.git\n</code></pre> 2. Enter the <code>nebula-spark-connector</code> directory.</p> </li> <li> <p>Compile package. The procedure varies with Spark versions.</p> </li> </ol> <p>Note<p>Spark of the corresponding version has been installed.</p> </p> <p>- Spark 2.4</p> <pre><code>```bash\n$ mvn clean package -Dmaven.test.skip=true -Dgpg.skip -Dmaven.javadoc.skip=true -pl nebula-spark-connector -am -Pscala-2.11 -Pspark-2.4\n```\n</code></pre> <p>- Spark 2.2</p> <pre><code>```bash\n$ mvn clean package -Dmaven.test.skip=true -Dgpg.skip -Dmaven.javadoc.skip=true -pl nebula-spark-connector_2.2 -am -Pscala-2.11 -Pspark-2.2\n```\n</code></pre> <p>- Spark 3.x</p> <pre><code>```bash\n$ mvn clean package -Dmaven.test.skip=true -Dgpg.skip -Dmaven.javadoc.skip=true -pl nebula-spark-connector_3.0 -am -Pscala-2.12 -Pspark-3.0\n```\n</code></pre> <p>After compilation, a similar file <code>nebula-spark-connector-3.4.0-SHANPSHOT.jar</code> is generated in the directory <code>target</code> of the folder.</p>"},{"location":"nebula-spark-connector/#download_maven_remote_repository","title":"Download maven remote repository","text":"<p>Download</p>"},{"location":"nebula-spark-connector/#how_to_use","title":"How to use","text":"<p>When using NebulaGraph Spark Connector to reading and writing NebulaGraph data, You can refer to the following code.</p> <pre><code># Read vertex and edge data from NebulaGraph.\nspark.read.nebula().loadVerticesToDF()\nspark.read.nebula().loadEdgesToDF()\n\n# Write dataframe data into NebulaGraph as vertex and edges.\ndataframe.write.nebula().writeVertices()\ndataframe.write.nebula().writeEdges()\n</code></pre> <p><code>nebula()</code> receives two configuration parameters, including connection configuration and read-write configuration.</p>"},{"location":"nebula-spark-connector/#reading_data_from_nebulagraph","title":"Reading data from NebulaGraph","text":"<pre><code>val config = NebulaConnectionConfig\n  .builder()\n  .withMetaAddress(\"127.0.0.1:9559\")\n  .withConenctionRetry(2)\n  .withExecuteRetry(2)\n  .withTimeout(6000)\n  .build()\n\nval nebulaReadVertexConfig: ReadNebulaConfig = ReadNebulaConfig\n  .builder()\n  .withSpace(\"test\")\n  .withLabel(\"person\")\n  .withNoColumn(false)\n  .withReturnCols(List(\"birthday\"))\n  .withLimit(10)\n  .withPartitionNum(10)\n  .build()\nval vertex = spark.read.nebula(config, nebulaReadVertexConfig).loadVerticesToDF()\n\nval nebulaReadEdgeConfig: ReadNebulaConfig = ReadNebulaConfig\n  .builder()\n  .withSpace(\"test\")\n  .withLabel(\"knows\")\n  .withNoColumn(false)\n  .withReturnCols(List(\"degree\"))\n  .withLimit(10)\n  .withPartitionNum(10)\n  .build()\nval edge = spark.read.nebula(config, nebulaReadEdgeConfig).loadEdgesToDF()\n</code></pre> <ul> <li> <p><code>NebulaConnectionConfig</code> is the configuration for connecting to the nebula graph, as described below.</p> Parameter Required Description <code>withMetaAddress</code> Yes Specifies the IP addresses and ports of all Meta Services. Separate multiple addresses with commas. The format is <code>ip1:port1,ip2:port2,...</code>. Read data is no need to configure <code>withGraphAddress</code>. <code>withConnectionRetry</code> No The number of retries that the NebulaGraph Java Client connected to the NebulaGraph. The default value is <code>1</code>. <code>withExecuteRetry</code> No The number of retries that the NebulaGraph Java Client executed query statements. The default value is <code>1</code>. <code>withTimeout</code> No The timeout for the NebulaGraph Java Client request response. The default value is <code>6000</code>, Unit: ms. </li> </ul> <ul> <li> <p><code>ReadNebulaConfig</code> is the configuration to read NebulaGraph data, as described below.</p> Parameter Required Description <code>withSpace</code> Yes NebulaGraph space name. <code>withLabel</code> Yes The Tag or Edge type name within the NebulaGraph space. <code>withNoColumn</code> No Whether the property is not read. The default value is <code>false</code>, read property. If the value is <code>true</code>, the property is not read, the <code>withReturnCols</code> configuration is invalid. <code>withReturnCols</code> No Configures the set of properties for vertex or edges to read. the format is <code>List(property1,property2,...)</code>, The default value is <code>List()</code>, indicating that all properties are read. <code>withLimit</code> No Configure the number of rows of data read from the server by the NebulaGraph Java Storage Client at a time. The default value is <code>1000</code>. <code>withPartitionNum</code> No Configures the number of Spark partitions to read the NebulaGraph data. The default value is <code>100</code>. This value should not exceed the number of slices in the graph space (partition_num). </li> </ul>"},{"location":"nebula-spark-connector/#write_data_into_nebulagraph","title":"Write data into NebulaGraph","text":"<p>Note</p> <p>The values of columns in a dataframe are automatically written to the NebulaGraph as property values.</p> <pre><code>val config = NebulaConnectionConfig\n  .builder()\n  .withMetaAddress(\"127.0.0.1:9559\")\n  .withGraphAddress(\"127.0.0.1:9669\")\n  .withConenctionRetry(2)\n  .build()\n\nval nebulaWriteVertexConfig: WriteNebulaVertexConfig = WriteNebulaVertexConfig      \n  .builder()\n  .withSpace(\"test\")\n  .withTag(\"person\")\n  .withVidField(\"id\")\n  .withVidPolicy(\"hash\")\n  .withVidAsProp(true)\n  .withUser(\"root\")\n  .withPasswd(\"nebula\")\n  .withBatch(1000)\n  .build()    \ndf.write.nebula(config, nebulaWriteVertexConfig).writeVertices()\n\nval nebulaWriteEdgeConfig: WriteNebulaEdgeConfig = WriteNebulaEdgeConfig      \n  .builder()\n  .withSpace(\"test\")\n  .withEdge(\"friend\")\n  .withSrcIdField(\"src\")\n  .withSrcPolicy(null)\n  .withDstIdField(\"dst\")\n  .withDstPolicy(null)\n  .withRankField(\"degree\")\n  .withSrcAsProperty(true)\n  .withDstAsProperty(true)\n  .withRankAsProperty(true)\n  .withUser(\"root\")\n  .withPasswd(\"nebula\")\n  .withBatch(1000)\n  .build()\ndf.write.nebula(config, nebulaWriteEdgeConfig).writeEdges()\n</code></pre> <p>The default write mode is <code>insert</code>, which can be changed to <code>update</code> or <code>delete</code> via <code>withWriteMode</code> configuration:</p> <pre><code>val config = NebulaConnectionConfig\n  .builder()\n  .withMetaAddress(\"127.0.0.1:9559\")\n  .withGraphAddress(\"127.0.0.1:9669\")\n  .build()\nval nebulaWriteVertexConfig = WriteNebulaVertexConfig\n  .builder()\n  .withSpace(\"test\")\n  .withTag(\"person\")\n  .withVidField(\"id\")\n  .withVidAsProp(true)\n  .withBatch(1000)\n  .withWriteMode(WriteMode.UPDATE)\n  .build()\ndf.write.nebula(config, nebulaWriteVertexConfig).writeVertices()\n</code></pre> <ul> <li> <p><code>NebulaConnectionConfig</code> is the configuration for connecting to the nebula graph, as described below.</p> Parameter Required Description <code>withMetaAddress</code> Yes Specifies the IP addresses and ports of all Meta Services. Separate multiple addresses with commas. The format is <code>ip1:port1,ip2:port2,...</code>. <code>withGraphAddress</code> Yes Specifies the IP addresses and ports of Graph Services. Separate multiple addresses with commas. The format is <code>ip1:port1,ip2:port2,...</code>. <code>withConnectionRetry</code> No Number of retries that the NebulaGraph Java Client connected to the NebulaGraph. The default value is <code>1</code>. </li> </ul> <ul> <li> <p><code>WriteNebulaVertexConfig</code> is the configuration of the write vertex, as described below.</p> Parameter Required Description <code>withSpace</code> Yes NebulaGraph space name. <code>withTag</code> Yes The Tag name that needs to be associated when a vertex is written. <code>withVidField</code> Yes The column in the DataFrame as the vertex ID. <code>withVidPolicy</code> No When writing the vertex ID, NebulaGraph use mapping function, supports HASH only. No mapping is performed by default. <code>withVidAsProp</code> No Whether the column in the DataFrame that is the vertex ID is also written as an property. The default value is <code>false</code>. If set to <code>true</code>, make sure the Tag has the same property name as <code>VidField</code>. <code>withUser</code> No NebulaGraph user name. If authentication is disabled, you do not need to configure the user name and password. <code>withPasswd</code> No The password for the NebulaGraph user name. <code>withBatch</code> Yes The number of rows of data written at a time. The default value is  <code>1000</code>. <code>withWriteMode</code> No Write mode. The optional values are <code>insert</code>, <code>update</code> and <code>delete</code>. The default value is <code>insert</code>. <code>withDeleteEdge</code> No Whether to delete the related edges synchronously when deleting a vertex. The default value is <code>false</code>. It takes effect when <code>withWriteMode</code> is <code>delete</code>. </li> </ul> <ul> <li> <p><code>WriteNebulaEdgeConfig</code> is the configuration of the write edge, as described below.</p> Parameter Required Description <code>withSpace</code> Yes NebulaGraph space name. <code>withEdge</code> Yes The Edge type name that needs to be associated when a edge is written. <code>withSrcIdField</code> Yes The column in the DataFrame as the vertex ID. <code>withSrcPolicy</code> No When writing the starting vertex ID, NebulaGraph use mapping function, supports HASH only. No mapping is performed by default. <code>withDstIdField</code> Yes The column in the DataFrame that serves as the destination vertex. <code>withDstPolicy</code> No When writing the destination vertex ID, NebulaGraph use mapping function, supports HASH only. No mapping is performed by default. <code>withRankField</code> No The column in the DataFrame as the rank. Rank is not written by default. <code>withSrcAsProperty</code> No Whether the column in the DataFrame that is the starting vertex is also written as an property.  The default value is <code>false</code>. If set to <code>true</code>, make sure Edge type has the same property name as <code>SrcIdField</code>. <code>withDstAsProperty</code> No Whether column that are destination vertex in the DataFrame are also written as property. The default value is <code>false</code>. If set to <code>true</code>, make sure Edge type has the same property name as <code>DstIdField</code>. <code>withRankAsProperty</code> No Whether column in the DataFrame that is the rank is also written as property.The default value is <code>false</code>. If set to <code>true</code>, make sure Edge type has the same property name as <code>RankField</code>. <code>withUser</code> No NebulaGraph user name. If authentication is disabled, you do not need to configure the user name and password. <code>withPasswd</code> No The password for the NebulaGraph user name. <code>withBatch</code> Yes The number of rows of data written at a time. The default value is  <code>1000</code>. <code>withWriteMode</code> No Write mode. The optional values are <code>insert</code>, <code>update</code> and <code>delete</code>. The default value is <code>insert</code>. </li> </ul>"},{"location":"1.introduction/0-0-graph/","title":"An introduction to graphs","text":"<p>People from tech giants (such as Amazon and Facebook) to small research teams are devoting significant resources to exploring the potential of graph databases to solve data relationships problems. What exactly is a graph database? What can it do? Where does it fit in the database landscape? To answer these questions, we first need to understand graphs.</p> <p>Graphs are one of the main areas of research in computer science. Graphs can efficiently solve many of the problems that exist today. This topic will start with graphs and explain the advantages of graph databases and their great potential in modern application development, and then describe the differences between distributed graph databases and several other types of databases.</p>"},{"location":"1.introduction/0-0-graph/#what_are_graphs","title":"What are graphs?","text":"<p>Graphs are everywhere. When hearing the word graph, many people think of bar charts or line charts, because sometimes we call them graphs, which show the connections between two or more data systems. The simplest example is the following picture, which shows the number of NebulaGraph GitHub repository stars over time.</p> <p></p> <p>This type of diagram is often called a line chart. As you can see, the number of starts rises over time. A line chart can show data changes over time (depending on the scale settings). Here we have given only examples of line charts. There are various graphs, such as pie charts, bar charts, etc.</p> <p>Another kind of diagram is often used in daily conversation, such as image recognition, retouched photos. This type of diagram is called a picture/photo/image.</p> <p></p> <p>The diagram we discuss in this topic is a different concept, the graph in graph theory.</p> <p>In graph theory, a branch of mathematics, graphs are used to represent the relationships between entities. A graph consists of several small dots (called vertices or nodes) and lines or curves (called edges) that connect these dots. The term graph was proposed by Sylvester in 1878.</p> <p>The following picture is what this topic calls a graph.</p> <p></p> <p>Simply put, graph theory is the study of graphs. Graph theory began in the early 18th century with the problem of the Seven Bridges of K\u00f6nigsberg. K\u00f6nigsberg was then a Prussian city (now part of Russia, renamed Kaliningrad). The river Preger crossed K\u00f6nigsberg and not only divided K\u00f6nigsberg into two parts, but also formed two small islands in the middle of the river. This divided the city into four areas, each connected by seven bridges. There was a game associated with K\u00f6nigsberg at the time, namely how to cross each bridge only once and navigate the entire four areas of the city. A simplified view of the seven bridges is shown below. Try to find the answer to this game if you are interested <sup>1</sup>.</p> <p></p> <p>To solve this problem, the great mathematician Euler proved that the problem was unsolvable by abstracting the four regions of the city into points and the seven bridges connecting the city into edges connecting the points. The simplified abstract diagram is as follows <sup>2</sup>.</p> <p></p> <p>The four dots in the picture represent the four regions of K\u00f6nigsberg, and the lines between the dots represent the seven bridges connecting the four regions. It is easy to see that the area connected by the even-numbered bridges can be easily passed because different routes can be chosen to come and go. The areas connected by the odd-numbered bridges can only be used as starting or endings points because the same route can only be taken once. The number of edges associated with a node is called the node degree. Now it can be shown that the K\u00f6nigsberg problem can only be solved if two nodes have odd degrees and the other nodes have even degrees, i.e., two regions must have an even number of bridges and the remaining regions have an odd number of bridges. However, as we know from the above picture, there is no even number of bridges in any region of K\u00f6nigsberg, so this puzzle is unsolvable.</p>"},{"location":"1.introduction/0-0-graph/#property_graphs","title":"Property graphs","text":"<p>From a mathematical point of view, graph theory studies the relationships between modeled objects. However, it is common to extend the underlying graph model. The extended graphs are called the attribute graph model. A property graph usually consists of the following components.</p> <ul> <li>Node, an object or entity. In this topic, nodes are called vertices.</li> <li>Relationship between nodes. In this topic, relationships are called edges. Usually, the edges can be directed or undirected to indicate a relationship between two entities.</li> <li>There can be properties on nodes and edges.</li> </ul> <p>In real life, there are many examples of property graphs.</p> <p>For example, Qichacha or BOSS Zhipin use graphs to model business equity relationships. A vertex usually represents a natural person or a business, and the edge represents the equity relationship between a person and a business. The properties on vertices can be the name, age, ID number, etc. of the natural person. The properties on edges can be the investment amount, investment time, position such as director and supervisor.</p> <p>A vertex can be a listed company and an edge can be a correlation between listed companies. The vertex property can be a stock code, abbreviation, market capitalization, sector, etc. The edge property can be the time-series correlation coefficient of the stock price <sup>3</sup>.</p> <p>The graph relationship can also be similar to the character relationship in a TV series like Game of Thrones <sup>4</sup>. Vertices stand for the characters. Edges represent the interactions between the characters. Vertex properties are the character's names, ages, camps, etc., and edge properties are the number of interactions between two characters. </p> <p></p> <p>Graphs are also used for governance within IT systems. For example, a company like WeBank has a very large data warehouse and corresponding data warehouse management tools. These management tools record the ETL relationships between the Hive tables in the data warehouse through Job implementation <sup>5</sup>. Such ETL relationships can be very easily presented and managed in the form of graphs, and the root cause can be easily traced when problems arise.</p> <p></p> <p>Graphs can also be used to document the invocation relationships between the intricate microservices within a large IT system <sup>6</sup>, which is used by operations teams for service governance. Here each point represents a microservice and the edge represents the invocation relationship between two microservices; thus, Ops can easily find invocation links with availability below a threshold (99.99%) or discover microservice nodes that would be particularly affected by a failure.</p> <p>Graphs are also used to record the invocation relationships between the intricate microservices <sup>6</sup>. Each vertex represents a microservice and an edge represents the invocation relationship between two microservices. This allows Ops to easily find call links with availability below a threshold (99.99%), or to discover microservice nodes where a failure would have a particularly large impact.</p> <p>Graphs can also be used to improve the efficiency of code development. Graphs store function call relationships between codes <sup>6</sup> to improve the efficiency of reviewing and testing the code. In such a graph, each vertex is a function or variable, each edge is a call relationship between functions or variables. When there is a new code commit, one can more easily see other interfaces that may be affected, which helps testers better assess potential go-live risks.</p> <p>In addition, we can discover more scenarios by adding some temporal information as opposed to a static property graph that does not change.</p> <p>For example, inside a network of interbank account fund flows <sup>7</sup>, a vertex is an account, an edge is the transfer record between accounts. Edge properties record the time, amount, etc. of the transfer. Companies can use graph technology to easily explore the graph to discover obvious misappropriation of funds, paying back a load to with the loan, loan gang scams, and other phenomena.  </p> <p></p> <p>The same approach can be used to explore the discovery of the flow of cryptocurrencies.</p> <p></p> <p>In a network of accounts and devices <sup>8</sup>, vertices can be accounts, mobile devices, and WIFI networks, edges are the login relationships between these accounts and mobile devices, and the access relationships between mobile devices and WIFI networks.</p> <p></p> <p>These graph data records the characteristic of the network black production operations. Some big companies such as 360 DigiTech<sup>8</sup>, Kuaishou<sup>9</sup>, WeChat<sup>10</sup>, Zhihu<sup>11</sup>, and Ctrip Finance all identified over a million crime groups through technology.</p> <p></p> <p>In addition to the dimension of time, you can find more scenarios for property graphs by adding some geographic location information.</p> <p>For an example of tracing the source of the Coronavirus Disease (COVID-19) <sup>12</sup>, vertices are the person and edges are the contact between people. Vertex properties are the information of the person's ID card and onset time, and edge properties are the time and geographical location of the close contact between people, etc. It provides help for health prevention departments to quickly identify high-risk people and their behavioral trajectories.</p> <p></p> <p>The combination of geographic location and graph is also used in some O2O scenarios, such as real-time food recommendation based on POI (Point-of-Interest) <sup>13</sup>, which enables local life service platform companies like Meituan to recommend more suitable businesses in real-time when consumers open the APP.</p> <p>A graph is also used for knowledge inference. Huawei, Vivo, OPPO, WeChat, Meituan, and other companies use graphs for the representation of the underlying knowledge relationships.</p>"},{"location":"1.introduction/0-0-graph/#why_do_we_use_graph_databases","title":"Why do we use graph databases?","text":"<p>Although relational databases and semi-structured databases such as XML/JSON can be used to describe a graph-structured data model, a graph (database) not only describes the graph structure and stores data itself but also focuses on handling the associative relationships between the data. Specifically, graph databases have several advantages:</p> <ul> <li>Graphs are a more visual and intuitive way of representing knowledge to human brains. This allows us to focus on the business problem itself rather than how to describe the problem as a particular structure of the database (e.g., a table structure).</li> </ul> <ul> <li> <p>It is easier to show the characteristic of the data in graphs. Such as transfer paths and nearby communities. To analyze the relationships of characters and character importance in Game of Thrones, data displayed with tables is not as intuitive as with graphs.</p> <p></p> <p>Especially when some central vertices are deleted:</p> <p></p> <p>Adding an edge can completely change the entire topology.</p> <p></p> <p>We can intuitively sense the importance of minor changes in graphs rather than in tables.</p> </li> </ul> <ul> <li> <p>Graph query language is designed based on graph structures. The following is a query example in LDBC. Requirements: Query the posts posted by a person, and query the corresponding replies (the replies themselves will also be replied multiple times). Since the posting time and reply time both meet certain conditions, you can sort the results according to the number of replies.</p> <p></p> <p>Write querying statements using PostgreSQL:</p> <pre><code>--PostgreSQL\nWITH RECURSIVE post_all(psa_threadid\n                      , psa_thread_creatorid, psa_messageid\n                      , psa_creationdate, psa_messagetype\n                       ) AS (\n    SELECT m_messageid AS psa_threadid\n         , m_creatorid AS psa_thread_creatorid\n         , m_messageid AS psa_messageid\n         , m_creationdate, 'Post'\n      FROM message\n     WHERE 1=1 AND m_c_replyof IS NULL -- post, not comment\n       AND m_creationdate BETWEEN :startDate AND :endDate\n  UNION ALL\n    SELECT psa.psa_threadid AS psa_threadid\n         , psa.psa_thread_creatorid AS psa_thread_creatorid\n         , m_messageid, m_creationdate, 'Comment'\n      FROM message p, post_all psa\n     WHERE 1=1 AND p.m_c_replyof = psa.psa_messageid\n     AND m_creationdate BETWEEN :startDate AND :endDate\n)\nSELECT p.p_personid AS \"person.id\"\n     , p.p_firstname AS \"person.firstName\"\n     , p.p_lastname AS \"person.lastName\"\n     , count(DISTINCT psa.psa_threadid) AS threadCount\nEND) AS messageCount\n     , count(DISTINCT psa.psa_messageid) AS messageCount\n  FROM person p left join post_all psa on (\n       1=1   AND p.p_personid = psa.psa_thread_creatorid\n   AND psa_creationdate BETWEEN :startDate AND :endDate\n   )\n GROUP BY p.p_personid, p.p_firstname, p.p_lastname\n ORDER BY messageCount DESC, p.p_personid\n LIMIT 100;\n</code></pre> <p>Write querying statements using Cypher designed especially for graphs:</p> <pre><code>--Cypher\nMATCH (person:Person)&lt;-[:HAS_CREATOR]-(post:Post)&lt;-[:REPLY_OF*0..]-(reply:Message)\nWHERE post.creationDate &gt;= $startDate AND post.creationDate &lt;= $endDate\n  AND reply.creationDate &gt;= $startDate AND reply.creationDate &lt;= $endDate\nRETURN\n  person.id, person.firstName, person.lastName, count(DISTINCT post) AS threadCount,\n  count(DISTINCT reply) AS messageCount\nORDER BY\n  messageCount DESC, person.id ASC\nLIMIT 100\n</code></pre> </li> </ul> <ul> <li>Graph traversal (corresponding to Join in SQL) is much more efficient because the storage and query engines are designed specifically for the structure of the graph. </li> </ul> <ul> <li>Graph databases have a wide range of application scenarios. Examples include data integration (knowledge graph), personalized recommendations, fraud, and threat detection, risk analysis, and compliance, identity (and control) verification, IT infrastructure management, supply chain, and logistics, social network research, etc.</li> </ul> <ul> <li>According to the literature <sup>14</sup>, the fields that use graph technology are (from the greatest to least): information technology (IT), research in academia, finance, laboratories in industry, government, healthcare, defense, pharmaceuticals, retail, and e-commerce, transportation, telecommunications, and insurance.</li> </ul> <ul> <li>In 2019, according to Gartner's questionnaire research, 27% of customers (500 groups) are using graph databases and 20% have plans to use them.</li> </ul>"},{"location":"1.introduction/0-0-graph/#rdf","title":"RDF","text":"<p>This topic does not discuss the RDF data model due to space limitations.</p> <ol> <li> <p>Souce of the picture: https://medium.freecodecamp.org/i-dont-understand-graph-theory-1c96572a1401.\u00a0\u21a9</p> </li> <li> <p>Source of the picture: https://medium.freecodecamp.org/i-dont-understand-graph-theory-1c96572a1401\u00a0\u21a9</p> </li> <li> <p>https://nebula-graph.com.cn/posts/stock-interrelation-analysis-jgrapht-nebula-graph/\u00a0\u21a9</p> </li> <li> <p>https://nebula-graph.com.cn/posts/game-of-thrones-relationship-networkx-gephi-nebula-graph/\u00a0\u21a9</p> </li> <li> <p>https://nebula-graph.com.cn/posts/practicing-nebula-graph-webank/\u00a0\u21a9</p> </li> <li> <p>https://nebula-graph.com.cn/posts/meituan-graph-database-platform-practice/\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>https://zhuanlan.zhihu.com/p/90635957\u00a0\u21a9</p> </li> <li> <p>https://nebula-graph.com.cn/posts/graph-database-data-connections-insight/\u00a0\u21a9\u21a9</p> </li> <li> <p>https://nebula-graph.com.cn/posts/kuaishou-security-intelligence-platform-with-nebula-graph/\u00a0\u21a9</p> </li> <li> <p>https://nebula-graph.com.cn/posts/nebula-graph-for-social-networking/\u00a0\u21a9</p> </li> <li> <p>https://mp.weixin.qq.com/s/K2QinpR5Rplw1teHpHtf4w\u00a0\u21a9</p> </li> <li> <p>https://nebula-graph.com.cn/posts/detect-corona-virus-spreading-with-graph-database/\u00a0\u21a9</p> </li> <li> <p>https://nebula-graph.com.cn/posts/meituan-graph-database-platform-practice/\u00a0\u21a9</p> </li> <li> <p>https://arxiv.org/abs/1709.03188\u00a0\u21a9</p> </li> </ol>"},{"location":"1.introduction/0-1-graph-database/","title":"Market overview of graph databases","text":"<p>Now that we have discussed what a graph is, let's move on to further understanding graph databases developed based on graph theory and the property graph model.</p> <p>Different graph databases may differ slightly in terms of terminology, but in the end, they all talk about vertices, edges, and properties. As for more advanced features such as labels, indexes, constraints, TTL, long tasks, stored procedures, and UDFs, these advanced features will vary significantly from one graph database to another.</p> <p>Graph databases use graphs to store data, and the graph structure is one of the structures that are closest to high flexibility and high performance. A graph database is a storage engine specifically designed to store and retrieve large information, which efficiently stores data as vertices and edges and allows high-performance retrieval and querying of these vertex-edge structures. We can also add properties to these vertices and edges.</p>"},{"location":"1.introduction/0-1-graph-database/#third-party_services_market_predictions","title":"Third-party services market predictions","text":""},{"location":"1.introduction/0-1-graph-database/#db-engines_ranking","title":"DB-Engines ranking","text":"<p>According to DB-Engines.com, the world's leading database ranking site, graph databases have been the fastest growing database category since 2013 <sup>1</sup>.</p> <p>The site counts trends in the popularity of each category based on several metrics, including records and trends based on search engines such as Google, technical topics discussed on major IT technology forums and social networking sites, job posting changes on job boards. 371 database products are included in the site and are divided into 12 categories. Of these 12 categories, a category like graph databases is growing much faster than any of the others.</p> <p></p>"},{"location":"1.introduction/0-1-graph-database/#gartners_predictions","title":"Gartner\u2019s predictions","text":"<p>Gartner, one of the world's top think tanks, identified graph databases as a major business intelligence and analytics technology trend long before 2013 <sup>2</sup>. At that time, big data was hot as ever, and data scientists were in a hot position.</p> <p></p> <p>Until recently, graph databases and related graph technologies were ranked in the Top 10 Data and Analytics Trends for 2021 <sup>3</sup>.</p> <p></p> <p>Trend 8: Graph Relates Everything</p> <p>Graphs form the foundation of many modern data and analytics capabilities to find relationships between people, places, things, events, and locations across diverse data assets. D&amp;A leaders rely on graphs to quickly answer complex business questions which require contextual awareness and an understanding of the nature of connections and strengths across multiple entities.</p> <p>Gartner predicts that by 2025, graph technologies will be used in 80% of data and analytics innovations, up from 10% in 2021, facilitating rapid decision-making across the organization.      </p> <p>It can be noted that Gartner's predictions match the DB-Engines ranking well. There is usually a period of rapid bubble development, then a plateau period, followed by a new bubble period due to the emergence of new technologies, and then a plateau period again. </p>"},{"location":"1.introduction/0-1-graph-database/#market_size_of_graph_databases","title":"Market size of graph databases","text":"<p>According to statistics and forecasts from Verifiedmarketresearc<sup>4</sup>, fnfresearch<sup>5</sup>, MarketsandMarkets<sup>6</sup>, and Gartner<sup>7</sup>, the global graph database market size is about to grow from about USD 0.8 billion in 2019 to USD 3-4 billion by 2026, at a Compound Annual Growth Rate (CAGR) of about 25%, which corresponds to about 5%-10% market share of the global database market. </p> <p></p>"},{"location":"1.introduction/0-1-graph-database/#market_participants","title":"Market participants","text":""},{"location":"1.introduction/0-1-graph-database/#neo4j_the_pioneer_of_first_generation_graph_databases","title":"Neo4j, the pioneer of (first generation) graph databases","text":"<p>Although some graph-like data models and products, and the corresponding graph language G/G+ had been proposed in the 1970s (e.g. CODASYL <sup>8</sup>). But it is Neo4j, the main pioneer in this market, that has really made the concept of graph databases popular, and even the two main terms (labeled) property graphs and graph databases were first introduced and practiced by Neo4j. </p> <p>!!! Info \"This section on the history of Neo4j and the graph query language it created, Cypher, is largely excerpted from the ISO WG3 paper An overview of the recent history of Graph Query Languages <sup>10</sup> and <sup>9</sup>. To take into account the latest two years of development, the content mentioned in this topic has been abridged and updated by the authors of this book.  </p> <p>About GQL (Graph Query Language) and the development of an International Standard</p> <p>Readers familiar with databases are probably aware of the Structured Query Language SQL. by using SQL, people access databases in a way that is close to natural language. Before SQL was widely adopted and standardized, the market for relational databases was very fragmented. Each vendor's product had a completely different way of accessing. Developers of the database product itself, developers of the tools surrounding the database product, and end-users of the database, all had to learn each product. When the SQL-89 standard was developed in 1989, the entire relational database market quickly focus on SQL-89. This greatly reduced the learning costs for the people mentioned above.</p> <p>GQL (Graph Query Language) assumes a role similar to SQL in the field of graph databases. Uses interacts with graphs with GQL. Unlike international standards such as SQL-89, there are no international standards for GQL. Two mainstream graph languages are Neo4j\u2019s Cypher and Apache TinkerPop's Gremlin. The former is often referred to as the DQL, Declarative Query Language. DQL tells the system \"what to do\", regardless of \"how to do\". The latter is referred to as the IQL, Imperative Query Language. IQL explicitly specifies the system's actions.</p> <p>The GQL International Standard is in the process of being developed. </p>"},{"location":"1.introduction/0-1-graph-database/#overview_of_the_recent_history_of_graph_databases","title":"Overview of the recent history of graph databases","text":"<ul> <li>In 2000, the idea of modeling data as a network came to the founders of Neo4j.</li> <li>In 2001, Neo4j developed the earliest core part of the code.</li> <li>In 2007, Neo4j started operating as a company. </li> <li>In 2009, Neo4j borrowed XPath as a graph query language. Gremlin <sup>11</sup> is also similar to XPath.</li> <li>In 2010, Marko Rodriguez, a Neo4j employee, used the term Property Graph to describe the data model of Neo4j and TinkerPop (Gremlin).</li> <li>In 2011, the first public version Neo4j 1.4 was released, and the first version of Cypher was released.</li> <li>In 2012, Neo4j 1.8 enabled you to write a Cypher. Neo4j 2.0 added labels and indexes. Cypher became a declarative graph query language.</li> <li>In 2015, Cypher was opened up by Neo4j through the openCypher project.</li> <li>In 2017, the ISO WG3 organization discussed how to use SQL to query property graph data.</li> <li>In 2018, Starting from the Neo4j 3.5 GA, the core of Neo4j only for the Enterprise Edition will no longer be open source.</li> <li>In 2019, ISO officially established two projects ISO/IEC JTC 1 N 14279 and ISO/IEC JTC 1/SC 32 N 3228 to develop an international standard for graph database language.</li> <li>In 2021, the $325 million Series F funding round for Neo4j marks the largest investment round in database history.</li> </ul>"},{"location":"1.introduction/0-1-graph-database/#the_early_history_of_neo4j","title":"The early history of Neo4j","text":"<p>The data model property graph was first conceived in 2000. The founders of Neo4j were developing a media management system, and the schema of the system was often changed. To adapt to such changes, Peter Neubauer, one of the founders, wanted to enable the system to be modeled to a conceptually interconnected network. A group of graduate students at the Indian Institute of Technology Bombay implemented the earliest prototypes. Emil Eifr\u00e9m, the Neo4j co-founder, and these students spent a week extending Peter's idea into a more abstract model: vertices were connected by relationships, and key-values were used as properties of vertices and relationships. They developed a Java API to interact with this data model and implemented an abstraction layer on top of the relational database. </p> <p>Although this network model greatly improved productivity, its performance has been poor. So Johan Svensson, Neo4j co-founder, put a lot of effort into implementing a native data management system, that is Neo4j. For the first few years, Neo4j was successful as an in-house product. In 2007, the intellectual property of Neo4j was transferred to an independent database company. </p> <p>In the first public release of Neo4j ( Neo4j 1.4, 2011), the data model was consisted of vertices and typed edges. Vertices and edges have properties. The early versions of Neo4j did not have indexes. Applications had to construct their search structure from the root vertex. Because this was very unwieldy for the applications, Neo4j 2.0 (2013.12) introduced a new concept label on vertices. Based on labels, Neo4j can index some predefined vertex properties.</p> <p>\"Vertex\", \"Relationship\", \"Property\", \"Relationships can only have one label.\", \"Vertices can have zero or multiple labels.\". All these concepts form the data model definitions for Neo4j property graphs. With the later addition of indexing, Cypher became the main way of interacting with Neo4j. This is because the application developer only needs to focus on the data itself, not on the search structure that the developer built himself as mentioned above.</p>"},{"location":"1.introduction/0-1-graph-database/#the_creation_of_gremlin","title":"The creation of Gremlin","text":"<p>Gremlin is a graph query language based on Apache TinkerPop, which is close in style to a sequence of function (procedure) calls. Initially, Neo4j was queried through the Java API. applications could embed the query engine as a library into the application and then use the API to query the graph.</p> <p>The early Neo4j employees Tobias Lindaaker, Ivarsson, Peter Neubauer, and Marko Rodriguez used XPath as a graph query. Groovy provides loop structures, branching, and computation. This was the original prototype of Gremlin, the first version of which was released in November 2009.</p> <p>Later, Marko found a lot of problems with using two different parsers (XPath and Groovy) at the same time and changed Gremlin to a Domain Specific Language (DSL) based on Groovy.</p>"},{"location":"1.introduction/0-1-graph-database/#the_creation_of_cypher","title":"The creation of Cypher","text":"<p>Gremlin, like Neo4j's Java API, was originally intended to be a procedural way of expressing how to query databases. It uses shorter syntaxes to query and remotely access databases through the network. The procedural nature of Gremlin requires users to know the best way to query results, which is still burdensome for application developers. Over the last 30 years, the declarative language SQL has been a great success. SQL can separate the declarative way to get data from how the engine gets data. So the Neo4j engineers wanted to develop a declarative graph query language.</p> <p>In 2010, Andr\u00e9s Taylor joined Neo4j as an engineer. Inspired by SQL, he started a project to develop graph query language, which was released as Neo4j 1.4 in 2011. The language is the ancestor of most graph query languages today - Cypher. </p> <p>Cypher's syntax is based on the use of ASCII art to describe graph patterns. This approach originally came from the annotations on how to describe graph patterns in the source code. An example can be seen as follows.</p> <p></p> <p>Simply put, ASCII art uses printable text to describe vertices and edges. Cypher syntax uses <code>()</code> for vertices and <code>-[]-&gt;</code> for edges. <code>(query)-[modeled as]-&gt;(drawing)</code> is used to represent a simple graph relationship (which can also be called graph schema): <code>the starting vertex - query</code>, <code>the destination vertex - drawing</code>, and <code>the edge - modeled as</code>. </p> <p>The first version of Cypher implemented graph reading, but users should specify vertices from which to start querying. Only from these vertices could graph schema matching be supported. </p> <p>In a later version, Neo4j 1.8, released in October 2012, Cypher added the ability to modify graphs. However, queries still need to specify which nodes to start from.</p> <p>In December 2013, Neo4j 2.0 introduced the concept of a label, which is essentially an index. This allows the query engine to use the index to select the vertices matched by the schema, without requiring the user to specify the vertex to start the query.</p> <p>With the popularity of Neo4j, Cypher has a wide community of developers and is widely used in a variety of industries. It is still the most popular graph query language.</p> <p>In September 2015, Neo4j established the openCypher Implementors Group (oCIG) to open source Cypher to openCypher, to govern and advance the evolution of the language itself through open source.</p>"},{"location":"1.introduction/0-1-graph-database/#subsequent_events","title":"Subsequent events","text":"<p>Cypher has inspired a series of graph query languages, including:</p> <p>2015, Oracle released PGQL, a graph language used by the graph engine PGX.</p> <p>2016, the Linked Data Benchmarking Council (short for LDBC) an industry-renowned benchmarking organization for graph performance, released G-CORE.</p> <p>2018, RedisGraph, a Redis-based graph library, adopted Cypher as its graph language. </p> <p>2019, the International Standards Organization ISO started two projects to initiate the process of developing an international standard for graph languages based on existing industry achievements such as openCypher, PGQL, GSQL<sup>12</sup>, and G-CORE.</p> <p>2019, NebulaGraph released NebulaGraph Query Language (nGQL) based on openCypher.</p> <p></p>"},{"location":"1.introduction/0-1-graph-database/#distributed_graph_databases","title":"Distributed graph databases","text":"<p>From 2005 to 2010, with the release of Google's cloud computing \"Troika\", various distributed architectures became increasingly popular, including Hadoop and Cassandra, which have been open-sourced. Several implications are as follows:</p> <ol> <li> <p>The technical and cost advantages of distributed systems over single machines (e.g. Neo4j) or small machines are more obvious due to the increasing volume of data and computation. Distributed systems allow applications to access these thousands of machines as if they were local systems, without the need for much modification at the code level.</p> </li> <li> <p>The open-source approach allows more people to know emerging technologies and feedback to the community in a more cost-effective way, including code developers, data scientists, and product managers. </p> </li> </ol> <p>Strictly speaking, Neo4j also offers several distributed capabilities, which are quite different from the industry's sense of the distributed system.</p> <ul> <li> <p>Neo4j 3. x requires that the full amount of data must be stored on a single machine. Although it supports full replication and high availability between multiple machines, the data cannot be sliced into different subgraphs. </p> <p></p> </li> </ul> <ul> <li> <p>Neo4j 4. x stores a part of data on different machines (subgraphs), and then the application layer assembles data in a certain way (called Fabric)<sup>13</sup> and distributes the reads and writes to each machine. This approach requires a log of involvement and work from the application layer code. For example, designing how to place different subgraphs on which machines they should be placed and how to assemble some of the results obtained from each machine into the final result.</p> <p></p> <p>The style of its syntax is as follows:</p> <pre><code>USE graphA  \nMATCH (movie:Movie)\nReturn movie.title AS title\n    UNION   \nUSE graphB  \nMATCH (move:Movie)\nRETURN movie.title AS title\n</code></pre> <p></p> </li> </ul>"},{"location":"1.introduction/0-1-graph-database/#the_second_generation_distributed_graph_database_titan_and_its_successor_janusgraph","title":"The second generation (distributed) graph database: Titan and its successor JanusGraph","text":"<p>In 2011, Aurelius was founded to develop an open-source distributed graph database called Titan <sup>14</sup>. By the first official release of Titan in 2015, the backend of Titan can support many major distributed storage architectures (e.g. Cassandra, HBase, Elasticsearch, BerkeleyDB) and can reuse many of the conveniences of the Hadoop ecosystem, with Gremlin as a unified query language on the frontend. It is easy for programmers to use, develop and participate in the community. Large-scale graphs could be sharded and stored on HBase or Cassandra (which were relatively mature distributed storage solutions at the time), and the Gremlin language was relatively full-featured though slightly lengthy. The whole solution was competitive at that time (2011-2015).</p> <p>The following picture shows the growth of Titan and Neo4j stars on Github.com from 2012 to 2015.</p> <p></p> <p>After Aurelius (Titan) was acquired by DataStax in 2015, Titan was gradually transformed into a closed-source commercial product(DataStax Enterprise Graph).</p> <p>After the acquisition of Aurelius(Titan), there has been a strong demand for an open-source distributed graph database, and there were not many mature and active products in the market. In the era of big data, data is still being generated in a steady stream, far faster than Moore's Law. The Linux Foundation, along with some technology giants (Expero, Google, GRAKN.AI, Hortonworks, IBM, and Amazon) replicated and forked the original Titan project and started it as a new project  JanusGraph<sup>15</sup>. Most of the community work including development, testing, release, and promotion, has been gradually shifted to the new JanusGraph.</p> <p>The following graph shows the evolution of daily code commits (pull requests) for the two projects, and we can see:</p> <ol> <li> <p>Although Aurelius(Titan) still has some activity in its open-source code after its acquisition in 2015, the growth rate has slowed down significantly. This reflects the strength of the community.</p> </li> <li> <p>After the new project was started in January 2017, its community became active quickly, surpassing the number of pull requests accumulated by Titan in the past 5 years in just one year. At the same time, the open-source Titan came to a halt.</p> <p></p> </li> </ol>"},{"location":"1.introduction/0-1-graph-database/#famous_products_of_the_same_period_orientdb_tigergraph_arangodb_and_dgraph","title":"Famous products of the same period OrientDB, TigerGraph, ArangoDB, and DGraph","text":"<p>In addition to JanusGraph managed by the Linux Foundation, more vendors have been joined the overall market. Some distributed graph databases that were developed by commercial companies use different data models and access methods.</p> <p>The following table only lists the main differences.</p> Vendors Creation time Core product Open source protocol Data model Query language OrientDB LTD (Acquired by SAP in 2017) 2011 OrientDB Open source Document + KV + Graph OrientDB SQL (SQL-based extended graph abilities) GraphSQL (was renamed TigerGraph) 2012 TigerGraph Commercial version Graph (Analysis) GraphSQL (similar to SQL) ArangoDB GmbH 2014 ArangoDB Apache License 2.0 Document + KV + Graph AQL (Simultaneous operation of documents, KVs and graphs) DGraph Labs 2016 DGraph Apache Public License 2.0 + Dgraph Community License Originally RDF, later changed to GraphQL GraphQL+-"},{"location":"1.introduction/0-1-graph-database/#traditional_giants_microsoft_amazon_and_oracle","title":"Traditional giants Microsoft, Amazon, and Oracle","text":"<p>In addition to vendors focused on graph products, traditional giants have also entered the graph database field.</p> <p>Microsoft Azure Cosmos DB<sup>16</sup> is a multimodal database cloud service on the Microsoft cloud that provides SQL, document, graph, key-value, and other capabilities. Amazon AWS Neptune<sup>17</sup> is a graph database cloud service provided by AWS support property graphs and RDF two data models. Oracle Graph<sup>18</sup> is a product of the relational database giant Oracle in the direction of graph technology and graph databases.</p>"},{"location":"1.introduction/0-1-graph-database/#nebulagraph_a_new_generation_of_open-source_distributed_graph_databases","title":"NebulaGraph, a new generation of open-source distributed graph databases","text":"<p>In the following topics, we will formally introduce NebulaGraph, a new generation of open-source distributed graph databases.</p> <ol> <li> <p>https://db-engines.com/en/ranking_categories\u00a0\u21a9</p> </li> <li> <p>https://www.yellowfinbi.com/blog/2014/06/yfcommunitynews-big-data-analytics-the-need-for-pragmatism-tangible-benefits-and-real-world-case-165305\u00a0\u21a9</p> </li> <li> <p>https://www.gartner.com/smarterwithgartner/gartner-top-10-data-and-analytics-trends-for-2021/\u00a0\u21a9</p> </li> <li> <p>https://www.verifiedmarketresearch.com/product/graph-database-market/\u00a0\u21a9</p> </li> <li> <p>https://www.globenewswire.com/news-release/2021/01/28/2165742/0/en/Global-Graph-Database-Market-Size-Share-to-Exceed-USD-4-500-Million-By-2026-Facts-Factors.html\u00a0\u21a9</p> </li> <li> <p>https://www.marketsandmarkets.com/Market-Reports/graph-database-market-126230231.html\u00a0\u21a9</p> </li> <li> <p>https://www.gartner.com/en/newsroom/press-releases/2019-07-01-gartner-says-the-future-of-the-database-market-is-the\u00a0\u21a9</p> </li> <li> <p>https://www.amazon.com/Designing-Data-Intensive-Applications-Reliable-Maintainable/dp/1449373321\u00a0\u21a9</p> </li> <li> <p>I. F. Cruz, A. O. Mendelzon, and P. T. Wood. A Graphical Query Language Supporting Recursion. In Proceedings of the Association for Computing Machinery Special Interest Group on Management of Data, pages 323\u2013330. ACM Press, May 1987.\u00a0\u21a9</p> </li> <li> <p>\"An overview of the recent history of Graph Query Languages\". Authors: Tobias Lindaaker, U.S. National Expert.Date: 2018-05-14\u00a0\u21a9</p> </li> <li> <p>Gremlin is a graph language developed based on Apache TinkerPop.\u00a0\u21a9</p> </li> <li> <p>https://docs.tigergraph.com/dev/gsql-ref\u00a0\u21a9</p> </li> <li> <p>https://neo4j.com/fosdem20/\u00a0\u21a9</p> </li> <li> <p>https://github.com/thinkaurelius/titan\u00a0\u21a9</p> </li> <li> <p>https://github.com/JanusGraph/janusgraph\u00a0\u21a9</p> </li> <li> <p>https://azure.microsoft.com/en-us/free/cosmos-db/\u00a0\u21a9</p> </li> <li> <p>https://aws.amazon.com/cn/neptune/\u00a0\u21a9</p> </li> <li> <p>https://www.oracle.com/database/graph/\u00a0\u21a9</p> </li> </ol>"},{"location":"1.introduction/0-2.relates/","title":"Related technologies","text":"<p>This topic introduces databases and graph-related technologies that are closely related to distributed graph databases.</p>"},{"location":"1.introduction/0-2.relates/#databases","title":"Databases","text":""},{"location":"1.introduction/0-2.relates/#relational_databases","title":"Relational databases","text":"<p>A relational database is a database that uses a relational model to organize data. The relational model is a two-dimensional table model, and a relational database consists of two-dimensional tables and the relationships between them. When it comes to relational databases, most people think of MySQL, one of the most popular database management systems that support database operations using the most common structured query language (SQL) and stores data in the form of tables, rows, and columns. This approach to storing data is derived from the relational data model proposed by Edgar Frank Codd in 1970.</p> <p>In a relational database, a table can be created for each type of data to be stored. For example, the player table is used to store all player information, the team table is used to store team information. Each row of data in a SQL table must contain a primary key. The primary key is a unique identifier for the row of data. Generally, the primary key is self-incrementing with the number of rows as the field ID. Relational databases have served the computer industry very well since their inception and will continue to do so for a long time to come.</p> <p>If you have used Excel, WPS, or other similar applications, you have a rough idea of how relational databases work. First, you set up the columns, then you add rows of data under the corresponding columns. You can average or otherwise aggregate the data in a column, similar to averaging in a relational database MySQL. Pivot tables in Excel are the equivalent of querying data in a relational database MySQL using aggregation functions and CASE statements. An Excel file can have multiple tables, and a single table is equivalent to a single table in MySQL. An Excel file is similar to a MySQL database.</p>"},{"location":"1.introduction/0-2.relates/#relationships_in_relational_databases","title":"Relationships in relational databases","text":"<p>Unlike graph databases, edges in relational databases (or SQL-type databases) are also stored as entities in specialized edge tables. Two tables are created, player and team, and then player_team is created as an edge table. Edge tables are usually formed by joining related tables. For example, here the edge table player_team is made by joining the player table and the team table.</p> <p></p> <p>The way of storing edges is not a big problem when associating small data sets, but problems arise when there are too many relationships in a relational database. Specifically, when you want to query just one player's teammates, you have to join all the data in the table and then filter out all the data you don't need, which puts a huge strain on the relational database when your dataset reaches a certain size. If you want to associate multiple different tables, the system may not be able to respond before the join bombs.</p>"},{"location":"1.introduction/0-2.relates/#origins_of_relational_databases","title":"Origins of relational databases","text":"<p>As mentioned above, the relational data model was first proposed by Edgar Frank Codd, an IBM engineer, in 1970. Codd wrote several papers on database management systems that addressed the potential of the relational data model. The relational data model does not rely on linked lists of data (mesh or hierarchical data), but more on data sets. Using the mathematical method of tuple calculus, he argued that these datasets can perform the same tasks as a navigational database. The only requirement was that the relational data model needed a suitable query language to guarantee the consistency requirements of the database. This became the inspiration for declarative query languages such as Structured Query Language (SQL). IBM's System R was one of the first implementations of such a system. But Software Development Laboratories, a small company founded by ex-IBM people and one illustrious Mr.Larry Ellison, beat IBM to the market with the product that would become known as Oracle.</p> <p>Since the relational database was a trendy term at the time, many database vendors preferred to use it in their product names, even though their products were not actually relational. To prevent this and reduce the misuse of the relational data model, Codd introduced the famous Codd's 12 Rules. All relational data systems must follow Codd's 12 Rules.</p>"},{"location":"1.introduction/0-2.relates/#nosql_databases","title":"NoSQL databases","text":"<p>Graph databases are not the only alternative that can overcome the shortcomings of relational databases. There are many non-relational database products on the market that can be called NoSQL. The term NoSQL was first introduced in the late 1990s and can be interpreted as \"not SQL\" or \"not only SQL\". For the sake of understanding, NoSQL can be interpreted as a \"non-relational database\" here. Unlike relational databases, the data storage and retrieval mechanisms provided by NoSQL databases are not modeled based on table relationships. NoSQL databases can be divided into four categories.</p> <ul> <li>Key-value Data Store</li> <li>Columnar Store</li> <li>Document Store</li> <li>Graph Store</li> </ul> <p>The following describes the four types of NoSQL databases.</p>"},{"location":"1.introduction/0-2.relates/#key-value_data_store","title":"Key-value Data Store","text":"<p>Key-value databases store data in unique key-value pairs. Unlike relational databases, key-value stores do not have tables and columns. A key-value database itself is like a large table with many columns (i.e., keys). In a key-value store database, data are stored and queried by means of keys, usually implemented as hash lists. This is much simpler than traditional SQL databases, and for some web applications, it is sufficient.</p> <p>The advantage of the key-value model for IT systems is that it is simple and easy to deploy. In most cases, this type of storage works well for unrelated data. If you are just storing data without querying it, there is no problem using this storage method. However, if the DBA only queries or updates some of the values, the key-value model becomes inefficient. Common key-value storage databases include Redis, Voldemort, and Oracle BDB.</p>"},{"location":"1.introduction/0-2.relates/#columnar_store","title":"Columnar Store","text":"<p>A NoSQL database's columnar store has many similarities to a NoSQL database's key-value store because the columnar store is still using keys for storage and retrieval. The difference is that in a columnar store database, the column is the smallest storage unit, and each column consists of a key, a value, and a timestamp for version control and conflict resolution. This is particularly useful when scaling in a distributed manner, as timestamps can be used to locate expired data when the database is updated. Because of the good scalability of columnar storage, the columnar store is suitable for very large data sets. Common columnar storage databases include HBase, Cassandra, HadoopDB, etc.</p>"},{"location":"1.introduction/0-2.relates/#document_store","title":"Document Store","text":"<p>A NoSQL database document store is a key-value-based database, but with enhanced functionality. Data is still stored as keys, but the values in a document store are structured documents, not just a string or a single value. That is, because of the increased information structure, document stores are able to perform more optimized queries and make data retrieval easier. Therefore, document stores are particularly well suited for storing, indexing, and managing document-oriented data or similar semi-structured data.</p> <p>Technically speaking, as a semi-structured unit of information, a document in a document store can be any form of document available, including XML, JSON, YAML, etc., depending on the design of the database vendor. For example, JSON is a common choice. While JSON is not the best choice for structured data, JSON-type data can be used in both front-end and back-end applications. Common document storage databases include MongoDB, CouchDB, Terrastore, etc.</p>"},{"location":"1.introduction/0-2.relates/#graph_store","title":"Graph Store","text":"<p>The last class of NoSQL databases is graph databases. NebulaGraph, is also a graph database. Although graph databases are also NoSQL databases, graph databases are fundamentally different from the above-mentioned NoSQL databases. Graph databases store data in the form of vertices, edges, and properties. Its advantages include high flexibility, support for complex graph algorithms, and can be used to build complex relational graphs. We will discuss graph databases in detail in the subsequent topics. But in this topic, you just need to know that a graph database is a NoSQL type of database. Common graph databases include NebulaGraph, Neo4j, OrientDB, etc.</p>"},{"location":"1.introduction/0-2.relates/#graph-related_technologies","title":"Graph-related technologies","text":"<p>Take a look at a panoramic view of graph technology in 2020 <sup>1</sup>.</p> <p></p> <p>There are many technologies that are related to graphs, which can be broadly classified into these categories:</p> <ul> <li>Infrastructure: Graph databases, graph computing (processing) engines, graph deep learning, cloud services, etc.</li> </ul> <ul> <li>Applications: Visualization, knowledge graph, anti-fraud, cyber security, social network, etc.</li> </ul> <ul> <li>Development tools: Graph query languages, modeling tools, development frameworks, and libraries.</li> </ul> <ul> <li>E-books and conferences, etc.</li> </ul>"},{"location":"1.introduction/0-2.relates/#graph_language","title":"Graph language","text":"<p>In the previous topic, we introduced the history of graph languages. In this section, we make a classification of the functions of graph languages.</p> <ul> <li>Nearest neighbor query (NNS): Query the neighboring edges, neighbors, or K-hops neighbors.</li> </ul> <ul> <li>Find one/all subgraphs that satisfy a given graph pattern. This problem is very close to Subgraph Isomorphism - two seemingly different graphs that are actually identical <sup>2</sup> as shown below.</li> </ul> <p></p> <ul> <li>Reachability (connectivity) problems: The most common reachability problem is the shortest path problem. Such problems are usually described in terms of Regular Path Query - a series of connected groups of vertices forming a path that needs to satisfy some regular expression.</li> </ul> <ul> <li>Analytic problems: It is related to some convergent operators, such as Average, Count, Max, Vertex Degree. Measures the distance between all two vertices, the degree of interaction between a vertex and other vertices.</li> </ul>"},{"location":"1.introduction/0-2.relates/#graph_database_and_graph_processing_systems","title":"Graph database and graph processing systems","text":"<p>A graph system usually includes a complex data pipeline <sup>3</sup>. From the data source (the left side of the picture below) to the processing output (the right side), multiple data processing steps and systems are used, such as the ETL module, Graph OLTP module, OLAP module, BI, and knowledge graph. </p> <p></p> <p>Graph databases and graph processing systems have different origins and specialties (and weaknesses).</p> <ul> <li>(Online) The graph database is designed for persistent storage management of graphs and efficient subgraph operations. Hard disks and network are the target operating devices, physical/logical data mapping, data integrity, and (fault) consistency are the main goals. Each request typically involves only a small part of the full graph and can usually be done on a single server. Request latency is usually in milliseconds or seconds, and request concurrency is typically in the thousands or hundreds of thousands. The early Neo4j was one of the origins of the graph database space.</li> </ul> <ul> <li>(Offline) The graph processing system is for high-volume, concurrency, iteration, processing, and analysis of the full graph. Memory and network are the target operating devices. Each request involves all graph vertices and requires all servers to be involved in its completion. The latency of a single request is in the range of minutes to hours (days). The request concurrency is in single digits. Google's Pregel <sup>4</sup> represents the typical origin of graph processing systems. Its point-centric programming abstraction and BSP's operational model constitute a programming paradigm that is a more graph-friendly API abstraction than the previous Hadoop Map-Reduce.</li> </ul> <p> <sup>5</sup></p>"},{"location":"1.introduction/0-2.relates/#graph_sharding_methods","title":"Graph sharding methods","text":"<p>For large-scale graph data, it is difficult to store it in the memory of a single server, and even just storing the graph structure itself is not enough. By increasing the capacity of a single server, its cost price usually rises exponentially.</p> <p>As the volume of data increases, for example, 100 billion data already exceeds the capacity of all commercially available servers on the market.</p> <p>Another option is to shard data and place each shard on a different server to increase reliability and performance. For NoSQL systems, such as key-value or document systems, the sharding method is intuitive and natural. Each record and data unit can usually be placed on a different server based on the key or docID.</p> <p>However, the sharding of data structures like graphs is usually less intuitive, because usually, graphs are \"fully connected\" and each vertex can be connected to any other vertex in usually 6 hops.</p> <p>And it has been theoretically proven that the graph sharding problem is NP.</p> <p>When distributing the entire graph data across multiple servers, the cross-server network access latency is 10 times higher than the hardware (memory) access time inside the same server. Therefore, for some depth-first traversal scenarios, a large number of cross-network accesses occur, resulting in extremely high overall latency.</p> <p><sup>6</sup></p> <p>Usually, graphs have a clear power-law distribution. A small number of vertices have much denser neighboring edges than the average vertices. Though processing these vertices can usually be within the same server which reduces cross-network access, load will be far more heavier than the average.</p> <p></p> <p></p> <p>The common graph sharding methods are as follows:</p> <ul> <li>Application-level sharding: The application layer senses and controls which shard each vertex and edge should locate on based on the type of vertices and edges. A set of vertices of the same type is placed on one sharding and another set of vertices of the same type is placed on another sharding. Of course, for high reliability, the sharding itself can also be made multiple replicas. When used by the application, the desired vertices and edges are fetched from each shard, and then on the off-application side (or some proxy server-side), the fetched data is assembled into the final result. This is typically represented by the Neo4j 4. x Fabric.</li> </ul> <p></p> <ul> <li>Using a distributed cache layer: Add a memory cache layer on the top of the hard disk and cache important portions of the sharding and data and preheat that cache.</li> </ul> <ul> <li>Adding read-only replicas or views: Add read-only replicas or create a view for some of the graph sharding, and pass the heavier load of read requests through these sharding servers.  </li> </ul> <ul> <li>Performing fine-grained graph sharding: Form multiple small partitions of vertices and edges instead of one large sharding, and then place the more correlated partitions on the same server as much as possible. <sup>7</sup>. </li> </ul> <p></p> <p>A mixture of these approaches is also used in specific engineering practices. Usually, offline graph processing systems perform some degree of graph preprocessing to improve the locality through an ETL process, while online graph database systems usually choose a periodic data rebalancing process to improve data locality. </p>"},{"location":"1.introduction/0-2.relates/#technical_challenges","title":"Technical challenges","text":"<p>In the literature <sup>8</sup>, a thorough investigation of graphs and challenges is done, and the following lists the top ten graph technology challenges.</p> <ul> <li>Scalability: Loading and upgrading big graphs, performing graph computation and graph traversal, use of triggers and supernodes</li> <li>Visualization: Customizable layouts, rendering and display big images, and display dynamic and updated display</li> <li>Query language and programming API: Language expressiveness, standards compatibility, compatibility with existing systems, design of subqueries, and associative queries across multiple graphs</li> <li>Faster graph algorithms</li> <li>Easy to use (configuration and usage)</li> <li>Performance metrics and testing</li> <li>General graph technology software (e.g., to handle offline, online, streaming computations.)</li> <li>ETL</li> <li>Debug and test</li> </ul>"},{"location":"1.introduction/0-2.relates/#open-source_graph_tools_on_single_machines","title":"Open-source graph tools on single machines","text":"<p>There is a common misconception about graph databases that any data access involving graph structure needs to be stored in a graph database. </p> <p>When the amount of data is not large, single machine memory is enough to store the data. You can use some single-machine open-source tools to store tens of millions of vertices and edges.</p> <ul> <li>JGraphT<sup>9</sup>: A well-known open-source Java graph theory library, which implements a considerable number of efficient graph algorithms.</li> </ul> <ul> <li>igraph<sup>10</sup>: A lightweight and powerful library, supporting R, Python, and C++.</li> </ul> <ul> <li>NetworkX<sup>11</sup>: The first choice for data scientists doing graph theory analysis.</li> </ul> <ul> <li>Cytoscape<sup>12</sup>: A powerful visual open-source graph analysis tool.</li> </ul> <ul> <li>Gephi<sup>13</sup>: A powerful visual open-source graph analysis tool.</li> </ul> <ul> <li>arrows.app<sup>14</sup>: A simple brain mapping tool for visually generating Cypher statements.</li> </ul>"},{"location":"1.introduction/0-2.relates/#industry_databases_and_benchmarks","title":"Industry databases and benchmarks","text":""},{"location":"1.introduction/0-2.relates/#ldbc","title":"LDBC","text":"<p>LDBC<sup>15</sup> (Linked Data Benchmark Council) is a non-profit organization composed of hardware and software giants such as Oracle, Intel and mainstream graph database vendors such as Neo4j and TigerGraph, which is the benchmark guide developer and test result publisher for graphs and has a high influence in the industry.</p> <p>SNB (Social Network Benchmark) is one of the benchmarks developed by the Linked Data Benchmark Committee (LDBC) for graph databases and is divided into two scenarios: interactive query (Interactive) and business intelligence (BI). Its role is similar to that of TPC-C, TPC-H, and other tests in SQL-type databases, which can help users compare the functions, performance, and capacity of various graph database products.</p> <p>An SNB dataset simulates the relationship between people and posts of a social network, taking into account the distribution properties of the social network, the activity of people, and other social information.</p> <p></p> <p>The standard data size ranges from 0.1 GB (scale factor 0.1) to 1000 GB (sf 1000). Larger data sets of 10 TB and 100 TB can also be generated. The number of vertices and edges is as shown below.</p> <p></p>"},{"location":"1.introduction/0-2.relates/#trends","title":"Trends","text":""},{"location":"1.introduction/0-2.relates/#graph_technologies_of_different_origins_and_goals_are_learning_from_and_integrating_with_each_other","title":"Graph technologies of different origins and goals are learning from and integrating with each other","text":""},{"location":"1.introduction/0-2.relates/#the_trends_in_cloud_computing_place_higher_demands_on_scalability","title":"The trends in cloud computing place higher demands on scalability.","text":"<p>According to Gartner's projections, cloud services have been growing at a rapid rate and penetration <sup>16</sup>. A large number of commercial software is gradually moving from a completely local and private model 10 years ago to a cloud services-based business model. One of the major advantages of cloud services is that they offer near-infinite scalability. It requires that various cloud infrastructure-based software must have a better ability to scale quickly and elastically. </p> <p></p>"},{"location":"1.introduction/0-2.relates/#trends_in_hardware_that_ssd_will_be_the_mainstream_persistent_device","title":"Trends in hardware that SSD will be the mainstream persistent device","text":"<p>Hardware determines software architecture. From the 1950s, when Moore's Law was discovered, to the 00s, when multi-core was introduced, hardware trends and speeds have profoundly determined software architecture. Database systems are mostly designed around \"hard disk + memory\", high-performance computing systems are mostly designed around \"memory + CPU\", and distributed systems are designed completely differently for 1 gigabit, 10 gigabits, and RDMA.</p> <p>Graph traversals are featured as random access. Early graph database systems adopted the large memory + HDD architecture. By designing some data structure in memory, random access can be achieved in memory (B+ trees, Hash tables) for the purpose of optimizing graph topology traversal. And then the random access was converted into sequential reads and writes suitable for HDDs. The entire software architecture (including the storage and compute layers) must be based on and built around such IO processes. With the decline in SSD prices <sup>17</sup>, SSDs are replacing HDDs as the dominant device. Friendly random access, deep IO queue, fast access are the features of SSD that differ from HDD's highly repetitive sequence, random latency, and easily damaged disk. The redesign for all software architectures becomes a heavy historical technical burden.</p> <p></p> <ol> <li> <p>https://graphaware.com/graphaware/2020/02/17/graph-technology-landscape-2020.html\u00a0\u21a9</p> </li> <li> <p>https://en.wikipedia.org/wiki/Graph_isomorphism\u00a0\u21a9</p> </li> <li> <p>The Future is Big Graphs! A Community View on Graph Processing Systems. https://arxiv.org/abs/2012.06171\u00a0\u21a9</p> </li> <li> <p>G. Malewicz, M. H. Austern, A. J. Bik, J. C. Dehnert, I. Horn, N. Leiser, and G. Czajkowski. Pregel: a system for large-scale graph processing. In Proceedings of the International Conference on Management of data (SIGMOD), pages 135\u2013146, New York, NY, USA, 2010. ACM\u00a0\u21a9</p> </li> <li> <p>https://neo4j.com/graphacademy/training-iga-40/02-iga-40-overview-of-graph-algorithms/\u00a0\u21a9</p> </li> <li> <p>https://livebook.manning.com/book/graph-powered-machine-learning/welcome/v-8/\u00a0\u21a9</p> </li> <li> <p>https://www.arangodb.com/learn/graphs/using-smartgraphs-arangodb/\u00a0\u21a9</p> </li> <li> <p>https://arxiv.org/abs/1709.03188\u00a0\u21a9</p> </li> <li> <p>https://jgrapht.org/\u00a0\u21a9</p> </li> <li> <p>https://igraph.org/\u00a0\u21a9</p> </li> <li> <p>https://networkx.org/\u00a0\u21a9</p> </li> <li> <p>https://cytoscape.org/\u00a0\u21a9</p> </li> <li> <p>https://gephi.org/\u00a0\u21a9</p> </li> <li> <p>https://arrows.app/\u00a0\u21a9</p> </li> <li> <p>https://github.com/ldbc/ldbc_snb_docs\u00a0\u21a9</p> </li> <li> <p>https://cloudcomputing-news.net/news/2019/apr/15/public-cloud-soaring-to-331b-by-2022-according-to-gartner/\u00a0\u21a9</p> </li> <li> <p>https://blocksandfiles.com/2021/01/25/wikibon-ssds-vs-hard-drives-wrights-law/\u00a0\u21a9</p> </li> </ol>"},{"location":"1.introduction/1.what-is-nebula-graph/","title":"What is NebulaGraph","text":"<p>NebulaGraph is an open-source, distributed, easily scalable, and native graph database. It is capable of hosting graphs with hundreds of billions of vertices and trillions of edges, and serving queries with millisecond-latency.</p> <p></p>"},{"location":"1.introduction/1.what-is-nebula-graph/#what_is_a_graph_database","title":"What is a graph database","text":"<p>A graph database, such as NebulaGraph, is a database that specializes in storing vast graph networks and retrieving information from them. It efficiently stores data as vertices (nodes) and edges (relationships) in labeled property graphs. Properties can be attached to both vertices and edges. Each vertex can have one or multiple tags (labels).</p> <p></p> <p>Graph databases are well suited for storing most kinds of data models abstracted from reality. Things are connected in almost all fields in the world. Modeling systems like relational databases extract the relationships between entities and squeeze them into table columns alone, with their types and properties stored in other columns or even other tables. This makes data management time-consuming and cost-ineffective.</p> <p>NebulaGraph, as a typical native graph database, allows you to store the rich relationships as edges with edge types and properties directly attached to them.</p>"},{"location":"1.introduction/1.what-is-nebula-graph/#advantages_of_nebulagraph","title":"Advantages of NebulaGraph","text":""},{"location":"1.introduction/1.what-is-nebula-graph/#open_source","title":"Open source","text":"<p>NebulaGraph is open under the Apache 2.0 License. More and more people such as database developers, data scientists, security experts, and algorithm engineers are participating in the designing and development of NebulaGraph. To join the opening of source code and ideas, surf the NebulaGraph GitHub page.</p>"},{"location":"1.introduction/1.what-is-nebula-graph/#outstanding_performance","title":"Outstanding performance","text":"<p>Written in C++ and born for graphs, NebulaGraph handles graph queries in milliseconds. Among most databases, NebulaGraph shows superior performance in providing graph data services. The larger the data size, the greater the superiority of NebulaGraph.For more information, see NebulaGraph benchmarking.</p>"},{"location":"1.introduction/1.what-is-nebula-graph/#high_scalability","title":"High scalability","text":"<p>NebulaGraph is designed in a shared-nothing architecture and supports scaling in and out without interrupting the database service.</p>"},{"location":"1.introduction/1.what-is-nebula-graph/#developer_friendly","title":"Developer friendly","text":"<p>NebulaGraph supports clients in popular programming languages like Java, Python, C++, and Go, and more are under development. For more information, see NebulaGraph clients.</p>"},{"location":"1.introduction/1.what-is-nebula-graph/#reliable_access_control","title":"Reliable access control","text":"<p>NebulaGraph supports strict role-based access control and external authentication servers such as LDAP (Lightweight Directory Access Protocol) servers to enhance data security. For more information, see Authentication and authorization.</p>"},{"location":"1.introduction/1.what-is-nebula-graph/#diversified_ecosystem","title":"Diversified ecosystem","text":"<p>More and more native tools of NebulaGraph have been released, such as NebulaGraph Studio, NebulaGraph Console, and NebulaGraph Exchange. For more ecosystem tools, see Ecosystem tools overview.</p> <p>Besides, NebulaGraph has the ability to be integrated with many cutting-edge technologies, such as Spark, Flink, and HBase, for the purpose of mutual strengthening in a world of increasing challenges and chances.</p>"},{"location":"1.introduction/1.what-is-nebula-graph/#opencypher-compatible_query_language","title":"OpenCypher-compatible query language","text":"<p>The native NebulaGraph Query Language, also known as nGQL, is a declarative, openCypher-compatible textual query language. It is easy to understand and easy to use. For more information, see nGQL guide.</p>"},{"location":"1.introduction/1.what-is-nebula-graph/#future-oriented_hardware_with_balanced_reading_and_writing","title":"Future-oriented hardware with balanced reading and writing","text":"<p>Solid-state drives have extremely high performance and they are getting cheaper. NebulaGraph is a product based on SSD. Compared with products based on HDD and large memory, it is more suitable for future hardware trends and easier to achieve balanced reading and writing.</p>"},{"location":"1.introduction/1.what-is-nebula-graph/#easy_data_modeling_and_high_flexibility","title":"Easy data modeling and high flexibility","text":"<p>You can easily model the connected data into NebulaGraph for your business without forcing them into a structure such as a relational table, and properties can be added, updated, and deleted freely. For more information, see Data modeling.</p>"},{"location":"1.introduction/1.what-is-nebula-graph/#high_popularity","title":"High popularity","text":"<p>NebulaGraph is being used by tech leaders such as Tencent, Vivo, Meituan, and JD Digits. For more information, visit the NebulaGraph official website.</p>"},{"location":"1.introduction/1.what-is-nebula-graph/#use_cases","title":"Use cases","text":"<p>NebulaGraph can be used to support various graph-based scenarios. To spare the time spent on pushing the kinds of data mentioned in this section into relational databases and on bothering with join queries, use NebulaGraph.</p>"},{"location":"1.introduction/1.what-is-nebula-graph/#fraud_detection","title":"Fraud detection","text":"<p>Financial institutions have to traverse countless transactions to piece together potential crimes and understand how combinations of transactions and devices might be related to a single fraud scheme. This kind of scenario can be modeled in graphs, and with the help of NebulaGraph, fraud rings and other sophisticated scams can be easily detected.</p>"},{"location":"1.introduction/1.what-is-nebula-graph/#real-time_recommendation","title":"Real-time recommendation","text":"<p>NebulaGraph offers the ability to instantly process the real-time information produced by a visitor and make accurate recommendations on articles, videos, products, and services.</p>"},{"location":"1.introduction/1.what-is-nebula-graph/#intelligent_question-answer_system","title":"Intelligent question-answer system","text":"<p>Natural languages can be transformed into knowledge graphs and stored in NebulaGraph. A question organized in a natural language can be resolved by a semantic parser in an intelligent question-answer system and re-organized. Then, possible answers to the question can be retrieved from the knowledge graph and provided to the one who asked the question.</p>"},{"location":"1.introduction/1.what-is-nebula-graph/#social_networking","title":"Social networking","text":"<p>Information on people and their relationships is typical graph data. NebulaGraph can easily handle the social networking information of billions of people and trillions of relationships, and provide lightning-fast queries for friend recommendations and job promotions in the case of massive concurrency.</p>"},{"location":"1.introduction/1.what-is-nebula-graph/#related_links","title":"Related links","text":"<ul> <li>Official website</li> <li>Docs</li> <li>Blogs</li> <li>Forum</li> <li>GitHub</li> </ul>"},{"location":"1.introduction/2.1.path/","title":"Path types","text":"<p>In graph theory, a path in a graph is a finite or infinite sequence of edges which joins a sequence of vertices. Paths are fundamental concepts of graph theory.</p> <p>Paths can be categorized into 3 types: <code>walk</code>, <code>trail</code>, and <code>path</code>. For more information, see Wikipedia.</p> <p>The following figure is an example for a brief introduction.</p> <p></p>"},{"location":"1.introduction/2.1.path/#walk","title":"Walk","text":"<p>A <code>walk</code> is a finite or infinite sequence of edges. Both vertices and edges can be repeatedly visited in graph traversal.</p> <p>In the above figure C, D, and E form a cycle. So, this figure contains infinite paths, such as <code>A-&gt;B-&gt;C-&gt;D-&gt;E</code>, <code>A-&gt;B-&gt;C-&gt;D-&gt;E-&gt;C</code>, and <code>A-&gt;B-&gt;C-&gt;D-&gt;E-&gt;C-&gt;D</code>.</p> <p>Note</p> <p><code>GO</code> statements use <code>walk</code>.</p>"},{"location":"1.introduction/2.1.path/#trail","title":"Trail","text":"<p>A <code>trail</code> is a finite sequence of edges. Only vertices can be repeatedly visited in graph traversal. The Seven Bridges of K\u00f6nigsberg is a typical <code>trail</code>.</p> <p>In the above figure, edges cannot be repeatedly visited. So, this figure contains finite paths. The longest path in this figure consists of 5 edges: <code>A-&gt;B-&gt;C-&gt;D-&gt;E-&gt;C</code>.</p> <p>Note</p> <p><code>MATCH</code>, <code>FIND PATH</code>, and <code>GET SUBGRAPH</code> statements use <code>trail</code>.</p> <p>There are two special cases of trail, <code>cycle</code> and <code>circuit</code>. The following figure is an example for a brief introduction.</p> <p></p> <ul> <li> <p>cycle</p> <p>A <code>cycle</code> refers to a closed <code>trail</code>. Only the terminal vertices can be repeatedly visited. The longest path in this figure consists of 3 edges: <code>A-&gt;B-&gt;C-&gt;A</code> or <code>C-&gt;D-&gt;E-&gt;C</code>.</p> </li> </ul> <ul> <li> <p>circuit</p> <p>A <code>circuit</code> refers to a closed <code>trail</code>. Edges cannot be repeatedly visited in graph traversal. Apart from the terminal vertices, other vertices can also be repeatedly visited. The longest path in this figure: <code>A-&gt;B-&gt;C-&gt;D-&gt;E-&gt;C-&gt;A</code>.</p> </li> </ul>"},{"location":"1.introduction/2.1.path/#path","title":"Path","text":"<p>A <code>path</code> is a finite sequence of edges. Neither vertices nor edges can be repeatedly visited in graph traversal.</p> <p>So, the above figure contains finite paths. The longest path in this figure consists of 4 edges: <code>A-&gt;B-&gt;C-&gt;D-&gt;E</code>.</p>"},{"location":"1.introduction/2.data-model/","title":"Data modeling","text":"<p>A data model is a model that organizes data and specifies how they are related to one another. This topic describes the Nebula\u00a0Graph data model and provides suggestions for data modeling with NebulaGraph.</p>"},{"location":"1.introduction/2.data-model/#data_structures","title":"Data structures","text":"<p>NebulaGraph data model uses six data structures to store data. They are graph spaces, vertices, edges, tags, edge types and properties.</p> <ul> <li>Graph spaces: Graph spaces are used to isolate data from different teams or programs. Data stored in different graph spaces are securely isolated. Storage replications, privileges, and partitions can be assigned.</li> </ul> <ul> <li>Vertices: Vertices are used to store entities.</li> </ul> <ul> <li> <p>In NebulaGraph, vertices are identified with vertex identifiers (i.e. <code>VID</code>). The <code>VID</code> must be unique in the same graph space. VID should be int64, or fixed_string(N).</p> <ul> <li>A vertex has zero to multiple tags.</li> </ul> <p>Compatibility</p> <p>In NebulaGraph 2.x a vertex must have at least one tag. And in NebulaGraph 3.5.0, a tag is not required for a vertex.</p> </li> </ul> <ul> <li>Edges: Edges are used to connect vertices. An edge is a connection or behavior between two vertices.<ul> <li>There can be multiple edges between two vertices.</li> <li>Edges are directed. <code>-&gt;</code> identifies the directions of edges. Edges can be traversed in either direction.</li> <li>An edge is identified uniquely with <code>&lt;a source vertex, an edge type, a rank value, and a destination vertex&gt;</code>. Edges have no EID.</li> <li>An edge must have one and only one edge type.</li> <li>The rank value is an immutable user-assigned 64-bit signed integer. It identifies the edges with the same edge type between two vertices. Edges are sorted by their rank values. The edge with the greatest rank value is listed first. The default rank value is zero.</li> </ul> </li> </ul> <ul> <li>Tags: Tags are used to categorize vertices. Vertices that have the same tag share the same definition of properties.</li> </ul> <ul> <li>Edge types: Edge types are used to categorize edges. Edges that have the same edge type share the same definition of properties.</li> </ul> <ul> <li>Properties: Properties are key-value pairs. Both vertices and edges are containers for properties.</li> </ul> <p>Note</p> <p>Tags and Edge types are similar to \"vertex tables\" and \"edge tables\" in the relational databases.</p>"},{"location":"1.introduction/2.data-model/#directed_property_graph","title":"Directed property graph","text":"<p>NebulaGraph stores data in directed property graphs. A directed property graph has a set of vertices connected by directed edges. Both vertices and edges can have properties. A directed property graph is represented as:</p> <p>G = &lt; V, E, P<sub>V</sub>, P<sub>E</sub> &gt;</p> <ul> <li>V is a set of vertices.</li> <li>E is a set of directed edges.</li> <li>P<sub>V</sub> is the property of vertices.</li> <li>P<sub>E</sub> is the property of edges.</li> </ul> <p>The following table is an example of the structure of the basketball player dataset. We have two types of vertices, that is player and team, and two types of edges, that is serve and follow.</p> Element Name Property name (Data type) Description Tag player name (string) age (int) Represents players in the team. Tag team name (string) Represents the teams. Edge type serve start_year (int)  end_year (int) Represents actions taken by players in the team.An action links a player with a team, and the direction is from a player to a team. Edge type follow degree (int) Represents actions taken by players in the team.An action links a player with another player, and the direction is from one player to the other player. <p>Note</p> <p>NebulaGraph supports only directed edges.</p> <p>Compatibility</p> <p>NebulaGraph 3.5.0 allows dangling edges. Therefore, when adding or deleting, you need to ensure the corresponding source vertex and destination vertex of an edge exist. For details, see INSERT VERTEX, DELETE VERTEX, INSERT EDGE, and DELETE EDGE.</p> <p>The MERGE statement in openCypher is not supported.</p>"},{"location":"1.introduction/3.vid/","title":"VID","text":"<p>In a graph space, a vertex is uniquely identified by its ID, which is called a VID or a Vertex ID.</p>"},{"location":"1.introduction/3.vid/#features","title":"Features","text":"<ul> <li>The data types of VIDs are restricted to <code>FIXED_STRING(&lt;N&gt;)</code> or <code>INT64</code>. One graph space can only select one VID type.</li> </ul> <ul> <li>A VID in a graph space is unique. It functions just as a primary key in a relational database. VIDs in different graph spaces are independent.</li> </ul> <ul> <li>The VID generation method must be set by users, because NebulaGraph does not provide auto increasing ID, or UUID.</li> </ul> <ul> <li> <p>Vertices with the same VID will be identified as the same one. For example:</p> <ul> <li>A VID is the unique identifier of an entity, like a person's ID card number. A tag means the type of an entity,  such as driver, and boss. Different tags define two groups of different properties, such as driving license number, driving age, order amount, order taking alt, and job number, payroll, debt ceiling, business phone number.</li> </ul> <ul> <li>When two <code>INSERT</code> statements (neither uses a parameter of <code>IF NOT EXISTS</code>) with the same VID and tag are operated at the same time, the latter <code>INSERT</code> will overwrite the former.</li> </ul> <ul> <li>When two <code>INSERT</code> statements with the same VID but different tags, like <code>TAG A</code> and <code>TAG B</code>, are operated at the same time, the operation of <code>Tag A</code> will not affect <code>Tag B</code>.</li> </ul> </li> </ul> <ul> <li>VIDs will usually be indexed and stored into memory (in the way of LSM-tree). Thus, direct access to VIDs enjoys peak performance.</li> </ul>"},{"location":"1.introduction/3.vid/#vid_operation","title":"VID Operation","text":"<ul> <li>NebulaGraph 1.x only supports <code>INT64</code> while NebulaGraph 2.x supports <code>INT64</code> and <code>FIXED_STRING(&lt;N&gt;)</code>. In <code>CREATE SPACE</code>, VID types can be set via <code>vid_type</code>.</li> </ul> <ul> <li><code>id()</code> function can be used to specify or locate a VID.</li> </ul> <ul> <li><code>LOOKUP</code> or <code>MATCH</code> statements can be used to find a VID via property index.</li> </ul> <ul> <li>Direct access to vertices statements via VIDs enjoys peak performance, such as <code>DELETE xxx WHERE id(xxx) == \"player100\"</code> or <code>GO FROM \"player100\"</code>. Finding VIDs via properties and then operating the graph will cause poor performance, such as <code>LOOKUP | GO FROM $-.ids</code>, which will run both <code>LOOKUP</code> and <code>|</code> one more time.</li> </ul>"},{"location":"1.introduction/3.vid/#vid_generation","title":"VID Generation","text":"<p>VIDs can be generated via applications. Here are some tips:</p> <ul> <li>(Optimal) Directly take a unique primary key or property as a VID. Property access depends on the VID.</li> </ul> <ul> <li>Generate a VID via a unique combination of properties. Property access depends on property index.</li> </ul> <ul> <li>Generate a VID via algorithms like snowflake. Property access depends on property index.</li> </ul> <ul> <li>If short primary keys greatly outnumber long primary keys, do not enlarge the <code>N</code> of <code>FIXED_STRING(&lt;N&gt;)</code> too much. Otherwise, it will occupy a lot of memory and hard disks, and slow down performance. Generate VIDs via BASE64, MD5, hash by encoding and splicing.</li> </ul> <ul> <li>If you generate int64 VID via hash, the probability of collision is about 1/10 when there are 1 billion vertices. The number of edges has no concern with the probability of collision.</li> </ul>"},{"location":"1.introduction/3.vid/#define_and_modify_a_vid_and_its_data_type","title":"Define and modify a VID and its data type","text":"<p>The data type of a VID must be defined when you create the graph space. Once defined, it cannot be modified.</p> <p>A VID is set when you insert a vertex and cannot be modified. </p>"},{"location":"1.introduction/3.vid/#query_start_vid_and_global_scan","title":"Query <code>start vid</code> and global scan","text":"<p>In most cases, the execution plan of query statements in NebulaGraph (<code>MATCH</code>, <code>GO</code>, and <code>LOOKUP</code>) must query the <code>start vid</code> in a certain way.</p> <p>There are only two ways to locate <code>start vid</code>:</p> <ol> <li> <p>For example, <code>GO FROM \"player100\" OVER</code> explicitly indicates in the statement that <code>start vid</code> is \"player100\".</p> </li> <li> <p>For example, <code>LOOKUP ON player WHERE player.name == \"Tony Parker\"</code> or <code>MATCH (v:player {name:\"Tony Parker\"})</code> locates <code>start vid</code> by the index of the property <code>player.name</code>.</p> </li> </ol>"},{"location":"1.introduction/3.nebula-graph-architecture/1.architecture-overview/","title":"Architecture overview","text":"<p>NebulaGraph consists of three services: the Graph Service, the Storage Service, and the Meta Service. It applies the separation of storage and computing architecture.</p> <p>Each service has its executable binaries and processes launched from the binaries. Users can deploy a NebulaGraph cluster on a single machine or multiple machines using these binaries.</p> <p>The following figure shows the architecture of a typical NebulaGraph cluster.</p> <p></p>"},{"location":"1.introduction/3.nebula-graph-architecture/1.architecture-overview/#the_meta_service","title":"The Meta Service","text":"<p>The Meta Service in the NebulaGraph architecture is run by the nebula-metad processes. It is responsible for metadata management, such as schema operations, cluster administration, and user privilege management.</p> <p>For details on the Meta Service, see Meta Service.</p>"},{"location":"1.introduction/3.nebula-graph-architecture/1.architecture-overview/#the_graph_service_and_the_storage_service","title":"The Graph Service and the Storage Service","text":"<p>NebulaGraph applies the separation of storage and computing architecture. The Graph Service is responsible for querying. The Storage Service is responsible for storage. They are run by different processes, i.e., nebula-graphd and nebula-storaged. The benefits of the separation of storage and computing architecture are as follows:</p> <ul> <li>Great scalability<p>The separated structure makes both the Graph Service and the Storage Service flexible and easy to scale in or out.</p> </li> </ul> <ul> <li>High availability<p>If part of the Graph Service fails, the data stored by the Storage Service suffers no loss. And if the rest part of the Graph Service is still able to serve the clients, service recovery can be performed quickly, even unfelt by the users.</p> </li> </ul> <ul> <li>Cost-effective<p>The separation of storage and computing architecture provides a higher resource utilization rate, and it enables clients to manage the cost flexibly according to business demands.</p> </li> </ul> <ul> <li>Open to more possibilities<p>With the ability to run separately, the Graph Service may work with multiple types of storage engines, and the Storage Service may also serve more types of computing engines.</p> </li> </ul> <p>For details on the Graph Service and the Storage Service, see Graph Service and Storage Service.</p>"},{"location":"1.introduction/3.nebula-graph-architecture/2.meta-service/","title":"Meta Service","text":"<p>This topic introduces the architecture and functions of the Meta Service.</p>"},{"location":"1.introduction/3.nebula-graph-architecture/2.meta-service/#the_architecture_of_the_meta_service","title":"The architecture of the Meta Service","text":"<p>The architecture of the Meta Service is as follows:</p> <p></p> <p>The Meta Service is run by nebula-metad processes. Users can deploy nebula-metad processes according to the scenario:</p> <ul> <li>In a test environment, users can deploy one or three nebula-metad processes on different machines or a single machine.</li> <li>In a production environment, we recommend that users deploy three nebula-metad processes on different machines for high availability.</li> </ul> <p>All the nebula-metad processes form a Raft-based cluster, with one process as the leader and the others as the followers.</p> <p>The leader is elected by the majorities and only the leader can provide service to the clients or other components of NebulaGraph. The followers will be run in a standby way and each has a data replication of the leader. Once the leader fails, one of the followers will be elected as the new leader.</p> <p>Note</p> <p>The data of the leader and the followers will keep consistent through Raft. Thus the breakdown and election of the leader will not cause data inconsistency. For more information on Raft, see Storage service architecture.</p>"},{"location":"1.introduction/3.nebula-graph-architecture/2.meta-service/#functions_of_the_meta_service","title":"Functions of the Meta Service","text":""},{"location":"1.introduction/3.nebula-graph-architecture/2.meta-service/#manages_user_accounts","title":"Manages user accounts","text":"<p>The Meta Service stores the information of user accounts and the privileges granted to the accounts. When the clients send queries to the Meta Service through an account, the Meta Service checks the account information and whether the account has the right privileges to execute the queries or not.</p> <p>For more information on NebulaGraph access control, see Authentication.</p>"},{"location":"1.introduction/3.nebula-graph-architecture/2.meta-service/#manages_partitions","title":"Manages partitions","text":"<p>The Meta Service stores and manages the locations of the storage partitions and helps balance the partitions.</p>"},{"location":"1.introduction/3.nebula-graph-architecture/2.meta-service/#manages_graph_spaces","title":"Manages graph spaces","text":"<p>NebulaGraph supports multiple graph spaces. Data stored in different graph spaces are securely isolated. The Meta Service stores the metadata of all graph spaces and tracks the changes of them, such as adding or dropping a graph space.</p>"},{"location":"1.introduction/3.nebula-graph-architecture/2.meta-service/#manages_schema_information","title":"Manages schema information","text":"<p>NebulaGraph is a strong-typed graph database. Its schema contains tags (i.e., the vertex types), edge types, tag properties, and edge type properties.</p> <p>The Meta Service stores the schema information. Besides, it performs the addition, modification, and deletion of the schema, and logs the versions of them.</p> <p>For more information on NebulaGraph schema, see Data model.</p>"},{"location":"1.introduction/3.nebula-graph-architecture/2.meta-service/#manages_ttl_information","title":"Manages TTL information","text":"<p>The Meta Service stores the definition of TTL (Time to Live) options which are used to control data expiration. The Storage Service takes care of the expiring and evicting processes. For more information, see TTL.</p>"},{"location":"1.introduction/3.nebula-graph-architecture/2.meta-service/#manages_jobs","title":"Manages jobs","text":"<p>The Job Management module in the Meta Service is responsible for the creation, queuing, querying, and deletion of jobs.</p>"},{"location":"1.introduction/3.nebula-graph-architecture/3.graph-service/","title":"Graph Service","text":"<p>The Graph Service is used to process the query. It has four submodules: Parser, Validator, Planner, and Executor. This topic will describe the Graph Service accordingly.</p>"},{"location":"1.introduction/3.nebula-graph-architecture/3.graph-service/#the_architecture_of_the_graph_service","title":"The architecture of the Graph Service","text":"<p>After a query is sent to the Graph Service, it will be processed by the following four submodules:</p> <ol> <li> <p>Parser: Performs lexical analysis and syntax analysis.</p> </li> <li> <p>Validator: Validates the statements.</p> </li> <li> <p>Planner: Generates and optimizes the execution plans.</p> </li> <li> <p>Executor: Executes the plans with operators.</p> </li> </ol>"},{"location":"1.introduction/3.nebula-graph-architecture/3.graph-service/#parser","title":"Parser","text":"<p>After receiving a request, the statements will be parsed by Parser composed of Flex (lexical analysis tool) and Bison (syntax analysis tool), and its corresponding AST will be generated. Statements will be directly intercepted in this stage because of their invalid syntax.</p> <p>For example, the structure of the AST of <code>GO FROM \"Tim\" OVER like WHERE properties(edge).likeness &gt; 8.0 YIELD dst(edge)</code> is shown in the following figure.</p> <p></p>"},{"location":"1.introduction/3.nebula-graph-architecture/3.graph-service/#validator","title":"Validator","text":"<p>Validator performs a series of validations on the AST. It mainly works on these tasks:</p> <ul> <li>Validating metadata<p>Validator will validate whether the metadata is correct or not.</p> <p>When parsing the <code>OVER</code>, <code>WHERE</code>, and <code>YIELD</code> clauses, Validator looks up the Schema and verifies whether the edge type and tag data exist or not. For an <code>INSERT</code> statement, Validator verifies whether the types of the inserted data are the same as the ones defined in the Schema.</p> </li> </ul> <ul> <li>Validating contextual reference<p>Validator will verify whether the cited variable exists or not, or whether the cited property is variable or not.</p> <p>For composite statements, like <code>$var = GO FROM \"Tim\" OVER like YIELD dst(edge) AS ID; GO FROM $var.ID OVER serve YIELD dst(edge)</code>, Validator verifies first to see if <code>var</code> is defined, and then to check if the <code>ID</code> property is attached to the <code>var</code> variable.</p> </li> </ul> <ul> <li>Validating type inference<p>Validator infers what type the result of an expression is and verifies the type against the specified clause.</p> <p>For example, the <code>WHERE</code> clause requires the result to be a <code>bool</code> value, a <code>NULL</code> value, or <code>empty</code>.</p> </li> </ul> <ul> <li>Validating the information of <code>*</code><p>Validator needs to verify all the Schema that involves <code>*</code> when verifying the clause if there is a <code>*</code> in the statement.</p> <p>Take a statement like <code>GO FROM \"Tim\" OVER * YIELD dst(edge), properties(edge).likeness, dst(edge)</code> as an example. When verifying the <code>OVER</code> clause, Validator needs to verify all the edge types. If the edge type includes <code>like</code> and <code>serve</code>, the statement would be <code>GO FROM \"Tim\" OVER like,serve YIELD dst(edge), properties(edge).likeness, dst(edge)</code>.</p> </li> </ul> <ul> <li>Validating input and output<p>Validator will check the consistency of the clauses before and after the <code>|</code>.</p> <p>In the statement <code>GO FROM \"Tim\" OVER like YIELD dst(edge) AS ID | GO FROM $-.ID OVER serve YIELD dst(edge)</code>, Validator will verify whether <code>$-.ID</code> is defined in the clause before the <code>|</code>.</p> </li> </ul> <p>When the validation succeeds, an execution plan will be generated. Its data structure will be stored in the <code>src/planner</code> directory.</p>"},{"location":"1.introduction/3.nebula-graph-architecture/3.graph-service/#planner","title":"Planner","text":"<p>In the <code>nebula-graphd.conf</code> file, when <code>enable_optimizer</code> is set to be <code>false</code>, Planner will not optimize the execution plans generated by Validator. It will be executed by Executor directly.</p> <p>In the <code>nebula-graphd.conf</code> file, when <code>enable_optimizer</code> is set to be <code>true</code>, Planner will optimize the execution plans generated by Validator. The structure is as follows.</p> <p></p> <ul> <li>Before optimization<p>In the execution plan on the right side of the preceding figure, each node directly depends on other nodes. For example, the root node <code>Project</code> depends on the <code>Filter</code> node, the <code>Filter</code> node depends on the <code>GetNeighbor</code> node, and so on, up to the leaf node <code>Start</code>. Then the execution plan is (not truly) executed.</p> <p>During this stage, every node has its input and output variables, which are stored in a hash table. The execution plan is not truly executed, so the value of each key in the associated hash table is empty (except for the <code>Start</code> node, where the input variables hold the starting data), and the hash table is defined in <code>src/context/ExecutionContext.cpp</code> under the <code>nebula-graph</code> repository.</p> <p>For example, if the hash table is named as <code>ResultMap</code> when creating the <code>Filter</code> node, users can determine that the node takes data from <code>ResultMap[\"GN1\"]</code>, then puts the result into <code>ResultMap[\"Filter2\"]</code>, and so on. All these work as the input and output of each node.</p> </li> </ul> <ul> <li>Process of optimization<p>The optimization rules that Planner has implemented so far are considered RBO (Rule-Based Optimization), namely the pre-defined optimization rules. The CBO (Cost-Based Optimization) feature is under development. The optimized code is in the <code>src/optimizer/</code> directory under the <code>nebula-graph</code> repository.</p> <p>RBO is a \u201cbottom-up\u201d exploration process. For each rule, the root node of the execution plan (in this case, the <code>Project</code> node) is the entry point, and step by step along with the node dependencies, it reaches the node at the bottom to see if it matches the rule.</p> <p>As shown in the preceding figure, when the <code>Filter</code> node is explored, it is found that its children node is <code>GetNeighbors</code>, which matches successfully with the pre-defined rules, so a transformation is initiated to integrate the <code>Filter</code> node into the <code>GetNeighbors</code> node, the <code>Filter</code> node is removed, and then the process continues to the next rule. Therefore, when the <code>GetNeighbor</code> operator calls interfaces of the Storage layer to get the neighboring edges of a vertex during the execution stage, the Storage layer will directly filter out the unqualified edges internally. Such optimization greatly reduces the amount of data transfer, which is commonly known as filter pushdown.</p> </li> </ul>"},{"location":"1.introduction/3.nebula-graph-architecture/3.graph-service/#executor","title":"Executor","text":"<p>The Executor module consists of Scheduler and Executor. The Scheduler generates the corresponding execution operators against the execution plan, starting from the leaf nodes and ending at the root node. The structure is as follows.</p> <p></p> <p>Each node of the execution plan has one execution operator node, whose input and output have been determined in the execution plan. Each operator only needs to get the values for the input variables, compute them, and finally put the results into the corresponding output variables. Therefore, it is only necessary to execute step by step from <code>Start</code>, and the result of the last operator is returned to the user as the final result.</p>"},{"location":"1.introduction/3.nebula-graph-architecture/3.graph-service/#source_code_hierarchy","title":"Source code hierarchy","text":"<p>The source code hierarchy under the nebula-graph repository is as follows.</p> <pre><code>|--src\n   |--graph\n      |--context    //contexts for validation and execution\n      |--executor   //execution operators\n      |--gc         //garbage collector\n      |--optimizer  //optimization rules\n      |--planner    //structure of the execution plans\n      |--scheduler  //scheduler\n      |--service    //external service management\n      |--session    //session management\n      |--stats      //monitoring metrics\n      |--util       //basic components\n      |--validator  //validation of the statements\n      |--visitor    //visitor expression\n</code></pre>"},{"location":"1.introduction/3.nebula-graph-architecture/4.storage-service/","title":"Storage Service","text":"<p>The persistent data of NebulaGraph have two parts. One is the Meta Service that stores the meta-related data.</p> <p>The other is the Storage Service that stores the data, which is run by the nebula-storaged process. This topic will describe the architecture of the Storage Service.</p>"},{"location":"1.introduction/3.nebula-graph-architecture/4.storage-service/#advantages","title":"Advantages","text":"<ul> <li>High performance (Customized built-in KVStore)</li> </ul> <ul> <li>Great scalability (Shared-nothing architecture, not rely on NAS/SAN-like devices)</li> </ul> <ul> <li>Strong consistency (Raft)</li> </ul> <ul> <li>High availability (Raft)</li> </ul> <ul> <li>Supports synchronizing with the third party systems, such as Elasticsearch.</li> </ul>"},{"location":"1.introduction/3.nebula-graph-architecture/4.storage-service/#the_architecture_of_the_storage_service","title":"The architecture of the Storage Service","text":"<p>The Storage Service is run by the nebula-storaged process. Users can deploy nebula-storaged processes on different occasions. For example, users can deploy 1 nebula-storaged process in a test environment and deploy 3 nebula-storaged processes in a production environment.</p> <p>All the nebula-storaged processes consist of a Raft-based cluster. There are three layers in the Storage Service:</p> <ul> <li> <p>Storage interface</p> <p>The top layer is the storage interface. It defines a set of APIs that are related to the graph concepts. These API requests will be translated into a set of KV operations targeting the corresponding Partition. For example:</p> <ul> <li><code>getNeighbors</code>: queries the in-edge or out-edge of a set of vertices, returns the edges and the corresponding properties, and supports conditional filtering.</li> </ul> <ul> <li><code>insert vertex/edge</code>: inserts a vertex or edge and its properties.</li> </ul> <ul> <li><code>getProps</code>: gets the properties of a vertex or an edge.</li> </ul> <p>It is this layer that makes the Storage Service a real graph storage. Otherwise, it is just a KV storage.</p> </li> </ul> <ul> <li> <p>Consensus</p> <p>Below the storage interface is the consensus layer that implements Multi Group Raft, which ensures the strong consistency and high availability of the Storage Service.</p> </li> </ul> <ul> <li> <p>Store engine</p> <p>The bottom layer is the local storage engine library, providing operations like <code>get</code>, <code>put</code>, and <code>scan</code> on local disks. The related interfaces are stored in <code>KVStore.h</code> and <code>KVEngine.h</code> files. You can develop your own local store plugins based on your needs.</p> </li> </ul> <p>The following will describe some features of the Storage Service based on the above architecture.</p>"},{"location":"1.introduction/3.nebula-graph-architecture/4.storage-service/#storage_writing_process","title":"Storage writing process","text":""},{"location":"1.introduction/3.nebula-graph-architecture/4.storage-service/#kvstore","title":"KVStore","text":"<p>NebulaGraph develops and customizes its built-in KVStore for the following reasons.</p> <ul> <li>It is a high-performance KVStore.</li> </ul> <ul> <li>It is provided as a (kv) library and can be easily developed for the filter pushdown purpose. As a strong-typed database, how to provide Schema during pushdown is the key to efficiency for NebulaGraph.</li> </ul> <ul> <li>It has strong data consistency.</li> </ul> <p>Therefore, NebulaGraph develops its own KVStore with RocksDB as the local storage engine. The advantages are as follows.</p> <ul> <li>For multiple local hard disks, NebulaGraph can make full use of its concurrent capacities through deploying multiple data directories.</li> </ul> <ul> <li> <p>The Meta Service manages all the Storage servers. All the partition distribution data and current machine status can be found in the meta service. Accordingly, users can execute a manual load balancing plan in meta service.</p> <p>Note</p> <p>NebulaGraph does not support auto load balancing because auto data transfer will affect online business.</p> </li> </ul> <ul> <li>NebulaGraph provides its own WAL mode so one can customize the WAL. Each partition owns its WAL.</li> </ul> <ul> <li>One NebulaGraph KVStore cluster supports multiple graph spaces, and each graph space has its own partition number and replica copies. Different graph spaces are isolated physically from each other in the same cluster.</li> </ul>"},{"location":"1.introduction/3.nebula-graph-architecture/4.storage-service/#data_storage_structure","title":"Data storage structure","text":"<p>Graphs consist of vertices and edges. NebulaGraph uses key-value pairs to store vertices, edges, and their properties. Vertices and edges are stored in keys and their properties are stored in values. Such structure enables efficient property filtering.</p> <ul> <li> <p>The storage structure of vertices</p> <p>Different from NebulaGraph version 2.x, version 3.x added a new key for each vertex. Compared to the old key that still exists, the new key has no <code>TagID</code> field and no value. Vertices in NebulaGraph can now live without tags owing to the new key.</p> <p></p> Field Description <code>Type</code> One byte, used to indicate the key type. <code>PartID</code> Three bytes, used to indicate the sharding partition and to scan the partition data based on the prefix when re-balancing the partition. <code>VertexID</code> The vertex ID. For an integer VertexID, it occupies eight bytes. However, for a string VertexID, it is changed to <code>fixed_string</code> of a fixed length which needs to be specified by users when they create the space. <code>TagID</code> Four bytes, used to indicate the tags that vertex relate with. <code>SerializedValue</code> The serialized value of the key. It stores the property information of the vertex. </li> </ul> <ul> <li>The storage structure of edges<p></p> Field Description <code>Type</code> One byte, used to indicate the key type. <code>PartID</code> Three bytes, used to indicate the partition ID. This field can be used to scan the partition data based on the prefix when re-balancing the partition. <code>VertexID</code> Used to indicate vertex ID. The former VID refers to the source VID in the outgoing edge and the dest VID in the incoming edge, while the latter VID refers to the dest VID in the outgoing edge and the source VID in the incoming edge. <code>Edge Type</code> Four bytes, used to indicate the edge type. Greater than zero indicates out-edge, less than zero means in-edge. <code>Rank</code> Eight bytes, used to indicate multiple edges in one edge type. Users can set the field based on needs and store weight, such as transaction time and transaction number. <code>PlaceHolder</code> One byte. Reserved. <code>SerializedValue</code> The serialized value of the key. It stores the property information of the edge. </li> </ul>"},{"location":"1.introduction/3.nebula-graph-architecture/4.storage-service/#property_descriptions","title":"Property descriptions","text":"<p>NebulaGraph uses strong-typed Schema.</p> <p>NebulaGraph will store the properties of vertex and edges in order after encoding them. Since the length of fixed-length properties is fixed, queries can be made in no time according to offset. Before decoding, NebulaGraph needs to get (and cache) the schema information in the Meta Service. In addition, when encoding properties, NebulaGraph will add the corresponding schema version to support online schema change.</p>"},{"location":"1.introduction/3.nebula-graph-architecture/4.storage-service/#data_partitioning","title":"Data partitioning","text":"<p>Since in an ultra-large-scale relational network, vertices can be as many as tens to hundreds of billions, and edges are even more than trillions. Even if only vertices and edges are stored, the storage capacity of both exceeds that of ordinary servers. Therefore, NebulaGraph uses hash to shard the graph elements and store them in different partitions.</p> <p></p>"},{"location":"1.introduction/3.nebula-graph-architecture/4.storage-service/#edge_partitioning_and_storage_amplification","title":"Edge partitioning and storage amplification","text":"<p>In NebulaGraph, an edge corresponds to two key-value pairs on the hard disk. When there are lots of edges and each has many properties, storage amplification will be obvious. The storage format of edges is shown in the figure below.</p> <p></p> <p>In this example, SrcVertex connects DstVertex via EdgeA, forming the path of <code>(SrcVertex)-[EdgeA]-&gt;(DstVertex)</code>. SrcVertex, DstVertex, and EdgeA will all be stored in Partition x and Partition y as four key-value pairs in the storage layer. Details are as follows:</p> <ul> <li>The key value of SrcVertex is stored in Partition x. Key fields include Type, PartID(x), VID(Src), and TagID. SerializedValue, namely Value, refers to serialized vertex properties.</li> </ul> <ul> <li>The first key value of EdgeA, namely EdgeA_Out, is stored in the same partition as the SrcVertex. Key fields include Type, PartID(x), VID(Src), EdgeType(+ means out-edge), Rank(0), VID(Dst), and PlaceHolder. SerializedValue, namely Value, refers to serialized edge properties.</li> </ul> <ul> <li>The key value of DstVertex is stored in Partition y. Key fields include Type, PartID(y), VID(Dst), and TagID. SerializedValue, namely Value, refers to serialized vertex properties.</li> </ul> <ul> <li>The second key value of EdgeA, namely EdgeA_In, is stored in the same partition as the DstVertex. Key fields include Type, PartID(y), VID(Dst), EdgeType(- means in-edge), Rank(0), VID(Src), and PlaceHolder. SerializedValue, namely Value, refers to serialized edge properties, which is exactly the same as that in EdgeA_Out.</li> </ul> <p>EdgeA_Out and EdgeA_In are stored in storage layer with opposite directions, constituting EdgeA logically. EdgeA_Out is used for traversal requests starting from SrcVertex, such as <code>(a)-[]-&gt;()</code>; EdgeA_In is used for traversal requests starting from DstVertex, such as <code>()-[]-&gt;(a)</code>.</p> <p>Like EdgeA_Out and EdgeA_In, NebulaGraph redundantly stores the information of each edge, which doubles the actual capacities needed for edge storage. The key corresponding to the edge occupies a small hard disk space, but the space occupied by Value is proportional to the length and amount of the property value. Therefore, it will occupy a relatively large hard disk space if the property value of the edge is large or there are many edge property values.</p>"},{"location":"1.introduction/3.nebula-graph-architecture/4.storage-service/#partition_algorithm","title":"Partition algorithm","text":"<p>NebulaGraph uses a static Hash strategy to shard data through a modulo operation on vertex ID. All the out-keys, in-keys, and tag data will be placed in the same partition. In this way, query efficiency is increased dramatically.</p> <p>Note</p> <p>The number of partitions needs to be determined when users are creating a graph space since it cannot be changed afterward. Users are supposed to take into consideration the demands of future business when setting it.</p> <p>When inserting into NebulaGraph, vertices and edges are distributed across different partitions. And the partitions are located on different machines. The number of partitions is set in the CREATE SPACE statement and cannot be changed afterward.</p> <p>If certain vertices need to be placed on the same partition (i.e., on the same machine), see Formula/code.</p> <p>The following code will briefly describe the relationship between VID and partition.</p> <pre><code>// If VertexID occupies 8 bytes, it will be stored in int64 to be compatible with the version 1.0.\nuint64_t vid = 0;\nif (id.size() == 8) {\n    memcpy(static_cast&lt;void*&gt;(&amp;vid), id.data(), 8);\n} else {\n    MurmurHash2 hash;\n    vid = hash(id.data());\n}\nPartitionID pId = vid % numParts + 1;\n</code></pre> <p>Roughly speaking, after hashing a fixed string to int64, (the hashing of int64 is the number itself), do modulo, and then plus one, namely:</p> <pre><code>pId = vid % numParts + 1;\n</code></pre> <p>Parameters and descriptions of the preceding formula are as follows:</p> Parameter Description <code>%</code> The modulo operation. <code>numParts</code> The number of partitions for the graph space where the <code>VID</code> is located, namely the value of <code>partition_num</code> in the CREATE SPACE statement. <code>pId</code> The ID for the partition where the <code>VID</code> is located. <p>Suppose there are 100 partitions, the vertices with <code>VID</code> 1, 101, and 1001 will be stored on the same partition. But, the mapping between the partition ID and the machine address is random. Therefore, we cannot assume that any two partitions are located on the same machine.</p>"},{"location":"1.introduction/3.nebula-graph-architecture/4.storage-service/#raft","title":"Raft","text":""},{"location":"1.introduction/3.nebula-graph-architecture/4.storage-service/#raft_implementation","title":"Raft implementation","text":"<p>In a distributed system, one data usually has multiple replicas so that the system can still run normally even if a few copies fail. It requires certain technical means to ensure consistency between replicas.</p> <p>Basic principle: Raft is designed to ensure consistency between replicas. Raft uses election between replicas, and the (candidate) replica that wins more than half of the votes will become the Leader, providing external services on behalf of all replicas. The rest Followers will play backups. When the Leader fails (due to communication failure, operation and maintenance commands, etc.), the rest Followers will conduct a new round of elections and vote for a new Leader. The Leader and Followers will detect each other's survival through heartbeats and write them to the hard disk in Raft-wal mode. Replicas that do not respond to more than multiple heartbeats will be considered faulty.</p> <p>Note</p> <p>Raft-wal needs to be written into the hard disk periodically. If hard disk bottlenecks to write, Raft will fail to send a heartbeat and conduct a new round of elections. If the hard disk IO is severely blocked, there will be no Leader for a long time.</p> <p>Read and write: For every writing request of the clients, the Leader will initiate a Raft-wal and synchronize it with the Followers. Only after over half replicas have received the Raft-wal will it return to the clients successfully. For every reading request of the clients, it will get to the Leader directly, while Followers will not be involved.</p> <p>Failure: Scenario 1: Take a (space) cluster of a single replica as an example. If the system has only one replica, the Leader will be itself. If failure happens, the system will be completely unavailable. Scenario 2: Take a (space) cluster of three replicas as an example. If the system has three replicas, one of them will be the Leader and the rest will be the Followers. If the Leader fails, the rest two can still vote for a new Leader (and a Follower), and the system is still available. But if any of the two Followers fails again, the system will be completely unavailable due to inadequate voters.</p> <p>Note</p> <p>Raft and HDFS have different modes of duplication. Raft is based on a quorum vote, so the number of replicas cannot be even.</p>"},{"location":"1.introduction/3.nebula-graph-architecture/4.storage-service/#multi_group_raft","title":"Multi Group Raft","text":"<p>The Storage Service supports a distributed cluster architecture, so NebulaGraph implements Multi Group Raft according to Raft protocol. Each Raft group stores all the replicas of each partition. One replica is the leader, while others are followers. In this way, NebulaGraph achieves strong consistency and high availability. The functions of Raft are as follows.</p> <p>NebulaGraph uses Multi Group Raft to improve performance when there are many partitions because Raft-wal cannot be NULL. When there are too many partitions, costs will increase, such as storing information in Raft group, WAL files, or batch operation in low load.</p> <p>There are two key points to implement the Multi Raft Group:</p> <ul> <li> <p>To share transport layer</p> <p>Each Raft Group sends messages to its corresponding peers. So if the transport layer cannot be shared, the connection costs will be very high.</p> </li> </ul> <ul> <li> <p>To share thread pool</p> <p>Raft Groups share the same thread pool to prevent starting too many threads and a high context switch cost.</p> </li> </ul>"},{"location":"1.introduction/3.nebula-graph-architecture/4.storage-service/#batch","title":"Batch","text":"<p>For each partition, it is necessary to do a batch to improve throughput when writing the WAL serially. As NebulaGraph uses WAL to implement some special functions, batches need to be grouped, which is a feature of NebulaGraph.</p> <p>For example, lock-free CAS operations will execute after all the previous WALs are committed. So for a batch, if there are several WALs in CAS type, we need to divide this batch into several smaller groups and make sure they are committed serially.</p>"},{"location":"1.introduction/3.nebula-graph-architecture/4.storage-service/#transfer_leadership","title":"Transfer Leadership","text":"<p>Transfer leadership is extremely important for balance. When moving a partition from one machine to another, NebulaGraph first checks if the source is a leader. If so, it should be moved to another peer. After data migration is completed, it is important to balance leader distribution again.</p> <p>When a transfer leadership command is committed, the leader will abandon its leadership and the followers will start a leader election.</p>"},{"location":"1.introduction/3.nebula-graph-architecture/4.storage-service/#peer_changes","title":"Peer changes","text":"<p>To avoid split-brain, when members in a Raft Group change, an intermediate state is required. In such a state, the quorum of the old group and new group always have an overlap. Thus it prevents the old or new group from making decisions unilaterally. To make it even simpler, in his doctoral thesis Diego Ongaro suggests adding or removing a peer once to ensure the overlap between the quorum of the new group and the old group. NebulaGraph also uses this approach, except that the way to add or remove a member is different. For details, please refer to addPeer/removePeer in the Raft Part class.</p>"},{"location":"1.introduction/3.nebula-graph-architecture/4.storage-service/#differences_with_hdfs","title":"Differences with HDFS","text":"<p>The Storage Service is a Raft-based distributed architecture, which has certain differences with that of HDFS. For example:</p> <ul> <li>The Storage Service ensures consistency through Raft. Usually, the number of its replicas is odd to elect a leader. However, DataNode used by HDFS ensures consistency through NameNode, which has no limit on the number of replicas.</li> </ul> <ul> <li>In the Storage Service, only the replicas of the leader can read and write, while in HDFS all the replicas can do so.</li> </ul> <ul> <li>In the Storage Service, the number of replicas needs to be determined when creating a space, since it cannot be changed afterward. But in HDFS, the number of replicas can be changed freely.</li> </ul> <ul> <li>The Storage Service can access the file system directly. While the applications of HDFS (such as HBase) have to access HDFS before the file system, which requires more RPC times.</li> </ul> <p>In a word, the Storage Service is more lightweight with some functions simplified and its architecture is simpler than HDFS, which can effectively improve the read and write performance of a smaller block of data.</p>"},{"location":"14.client/1.nebula-client/","title":"Clients overview","text":"<p>NebulaGraph supports multiple types of clients for users to connect to and manage the NebulaGraph database.</p> <ul> <li>NebulaGraph Console: the native CLI client</li> </ul> <ul> <li>NebulaGraph CPP: the NebulaGraph client for C++</li> </ul> <ul> <li>NebulaGraph Java: the NebulaGraph client for Java</li> </ul> <ul> <li>NebulaGraph Python: the NebulaGraph client for Python</li> </ul> <ul> <li>NebulaGraph Go: the NebulaGraph client for Golang</li> </ul> <p>Note</p> <p>For now, only NebulaGraph Java is thread-safe.</p> <p>Caution</p> <p>The following clients can also be used to connect to and manage NebulaGraph, but there is no uptime guarantee.</p> <ul> <li>NebulaGraph PHP </li> <li>NebulaGraph Node</li> <li>NebulaGraph .net</li> <li>NebulaGraph JDBC</li> <li>NebulaGraph Carina\uff08Python ORM\uff09</li> <li>NORM (Golang ORM)</li> <li>Graph-Ocean (Java ORM)</li> <li>NebulaGraph Ngbatis (Java ORM in a MyBatis fashion)</li> </ul>"},{"location":"14.client/3.nebula-cpp-client/","title":"NebulaGraph CPP","text":"<p>NebulaGraph CPP is a C++ client for connecting to and managing the NebulaGraph database.</p>"},{"location":"14.client/3.nebula-cpp-client/#limitations","title":"Limitations","text":"<p>You have installed C++ and GCC 4.8 or later versions.</p>"},{"location":"14.client/3.nebula-cpp-client/#compatibility_with_nebulagraph","title":"Compatibility with NebulaGraph","text":"<p>See github.</p>"},{"location":"14.client/3.nebula-cpp-client/#install_nebulagraph_cpp","title":"Install NebulaGraph CPP","text":"<p>This document describes how to install NebulaGraph CPP with the source code.</p>"},{"location":"14.client/3.nebula-cpp-client/#prerequisites","title":"Prerequisites","text":"<ul> <li>You have prepared the correct resources.</li> <li>You have installed C++ and GCC version is: {10.1.0 | 9.3.0 | 9.2.0 | 9.1.0 | 8.3.0 | 7.5.0 | 7.1.0}. For details, see the gcc_preset_versions parameter.</li> </ul>"},{"location":"14.client/3.nebula-cpp-client/#steps","title":"Steps","text":"<ol> <li> <p>Clone the NebulaGraph CPP source code to the host.</p> <ul> <li> <p>(Recommended) To install a specific version of NebulaGraph CPP, use the Git option <code>--branch</code> to specify the branch. For example, to install v3.4.0, run the following command:</p> <pre><code>$ git clone --branch release-3.4 https://github.com/vesoft-inc/nebula-cpp.git\n</code></pre> </li> </ul> <ul> <li> <p>To install the daily development version, run the following command to download the source code from the <code>master</code> branch:</p> <pre><code>$ git clone https://github.com/vesoft-inc/nebula-cpp.git\n</code></pre> </li> </ul> </li> <li> <p>Change the working directory to <code>nebula-cpp</code>.</p> <pre><code>$ cd nebula-cpp\n</code></pre> </li> <li> <p>Create a directory named <code>build</code> and change the working directory to it.</p> <pre><code>$ mkdir build &amp;&amp; cd build\n</code></pre> </li> <li> <p>Generate the <code>makefile</code> file with CMake.</p> <p>Note</p> <p>The default installation path is <code>/usr/local/nebula</code>. To modify it, add the <code>-DCMAKE_INSTALL_PREFIX=&lt;installation_path&gt;</code> option while running the following command.</p> <pre><code>$ cmake -DCMAKE_BUILD_TYPE=Release ..\n</code></pre> <p>Note</p> <p>If G++ does not support C++ 11, add the option <code>-DDISABLE_CXX11_ABI=ON</code>.</p> </li> <li> <p>Compile NebulaGraph CPP.</p> <p>To speed up the compiling, use the <code>-j</code> option to set a concurrent number <code>N</code>. It should be \\(\\min(\\text{CPU}core number,\\frac{the_memory_size(GB)}{2})\\).</p> <pre><code>$ make -j{N}\n</code></pre> </li> <li> <p>Install NebulaGraph CPP.</p> <pre><code>$ sudo make install\n</code></pre> </li> <li> <p>Update the dynamic link library.</p> <pre><code>$ sudo ldconfig\n</code></pre> </li> </ol>"},{"location":"14.client/3.nebula-cpp-client/#use_nebulagraph_cpp","title":"Use NebulaGraph CPP","text":"<p>Compile the CPP file to an executable file, then you can use it. The following steps take using <code>SessionExample.cpp</code> for example.</p> <ol> <li> <p>Use the example code to create the <code>SessionExample.cpp</code> file.</p> </li> <li> <p>Run the following command to compile the file.</p> <pre><code>$ LIBRARY_PATH=&lt;library_folder_path&gt;:$LIBRARY_PATH g++ -std=c++11 SessionExample.cpp -I&lt;include_folder_path&gt; -lnebula_graph_client -o session_example\n</code></pre> <ul> <li><code>library_folder_path</code>: The storage path of the NebulaGraph dynamic libraries. The default path is <code>/usr/local/nebula/lib64</code>.</li> </ul> <ul> <li><code>include_folder_path</code>: The storage of the NebulaGraph header files. The default path is <code>/usr/local/nebula/include</code>.</li> </ul> </li> </ol> <p>For example:</p> <pre><code>$ LIBRARY_PATH=/usr/local/nebula/lib64:$LIBRARY_PATH g++ -std=c++11 SessionExample.cpp -I/usr/local/nebula/include -lnebula_graph_client -o session_example\n</code></pre>"},{"location":"14.client/3.nebula-cpp-client/#core_of_the_example_code","title":"Core of the example code","text":"<p>Nebula CPP clients provide both Session Pool and Connection Pool methods to connect to NebulaGraph. Using the Connection Pool method requires users to manage session instances by themselves.</p> <ul> <li> <p>Session Pool</p> <p>For more details about all the code, see SessionPoolExample.</p> </li> </ul> <ul> <li> <p>Connection Pool</p> <p>For more details about all the code, see SessionExample.</p> </li> </ul>"},{"location":"14.client/4.nebula-java-client/","title":"NebulaGraph Java","text":"<p>NebulaGraph Java is a Java client for connecting to and managing the NebulaGraph database.</p>"},{"location":"14.client/4.nebula-java-client/#prerequisites","title":"Prerequisites","text":"<p>You have installed Java 8.0 or later versions.</p>"},{"location":"14.client/4.nebula-java-client/#compatibility_with_nebulagraph","title":"Compatibility with NebulaGraph","text":"<p>See github.</p>"},{"location":"14.client/4.nebula-java-client/#download_nebulagraph_java","title":"Download NebulaGraph Java","text":"<ul> <li> <p>(Recommended) To install a specific version of NebulaGraph Java, use the Git option <code>--branch</code> to specify the branch. For example, to install v3.5.0, run the following command:</p> <pre><code>$ git clone --branch release-3.5 https://github.com/vesoft-inc/nebula-java.git\n</code></pre> </li> </ul> <ul> <li> <p>To install the daily development version, run the following command to download the source code from the <code>master</code> branch:</p> <pre><code>$ git clone https://github.com/vesoft-inc/nebula-java.git\n</code></pre> </li> </ul>"},{"location":"14.client/4.nebula-java-client/#use_nebulagraph_java","title":"Use NebulaGraph Java","text":"<p>Note</p> <p>We recommend that each thread uses one session. If multiple threads use the same session, the performance will be reduced.</p> <p>When importing a Maven project with tools such as IDEA, set the following dependency in <code>pom.xml</code>.</p> <p>Note</p> <p><code>3.0.0-SNAPSHOT</code> indicates the daily development version that may have unknown issues. We recommend that you replace <code>3.0.0-SNAPSHOT</code> with a released version number to use a table version.</p> <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;com.vesoft&lt;/groupId&gt;\n  &lt;artifactId&gt;client&lt;/artifactId&gt;\n  &lt;version&gt;3.0.0-SNAPSHOT&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>If you cannot download the dependency for the daily development version, set the following content in <code>pom.xml</code>. Released versions have no such issue.</p> <pre><code>&lt;repositories&gt; \n  &lt;repository&gt; \n    &lt;id&gt;snapshots&lt;/id&gt; \n    &lt;url&gt;https://oss.sonatype.org/content/repositories/snapshots/&lt;/url&gt; \n  &lt;/repository&gt; \n&lt;/repositories&gt;\n</code></pre> <p>If there is no Maven to manage the project, manually download the JAR file to install NebulaGraph Java.</p>"},{"location":"14.client/4.nebula-java-client/#core_of_the_example_code","title":"Core of the example code","text":"<p>The NebulaGraph Java client provides both Connection Pool and Session Pool modes, using Connection Pool requires the user to manage session instances.</p> <ul> <li> <p>Session Pool</p> <p>For all the code, see GraphSessionPoolExample.</p> </li> </ul> <ul> <li> <p>Connection Pool</p> <p>For all the code, see GraphClientExample.</p> </li> </ul>"},{"location":"14.client/5.nebula-python-client/","title":"NebulaGraph Python","text":"<p>NebulaGraph Python is a Python client for connecting to and managing the NebulaGraph database.</p>"},{"location":"14.client/5.nebula-python-client/#prerequisites","title":"Prerequisites","text":"<p>You have installed Python 3.6 or later versions.</p>"},{"location":"14.client/5.nebula-python-client/#compatibility_with_nebulagraph","title":"Compatibility with NebulaGraph","text":"<p>See github.</p>"},{"location":"14.client/5.nebula-python-client/#install_nebulagraph_python","title":"Install NebulaGraph Python","text":""},{"location":"14.client/5.nebula-python-client/#install_nebulagraph_python_with_pip","title":"Install NebulaGraph Python with pip","text":"<pre><code>$ pip install nebula3-python==&lt;version&gt;\n</code></pre>"},{"location":"14.client/5.nebula-python-client/#install_nebulagraph_python_from_the_source_code","title":"Install NebulaGraph Python from the source code","text":"<ol> <li> <p>Clone the NebulaGraph Python source code to the host.</p> <ul> <li> <p>(Recommended) To install a specific version of NebulaGraph Python, use the Git option <code>--branch</code> to specify the branch. For example, to install v3.4.0, run the following command:</p> <pre><code>$ git clone --branch release-3.4 https://github.com/vesoft-inc/nebula-python.git\n</code></pre> </li> </ul> <ul> <li> <p>To install the daily development version, run the following command to download the source code from the <code>master</code> branch:</p> <pre><code>$ git clone https://github.com/vesoft-inc/nebula-python.git\n</code></pre> </li> </ul> </li> <li> <p>Change the working directory to nebula-python.</p> <pre><code>$ cd nebula-python\n</code></pre> </li> <li> <p>Run the following command to install NebulaGraph Python.</p> <pre><code>$ pip install .\n</code></pre> </li> </ol>"},{"location":"14.client/5.nebula-python-client/#core_of_the_example_code","title":"Core of the example code","text":"<p>NebulaGraph Python clients provides Connection Pool and Session Pool methods to connect to NebulaGraph. Using the Connection Pool method requires users to manage sessions by themselves.</p> <ul> <li> <p>Session Pool</p> <p>For details about all the code, see SessinPoolExample.py.</p> <p>For limitations of using the Session Pool method, see Example of using session pool.</p> </li> </ul> <ul> <li> <p>Connection Pool</p> <p>For details about all the code, see Example.</p> </li> </ul>"},{"location":"14.client/6.nebula-go-client/","title":"NebulaGraph Go","text":"<p>NebulaGraph Go is a Golang client for connecting to and managing the NebulaGraph database.</p>"},{"location":"14.client/6.nebula-go-client/#prerequisites","title":"Prerequisites","text":"<p>You have installed Golang 1.13 or later versions.</p>"},{"location":"14.client/6.nebula-go-client/#compatibility_with_nebulagraph","title":"Compatibility with NebulaGraph","text":"<p>See github.</p>"},{"location":"14.client/6.nebula-go-client/#download_nebulagraph_go","title":"Download NebulaGraph Go","text":"<ul> <li> <p>(Recommended) To install a specific version of NebulaGraph Go, use the Git option <code>--branch</code> to specify the branch. For example, to install v3.5.0, run the following command:</p> <pre><code>$ git clone --branch release-3.5 https://github.com/vesoft-inc/nebula-go.git\n</code></pre> </li> </ul> <ul> <li> <p>To install the daily development version, run the following command to download the source code from the <code>master</code> branch:</p> <pre><code>$ git clone https://github.com/vesoft-inc/nebula-go.git\n</code></pre> </li> </ul>"},{"location":"14.client/6.nebula-go-client/#install_or_update","title":"Install or update","text":"<p>Run the following command to install or update NebulaGraph Go:</p> <pre><code>$ go get -u -v github.com/vesoft-inc/nebula-go/v3@v3.5.0\n</code></pre>"},{"location":"14.client/6.nebula-go-client/#core_of_the_example_code","title":"Core of the example code","text":"<p>The NebulaGraph GO client provides both Connection Pool and Session Pool, using Connection Pool requires the user to manage the session instances.</p> <ul> <li> <p>Session Pool  </p> <p>For details about all the code, see session_pool_example.go.</p> <p>For limitations of using Session Pool, see Usage example.</p> </li> </ul> <ul> <li> <p>Connection Pool</p> <p>For all the code, see graph_client_basic_example and graph_client_goroutines_example.</p> </li> </ul>"},{"location":"15.contribution/how-to-contribute/","title":"How to Contribute","text":""},{"location":"15.contribution/how-to-contribute/#before_you_get_started","title":"Before you get started","text":""},{"location":"15.contribution/how-to-contribute/#commit_an_issue_on_the_github_or_forum","title":"Commit an issue on the github or forum","text":"<p>You are welcome to contribute any code or files to the project. But firstly we suggest you raise an issue on the github or the forum to start a discussion with the community. Check through the topic for Github.</p>"},{"location":"15.contribution/how-to-contribute/#sign_the_contributor_license_agreement_cla","title":"Sign the Contributor License Agreement (CLA)","text":"<p>What is CLA?</p> <p>Here is the vesoft inc. Contributor License Agreement.</p> <p>Click the Sign in with GitHub to agree button to sign the CLA.</p> <p>If you have any questions, send an email to <code>info@vesoft.com</code>.</p>"},{"location":"15.contribution/how-to-contribute/#modify_a_single_document","title":"Modify a single document","text":"<p>This manual is written in the Markdown language. Click the <code>pencil</code> icon on the right of the document title to commit the modification.</p> <p>This method applies to modify a single document only.</p>"},{"location":"15.contribution/how-to-contribute/#batch_modify_or_add_files","title":"Batch modify or add files","text":"<p>This method applies to contribute codes, modify multiple documents in batches, or add new documents.</p>"},{"location":"15.contribution/how-to-contribute/#step_1_fork_in_the_githubcom","title":"Step 1: Fork in the github.com","text":"<p>The NebulaGraph project has many repositories. Take the nebul repository for example:</p> <ol> <li> <p>Visit https://github.com/vesoft-inc/nebula.</p> </li> <li> <p>Click the <code>Fork</code> button to establish an online fork.</p> </li> </ol>"},{"location":"15.contribution/how-to-contribute/#step_2_clone_fork_to_local_storage","title":"Step 2: Clone Fork to Local Storage","text":"<ol> <li> <p>Define a local working directory.</p> <pre><code># Define the working directory.\nworking_dir=$HOME/Workspace\n</code></pre> </li> <li> <p>Set <code>user</code> to match the Github profile name.</p> <pre><code>user={the Github profile name}\n</code></pre> </li> <li> <p>Create your clone.</p> <pre><code>mkdir -p $working_dir\ncd $working_dir\ngit clone https://github.com/$user/nebula.git\n# or: git clone git@github.com:$user/nebula.git\n\ncd $working_dir/nebula\ngit remote add upstream https://github.com/vesoft-inc/nebula.git\n# or: git remote add upstream git@github.com:vesoft-inc/nebula.git\n\n# Never push to upstream master since you do not have write access.\ngit remote set-url --push upstream no_push\n\n# Confirm that the remote branch is valid.\n# The correct format is:\n# origin    git@github.com:$(user)/nebula.git (fetch)\n# origin    git@github.com:$(user)/nebula.git (push)\n# upstream  https://github.com/vesoft-inc/nebula (fetch)\n# upstream  no_push (push)\ngit remote -v\n</code></pre> </li> <li> <p>(Optional) Define a pre-commit hook.</p> <p>Please link the NebulaGraph pre-commit hook into the <code>.git</code> directory.</p> <p>This hook checks the commits for formatting, building, doc generation, etc.</p> <pre><code>cd $working_dir/nebula/.git/hooks\nln -s $working_dir/nebula/.linters/cpp/hooks/pre-commit.sh .\n</code></pre> <p>Sometimes, the pre-commit hook cannot be executed. You have to execute it manually.</p> <pre><code>cd $working_dir/nebula/.git/hooks\nchmod +x pre-commit\n</code></pre> </li> </ol>"},{"location":"15.contribution/how-to-contribute/#step_3_branch","title":"Step 3: Branch","text":"<ol> <li> <p>Get your local master up to date.</p> <pre><code>cd $working_dir/nebula\ngit fetch upstream\ngit checkout master\ngit rebase upstream/master\n</code></pre> </li> <li> <p>Checkout a new branch from master.</p> <pre><code>git checkout -b myfeature\n</code></pre> <p>Note</p> <p>Because the PR often consists of several commits, which might be squashed while being merged into upstream. We strongly suggest you to open a separate topic branch to make your changes on. After merged, this topic branch can be just abandoned, thus you could synchronize your master branch with upstream easily with a rebase like above. Otherwise, if you commit your changes directly into master, you need to use a hard reset on the master branch. For example:</p> <pre><code>git fetch upstream\ngit checkout master\ngit reset --hard upstream/master\ngit push --force origin master\n</code></pre> </li> </ol>"},{"location":"15.contribution/how-to-contribute/#step_4_develop","title":"Step 4: Develop","text":"<ul> <li> <p>Code style</p> <p>NebulaGraph adopts <code>cpplint</code> to make sure that the project conforms to Google's coding style guides. The checker will be implemented before the code is committed.</p> </li> </ul> <ul> <li> <p>Unit tests requirements</p> <p>Please add unit tests for the new features or bug fixes.</p> </li> </ul> <ul> <li> <p>Build your code with unit tests enabled</p> <p>For more information, see Install NebulaGraph by compiling the source code.</p> <p>Note</p> <p>Make sure you have enabled the building of unit tests by setting <code>-DENABLE_TESTING=ON</code>.</p> </li> </ul> <ul> <li> <p>Run tests</p> <p>In the root directory of <code>nebula</code>, run the following command:</p> <pre><code>cd nebula/build\nctest -j$(nproc)\n</code></pre> </li> </ul>"},{"location":"15.contribution/how-to-contribute/#step_5_bring_your_branch_update_to_date","title":"Step 5: Bring Your Branch Update to Date","text":"<pre><code># While on your myfeature branch.\ngit fetch upstream\ngit rebase upstream/master\n</code></pre> <p>Users need to bring the head branch up to date after other contributors merge PR to the base branch.</p>"},{"location":"15.contribution/how-to-contribute/#step_6_commit","title":"Step 6: Commit","text":"<p>Commit your changes.</p> <pre><code>git commit -a\n</code></pre> <p>Users can use the command <code>--amend</code> to re-edit the previous code.</p>"},{"location":"15.contribution/how-to-contribute/#step_7_push","title":"Step 7: Push","text":"<p>When ready to review or just to establish an offsite backup, push your branch to your fork on <code>github.com</code>:</p> <pre><code>git push origin myfeature\n</code></pre>"},{"location":"15.contribution/how-to-contribute/#step_8_create_a_pull_request","title":"Step 8: Create a Pull Request","text":"<ol> <li> <p>Visit your fork at <code>https://github.com/$user/nebula</code> (replace <code>$user</code> here).</p> </li> <li> <p>Click the <code>Compare &amp; pull request</code> button next to your <code>myfeature</code> branch.</p> </li> </ol>"},{"location":"15.contribution/how-to-contribute/#step_9_get_a_code_review","title":"Step 9: Get a Code Review","text":"<p>Once your pull request has been created, it will be assigned to at least two reviewers. Those reviewers will do a thorough code review to make sure that the changes meet the repository's contributing guidelines and other quality standards.</p>"},{"location":"15.contribution/how-to-contribute/#add_test_cases","title":"Add test cases","text":"<p>For detailed methods, see How to add test cases.</p>"},{"location":"15.contribution/how-to-contribute/#donation","title":"Donation","text":""},{"location":"15.contribution/how-to-contribute/#step_1_confirm_the_project_donation","title":"Step 1: Confirm the project donation","text":"<p>Contact the official NebulaGraph staff via email, WeChat, Slack, etc. to confirm the donation project. The project will be donated to the NebulaGraph Contrib organization.</p> <p>Email address: info@vesoft.com</p> <p>WeChat: NebulaGraphbot</p> <p>Slack: Join Slack</p>"},{"location":"15.contribution/how-to-contribute/#step_2_get_the_information_of_the_project_recipient","title":"Step 2: Get the information of the project recipient","text":"<p>The NebulaGraph official staff will give the recipient ID of the NebulaGraph Contrib project.</p>"},{"location":"15.contribution/how-to-contribute/#step_3_donate_a_project","title":"Step 3: Donate a project","text":"<p>The user transfers the project to the recipient of this donation, and the recipient transfers the project to the NebulaGraph Contrib organization. After the donation, the user will continue to lead the development of community projects as a Maintainer.</p> <p>For operations of transferring a repository on GitHub, see Transferring a repository owned by your user account.</p>"},{"location":"2.quick-start/1.quick-start-workflow/","title":"Quickly deploy NebulaGraph using Docker","text":"<p>You can quickly get started with NebulaGraph by deploying NebulaGraph with Docker Desktop or Docker Compose. </p> Using Docker DesktopUsing Docker Compose <p>NebulaGraph is available as a Docker Extension that you can easily install and run on your Docker Desktop. You can quickly deploy NebulaGraph using Docker Desktop with just one click.</p> <ol> <li> <p>Install Docker Desktop</p> <ul> <li>Install Docker Desktop on Mac</li> <li>Install Docker Desktop on Windows</li> </ul> <p>Caution</p> <p>To install Docker Desktop, you need to install WSL 2 first. </p> </li> <li> <p>In the left sidebar of Docker Desktop, click Extensions or Add Extensions.</p> </li> <li> <p>On the Extensions Marketplace, search for NebulaGraph and click Install.</p> <p></p> <p>Click Update to update NebulaGraph to the latest version when a new version is available.</p> <p></p> </li> <li> <p>Click Open to navigate to the NebulaGraph extension page.</p> </li> <li> <p>At the top of the page, click Studio in Browser to use NebulaGraph.</p> </li> </ol> <p>For more information about how to use NebulaGraph with Docker Desktop, see the following video:</p> <p></p> <p>Using Docker Compose can quickly deploy NebulaGraph services based on the prepared configuration file. It is only recommended to use this method when testing the functions of NebulaGraph.</p>"},{"location":"2.quick-start/1.quick-start-workflow/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>You have installed the following applications on your host.</p> Application Recommended version Official installation reference Docker Latest Install Docker Engine Docker Compose Latest Install Docker Compose Git Latest Download Git </li> </ul> <ul> <li>If you are deploying NebulaGraph as a non-root user, grant the user with Docker-related privileges. For detailed instructions, see Manage Docker as a non-root user.</li> </ul> <ul> <li>You have started the Docker service on your host.</li> </ul> <ul> <li>If you have already deployed another version of NebulaGraph with Docker Compose on your host, to avoid compatibility issues, you need to delete the <code>nebula-docker-compose/data</code> directory.</li> </ul> <ul> <li>For NebulaGraph Enterprise, you must have the license key loaded in LM.</li> </ul>"},{"location":"2.quick-start/1.quick-start-workflow/#deploy_nebulagraph","title":"Deploy NebulaGraph","text":"<ol> <li> <p>Clone the <code>3.5.0</code> branch of the <code>nebula-docker-compose</code> repository to your host with Git.</p> <p>Danger</p> <p>The <code>master</code> branch contains the untested code for the latest NebulaGraph development release. DO NOT use this release in a production environment.</p> <pre><code>$ git clone -b release-3.5 https://github.com/vesoft-inc/nebula-docker-compose.git\n</code></pre> <p>Note</p> <p>The <code>x.y</code> version of Docker Compose aligns to the <code>x.y</code> version of NebulaGraph. For the NebulaGraph <code>z</code> version, Docker Compose does not publish the corresponding <code>z</code> version, but pulls the <code>z</code> version of the NebulaGraph image.</p> <p>Note</p> <p>For installation of the NebulaGraph enterprise version, contact us.</p> </li> <li> <p>Go to the <code>nebula-docker-compose</code> directory.</p> <pre><code>$ cd nebula-docker-compose/\n</code></pre> </li> <li> <p>Configure License Manager address.</p> <p>Enterpriseonly</p> <p>Skip this step if you are using the community version.</p> <ol> <li> <p>Edit the <code>docker-compose.yml</code> file.</p> <pre><code>$ cd nebula-docker-compose/\n$ vim docker-compose.yml\n</code></pre> </li> <li> <p>Add the <code>license_manager_url</code> field under all <code>services.metad{number}.command</code> and set its value to the access address of LM.</p> <pre><code>...\nservices:\n  metad0:\n    command:\n      - --license_manager_url=&lt;LM_ADDR&gt;:&lt;LM_PORT&gt; // &lt;LM_ADDR&gt; is the address of the LM service, and &lt;LM_PORT&gt; is the port of the LM service, which is 9119 by default.\n  metad1:\n    command:\n      - --license_manager_url=&lt;LM_ADDR&gt;:&lt;LM_PORT&gt;\n  ...\n</code></pre> </li> <li> <p>Save and exit.</p> </li> </ol> </li> <li> <p>Run the following command to start all the NebulaGraph services.</p> <p>Note</p> <ul> <li>Update the NebulaGraph images and NebulaGraph Console images first if they are out of date.</li> <li>The return result after executing the command varies depending on the installation directory.</li> </ul> <pre><code>[nebula-docker-compose]$ docker-compose up -d\nCreating nebuladockercompose_metad0_1 ... done\nCreating nebuladockercompose_metad2_1 ... done\nCreating nebuladockercompose_metad1_1 ... done\nCreating nebuladockercompose_graphd2_1   ... done\nCreating nebuladockercompose_graphd_1    ... done\nCreating nebuladockercompose_graphd1_1   ... done\nCreating nebuladockercompose_storaged0_1 ... done\nCreating nebuladockercompose_storaged2_1 ... done\nCreating nebuladockercompose_storaged1_1 ... done\n</code></pre> <p>Compatibility</p> <p>Starting from NebulaGraph version 3.1.0, nebula-docker-compose automatically starts a NebulaGraph Console docker container and adds the storage host to the cluster (i.e. <code>ADD HOSTS</code> command).</p> <p>Note</p> <p>For more information of the preceding services, see NebulaGraph architecture.</p> </li> </ol>"},{"location":"2.quick-start/1.quick-start-workflow/#connect_to_nebulagraph","title":"Connect to NebulaGraph","text":"<p>There are two ways to connect to NebulaGraph:</p> <ul> <li>Connected with Nebula Console outside the container. Because the external mapping port for the Graph service is also fixed as <code>9669</code> in the container's configuration file, you can connect directly through the default port. For details, see Connect to NebulaGraph.</li> </ul> <ul> <li>Log into the container installed NebulaGraph Console, then connect to the Graph service. This section describes this approach.</li> </ul> <ol> <li> <p>Run the following command to view the name of NebulaGraph Console docker container.</p> <pre><code>$ docker-compose ps\n          Name                         Command             State                 Ports\n--------------------------------------------------------------------------------------------\nnebuladockercompose_console_1     sh -c sleep 3 &amp;&amp;          Up\n                                  nebula-co ...\n......\n</code></pre> </li> <li> <p>Run the following command to enter the NebulaGraph Console docker container.</p> <pre><code>docker exec -it nebuladockercompose_console_1 /bin/sh\n/ #\n</code></pre> </li> <li> <p>Connect to NebulaGraph with NebulaGraph Console.</p> <pre><code>/ # ./usr/local/bin/nebula-console -u &lt;user_name&gt; -p &lt;password&gt; --address=graphd --port=9669\n</code></pre> <p>Note</p> <p>By default, the authentication is off, you can only log in with an existing username (the default is <code>root</code>) and any password. To turn it on, see Enable authentication.</p> </li> <li> <p>Run the following commands to view the cluster state.</p> <pre><code>nebula&gt; SHOW HOSTS;\n+-------------+------+----------+--------------+----------------------+------------------------+---------+\n| Host        | Port | Status   | Leader count | Leader distribution  | Partition distribution | Version |\n+-------------+------+----------+--------------+----------------------+------------------------+---------+\n| \"storaged0\" | 9779 | \"ONLINE\" | 0            | \"No valid partition\" | \"No valid partition\"   | \"3.5.0\" |\n| \"storaged1\" | 9779 | \"ONLINE\" | 0            | \"No valid partition\" | \"No valid partition\"   | \"3.5.0\" |\n| \"storaged2\" | 9779 | \"ONLINE\" | 0            | \"No valid partition\" | \"No valid partition\"   | \"3.5.0\" |\n+-------------+------+----------+--------------+----------------------+------------------------+---------+\n</code></pre> </li> </ol> <p>Run <code>exit</code> twice to switch back to your terminal (shell).</p>"},{"location":"2.quick-start/1.quick-start-workflow/#check_the_nebulagraph_service_status_and_ports","title":"Check the NebulaGraph service status and ports","text":"<p>Run <code>docker-compose ps</code> to list all the services of NebulaGraph and their status and ports.</p> <p>Note</p> <p>NebulaGraph provides services to the clients through port <code>9669</code> by default. To use other ports, modify the <code>docker-compose.yaml</code> file in the <code>nebula-docker-compose</code> directory and restart the NebulaGraph services.</p> <pre><code>$ docker-compose ps\nnebuladockercompose_console_1     sh -c sleep 3 &amp;&amp;                 Up\n                                  nebula-co ...\nnebuladockercompose_graphd1_1     /usr/local/nebula/bin/nebu ...   Up      0.0.0.0:49174-&gt;19669/tcp,:::49174-&gt;19669/tcp, 0.0.0.0:49171-&gt;19670/tcp,:::49171-&gt;19670/tcp, 0.0.0.0:49177-&gt;9669/tcp,:::49177-&gt;9669/tcp\nnebuladockercompose_graphd2_1     /usr/local/nebula/bin/nebu ...   Up      0.0.0.0:49175-&gt;19669/tcp,:::49175-&gt;19669/tcp, 0.0.0.0:49172-&gt;19670/tcp,:::49172-&gt;19670/tcp, 0.0.0.0:49178-&gt;9669/tcp,:::49178-&gt;9669/tcp\nnebuladockercompose_graphd_1      /usr/local/nebula/bin/nebu ...   Up      0.0.0.0:49180-&gt;19669/tcp,:::49180-&gt;19669/tcp, 0.0.0.0:49179-&gt;19670/tcp,:::49179-&gt;19670/tcp, 0.0.0.0:9669-&gt;9669/tcp,:::9669-&gt;9669/tcp\nnebuladockercompose_metad0_1      /usr/local/nebula/bin/nebu ...   Up      0.0.0.0:49157-&gt;19559/tcp,:::49157-&gt;19559/tcp, 0.0.0.0:49154-&gt;19560/tcp,:::49154-&gt;19560/tcp, 0.0.0.0:49160-&gt;9559/tcp,:::49160-&gt;9559/tcp, 9560/tcp\nnebuladockercompose_metad1_1      /usr/local/nebula/bin/nebu ...   Up      0.0.0.0:49156-&gt;19559/tcp,:::49156-&gt;19559/tcp, 0.0.0.0:49153-&gt;19560/tcp,:::49153-&gt;19560/tcp, 0.0.0.0:49159-&gt;9559/tcp,:::49159-&gt;9559/tcp, 9560/tcp\nnebuladockercompose_metad2_1      /usr/local/nebula/bin/nebu ...   Up      0.0.0.0:49158-&gt;19559/tcp,:::49158-&gt;19559/tcp, 0.0.0.0:49155-&gt;19560/tcp,:::49155-&gt;19560/tcp, 0.0.0.0:49161-&gt;9559/tcp,:::49161-&gt;9559/tcp, 9560/tcp\nnebuladockercompose_storaged0_1   /usr/local/nebula/bin/nebu ...   Up      0.0.0.0:49166-&gt;19779/tcp,:::49166-&gt;19779/tcp, 0.0.0.0:49163-&gt;19780/tcp,:::49163-&gt;19780/tcp, 9777/tcp, 9778/tcp, 0.0.0.0:49169-&gt;9779/tcp,:::49169-&gt;9779/tcp, 9780/tcp\nnebuladockercompose_storaged1_1   /usr/local/nebula/bin/nebu ...   Up      0.0.0.0:49165-&gt;19779/tcp,:::49165-&gt;19779/tcp, 0.0.0.0:49162-&gt;19780/tcp,:::49162-&gt;19780/tcp, 9777/tcp, 9778/tcp, 0.0.0.0:49168-&gt;9779/tcp,:::49168-&gt;9779/tcp, 9780/tcp\nnebuladockercompose_storaged2_1   /usr/local/nebula/bin/nebu ...   Up      0.0.0.0:49167-&gt;19779/tcp,:::49167-&gt;19779/tcp, 0.0.0.0:49164-&gt;19780/tcp,:::49164-&gt;19780/tcp, 9777/tcp, 9778/tcp, 0.0.0.0:49170-&gt;9779/tcp,:::49170-&gt;9779/tcp, 9780/tcp\n</code></pre> <p>If the service is abnormal, you can first confirm the abnormal container name (such as <code>nebuladockercompose_graphd2_1</code>).</p> <p>Then you can execute <code>docker ps</code> to view the corresponding <code>CONTAINER ID</code> (such as <code>2a6c56c405f5</code>).</p> <pre><code>[nebula-docker-compose]$ docker ps\nCONTAINER ID   IMAGE                               COMMAND                  CREATED          STATUS                    PORTS                                                                                                  NAMES\n2a6c56c405f5   vesoft/nebula-graphd:nightly     \"/usr/local/nebula/b\u2026\"   36 minutes ago   Up 36 minutes (healthy)   0.0.0.0:49230-&gt;9669/tcp, 0.0.0.0:49229-&gt;19669/tcp, 0.0.0.0:49228-&gt;19670/tcp                            nebuladockercompose_graphd2_1\n7042e0a8e83d   vesoft/nebula-storaged:nightly   \"./bin/nebula-storag\u2026\"   36 minutes ago   Up 36 minutes (healthy)   9777-9778/tcp, 9780/tcp, 0.0.0.0:49227-&gt;9779/tcp, 0.0.0.0:49226-&gt;19779/tcp, 0.0.0.0:49225-&gt;19780/tcp   nebuladockercompose_storaged2_1\n18e3ea63ad65   vesoft/nebula-storaged:nightly   \"./bin/nebula-storag\u2026\"   36 minutes ago   Up 36 minutes (healthy)   9777-9778/tcp, 9780/tcp, 0.0.0.0:49219-&gt;9779/tcp, 0.0.0.0:49218-&gt;19779/tcp, 0.0.0.0:49217-&gt;19780/tcp   nebuladockercompose_storaged0_1\n4dcabfe8677a   vesoft/nebula-graphd:nightly     \"/usr/local/nebula/b\u2026\"   36 minutes ago   Up 36 minutes (healthy)   0.0.0.0:49224-&gt;9669/tcp, 0.0.0.0:49223-&gt;19669/tcp, 0.0.0.0:49222-&gt;19670/tcp                            nebuladockercompose_graphd1_1\na74054c6ae25   vesoft/nebula-graphd:nightly     \"/usr/local/nebula/b\u2026\"   36 minutes ago   Up 36 minutes (healthy)   0.0.0.0:9669-&gt;9669/tcp, 0.0.0.0:49221-&gt;19669/tcp, 0.0.0.0:49220-&gt;19670/tcp                             nebuladockercompose_graphd_1\n880025a3858c   vesoft/nebula-storaged:nightly   \"./bin/nebula-storag\u2026\"   36 minutes ago   Up 36 minutes (healthy)   9777-9778/tcp, 9780/tcp, 0.0.0.0:49216-&gt;9779/tcp, 0.0.0.0:49215-&gt;19779/tcp, 0.0.0.0:49214-&gt;19780/tcp   nebuladockercompose_storaged1_1\n45736a32a23a   vesoft/nebula-metad:nightly      \"./bin/nebula-metad \u2026\"   36 minutes ago   Up 36 minutes (healthy)   9560/tcp, 0.0.0.0:49213-&gt;9559/tcp, 0.0.0.0:49212-&gt;19559/tcp, 0.0.0.0:49211-&gt;19560/tcp                  nebuladockercompose_metad0_1\n3b2c90eb073e   vesoft/nebula-metad:nightly      \"./bin/nebula-metad \u2026\"   36 minutes ago   Up 36 minutes (healthy)   9560/tcp, 0.0.0.0:49207-&gt;9559/tcp, 0.0.0.0:49206-&gt;19559/tcp, 0.0.0.0:49205-&gt;19560/tcp                  nebuladockercompose_metad2_1\n7bb31b7a5b3f   vesoft/nebula-metad:nightly      \"./bin/nebula-metad \u2026\"   36 minutes ago   Up 36 minutes (healthy)   9560/tcp, 0.0.0.0:49210-&gt;9559/tcp, 0.0.0.0:49209-&gt;19559/tcp, 0.0.0.0:49208-&gt;19560/tcp                  nebuladockercompose_metad1_1\n</code></pre> <p>Use the <code>CONTAINER ID</code> to log in the container and troubleshoot.</p> <pre><code>nebula-docker-compose]$ docker exec -it 2a6c56c405f5 bash\n[root@2a6c56c405f5 nebula]#\n</code></pre>"},{"location":"2.quick-start/1.quick-start-workflow/#check_the_service_data_and_logs","title":"Check the service data and logs","text":"<p>All the data and logs of NebulaGraph are stored persistently in the <code>nebula-docker-compose/data</code> and <code>nebula-docker-compose/logs</code> directories.</p> <p>The structure of the directories is as follows:</p> <pre><code>nebula-docker-compose/\n  |-- docker-compose.yaml\n  \u251c\u2500\u2500 data\n  \u2502\u00a0\u00a0 \u251c\u2500\u2500 meta0\n  \u2502\u00a0\u00a0 \u251c\u2500\u2500 meta1\n  \u2502\u00a0\u00a0 \u251c\u2500\u2500 meta2\n  \u2502\u00a0\u00a0 \u251c\u2500\u2500 storage0\n  \u2502\u00a0\u00a0 \u251c\u2500\u2500 storage1\n  \u2502\u00a0\u00a0 \u2514\u2500\u2500 storage2\n  \u2514\u2500\u2500 logs\n      \u251c\u2500\u2500 graph\n      \u251c\u2500\u2500 graph1\n      \u251c\u2500\u2500 graph2\n      \u251c\u2500\u2500 meta0\n      \u251c\u2500\u2500 meta1\n      \u251c\u2500\u2500 meta2\n      \u251c\u2500\u2500 storage0\n      \u251c\u2500\u2500 storage1\n      \u2514\u2500\u2500 storage2\n</code></pre>"},{"location":"2.quick-start/1.quick-start-workflow/#stop_the_nebulagraph_services","title":"Stop the NebulaGraph services","text":"<p>You can run the following command to stop the NebulaGraph services:</p> <pre><code>$ docker-compose down\n</code></pre> <p>The following information indicates you have successfully stopped the NebulaGraph services:</p> <pre><code>Stopping nebuladockercompose_console_1   ... done\nStopping nebuladockercompose_graphd1_1   ... done\nStopping nebuladockercompose_graphd_1    ... done\nStopping nebuladockercompose_graphd2_1   ... done\nStopping nebuladockercompose_storaged1_1 ... done\nStopping nebuladockercompose_storaged0_1 ... done\nStopping nebuladockercompose_storaged2_1 ... done\nStopping nebuladockercompose_metad2_1    ... done\nStopping nebuladockercompose_metad0_1    ... done\nStopping nebuladockercompose_metad1_1    ... done\nRemoving nebuladockercompose_console_1   ... done\nRemoving nebuladockercompose_graphd1_1   ... done\nRemoving nebuladockercompose_graphd_1    ... done\nRemoving nebuladockercompose_graphd2_1   ... done\nRemoving nebuladockercompose_storaged1_1 ... done\nRemoving nebuladockercompose_storaged0_1 ... done\nRemoving nebuladockercompose_storaged2_1 ... done\nRemoving nebuladockercompose_metad2_1    ... done\nRemoving nebuladockercompose_metad0_1    ... done\nRemoving nebuladockercompose_metad1_1    ... done\nRemoving network nebuladockercompose_nebula-net\n</code></pre> <p>Danger</p> <p>The parameter <code>-v</code> in the command <code>docker-compose down -v</code> will delete all your local NebulaGraph storage data. Try this command if you are using the nightly release and having some compatibility issues.</p>"},{"location":"2.quick-start/1.quick-start-workflow/#modify_configurations","title":"Modify configurations","text":"<p>The configuration file of NebulaGraph deployed by Docker Compose is <code>nebula-docker-compose/docker-compose.yaml</code>. To make the new configuration take effect, modify the configuration in this file and restart the service.</p> <p>For more instructions, see Configurations.</p>"},{"location":"2.quick-start/1.quick-start-workflow/#faq","title":"FAQ","text":""},{"location":"2.quick-start/1.quick-start-workflow/#how_to_fix_the_docker_mapping_to_external_ports","title":"How to fix the docker mapping to external ports?","text":"<p>To set the <code>ports</code> of corresponding services as fixed mapping, modify the <code>docker-compose.yaml</code> in the <code>nebula-docker-compose</code> directory. For example:</p> <pre><code>graphd:\n    image: vesoft/nebula-graphd:release-3.5\n    ...\n    ports:\n      - 9669:9669\n      - 19669\n      - 19670\n</code></pre> <p><code>9669:9669</code> indicates the internal port 9669 is uniformly mapped to external ports, while <code>19669</code> indicates the internal port 19669 is randomly mapped to external ports.</p>"},{"location":"2.quick-start/1.quick-start-workflow/#how_to_upgrade_or_update_the_docker_images_of_nebulagraph_services","title":"How to upgrade or update the docker images of NebulaGraph services","text":"<ol> <li> <p>In the <code>nebula-docker-compose/docker-compose.yaml</code> file, change all the <code>image</code> values to the required image version.</p> </li> <li> <p>In the <code>nebula-docker-compose</code> directory, run <code>docker-compose pull</code> to update the images of the Graph Service, Storage Service, Meta Service, and NebulaGraph Console.</p> </li> <li> <p>Run <code>docker-compose up -d</code> to start the NebulaGraph services again.</p> </li> <li> <p>After connecting to NebulaGraph with NebulaGraph Console, run <code>SHOW HOSTS GRAPH</code>, <code>SHOW HOSTS STORAGE</code>, or <code>SHOW HOSTS META</code> to check the version of the responding service respectively.</p> </li> </ol>"},{"location":"2.quick-start/1.quick-start-workflow/#error_toomanyrequests_when_docker-compose_pull","title":"<code>ERROR: toomanyrequests</code> when <code>docker-compose pull</code>","text":"<p>You may meet the following error.</p> <p><code>ERROR: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit</code>.</p> <p>You have met the rate limit of Docker Hub. Learn more on Understanding Docker Hub Rate Limiting.</p>"},{"location":"2.quick-start/1.quick-start-workflow/#how_to_update_the_nebulagraph_console_client","title":"How to update the NebulaGraph Console client","text":"<p>The command <code>docker-compose pull</code> updates both the NebulaGraph services and the NebulaGraph Console.</p>"},{"location":"2.quick-start/2.install-nebula-graph/","title":"Step 1: Install NebulaGraph","text":"<p>RPM and DEB are common package formats on Linux systems. This topic shows how to quickly install NebulaGraph with the RPM or DEB package.</p> <p>Note</p> <p>The console is not complied or packaged with NebulaGraph server binaries. You can install nebula-console by yourself.</p> <p>Enterpriseonly</p> <p>For NebulaGraph Enterprise, please contact us.</p>"},{"location":"2.quick-start/2.install-nebula-graph/#prerequisites","title":"Prerequisites","text":"<ul> <li>The tool <code>wget</code> is installed.</li> </ul> <ul> <li>For NebulaGraph Enterprise, you must have the license key loaded in LM.</li> </ul>"},{"location":"2.quick-start/2.install-nebula-graph/#step_1_download_the_package_from_cloud_service","title":"Step 1: Download the package from cloud service","text":"<p>Note</p> <p>NebulaGraph is currently only supported for installation on Linux systems, and only CentOS 7.x, CentOS 8.x, Ubuntu 16.04, Ubuntu 18.04, and Ubuntu 20.04 operating systems are supported. </p> <ul> <li>Download the released version.<p>URL:</p> <pre><code>//Centos 7\nhttps://oss-cdn.nebula-graph.io/package/&lt;release_version&gt;/nebula-graph-&lt;release_version&gt;.el7.x86_64.rpm\n\n//Centos 8\nhttps://oss-cdn.nebula-graph.io/package/&lt;release_version&gt;/nebula-graph-&lt;release_version&gt;.el8.x86_64.rpm\n\n//Ubuntu 1604\nhttps://oss-cdn.nebula-graph.io/package/&lt;release_version&gt;/nebula-graph-&lt;release_version&gt;.ubuntu1604.amd64.deb\n\n//Ubuntu 1804\nhttps://oss-cdn.nebula-graph.io/package/&lt;release_version&gt;/nebula-graph-&lt;release_version&gt;.ubuntu1804.amd64.deb\n\n//Ubuntu 2004\nhttps://oss-cdn.nebula-graph.io/package/&lt;release_version&gt;/nebula-graph-&lt;release_version&gt;.ubuntu2004.amd64.deb\n</code></pre> <p>For example, download the release package <code>3.5.0</code> for <code>Centos 7.5</code>:</p> <pre><code>wget https://oss-cdn.nebula-graph.io/package/3.5.0/nebula-graph-3.5.0.el7.x86_64.rpm\nwget https://oss-cdn.nebula-graph.io/package/3.5.0/nebula-graph-3.5.0.el7.x86_64.rpm.sha256sum.txt\n</code></pre> <p>Download the release package <code>3.5.0</code> for <code>Ubuntu 1804</code>:</p> <pre><code>wget https://oss-cdn.nebula-graph.io/package/3.5.0/nebula-graph-3.5.0.ubuntu1804.amd64.deb\nwget https://oss-cdn.nebula-graph.io/package/3.5.0/nebula-graph-3.5.0.ubuntu1804.amd64.deb.sha256sum.txt\n</code></pre> </li> </ul> <ul> <li> <p>Download the nightly version.</p> <p>Danger</p> <ul> <li>Nightly versions are usually used to test new features. Do not use it in a production environment.</li> <li>Nightly versions may not be built successfully every night. And the names may change from day to day.</li> </ul> <p>URL:</p> <pre><code>//Centos 7\nhttps://oss-cdn.nebula-graph.io/package/nightly/&lt;yyyy.mm.dd&gt;/nebula-graph-&lt;yyyy.mm.dd&gt;-nightly.el7.x86_64.rpm\n\n//Centos 8\nhttps://oss-cdn.nebula-graph.io/package/nightly/&lt;yyyy.mm.dd&gt;/nebula-graph-&lt;yyyy.mm.dd&gt;-nightly.el8.x86_64.rpm\n\n//Ubuntu 1604\nhttps://oss-cdn.nebula-graph.io/package/nightly/&lt;yyyy.mm.dd&gt;/nebula-graph-&lt;yyyy.mm.dd&gt;-nightly.ubuntu1604.amd64.deb\n\n//Ubuntu 1804\nhttps://oss-cdn.nebula-graph.io/package/nightly/&lt;yyyy.mm.dd&gt;/nebula-graph-&lt;yyyy.mm.dd&gt;-nightly.ubuntu1804.amd64.deb\n\n//Ubuntu 2004\nhttps://oss-cdn.nebula-graph.io/package/nightly/&lt;yyyy.mm.dd&gt;/nebula-graph-&lt;yyyy.mm.dd&gt;-nightly.ubuntu2004.amd64.deb\n</code></pre> <p>For example, download the <code>Centos 7.5</code> package developed and built in <code>2021.11.28</code>:</p> <pre><code>wget https://oss-cdn.nebula-graph.io/package/nightly/2021.11.28/nebula-graph-2021.11.28-nightly.el7.x86_64.rpm\nwget https://oss-cdn.nebula-graph.io/package/nightly/2021.11.28/nebula-graph-2021.11.28-nightly.el7.x86_64.rpm.sha256sum.txt\n</code></pre> <p>For example, download the <code>Ubuntu 1804</code> package developed and built in <code>2021.11.28</code>:</p> <pre><code>wget https://oss-cdn.nebula-graph.io/package/nightly/2021.11.28/nebula-graph-2021.11.28-nightly.ubuntu1804.amd64.deb\nwget https://oss-cdn.nebula-graph.io/package/nightly/2021.11.28/nebula-graph-2021.11.28-nightly.ubuntu1804.amd64.deb.sha256sum.txt\n</code></pre> </li> </ul>"},{"location":"2.quick-start/2.install-nebula-graph/#step_2_install_nebulagraph","title":"Step 2: Install NebulaGraph","text":"<ul> <li> <p>Use the following syntax to install with an RPM package.</p> <pre><code>$ sudo rpm -ivh --prefix=&lt;installation_path&gt; &lt;package_name&gt;\n</code></pre> <p>The option <code>--prefix</code> indicates the installation path. The default path is <code>/usr/local/nebula/</code>.</p> <p>For example, to install an RPM package in the default path for the 3.5.0 version, run the following command.</p> <pre><code>sudo rpm -ivh nebula-graph-3.5.0.el7.x86_64.rpm\n</code></pre> </li> </ul> <ul> <li> <p>Use the following syntax to install with a DEB package.</p> <pre><code>$ sudo dpkg -i &lt;package_name&gt;\n</code></pre> <p>Note</p> <p>Customizing the installation path is not supported when installing NebulaGraph with a DEB package. The default installation path is <code>/usr/local/nebula/</code>.</p> <p>For example, to install a DEB package for the 3.5.0 version, run the following command.</p> <pre><code>sudo dpkg -i nebula-graph-3.5.0.ubuntu1804.amd64.deb\n</code></pre> <p>Note</p> <p>The default installation path is <code>/usr/local/nebula/</code>.</p> </li> </ul>"},{"location":"2.quick-start/2.install-nebula-graph/#step_3_configure_the_address_of_the_license_manager","title":"Step 3: Configure the address of the License Manager","text":"<p>Enterpriseonly</p> <p>This step is required only for NebulaGraph Enterprise.</p> <p>In the Meta service configuration file (<code>nebula-metad.conf</code>) of NebulaGraph, set the value of <code>license_manager_url</code> to the host IP and port number <code>9119</code> where the License Manager (LM) is located, e.g. <code>192.168.8.100:9119</code>.</p>"},{"location":"2.quick-start/2.install-nebula-graph/#next_to_do","title":"Next to do","text":"<ul> <li>Start NebulaGraph </li> </ul> <ul> <li>Connect to NebulaGraph</li> </ul>"},{"location":"2.quick-start/3.1add-storage-hosts/","title":"Register the Storage Service","text":"<p>When connecting to NebulaGraph for the first time, you have to add the Storage hosts, and confirm that all the hosts are online.</p> <p>Compatibility</p> <ul> <li>Starting from NebulaGraph 3.0.0, you have to run <code>ADD HOSTS</code> before reading or writing data into the Storage Service.</li> <li>For NebulaGraph of versions earlier than 3.0.0 and NebulaGraph Cloud clusters, <code>ADD HOSTS</code> is not needed. </li> </ul>"},{"location":"2.quick-start/3.1add-storage-hosts/#prerequisites","title":"Prerequisites","text":"<p>You have connected to NebulaGraph.</p>"},{"location":"2.quick-start/3.1add-storage-hosts/#steps","title":"Steps","text":"<ol> <li> <p>Add the Storage hosts.</p> <p>Run the following command to add hosts:</p> <pre><code>ADD HOSTS &lt;ip&gt;:&lt;port&gt; [,&lt;ip&gt;:&lt;port&gt; ...];\n</code></pre> <p>Example\uff1a</p> <pre><code>nebula&gt; ADD HOSTS 192.168.10.100:9779, 192.168.10.101:9779, 192.168.10.102:9779;\n</code></pre> <p>Caution</p> <p>Make sure that the IP you added is the same as the IP configured for <code>local_ip</code> in the <code>nebula-storaged.conf</code> file. Otherwise, the Storage service will fail to start. For information about configurations, see Configurations.</p> </li> <li> <p>Check the status of the hosts to make sure that they are all online.</p> <pre><code>nebula&gt; SHOW HOSTS;\n+------------------+------+----------+--------------+----------------------  +------------------------+---------+\n| Host             | Port | Status   | Leader count | Leader distribution  |   Partition distribution | Version |\n+------------------+------+----------+--------------+----------------------  +------------------------+---------+\n| \"192.168.10.100\" | 9779 | \"ONLINE\" | 0            | \"No valid partition\" | \"No   valid partition\"   | \"3.5.0\" |\n| \"192.168.10.101\" | 9779 | \"ONLINE\" | 0            | \"No valid partition\" | \"No   valid partition\"   | \"3.5.0\"|\n| \"192.168.10.102\" | 9779 | \"ONLINE\" | 0            | \"No valid partition\" | \"No   valid partition\"   | \"3.5.0\"|\n+------------------+------+----------+--------------+----------------------  +------------------------+---------+\n</code></pre> <p>The <code>Status</code> column of the result above shows that all Storage hosts are online.</p> </li> </ol>"},{"location":"2.quick-start/3.connect-to-nebula-graph/","title":"Step 3: Connect to NebulaGraph","text":"<p>This topic provides basic instruction on how to use the native CLI client NebulaGraph Console to connect to NebulaGraph.</p> <p>Caution</p> <p>When connecting to NebulaGraph for the first time, you must register the Storage Service before querying data.</p> <p>NebulaGraph supports multiple types of clients, including a CLI client, a GUI client, and clients developed in popular programming languages. For more information, see the client list.</p>"},{"location":"2.quick-start/3.connect-to-nebula-graph/#prerequisites","title":"Prerequisites","text":"<ul> <li>You have started NebulaGraph services.</li> </ul> <ul> <li>The machine on which you plan to run NebulaGraph Console has network access to the Graph Service of NebulaGraph.</li> </ul> <ul> <li> <p>The NebulaGraph Console version is compatible with the NebulaGraph version.</p> <p>Note</p> <p>NebulaGraph Console and NebulaGraph of the same version number are the most compatible. There may be compatibility issues when connecting to NebulaGraph with a different version of NebulaGraph Console. The error message <code>incompatible version between client and server</code> is displayed when there is such an issue.</p> </li> </ul>"},{"location":"2.quick-start/3.connect-to-nebula-graph/#steps","title":"Steps","text":"<ol> <li> <p>On the NebulaGraph Console releases page, select a NebulaGraph Console version and click Assets.</p> <p>Note</p> <p>It is recommended to select the latest version.</p> </li> <li> <p>In the Assets area, find the correct binary file for the machine where you want to run NebulaGraph Console and download the file to the machine.</p> </li> <li> <p>(Optional) Rename the binary file to <code>nebula-console</code> for convenience.</p> <p>Note</p> <p>For Windows, rename the file to <code>nebula-console.exe</code>.</p> </li> <li> <p>On the machine to run NebulaGraph Console, grant the execute permission of the nebula-console binary file to the user.</p> <p>Note</p> <p>For Windows, skip this step.</p> <pre><code>$ chmod 111 nebula-console\n</code></pre> </li> <li> <p>In the command line interface, change the working directory to the one where the nebula-console binary file is stored.</p> </li> <li> <p>Run the following command to connect to NebulaGraph.</p> <ul> <li>For Linux or macOS:</li> </ul> <pre><code>$ ./nebula-console -addr &lt;ip&gt; -port &lt;port&gt; -u &lt;username&gt; -p &lt;password&gt;\n[-t 120] [-e \"nGQL_statement\" | -f filename.nGQL]\n</code></pre> <ul> <li>For Windows:</li> </ul> <pre><code>&gt; nebula-console.exe -addr &lt;ip&gt; -port &lt;port&gt; -u &lt;username&gt; -p &lt;password&gt;\n[-t 120] [-e \"nGQL_statement\" | -f filename.nGQL]\n</code></pre> <p>Parameter descriptions are as follows:</p> Parameter Description <code>-h/-help</code> Shows the help menu. <code>-addr/-address</code> Sets the IP address of the Graph service. The default address is 127.0.0.1.  <code>-P/-port</code> Sets the port number of the graphd service. The default port number is 9669. <code>-u/-user</code> Sets the username of your NebulaGraph account. Before enabling authentication, you can use any existing username. The default username is <code>root</code>. <code>-p/-password</code> Sets the password of your NebulaGraph account. Before enabling authentication, you can use any characters as the password. <code>-t/-timeout</code> Sets an integer-type timeout threshold of the connection. The unit is millisecond. The default value is 120. <code>-e/-eval</code> Sets a string-type nGQL statement. The nGQL statement is executed once the connection succeeds. The connection stops after the result is returned. <code>-f/-file</code> Sets the path of an nGQL file. The nGQL statements in the file are executed once the connection succeeds. The result will be returned and the connection stops then. <code>-enable_ssl</code> Enables SSL encryption when connecting to NebulaGraph. <code>-ssl_root_ca_path</code> Sets the storage path of the certification authority file. <code>-ssl_cert_path</code> Sets the storage path of the certificate file. <code>-ssl_private_key_path</code> Sets the storage path of the private key file. <p>For information on more parameters, see the project repository.</p> </li> </ol>"},{"location":"2.quick-start/4.nebula-graph-crud/","title":"Step 4: Use nGQL (CRUD)","text":"<p>This topic will describe the basic CRUD operations in NebulaGraph.</p> <p>For more information, see nGQL guide.</p>"},{"location":"2.quick-start/4.nebula-graph-crud/#graph_space_and_nebulagraph_schema","title":"Graph space and NebulaGraph schema","text":"<p>A NebulaGraph instance consists of one or more graph spaces. Graph spaces are physically isolated from each other. You can use different graph spaces in the same instance to store different datasets.</p> <p></p> <p>To insert data into a graph space, define a schema for the graph database. NebulaGraph schema is based on the following components.</p> Schema component Description Vertex Represents an entity in the real world. A vertex can have zero to multiple tags. Tag The type of the same group of vertices. It defines a set of properties that describes the types of vertices. Edge Represents a directed relationship between two vertices. Edge type The type of an edge. It defines a group of properties that describes the types of edges. <p>For more information, see Data modeling.</p> <p>In this topic, we will use the following dataset to demonstrate basic CRUD operations.</p> <p></p>"},{"location":"2.quick-start/4.nebula-graph-crud/#async_implementation_of_create_and_alter","title":"Async implementation of <code>CREATE</code> and <code>ALTER</code>","text":"<p>Caution</p> <p>In NebulaGraph, the following <code>CREATE</code> or <code>ALTER</code> commands are implemented in an async way and take effect in the next heartbeat cycle. Otherwise, an error will be returned. To make sure the follow-up operations work as expected, Wait for two heartbeat cycles, i.e., 20 seconds.</p> <ul> <li><code>CREATE SPACE</code></li> <li><code>CREATE TAG</code></li> <li><code>CREATE EDGE</code></li> <li><code>ALTER TAG</code></li> <li><code>ALTER EDGE</code></li> <li><code>CREATE TAG INDEX</code></li> <li><code>CREATE EDGE INDEX</code></li> </ul> <p>Note</p> <p>The default heartbeat interval is 10 seconds. To change the heartbeat interval, modify the <code>heartbeat_interval_secs</code> parameter in the configuration files for all services.</p>"},{"location":"2.quick-start/4.nebula-graph-crud/#create_and_use_a_graph_space","title":"Create and use a graph space","text":""},{"location":"2.quick-start/4.nebula-graph-crud/#ngql_syntax","title":"nGQL syntax","text":"<ul> <li>Create a graph space:<pre><code>CREATE SPACE [IF NOT EXISTS] &lt;graph_space_name&gt; (\n[partition_num = &lt;partition_number&gt;,]\n[replica_factor = &lt;replica_number&gt;,]\nvid_type = {FIXED_STRING(&lt;N&gt;) | INT64}\n)\n[COMMENT = '&lt;comment&gt;'];\n</code></pre> <p>For more information on parameters, see CREATE SPACE.</p> </li> </ul> <ul> <li>List graph spaces and check if the creation is successful:<pre><code>nebula&gt; SHOW SPACES;\n</code></pre> </li> </ul> <ul> <li>Use a graph space:<pre><code>USE &lt;graph_space_name&gt;;\n</code></pre> </li> </ul>"},{"location":"2.quick-start/4.nebula-graph-crud/#examples","title":"Examples","text":"<ol> <li> <p>Use the following statement to create a graph space named <code>basketballplayer</code>.</p> <pre><code>nebula&gt; CREATE SPACE basketballplayer(partition_num=15, replica_factor=1, vid_type=fixed_string(30));\n</code></pre> <p>Note</p> <p>If the system returns the error <code>[ERROR (-1005)]: Host not enough!</code>, check whether registered the Storage Service.</p> </li> <li> <p>Check the partition distribution with <code>SHOW HOSTS</code> to make sure that the partitions are distributed in a balanced way.</p> <pre><code>nebula&gt; SHOW HOSTS;\n+-------------+-----------+-----------+--------------+----------------------------------+------------------------+---------+\n| Host        | Port      | Status    | Leader count | Leader distribution              | Partition distribution | Version |\n+-------------+-----------+-----------+--------------+----------------------------------+------------------------+---------+\n| \"storaged0\" | 9779      | \"ONLINE\"  | 5            | \"basketballplayer:5\"             | \"basketballplayer:5\"   | \"3.5.0\"|\n| \"storaged1\" | 9779      | \"ONLINE\"  | 5            | \"basketballplayer:5\"             | \"basketballplayer:5\"   | \"3.5.0\"|\n| \"storaged2\" | 9779      | \"ONLINE\"  | 5            | \"basketballplayer:5\"             | \"basketballplayer:5\"   | \"3.5.0\"|\n+-------------+-----------+-----------+-----------+--------------+----------------------------------+------------------------+---------+\n</code></pre> <p>If the Leader distribution is uneven, use <code>BALANCE LEADER</code> to redistribute the partitions. For more information, see BALANCE.</p> </li> <li> <p>Use the <code>basketballplayer</code> graph space.</p> <pre><code>nebula[(none)]&gt; USE basketballplayer;\n</code></pre> <p>You can use <code>SHOW SPACES</code> to check the graph space you created.</p> <pre><code>nebula&gt; SHOW SPACES;\n+--------------------+\n| Name               |\n+--------------------+\n| \"basketballplayer\" |\n+--------------------+\n</code></pre> </li> </ol>"},{"location":"2.quick-start/4.nebula-graph-crud/#create_tags_and_edge_types","title":"Create tags and edge types","text":""},{"location":"2.quick-start/4.nebula-graph-crud/#ngql_syntax_1","title":"nGQL syntax","text":"<pre><code>CREATE {TAG | EDGE} [IF NOT EXISTS] {&lt;tag_name&gt; | &lt;edge_type_name&gt;}\n    (\n      &lt;prop_name&gt; &lt;data_type&gt; [NULL | NOT NULL] [DEFAULT &lt;default_value&gt;] [COMMENT '&lt;comment&gt;']\n      [{, &lt;prop_name&gt; &lt;data_type&gt; [NULL | NOT NULL] [DEFAULT &lt;default_value&gt;] [COMMENT '&lt;comment&gt;']} ...] \n    )\n    [TTL_DURATION = &lt;ttl_duration&gt;]\n    [TTL_COL = &lt;prop_name&gt;]\n    [COMMENT = '&lt;comment&gt;'];\n</code></pre> <p>For more information on parameters, see CREATE TAG and CREATE EDGE.</p>"},{"location":"2.quick-start/4.nebula-graph-crud/#examples_1","title":"Examples","text":"<p>Create tags <code>player</code> and <code>team</code>, and edge types <code>follow</code> and <code>serve</code>. Descriptions are as follows.</p> Component name Type Property player Tag name (string), age (int) team Tag name (string) follow Edge type degree (int) serve Edge type start_year (int), end_year (int) <pre><code>nebula&gt; CREATE TAG player(name string, age int);\n\nnebula&gt; CREATE TAG team(name string);\n\nnebula&gt; CREATE EDGE follow(degree int);\n\nnebula&gt; CREATE EDGE serve(start_year int, end_year int);\n</code></pre>"},{"location":"2.quick-start/4.nebula-graph-crud/#insert_vertices_and_edges","title":"Insert vertices and edges","text":"<p>You can use the <code>INSERT</code> statement to insert vertices or edges based on existing tags or edge types.</p>"},{"location":"2.quick-start/4.nebula-graph-crud/#ngql_syntax_2","title":"nGQL syntax","text":"<ul> <li>Insert vertices:<pre><code>INSERT VERTEX [IF NOT EXISTS] [tag_props, [tag_props] ...]\nVALUES &lt;vid&gt;: ([prop_value_list])\n\ntag_props:\n  tag_name ([prop_name_list])\n\nprop_name_list:\n   [prop_name [, prop_name] ...]\n\nprop_value_list:\n   [prop_value [, prop_value] ...]   \n</code></pre> <p><code>vid</code> is short for Vertex ID. A <code>vid</code> must be a unique string value in a graph space. For details, see INSERT VERTEX.</p> </li> </ul> <ul> <li> <p>Insert edges:</p> <pre><code>INSERT EDGE [IF NOT EXISTS] &lt;edge_type&gt; ( &lt;prop_name_list&gt; ) VALUES \n&lt;src_vid&gt; -&gt; &lt;dst_vid&gt;[@&lt;rank&gt;] : ( &lt;prop_value_list&gt; )\n[, &lt;src_vid&gt; -&gt; &lt;dst_vid&gt;[@&lt;rank&gt;] : ( &lt;prop_value_list&gt; ), ...];\n&lt;prop_name_list&gt; ::=\n[ &lt;prop_name&gt; [, &lt;prop_name&gt; ] ...]\n&lt;prop_value_list&gt; ::=\n[ &lt;prop_value&gt; [, &lt;prop_value&gt; ] ...]\n</code></pre> <p>For more information on parameters, see INSERT EDGE.</p> </li> </ul>"},{"location":"2.quick-start/4.nebula-graph-crud/#examples_2","title":"Examples","text":"<ul> <li>Insert vertices representing basketball players and teams:<pre><code>nebula&gt; INSERT VERTEX player(name, age) VALUES \"player100\":(\"Tim Duncan\", 42);\n\nnebula&gt; INSERT VERTEX player(name, age) VALUES \"player101\":(\"Tony Parker\", 36);\n\nnebula&gt; INSERT VERTEX player(name, age) VALUES \"player102\":(\"LaMarcus Aldridge\", 33);\n\nnebula&gt; INSERT VERTEX team(name) VALUES \"team203\":(\"Trail Blazers\"), \"team204\":(\"Spurs\");\n</code></pre> </li> </ul> <ul> <li>Insert edges representing the relations between basketball players and teams:<pre><code>nebula&gt; INSERT EDGE follow(degree) VALUES \"player101\" -&gt; \"player100\":(95);\n\nnebula&gt; INSERT EDGE follow(degree) VALUES \"player101\" -&gt; \"player102\":(90);\n\nnebula&gt; INSERT EDGE follow(degree) VALUES \"player102\" -&gt; \"player100\":(75);\n\nnebula&gt; INSERT EDGE serve(start_year, end_year) VALUES \"player101\" -&gt; \"team204\":(1999, 2018),\"player102\" -&gt; \"team203\":(2006,  2015);\n</code></pre> </li> </ul>"},{"location":"2.quick-start/4.nebula-graph-crud/#read_data","title":"Read data","text":"<ul> <li>The GO statement can traverse the database based on specific conditions. A <code>GO</code> traversal starts from one or more vertices, along one or more edges, and returns information in a form specified in the <code>YIELD</code> clause.</li> </ul> <ul> <li>The FETCH statement is used to get properties from vertices or edges.</li> </ul> <ul> <li>The LOOKUP statement is based on indexes. It is used together with the <code>WHERE</code> clause to search for the data that meet the specific conditions.</li> </ul> <ul> <li>The MATCH statement is the most commonly used statement for graph data querying. It can describe all kinds of graph patterns, but it relies on indexes to match data patterns in NebulaGraph. Therefore, its performance still needs optimization.</li> </ul>"},{"location":"2.quick-start/4.nebula-graph-crud/#ngql_syntax_3","title":"nGQL syntax","text":"<ul> <li><code>GO</code><pre><code>GO [[&lt;M&gt; TO] &lt;N&gt; {STEP|STEPS} ] FROM &lt;vertex_list&gt;\nOVER &lt;edge_type_list&gt; [{REVERSELY | BIDIRECT}]\n[ WHERE &lt;conditions&gt; ]\nYIELD [DISTINCT] &lt;return_list&gt;\n[{ SAMPLE &lt;sample_list&gt; | &lt;limit_by_list_clause&gt; }]\n[| GROUP BY {&lt;col_name&gt; | expression&gt; | &lt;position&gt;} YIELD &lt;col_name&gt;]\n[| ORDER BY &lt;expression&gt; [{ASC | DESC}]]\n[| LIMIT [&lt;offset&gt;,] &lt;number_rows&gt;];\n</code></pre> </li> </ul> <ul> <li> <p><code>FETCH</code></p> <ul> <li> <p>Fetch properties on tags:</p> <pre><code>FETCH PROP ON {&lt;tag_name&gt;[, tag_name ...] | *}\n&lt;vid&gt; [, vid ...]\nYIELD &lt;return_list&gt; [AS &lt;alias&gt;];\n</code></pre> </li> </ul> <ul> <li> <p>Fetch properties on edges:</p> <pre><code>FETCH PROP ON &lt;edge_type&gt; &lt;src_vid&gt; -&gt; &lt;dst_vid&gt;[@&lt;rank&gt;] [, &lt;src_vid&gt; -&gt; &lt;dst_vid&gt; ...]\nYIELD &lt;output&gt;;\n</code></pre> </li> </ul> </li> </ul> <ul> <li><code>LOOKUP</code><pre><code>LOOKUP ON {&lt;vertex_tag&gt; | &lt;edge_type&gt;}\n[WHERE &lt;expression&gt; [AND &lt;expression&gt; ...]]\nYIELD &lt;return_list&gt; [AS &lt;alias&gt;];\n&lt;return_list&gt;\n    &lt;prop_name&gt; [AS &lt;col_alias&gt;] [, &lt;prop_name&gt; [AS &lt;prop_alias&gt;] ...];\n</code></pre> </li> </ul> <ul> <li><code>MATCH</code><pre><code>MATCH &lt;pattern&gt; [&lt;clause_1&gt;] RETURN &lt;output&gt; [&lt;clause_2&gt;];\n</code></pre> </li> </ul>"},{"location":"2.quick-start/4.nebula-graph-crud/#examples_of_go_statement","title":"Examples of <code>GO</code> statement","text":"<ul> <li>Search for the players that the player with VID <code>player101</code> follows.<pre><code>nebula&gt; GO FROM \"player101\" OVER follow YIELD id($$);\n+-------------+\n| id($$)      |\n+-------------+\n| \"player100\" |\n| \"player102\" |\n| \"player125\" |\n+-------------+\n</code></pre> </li> </ul> <ul> <li>Filter the players that the player with VID <code>player101</code> follows whose age is equal to or greater than 35. Rename the corresponding columns in the results with <code>Teammate</code> and <code>Age</code>.<pre><code>nebula&gt; GO FROM \"player101\" OVER follow WHERE properties($$).age &gt;= 35 \\\n        YIELD properties($$).name AS Teammate, properties($$).age AS Age;\n+-----------------+-----+\n| Teammate        | Age |\n+-----------------+-----+\n| \"Tim Duncan\"    | 42  |\n| \"Manu Ginobili\" | 41  |\n+-----------------+-----+\n</code></pre> <p>| Clause/Sign | Description                                                         |   |-------------+---------------------------------------------------------------------|   | <code>YIELD</code>     | Specifies what values or results you want to return from the query. |   | <code>$$</code>        | Represents the target vertices.                                     |   | <code>\\</code>         | A line-breaker.                                                     |</p> </li> </ul> <ul> <li> <p>Search for the players that the player with VID <code>player101</code> follows. Then retrieve the teams of the players that the player with VID <code>player100</code> follows. To combine the two queries, use a pipe or a temporary variable.</p> <ul> <li> <p>With a pipe:</p> <pre><code>nebula&gt; GO FROM \"player101\" OVER follow YIELD dst(edge) AS id | \\\n        GO FROM $-.id OVER serve YIELD properties($$).name AS Team, \\\n        properties($^).name AS Player;\n+-----------------+---------------------+\n| Team            | Player              |\n+-----------------+---------------------+\n| \"Spurs\"         | \"Tim Duncan\"        |\n| \"Trail Blazers\" | \"LaMarcus Aldridge\" |\n| \"Spurs\"         | \"LaMarcus Aldridge\" |\n| \"Spurs\"         | \"Manu Ginobili\"     |\n+-----------------+---------------------+\n</code></pre> Clause/Sign Description <code>$^</code> Represents the source vertex of the edge. <code>|</code> A pipe symbol can combine multiple queries. <code>$-</code> Represents the outputs of the query before the pipe symbol. </li> </ul> <ul> <li> <p>With a temporary variable:</p> <p>Note</p> <p>Once a composite statement is submitted to the server as a whole, the life cycle of the temporary variables in the statement ends.</p> <pre><code>nebula&gt; $var = GO FROM \"player101\" OVER follow YIELD dst(edge) AS id; \\\n        GO FROM $var.id OVER serve YIELD properties($$).name AS Team, \\\n        properties($^).name AS Player;\n+-----------------+---------------------+\n| Team            | Player              |\n+-----------------+---------------------+\n| \"Spurs\"         | \"Tim Duncan\"        |\n| \"Trail Blazers\" | \"LaMarcus Aldridge\" |\n| \"Spurs\"         | \"LaMarcus Aldridge\" |\n| \"Spurs\"         | \"Manu Ginobili\"     |\n+-----------------+---------------------+\n</code></pre> </li> </ul> </li> </ul>"},{"location":"2.quick-start/4.nebula-graph-crud/#example_of_fetch_statement","title":"Example of <code>FETCH</code> statement","text":"<p>Use <code>FETCH</code>: Fetch the properties of the player with VID <code>player100</code>.</p> <pre><code>nebula&gt; FETCH PROP ON player \"player100\" YIELD properties(vertex);\n+-------------------------------+\n| properties(VERTEX)            |\n+-------------------------------+\n| {age: 42, name: \"Tim Duncan\"} |\n+-------------------------------+\n</code></pre> <p>Note</p> <p>The examples of <code>LOOKUP</code> and <code>MATCH</code> statements are in indexes.</p>"},{"location":"2.quick-start/4.nebula-graph-crud/#update_vertices_and_edges","title":"Update vertices and edges","text":"<p>Users can use the <code>UPDATE</code> or the <code>UPSERT</code> statements to update existing data.</p> <p><code>UPSERT</code> is the combination of <code>UPDATE</code> and <code>INSERT</code>. If you update a vertex or an edge with <code>UPSERT</code>, the database will insert a new vertex or edge if it does not exist.</p> <p>Note</p> <p><code>UPSERT</code> operates serially in a partition-based order. Therefore, it is slower than <code>INSERT</code> OR <code>UPDATE</code>. And <code>UPSERT</code> has concurrency only between multiple partitions.</p>"},{"location":"2.quick-start/4.nebula-graph-crud/#ngql_syntax_4","title":"nGQL syntax","text":"<ul> <li><code>UPDATE</code> vertices:<pre><code>UPDATE VERTEX &lt;vid&gt; SET &lt;properties to be updated&gt;\n[WHEN &lt;condition&gt;] [YIELD &lt;columns&gt;];\n</code></pre> </li> </ul> <ul> <li><code>UPDATE</code> edges:<pre><code>UPDATE EDGE ON &lt;edge_type&gt; &lt;source vid&gt; -&gt; &lt;destination vid&gt; [@rank] \nSET &lt;properties to be updated&gt; [WHEN &lt;condition&gt;] [YIELD &lt;columns to be output&gt;];\n</code></pre> </li> </ul> <ul> <li><code>UPSERT</code> vertices or edges:<pre><code>UPSERT {VERTEX &lt;vid&gt; | EDGE &lt;edge_type&gt;} SET &lt;update_columns&gt;\n[WHEN &lt;condition&gt;] [YIELD &lt;columns&gt;];\n</code></pre> </li> </ul>"},{"location":"2.quick-start/4.nebula-graph-crud/#examples_3","title":"Examples","text":"<ul> <li><code>UPDATE</code> the <code>name</code> property of the vertex with VID <code>player100</code> and check the result with the <code>FETCH</code> statement.<pre><code>nebula&gt; UPDATE VERTEX \"player100\" SET player.name = \"Tim\";\n\nnebula&gt; FETCH PROP ON player \"player100\" YIELD properties(vertex);\n+------------------------+\n| properties(VERTEX)     |\n+------------------------+\n| {age: 42, name: \"Tim\"} |\n+------------------------+\n</code></pre> </li> </ul> <ul> <li><code>UPDATE</code> the <code>degree</code> property of an edge and check the result with the <code>FETCH</code> statement.<pre><code>nebula&gt; UPDATE EDGE ON follow \"player101\" -&gt; \"player100\" SET degree = 96;\n\nnebula&gt; FETCH PROP ON follow \"player101\" -&gt; \"player100\" YIELD properties(edge);\n+------------------+\n| properties(EDGE) |\n+------------------+\n| {degree: 96}     |\n+------------------+\n</code></pre> </li> </ul> <ul> <li>Insert a vertex with VID <code>player111</code> and <code>UPSERT</code> it.<pre><code>nebula&gt; INSERT VERTEX player(name,age) VALUES \"player111\":(\"David West\", 38);\n\nnebula&gt; UPSERT VERTEX \"player111\" SET player.name = \"David\", player.age = $^.player.age + 11 \\\n        WHEN $^.player.name == \"David West\" AND $^.player.age &gt; 20 \\\n        YIELD $^.player.name AS Name, $^.player.age AS Age;\n+---------+-----+\n| Name    | Age |\n+---------+-----+\n| \"David\" | 49  |\n+---------+-----+\n</code></pre> </li> </ul>"},{"location":"2.quick-start/4.nebula-graph-crud/#delete_vertices_and_edges","title":"Delete vertices and edges","text":""},{"location":"2.quick-start/4.nebula-graph-crud/#ngql_syntax_5","title":"nGQL syntax","text":"<ul> <li>Delete vertices:<pre><code>DELETE VERTEX &lt;vid1&gt;[, &lt;vid2&gt;...]\n</code></pre> </li> </ul> <ul> <li>Delete edges:<pre><code>DELETE EDGE &lt;edge_type&gt; &lt;src_vid&gt; -&gt; &lt;dst_vid&gt;[@&lt;rank&gt;]\n[, &lt;src_vid&gt; -&gt; &lt;dst_vid&gt;...]\n</code></pre> </li> </ul>"},{"location":"2.quick-start/4.nebula-graph-crud/#examples_4","title":"Examples","text":"<ul> <li>Delete vertices:<pre><code>nebula&gt; DELETE VERTEX \"player111\", \"team203\";\n</code></pre> </li> </ul> <ul> <li>Delete edges:<pre><code>nebula&gt; DELETE EDGE follow \"player101\" -&gt; \"team204\";\n</code></pre> </li> </ul>"},{"location":"2.quick-start/4.nebula-graph-crud/#about_indexes","title":"About indexes","text":"<p>Users can add indexes to tags and edge types with the CREATE INDEX statement.</p> <p>Must-read for using indexes</p> <p>Both <code>MATCH</code> and <code>LOOKUP</code> statements depend on the indexes. But indexes can dramatically reduce the write performance. DO NOT use indexes in production environments unless you are fully aware of their influences on your service.</p> <p>Users MUST rebuild indexes for pre-existing data. Otherwise, the pre-existing data cannot be indexed and therefore cannot be returned in <code>MATCH</code> or <code>LOOKUP</code> statements. For more information, see REBUILD INDEX.</p>"},{"location":"2.quick-start/4.nebula-graph-crud/#ngql_syntax_6","title":"nGQL syntax","text":"<ul> <li>Create an index:<pre><code>CREATE {TAG | EDGE} INDEX [IF NOT EXISTS] &lt;index_name&gt;\nON {&lt;tag_name&gt; | &lt;edge_name&gt;} ([&lt;prop_name_list&gt;]) [COMMENT = '&lt;comment&gt;'];\n</code></pre> </li> </ul> <ul> <li>Rebuild an index:<pre><code>REBUILD {TAG | EDGE} INDEX &lt;index_name&gt;;\n</code></pre> </li> </ul> <p>Note</p> <p>Define the index length when creating an index for a variable-length property. In UTF-8 encoding, a non-ascii character occupies 3 bytes. You should set an appropriate index length according to the variable-length property. For example, the index should be 30 bytes for 10 non-ascii characters. For more information, see CREATE INDEX</p>"},{"location":"2.quick-start/4.nebula-graph-crud/#examples_of_lookup_and_match_index-based","title":"Examples of <code>LOOKUP</code> and <code>MATCH</code> (index-based)","text":"<p>Make sure there is an index for <code>LOOKUP</code> or <code>MATCH</code> to use. If there is not, create an index first.</p> <p>Find the information of the vertex with the tag <code>player</code> and its value of the <code>name</code> property is <code>Tony Parker</code>.</p> <p>This example creates the index <code>player_index_1</code> on the <code>name</code> property.</p> <pre><code>nebula&gt; CREATE TAG INDEX IF NOT EXISTS player_index_1 ON player(name(20));\n</code></pre> <p>This example rebuilds the index to make sure it takes effect on pre-existing data.</p> <pre><code>nebula&gt; REBUILD TAG INDEX player_index_1\n+------------+\n| New Job Id |\n+------------+\n| 31         |\n+------------+\n</code></pre> <p>This example uses the <code>LOOKUP</code> statement to retrieve the vertex property.</p> <pre><code>nebula&gt; LOOKUP ON player WHERE player.name == \"Tony Parker\" \\\n        YIELD properties(vertex).name AS name, properties(vertex).age AS age;\n+---------------+-----+\n| name          | age |\n+---------------+-----+\n| \"Tony Parker\" | 36  |\n+---------------+-----+\n</code></pre> <p>This example uses the <code>MATCH</code> statement to retrieve the vertex property.</p> <pre><code>nebula&gt; MATCH (v:player{name:\"Tony Parker\"}) RETURN v;\n+-----------------------------------------------------+\n| v                                                   |\n+-----------------------------------------------------+\n| (\"player101\" :player{age: 36, name: \"Tony Parker\"}) |\n+-----------------------------------------------------+\n</code></pre>"},{"location":"2.quick-start/5.start-stop-service/","title":"Step 2: Manage NebulaGraph Service","text":"<p>NebulaGraph supports managing services with scripts. </p> <p>Enterpriseonly</p> <p>You can also manage NebulaGraph with systemd in the NebulaGraph Enterprise Edition.</p> <p>Danger</p> <p>The two methods are incompatible. It is recommended to use only one method in a cluster.</p>"},{"location":"2.quick-start/5.start-stop-service/#manage_services_with_script","title":"Manage services with script","text":"<p>You can use the <code>nebula.service</code> script to start, stop, restart, terminate, and check the NebulaGraph services.</p> <p>Note</p> <p><code>nebula.service</code> is stored in the <code>/usr/local/nebula/scripts</code> directory by default. If you have customized the path, use the actual path in your environment.</p>"},{"location":"2.quick-start/5.start-stop-service/#syntax","title":"Syntax","text":"<pre><code>$ sudo /usr/local/nebula/scripts/nebula.service\n[-v] [-c &lt;config_file_path&gt;]\n&lt;start | stop | restart | kill | status&gt;\n&lt;metad | graphd | storaged | all&gt;\n</code></pre> Parameter Description <code>-v</code> Display detailed debugging information. <code>-c</code> Specify the configuration file path. The default path is <code>/usr/local/nebula/etc/</code>. <code>start</code> Start the target services. <code>stop</code> Stop the target services. <code>restart</code> Restart the target services. <code>kill</code> Terminate the target services. <code>status</code> Check the status of the target services. <code>metad</code> Set the Meta Service as the target service. <code>graphd</code> Set the Graph Service as the target service. <code>storaged</code> Set the Storage Service as the target service. <code>all</code> Set all the NebulaGraph services as the target services."},{"location":"2.quick-start/5.start-stop-service/#manage_services_with_systemd","title":"Manage services with systemd","text":"<p>For easy maintenance, NebulaGraph Enterprise Edition supports managing services with systemd. You can start, stop, restart, and check services with <code>systemctl</code> commands.</p> <p>Note</p> <ul> <li>After installing NebulaGraph Enterprise Edition, the <code>.service</code> files required by systemd are located in the <code>etc/unit</code> path in the installation directory. NebulaGraph installed with the RPM/DEB package automatically places the <code>.service</code> files into the path <code>/usr/lib/systemd/system</code> and the parameter <code>ExecStart</code> is generated based on the specified NebulaGraph installation path, so you can use <code>systemctl</code> commands directly.</li> </ul> <ul> <li>The <code>systemctl</code> commands cannot be used to manage the Enterprise Edition cluster that is created with Dashboard of the Enterprise Edition.</li> </ul> <ul> <li>Otherwise, users need to move the <code>.service</code> files manually into the directory <code>/usr/lib/systemd/system</code>, and modify the file path of the parameter <code>ExecStart</code> in the <code>.service</code> files.</li> </ul>"},{"location":"2.quick-start/5.start-stop-service/#syntax_1","title":"Syntax","text":"<pre><code>$ systemctl &lt;start | stop | restart | status &gt; &lt;nebula | nebula-metad | nebula-graphd | nebula-storaged&gt;\n</code></pre> Parameter Description <code>start</code> Start the target services. <code>stop</code> Stop the target services. <code>restart</code> Restart the target services. <code>status</code> Check the status of the target services. <code>nebula</code> Set all the NebulaGraph services as the target services. <code>nebula-metad</code> Set the Meta Service as the target service. <code>nebula-graphd</code> Set the Graph Service as the target service. <code>nebula-storaged</code> Set the Storage Service as the target service."},{"location":"2.quick-start/5.start-stop-service/#start_nebulagraph","title":"Start NebulaGraph","text":"<p>Run the following command to start NebulaGraph.</p> <pre><code>$ sudo /usr/local/nebula/scripts/nebula.service start all\n[INFO] Starting nebula-metad...\n[INFO] Done\n[INFO] Starting nebula-graphd...\n[INFO] Done\n[INFO] Starting nebula-storaged...\n[INFO] Done\n</code></pre> <p>Users can also run the following command:</p> <pre><code>$ systemctl start nebula\n</code></pre> <p>If users want to automatically start NebulaGraph when the machine starts, run the following command:</p> <pre><code>$ systemctl enable nebula\n</code></pre>"},{"location":"2.quick-start/5.start-stop-service/#stop_nebulagraph","title":"Stop NebulaGraph","text":"<p>Danger</p> <p>Do not run <code>kill -9</code> to forcibly terminate the processes. Otherwise, there is a low probability of data loss.</p> <p>Run the following command to stop NebulaGraph.</p> <pre><code>$ sudo /usr/local/nebula/scripts/nebula.service stop all\n[INFO] Stopping nebula-metad...\n[INFO] Done\n[INFO] Stopping nebula-graphd...\n[INFO] Done\n[INFO] Stopping nebula-storaged...\n[INFO] Done\n</code></pre> <p>Users can also run the following command:</p> <pre><code>$ systemctl stop nebula\n</code></pre>"},{"location":"2.quick-start/5.start-stop-service/#check_the_service_status","title":"Check the service status","text":"<p>Run the following command to check the service status of NebulaGraph.</p> <pre><code>$ sudo /usr/local/nebula/scripts/nebula.service status all\n</code></pre> <ul> <li> <p>NebulaGraph is running normally if the following information is returned.</p> <pre><code>INFO] nebula-metad(33fd35e): Running as 29020, Listening on 9559\n[INFO] nebula-graphd(33fd35e): Running as 29095, Listening on 9669\n[WARN] nebula-storaged after v3.0.0 will not start service until it is added to cluster.\n[WARN] See Manage Storage hosts:ADD HOSTS in https://docs.nebula-graph.io/\n[INFO] nebula-storaged(33fd35e): Running as 29147, Listening on 9779\n</code></pre> <p>Note</p> <p>After starting NebulaGraph, the port of the <code>nebula-storaged</code> process is shown in red. Because the <code>nebula-storaged</code> process waits for the <code>nebula-metad</code> to add the current Storage service during the startup process. The Storage works after it receives the ready signal. Starting from NebulaGraph 3.0.0, the Meta service cannot directly read or write data in the Storage service that you add in the configuration file. The configuration file only registers the Storage service to the Meta service. You must run the <code>ADD HOSTS</code> command to enable the Meta to read and write data in the Storage service. For more information, see Manage Storage hosts.</p> </li> </ul> <ul> <li>If the returned result is similar to the following one, there is a problem. You may also go to the NebulaGraph community for help.<pre><code>[INFO] nebula-metad: Running as 25600, Listening on 9559\n[INFO] nebula-graphd: Exited\n[INFO] nebula-storaged: Running as 25646, Listening on 9779\n</code></pre> </li> </ul> <p>Users can also run the following command:</p> <pre><code>$ systemctl status nebula\n\u25cf nebula.service\n   Loaded: loaded (/usr/lib/systemd/system/nebula.service; disabled; vendor preset: disabled)\n   Active: active (exited) since \u4e00 2022-03-28 04:13:24 UTC; 1h 47min ago\n  Process: 21772 ExecStart=/usr/local/ent-nightly/scripts/nebula.service start all (code=exited, status=0/SUCCESS)\n Main PID: 21772 (code=exited, status=0/SUCCESS)\n    Tasks: 325\n   Memory: 424.5M\n   CGroup: /system.slice/nebula.service\n           \u251c\u250021789 /usr/local/ent-nightly/bin/nebula-metad --flagfile /usr/local/ent-nightly/etc/nebula-metad.conf\n           \u251c\u250021827 /usr/local/ent-nightly/bin/nebula-graphd --flagfile /usr/local/ent-nightly/etc/nebula-graphd.conf\n           \u2514\u250021900 /usr/local/ent-nightly/bin/nebula-storaged --flagfile /usr/local/ent-nightly/etc/nebula-storaged.conf\n3\u6708 28 04:13:24 xxxxxx systemd[1]: Started nebula.service.\n...\n</code></pre> <p>The NebulaGraph services consist of the Meta Service, Graph Service, and Storage Service. The configuration files for all three services are stored in the <code>/usr/local/nebula/etc/</code> directory by default. You can check the configuration files according to the returned result to troubleshoot problems.</p>"},{"location":"2.quick-start/5.start-stop-service/#next_to_do","title":"Next to do","text":"<p>Connect to NebulaGraph</p>"},{"location":"2.quick-start/6.cheatsheet-for-ngql/","title":"nGQL cheatsheet","text":""},{"location":"2.quick-start/6.cheatsheet-for-ngql/#functions","title":"Functions","text":"<ul> <li> <p>Math functions</p> Function Description double abs(double x) Returns the absolute value of the argument. double floor(double x) Returns the largest integer value smaller than or equal to the argument. (Rounds down) double ceil(double x) Returns the smallest integer greater than or equal to the argument. (Rounds up) double round(double x) Returns the integer value nearest to the argument. Returns a number farther away from 0 if the argument is in the middle. double sqrt(double x) Returns the square root of the argument. double cbrt(double x) Returns the cubic root of the argument. double hypot(double x, double y) Returns the hypotenuse of a right-angled triangle. double pow(double x, double y) Returns the result of x<sup>y</sup>. double exp(double x) Returns the result of e<sup>x</sup>. double exp2(double x) Returns the result of 2<sup>x</sup>. double log(double x) Returns the base-e logarithm of the argument. double log2(double x) Returns the base-2 logarithm of the argument. double log10(double x) Returns the base-10 logarithm of the argument. double sin(double x) Returns the sine of the argument. double asin(double x) Returns the inverse sine of the argument. double cos(double x) Returns the cosine of the argument. double acos(double x) Returns the inverse cosine of the argument. double tan(double x) Returns the tangent of the argument. double atan(double x) Returns the inverse tangent of the argument. double rand() Returns a random floating point number in the range from 0 (inclusive) to 1 (exclusive); i.e.[0,1). int rand32(int min, int max) Returns a random 32-bit integer in <code>[min, max)</code>.If you set only one argument, it is parsed as <code>max</code> and <code>min</code> is <code>0</code> by default.If you set no argument, the system returns a random signed 32-bit integer. int rand64(int min, int max) Returns a random 64-bit integer in <code>[min, max)</code>.If you set only one argument, it is parsed as <code>max</code> and <code>min</code> is <code>0</code> by default.If you set no argument, the system returns a random signed 64-bit integer. bit_and() Bitwise AND. bit_or() Bitwise OR. bit_xor() Bitwise XOR. int size() Returns the number of elements in a list or a map or the length of a string. int range(int start, int end, int step) Returns a list of integers from <code>[start,end]</code> in the specified steps. <code>step</code> is 1 by default. int sign(double x) Returns the signum of the given number.If the number is <code>0</code>, the system returns <code>0</code>.If the number is negative, the system returns <code>-1</code>.If the number is positive, the system returns <code>1</code>. double e() Returns the base of the natural logarithm, e (2.718281828459045). double pi() Returns the mathematical constant pi (3.141592653589793). double radians() Converts degrees to radians. <code>radians(180)</code> returns <code>3.141592653589793</code>. </li> </ul> <ul> <li> <p>Aggregating functions</p> Function Description avg() Returns the average value of the argument. count() Syntax: <code>count({expr | *})</code> .<code>count()</code>returns the number of rows (including NULL). <code>count(expr)</code>returns the number of non-NULL values that meet the expression. count() and size() are different. max() Returns the maximum value. min() Returns the minimum value. collect() The collect() function returns a list containing the values returned by an expression. Using this function aggregates data by merging multiple records or values into a single list. std() Returns the population standard deviation. sum() Returns the sum value. </li> </ul> <ul> <li> <p>String functions</p> Function Description int strcasecmp(string a, string b) Compares string a and b without case sensitivity. When a = b, the return string lower(string a) Returns the argument in lowercase. string toLower(string a) The same as <code>lower()</code>. string upper(string a) Returns the argument in uppercase. string toUpper(string a) The same as <code>upper()</code>. int length(a) Returns the length of the given string in bytes or the length of a path in hops. string trim(string a) Removes leading and trailing spaces. string ltrim(string a) Removes leading spaces. string rtrim(string a) Removes trailing spaces. string left(string a, int count) Returns a substring consisting of <code>count</code> characters from the left side of string right(string a, int count) Returns a substring consisting of <code>count</code> characters from the right side of string lpad(string a, int size, string letters) Left-pads string a with string <code>letters</code> and returns a string rpad(string a, int size, string letters) Right-pads string a with string <code>letters</code> and returns a string substr(string a, int pos, int count) Returns a substring extracting <code>count</code> characters starting from string substring(string a, int pos, int count) The same as <code>substr()</code>. string reverse(string) Returns a string in reverse order. string replace(string a, string b, string c) Replaces string b in string a with string c. list split(string a, string b) Splits string a at string b and returns a list of strings. concat() The <code>concat()</code> function requires at least two or more strings. All the parameters are concatenated into one string.Syntax: <code>concat(string1,string2,...)</code> concat_ws() The <code>concat_ws()</code> function connects two or more strings with a predefined separator. extract() <code>extract()</code> uses regular expression matching to retrieve a single substring or all substrings from a string. json_extract() The <code>json_extract()</code> function converts the specified JSON string to map. </li> </ul> <ul> <li> <p>Data and time functions</p> Function Description int now() Returns the current timestamp of the system. timestamp timestamp() Returns the current timestamp of the system. date date() Returns the current UTC date based on the current system. time time() Returns the current UTC time based on the current system. datetime datetime() Returns the current UTC date and time based on the current system. </li> </ul> <ul> <li> <p>Schema-related functions</p> <ul> <li> <p>For nGQL statements</p> Function Description id(vertex) Returns the ID of a vertex. The data type of the result is the same as the vertex ID. map properties(vertex) Returns the properties of a vertex. map properties(edge) Returns the properties of an edge. string type(edge) Returns the edge type of an edge. src(edge) Returns the source vertex ID of an edge. The data type of the result is the same as the vertex ID. dst(edge) Returns the destination vertex ID of an edge. The data type of the result is the same as the vertex ID. int rank(edge) Returns the rank value of an edge. vertex Returns the information of vertices, including VIDs, tags, properties, and values. edge Returns the information of edges, including edge types, source vertices, destination vertices, ranks, properties, and values. vertices Returns the information of vertices in a subgraph. For more information, see GET SUBGRAPH. edges Returns the information of edges in a subgraph. For more information, see GET SUBGRAPH. path Returns the information of a path. For more information, see FIND PATH. </li> </ul> <ul> <li> <p>For statements compatible with openCypher</p> Function Description id(&lt;vertex&gt;) Returns the ID of a vertex. The data type of the result is the same as the vertex ID. list tags(&lt;vertex&gt;) Returns the Tag of a vertex, which serves the same purpose as labels(). list labels(&lt;vertex&gt;) Returns the Tag of a vertex, which serves the same purpose as tags(). This function is used for compatibility with openCypher syntax. map properties(&lt;vertex_or_edge&gt;) Returns the properties of a vertex or an edge. string type(&lt;edge&gt;) Returns the edge type of an edge. src(&lt;edge&gt;) Returns the source vertex ID of an edge. The data type of the result is the same as the vertex ID. dst(&lt;edge&gt;) Returns the destination vertex ID of an edge. The data type of the result is the same as the vertex ID. vertex startNode(&lt;path&gt;) Visits an edge or a path and returns its source vertex ID. string endNode(&lt;path&gt;) Visits an edge or a path and returns its destination vertex ID. int rank(&lt;edge&gt;) Returns the rank value of an edge. </li> </ul> </li> </ul> <ul> <li> <p>List functions</p> Function Description keys(expr) Returns a list containing the string representations for all the property names of vertices, edges, or maps. labels(vertex) Returns the list containing all the tags of a vertex. nodes(path) Returns the list containing all the vertices in a path. range(start, end [, step]) Returns the list containing all the fixed-length steps in <code>[start,end]</code>. <code>step</code> is 1 by default. relationships(path) Returns the list containing all the relationships in a path. reverse(list) Returns the list reversing the order of all elements in the original list. tail(list) Returns all the elements of the original list, excluding the first one. head(list) Returns the first element of a list. last(list) Returns the last element of a list. reduce() The <code>reduce()</code> function applies an expression to each element in a list one by one, chains the result to the next iteration by taking it as the initial value, and returns the final result. </li> </ul> <ul> <li> <p>Type conversion functions</p> Function Description bool toBoolean() Converts a string value to a boolean value. float toFloat() Converts an integer or string value to a floating point number. string toString() Converts non-compound types of data, such as numbers, booleans, and so on, to strings. int toInteger() Converts a floating point or string value to an integer value. set toSet() Converts a list or set value to a set value. int hash() The <code>hash()</code> function returns the hash value of the argument. The argument can be a number, a string, a list, a boolean, null, or an expression that evaluates to a value of the preceding data types. </li> </ul> <ul> <li> <p>Predicate functions</p> <p>Predicate functions return <code>true</code> or <code>false</code>. They are most commonly used in <code>WHERE</code> clauses.</p> <pre><code>&lt;predicate&gt;(&lt;variable&gt; IN &lt;list&gt; WHERE &lt;condition&gt;)\n</code></pre> Function Description exists() Returns <code>true</code> if the specified property exists in the vertex, edge or map. Otherwise, returns <code>false</code>. any() Returns <code>true</code> if the specified predicate holds for at least one element in the given list. Otherwise, returns <code>false</code>. all() Returns <code>true</code> if the specified predicate holds for all elements in the given list. Otherwise, returns <code>false</code>. none() Returns <code>true</code> if the specified predicate holds for no element in the given list. Otherwise, returns <code>false</code>. single() Returns <code>true</code> if the specified predicate holds for exactly one of the elements in the given list. Otherwise, returns <code>false</code>. </li> </ul> <ul> <li> <p>Conditional expressions functions</p> Function Description CASE The <code>CASE</code> expression uses conditions to filter the result of an nGQL query statement. It is usually used in the <code>YIELD</code> and <code>RETURN</code> clauses. The <code>CASE</code> expression will traverse all the conditions. When the first condition is met, the <code>CASE</code> expression stops reading the conditions and returns the result. If no conditions are met, it returns the result in the <code>ELSE</code> clause. If there is no <code>ELSE</code> clause and no conditions are met, it returns <code>NULL</code>. coalesce() Returns the first not null value in all expressions. </li> </ul>"},{"location":"2.quick-start/6.cheatsheet-for-ngql/#general_queries_statements","title":"General queries statements","text":"<ul> <li> <p>MATCH</p> <pre><code>MATCH &lt;pattern&gt; [&lt;clause_1&gt;] RETURN &lt;output&gt; [&lt;clause_2&gt;];\n</code></pre> Pattern Example Description Match vertices <code>(v)</code> You can use a user-defined variable in a pair of parentheses to represent a vertex in a pattern. For example: <code>(v)</code>. Match tags <code>MATCH (v:player) RETURN v</code> You can specify a tag with <code>:&lt;tag_name&gt;</code> after the vertex in a pattern. Match multiple tags <code>MATCH (v:player:team) RETURN v</code> To match vertices with multiple tags, use colons (:). Match vertex properties <code>MATCH (v:player{name:\"Tim Duncan\"}) RETURN v</code> <code>MATCH (v) WITH v, properties(v) as props, keys(properties(v)) as kk WHERE [i in kk where props[i] == \"Tim Duncan\"] RETURN v</code> You can specify a vertex property with <code>{&lt;prop_name&gt;: &lt;prop_value&gt;}</code> after the tag in a pattern; or use a vertex property value to get vertices directly. Match a VID. <code>MATCH (v) WHERE id(v) == 'player101' RETURN v</code> You can use the VID to match a vertex. The <code>id()</code> function can retrieve the VID of a vertex. Match multiple VIDs. <code>MATCH (v:player { name: 'Tim Duncan' })--(v2) WHERE id(v2) IN [\"player101\", \"player102\"] RETURN v2</code> To match multiple VIDs, use <code>WHERE id(v) IN [vid_list]</code>. Match connected vertices <code>MATCH (v:player{name:\"Tim Duncan\"})--(v2) RETURN v2.player.name AS Name</code> You can use the <code>--</code> symbol to represent edges of both directions and match vertices connected by these edges. You can add a <code>&gt;</code> or <code>&lt;</code> to the <code>--</code> symbol to specify the direction of an edge. Match paths <code>MATCH p=(v:player{name:\"Tim Duncan\"})--&gt;(v2) RETURN p</code> Connected vertices and edges form a path. You can use a user-defined variable to name a path as follows. Match edges <code>MATCH (v:player{name:\"Tim Duncan\"})-[e]-(v2) RETURN e</code><code>MATCH ()&lt;-[e]-() RETURN e</code> Besides using <code>--</code>, <code>--&gt;</code>, or <code>&lt;--</code> to indicate a nameless edge, you can use a user-defined variable in a pair of square brackets to represent a named edge. For example: <code>-[e]-</code>. Match an edge type <code>MATCH ()-[e:follow]-() RETURN e</code> Just like vertices, you can specify an edge type with <code>:&lt;edge_type&gt;</code> in a pattern. For example: <code>-[e:follow]-</code>. Match edge type properties <code>MATCH (v:player{name:\"Tim Duncan\"})-[e:follow{degree:95}]-&gt;(v2) RETURN e</code> <code>MATCH ()-[e]-&gt;() WITH e, properties(e) as props, keys(properties(e)) as kk WHERE [i in kk where props[i] == 90] RETURN e</code> You can specify edge type properties with <code>{&lt;prop_name&gt;: &lt;prop_value&gt;}</code> in a pattern. For example: <code>[e:follow{likeness:95}]</code>; or use an edge type property value to get edges directly. Match multiple edge types <code>MATCH (v:player{name:\"Tim Duncan\"})-[e:follow | :serve]-&gt;(v2) RETURN e</code> The <code>|</code> symbol can help matching multiple edge types. For example: <code>[e:follow|:serve]</code>. The English colon (:) before the first edge type cannot be omitted, but the English colon before the subsequent edge type can be omitted, such as <code>[e:follow|serve]</code>. Match multiple edges <code>MATCH (v:player{name:\"Tim Duncan\"})-[]-&gt;(v2)&lt;-[e:serve]-(v3) RETURN v2, v3</code> You can extend a pattern to match multiple edges in a path. Match fixed-length paths <code>MATCH p=(v:player{name:\"Tim Duncan\"})-[e:follow*2]-&gt;(v2) RETURN DISTINCT v2 AS Friends</code> You can use the <code>:&lt;edge_type&gt;*&lt;hop&gt;</code> pattern to match a fixed-length path. <code>hop</code> must be a non-negative integer. The data type of <code>e</code> is the list. Match variable-length paths <code>MATCH p=(v:player{name:\"Tim Duncan\"})-[e:follow*1..3]-&gt;(v2) RETURN v2 AS Friends</code> <code>minHop</code>: Optional. It represents the minimum length of the path. <code>minHop</code>: must be a non-negative integer. The default value is 1.<code>minHop</code> and <code>maxHop</code> are optional and the default value is 1 and infinity respectively. The data type of <code>e</code> is the list. Match variable-length paths with multiple edge types <code>MATCH p=(v:player{name:\"Tim Duncan\"})-[e:follow | serve*2]-&gt;(v2) RETURN DISTINCT v2</code> You can specify multiple edge types in a fixed-length or variable-length pattern. In this case, <code>hop</code>, <code>minHop</code>, and <code>maxHop</code> take effect on all edge types. The data type of <code>e</code> is the list. Retrieve vertex or edge information <code>MATCH (v:player{name:\"Tim Duncan\"}) RETURN v</code><code>MATCH (v:player{name:\"Tim Duncan\"})-[e]-&gt;(v2) RETURN e</code> Use <code>RETURN {&lt;vertex_name&gt; | &lt;edge_name&gt;}</code> to retrieve all the information of a vertex or an edge. Retrieve VIDs <code>MATCH (v:player{name:\"Tim Duncan\"}) RETURN id(v)</code> Use the <code>id()</code> function to retrieve VIDs. Retrieve tags <code>MATCH (v:player{name:\"Tim Duncan\"}) RETURN labels(v)</code> Use the <code>labels()</code> function to retrieve the list of tags on a vertex.To retrieve the nth element in the <code>labels(v)</code> list, use <code>labels(v)[n-1]</code>. Retrieve a single property on a vertex or an edge <code>MATCH (v:player{name:\"Tim Duncan\"}) RETURN v.player.age</code> Use <code>RETURN {&lt;vertex_name&gt; | &lt;edge_name&gt;}.&lt;property&gt;</code> to retrieve a single property.Use <code>AS</code> to specify an alias for a property. Retrieve all properties on a vertex or an edge <code>MATCH p=(v:player{name:\"Tim Duncan\"})-[]-&gt;(v2) RETURN properties(v2)</code> Use the <code>properties()</code> function to retrieve all properties on a vertex or an edge. Retrieve edge types <code>MATCH p=(v:player{name:\"Tim Duncan\"})-[e]-&gt;() RETURN DISTINCT type(e)</code> Use the <code>type()</code> function to retrieve the matched edge types. Retrieve paths <code>MATCH p=(v:player{name:\"Tim Duncan\"})-[*3]-&gt;() RETURN p</code> Use <code>RETURN &lt;path_name&gt;</code> to retrieve all the information of the matched paths. Retrieve vertices in a path <code>MATCH p=(v:player{name:\"Tim Duncan\"})-[]-&gt;(v2) RETURN nodes(p)</code> Use the <code>nodes()</code> function to retrieve all vertices in a path. Retrieve edges in a path <code>MATCH p=(v:player{name:\"Tim Duncan\"})-[]-&gt;(v2) RETURN relationships(p)</code> Use the <code>relationships()</code> function to retrieve all edges in a path. Retrieve path length <code>MATCH p=(v:player{name:\"Tim Duncan\"})-[*..2]-&gt;(v2) RETURN p AS Paths, length(p) AS Length</code> Use the <code>length()</code> function to retrieve the length of a path. </li> </ul> <ul> <li> <p>OPTIONAL MATCH</p> Pattern Example Description Matches patterns against your graph database, just like <code>MATCH</code> does. <code>MATCH (m)-[]-&gt;(n) WHERE id(m)==\"player100\" OPTIONAL MATCH (n)-[]-&gt;(l) RETURN id(m),id(n),id(l)</code> If no matches are found, <code>OPTIONAL MATCH</code> will use a null for missing parts of the pattern. </li> </ul> <ul> <li> <p>LOOKUP</p> <pre><code>LOOKUP ON {&lt;vertex_tag&gt; | &lt;edge_type&gt;} \n[WHERE &lt;expression&gt; [AND &lt;expression&gt; ...]] \nYIELD &lt;return_list&gt; [AS &lt;alias&gt;]\n</code></pre> Pattern Example Description Retrieve vertices <code>LOOKUP ON player WHERE player.name == \"Tony Parker\" YIELD player.name AS name, player.age AS age</code> The following example returns vertices whose <code>name</code> is <code>Tony Parker</code> and the tag is <code>player</code>. Retrieve edges <code>LOOKUP ON follow WHERE follow.degree == 90 YIELD follow.degree</code> Returns edges whose <code>degree</code> is <code>90</code> and the edge type is <code>follow</code>. List vertices with a tag <code>LOOKUP ON player YIELD properties(vertex),id(vertex)</code> Shows how to retrieve the VID of all vertices tagged with <code>player</code>. List edges with an edge types <code>LOOKUP ON follow YIELD edge AS e</code> Shows how to retrieve the source Vertex IDs, destination vertex IDs, and ranks of all edges of the <code>follow</code> edge type. Count the numbers of vertices or edges <code>LOOKUP ON player YIELD id(vertex)| YIELD COUNT(*) AS Player_Count</code> Shows how to count the number of vertices tagged with <code>player</code>. Count the numbers of edges <code>LOOKUP ON follow YIELD edge as e| YIELD COUNT(*) AS Like_Count</code> Shows how to count the number of edges of the <code>follow</code> edge type. </li> </ul> <ul> <li> <p>GO</p> <pre><code>GO [[&lt;M&gt; TO] &lt;N&gt; {STEP|STEPS} ] FROM &lt;vertex_list&gt;\nOVER &lt;edge_type_list&gt; [{REVERSELY | BIDIRECT}]\n[ WHERE &lt;conditions&gt; ]\nYIELD [DISTINCT] &lt;return_list&gt;\n[{SAMPLE &lt;sample_list&gt; | LIMIT &lt;limit_list&gt;}]\n[| GROUP BY {col_name | expr | position} YIELD &lt;col_name&gt;]\n[| ORDER BY &lt;expression&gt; [{ASC | DESC}]]\n[| LIMIT [&lt;offset_value&gt;,] &lt;number_rows&gt;]\n</code></pre> Example Description <code>GO FROM \"player102\" OVER serve YIELD dst(edge)</code> Returns the teams that player 102 serves. <code>GO 2 STEPS FROM \"player102\" OVER follow YIELD dst(edge)</code> Returns the friends of player 102 with 2 hops. <code>GO FROM \"player100\", \"player102\" OVER serve WHERE properties(edge).start_year &gt; 1995 YIELD DISTINCT properties($$).name AS team_name, properties(edge).start_year AS start_year, properties($^).name AS player_name</code> Adds a filter for the traversal. <code>GO FROM \"player100\" OVER follow, serve YIELD properties(edge).degree, properties(edge).start_year</code> The following example traverses along with multiple edge types. If there is no value for a property, the output is <code>NULL</code>. <code>GO FROM \"player100\" OVER follow REVERSELY YIELD src(edge) AS destination</code> The following example returns the neighbor vertices in the incoming direction of player 100. <code>GO FROM \"player100\" OVER follow REVERSELY YIELD src(edge) AS id | GO FROM $-.id OVER serve WHERE properties($^).age &gt; 20 YIELD properties($^).name AS FriendOf, properties($$).name AS Team</code> The following example retrieves the friends of player 100 and the teams that they serve. <code>GO FROM \"player102\" OVER follow YIELD dst(edge) AS both</code> The following example returns all the neighbor vertices of player 102. <code>GO 2 STEPS FROM \"player100\" OVER follow YIELD src(edge) AS src, dst(edge) AS dst, properties($$).age AS age | GROUP BY $-.dst YIELD $-.dst AS dst, collect_set($-.src) AS src, collect($-.age) AS age</code> The following example the outputs according to age. </li> </ul> <ul> <li> <p>FETCH</p> <ul> <li> <p>Fetch vertex properties</p> <pre><code>FETCH PROP ON {&lt;tag_name&gt;[, tag_name ...] | *} \n&lt;vid&gt; [, vid ...] \nYIELD &lt;return_list&gt; [AS &lt;alias&gt;]\n</code></pre> Example Description <code>FETCH PROP ON player \"player100\" YIELD properties(vertex)</code> Specify a tag in the <code>FETCH</code> statement to fetch the vertex properties by that tag. <code>FETCH PROP ON player \"player100\" YIELD player.name AS name</code> Use a <code>YIELD</code> clause to specify the properties to be returned. <code>FETCH PROP ON player \"player101\", \"player102\", \"player103\" YIELD properties(vertex)</code> Specify multiple VIDs (vertex IDs) to fetch properties of multiple vertices. Separate the VIDs with commas. <code>FETCH PROP ON player, t1 \"player100\", \"player103\" YIELD properties(vertex)</code> Specify multiple tags in the <code>FETCH</code> statement to fetch the vertex properties by the tags. Separate the tags with commas. <code>FETCH PROP ON * \"player100\", \"player106\", \"team200\" YIELD properties(vertex)</code> Set an asterisk symbol <code>*</code> to fetch properties by all tags in the current graph space. </li> </ul> <ul> <li> <p>Fetch edge properties</p> <pre><code>FETCH PROP ON &lt;edge_type&gt; &lt;src_vid&gt; -&gt; &lt;dst_vid&gt;[@&lt;rank&gt;] [, &lt;src_vid&gt; -&gt; &lt;dst_vid&gt; ...]\nYIELD &lt;output&gt;;\n</code></pre> Example Description <code>FETCH PROP ON serve \"player100\" -&gt; \"team204\" YIELD properties(edge)</code> The following statement fetches all the properties of the <code>serve</code> edge that connects vertex <code>\"player100\"</code> and vertex <code>\"team204\"</code>. <code>FETCH PROP ON serve \"player100\" -&gt; \"team204\" YIELD serve.start_year</code> Use a <code>YIELD</code> clause to fetch specific properties of an edge. <code>FETCH PROP ON serve \"player100\" -&gt; \"team204\", \"player133\" -&gt; \"team202\" YIELD properties(edge)</code> Specify multiple edge patterns (<code>&lt;src_vid&gt; -&gt; &lt;dst_vid&gt;[@&lt;rank&gt;]</code>) to fetch properties of multiple edges. Separate the edge patterns with commas. <code>FETCH PROP ON serve \"player100\" -&gt; \"team204\"@1 YIELD properties(edge)</code> To fetch on an edge whose rank is not 0, set its rank in the FETCH statement. <code>GO FROM \"player101\" OVER follow YIELD follow._src AS s, follow._dst AS d | FETCH PROP ON follow $-.s -&gt; $-.d YIELD follow.degree</code> The following statement returns the <code>degree</code> values of the <code>follow</code> edges that start from vertex <code>\"player101\"</code>. <code>$var = GO FROM \"player101\" OVER follow YIELD follow._src AS s, follow._dst AS d; FETCH PROP ON follow $var.s -&gt; $var.d YIELD follow.degree</code> You can use user-defined variables to construct similar queries. </li> </ul> </li> </ul> <ul> <li> <p>SHOW</p> Statement Syntax Example Description SHOW CHARSET <code>SHOW CHARSET</code> <code>SHOW CHARSET</code> Shows the available character sets. SHOW COLLATION <code>SHOW COLLATION</code> <code>SHOW COLLATION</code> Shows the collations supported by NebulaGraph. SHOW CREATE SPACE <code>SHOW CREATE SPACE &lt;space_name&gt;</code> <code>SHOW CREATE SPACE basketballplayer</code> Shows the creating statement of the specified graph space. SHOW CREATE TAG/EDGE <code>SHOW CREATE {TAG &lt;tag_name&gt; | EDGE &lt;edge_name&gt;}</code> <code>SHOW CREATE TAG player</code> Shows the basic information of the specified tag. SHOW HOSTS <code>SHOW HOSTS [GRAPH | STORAGE | META]</code> <code>SHOW HOSTS</code><code>SHOW HOSTS GRAPH</code> Shows the host and version information of Graph Service, Storage Service, and Meta Service. SHOW INDEX STATUS <code>SHOW {TAG | EDGE} INDEX STATUS</code> <code>SHOW TAG INDEX STATUS</code> Shows the status of jobs that rebuild native indexes, which helps check whether a native index is successfully rebuilt or not. SHOW INDEXES <code>SHOW {TAG | EDGE} INDEXES</code> <code>SHOW TAG INDEXES</code> Shows the names of existing native indexes. SHOW PARTS <code>SHOW PARTS [&lt;part_id&gt;]</code> <code>SHOW PARTS</code> Shows the information of a specified partition or all partitions in a graph space. SHOW ROLES <code>SHOW ROLES IN &lt;space_name&gt;</code> <code>SHOW ROLES in basketballplayer</code> Shows the roles that are assigned to a user account. SHOW SNAPSHOTS <code>SHOW SNAPSHOTS</code> <code>SHOW SNAPSHOTS</code> Shows the information of all the snapshots. SHOW SPACES <code>SHOW SPACES</code> <code>SHOW SPACES</code> Shows existing graph spaces in NebulaGraph. SHOW STATS <code>SHOW STATS</code> <code>SHOW STATS</code> Shows the statistics of the graph space collected by the latest <code>STATS</code> job. SHOW TAGS/EDGES <code>SHOW TAGS | EDGES</code> <code>SHOW TAGS</code>,<code>SHOW EDGES</code> Shows all the tags in the current graph space. SHOW USERS <code>SHOW USERS</code> <code>SHOW USERS</code> Shows the user information. SHOW SESSIONS <code>SHOW SESSIONS</code> <code>SHOW SESSIONS</code> Shows the information of all the sessions. SHOW SESSIONS <code>SHOW SESSION &lt;Session_Id&gt;</code> <code>SHOW SESSION 1623304491050858</code> Shows a specified session with its ID. SHOW QUERIES <code>SHOW [ALL] QUERIES</code> <code>SHOW QUERIES</code> Shows the information of working queries in the current session. SHOW META LEADER <code>SHOW META LEADER</code> <code>SHOW META LEADER</code> Shows the information of the leader in the current Meta cluster. </li> </ul>"},{"location":"2.quick-start/6.cheatsheet-for-ngql/#clauses_and_options","title":"Clauses and options","text":"Clause Syntax Example Description GROUP BY <code>GROUP BY &lt;var&gt; YIELD &lt;var&gt;, &lt;aggregation_function(var)&gt;</code> <code>GO FROM \"player100\" OVER follow BIDIRECT YIELD $$.player.name as Name | GROUP BY $-.Name YIELD $-.Name as Player, count(*) AS Name_Count</code> Finds all the vertices connected directly to vertex <code>\"player100\"</code>, groups the result set by player names, and counts how many times the name shows up in the result set. LIMIT <code>YIELD &lt;var&gt; [| LIMIT [&lt;offset_value&gt;,] &lt;number_rows&gt;]</code> <code>GO FROM \"player100\" OVER follow REVERSELY YIELD $$.player.name AS Friend, $$.player.age AS Age | ORDER BY $-.Age, $-.Friend | LIMIT 1, 3</code> Returns the 3 rows of data starting from the second row of the sorted output. SKIP <code>RETURN &lt;var&gt; [SKIP &lt;offset&gt;] [LIMIT &lt;number_rows&gt;]</code> <code>MATCH (v:player{name:\"Tim Duncan\"}) --&gt; (v2) RETURN v2.player.name AS Name, v2.player.age AS Age ORDER BY Age DESC SKIP 1</code> <code>SKIP</code> can be used alone to set the offset and return the data after the specified position. SAMPLE <code>&lt;go_statement&gt; SAMPLE &lt;sample_list&gt;;</code> <code>GO 3 STEPS FROM \"player100\" OVER * YIELD properties($$).name AS NAME, properties($$).age AS Age SAMPLE [1,2,3];</code> Takes samples evenly in the result set and returns the specified amount of data. ORDER BY <code>&lt;YIELD clause&gt; ORDER BY &lt;expression&gt; [ASC | DESC] [, &lt;expression&gt; [ASC | DESC] ...]</code> <code>FETCH PROP ON player \"player100\", \"player101\", \"player102\", \"player103\" YIELD player.age AS age, player.name AS name | ORDER BY $-.age ASC, $-.name DESC</code> The <code>ORDER BY</code> clause specifies the order of the rows in the output. RETURN <code>RETURN {&lt;vertex_name&gt;|&lt;edge_name&gt;|&lt;vertex_name&gt;.&lt;property&gt;|&lt;edge_name&gt;.&lt;property&gt;|...}</code> <code>MATCH (v:player) RETURN v.player.name, v.player.age LIMIT 3</code> Returns the first three rows with values of the vertex properties <code>name</code> and <code>age</code>. TTL <code>CREATE TAG &lt;tag_name&gt;(&lt;property_name_1&gt; &lt;property_value_1&gt;, &lt;property_name_2&gt; &lt;property_value_2&gt;, ...) ttl_duration= &lt;value_int&gt;, ttl_col = &lt;property_name&gt;</code> <code>CREATE TAG t2(a int, b int, c string) ttl_duration= 100, ttl_col = \"a\"</code> Create a tag and set the TTL options. WHERE <code>WHERE {&lt;vertex|edge_alias&gt;.&lt;property_name&gt; {&gt;|==|&lt;|...} &lt;value&gt;...}</code> <code>MATCH (v:player) WHERE v.player.name == \"Tim Duncan\" XOR (v.player.age &lt; 30 AND v.player.name == \"Yao Ming\") OR NOT (v.player.name == \"Yao Ming\" OR v.player.name == \"Tim Duncan\") RETURN v.player.name, v.player.age</code> The <code>WHERE</code> clause filters the output by conditions. The <code>WHERE</code> clause usually works in Native nGQL <code>GO</code> and <code>LOOKUP</code> statements, and OpenCypher <code>MATCH</code> and <code>WITH</code> statements. YIELD <code>YIELD [DISTINCT] &lt;col&gt; [AS &lt;alias&gt;] [, &lt;col&gt; [AS &lt;alias&gt;] ...] [WHERE &lt;conditions&gt;];</code> <code>GO FROM \"player100\" OVER follow YIELD dst(edge) AS ID | FETCH PROP ON player $-.ID YIELD player.age AS Age | YIELD AVG($-.Age) as Avg_age, count(*)as Num_friends</code> Finds the players that \"player100\" follows and calculates their average age. WITH <code>MATCH $expressions WITH {nodes()|labels()|...}</code> <code>MATCH p=(v:player{name:\"Tim Duncan\"})--() WITH nodes(p) AS n UNWIND n AS n1 RETURN DISTINCT n1</code> The <code>WITH</code> clause can retrieve the output from a query part, process it, and pass it to the next query part as the input. UNWIND <code>UNWIND &lt;list&gt; AS &lt;alias&gt; &lt;RETURN clause&gt;</code> <code>UNWIND [1,2,3] AS n RETURN n</code> Splits a list into rows."},{"location":"2.quick-start/6.cheatsheet-for-ngql/#space_statements","title":"Space statements","text":"Statement Syntax Example Description CREATE SPACE <code>CREATE SPACE [IF NOT EXISTS] &lt;graph_space_name&gt; ( [partition_num = &lt;partition_number&gt;,] [replica_factor = &lt;replica_number&gt;,] vid_type = {FIXED_STRING(&lt;N&gt;) | INT[64]} ) [COMMENT = '&lt;comment&gt;']</code> <code>CREATE SPACE my_space_1 (vid_type=FIXED_STRING(30))</code> Creates a graph space with CREATE SPACE <code>CREATE SPACE &lt;new_graph_space_name&gt; AS &lt;old_graph_space_name&gt;</code> <code>CREATE SPACE my_space_4 as my_space_3</code> Clone a graph. space. USE <code>USE &lt;graph_space_name&gt;</code> <code>USE space1</code> Specifies a graph space as the current working graph space for subsequent queries. SHOW SPACES <code>SHOW SPACES</code> <code>SHOW SPACES</code> Lists all the graph spaces in the NebulaGraph examples. DESCRIBE SPACE <code>DESC[RIBE] SPACE &lt;graph_space_name&gt;</code> <code>DESCRIBE SPACE basketballplayer</code> Returns the information about the specified graph space. CLEAR SPACE <code>CLEAR SPACE [IF EXISTS] &lt;graph_space_name&gt;</code> Deletes the vertices and edges in a graph space, but does not delete the graph space itself and the schema information. DROP SPACE <code>DROP SPACE [IF EXISTS] &lt;graph_space_name&gt;</code> <code>DROP SPACE basketballplayer</code> Deletes everything in the specified graph space."},{"location":"2.quick-start/6.cheatsheet-for-ngql/#tag_statements","title":"TAG statements","text":"Statement Syntax Example Description CREATE TAG <code>CREATE TAG [IF NOT EXISTS] &lt;tag_name&gt; ( &lt;prop_name&gt; &lt;data_type&gt; [NULL | NOT NULL] [DEFAULT &lt;default_value&gt;] [COMMENT '&lt;comment&gt;'] [{, &lt;prop_name&gt; &lt;data_type&gt; [NULL |  NOT NULL] [DEFAULT &lt;default_value&gt;] [COMMENT '&lt;comment&gt;']} ...] ) [TTL_DURATION = &lt;ttl_duration&gt;] [TTL_COL = &lt;prop_name&gt;] [COMMENT = '&lt;comment&gt;']</code> <code>CREATE TAG woman(name string, age int, married bool, salary double, create_time timestamp) TTL_DURATION = 100, TTL_COL = \"create_time\"</code> Creates a tag with the given name in a graph space. DROP TAG <code>DROP TAG [IF EXISTS] &lt;tag_name&gt;</code> <code>DROP TAG test;</code> Drops a tag with the given name in the current working graph space. ALTER TAG <code>ALTER TAG &lt;tag_name&gt;    &lt;alter_definition&gt; [, alter_definition] ...]    [ttl_definition [, ttl_definition] ... ]    [COMMENT = '&lt;comment&gt;']</code> <code>ALTER TAG t1 ADD (p3 int, p4 string)</code> Alters the structure of a tag with the given name in a graph space. You can add or drop properties, and change the data type of an existing property. You can also set a TTL (Time-To-Live) on a property, or change its TTL duration. SHOW TAGS <code>SHOW TAGS</code> <code>SHOW TAGS</code> Shows the name of all tags in the current graph space. DESCRIBE TAG <code>DESC[RIBE] TAG &lt;tag_name&gt;</code> <code>DESCRIBE TAG player</code> Returns the information about a tag with the given name in a graph space, such as field names, data type, and so on. DELETE TAG <code>DELETE TAG &lt;tag_name_list&gt; FROM &lt;VID&gt;</code> <code>DELETE TAG test1 FROM \"test\"</code> Deletes a tag with the given name on a specified vertex."},{"location":"2.quick-start/6.cheatsheet-for-ngql/#edge_type_statements","title":"Edge type statements","text":"Statement Syntax Example Description CREATE EDGE <code>CREATE EDGE [IF NOT EXISTS] &lt;edge_type_name&gt;    ( &lt;prop_name&gt; &lt;data_type&gt; [NULL | NOT NULL] [DEFAULT &lt;default_value&gt;] [COMMENT '&lt;comment&gt;'] [{, &lt;prop_name&gt; &lt;data_type&gt; [NULL | NOT NULL] [DEFAULT &lt;default_value&gt;] [COMMENT '&lt;comment&gt;']} ...] ) [TTL_DURATION = &lt;ttl_duration&gt;] [TTL_COL = &lt;prop_name&gt;] [COMMENT = '&lt;comment&gt;']</code> <code>CREATE EDGE e1(p1 string, p2 int, p3 timestamp) TTL_DURATION = 100, TTL_COL = \"p2\"</code> Creates an edge type with the given name in a graph space. DROP EDGE <code>DROP EDGE [IF EXISTS] &lt;edge_type_name&gt;</code> <code>DROP EDGE e1</code> Drops an edge type with the given name in a graph space. ALTER EDGE <code>ALTER EDGE &lt;edge_type_name&gt;    &lt;alter_definition&gt; [, alter_definition] ...]    [ttl_definition [, ttl_definition] ... ]    [COMMENT = '&lt;comment&gt;']</code> <code>ALTER EDGE e1 ADD (p3 int, p4 string)</code> Alters the structure of an edge type with the given name in a graph space. SHOW EDGES <code>SHOW EDGES</code> <code>SHOW EDGES</code> Shows all edge types in the current graph space. DESCRIBE EDGE <code>DESC[RIBE] EDGE &lt;edge_type_name&gt;</code> <code>DESCRIBE EDGE follow</code> Returns the information about an edge type with the given name in a graph space, such as field names, data type, and so on."},{"location":"2.quick-start/6.cheatsheet-for-ngql/#vertex_statements","title":"Vertex statements","text":"Statement Syntax Example Description INSERT VERTEX <code>INSERT VERTEX [IF NOT EXISTS] [tag_props, [tag_props] ...] VALUES &lt;vid&gt;: ([prop_value_list])</code> <code>INSERT VERTEX t2 (name, age) VALUES \"13\":(\"n3\", 12), \"14\":(\"n4\", 8)</code> Inserts one or more vertices into a graph space in NebulaGraph. DELETE VERTEX <code>DELETE VERTEX &lt;vid&gt; [, &lt;vid&gt; ...]</code> <code>DELETE VERTEX \"team1\"</code> Deletes vertices and the related incoming and outgoing edges of the vertices. UPDATE VERTEX <code>UPDATE VERTEX ON &lt;tag_name&gt; &lt;vid&gt; SET &lt;update_prop&gt; [WHEN &lt;condition&gt;] [YIELD &lt;output&gt;]</code> <code>UPDATE VERTEX ON player \"player101\" SET age = age + 2</code> Updates properties on tags of a vertex. UPSERT VERTEX <code>UPSERT VERTEX ON &lt;tag&gt; &lt;vid&gt; SET &lt;update_prop&gt; [WHEN &lt;condition&gt;] [YIELD &lt;output&gt;]</code> <code>UPSERT VERTEX ON player \"player667\" SET age = 31</code> The <code>UPSERT</code> statement is a combination of <code>UPDATE</code> and <code>INSERT</code>. You can use <code>UPSERT VERTEX</code> to update the properties of a vertex if it exists or insert a new vertex if it does not exist."},{"location":"2.quick-start/6.cheatsheet-for-ngql/#edge_statements","title":"Edge statements","text":"Statement Syntax Example Description INSERT EDGE <code>INSERT EDGE [IF NOT EXISTS] &lt;edge_type&gt; ( &lt;prop_name_list&gt; ) VALUES &lt;src_vid&gt; -&gt; &lt;dst_vid&gt;[@&lt;rank&gt;] : ( &lt;prop_value_list&gt; ) [, &lt;src_vid&gt; -&gt; &lt;dst_vid&gt;[@&lt;rank&gt;] : ( &lt;prop_value_list&gt; ), ...]</code> <code>INSERT EDGE e2 (name, age) VALUES \"11\"-&gt;\"13\":(\"n1\", 1)</code> Inserts an edge or multiple edges into a graph space from a source vertex (given by src_vid) to a destination vertex (given by dst_vid) with a specific rank in NebulaGraph. DELETE EDGE <code>DELETE EDGE &lt;edge_type&gt; &lt;src_vid&gt; -&gt; &lt;dst_vid&gt;[@&lt;rank&gt;] [, &lt;src_vid&gt; -&gt; &lt;dst_vid&gt;[@&lt;rank&gt;] ...]</code> <code>DELETE EDGE serve \"player100\" -&gt; \"team204\"@0</code> Deletes one edge or multiple edges at a time. UPDATE EDGE <code>UPDATE EDGE ON &lt;edge_type&gt; &lt;src_vid&gt; -&gt; &lt;dst_vid&gt; [@&lt;rank&gt;] SET &lt;update_prop&gt; [WHEN &lt;condition&gt;] [YIELD &lt;output&gt;]</code> <code>UPDATE EDGE ON serve \"player100\" -&gt; \"team204\"@0 SET start_year = start_year + 1</code> Updates properties on an edge. UPSERT EDGE <code>UPSERT EDGE ON &lt;edge_type&gt; &lt;src_vid&gt; -&gt; &lt;dst_vid&gt; [@rank] SET &lt;update_prop&gt; [WHEN &lt;condition&gt;] [YIELD &lt;properties&gt;]</code> <code>UPSERT EDGE on serve \"player666\" -&gt; \"team200\"@0 SET end_year = 2021</code> The <code>UPSERT</code> statement is a combination of <code>UPDATE</code> and <code>INSERT</code>. You can use <code>UPSERT EDGE</code> to update the properties of an edge if it exists or insert a new edge if it does not exist."},{"location":"2.quick-start/6.cheatsheet-for-ngql/#index","title":"Index","text":"<ul> <li> <p>Native index</p> <p>You can use native indexes together with <code>LOOKUP</code> and <code>MATCH</code> statements.</p> Statement Syntax Example Description CREATE INDEX <code>CREATE {TAG | EDGE} INDEX [IF NOT EXISTS] &lt;index_name&gt; ON {&lt;tag_name&gt; | &lt;edge_name&gt;} ([&lt;prop_name_list&gt;]) [COMMENT = '&lt;comment&gt;']</code> <code>CREATE TAG INDEX player_index on player()</code> Add native indexes for the existing tags, edge types, or properties. SHOW CREATE INDEX <code>SHOW CREATE {TAG | EDGE} INDEX &lt;index_name&gt;</code> <code>show create tag index index_2</code> Shows the statement used when creating a tag or an edge type. It contains detailed information about the index, such as its associated properties. SHOW INDEXES <code>SHOW {TAG | EDGE} INDEXES</code> <code>SHOW TAG INDEXES</code> Shows the defined tag or edge type indexes names in the current graph space. DESCRIBE INDEX <code>DESCRIBE {TAG | EDGE} INDEX &lt;index_name&gt;</code> <code>DESCRIBE TAG INDEX player_index_0</code> Gets the information about the index with a given name, including the property name (Field) and the property type (Type) of the index. REBUILD INDEX <code>REBUILD {TAG | EDGE} INDEX [&lt;index_name_list&gt;]</code> <code>REBUILD TAG INDEX single_person_index</code> Rebuilds the created tag or edge type index. If data is updated or inserted before the creation of the index, you must rebuild the indexes manually to make sure that the indexes contain the previously added data. SHOW INDEX STATUS <code>SHOW {TAG | EDGE} INDEX STATUS</code> <code>SHOW TAG INDEX STATUS</code> Returns the name of the created tag or edge type index and its status. DROP INDEX <code>DROP {TAG | EDGE} INDEX [IF EXISTS] &lt;index_name&gt;</code> <code>DROP TAG INDEX player_index_0</code> Removes an existing index from the current graph space. </li> </ul> <ul> <li> <p>Full-text index</p> Syntax Example Description <code>SIGN IN TEXT SERVICE [(&lt;elastic_ip:port&gt; [,&lt;username&gt;, &lt;password&gt;]), (&lt;elastic_ip:port&gt;), ...]</code> <code>SIGN IN TEXT SERVICE (127.0.0.1:9200)</code> The full-text indexes is implemented based on Elasticsearch. After deploying an Elasticsearch cluster, you can use the <code>SIGN IN</code> statement to log in to the Elasticsearch client. <code>SHOW TEXT SEARCH CLIENTS</code> <code>SHOW TEXT SEARCH CLIENTS</code> Shows text search clients. <code>SIGN OUT TEXT SERVICE</code> <code>SIGN OUT TEXT SERVICE</code> Signs out to the text search clients. <code>CREATE FULLTEXT {TAG | EDGE} INDEX &lt;index_name&gt; ON {&lt;tag_name&gt; |  &lt;edge_name&gt;} ([&lt;prop_name_list&gt;])</code> <code>CREATE FULLTEXT TAG INDEX nebula_index_1 ON player(name)</code> Creates full-text indexes. <code>SHOW FULLTEXT INDEXES</code> <code>SHOW FULLTEXT INDEXES</code> Show full-text indexes. <code>REBUILD FULLTEXT INDEX</code> <code>REBUILD FULLTEXT INDEX</code> Rebuild full-text indexes. <code>DROP FULLTEXT INDEX &lt;index_name&gt;</code> <code>DROP FULLTEXT INDEX nebula_index_1</code> Drop full-text indexes. <code>LOOKUP ON {&lt;tag&gt; | &lt;edge_type&gt;} WHERE &lt;expression&gt; [YIELD &lt;return_list&gt;]</code> <code>LOOKUP ON player WHERE FUZZY(player.name, \"Tim Dunncan\", AUTO, OR) YIELD player.name</code> Use query options. </li> </ul>"},{"location":"2.quick-start/6.cheatsheet-for-ngql/#subgraph_and_path_statements","title":"Subgraph and path statements","text":"Type Syntax Example Description GET SUBGRAPH <code>GET SUBGRAPH [WITH PROP] [&lt;step_count&gt; {STEP|STEPS}] FROM {&lt;vid&gt;, &lt;vid&gt;...} [{IN | OUT | BOTH} &lt;edge_type&gt;, &lt;edge_type&gt;...] YIELD [VERTICES AS &lt;vertex_alias&gt;] [,EDGES AS &lt;edge_alias&gt;]</code> <code>GET SUBGRAPH 1 STEPS FROM \"player100\" YIELD VERTICES AS nodes, EDGES AS relationships</code> Retrieves information of vertices and edges reachable from the source vertices of the specified edge types and returns information of the subgraph. FIND PATH <code>FIND { SHORTEST | ALL | NOLOOP } PATH [WITH PROP] FROM &lt;vertex_id_list&gt; TO &lt;vertex_id_list&gt; OVER &lt;edge_type_list&gt; [REVERSELY | BIDIRECT] [&lt;WHERE clause&gt;] [UPTO &lt;N&gt; {STEP|STEPS}] YIELD path as &lt;alias&gt; [| ORDER BY $-.path] [| LIMIT &lt;M&gt;]</code> <code>FIND SHORTEST PATH FROM \"player102\" TO \"team204\" OVER * YIELD path as p</code> Finds the paths between the selected source vertices and destination vertices. A returned path is like <code>(&lt;vertex_id&gt;)-[:&lt;edge_type_name&gt;@&lt;rank&gt;]-&gt;(&lt;vertex_id)</code>."},{"location":"2.quick-start/6.cheatsheet-for-ngql/#query_tuning_statements","title":"Query tuning statements","text":"Type Syntax Example Description EXPLAIN <code>EXPLAIN [format=\"row\" | \"dot\"] &lt;your_nGQL_statement&gt;</code> <code>EXPLAIN format=\"row\" SHOW TAGS</code><code>EXPLAIN format=\"dot\" SHOW TAGS</code> Helps output the execution plan of an nGQL statement without executing the statement. PROFILE <code>PROFILE [format=\"row\" | \"dot\"] &lt;your_nGQL_statement&gt;</code> <code>PROFILE format=\"row\" SHOW TAGS</code><code>EXPLAIN format=\"dot\" SHOW TAGS</code> Executes the statement, then outputs the execution plan as well as the execution profile."},{"location":"2.quick-start/6.cheatsheet-for-ngql/#operation_and_maintenance_statements","title":"Operation and maintenance statements","text":"<ul> <li> <p>SUBMIT JOB BALANCE</p> Syntax Description <code>BALANCE LEADER</code> Starts a job to balance the distribution of all the storage leaders in graph spaces. It returns the job ID. </li> </ul> <ul> <li> <p>Job statements</p> Syntax Description <code>SUBMIT JOB COMPACT</code> Triggers the long-term RocksDB <code>compact</code> operation. <code>SUBMIT JOB FLUSH</code> Writes the RocksDB memfile in the memory to the hard disk. <code>SUBMIT JOB STATS</code> Starts a job that makes the statistics of the current graph space. Once this job succeeds, you can use the <code>SHOW STATS</code> statement to list the statistics. <code>SHOW JOB &lt;job_id&gt;</code> Shows the information about a specific job and all its tasks in the current graph space. The Meta Service parses a <code>SUBMIT JOB</code> request into multiple tasks and assigns them to the nebula-storaged processes. <code>SHOW JOBS</code> Lists all the unexpired jobs in the current graph space. <code>STOP JOB</code> Stops jobs that are not finished in the current graph space. <code>RECOVER JOB</code> Re-executes the failed jobs in the current graph space and returns the number of recovered jobs. </li> </ul> <ul> <li> <p>Kill queries</p> Syntax Example Description <code>KILL QUERY (session=&lt;session_id&gt;, plan=&lt;plan_id&gt;)</code> <code>KILL QUERY(SESSION=1625553545984255,PLAN=163)</code> Terminates the query being executed, and is often used to terminate slow queries. </li> </ul>"},{"location":"20.appendix/0.FAQ/","title":"FAQ","text":"<p>This topic lists the frequently asked questions for using NebulaGraph 3.5.0. You can use the search box in the help center or the search function of the browser to match the questions you are looking for.</p> <p>If the solutions described in this topic cannot solve your problems, ask for help on the NebulaGraph forum or submit an issue on GitHub issue.</p>"},{"location":"20.appendix/0.FAQ/#about_manual_updates","title":"About manual updates","text":""},{"location":"20.appendix/0.FAQ/#why_is_the_behavior_in_the_manual_not_consistent_with_the_system","title":"\"Why is the behavior in the manual not consistent with the system?\"","text":"<p>NebulaGraph is still under development. Its behavior changes from time to time. Users can submit an issue to inform the team if the manual and the system are not consistent.</p> <p>Note</p> <p>If you find some errors in this topic:</p> <ol> <li>Click the <code>pencil</code> button at the top right side of this page.</li> <li>Use markdown to fix this error. Then click \"Commit changes\" at the bottom, which will start a Github pull request.</li> <li>Sign the CLA. This pull request will be merged after the acceptance of at least two reviewers.</li> </ol>"},{"location":"20.appendix/0.FAQ/#about_legacy_version_compatibility","title":"About legacy version compatibility","text":"<p><code>X</code> version compatibility</p> <p>Neubla Graph 3.5.0 is not compatible with NebulaGraph 1.x nor 2.0-RC in both data formats and RPC-protocols, and vice versa. The service process may quit if using an lower version client to connect to a higher version server.</p> <p>To upgrade data formats, see Upgrade NebulaGraph to the current version. Users must upgrade all clients.</p>"},{"location":"20.appendix/0.FAQ/#about_execution_errors","title":"About execution errors","text":""},{"location":"20.appendix/0.FAQ/#how_to_resolve_the_error_-1005graphmemoryexceeded_-2600","title":"\"How to resolve the error <code>-1005:GraphMemoryExceeded: (-2600)</code>?\"","text":"<p>This error is issued by the Memory Tracker when it observes that memory usage has exceeded a set threshold. This mechanism can help avoid service processes from being terminated by the system's OOM (Out of Memory) killer. Steps to resolve:</p> <ol> <li> <p>Check memory usage: First, you need to check the memory usage during the execution of the command. If the memory usage is indeed high, then this error might be expected.</p> </li> <li> <p>Check the configuration of the Memory Tracker: If the memory usage is not high, check the relevant configurations of the Memory Tracker. These include <code>memory_tracker_untracked_reserved_memory_mb</code> (untracked reserved memory in MB), <code>memory_tracker_limit_ratio</code> (memory limit ratio), and <code>memory_purge_enabled</code> (whether memory purge is enabled). For the configuration of the Memory Tracker, see memory tracker configuration.</p> </li> <li> <p>Optimize configurations: Adjust these configurations according to the actual situation. For example, if the available memory limit is too low, you can increase the value of <code>memory_tracker_limit_ratio</code>.</p> </li> </ol>"},{"location":"20.appendix/0.FAQ/#how_to_resolve_the_error_semanticerror_missing_yield_clause","title":"\"How to resolve the error <code>SemanticError: Missing yield clause.</code>?\"","text":"<p>Starting with NebulaGraph 3.0.0, the statements <code>LOOKUP</code>, <code>GO</code>, and <code>FETCH</code> must output results with the <code>YIELD</code> clause. For more information, see YIELD.</p>"},{"location":"20.appendix/0.FAQ/#how_to_resolve_the_error_host_not_enough","title":"\"How to resolve the error <code>Host not enough!</code>?\"","text":"<p>From NebulaGraph version 3.0.0, the Storage services added in the configuration files CANNOT be read or written directly. The configuration files only register the Storage services into the Meta services. You must run the <code>ADD HOSTS</code> command to read and write data on Storage servers. For more information, see Manage Storage hosts.</p>"},{"location":"20.appendix/0.FAQ/#how_to_resolve_the_error_to_get_the_property_of_the_vertex_in_vage_should_use_the_format_vartagprop","title":"\"How to resolve the error <code>To get the property of the vertex in 'v.age', should use the format 'var.tag.prop'</code>?\"","text":"<p>From NebulaGraph version 3.0.0, patterns support matching multiple tags at the same time, so you need to specify a tag name when querying properties. The original statement <code>RETURN variable_name.property_name</code> is changed to <code>RETURN variable_name.&lt;tag_name&gt;.property_name</code>.</p>"},{"location":"20.appendix/0.FAQ/#how_to_resolve_the_error_storage_error_e_rpc_failure","title":"\"How to resolve the error <code>Storage Error E_RPC_FAILURE</code>?\"","text":"<p>The reason for this error is usually that the storaged process returns too many data back to the graphd process. Possible solutions are as follows:</p> <ul> <li>Modify configuration files: Modify the value of <code>--storage_client_timeout_ms</code> in the <code>nebula-graphd.conf</code> file to extend the connection timeout of the Storage client. This configuration is measured in milliseconds (ms). For example, set <code>--storage_client_timeout_ms=60000</code>. If this parameter is not specified in the <code>nebula-graphd.conf</code> file, specify it manually. Tip: Add <code>--local_config=true</code> at the beginning of the configuration file and restart the service.</li> <li>Optimize the query statement: Reduce queries that scan the entire database. No matter whether <code>LIMIT</code> is used to limit the number of returned results, use the <code>GO</code> statement to rewrite the <code>MATCH</code> statement (the former is optimized, while the latter is not).</li> <li>Check whether the Storaged process has OOM. (<code>dmesg |grep nebula</code>).</li> <li>Use better SSD or memory for the Storage Server.</li> <li>Retry.</li> </ul>"},{"location":"20.appendix/0.FAQ/#how_to_resolve_the_error_the_leader_has_changed_try_again_later","title":"\"How to resolve the error <code>The leader has changed. Try again later</code>?\"","text":"<p>It is a known issue. Just retry 1 to N times, where N is the partition number. The reason is that the meta client needs some heartbeats to update or errors to trigger the new leader information.</p> <p>If this error occurs when logging in to NebulaGraph, you can consider using <code>df -h</code> to view the disk space and check whether the local disk is full.</p>"},{"location":"20.appendix/0.FAQ/#unable_to_download_snapshot_packages_when_compiling_exchange_connectors_or_algorithm","title":"Unable to download SNAPSHOT packages when compiling Exchange, Connectors, or Algorithm","text":"<p>Problem description: The system reports <code>Could not find artifact com.vesoft:client:jar:xxx-SNAPSHOT</code> when compiling.</p> <p>Cause: There is no local Maven repository for storing or downloading SNAPSHOT packages. The default central repository in Maven only stores official releases, not development versions (SNAPSHOTs).</p> <p>Solution: Add the following configuration in the <code>profiles</code> scope of Maven's <code>setting.xml</code> file:</p> <pre><code>  &lt;profile&gt;\n     &lt;activation&gt;\n        &lt;activeByDefault&gt;true&lt;/activeByDefault&gt;\n     &lt;/activation&gt;\n     &lt;repositories&gt;\n        &lt;repository&gt;\n            &lt;id&gt;snapshots&lt;/id&gt;\n            &lt;url&gt;https://oss.sonatype.org/content/repositories/snapshots/&lt;/url&gt;\n            &lt;snapshots&gt;\n               &lt;enabled&gt;true&lt;/enabled&gt;\n            &lt;/snapshots&gt;\n      &lt;/repository&gt;\n     &lt;/repositories&gt;\n  &lt;/profile&gt;\n</code></pre>"},{"location":"20.appendix/0.FAQ/#how_to_resolve_error_-1004_syntaxerror_syntax_error_near","title":"\"How to resolve <code>[ERROR (-1004)]: SyntaxError: syntax error near</code>?\"","text":"<p>In most cases, a query statement requires a <code>YIELD</code> or a <code>RETURN</code>. Check your query statement to see if <code>YIELD</code> or <code>RETURN</code> is provided.</p>"},{"location":"20.appendix/0.FAQ/#how_to_resolve_the_error_cant_solve_the_start_vids_from_the_sentence","title":"\"How to resolve the error <code>can\u2019t solve the start vids from the sentence</code>?\"","text":"<p>The graphd process requires <code>start vids</code> to begin a graph traversal. The <code>start vids</code> can be specified by the user. For example:</p> <pre><code>&gt; GO FROM ${vids} ...\n&gt; MATCH (src) WHERE id(src) == ${vids}\n# The \"start vids\" are explicitly given by ${vids}.\n</code></pre> <p>It can also be found from a property index. For example:</p> <pre><code># CREATE TAG INDEX IF NOT EXISTS i_player ON player(name(20));\n# REBUILD TAG INDEX i_player;\n\n&gt; LOOKUP ON player WHERE player.name == \"abc\" | ... YIELD ...\n&gt; MATCH (src) WHERE src.name == \"abc\" ...\n# The \"start vids\" are found from the property index \"name\".\n</code></pre> <p>Otherwise, an error like <code>can\u2019t solve the start vids from the sentence</code> will be returned.</p>"},{"location":"20.appendix/0.FAQ/#how_to_resolve_the_error_wrong_vertex_id_type_1001","title":"\"How to resolve the error <code>Wrong vertex id type: 1001</code>?\"","text":"<p>Check whether the VID is <code>INT64</code> or <code>FIXED_STRING(N)</code> set by <code>create space</code>. For more information, see create space.</p>"},{"location":"20.appendix/0.FAQ/#how_to_resolve_the_error_the_vid_must_be_a_64-bit_integer_or_a_string_fitting_space_vertex_id_length_limit","title":"\"How to resolve the error <code>The VID must be a 64-bit integer or a string fitting space vertex id length limit.</code>?\"","text":"<p>Check whether the length of the VID exceeds the limitation. For more information, see create space.</p>"},{"location":"20.appendix/0.FAQ/#how_to_resolve_the_error_edge_conflict_or_vertex_conflict","title":"\"How to resolve the error <code>edge conflict</code> or <code>vertex conflict</code>?\"","text":"<p>NebulaGraph may return such errors when the Storage service receives multiple requests to insert or update the same vertex or edge within milliseconds. Try the failed requests again later.</p>"},{"location":"20.appendix/0.FAQ/#how_to_resolve_the_error_rpc_failure_in_metaclient_connection_refused","title":"\"How to resolve the error <code>RPC failure in MetaClient: Connection refused</code>?\"","text":"<p>The reason for this error is usually that the metad service status is unusual, or the network of the machine where the metad and graphd services are located is disconnected. Possible solutions are as follows:</p> <ul> <li>Check the metad service status on the server where the metad is located. If the service status is unusual, restart the metad service.</li> </ul> <ul> <li>Use <code>telnet meta-ip:port</code> to check the network status under the server that returns an error.</li> </ul> <ul> <li>Check the port information in the configuration file. If the port is different from the one used when connecting, use the port in the configuration file or modify the configuration.</li> </ul>"},{"location":"20.appendix/0.FAQ/#how_to_resolve_the_error_storageclientbaseinl214_request_to_xxxx9779_failed_n6apache6thrift9transport19ttransportexceptione_timed_out_in_nebula-graphinfo","title":"\"How to resolve the error <code>StorageClientBase.inl:214] Request to \"x.x.x.x\":9779 failed: N6apache6thrift9transport19TTransportExceptionE: Timed Out</code> in <code>nebula-graph.INFO</code>?\"","text":"<p>The reason for this error may be that the amount of data to be queried is too large, and the storaged process has timed out. Possible solutions are as follows:</p> <ul> <li>When importing data, set Compaction manually to make read faster.</li> </ul> <ul> <li>Extend the RPC connection timeout of the Graph service and the Storage service. Modify the value of <code>--storage_client_timeout_ms</code> in the <code>nebula-graphd.conf</code> file. This configuration is measured in milliseconds (ms). The default value is 60000ms.</li> </ul>"},{"location":"20.appendix/0.FAQ/#how_to_resolve_the_error_metaclientcpp65_heartbeat_failed_statuswrong_cluster_in_nebula-storagedinfo_or_hbprocessorcpp54_reject_wrong_cluster_host_xxxx9771_in_nebula-metadinfo","title":"\"How to resolve the error <code>MetaClient.cpp:65] Heartbeat failed, status:Wrong cluster!</code> in <code>nebula-storaged.INFO</code>, or <code>HBProcessor.cpp:54] Reject wrong cluster host \"x.x.x.x\":9771!</code> in <code>nebula-metad.INFO</code>?\"","text":"<p>The reason for this error may be that the user has modified the IP or the port information of the metad process, or the storage service has joined other clusters before. Possible solutions are as follows:</p> <p>Delete the <code>cluster.id</code> file in the installation directory where the storage machine is deployed (the default installation directory is <code>/usr/local/nebula</code>), and restart the storaged service.</p>"},{"location":"20.appendix/0.FAQ/#how_to_resolve_the_error_storage_error_more_than_one_request_trying_to_addupdatedelete_one_edgevertex_at_he_same_time","title":"\"How to resolve the error <code>Storage Error: More than one request trying to add/update/delete one edge/vertex at he same time.</code>?\"","text":"<p>The reason for this error is that the current NebulaGraph version does not support concurrent requests to the same vertex or edge at the same time. To solve this error, re-execute your commands.</p>"},{"location":"20.appendix/0.FAQ/#about_design_and_functions","title":"About design and functions","text":""},{"location":"20.appendix/0.FAQ/#how_is_the_time_spent_value_at_the_end_of_each_return_message_calculated","title":"\"How is the <code>time spent</code> value at the end of each return message calculated?\"","text":"<p>Take the returned message of <code>SHOW SPACES</code> as an example:</p> <pre><code>nebula&gt; SHOW SPACES;\n+--------------------+\n| Name               |\n+--------------------+\n| \"basketballplayer\" |\n+--------------------+\nGot 1 rows (time spent 1235/1934 us)\n</code></pre> <ul> <li>The first number <code>1235</code> shows the time spent by the database itself, that is, the time it takes for the query engine to receive a query from the client, fetch the data from the storage server, and perform a series of calculations.</li> </ul> <ul> <li>The second number <code>1934</code> shows the time spent from the client's perspective, that is, the time it takes for the client from sending a request, receiving a response, and displaying the result on the screen.</li> </ul>"},{"location":"20.appendix/0.FAQ/#why_does_the_port_number_of_the_nebula-storaged_process_keep_showing_red_after_connecting_to_nebulagraph","title":"\"Why does the port number of the <code>nebula-storaged</code> process keep showing red after connecting to NebulaGraph?\"","text":"<p>Because the <code>nebula-storaged</code> process waits for <code>nebula-metad</code> to add the current Storage service during the startup process. The Storage works after it receives the ready signal. Starting from NebulaGraph 3.0.0, the Meta service cannot directly read or write data in the Storage service that you add in the configuration file. The configuration file only registers the Storage service to the Meta service. You must run the <code>ADD HOSTS</code> command to enable the Meta to read and write data in the Storage service. For more information, see Manage Storage hosts.</p>"},{"location":"20.appendix/0.FAQ/#why_is_there_no_line_separating_each_row_in_the_returned_result_of_nebulagraph_260","title":"\"Why is there no line separating each row in the returned result of NebulaGraph 2.6.0?\"","text":"<p>This is caused by the release of NebulaGraph Console 2.6.0, not the change of NebulaGraph core. And it will not affect the content of the returned data itself.</p>"},{"location":"20.appendix/0.FAQ/#about_dangling_edges","title":"About dangling edges","text":"<p>A dangling edge is an edge that only connects to a single vertex and only one part of the edge connects to the vertex.</p> <p>Dangling edges may appear in NebulaGraph 3.5.0 as the design. And there is no <code>MERGE</code> statements of openCypher. The guarantee for dangling edges depends entirely on the application level. For more information, see INSERT VERTEX, DELETE VERTEX, INSERT EDGE, DELETE EDGE.</p>"},{"location":"20.appendix/0.FAQ/#can_i_set_replica_factor_as_an_even_number_in_create_space_statements_eg_replica_factor_2","title":"\"Can I set <code>replica_factor</code> as an even number in <code>CREATE SPACE</code> statements, e.g., <code>replica_factor = 2</code>?\"","text":"<p>NO.</p> <p>The Storage service guarantees its availability based on the Raft consensus protocol. The number of failed replicas must not exceed half of the total replica number.</p> <p>When the number of machines is 1, <code>replica_factor</code> can only be set to<code>1</code>.</p> <p>When there are enough machines and <code>replica_factor=2</code>, if one replica fails, the Storage service fails. No matter <code>replica_factor=3</code> or <code>replica_factor=4</code>, if more than one replica fails, the Storage Service fails. To prevent unnecessary waste of resources, we recommend that you set an odd replica number.</p> <p>We suggest that you set <code>replica_factor=3</code> for a production environment and <code>replica_factor=1</code> for a test environment. Do not use an even number.</p>"},{"location":"20.appendix/0.FAQ/#is_stopping_or_killing_slow_queries_supported","title":"\"Is stopping or killing slow queries supported?\"","text":"<p>Yes. For more information, see Kill query.</p>"},{"location":"20.appendix/0.FAQ/#why_are_the_query_results_different_when_using_go_and_match_to_execute_the_same_semantic_query","title":"\"Why are the query results different when using <code>GO</code> and <code>MATCH</code> to execute the same semantic query?\"","text":"<p>The possible reasons are listed as follows.</p> <ul> <li><code>GO</code> statements find the dangling edges.</li> </ul> <ul> <li><code>RETURN</code> commands do not specify the sequence.</li> </ul> <ul> <li>The dense vertex truncation limitation defined by <code>max_edge_returned_per_vertex</code> in the Storage service is triggered.</li> </ul> <ul> <li> <p>Using different types of paths may cause different query results.</p> <ul> <li><code>GO</code> statements use <code>walk</code>. Both vertices and edges can be repeatedly visited in graph traversal.</li> </ul> <ul> <li><code>MATCH</code> statements are compatible with openCypher and use <code>trail</code>. Only vertices can be repeatedly visited in graph traversal.</li> </ul> </li> </ul> <p>The example is as follows.</p> <p></p> <p>All queries that start from <code>A</code> with 5 hops will end at <code>C</code> (<code>A-&gt;B-&gt;C-&gt;D-&gt;E-&gt;C</code>). If it is 6 hops, the <code>GO</code> statement will end at <code>D</code> (<code>A-&gt;B-&gt;C-&gt;D-&gt;E-&gt;C-&gt;D</code>), because the edge <code>C-&gt;D</code> can be visited repeatedly. However, the <code>MATCH</code> statement returns empty, because edges cannot be visited repeatedly.</p> <p>Therefore, using <code>GO</code> and <code>MATCH</code> to execute the same semantic query may cause different query results.</p> <p>For more information, see Wikipedia.</p>"},{"location":"20.appendix/0.FAQ/#how_to_count_the_verticesedges_number_of_each_tagedge_type","title":"\"How to count the vertices/edges number of each tag/edge type?\"","text":"<p>See show-stats.</p>"},{"location":"20.appendix/0.FAQ/#how_to_get_all_the_verticesedge_of_each_tagedge_type","title":"\"How to get all the vertices/edge of each tag/edge type?\"","text":"<ol> <li> <p>Create and rebuild the index.</p> <pre><code>&gt; CREATE TAG INDEX IF NOT EXISTS i_player ON player();\n&gt; REBUILD TAG INDEX IF NOT EXISTS i_player;\n</code></pre> </li> <li> <p>Use <code>LOOKUP</code> or <code>MATCH</code>. For example:</p> <pre><code>&gt; LOOKUP ON player;\n&gt; MATCH (n:player) RETURN n;\n</code></pre> </li> </ol> <p>For more information, see <code>INDEX</code>, <code>LOOKUP</code>, and <code>MATCH</code>.</p>"},{"location":"20.appendix/0.FAQ/#can_non-english_characters_be_used_as_identifiers_such_as_the_names_of_graph_spaces_tags_edge_types_properties_and_indexes","title":"\"Can non-English characters be used as identifiers, such as the names of graph spaces, tags, edge types, properties, and indexes?\"","text":"<p>Yes, for more information, see Keywords and reserved words.</p>"},{"location":"20.appendix/0.FAQ/#how_to_get_the_out-degreethe_in-degree_of_a_given_vertex","title":"\"How to get the out-degree/the in-degree of a given vertex?\"","text":"<p>The out-degree of a vertex refers to the number of edges starting from that vertex, while the in-degree refers to the number of edges pointing to that vertex.</p> <pre><code>nebula &gt; MATCH (s)-[e]-&gt;() WHERE id(s) == \"given\" RETURN count(e); #Out-degree\nnebula &gt; MATCH (s)&lt;-[e]-() WHERE id(s) == \"given\" RETURN count(e); #In-degree\n</code></pre> <p>This is a very slow operation to get the out/in degree since no accelaration can be applied (no indices or caches). It also could be out-of-memory when hitting a supper-node.</p>"},{"location":"20.appendix/0.FAQ/#how_to_quickly_get_the_out-degree_and_in-degree_of_all_vertices","title":"\"How to quickly get the out-degree and in-degree of all vertices?\"","text":"<p>There is no such command.</p> <p>You can use NebulaGraph Algorithm.</p>"},{"location":"20.appendix/0.FAQ/#about_operation_and_maintenance","title":"About operation and maintenance","text":""},{"location":"20.appendix/0.FAQ/#the_runtime_log_files_are_too_large_how_to_recycle_the_logs","title":"\"The runtime log files are too large. How to recycle the logs?\"","text":"<p>By default, the runtime logs of NebulaGraph are stored in  <code>/usr/local/nebula/logs/</code>. The INFO level log files are <code>nebula-graphd.INFO, nebula-storaged.INFO, nebula-metad.INFO</code>. If an alarm or error occurs, the suffixes are modified as <code>.WARNING</code> or <code>.ERROR</code>.</p> <p>NebulaGraph uses glog to print logs. <code>glog</code> cannot recycle the outdated files. To rotate logs, you can:</p> <ul> <li>Use crontab to delete logs periodically. For more information, see <code>Glog should delete old log files automatically</code>.</li> <li>Use logrotate to manage log files. Before using logrotate, modify the configurations of corresponding services and set <code>timestamp_in_logfile_name</code> to <code>false</code>.</li> </ul>"},{"location":"20.appendix/0.FAQ/#how_to_check_the_nebulagraph_version","title":"\"How to check the NebulaGraph version?\"","text":"<p>If the service is running: run command <code>SHOW HOSTS META</code> in <code>nebula-console</code>. See SHOW HOSTS.</p> <p>If the service is not running:</p> <p>Different installation methods make the method of checking the version different. The instructions are as follows:</p> <p>If the service is not running, run the command <code>./&lt;binary_name&gt; --version</code> to get the version and the Git commit IDs of the NebulaGraph binary files. For example:</p> <pre><code>$ ./nebula-graphd --version\n</code></pre> <ul> <li> <p>If you deploy NebulaGraph with Docker Compose</p> <p>Check the version of NebulaGraph deployed by Docker Compose. The method is similar to the previous method, except that you have to enter the container first. The commands are as follows:</p> <pre><code>docker exec -it nebula-docker-compose_graphd_1 bash\ncd bin/\n./nebula-graphd --version\n</code></pre> </li> </ul> <ul> <li> <p>If you install NebulaGraph with RPM/DEB package</p> <p>Run <code>rpm -qa |grep nebula</code> to check the version of NebulaGraph.</p> </li> </ul>"},{"location":"20.appendix/0.FAQ/#how_to_scale_my_cluster_updown_or_outin","title":"\"How to scale my cluster up/down or out/in?\"","text":"<p>Enterpriseonly</p> <p>The cluster scaling function has not been officially released in the community edition. The operations involving <code>SUBMIT JOB BALANCE DATA REMOVE</code> and <code>SUBMIT JOB BALANCE DATA</code> are experimental features in the community edition and the functionality is not stable. Before using it in the community edition, make sure to back up your data first and set <code>enable_experimental_feature</code> and <code>enable_data_balance</code> to <code>true</code> in the Graph configuration file.</p>"},{"location":"20.appendix/0.FAQ/#increase_or_decrease_the_number_of_meta_graph_or_storage_nodes","title":"Increase or decrease the number of Meta, Graph, or Storage nodes","text":"<p>NebulaGraph 3.5.0 does not provide any commands or tools to support automatic scale out/in. You can refer to the following steps:</p> <ol> <li> <p>Scale out and scale in metad: The metad process can not be scaled out or scale in. The process cannot be moved to a new machine. You cannot add a new metad process to the service.</p> <p>Note</p> <p>You can use the Meta transfer script tool to migrate Meta services. Note that the Meta-related settings in the configuration files of Storage and Graph services need to be modified correspondingly.</p> </li> <li> <p>Scale in graphd: Remove the IP of the graphd process from the code in the client. Close this graphd process.</p> </li> <li> <p>Scale out graphd: Prepare the binary and config files of the graphd process in the new host. Modify the config files and add all existing addresses of the metad processes. Then start the new graphd process.</p> </li> <li> <p>Scale in storaged: See Balance remove command. After the command is finished, stop this storaged process.</p> <p>Caution</p> <ul> <li>Before executing this command to migrate the data in the specified Storage node, make sure that the number of other Storage nodes is sufficient to meet the set replication factor. For example, if the replication factor is set to 3, then before executing this command, make sure that the number of other Storage nodes is greater than or equal to 3.</li> </ul> <ul> <li>If there are multiple space partitions in the Storage node to be migrated, execute this command in each space to migrate all space partitions in the Storage node.</li> </ul> </li> <li> <p>Scale out storaged: Prepare the binary and config files of the storaged process in the new host, Modify the config files and add all existing addresses of the metad processes. Then register the storaged process to the metad, and then start the new storaged process. For details, see Register storaged services.</p> </li> </ol> <p>You also need to run Balance Data and Balance leader after scaling in/out storaged.</p> <p>You can scale Graph and Storage services with Dashboard Enterprise Edition. For details, see Scale. </p> <p>You can also use NebulaGraph Operator to scale Graph and Storage services. For details, see Deploy NebulaGraph clusters with Kubectl and Deploy NebulaGraph clusters with Helm.   </p>"},{"location":"20.appendix/0.FAQ/#add_or_remove_disks_in_the_storage_nodes","title":"Add or remove disks in the Storage nodes","text":"<p>Currently, Storage cannot dynamically recognize new added disks. You can add or remove disks in the Storage nodes of the distributed cluster by following these steps:</p> <ol> <li> <p>Execute <code>SUBMIT JOB BALANCE DATA REMOVE &lt;ip:port&gt;</code> to migrate data in the Storage node with the disk to be added or removed to other Storage nodes.</p> <p>Caution</p> <ul> <li>Before executing this command to migrate the data in the specified Storage node, make sure that the number of other Storage nodes is sufficient to meet the set replication factor. For example, if the replication factor is set to 3, then before executing this command, make sure that the number of other Storage nodes is greater than or equal to 3.</li> </ul> <ul> <li>If there are multiple space partitions in the Storage node to be migrated, execute this command in each space to migrate all space partitions in the Storage node.</li> </ul> </li> <li> <p>Execute <code>DROP HOSTS &lt;ip:port&gt;</code> to remove the Storage node with the disk to be added or removed.</p> </li> <li> <p>In the configuration file of all Storage nodes, configure the path of the new disk to be added or removed through <code>--data_path</code>, see Storage configuration file for details.</p> </li> <li>Execute <code>ADD HOSTS &lt;ip:port&gt;</code> to re-add the Storage node with the disk to be added or removed.</li> <li>As needed, execute <code>SUBMIT JOB BALANCE DATA</code> to evenly distribute the shards of the current space to all Storage nodes and execute <code>SUBMIT JOB BALANCE LEADER</code> command to balance the leaders in all spaces. Before running the command, select a space.</li> </ol>"},{"location":"20.appendix/0.FAQ/#after_changing_the_name_of_the_host_the_old_one_keeps_displaying_offline_what_should_i_do","title":"\"After changing the name of the host, the old one keeps displaying <code>OFFLINE</code>. What should I do?\"","text":"<p>Hosts with the status of <code>OFFLINE</code> will be automatically deleted after one day.</p>"},{"location":"20.appendix/0.FAQ/#how_do_i_view_the_dmp_file","title":"\"How do I view the dmp file?\"","text":"<p>The dmp file is an error report file detailing the exit of the process and can be viewed with the gdb utility. the Coredump file is saved in the directory of the startup binary (by default it is <code>/usr/local/nebula</code>) and is generated automatically when the NebulaGraph service crashes.</p> <ol> <li>Check the Core file process name, pid is usually a numeric value.    <pre><code>$ file core.&lt;pid&gt;\n</code></pre></li> <li>Use gdb to debug.    <pre><code>$ gdb &lt;process.name&gt; core.&lt;pid&gt;\n</code></pre></li> <li>View the contents of the file.    <pre><code>$(gdb) bt\n</code></pre></li> </ol> <p>For example:    <pre><code>$ file core.1316027\ncore.1316027: ELF 64-bit LSB core file, x86-64, version 1 (SYSV), SVR4-style, from '/home/workspace/fork/nebula-debug/bin/nebula-metad --flagfile /home/k', real uid: 1008, effective uid: 1008, real gid: 1008, effective gid: 1008, execfn: '/home/workspace/fork/nebula-debug/bin/nebula-metad', platform: 'x86_64'\n\n$ gdb /home/workspace/fork/nebula-debug/bin/nebula-metad core.1316027\n\n$(gdb) bt\n#0  0x00007f9de58fecf5 in __memcpy_ssse3_back () from /lib64/libc.so.6\n#1  0x0000000000eb2299 in void std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;::_M_construct&lt;char*&gt;(char*, char*, std::forward_iterator_tag) ()\n#2  0x0000000000ef71a7 in nebula::meta::cpp2::QueryDesc::QueryDesc(nebula::meta::cpp2::QueryDesc const&amp;) ()\n...\n</code></pre></p> <p>If you are not clear about the information that dmp prints out, you can post the printout with the OS version, hardware configuration, error logs before and after the Core file was created and actions that may have caused the error on the NebulaGraph forum.</p>"},{"location":"20.appendix/0.FAQ/#how_can_i_set_the_nebulagraph_service_to_start_automatically_on_boot_via_systemctl","title":"How can I set the NebulaGraph service to start automatically on boot via systemctl?","text":"<ol> <li> <p>Execute <code>systemctl enable</code> to start the metad, graphd and storaged services.</p> <pre><code>[root]# systemctl enable nebula-metad.service\nCreated symlink from /etc/systemd/system/multi-user.target.wants/nebula-metad.service to /usr/lib/systemd/system/nebula-metad.service.\n[root]# systemctl enable nebula-graphd.service\nCreated symlink from /etc/systemd/system/multi-user.target.wants/nebula-graphd.service to /usr/lib/systemd/system/nebula-graphd.service.\n[root]# systemctl enable nebula-storaged.service\nCreated symlink from /etc/systemd/system/multi-user.target.wants/nebula-storaged.service to /usr/lib/systemd/system/nebula-storaged.service.\n</code></pre> </li> <li> <p>Configure the service files for metad, graphd and storaged to set the service to pull up automatically.</p> <p>Caution</p> <p>The following points need to be noted when configuring the service file.    - The paths of the PIDFile, ExecStart, ExecReload and ExecStop parameters need to be the same as those on the server.    - RestartSec is the length of time (in seconds) to wait before restarting, which can be modified according to the actual situation.    - (Optional) StartLimitInterval is the unlimited restart, the default is 10 seconds if the restart exceeds 5 times, and set to 0 means unlimited restart.    - (Optional) LimitNOFILE is the maximum number of open files for the service, the default is 1024 and can be changed according to the actual situation.</p> <p>Configure the service file for the metad service.</p> <pre><code>$ vi /usr/lib/systemd/system/nebula-metad.service\n\n[Unit]\nDescription=Nebula Graph Metad Service\nAfter=network.target\n\n[Service ]\nType=forking\nRestart=always\nRestartSec=15s\nPIDFile=/usr/local/nebula/pids/nebula-metad.pid\nExecStart=/usr/local/nebula/scripts/nebula.service start metad\nExecReload=/usr/local/nebula/scripts/nebula.service restart metad\nExecStop=/usr/local/nebula/scripts/nebula.service stop metad\nPrivateTmp=true\nStartLimitInterval=0\nLimitNOFILE=1024\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Configure the service file for the graphd service.</p> <p><pre><code>$ vi /usr/lib/systemd/system/nebula-graphd.service\n[Unit]\nDescription=Nebula Graph Graphd Service\nAfter=network.target\n\n[Service]\nType=forking\nRestart=always\nRestartSec=15s\nPIDFile=/usr/local/nebula/pids/nebula-graphd.pid\nExecStart=/usr/local/nebula/scripts/nebula.service start graphd\nExecReload=/usr/local/nebula/scripts/nebula.service restart graphd\nExecStop=/usr/local/nebula/scripts/nebula.service stop graphd\nPrivateTmp=true\nStartLimitInterval=0\nLimitNOFILE=1024\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>  Configure the service file for the storaged service.</p> <pre><code>$ vi /usr/lib/systemd/system/nebula-storaged.service\n[Unit]\nDescription=Nebula Graph Storaged Service\nAfter=network.target\n\n[Service]\nType=forking\nRestart=always\nRestartSec=15s\nPIDFile=/usr/local/nebula/pids/nebula-storaged.pid\nExecStart=/usr/local/nebula/scripts/nebula.service start storaged\nExecReload=/usr/local/nebula/scripts/nebula.service restart storaged\nExecStop=/usr/local/nebula/scripts/nebula.service stop storaged\nPrivateTmp=true\nStartLimitInterval=0\nLimitNOFILE=1024\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> </li> <li> <p>Reload the configuration file.</p> <pre><code>[root]# sudo systemctl daemon-reload\n</code></pre> </li> <li> <p>Restart the service.</p> <pre><code>$ systemctl restart nebula-metad.service\n$ systemctl restart nebula-graphd.service\n$ systemctl restart nebula-storaged.service\n</code></pre> </li> </ol>"},{"location":"20.appendix/0.FAQ/#about_connections","title":"About connections","text":""},{"location":"20.appendix/0.FAQ/#which_ports_should_be_opened_on_the_firewalls","title":"\"Which ports should be opened on the firewalls?\"","text":"<p>If you have not modified the predefined ports in the Configurations, open the following ports for the NebulaGraph services:</p> Service Port Meta 9559, 9560, 19559 Graph 9669, 19669 Storage 9777 ~ 9780, 19779 <p>If you have customized the configuration files and changed the predefined ports, find the port numbers in your configuration files and open them on the firewalls.</p> <p>For more port information, see Port Guide for Company Products.</p>"},{"location":"20.appendix/0.FAQ/#how_to_test_whether_a_port_is_open_or_closed","title":"\"How to test whether a port is open or closed?\"","text":"<p>You can use telnet as follows to check for port status.</p> <pre><code>telnet &lt;ip&gt; &lt;port&gt;\n</code></pre> <p>Note</p> <p>If you cannot use the telnet command, check if telnet is installed or enabled on your host.</p> <p>For example:</p> <pre><code>// If the port is open:\n$ telnet 192.168.1.10 9669\nTrying 192.168.1.10...\nConnected to 192.168.1.10.\nEscape character is '^]'.\n\n// If the port is closed or blocked:\n$ telnet 192.168.1.10 9777\nTrying 192.168.1.10...\ntelnet: connect to address 192.168.1.10: Connection refused\n</code></pre>"},{"location":"20.appendix/6.eco-tool-version/","title":"Ecosystem tools overview","text":"<p>Compatibility</p> <p>The core release number naming rule is <code>X.Y.Z</code>, which means <code>Major version X</code>, <code>Medium version Y</code>, and <code>Minor version Z</code>. The upgrade requirements for the client are:</p> <ul> <li>Upgrade the core from <code>X.Y.Z1</code> to <code>X.Y.Z2</code>: It means that the core is fully forward compatible and is usually used for bugfixes. It is recommended to upgrade the minor version of the core as soon as possible. At this time, the client can stay not upgraded.</li> </ul> <ul> <li>Upgrade the core from <code>X.Y1.*</code> to <code>X.Y2.*</code>: It means that there is some incompatibility of API, syntax, and return value. It is usually used to add functions, improve performance, and optimize code. The client needs to be upgraded to <code>X.Y2.*</code>.</li> </ul> <ul> <li>Upgrade the core from <code>X1.*.*</code> to <code>X2.*.*</code>: It means that there is a major incompatibility in storage formats, API, syntax, etc. You need to use tools to upgrade the core data. The client must be upgraded.</li> </ul> <ul> <li>The default core and client do not support downgrade: You cannot downgrade from <code>X.Y.Z2</code> to <code>X.Y.Z1</code>.</li> </ul> <ul> <li>The release cycle of a <code>Y</code> version is about 6 months, and its maintenance and support cycle is 6 months.</li> </ul> <ul> <li>The version released at the beginning of the year is usually named <code>X.0.0</code>, and in the middle of the year, it is named <code>X.5.0</code>.</li> </ul> <ul> <li>The file name contains <code>RC</code> to indicate an unofficial version (<code>Release Candidate</code>) that is only used for preview. Its maintenance period is only until the next RC or official version is released. Its client, data compatibility, etc. are not guaranteed.</li> </ul> <ul> <li>The files with <code>nightly</code>, <code>SNAPSHOT</code>, or date are the nightly versions. There is no quality assurance and maintenance period.</li> </ul>"},{"location":"20.appendix/6.eco-tool-version/#nebulagraph_studio","title":"NebulaGraph Studio","text":"<p>NebulaGraph Studio (Studio for short) is a graph database visualization tool that can be accessed through the Web. It can be used with NebulaGraph DBMS to provide one-stop services such as composition, data import, writing nGQL queries, and graph exploration. For details, see What is NebulaGraph Studio.</p> <p>Note</p> <p>The release of the Studio is independent of NebulaGraph core, and its naming method is also not the same as the core naming rules. </p> NebulaGraph version Studio version v3.5.0 v3.7.0"},{"location":"20.appendix/6.eco-tool-version/#nebulagraph_dashboard_community_edition","title":"NebulaGraph Dashboard Community Edition","text":"<p>NebulaGraph Dashboard Community Edition (Dashboard for short) is a visualization tool for monitoring the status of machines and services in the NebulaGraph cluster. For details, see What is NebulaGraph Dashboard.</p> NebulaGraph version Dashboard Community version v3.5.0 v3.4.0"},{"location":"20.appendix/6.eco-tool-version/#nebulagraph_dashboard_enterprise_edition","title":"NebulaGraph Dashboard Enterprise Edition","text":"<p>NebulaGraph Dashboard Enterprise Edition (Dashboard for short) is a visualization tool that monitors and manages the status of machines and services in NebulaGraph cluster. For details, see What is NebulaGraph Dashboard.</p> NebulaGraph version Dashboard Enterprise version v3.5.0 v3.5.0"},{"location":"20.appendix/6.eco-tool-version/#nebulagraph_explorer","title":"NebulaGraph Explorer","text":"<p>NebulaGraph Explorer (Explorer for short) is a graph exploration visualization tool that can be accessed through the Web. It is used with the NebulaGraph core to visualize interaction with graph data. Users can quickly become map experts, even without experience in map data manipulation. For details, see What is NebulaGraph Explorer.</p> NebulaGraph version Explorer Enterprise version v3.5.0 v3.5.1"},{"location":"20.appendix/6.eco-tool-version/#nebulagraph_stats_exporter","title":"NebulaGraph Stats Exporter","text":"<p>Nebula-stats-exporter exports monitor metrics to Promethus.</p> NebulaGraph version Stats Exporter version v3.5.0 v3.3.0"},{"location":"20.appendix/6.eco-tool-version/#nebulagraph_exchange","title":"NebulaGraph Exchange","text":"<p>NebulaGraph Exchange (Exchange for short) is an Apache Spark&amp;trade application for batch migration of data in a cluster to NebulaGraph in a distributed environment. It can support the migration of batch data and streaming data in a variety of different formats. For details, see What is NebulaGraph Exchange.</p> NebulaGraph version Exchange Community version Exchange Enterprise version v3.5.0 v3.5.0 v3.5.0"},{"location":"20.appendix/6.eco-tool-version/#nebulagraph_operator","title":"NebulaGraph Operator","text":"<p>NebulaGraph Operator (Operator for short) is a tool to automate the deployment, operation, and maintenance of NebulaGraph clusters on Kubernetes. Building upon the excellent scalability mechanism of Kubernetes, NebulaGraph introduced its operation and maintenance knowledge into the Kubernetes system, which makes NebulaGraph a real cloud-native graph database. For more information, see What is NebulaGraph Operator.</p> NebulaGraph version Operator version v3.5.0 v1.6.2"},{"location":"20.appendix/6.eco-tool-version/#nebulagraph_importer","title":"NebulaGraph Importer","text":"<p>NebulaGraph Importer (Importer for short) is a CSV file import tool for NebulaGraph. The Importer can read the local CSV file, and then import the data into the NebulaGraph database. For details, see What is NebulaGraph Importer.</p> NebulaGraph version Importer version v3.5.0 v4.0.0"},{"location":"20.appendix/6.eco-tool-version/#nebulagraph_spark_connector","title":"NebulaGraph Spark Connector","text":"<p>NebulaGraph Spark Connector is a Spark connector that provides the ability to read and write NebulaGraph data in the Spark standard format. NebulaGraph Spark Connector consists of two parts, Reader and Writer. For details, see What is NebulaGraph Spark Connector.</p> NebulaGraph version Spark Connector version v3.5.0 v3.4.0"},{"location":"20.appendix/6.eco-tool-version/#nebulagraph_flink_connector","title":"NebulaGraph Flink Connector","text":"<p>NebulaGraph Flink Connector is a connector that helps Flink users quickly access NebulaGraph. It supports reading data from the NebulaGraph database or writing data read from other external data sources to the NebulaGraph database. For details, see What is NebulaGraph Flink Connector.</p> NebulaGraph version Flink Connector version v3.5.0 v3.5.0"},{"location":"20.appendix/6.eco-tool-version/#nebulagraph_algorithm","title":"NebulaGraph Algorithm","text":"<p>NebulaGraph Algorithm (Algorithm for short) is a Spark application based on GraphX, which uses a complete algorithm tool to analyze data in the NebulaGraph database by submitting a Spark task To perform graph computing, use the algorithm under the lib repository through programming to perform graph computing for DataFrame. For details, see What is NebulaGraph Algorithm.</p> NebulaGraph version Algorithm version v3.5.0 v3.0.0"},{"location":"20.appendix/6.eco-tool-version/#nebulagraph_analytics","title":"NebulaGraph Analytics","text":"<p>NebulaGraph Analytics is an application that integrates the open-source Plato Graph Computing Framework, with which NebulaGraph Analytics performs graph computations on NebulaGraph database data. For details, see What is NebulaGraph Analytics.</p> NebulaGraph version Analytics version v3.5.0 v3.5.0"},{"location":"20.appendix/6.eco-tool-version/#nebulagraph_console","title":"NebulaGraph Console","text":"<p>NebulaGraph Console is the native CLI client of NebulaGraph. For how to use it, see NebulaGraph Console.</p> NebulaGraph version Console version v3.5.0 v3.5.0"},{"location":"20.appendix/6.eco-tool-version/#nebulagraph_docker_compose","title":"NebulaGraph Docker Compose","text":"<p>Docker Compose can quickly deploy NebulaGraph clusters. For how to use it, please refer to Docker Compose Deployment NebulaGraph.</p> NebulaGraph version Docker Compose version v3.5.0 v3.5.0"},{"location":"20.appendix/6.eco-tool-version/#backup_restore","title":"Backup &amp; Restore","text":"<p>Backup&amp;Restore (BR for short) is a command line interface (CLI) tool that can help back up the graph space data of NebulaGraph, or restore it through a backup file data.</p> NebulaGraph version BR version v3.5.0 v3.5.0"},{"location":"20.appendix/6.eco-tool-version/#backup_restore_enterprise_edition","title":"Backup &amp; Restore Enterprise Edition","text":"<p>Backup Restore (BR for short) Enterprise Edition is a Command-Line Interface (CLI) tool. With BR Enterprise Edition, you can back up and restore NebulaGraph Enterprise Edition data.</p> NebulaGraph version BR version v3.5.0 v3.5.1"},{"location":"20.appendix/6.eco-tool-version/#nebulagraph_bench","title":"NebulaGraph Bench","text":"<p>NebulaGraph Bench is used to test the baseline performance data of NebulaGraph. It uses the standard data set of LDBC.</p> NebulaGraph version Bench version v3.5.0 v1.2.0"},{"location":"20.appendix/6.eco-tool-version/#api_sdk","title":"API, SDK","text":"<p>Compatibility</p> <p>Select the latest version of <code>X.Y.*</code> which is the same as the core version.</p> NebulaGraph version Language v3.5.0 C++ v3.5.0 Go v3.5.0 Python v3.5.0 Java v3.5.0 HTTP"},{"location":"20.appendix/6.eco-tool-version/#not_released","title":"Not Released","text":"<ul> <li>Rust Client</li> <li>Node.js Client</li> <li>Object Graph Mapping Library (OGM, or ORM) </li> </ul>"},{"location":"20.appendix/error-code/","title":"Error code","text":"<p>NebulaGraph returns an error code when an error occurs. This topic describes the details of the error code returned.  </p> <p>Note</p> <ul> <li>If an error occurs but no error code is returned, or if the error code description is unclear, we welcome your feedback or suggestions on the forum or GitHub. </li> </ul> <ul> <li>When the code returned is <code>0</code>, it means that the operation is successful.</li> </ul> Error name Error Code Description <code>E_DISCONNECTED</code> <code>-1</code> Lost connection <code>E_FAIL_TO_CONNECT</code> <code>-2</code> Unable to establish connection <code>E_RPC_FAILURE</code> <code>-3</code> RPC failure <code>E_LEADER_CHANGED</code> <code>-4</code> Raft leader has been changed <code>E_SPACE_NOT_FOUND</code> <code>-5</code> Graph space does not exist <code>E_TAG_NOT_FOUND</code> <code>-6</code> Tag does not exist <code>E_EDGE_NOT_FOUND</code> <code>-7</code> Edge type does not exist <code>E_INDEX_NOT_FOUND</code> <code>-8</code> Index does not exist <code>E_EDGE_PROP_NOT_FOUND</code> <code>-9</code> Edge type property does not exist <code>E_TAG_PROP_NOT_FOUND</code> <code>-10</code> Tag property does not exist <code>E_ROLE_NOT_FOUND</code> <code>-11</code> The current role does not exist <code>E_CONFIG_NOT_FOUND</code> <code>-12</code> The current configuration does not exist <code>E_MACHINE_NOT_FOUND</code> <code>-13</code> The current host does not exist <code>E_LISTENER_NOT_FOUND</code> <code>-15</code> Listener does not exist <code>E_PART_NOT_FOUND</code> <code>-16</code> The current partition does not exist <code>E_KEY_NOT_FOUND</code> <code>-17</code> Key does not exist <code>E_USER_NOT_FOUND</code> <code>-18</code> User does not exist <code>E_STATS_NOT_FOUND</code> <code>-19</code> Statistics do not exist <code>E_SERVICE_NOT_FOUND</code> <code>-20</code> No current service found <code>E_DRAINER_NOT_FOUND</code> <code>-21</code> Drainer does not exist <code>E_DRAINER_CLIENT_NOT_FOUND</code> <code>-22</code> Drainer client does not exist <code>E_PART_STOPPED</code> <code>-23</code> The current partition has already been stopped <code>E_BACKUP_FAILED</code> <code>-24</code> Backup failed <code>E_BACKUP_EMPTY_TABLE</code> <code>-25</code> The backed-up table is empty <code>E_BACKUP_TABLE_FAILED</code> <code>-26</code> Table backup failure <code>E_PARTIAL_RESULT</code> <code>-27</code> MultiGet could not get all data <code>E_REBUILD_INDEX_FAILED</code> <code>-28</code> Index rebuild failed <code>E_INVALID_PASSWORD</code> <code>-29</code> Password is invalid <code>E_FAILED_GET_ABS_PATH</code> <code>-30</code> Unable to get absolute path <code>E_BAD_USERNAME_PASSWORD</code> <code>-1001</code> Authentication failed <code>E_SESSION_INVALID</code> <code>-1002</code> Invalid session <code>E_SESSION_TIMEOUT</code> <code>-1003</code> Session timeout <code>E_SYNTAX_ERROR</code> <code>-1004</code> Syntax error <code>E_EXECUTION_ERROR</code> <code>-1005</code> Execution error <code>E_STATEMENT_EMPTY</code> <code>-1006</code> Statement is empty <code>E_BAD_PERMISSION</code> <code>-1008</code> Permission denied <code>E_SEMANTIC_ERROR</code> <code>-1009</code> Semantic error <code>E_TOO_MANY_CONNECTIONS</code> <code>-1010</code> Maximum number of connections exceeded <code>E_PARTIAL_SUCCEEDED</code> <code>-1011</code> Access to storage failed (only some requests succeeded) <code>E_NO_HOSTS</code> <code>-2001</code> Host does not exist <code>E_EXISTED</code> <code>-2002</code> Host already exists <code>E_INVALID_HOST</code> <code>-2003</code> Invalid host <code>E_UNSUPPORTED</code> <code>-2004</code> The current command, statement, or function is not supported <code>E_NOT_DROP</code> <code>-2005</code> Not allowed to drop <code>E_CONFIG_IMMUTABLE</code> <code>-2007</code> Configuration items cannot be changed <code>E_CONFLICT</code> <code>-2008</code> Parameters conflict with meta data <code>E_INVALID_PARM</code> <code>-2009</code> Invalid parameter <code>E_WRONGCLUSTER</code> <code>-2010</code> Wrong cluster <code>E_ZONE_NOT_ENOUGH</code> <code>-2011</code> Listener conflicts <code>E_ZONE_IS_EMPTY</code> <code>-2012</code> Host not exist <code>E_SCHEMA_NAME_EXISTS</code> <code>-2013</code> Schema name already exists <code>E_RELATED_INDEX_EXISTS</code> <code>-2014</code> There are still indexes related to tag or edge, cannot drop it <code>E_RELATED_SPACE_EXISTS</code> <code>-2015</code> There are still some space on the host, cannot drop it <code>E_STORE_FAILURE</code> <code>-2021</code> Failed to store data <code>E_STORE_SEGMENT_ILLEGAL</code> <code>-2022</code> Illegal storage segment <code>E_BAD_BALANCE_PLAN</code> <code>-2023</code> Invalid data balancing plan <code>E_BALANCED</code> <code>-2024</code> The cluster is already in the data balancing status <code>E_NO_RUNNING_BALANCE_PLAN</code> <code>-2025</code> There is no running data balancing plan <code>E_NO_VALID_HOST</code> <code>-2026</code> Lack of valid hosts <code>E_CORRUPTED_BALANCE_PLAN</code> <code>-2027</code> A data balancing plan that has been corrupted <code>E_IMPROPER_ROLE</code> <code>-2030</code> Failed to recover user role <code>E_INVALID_PARTITION_NUM</code> <code>-2031</code> Number of invalid partitions <code>E_INVALID_REPLICA_FACTOR</code> <code>-2032</code> Invalid replica factor <code>E_INVALID_CHARSET</code> <code>-2033</code> Invalid character set <code>E_INVALID_COLLATE</code> <code>-2034</code> Invalid character sorting rules <code>E_CHARSET_COLLATE_NOT_MATCH</code> <code>-2035</code> Character set and character sorting rule mismatch <code>E_SNAPSHOT_FAILURE</code> <code>-2040</code> Failed to generate a snapshot <code>E_BLOCK_WRITE_FAILURE</code> <code>-2041</code> Failed to write block data <code>E_ADD_JOB_FAILURE</code> <code>-2044</code> Failed to add new task <code>E_STOP_JOB_FAILURE</code> <code>-2045</code> Failed to stop task <code>E_SAVE_JOB_FAILURE</code> <code>-2046</code> Failed to save task information <code>E_BALANCER_FAILURE</code> <code>-2047</code> Data balancing failed <code>E_JOB_NOT_FINISHED</code> <code>-2048</code> The current task has not been completed <code>E_TASK_REPORT_OUT_DATE</code> <code>-2049</code> Task report failed <code>E_JOB_NOT_IN_SPACE</code> <code>-2050</code> The current task is not in the graph space <code>E_JOB_NEED_RECOVER</code> <code>-2051</code> The current task needs to be resumed <code>E_JOB_ALREADY_FINISH</code> <code>-2052</code> The job status has already been failed or finished <code>E_JOB_SUBMITTED</code> <code>-2053</code> Job default status <code>E_JOB_NOT_STOPPABLE</code> <code>-2054</code> The given job do not support stop <code>E_JOB_HAS_NO_TARGET_STORAGE</code> <code>-2055</code> The leader distribution has not been reported, so can't send task to storage <code>E_INVALID_JOB</code> <code>-2065</code> Invalid task <code>E_BACKUP_BUILDING_INDEX</code> <code>-2066</code> Backup terminated (index being created) <code>E_BACKUP_SPACE_NOT_FOUND</code> <code>-2067</code> Graph space does not exist at the time of backup <code>E_RESTORE_FAILURE</code> <code>-2068</code> Backup recovery failed <code>E_SESSION_NOT_FOUND</code> <code>-2069</code> Session does not exist <code>E_LIST_CLUSTER_FAILURE</code> <code>-2070</code> Failed to get cluster information <code>E_LIST_CLUSTER_GET_ABS_PATH_FAILURE</code> <code>-2071</code> Failed to get absolute path when getting cluster information <code>E_LIST_CLUSTER_NO_AGENT_FAILURE</code> <code>-2072</code> Unable to get an agent when getting cluster information <code>E_QUERY_NOT_FOUND</code> <code>-2073</code> Query not found <code>E_AGENT_HB_FAILUE</code> <code>-2074</code> Failed to receive heartbeat from agent <code>E_HOST_CAN_NOT_BE_ADDED</code> <code>-2082</code> The host can not be added for it's not a storage host <code>E_ACCESS_ES_FAILURE</code> <code>-2090</code> Failed to access elasticsearch <code>E_GRAPH_MEMORY_EXCEEDED</code> <code>-2600</code> Graph memory exceeded <code>E_CONSENSUS_ERROR</code> <code>-3001</code> Consensus cannot be reached during an election <code>E_KEY_HAS_EXISTS</code> <code>-3002</code> Key already exists <code>E_DATA_TYPE_MISMATCH</code> <code>-3003</code> Data type mismatch <code>E_INVALID_FIELD_VALUE</code> <code>-3004</code> Invalid field value <code>E_INVALID_OPERATION</code> <code>-3005</code> Invalid operation <code>E_NOT_NULLABLE</code> <code>-3006</code> Current value is not allowed to be empty <code>E_FIELD_UNSET</code> <code>-3007</code> Field value must be set if the field value is <code>NOT NULL</code> or has no default value <code>E_OUT_OF_RANGE</code> <code>-3008</code> The value is out of the range of the current type <code>E_DATA_CONFLICT_ERROR</code> <code>-3010</code> Data conflict <code>E_WRITE_STALLED</code> <code>-3011</code> Writes are delayed <code>E_IMPROPER_DATA_TYPE</code> <code>-3021</code> Incorrect data type <code>E_INVALID_SPACEVIDLEN</code> <code>-3022</code> Invalid VID length <code>E_INVALID_FILTER</code> <code>-3031</code> Invalid filter <code>E_INVALID_UPDATER</code> <code>-3032</code> Invalid field update <code>E_INVALID_STORE</code> <code>-3033</code> Invalid KV storage <code>E_INVALID_PEER</code> <code>-3034</code> Peer invalid <code>E_RETRY_EXHAUSTED</code> <code>-3035</code> Out of retries <code>E_TRANSFER_LEADER_FAILED</code> <code>-3036</code> Leader change failed <code>E_INVALID_STAT_TYPE</code> <code>-3037</code> Invalid stat type <code>E_INVALID_VID</code> <code>-3038</code> VID is invalid <code>E_LOAD_META_FAILED</code> <code>-3040</code> Failed to load meta information <code>E_FAILED_TO_CHECKPOINT</code> <code>-3041</code> Failed to generate checkpoint <code>E_CHECKPOINT_BLOCKED</code> <code>-3042</code> Generating checkpoint is blocked <code>E_FILTER_OUT</code> <code>-3043</code> Data is filtered <code>E_INVALID_DATA</code> <code>-3044</code> Invalid data <code>E_MUTATE_EDGE_CONFLICT</code> <code>-3045</code> Concurrent write conflicts on the same edge <code>E_MUTATE_TAG_CONFLICT</code> <code>-3046</code> Concurrent write conflict on the same vertex <code>E_OUTDATED_LOCK</code> <code>-3047</code> Lock is invalid <code>E_INVALID_TASK_PARA</code> <code>-3051</code> Invalid task parameter <code>E_USER_CANCEL</code> <code>-3052</code> The user canceled the task <code>E_TASK_EXECUTION_FAILED</code> <code>-3053</code> Task execution failed <code>E_PLAN_IS_KILLED</code> <code>-3060</code> Execution plan was cleared <code>E_NO_TERM</code> <code>-3070</code> The heartbeat process was not completed when the request was received <code>E_OUTDATED_TERM</code> <code>-3071</code> Out-of-date heartbeat received from the old leader (the new leader has been elected) <code>E_WRITE_WRITE_CONFLICT</code> <code>-3073</code> Concurrent write conflicts with later requests <code>E_RAFT_UNKNOWN_PART</code> <code>-3500</code> Unknown partition <code>E_RAFT_LOG_GAP</code> <code>-3501</code> Raft logs lag behind <code>E_RAFT_LOG_STALE</code> <code>-3502</code> Raft logs are out of date <code>E_RAFT_TERM_OUT_OF_DATE</code> <code>-3503</code> Heartbeat messages are out of date <code>E_RAFT_UNKNOWN_APPEND_LOG</code> <code>-3504</code> Unknown additional logs <code>E_RAFT_WAITING_SNAPSHOT</code> <code>-3511</code> Waiting for the snapshot to complete <code>E_RAFT_SENDING_SNAPSHOT</code> <code>-3512</code> There was an error sending the snapshot <code>E_RAFT_INVALID_PEER</code> <code>-3513</code> Invalid receiver <code>E_RAFT_NOT_READY</code> <code>-3514</code> Raft did not start <code>E_RAFT_STOPPED</code> <code>-3515</code> Raft has stopped <code>E_RAFT_BAD_ROLE</code> <code>-3516</code> Wrong role <code>E_RAFT_WAL_FAIL</code> <code>-3521</code> Write to a WAL failed <code>E_RAFT_HOST_STOPPED</code> <code>-3522</code> The host has stopped <code>E_RAFT_TOO_MANY_REQUESTS</code> <code>-3523</code> Too many requests <code>E_RAFT_PERSIST_SNAPSHOT_FAILED</code> <code>-3524</code> Persistent snapshot failed <code>E_RAFT_RPC_EXCEPTION</code> <code>-3525</code> RPC exception <code>E_RAFT_NO_WAL_FOUND</code> <code>-3526</code> No WAL logs found <code>E_RAFT_HOST_PAUSED</code> <code>-3527</code> Host suspended <code>E_RAFT_WRITE_BLOCKED</code> <code>-3528</code> Writes are blocked <code>E_RAFT_BUFFER_OVERFLOW</code> <code>-3529</code> Cache overflow <code>E_RAFT_ATOMIC_OP_FAILED</code> <code>-3530</code> Atomic operation failed <code>E_LEADER_LEASE_FAILED</code> <code>-3531</code> Leader lease expired <code>E_RAFT_CAUGHT_UP</code> <code>-3532</code> Data has been synchronized on Raft <code>E_STORAGE_MEMORY_EXCEEDED</code> <code>-3600</code> Storage memory exceeded <code>E_LOG_GAP</code> <code>-4001</code> Drainer logs lag behind <code>E_LOG_STALE</code> <code>-4002</code> Drainer logs are out of date <code>E_INVALID_DRAINER_STORE</code> <code>-4003</code> The drainer data storage is invalid <code>E_SPACE_MISMATCH</code> <code>-4004</code> Graph space mismatch <code>E_PART_MISMATCH</code> <code>-4005</code> Partition mismatch <code>E_DATA_CONFLICT</code> <code>-4006</code> Data conflict <code>E_REQ_CONFLICT</code> <code>-4007</code> Request conflict <code>E_DATA_ILLEGAL</code> <code>-4008</code> Illegal data <code>E_CACHE_CONFIG_ERROR</code> <code>-5001</code> Cache configuration error <code>E_NOT_ENOUGH_SPACE</code> <code>-5002</code> Insufficient space <code>E_CACHE_MISS</code> <code>-5003</code> No cache hit <code>E_CACHE_WRITE_FAILURE</code> <code>-5005</code> Write cache failed <code>E_NODE_NUMBER_EXCEED_LIMIT</code> <code>-7001</code> Number of machines exceeded the limit <code>E_PARSING_LICENSE_FAILURE</code> <code>-7002</code> Failed to resolve certificate <code>E_UNKNOWN</code> <code>-8000</code> Unknown error"},{"location":"20.appendix/history/","title":"History timeline for NebulaGraph","text":"<ol> <li> <p>2018.9: dutor wrote and submitted the first line of NebulaGraph database code. </p> <p></p> </li> <li> <p>2019.5: NebulaGraph v0.1.0-alpha was released as open-source.</p> <p> </p> <p>NebulaGraph v1.0.0-beta, v1.0.0-rc1, v1.0.0-rc2, v1.0.0-rc3, and v1.0.0-rc4 were released one after another within a year thereafter.</p> <p></p> </li> <li> <p>2019.7: NebulaGraph's debut at HBaseCon<sup>1</sup>. @dangleptr</p> <p></p> </li> <li> <p>2020.3: NebulaGraph v2.0 was starting developed in the final stage of v1.0 development. </p> </li> <li> <p>2020.6: The first major version of NebulaGraph v1.0.0 GA was released.</p> <p></p> </li> <li> <p>2021.3: The second major version of NebulaGraph v2.0 GA was released.</p> <p></p> </li> <li> <p>2021.8: NebulaGraph v2.5.0 was released.</p> </li> <li> <p>2021.10: NebulaGraph v2.6.0 was released.</p> </li> <li> <p>2022.2: NebulaGraph v3.0.0 was released.</p> </li> <li> <p>2022.4: NebulaGraph v3.1.0 was released.</p> </li> <li> <p>2022.7: NebulaGraph v3.2.0 was released.</p> </li> <li> <p>2022.10: NebulaGraph v3.3.0 was released.</p> </li> <li> <p>2023.2: NebulaGraph v3.4.0 was released.</p> </li> <li> <p>2023.5: NebulaGraph v3.5.0 was released.</p> </li> </ol> <ol> <li> <p>NebulaGraph v1.x supports both RocksDB and HBase as its storage engines. NebulaGraph v2.x removes HBase supports.\u00a0\u21a9</p> </li> </ol>"},{"location":"20.appendix/learning-path/","title":"NebulaGraph learning path","text":"<p>This topic is for anyone interested in learning more about NebulaGraph. You can master NebulaGraph from zero to hero through the documentation and videos in NebulaGraph learning path. </p> <p></p> <p>After completing the NebulaGraph learning path, taking NebulaGraph Certification exams will earn you certifications. For more information, see the Get NebulaGraph Certifications section below.</p>"},{"location":"20.appendix/learning-path/#1_about_nebulagraph","title":"1. About NebulaGraph","text":""},{"location":"20.appendix/learning-path/#11_what_is_nebulagraph","title":"1.1 What is NebulaGraph?","text":"Document Video What is NebulaGraph NebulaGraph"},{"location":"20.appendix/learning-path/#12_data_models","title":"1.2 Data models","text":"Document Data modeling"},{"location":"20.appendix/learning-path/#13_path","title":"1.3 Path","text":"Document Path"},{"location":"20.appendix/learning-path/#14_nebulagraph_architecture","title":"1.4 NebulaGraph architecture","text":"Document Meta service Graph service Storage service"},{"location":"20.appendix/learning-path/#2_quick_start","title":"2. Quick start","text":""},{"location":"20.appendix/learning-path/#21_install_nebulagraph","title":"2.1 Install NebulaGraph","text":"Document Video Install with a RPM or DEB package - Install with a TAR package - Install with Docker Install NebulaGraph with Docker and Docker Compose Install from source Install NebulaGraph with Source Code"},{"location":"20.appendix/learning-path/#22_start_nebulagraph","title":"2.2 Start NebulaGraph","text":"Document Start and stop NebulaGraph"},{"location":"20.appendix/learning-path/#23_connect_to_nebulagraph","title":"2.3 Connect to NebulaGraph","text":"Document Connect to NebulaGraph"},{"location":"20.appendix/learning-path/#24_use_ngql_statements","title":"2.4 Use nGQL statements","text":"Document nGQL cheatsheet"},{"location":"20.appendix/learning-path/#3_hands-on_practices","title":"3. Hands-on practices","text":""},{"location":"20.appendix/learning-path/#31_deploy_a_multi-machine_cluster","title":"3.1 Deploy a multi-machine cluster","text":"Document Deploy a NebulaGraph cluster with RPM/DEB"},{"location":"20.appendix/learning-path/#32_upgrade_nebulagraph","title":"3.2 Upgrade NebulaGraph","text":"Document Upgrade NebulaGraph to release-3.5"},{"location":"20.appendix/learning-path/#33_configure_nebulagraph","title":"3.3 Configure NebulaGraph","text":"Document Configure Meta Configure Graph Configure Storage Configure Linux kernel"},{"location":"20.appendix/learning-path/#34_configure_logs","title":"3.4 Configure logs","text":"Document Log managements"},{"location":"20.appendix/learning-path/#35_om_and_management","title":"3.5 O&amp;M and Management","text":"<ul> <li> <p>Account authentication and authorization</p> Document Local authentication OpenLDAP User management Roles and privileges </li> </ul> <ul> <li> <p>Balance the distribution of partitions</p> Document Storage load balancing </li> </ul> <ul> <li> <p>Monitoring</p> Document NebulaGraph metrics RocksDB statistics </li> </ul> <ul> <li> <p>Data snapshot</p> Document Create snapshots </li> </ul> <ul> <li> <p>Backup &amp; Restore</p> Document Backup&amp;Restore </li> </ul> <ul> <li> <p>SSL encryption</p> Document SSL </li> </ul>"},{"location":"20.appendix/learning-path/#36_performance_tuning","title":"3.6 Performance tuning","text":"Document Graph data modeling suggestions System design suggestions Compaction"},{"location":"20.appendix/learning-path/#37_derivative_software","title":"3.7 Derivative software","text":"<ul> <li> <p>Visualization</p> Visualization tools Document Video Data visualization NebulaGraph Studio NebulaGraph Studio Data monitoring and O&amp;M NebulaGraph Dashboard Community Edition NebulaGraph Dashboard Enterprise Edition - Data analysis NebulaGraph Explorer Enterprise Edition - </li> </ul> <ul> <li> <p>Data import and export</p> Import and export Document Video Data import NebulaGraph Importer NebulaGraph Importer Data import NebulaGraph Spark Connector - Data import NebulaGraph Flink Connector - Data import NebulaGraph Exchange Community Edition - Data export NebulaGraph Exchange Enterprise Edition - </li> </ul> <ul> <li> <p>Performance test</p> Document NebulaGraph Bench </li> </ul> <ul> <li> <p>Cluster O&amp;M</p> Document NebulaGraph Operator </li> </ul> <ul> <li> <p>Graph algorithm</p> Document NebulaGraph Algorithm </li> </ul> <ul> <li> <p>Clients</p> Document NebulaGraph Console NebulaGraph CPP NebulaGraph Java NebulaGraph Python NebulaGraph Go </li> </ul>"},{"location":"20.appendix/learning-path/#4_api_sdk","title":"4. API &amp; SDK","text":"Document API &amp; SDK"},{"location":"20.appendix/learning-path/#5_best_practices","title":"5. Best practices","text":"Document Handling Tens of Billions of Threat Intelligence Data with Graph Database at Kuaishou Import data from Neo4j to NebulaGraph via NebulaGraph Exchange: Best Practices Hands-On Experience: Import Data to NebulaGraph with Spark How to Select a Graph Database: Best Practices at RoyalFlush Practicing NebulaGraph Operator on Cloud Using Ansible to Automate Deployment of NebulaGraph Cluster"},{"location":"20.appendix/learning-path/#6_faq","title":"6. FAQ","text":"Document FAQ"},{"location":"20.appendix/learning-path/#7_practical_tasks","title":"7. Practical tasks","text":"<p>You can check if you have mastered NebulaGraph by completing the following practical tasks. </p> Task Reference Compile the source code of NebulaGraph Install NebulaGraph by compiling the source code Deploy Studio, Dashboard, and Explorer Deploy Studio, Deploy Dashboard, and Deploy Explorer Load test NebulaGraph with K6 NebulaGraph Bench Query LDBC data (such as queries for vertices, paths, or subgraphs.) LDBC and interactive-short-1.cypher"},{"location":"20.appendix/learning-path/#8_get_nebulagraph_certifications","title":"8. Get NebulaGraph Certifications","text":"<p>Now you could get NebulaGraph Certifications from NebulaGraph Academy.</p> <ul> <li>NebulaGraph Certified Insider(NGCI): The NGCI certification provides a birdview to graph databases and the NebulaGraph database. Passing NGCI shows that you have a good understanding of NebulaGraph.</li> </ul> <ul> <li>NebulaGraph Certified Professional(NGCP): The NGCP certification drives you deep into the NebulaGraph database and its ecosystem, providing a 360-degree view of the leading-edge graph database. Passing NGCP proves that you are a professional with a profound understanding of NebulaGraph.</li> </ul>"},{"location":"20.appendix/learning-path/#reference_documents","title":"Reference documents","text":"<ul> <li>For an introduction to the principles of NebulaGraph, see Nebula Graph: An open source distributed graph database.</li> </ul> <ul> <li>For the principle description of NebulaGraph indexes, see Section 2.4 in the Nebula Graph: An open source distributed graph database paper.</li> </ul> <ul> <li>For an overview of the NebulaGraph language, see Section 2.8 in the Nebula Graph: An open source distributed graph database paper.</li> </ul>"},{"location":"20.appendix/port-guide/","title":"Port guide for company products","text":"<p>The following are the default ports used by NebulaGraph core and peripheral tools.</p> No. Product / Service Type Default Description 1 NebulaGraph TCP 9669 Graph service RPC daemon listening port (commonly used for client connections to the Graph service). 2 NebulaGraph TCP 19669 Graph service HTTP port. 3 NebulaGraph TCP 19670 Graph service HTTP/2 port. (Deprecated after version 3.x) 4 NebulaGraph TCP 9559 Meta service RPC daemon listening port. (Commonly used by Graph and Storage services for querying and updating metadata in the graph database). 5 NebulaGraph TCP 9560 Raft communication port between Meta services. 6 NebulaGraph TCP 19559 Meta service HTTP port. 7 NebulaGraph TCP 19560 Meta service HTTP/2 port. (Deprecated after version 3.x) 8 NebulaGraph TCP 9777 Drainer service port in Storage services (exposed only in Enterprise Edition clusters). 9 NebulaGraph TCP 9778 Admin service port in Storage services. 10 NebulaGraph TCP 9779 Storage service RPC daemon listening port. (Commonly used by Graph services for data storage-related operations, such as reading, writing, or deleting data). 11 NebulaGraph TCP 9780 Raft communication port between Storage services. 12 NebulaGraph TCP 19779 Storage service HTTP port. 13 NebulaGraph TCP 19780 Storage service HTTP/2 port. (Deprecated after version 3.x) 14 NebulaGraph TCP 8888 Backup and restore Agent service port. The Agent is a daemon running on each machine in the cluster, responsible for starting and stopping NebulaGraph services and uploading and downloading backup files. 15 NebulaGraph TCP 9789, 9790, and 9788 Full-text index Raft Listener port, which reads data from Storage services and writes it to the Elasticsearch cluster.Also the port for Storage Listener in inter-cluster data synchronization, used for synchronizing Storage data from the primary cluster. Ports 9790 and 9788 are generated by adding and subtracting one from port 9789. 16 NebulaGraph TCP 9200 NebulaGraph uses this port for HTTP communication with Elasticsearch to perform full-text search queries and manage full-text indexes. 17 NebulaGraph TCP 9569, 9570, and 9568 Meta Listener port in inter-cluster data synchronization, used for synchronizing Meta data from the primary cluster. Ports 9570 and 9568 are generated by adding and subtracting one from port 9569. 18 NebulaGraph TCP 9889, 9890, and 9888 Drainer service port in inter-cluster data synchronization, used for synchronizing Storage and Meta data to the primary cluster. Ports 9890 and 9888 are generated by adding and subtracting one from port 9889. 19 NebulaGraph Studio TCP 7001 Studio web service port. 20 NebulaGraph Dashboard TCP 8090 Nebula HTTP Gateway dependency service port. Provides an HTTP interface for cluster services to interact with the NebulaGraph database using nGQL statements. 21 NebulaGraph Dashboard TCP 9200 Nebula Stats Exporter dependency service port. Collects cluster performance metrics, including service IP addresses, versions, and monitoring metrics (such as query count, query latency, heartbeat latency, etc.). 22 NebulaGraph Dashboard TCP 9100 Node Exporter dependency service port. Collects resource information for machines in the cluster, including CPU, memory, load, disk, and traffic. 23 NebulaGraph Dashboard TCP 9091 Prometheus service port. Time-series database for storing monitoring data. 24 NebulaGraph Dashboard TCP 7003 Dashboard Community Edition web service port. 25 NebulaGraph Dashboard TCP 7005 Dashboard Enterprise Edition web service port. 26 NebulaGraph Dashboard TCP 9093 Alertmanager service port. Receives alerts from Prometheus and sends alert notifications to Dashboard. 27 NebulaGraph Explorer TCP 7002 Explorer web service port. 28 License Manager TCP 9119 The port for the License Manager (LM) service. The LM service is used for managing licenses (only used in enterprise clusters)."},{"location":"20.appendix/write-tools/","title":"Import tools","text":"<p>There are many ways to write NebulaGraph 3.5.0:</p> <ul> <li>Import with the command -f: This method imports a small number of prepared nGQL files, which is suitable to prepare for a small amount of manual test data.</li> <li>Import with Studio: This method uses a browser to import multiple csv files of this machine. A single file cannot exceed 100 MB, and its format is limited.</li> <li>Import with Importer: This method imports multiple csv files on a single machine with unlimited size and flexible format.</li> <li>Import with Exchange: This method imports from various distribution sources, such as Neo4j, Hive, MySQL, etc., which requires a Spark cluster.</li> <li>Import with Spark-connector/Flink-connector: This method has corresponding components (Spark/Flink) and writes a small amount of code.</li> <li>Import with C++/GO/Java/Python SDK: This method imports in the way of writing programs, which requires certain programming and tuning skills.</li> </ul> <p>The following figure shows the positions of these ways:</p> <p></p>"},{"location":"20.appendix/release-notes/dashboard-comm-release-note/","title":"NebulaGraph Dashboard Community Edition 3.5.0 release notes","text":""},{"location":"20.appendix/release-notes/dashboard-comm-release-note/#community_edition_340","title":"Community Edition 3.4.0","text":"<ul> <li>Feature<ul> <li>Support the built-in dashboard.service script to manage the Dashboard services with one-click and view the Dashboard version.</li> <li>Support viewing the configuration of Meta services.</li> </ul> </li> </ul> <ul> <li>Enhancement<ul> <li>Adjust the directory structure and simplify the deployment steps.</li> <li>Display the names of the monitoring metrics on the overview page of <code>machine</code>.</li> <li>Optimize the calculation of monitoring metrics such as <code>num_queries</code>, and adjust the display to time series aggregation.</li> </ul> </li> </ul>"},{"location":"20.appendix/release-notes/dashboard-ent-release-note/","title":"NebulaGraph Dashboard Enterprise Edition release notes","text":""},{"location":"20.appendix/release-notes/dashboard-ent-release-note/#enterprise_edition_350","title":"Enterprise Edition 3.5.0","text":"<ul> <li> <p>Feature</p> <ul> <li>Support deploying License Manager (LM) through Dashboard. For more detail, see Activate Dashboard.</li> <li>Back up and restore support full backup to local.</li> <li>Add Slow query analyst function.</li> <li>The Cluster diagnostics formula supports configuration.</li> <li>Config Management support Add Config, view the Effective value of the current configuration, and View inconsistent configurations.</li> <li>In the Notification endpoint, the webhook supports configuring the Webhook request body.</li> <li>Support custom monitoring panel.</li> </ul> </li> </ul> <ul> <li> <p>Enhancement</p> <ul> <li>Cluster topology consistency: After scale, no user manual refresh and authorization are required.</li> <li>Cluster Overview page optimization.</li> <li>Data Synchronization optimization.</li> <li>By default, the configuration of newly added nodes is consistent with that of the first node in the cluster.</li> <li>Optimize cluster diagnostic report content.</li> <li>Support changing the port number of <code>Prometheus</code> service in the <code>config.yaml</code> file.</li> </ul> </li> </ul>"},{"location":"20.appendix/release-notes/dashboard-ent-release-note/#enterprise_edition_342","title":"Enterprise Edition 3.4.2","text":"<ul> <li> <p>Enhancement</p> <ul> <li>Support viewing the data backup and restoration progress on the Backup&amp;Restore page.</li> <li>The installation package for NebulaGraph Enterprise v3.4.1 is built in.</li> </ul> </li> </ul>"},{"location":"20.appendix/release-notes/dashboard-ent-release-note/#enterprise_edition_341","title":"Enterprise Edition 3.4.1","text":"<ul> <li> <p>Bugfix</p> <ul> <li>Fix the bug that the RPM package cannot execute <code>nebula-agent</code> due to permission issues.</li> <li>Fix the bug that the cluster import information can not be viewed due to the <code>goconfig</code> folder permission.</li> <li>Fix the page error when the license expiration time is less than <code>30</code> days and <code>gracePeriod</code> is greater than <code>0</code>.</li> </ul> </li> </ul>"},{"location":"20.appendix/release-notes/dashboard-ent-release-note/#enterprise_edition_340","title":"Enterprise Edition 3.4.0","text":"<ul> <li>Feature<ul> <li>Support viewing the runtime log of the NebulaGraph clusters.</li> <li>Support viewing the audit log of the NebulaGraph clusters.</li> <li>Support jog management.</li> <li>Support incremental backup for Backup &amp; Restore (BR) tool.</li> <li>Support the built-in dashboard.service script to manage the Dashboard services with one-click and view the Dashboard version.</li> <li>Add a product feedback page.</li> </ul> </li> </ul> <ul> <li> <p>Enhancement</p> <ul> <li>Automatically detects whether the installation package is compatible with the operating system when creating a cluster.</li> <li>Support specifying the NebulaGraph installation directory when importing nodes in batches.</li> <li>Support deleting the installation directory when deleting a cluster.</li> <li>Dependent services are displayed in the importing cluster and service monitoring.</li> <li>Support canceling the alert rule silence midway.</li> <li>Support killing the Graph service processes forcibly.</li> <li>Support viewing and modifying configuration information of multiple services.</li> <li>Support modifying the configuration of the Meta service.</li> <li>Support logging update configuration and delete backup operations on operation record page.</li> <li>Support auto-registration after LDAP is enabled.</li> <li>Detail Log information of the task center.</li> <li>Display browser compatibility hint.</li> <li>NebulaGraph license expiration reminder.</li> <li>Support for Red Flag OS Asianux Linux 7 (Core).</li> <li>Optimize multiple interactions such as connecting to the database, creating a cluster, scaling and batch node importing.</li> <li>Optimize the interface error message.</li> <li>Display the names of the monitoring metrics on the overview page of <code>node</code>.</li> <li>Optimize the calculation of monitoring metrics such as <code>num_queries</code>, and adjust the display to time series aggregation.</li> </ul> </li> </ul> <ul> <li> <p>Bugfix</p> <ul> <li>Fix the bug that the selection of monitoring time range does not take effect in the overview page of service monitoring.</li> <li>Fix the bug that the corresponding NebulaGraph file is not deleted when deleting empty nodes during scale-in reduction.</li> <li>Fix the bug that the global language is switched at the same time when switching the language of the diagnosis report.</li> <li>Fix the bug that an import cluster task blocks and causes other import tasks to be in waiting state.</li> </ul> </li> </ul>"},{"location":"20.appendix/release-notes/explorer-release-note/","title":"NebulaGraph Explorer release notes","text":""},{"location":"20.appendix/release-notes/explorer-release-note/#v351","title":"v3.5.1","text":"<ul> <li> <p>Bugfix</p> <ul> <li>Fix wrong links.</li> <li>Fix wrong text.</li> <li>Remove deprecated tool components.</li> </ul> </li> </ul>"},{"location":"20.appendix/release-notes/explorer-release-note/#v350","title":"v3.5.0","text":"<ul> <li> <p>Feature</p> <ul> <li>Support for using workflows via NFS configuration.</li> <li>Allow users to personalize the product, including the page logo and product name.</li> <li>Import data supports historical task re-import, and the data source type supports <code>cloud</code> and <code>SFTP</code>.</li> <li>Support for the new License.</li> </ul> </li> </ul>"},{"location":"20.appendix/release-notes/explorer-release-note/#v340","title":"v3.4.0","text":"<ul> <li> <p>Feature</p> <ul> <li>Support viewing the creation statements of the schema.</li> <li>Add a Beta functions switch button on the global settings page.</li> <li>Add a product feedback page.</li> </ul> </li> </ul> <ul> <li> <p>Enhancement</p> <ul> <li>Remove the timeout limit for slow queries.</li> <li>Keep history on the console page after switching pages.</li> <li>Support adding comments with <code>#</code> on the console page.</li> <li>Support adding comments with <code>#</code> or <code>//</code> when creating nGQL templates.</li> <li>Update the global settings page.</li> <li>Support the visual modification of the IP whitelist.</li> <li>Show VID on canvas by default.</li> <li>Display browser compatibility hints.</li> <li>Show the kernel version in the connection information.</li> <li>Add indexes to the built-in dataset.</li> <li>Optimize the login page.</li> <li>Optimize Workflow:<ul> <li>Add algorithm descriptions.</li> <li>Optimize the parameter configurations of the graph algorithm.</li> <li>Optimize the presentation of the result.</li> </ul> </li> <li>Optimize interactions:<ul> <li>Vertex filter</li> <li>Query by tag</li> <li>Search path</li> </ul> </li> <li>Optimize presentations:<ul> <li>Optimize the presentation of schema statistics.</li> <li>Optimize the layout of force.</li> <li>Optimize the layout of the visual query results after importing them to the canvas.</li> <li>Optimize the presentation of vertices on dangling edges.</li> <li>Optimize the console page.</li> </ul> </li> <li>Optimize hints:<ul> <li>Optimize guidances.</li> <li>Optimize error messages.</li> </ul> </li> </ul> </li> </ul> <ul> <li> <p>Bugfix</p> <ul> <li>Fix the bug that can not be able to view the import task log.</li> <li>Fix the bug that some data of the edges in the <code>demo_basketballplayer</code> dataset is missing.</li> <li>Fix the crash of the page.</li> <li>Fix the bug that the results of the graph algorithm in the workflow can not show the details of vertices after importing them to canvas.</li> </ul> </li> </ul>"},{"location":"20.appendix/release-notes/nebula-comm-release-note/","title":"NebulaGraph 3.5.0 release notes","text":""},{"location":"20.appendix/release-notes/nebula-comm-release-note/#features","title":"Features","text":"<ul> <li>Support UDF. #4804 #5391</li> <li>Support expressions like <code>v.tag</code> in return statements. #5440</li> <li>Support <code>json_extract</code> function in UPDATE statements. #5457</li> <li>Support TCK format in EXPLAIN output. #5414</li> <li>DML supports parameters. #5328</li> </ul>"},{"location":"20.appendix/release-notes/nebula-comm-release-note/#optimizations","title":"Optimizations","text":"<ul> <li>Support TTL in milliseconds. #5430</li> <li>Enhance attribute trimming in aggregation functions. #5301</li> <li>Improve the performance of traversal executor. #5308</li> <li>Optimize FIND ALL PATH performance. #5409</li> <li>Removes some Raft locks to improve performance. #5451</li> <li>Optimize predicate function filtering for variable-length edges. #5464 #5470 #5481 #5503</li> <li>Parallel traversal executor. #5314</li> <li>MATCH supports ID collection. #5360</li> <li>Refactor the GO planner. #5369</li> <li>Add some Graph performance options in the configuration file. #5463</li> <li>Add maximum connection number flag. #5309 </li> </ul>"},{"location":"20.appendix/release-notes/nebula-comm-release-note/#bug_fixes","title":"Bug fixes","text":"<ul> <li>Fix the defect where RocksDB data import invalidates the leader lease.  #5271</li> <li>Fix the error message when <code>DESC USER</code> does not exist. #5345</li> <li>Fix the defect where <code>CREATE IF NOT EXIST</code> fails when SPACE exists.  #5375</li> <li>Fix the incorrect edge direction in GetNeighbors plan. #5386</li> <li>Fix the client IP format in the <code>SHOW SESSIONS</code> command. #5388</li> <li>Fix the defect where attributes are pruned in USE and MATCH. #5263</li> <li>Fix the defect where the filter is not pushed down in some cases. #5395</li> <li>Fix the defect where the filter is incorrectly filtered in some cases. #5422</li> <li>Fix the incorrect handling of internal variables in pattern expressions. #5424</li> <li>Fix defects involving EMPTY comparisons. #5433</li> <li>Fix the defect where duplicate columns are returned when all columns are requested in MATCH. #5443</li> <li>Fix the error in comparing paths involving reflexive edges. #5444</li> <li>Fix the defect of redefining aliases in a MATCH path. #5446</li> <li>Fix the type check defect when inserting geographical location values. #5460</li> <li>Fix the crash in a shortest path. #5472</li> <li>Fix the crash in GEO. #5475</li> <li>Fix the error in <code>MATCH...contains</code>. #5485</li> <li>Fix the bug of incorrect session count in concurrency. #5496</li> <li>Fix the defect of SUBGRAPH and PATH parameters. #5500</li> <li>Fix the defect in regular expressions. #5507 </li> </ul>"},{"location":"20.appendix/release-notes/nebula-comm-release-note/#changes","title":"Changes","text":"<ul> <li>Disable <code>edge list join</code>, not supporting the use of edge list in multiple patterns. #5268</li> <li>Remove GLR parser, needs to change <code>YIELD 1\u2013-1</code> to <code>YIELD 1\u2013 -1</code>. #5290</li> </ul>"},{"location":"20.appendix/release-notes/nebula-comm-release-note/#legacy_versions","title":"Legacy versions","text":"<p>Release notes of legacy versions</p>"},{"location":"20.appendix/release-notes/nebula-ent-release-note/","title":"NebulaGraph 3.5.0 release notes","text":""},{"location":"20.appendix/release-notes/nebula-ent-release-note/#features","title":"Features","text":"<ul> <li>Support managing licenses through License Center and License Manager.</li> <li>Support full table scan without index.</li> <li>Support expressions like <code>v.tag</code> in return statements.</li> <li>Support <code>json_extract</code> function in UPDATE statements.</li> <li>Support TCK format in EXPLAIN output.</li> <li>DML supports parameters.</li> <li>Enhance full-text index.</li> </ul>"},{"location":"20.appendix/release-notes/nebula-ent-release-note/#optimizations","title":"Optimizations","text":"<ul> <li>Support TTL in milliseconds.</li> <li>Enhance attribute trimming in aggregation functions.</li> <li>Improve the performance of traversal executor.</li> <li>Optimize FIND ALL PATH performance.</li> <li>Removes some Raft locks to improve performance.</li> <li>Optimize predicate function filtering for variable-length edges.</li> <li>Parallel traversal executor.</li> <li>MATCH supports ID collection.</li> <li>Refactor the GO planner.</li> <li>Add some Graph performance options in the configuration file.</li> <li>Add maximum connection number flag.</li> <li>Support variable when seeking vertex id or property index in match clause.</li> </ul>"},{"location":"20.appendix/release-notes/nebula-ent-release-note/#bug_fixes","title":"Bug fixes","text":"<ul> <li>Fix the defect where RocksDB data import invalidates the leader lease.</li> <li>Fix the error message when <code>DESC USER</code> does not exist.</li> <li>Fix the defect where <code>CREATE IF NOT EXIST</code> fails when SPACE exists.</li> <li>Fix the incorrect edge direction in GetNeighbors plan.</li> <li>Fix the client IP format in the <code>SHOW SESSIONS</code> command.</li> <li>Fix the defect where attributes are pruned in USE and MATCH.</li> <li>Fix the defect where the filter is not pushed down in some cases.</li> <li>Fix the defect where the filter is incorrectly filtered in some cases.</li> <li>Fix the incorrect handling of internal variables in pattern expressions.</li> <li>Fix defects involving EMPTY comparisons.</li> <li>Fix the defect where duplicate columns are returned when all columns are requested in MATCH.</li> <li>Fix the error in comparing paths involving reflexive edges.</li> <li>Fix the defect of redefining aliases in a MATCH path.</li> <li>Fix the type check defect when inserting geographical location values.</li> <li>Fix the crash in a shortest path.</li> <li>Fix the crash in GEO.</li> <li>Fix the bug that caused storage crash during logical expression evaluation.</li> <li>Fix the error in <code>MATCH...contains</code>.</li> <li>Fix the bug of incorrect session count in concurrency.</li> <li>Fix the defect of SUBGRAPH and PATH parameters.</li> <li>Fix the defect in regular expressions.</li> <li>Fix the issue with non-expression pushing down.</li> <li>Fixed the bug of slaving cluster.</li> </ul>"},{"location":"20.appendix/release-notes/nebula-ent-release-note/#changes","title":"Changes","text":"<ul> <li>Disable <code>edge list join</code>, not supporting the use of edge list in multiple patterns.</li> <li>Remove GLR parser, needs to change <code>YIELD 1\u2013-1</code> to <code>YIELD 1\u2013 -1</code>.</li> </ul>"},{"location":"20.appendix/release-notes/nebula-ent-release-note/#legacy_versions","title":"Legacy versions","text":"<p>Release notes of legacy versions</p>"},{"location":"20.appendix/release-notes/studio-release-note/","title":"NebulaGraph Studio release notes","text":""},{"location":"20.appendix/release-notes/studio-release-note/#v360","title":"v3.6.0","text":"<ul> <li>Feature<ul> <li>Support viewing the creation statements of the schema.</li> <li>Add a product feedback page.</li> </ul> </li> </ul> <ul> <li>Enhancement<ul> <li>Remove the timeout limit for slow queries.</li> <li>Display browser compatibility hints.</li> <li>Optimize the login page.</li> <li>Support adding comments with <code>#</code> on the console page.</li> <li>Optimize the console page.</li> </ul> </li> </ul> <ul> <li> <p>Bugfix</p> <ul> <li>Fix the bug that the list has not been refreshed after uploading files.</li> <li>Fix the invalid error message of the schema drafting.</li> <li>Fix the bug that the view schema data has not been cleared after switching the login user.</li> <li>Fix the presentation problem of the thumbnail in the schema drafting.</li> </ul> </li> </ul>"},{"location":"3.ngql-guide/4.job-statements/","title":"Job manager and the JOB statements","text":"<p>The long-term tasks run by the Storage Service are called jobs, such as <code>COMPACT</code>, <code>FLUSH</code>, and <code>STATS</code>. These jobs can be time-consuming if the data amount in the graph space is large. The job manager helps you run, show, stop, and recover jobs.</p> <p>Note</p> <p>All job management commands can be executed only after selecting a graph space.</p>"},{"location":"3.ngql-guide/4.job-statements/#submit_job_balance_data","title":"SUBMIT JOB BALANCE DATA","text":"<p>Enterpriseonly</p> <p>Only available for the NebulaGraph Enterprise Edition.</p> <p>Caution</p> <ul> <li>Before performing the job, it is recommended to create a snapshot.</li> <li>During job execution, do not execute other jobs, such as <code>SUBMIT JOB STATS</code>, <code>REBUILD INDEX</code>, etc.</li> <li>During job execution, it is recommended not to write or read data in large batches.</li> </ul> <p>The <code>SUBMIT JOB BALANCE DATA</code> statement starts a job to balance the distribution of storage partitions in the current graph space. It returns the job ID.</p> <p>For example:</p> <pre><code>nebula&gt; SUBMIT JOB BALANCE DATA;\n+------------+\n| New Job Id |\n+------------+\n| 28         |\n+------------+\n</code></pre>"},{"location":"3.ngql-guide/4.job-statements/#submit_job_balance_data_remove","title":"SUBMIT JOB BALANCE DATA REMOVE","text":"<p>Enterpriseonly</p> <p>Only available for the NebulaGraph Enterprise Edition.</p> <p>Starts a job to balance the distribution of storage partitions in the current graph space. The default port is <code>9779</code>. It returns the job ID.</p> <p>For example:</p> <pre><code>nebula&gt; SUBMIT JOB BALANCE DATA REMOVE 192.168.8.100:9779;\n+------------+\n| New Job Id |\n+------------+\n| 29         |\n+------------+\n</code></pre>"},{"location":"3.ngql-guide/4.job-statements/#submit_job_balance_leader","title":"SUBMIT JOB BALANCE LEADER","text":"<p>Starts a job to balance the distribution of all the storage leaders in all graph spaces. It returns the job ID.</p> <p>For example:</p> <pre><code>nebula&gt; SUBMIT JOB BALANCE LEADER;\n+------------+\n| New Job Id |\n+------------+\n| 33         |\n+------------+\n</code></pre>"},{"location":"3.ngql-guide/4.job-statements/#submit_job_compact","title":"SUBMIT JOB COMPACT","text":"<p>The <code>SUBMIT JOB COMPACT</code> statement triggers the long-term RocksDB <code>compact</code> operation in the current graph space.</p> <p>For more information about <code>compact</code> configuration, see Storage Service configuration.</p> <p>For example:</p> <pre><code>nebula&gt; SUBMIT JOB COMPACT;\n+------------+\n| New Job Id |\n+------------+\n| 40         |\n+------------+\n</code></pre>"},{"location":"3.ngql-guide/4.job-statements/#submit_job_flush","title":"SUBMIT JOB FLUSH","text":"<p>The <code>SUBMIT JOB FLUSH</code> statement writes the RocksDB memfile in the memory to the hard disk in the current graph space.</p> <p>For example:</p> <pre><code>nebula&gt; SUBMIT JOB FLUSH;\n+------------+\n| New Job Id |\n+------------+\n| 96         |\n+------------+\n</code></pre>"},{"location":"3.ngql-guide/4.job-statements/#submit_job_stats","title":"SUBMIT JOB STATS","text":"<p>The <code>SUBMIT JOB STATS</code> statement starts a job that makes the statistics of the current graph space. Once this job succeeds, you can use the <code>SHOW STATS</code> statement to list the statistics. For more information, see SHOW STATS.</p> <p>Note</p> <p>If the data stored in the graph space changes, in order to get the latest statistics, you have to run <code>SUBMIT JOB STATS</code> again.</p> <p>For example:</p> <pre><code>nebula&gt; SUBMIT JOB STATS;\n+------------+\n| New Job Id |\n+------------+\n| 9          |\n+------------+\n</code></pre>"},{"location":"3.ngql-guide/4.job-statements/#submit_job_downloadingest","title":"SUBMIT JOB DOWNLOAD/INGEST","text":"<p>The <code>SUBMIT JOB DOWNLOAD HDFS</code> and <code>SUBMIT JOB INGEST</code> commands are used to import the SST file into NebulaGraph. For detail, see Import data from SST files.</p> <p>The <code>SUBMIT JOB DOWNLOAD HDFS</code> command will download the SST file on the specified HDFS.</p> <p>The <code>SUBMIT JOB INGEST</code> command will import the downloaded SST file into NebulaGraph.</p> <p>For example:</p> <pre><code>nebula&gt; SUBMIT JOB DOWNLOAD HDFS \"hdfs://192.168.10.100:9000/sst\";\n+------------+\n| New Job Id |\n+------------+\n| 10         |\n+------------+\nnebula&gt; SUBMIT JOB INGEST;\n+------------+\n| New Job Id |\n+------------+\n| 11         |\n+------------+\n</code></pre>"},{"location":"3.ngql-guide/4.job-statements/#show_job","title":"SHOW JOB","text":"<p>The Meta Service parses a <code>SUBMIT JOB</code> request into multiple tasks and assigns them to the nebula-storaged processes. The <code>SHOW JOB &lt;job_id&gt;</code> statement shows the information about a specific job and all its tasks in the current graph space.</p> <p><code>job_id</code> is returned when you run the <code>SUBMIT JOB</code> statement.</p> <p>For example:</p> <pre><code>nebula&gt; SHOW JOB 9;\n+----------------+-----------------+------------+----------------------------+----------------------------+-------------+\n| Job Id(TaskId) | Command(Dest)   | Status     | Start Time                 | Stop Time                  | Error Code  |\n+----------------+-----------------+------------+----------------------------+----------------------------+-------------+\n| 8              | \"STATS\"         | \"FINISHED\" | 2022-10-18T08:14:45.000000 | 2022-10-18T08:14:45.000000 | \"SUCCEEDED\" |\n| 0              | \"192.168.8.129\" | \"FINISHED\" | 2022-10-18T08:14:45.000000 | 2022-10-18T08:15:13.000000 | \"SUCCEEDED\" |\n| \"Total:1\"      | \"Succeeded:1\"   | \"Failed:0\" | \"In Progress:0\"            | \"\"                         | \"\"          |\n+----------------+-----------------+------------+----------------------------+----------------------------+-------------+\n</code></pre> <p>The descriptions are as follows.</p> Parameter Description <code>Job Id(TaskId)</code> The first row shows the job ID and the other rows show the task IDs and the last row shows the total number of job-related tasks. <code>Command(Dest)</code> The first row shows the command executed and the other rows show on which storaged processes the task is running. The last row shows the number of successful tasks related to the job. <code>Status</code> Shows the status of the job or task. The last row shows the number of failed tasks related to the job. For more information, see Job status. <code>Start Time</code> Shows a timestamp indicating the time when the job or task enters the <code>RUNNING</code> phase. The last row shows the number of ongoing tasks related to the job. <code>Stop Time</code> Shows a timestamp indicating the time when the job or task gets <code>FINISHED</code>, <code>FAILED</code>, or <code>STOPPED</code>. <code>Error Code</code> The error code of job."},{"location":"3.ngql-guide/4.job-statements/#job_status","title":"Job status","text":"<p>The descriptions are as follows.</p> Status Description QUEUE The job or task is waiting in a queue. The <code>Start Time</code> is empty in this phase. RUNNING The job or task is running. The <code>Start Time</code> shows the beginning time of this phase. FINISHED The job or task is successfully finished. The <code>Stop Time</code> shows the time when the job or task enters this phase. FAILED The job or task has failed. The <code>Stop Time</code> shows the time when the job or task enters this phase. STOPPED The job or task is stopped without running. The <code>Stop Time</code> shows the time when the job or task enters this phase. REMOVED The job or task is removed. <p>The description of switching the status is described as follows.</p> <pre><code>Queue -- running -- finished -- removed\n     \\          \\                /\n      \\          \\ -- failed -- /\n       \\          \\            /\n        \\ ---------- stopped -/\n</code></pre>"},{"location":"3.ngql-guide/4.job-statements/#show_jobs","title":"SHOW JOBS","text":"<p>The <code>SHOW JOBS</code> statement lists all the unexpired jobs in the current graph space.</p> <p>The default job expiration interval is one week. You can change it by modifying the <code>job_expired_secs</code> parameter of the Meta Service. For how to modify <code>job_expired_secs</code>, see Meta Service configuration.</p> <p>For example:</p> <pre><code>nebula&gt; SHOW JOBS;\n+--------+---------------------+------------+----------------------------+----------------------------+\n| Job Id | Command             | Status     | Start Time                 | Stop Time                  |\n+--------+---------------------+------------+----------------------------+----------------------------+\n| 34     | \"STATS\"             | \"FINISHED\" | 2021-11-01T03:32:27.000000 | 2021-11-01T03:32:27.000000 |\n| 33     | \"FLUSH\"             | \"FINISHED\" | 2021-11-01T03:32:15.000000 | 2021-11-01T03:32:15.000000 |\n| 32     | \"COMPACT\"           | \"FINISHED\" | 2021-11-01T03:32:06.000000 | 2021-11-01T03:32:06.000000 |\n| 31     | \"REBUILD_TAG_INDEX\" | \"FINISHED\" | 2021-10-29T05:39:16.000000 | 2021-10-29T05:39:17.000000 |\n| 10     | \"COMPACT\"           | \"FINISHED\" | 2021-10-26T02:27:05.000000 | 2021-10-26T02:27:05.000000 |\n+--------+---------------------+------------+----------------------------+----------------------------+\n</code></pre>"},{"location":"3.ngql-guide/4.job-statements/#stop_job","title":"STOP JOB","text":"<p>The <code>STOP JOB &lt;job_id&gt;</code> statement stops jobs that are not finished in the current graph space.</p> <p>For example:</p> <pre><code>nebula&gt; STOP JOB 22;\n+---------------+\n| Result        |\n+---------------+\n| \"Job stopped\" |\n+---------------+\n</code></pre>"},{"location":"3.ngql-guide/4.job-statements/#recover_job","title":"RECOVER JOB","text":"<p>The <code>RECOVER JOB [&lt;job_id&gt;]</code> statement re-executes the jobs that status is <code>FAILED</code> or <code>STOPPED</code> in the current graph space and returns the number of recovered jobs. If <code>&lt;job_id&gt;</code> is not specified, re-execution is performed from the earliest job and the number of jobs that have been recovered is returned.</p> <p>For example:</p> <pre><code>nebula&gt; RECOVER JOB;\n+-------------------+\n| Recovered job num |\n+-------------------+\n| 5 job recovered   |\n+-------------------+\n</code></pre>"},{"location":"3.ngql-guide/4.job-statements/#faq","title":"FAQ","text":""},{"location":"3.ngql-guide/4.job-statements/#how_to_troubleshoot_job_problems","title":"How to troubleshoot job problems?","text":"<p>The <code>SUBMIT JOB</code> operations use the HTTP port. Please check if the HTTP ports on the machines where the Storage Service is running are working well. You can use the following command to debug.</p> <pre><code>curl \"http://{storaged-ip}:19779/admin?space={space_name}&amp;op=compact\"\n</code></pre>"},{"location":"3.ngql-guide/1.nGQL-overview/1.overview/","title":"NebulaGraph Query Language (nGQL)","text":"<p>This topic gives an introduction to the query language of NebulaGraph, nGQL.</p>"},{"location":"3.ngql-guide/1.nGQL-overview/1.overview/#what_is_ngql","title":"What is nGQL","text":"<p>nGQL is a declarative graph query language for NebulaGraph. It allows expressive and efficient graph patterns. nGQL is designed for both developers and operations professionals. nGQL is an SQL-like query language, so it's easy to learn.</p> <p>nGQL is a project in progress. New features and optimizations are done steadily. There can be differences between syntax and implementation. Submit an issue to inform the NebulaGraph team if you find a new issue of this type. NebulaGraph 3.0 or later releases will support openCypher 9.</p>"},{"location":"3.ngql-guide/1.nGQL-overview/1.overview/#what_can_ngql_do","title":"What can nGQL do","text":"<ul> <li>Supports graph traversals</li> <li>Supports pattern match</li> <li>Supports aggregation</li> <li>Supports graph mutation</li> <li>Supports access control</li> <li>Supports composite queries</li> <li>Supports index</li> <li>Supports most openCypher 9 graph query syntax (but mutations and controls syntax are not supported)</li> </ul>"},{"location":"3.ngql-guide/1.nGQL-overview/1.overview/#example_data_basketballplayer","title":"Example data Basketballplayer","text":"<p>Users can download the example data Basketballplayer in NebulaGraph. After downloading the example data, you can import it to NebulaGraph by using the <code>-f</code> option in NebulaGraph Console.</p> <p>Note</p> <p>Ensure that you have executed the <code>ADD HOSTS</code> command to add the Storage service to your NebulaGraph cluster before importing the example data. For more information, see Manage Storage hosts.</p>"},{"location":"3.ngql-guide/1.nGQL-overview/1.overview/#placeholder_identifiers_and_values","title":"Placeholder identifiers and values","text":"<p>Refer to the following standards in nGQL:</p> <ul> <li>(Draft) ISO/IEC JTC1 N14279 SC 32 - Database_Languages - GQL</li> </ul> <ul> <li>(Draft) ISO/IEC JTC1 SC32 N3228 - SQL_Property_Graph_Queries - SQLPGQ</li> </ul> <ul> <li>OpenCypher 9</li> </ul> <p>In template code, any token that is not a keyword, a literal value, or punctuation is a placeholder identifier or a placeholder value.</p> <p>For details of the symbols in nGQL syntax, see the following table:</p> Token Meaning &lt; &gt; name of a syntactic element : formula that defines an element [ ] optional elements { } explicitly specified elements | complete alternative elements ... may be repeated any number of times <p>For example, create vertices in nGQL syntax:</p> <pre><code>INSERT VERTEX [IF NOT EXISTS] [tag_props, [tag_props] ...]\nVALUES &lt;vid&gt;: ([prop_value_list])\ntag_props:\n  tag_name ([prop_name_list])\nprop_name_list:\n   [prop_name [, prop_name] ...]\nprop_value_list:\n   [prop_value [, prop_value] ...]  \n</code></pre> <p>Example statement:</p> <pre><code>nebula&gt; CREATE TAG IF NOT EXISTS player(name string, age int);\n</code></pre>"},{"location":"3.ngql-guide/1.nGQL-overview/1.overview/#about_opencypher_compatibility","title":"About openCypher compatibility","text":""},{"location":"3.ngql-guide/1.nGQL-overview/1.overview/#native_ngql_and_opencypher","title":"Native nGQL and openCypher","text":"<p>Native nGQL is the part of a graph query language designed and implemented by NebulaGraph. OpenCypher is a graph query language maintained by openCypher Implementers Group.</p> <p>The latest release is openCypher 9. The compatible parts of openCypher in nGQL are called openCypher compatible sentences (short as openCypher).</p> <p>Note</p> <p><code>nGQL</code> = <code>native nGQL</code> + <code>openCypher compatible sentences</code></p>"},{"location":"3.ngql-guide/1.nGQL-overview/1.overview/#is_ngql_compatible_with_opencypher_9_completely","title":"Is nGQL compatible with openCypher 9 completely?","text":"<p>NO.</p> <p>Compatibility with openCypher</p> <p>nGQL is designed to be compatible with part of DQL (match, optional match, with, etc.).</p> <ul> <li>It is not planned to be compatible with any DDL, DML, or DCL.</li> <li>It is not planned to be compatible with the Bolt Protocol.</li> <li>It is not planned to be compatible with APOC and GDS.</li> </ul> <p>Users can search in this manual with the keyword <code>compatibility</code> to find major compatibility issues.  </p> <p>Multiple known incompatible items are listed in NebulaGraph Issues. Submit an issue with the <code>incompatible</code> tag if you find a new issue of this type. </p>"},{"location":"3.ngql-guide/1.nGQL-overview/1.overview/#what_are_the_major_differences_between_ngql_and_opencypher_9","title":"What are the major differences between nGQL and openCypher 9?","text":"<p>The following are some major differences (by design incompatible) between nGQL and openCypher.</p> Category openCypher 9 nGQL Schema Optional Schema Strong Schema Equality operator <code>=</code> <code>==</code> Math exponentiation <code>^</code> <code>^</code> is not supported. Use pow(x, y) instead. Edge rank No such concept. edge rank (reference by @) Statement - All DMLs (<code>CREATE</code>, <code>MERGE</code>, etc) of openCypher 9. Label and tag A label is used for searching a vertex, namely an index of vertex. A tag defines the type of a vertex and its corresponding properties. It cannot be used as an index. Pre-compiling and parameterized queries Support Parameterized queries are supported, but precompiling is not. <p>Compatibility</p> <p>OpenCypher 9 and Cypher have some differences in grammar and licence. For example,</p> <ol> <li> <p>Cypher requires that All Cypher statements are explicitly run within a transaction. While openCypher has no such requirement. And nGQL does not support transactions.</p> </li> <li> <p>Cypher has a variety of constraints, including Unique node property constraints, Node property existence constraints, Relationship property existence constraints, and Node key constraints. While OpenCypher has no such constraints. As a strong schema system, most of the constraints mentioned above can be solved through schema definitions (including NOT NULL) in nGQL. The only function that cannot be supported is the UNIQUE constraint.</p> </li> <li> <p>Cypher has APoC, while openCypher 9 does not have APoC. Cypher has Blot protocol support requirements, while openCypher 9 does not.</p> </li> </ol>"},{"location":"3.ngql-guide/1.nGQL-overview/1.overview/#where_can_i_find_more_ngql_examples","title":"Where can I find more nGQL examples?","text":"<p>Users can find more than 2500 nGQL examples in the features directory on the NebulaGraph GitHub page.</p> <p>The <code>features</code> directory consists of <code>.feature</code> files. Each file records scenarios that you can use as nGQL examples. Here is an example:</p> <pre><code>Feature: Basic match\n\n  Background:\n    Given a graph with space named \"basketballplayer\"\n\n  Scenario: Single node\n    When executing query:\n      \"\"\"\n      MATCH (v:player {name: \"Yao Ming\"}) RETURN v;\n      \"\"\"\n    Then the result should be, in any order, with relax comparison:\n      | v                                                |\n      | (\"player133\" :player{age: 38, name: \"Yao Ming\"}) |\n\n  Scenario: One step\n    When executing query:\n      \"\"\"\n      MATCH (v1:player{name: \"LeBron James\"}) -[r]-&gt; (v2)\n      RETURN type(r) AS Type, v2.player.name AS Name\n      \"\"\"\n    Then the result should be, in any order:\n\n      | Type     | Name        |\n      | \"follow\" | \"Ray Allen\" |\n      | \"serve\"  | \"Lakers\"    |\n      | \"serve\"  | \"Heat\"      |\n      | \"serve\"  | \"Cavaliers\" |\n\nFeature:  Comparison of where clause\n\n  Background:\n    Given a graph with space named \"basketballplayer\"\n\n    Scenario: push edge props filter down\n      When profiling query:\n        \"\"\"\n        GO FROM \"player100\" OVER follow \n        WHERE properties(edge).degree IN [v IN [95,99] WHERE v &gt; 0] \n        YIELD dst(edge), properties(edge).degree\n        \"\"\"\n      Then the result should be, in any order:\n        | follow._dst | follow.degree |\n        | \"player101\" | 95            |\n        | \"player125\" | 95            |\n      And the execution plan should be:\n        | id | name         | dependencies | operator info                                               |\n        | 0  | Project      | 1            |                                                             |\n        | 1  | GetNeighbors | 2            | {\"filter\": \"(properties(edge).degree IN [v IN [95,99] WHERE (v&gt;0)])\"} |\n        | 2  | Start        |              |                                                             |\n</code></pre> <p>The keywords in the preceding example are described as follows.</p> Keyword Description <code>Feature</code> Describes the topic of the current <code>.feature</code> file. <code>Background</code> Describes the background information of the current <code>.feature</code> file. <code>Given</code> Describes the prerequisites of running the test statements in the current <code>.feature</code> file. <code>Scenario</code> Describes the scenarios. If there is the <code>@skip</code> before one <code>Scenario</code>, this scenario may not work and do not use it as a working example in a production environment. <code>When</code> Describes the nGQL statement to be executed. It can be a <code>executing query</code> or <code>profiling query</code>. <code>Then</code> Describes the expected return results of running the statement in the <code>When</code> clause. If the return results in your environment do not match the results described in the <code>.feature</code> file, submit an issue to inform the NebulaGraph team. <code>And</code> Describes the side effects of running the statement in the <code>When</code> clause. <code>@skip</code> This test case will be skipped. Commonly, the to-be-tested code is not ready. <p>Welcome to add more tck case and return automatically to the using statements in CI/CD.</p>"},{"location":"3.ngql-guide/1.nGQL-overview/1.overview/#does_it_support_tinkerpop_gremlin","title":"Does it support TinkerPop Gremlin?","text":"<p>No. And no plan to support that.</p>"},{"location":"3.ngql-guide/1.nGQL-overview/1.overview/#does_nebulagraph_support_w3c_rdf_sparql_or_graphql","title":"Does NebulaGraph support W3C RDF (SPARQL) or GraphQL?","text":"<p>No. And no plan to support that.</p> <p>The data model of NebulaGraph is the property graph. And as a strong schema system, NebulaGraph does not support RDF.</p> <p>NebulaGraph Query Language does not support <code>SPARQL</code> nor <code>GraphQL</code>.</p>"},{"location":"3.ngql-guide/1.nGQL-overview/3.graph-patterns/","title":"Patterns","text":"<p>Patterns and graph pattern matching are the very heart of a graph query language. This topic will describe the patterns in NebulaGraph, some of which have not yet been implemented.</p>"},{"location":"3.ngql-guide/1.nGQL-overview/3.graph-patterns/#patterns_for_vertices","title":"Patterns for vertices","text":"<p>A vertex is described using a pair of parentheses and is typically given a name. For example:</p> <pre><code>(a)\n</code></pre> <p>This simple pattern describes a single vertex and names that vertex using the variable <code>a</code>.</p>"},{"location":"3.ngql-guide/1.nGQL-overview/3.graph-patterns/#patterns_for_related_vertices","title":"Patterns for related vertices","text":"<p>A more powerful construct is a pattern that describes multiple vertices and edges between them. Patterns describe an edge by employing an arrow between two vertices. For example:</p> <pre><code>(a)-[]-&gt;(b)\n</code></pre> <p>This pattern describes a very simple data structure: two vertices and a single edge from one to the other. In this example, the two vertices are named as <code>a</code> and <code>b</code> respectively and the edge is <code>directed</code>: it goes from <code>a</code> to <code>b</code>.</p> <p>This manner of describing vertices and edges can be extended to cover an arbitrary number of vertices and the edges between them, for example:</p> <pre><code>(a)-[]-&gt;(b)&lt;-[]-(c)\n</code></pre> <p>Such a series of connected vertices and edges is called a <code>path</code>.</p> <p>Note that the naming of the vertices in these patterns is only necessary when one needs to refer to the same vertex again, either later in the pattern or elsewhere in the query. If not, the name may be omitted as follows:</p> <pre><code>(a)-[]-&gt;()&lt;-[]-(c)\n</code></pre>"},{"location":"3.ngql-guide/1.nGQL-overview/3.graph-patterns/#patterns_for_tags","title":"Patterns for tags","text":"<p>Note</p> <p>The concept of <code>tag</code> in nGQL has a few differences from that of <code>label</code> in openCypher. For example, users must create a <code>tag</code> before using it. And a <code>tag</code> also defines the type of properties.</p> <p>In addition to simply describing the vertices in the graphs, patterns can also describe the tags of the vertices. For example:</p> <pre><code>(a:User)-[]-&gt;(b)\n</code></pre> <p>Patterns can also describe a vertex that has multiple tags. For example:</p> <pre><code>(a:User:Admin)-[]-&gt;(b)\n</code></pre>"},{"location":"3.ngql-guide/1.nGQL-overview/3.graph-patterns/#patterns_for_properties","title":"Patterns for properties","text":"<p>Vertices and edges are the fundamental elements in a graph. In nGQL, properties are added to them for richer models.</p> <p>In the patterns, the properties can be expressed as follows: some key-value pairs are enclosed in curly brackets and separated by commas, and the tag or edge type to which a property belongs must be specified.</p> <p>For example, a vertex with two properties will be like:</p> <pre><code>(a:player{name: \"Tim Duncan\", age: 42})\n</code></pre> <p>One of the edges that connect to this vertex can be like:</p> <pre><code>(a)-[e:follow{degree: 95}]-&gt;(b)\n</code></pre>"},{"location":"3.ngql-guide/1.nGQL-overview/3.graph-patterns/#patterns_for_edges","title":"Patterns for edges","text":"<p>The simplest way to describe an edge is by using the arrow between two vertices, as in the previous examples.</p> <p>Users can describe an edge and its direction using the following statement. If users do not care about its direction, the arrowhead can be omitted. For example:</p> <pre><code>(a)-[]-(b)\n</code></pre> <p>Like vertices, edges can also be named. A pair of square brackets will be used to separate the arrow and the variable will be placed between them. For example:</p> <pre><code>(a)-[r]-&gt;(b)\n</code></pre> <p>Like the tags on vertices, edges can also have types. To describe an edge with a specific type, use the pattern as follows:</p> <pre><code>(a)-[r:REL_TYPE]-&gt;(b)\n</code></pre> <p>An edge can only have one edge type. But if we'd like to describe some data such that the edge could have a set of types, then they can all be listed in the pattern, separating them with the pipe symbol <code>|</code> like this:</p> <pre><code>(a)-[r:TYPE1|TYPE2]-&gt;(b)\n</code></pre> <p>Like vertices, the name of an edge can be omitted. For example:</p> <pre><code>(a)-[:REL_TYPE]-&gt;(b)\n</code></pre>"},{"location":"3.ngql-guide/1.nGQL-overview/3.graph-patterns/#variable-length_pattern","title":"Variable-length pattern","text":"<p>Rather than describing a long path using a sequence of many vertex and edge descriptions in a pattern, many edges (and the intermediate vertices) can be described by specifying a length in the edge description of a pattern. For example:</p> <pre><code>(a)-[*2]-&gt;(b)\n</code></pre> <p>The following pattern describes a graph of three vertices and two edges, all in one path (a path of length 2). It is equivalent to:</p> <pre><code>(a)-[]-&gt;()-[]-&gt;(b)\n</code></pre> <p>The range of lengths can also be specified. Such edge patterns are called <code>variable-length edges</code>. For example:</p> <pre><code>(a)-[*3..5]-&gt;(b)\n</code></pre> <p>The preceding example defines a path with a minimum length of 3 and a maximum length of 5.</p> <p>It describes a graph of either 4 vertices and 3 edges, 5 vertices and 4 edges, or 6 vertices and 5 edges, all connected in a single path.</p> <p>The lower bound can be omitted. For example, to describe paths of length 5 or less, use:</p> <pre><code>(a)-[*..5]-&gt;(b)\n</code></pre> <p>Note</p> <p>The upper bound must be specified. The following are NOT accepted.</p> <pre><code>(a)-[*3..]-&gt;(b)\n(a)-[*]-&gt;(b)\n</code></pre>"},{"location":"3.ngql-guide/1.nGQL-overview/3.graph-patterns/#assigning_to_path_variables","title":"Assigning to path variables","text":"<p>As described above, a series of connected vertices and edges is called a <code>path</code>. nGQL allows paths to be named using variables. For example:</p> <pre><code>p = (a)-[*3..5]-&gt;(b)\n</code></pre> <p>Users can do this in the <code>MATCH</code> statement.</p>"},{"location":"3.ngql-guide/1.nGQL-overview/comments/","title":"Comments","text":"<p>This topic will describe the comments in nGQL.</p> <p>Legacy version compatibility</p> <ul> <li>In NebulaGraph 1.x, there are four comment styles: <code>#</code>, <code>--</code>, <code>//</code>, <code>/* */</code>.</li> <li>Since NebulaGraph 2.x, <code>--</code> cannot be used as comments.</li> </ul>"},{"location":"3.ngql-guide/1.nGQL-overview/comments/#examples","title":"Examples","text":"<pre><code>nebula&gt; # Do nothing in this line\nnebula&gt; RETURN 1+1;     # This comment continues to the end of this line.\nnebula&gt; RETURN 1+1;     // This comment continues to the end of this line.\nnebula&gt; RETURN 1 /* This is an in-line comment. */ + 1 == 2;\nnebula&gt; RETURN 11 +            \\\n/* Multi-line comment.       \\\nUse a backslash as a line break.   \\\n*/ 12;\n</code></pre> <p>In nGQL statement, the backslash <code>\\</code> in a line indicates a line break.</p>"},{"location":"3.ngql-guide/1.nGQL-overview/comments/#opencypher_compatibility","title":"OpenCypher compatibility","text":"<ul> <li>In nGQL, you must add a <code>\\</code> at the end of every line, even in multi-line comments <code>/* */</code>.</li> <li>In openCypher, there is no need to use a <code>\\</code> as a line break.</li> </ul> <pre><code>/* openCypher style:\nThe following comment\nspans more than\none line */\nMATCH (n:label)\nRETURN n;\n</code></pre> <pre><code>/* nGQL style:  \\\nThe following comment       \\\nspans more than     \\\none line */       \\\nMATCH (n:tag) \\\nRETURN n;\n</code></pre>"},{"location":"3.ngql-guide/1.nGQL-overview/identifier-case-sensitivity/","title":"Identifier case sensitivity","text":""},{"location":"3.ngql-guide/1.nGQL-overview/identifier-case-sensitivity/#identifiers_are_case-sensitive","title":"Identifiers are Case-Sensitive","text":"<p>The following statements will not work because they refer to two different spaces, i.e. <code>my_space</code> and <code>MY_SPACE</code>.</p> <pre><code>nebula&gt; CREATE SPACE IF NOT EXISTS my_space (vid_type=FIXED_STRING(30));\nnebula&gt; use MY_SPACE;\n[ERROR (-1005)]: SpaceNotFound:\n</code></pre>"},{"location":"3.ngql-guide/1.nGQL-overview/identifier-case-sensitivity/#keywords_and_reserved_words_are_case-insensitive","title":"Keywords and Reserved Words are Case-Insensitive","text":"<p>The following statements are equivalent since <code>show</code> and <code>spaces</code> are keywords.</p> <pre><code>nebula&gt; show spaces;  \nnebula&gt; SHOW SPACES;\nnebula&gt; SHOW spaces;\nnebula&gt; show SPACES;\n</code></pre>"},{"location":"3.ngql-guide/1.nGQL-overview/identifier-case-sensitivity/#functions_are_case-insensitive","title":"Functions are Case-Insensitive","text":"<p>Functions are case-insensitive. For example, <code>count()</code>, <code>COUNT()</code>, and <code>couNT()</code> are equivalent.</p> <pre><code>nebula&gt; WITH [NULL, 1, 1, 2, 2] As a \\\n        UNWIND a AS b \\\n        RETURN count(b), COUNT(*), couNT(DISTINCT b);\n+----------+----------+-------------------+\n| count(b) | COUNT(*) | couNT(distinct b) |\n+----------+----------+-------------------+\n| 4        | 5        | 2                 |\n+----------+----------+-------------------+\n</code></pre>"},{"location":"3.ngql-guide/1.nGQL-overview/keywords-and-reserved-words/","title":"Keywords","text":"<p>Keywords have significance in nGQL. It can be classified into reserved keywords and non-reserved keywords. It is not recommend to use keywords in schema.</p> <p>If you must use keywords in schema:</p> <ul> <li>Non-reserved keywords can be used as identifiers without quotes if they are all in lowercase. However, if a non-reserved keyword contains any uppercase letters when used as an identifier, it must be enclosed in backticks (`), for example, `Comment`.</li> </ul> <ul> <li>To use special characters or reserved keywords as identifiers, quote them with backticks such as <code>AND</code>.</li> </ul> <p>Note</p> <p>Keywords are case-insensitive.</p> <pre><code>nebula&gt; CREATE TAG TAG(name string);\n[ERROR (-1004)]: SyntaxError: syntax error near `TAG'\n\nnebula&gt; CREATE TAG `TAG` (name string);\nExecution succeeded\n\nnebula&gt; CREATE TAG SPACE(name string);\nExecution succeeded\n\nnebula&gt; CREATE TAG \u4e2d\u6587(\u7b80\u4f53 string);\nExecution succeeded\n\nnebula&gt; CREATE TAG `\uffe5%special characters&amp;*+-*/` (`q~\uff01\uff08\uff09=  wer` string);\nExecution succeeded\n</code></pre>"},{"location":"3.ngql-guide/1.nGQL-overview/keywords-and-reserved-words/#reserved_keywords","title":"Reserved keywords","text":"<pre><code>ACROSS\nADD\nALTER\nAND\nAS\nASC\nASCENDING\nBALANCE\nBOOL\nBY\nCASE\nCHANGE\nCOMPACT\nCREATE\nDATE\nDATETIME\nDELETE\nDESC\nDESCENDING\nDESCRIBE\nDISTINCT\nDOUBLE\nDOWNLOAD\nDROP\nDURATION\nEDGE\nEDGES\nEXISTS\nEXPLAIN\nFETCH\nFIND\nFIXED_STRING\nFLOAT\nFLUSH\nFORMAT\nFROM\nGET\nGO\nGRANT\nIF\nIGNORE_EXISTED_INDEX\nIN\nINDEX\nINDEXES\nINGEST\nINSERT\nINT\nINT16\nINT32\nINT64\nINT8\nINTERSECT\nIS\nLIMIT\nLIST\nLOOKUP\nMAP\nMATCH\nMINUS\nNO\nNOT\nNOT_IN\nNULL\nOF\nOFFSET\nON\nOR\nORDER\nOVER\nOVERWRITE\nPROFILE\nPROP\nREBUILD\nRECOVER\nREMOVE\nRESTART\nRETURN\nREVERSELY\nREVOKE\nSET\nSHOW\nSTEP\nSTEPS\nSTOP\nSTRING\nSUBMIT\nTAG\nTAGS\nTIME\nTIMESTAMP\nTO\nUNION\nUPDATE\nUPSERT\nUPTO\nUSE\nVERTEX\nVERTICES\nWHEN\nWHERE\nWITH\nXOR\nYIELD\n</code></pre>"},{"location":"3.ngql-guide/1.nGQL-overview/keywords-and-reserved-words/#non-reserved_keywords","title":"Non-reserved keywords","text":"<pre><code>ACCOUNT\nADMIN\nALL\nANY\nATOMIC_EDGE\nAUTO\nBIDIRECT\nBOTH\nCHARSET\nCLIENTS\nCOLLATE\nCOLLATION\nCOMMENT\nCONFIGS\nCONTAINS\nDATA\nDBA\nDEFAULT\nELASTICSEARCH\nELSE\nEND\nENDS\nENDS_WITH\nFORCE\nFULLTEXT\nFUZZY\nGOD\nGRAPH\nGROUP\nGROUPS\nGUEST\nHDFS\nHOST\nHOSTS\nINTO\nIS_EMPTY\nIS_NOT_EMPTY\nIS_NOT_NULL\nIS_NULL\nJOB\nJOBS\nKILL\nLEADER\nLISTENER\nMETA\nNOLOOP\nNONE\nNOT_CONTAINS\nNOT_ENDS_WITH\nNOT_STARTS_WITH\nOPTIONAL\nOUT\nPART\nPARTITION_NUM\nPARTS\nPASSWORD\nPATH\nPLAN\nPREFIX\nQUERIES\nQUERY\nREDUCE\nREGEXP\nREPLICA_FACTOR\nRESET\nROLE\nROLES\nSAMPLE\nSEARCH\nSERVICE\nSESSION\nSESSIONS\nSHORTEST\nSIGN\nSINGLE\nSKIP\nSNAPSHOT\nSNAPSHOTS\nSPACE\nSPACES\nSTARTS\nSTARTS_WITH\nSTATS\nSTATUS\nSTORAGE\nSUBGRAPH\nTEXT\nTEXT_SEARCH\nTHEN\nTOP\nTTL_COL\nTTL_DURATION\nUNWIND\nUSER\nUSERS\nUUID\nVALUE\nVALUES\nVID_TYPE\nWILDCARD\nZONE\nZONES\nFALSE\nTRUE\n</code></pre>"},{"location":"3.ngql-guide/1.nGQL-overview/ngql-style-guide/","title":"nGQL style guide","text":"<p>nGQL does not have strict formatting requirements, but creating nGQL statements according to an appropriate and uniform style can improve readability and avoid ambiguity. Using the same nGQL style in the same organization or project helps reduce maintenance costs and avoid problems caused by format confusion or misunderstanding. This topic will provide a style guide for writing nGQL statements.</p> <p>Compatibility</p> <p>The styles of nGQL and Cypher Style Guide are different.</p>"},{"location":"3.ngql-guide/1.nGQL-overview/ngql-style-guide/#newline","title":"Newline","text":"<ol> <li> <p>Start a new line to write a clause.</p> <p>Not recommended:</p> <pre><code>GO FROM \"player100\" OVER follow REVERSELY YIELD src(edge) AS id;\n</code></pre> <p>Recommended:</p> <pre><code>GO FROM \"player100\" \\\nOVER follow REVERSELY \\\nYIELD src(edge) AS id;\n</code></pre> </li> <li> <p>Start a new line to write different statements in a composite statement.</p> <p>Not recommended:</p> <pre><code>GO FROM \"player100\" OVER follow REVERSELY YIELD src(edge) AS id | GO FROM $-.id \\\nOVER serve WHERE properties($^).age &gt; 20 YIELD properties($^).name AS FriendOf, properties($$).name AS Team;\n</code></pre> <p>Recommended:</p> <pre><code>GO FROM \"player100\" \\\nOVER follow REVERSELY \\\nYIELD src(edge) AS id | \\\nGO FROM $-.id OVER serve \\\nWHERE properties($^).age &gt; 20 \\\nYIELD properties($^).name AS FriendOf, properties($$).name AS Team;\n</code></pre> </li> <li> <p>If the clause exceeds 80 characters, start a new line at the appropriate place.</p> <p>Not recommended:</p> <pre><code>MATCH (v:player{name:\"Tim Duncan\"})-[e]-&gt;(v2) \\\nWHERE (v2.player.name STARTS WITH \"Y\" AND v2.player.age &gt; 35 AND v2.player.age &lt; v.player.age) OR (v2.player.name STARTS WITH \"T\" AND v2.player.age &lt; 45 AND v2.player.age &gt; v.player.age) \\\nRETURN v2;\n</code></pre> <p>Recommended:</p> <pre><code>MATCH (v:player{name:\"Tim Duncan\"})-[e]-&gt;(v2) \\\nWHERE (v2.player.name STARTS WITH \"Y\" AND v2.player.age &gt; 35 AND v2.player.age &lt; v.player.age) \\\nOR (v2.player.name STARTS WITH \"T\" AND v2.player.age &lt; 45 AND v2.player.age &gt; v.player.age) \\\nRETURN v2;\n</code></pre> </li> </ol> <p>Note</p> <p>If needed, you can also start a new line for better understanding, even if the clause does not exceed 80 characters. </p>"},{"location":"3.ngql-guide/1.nGQL-overview/ngql-style-guide/#identifier_naming","title":"Identifier naming","text":"<p>In nGQL statements, characters other than keywords, punctuation marks, and blanks are all identifiers. Recommended methods to name the identifiers are as follows.</p> <ol> <li> <p>Use singular nouns to name tags, and use the base form of verbs or verb phrases to form Edge types.</p> <p>Not recommended:</p> <pre><code>MATCH p=(v:players)-[e:are_following]-(v2) \\\nRETURN nodes(p);\n</code></pre> <p>Recommended:</p> <pre><code>MATCH p=(v:player)-[e:follow]-(v2) \\\nRETURN nodes(p);\n</code></pre> </li> <li> <p>Use the snake case to name identifiers, and connect words with underscores (_) with all the letters lowercase.</p> <p>Not recommended:</p> <pre><code>MATCH (v:basketballTeam) \\\nRETURN v;\n</code></pre> <p>Recommended:</p> <pre><code>MATCH (v:basketball_team) \\\nRETURN v;\n</code></pre> </li> <li> <p>Use uppercase keywords and lowercase variables.</p> <p>Not recommended:</p> <pre><code>match (V:player) return V limit 5;\n</code></pre> <p>Recommended:</p> <pre><code>MATCH (v:player) RETURN v LIMIT 5;\n</code></pre> </li> </ol>"},{"location":"3.ngql-guide/1.nGQL-overview/ngql-style-guide/#pattern","title":"Pattern","text":"<ol> <li> <p>Start a new line on the right side of the arrow indicating an edge when writing patterns.</p> <p>Not recommended:</p> <pre><code>MATCH (v:player{name: \"Tim Duncan\", age: 42}) \\\n-[e:follow]-&gt;()-[e:serve]-&gt;()&lt;--(v2) \\\nRETURN v, e, v2;\n</code></pre> <p>Recommended:</p> <pre><code>MATCH (v:player{name: \"Tim Duncan\", age: 42})-[e:follow]-&gt; \\\n()-[e:serve]-&gt;()&lt;--(v2) \\\nRETURN v, e, v2;\n</code></pre> </li> <li> <p>Anonymize the vertices and edges that do not need to be queried.</p> <p>Not recommended:</p> <pre><code>MATCH (v:player)-[e:follow]-&gt;(v2) \\\nRETURN v;\n</code></pre> <p>Recommended:</p> <pre><code>MATCH (v:player)-[:follow]-&gt;() \\\nRETURN v;\n</code></pre> </li> <li> <p>Place named vertices in front of anonymous vertices.</p> <p>Not recommended:</p> <pre><code>MATCH ()-[:follow]-&gt;(v) \\\nRETURN v;\n</code></pre> <p>Recommended:</p> <pre><code>MATCH (v)&lt;-[:follow]-() \\\nRETURN v;\n</code></pre> </li> </ol>"},{"location":"3.ngql-guide/1.nGQL-overview/ngql-style-guide/#string","title":"String","text":"<p>The strings should be surrounded by double quotes.</p> <p>Not recommended:</p> <pre><code>RETURN 'Hello Nebula!';\n</code></pre> <p>Recommended:</p> <pre><code>RETURN \"Hello Nebula!\\\"123\\\"\";\n</code></pre> <p>Note</p> <p>When single or double quotes need to be nested in a string, use a backslash () to escape. For example:</p> <pre><code>RETURN \"\\\"NebulaGraph is amazing,\\\" the user says.\";\n</code></pre>"},{"location":"3.ngql-guide/1.nGQL-overview/ngql-style-guide/#statement_termination","title":"Statement termination","text":"<ol> <li> <p>End the nGQL statements with an English semicolon (;).</p> <p>Not recommended:</p> <pre><code>FETCH PROP ON player \"player100\" YIELD properties(vertex)\n</code></pre> <p>Recommended:</p> <pre><code>FETCH PROP ON player \"player100\" YIELD properties(vertex);\n</code></pre> </li> <li> <p>Use a pipe (|) to separate a composite statement, and end the statement with an English semicolon at the end of the last line. Using an English semicolon before a pipe will cause the statement to fail.</p> <p>Not supported:</p> <pre><code>GO FROM \"player100\" \\\nOVER follow \\\nYIELD dst(edge) AS id; | \\\nGO FROM $-.id \\\nOVER serve \\\nYIELD properties($$).name AS Team, properties($^).name AS Player;\n</code></pre> <p>Supported:</p> <pre><code>GO FROM \"player100\" \\\nOVER follow \\\nYIELD dst(edge) AS id | \\\nGO FROM $-.id \\\nOVER serve \\\nYIELD properties($$).name AS Team, properties($^).name AS Player;\n</code></pre> </li> <li> <p>In a composite statement that contains user-defined variables, use an English semicolon to end the statements that define the variables. If you do not follow the rules to add a semicolon or use a pipe to end the composite statement, the execution will fail.</p> <p>Not supported:</p> <pre><code>$var = GO FROM \"player100\" \\\nOVER follow \\\nYIELD follow._dst AS id \\\nGO FROM $var.id \\\nOVER serve \\\nYIELD $$.team.name AS Team, $^.player.name AS Player;\n</code></pre> <p>Not supported:</p> <pre><code>$var = GO FROM \"player100\" \\\nOVER follow \\\nYIELD follow._dst AS id | \\\nGO FROM $var.id \\\nOVER serve \\\nYIELD $$.team.name AS Team, $^.player.name AS Player;\n</code></pre> <p>Supported:</p> <pre><code>$var = GO FROM \"player100\" \\\nOVER follow \\\nYIELD follow._dst AS id; \\\nGO FROM $var.id \\\nOVER serve \\\nYIELD $$.team.name AS Team, $^.player.name AS Player;\n</code></pre> </li> </ol>"},{"location":"3.ngql-guide/10.tag-statements/1.create-tag/","title":"CREATE TAG","text":"<p><code>CREATE TAG</code> creates a tag with the given name in a graph space.</p>"},{"location":"3.ngql-guide/10.tag-statements/1.create-tag/#opencypher_compatibility","title":"OpenCypher compatibility","text":"<p>Tags in nGQL are similar to labels in openCypher. But they are also quite different. For example, the ways to create them are different.</p> <ul> <li>In openCypher, labels are created together with vertices in <code>CREATE</code> statements.</li> <li>In nGQL, tags are created separately using <code>CREATE TAG</code> statements. Tags in nGQL are more like tables in MySQL.</li> </ul>"},{"location":"3.ngql-guide/10.tag-statements/1.create-tag/#prerequisites","title":"Prerequisites","text":"<p>Running the <code>CREATE TAG</code> statement requires some privileges for the graph space. Otherwise, NebulaGraph throws an error.</p>"},{"location":"3.ngql-guide/10.tag-statements/1.create-tag/#syntax","title":"Syntax","text":"<p>To create a tag in a specific graph space, you must specify the current working space with the <code>USE</code> statement.</p> <pre><code>CREATE TAG [IF NOT EXISTS] &lt;tag_name&gt;\n    (\n      &lt;prop_name&gt; &lt;data_type&gt; [NULL | NOT NULL] [DEFAULT &lt;default_value&gt;] [COMMENT '&lt;comment&gt;']\n      [{, &lt;prop_name&gt; &lt;data_type&gt; [NULL | NOT NULL] [DEFAULT &lt;default_value&gt;] [COMMENT '&lt;comment&gt;']} ...] \n    )\n    [TTL_DURATION = &lt;ttl_duration&gt;]\n    [TTL_COL = &lt;prop_name&gt;]\n    [COMMENT = '&lt;comment&gt;'];\n</code></pre> Parameter Description <code>IF NOT EXISTS</code> Detects if the tag that you want to create exists. If it does not exist, a new one will be created. The tag existence detection here only compares the tag names (excluding properties). <code>&lt;tag_name&gt;</code> 1. Each tag name in the graph space must be unique. 2. Tag names cannot be modified after they are set.3. Tag names cannot start with a number; they support 1-4 byte UTF-8 encoded characters, including English letters (case sensitive), numbers, Chinese characters, etc., but do not support special characters other than underscores. To use special characters, reserved keywords, or start with a number in a tag name, enclose them in backticks (`), and do not use periods (<code>.</code>) in a tag name. For more information, see Keywords and reserved words. Note: If you name a tag in Chinese and encounter a <code>SyntaxError</code>, you need to quote the Chinese characters with backticks (`). <code>&lt;prop_name&gt;</code> The name of the property. It must be unique for each tag. The rules for permitted property names are the same as those for tag names. <code>&lt;data_type&gt;</code> Shows the data type of each property. For a full description of the property data types, see Data types and Boolean. <code>NULL \\| NOT NULL</code> Specifies if the property supports <code>NULL | NOT NULL</code>. The default value is <code>NULL</code>. <code>DEFAULT</code> Specifies a default value for a property. The default value can be a literal value or an expression supported by NebulaGraph. If no value is specified, the default value is used when inserting a new vertex. <code>COMMENT</code> The remarks of a certain property or the tag itself. The maximum length is 256 bytes. By default, there will be no comments on a tag. <code>TTL_DURATION</code> Specifies the life cycle for the property. The property that exceeds the specified TTL expires. The expiration threshold is the <code>TTL_COL</code> value plus the <code>TTL_DURATION</code>. The default value of <code>TTL_DURATION</code> is <code>0</code>. It means the data never expires. <code>TTL_COL</code> Specifies the property to set a timeout on. The data type of the property must be <code>int</code> or <code>timestamp</code>. A tag can only specify one field as <code>TTL_COL</code>. For more information on TTL, see TTL options."},{"location":"3.ngql-guide/10.tag-statements/1.create-tag/#examples","title":"Examples","text":"<pre><code>nebula&gt; CREATE TAG IF NOT EXISTS player(name string, age int);\n\n# The following example creates a tag with no properties.\nnebula&gt; CREATE TAG IF NOT EXISTS no_property();\u00a0\n\n# The following example creates a tag with a default value.\nnebula&gt; CREATE TAG IF NOT EXISTS player_with_default(name string, age int DEFAULT 20);\n\n# In the following example, the TTL of the create_time field is set to be 100 seconds.\nnebula&gt; CREATE TAG IF NOT EXISTS woman(name string, age int, \\\n        married bool, salary double, create_time timestamp) \\\n        TTL_DURATION = 100, TTL_COL = \"create_time\";\n</code></pre>"},{"location":"3.ngql-guide/10.tag-statements/1.create-tag/#implementation_of_the_operation","title":"Implementation of the operation","text":"<p>Trying to use a newly created tag may fail because the creation of the tag is implemented asynchronously. To make sure the follow-up operations work as expected, Wait for two heartbeat cycles, i.e., 20 seconds.</p> <p>To change the heartbeat interval, modify the <code>heartbeat_interval_secs</code> parameter in the configuration files for all services.</p>"},{"location":"3.ngql-guide/10.tag-statements/2.drop-tag/","title":"DROP TAG","text":"<p><code>DROP TAG</code> drops a tag with the given name in the current working graph space.</p> <p>A vertex can have one or more tags.</p> <ul> <li>If a vertex has only one tag, the vertex CANNOT be accessed after you drop it. The vertex will be dropped in the next compaction. But its edges are available, this operation will result in dangling edges.</li> </ul> <ul> <li>If a vertex has multiple tags, the vertex is still accessible after you drop one of them. But all the properties defined by this dropped tag CANNOT be accessed.</li> </ul> <p>This operation only deletes the Schema data. All the files or directories in the disk will not be deleted directly until the next compaction.</p> <p>Compatibility</p> <p>In NebulaGraph 3.5.0, inserting vertex without tag is not supported by default. If you want to use the vertex without tags, add <code>--graph_use_vertex_key=true</code> to the configuration files (<code>nebula-graphd.conf</code>) of all Graph services in the cluster, and add <code>--use_vertex_key=true</code> to the configuration files (<code>nebula-storaged.conf</code>) of all Storage services in the cluster.</p>"},{"location":"3.ngql-guide/10.tag-statements/2.drop-tag/#prerequisites","title":"Prerequisites","text":"<ul> <li>Running the <code>DROP TAG</code> statement requires some privileges for the graph space. Otherwise, NebulaGraph throws an error.</li> </ul> <ul> <li>Before you drop a tag, make sure that the tag does not have any indexes. Otherwise, the conflict error (<code>[ERROR (-1005)]: Conflict!</code>) will be returned when you run the <code>DROP TAG</code> statement. To drop an index, see DROP INDEX.</li> </ul>"},{"location":"3.ngql-guide/10.tag-statements/2.drop-tag/#syntax","title":"Syntax","text":"<pre><code>DROP TAG [IF EXISTS] &lt;tag_name&gt;;\n</code></pre> <ul> <li><code>IF NOT EXISTS</code>: Detects if the tag that you want to drop exists. Only when it exists will it be dropped.</li> </ul> <ul> <li><code>tag_name</code>: Specifies the tag name that you want to drop. You can drop only one tag in one statement.</li> </ul>"},{"location":"3.ngql-guide/10.tag-statements/2.drop-tag/#example","title":"Example","text":"<pre><code>nebula&gt; CREATE TAG IF NOT EXISTS test(p1 string, p2 int);\nnebula&gt; DROP TAG test;\n</code></pre>"},{"location":"3.ngql-guide/10.tag-statements/3.alter-tag/","title":"ALTER TAG","text":"<p><code>ALTER TAG</code> alters the structure of a tag with the given name in a graph space. You can add or drop properties, and change the data type of an existing property. You can also set a TTL (Time-To-Live) on a property, or change its TTL duration.</p>"},{"location":"3.ngql-guide/10.tag-statements/3.alter-tag/#notes","title":"Notes","text":"<ul> <li>Running the <code>ALTER TAG</code> statement requires some privileges for the graph space. Otherwise, NebulaGraph throws an error.</li> </ul> <ul> <li>Before you alter properties for a tag, make sure that the properties are not indexed. If the properties contain any indexes, the conflict error <code>[ERROR (-1005)]: Conflict!</code> will occur when you <code>ALTER TAG</code>. For more information on dropping an index, see DROP INDEX.</li> </ul> <ul> <li>The property name must be unique in a tag. If you add a property with the same name as an existing property or a dropped property, the operation fails.</li> </ul>"},{"location":"3.ngql-guide/10.tag-statements/3.alter-tag/#syntax","title":"Syntax","text":"<pre><code>ALTER TAG &lt;tag_name&gt;\n    &lt;alter_definition&gt; [[, alter_definition] ...]\n    [ttl_definition [, ttl_definition] ... ]\n    [COMMENT '&lt;comment&gt;'];\n\nalter_definition:\n| ADD    (prop_name data_type [NULL | NOT NULL] [DEFAULT &lt;default_value&gt;] [COMMENT '&lt;comment&gt;'])\n| DROP   (prop_name)\n| CHANGE (prop_name data_type [NULL | NOT NULL] [DEFAULT &lt;default_value&gt;] [COMMENT '&lt;comment&gt;'])\n\nttl_definition:\n    TTL_DURATION = ttl_duration, TTL_COL = prop_name\n</code></pre> <ul> <li><code>tag_name</code>: Specifies the tag name that you want to alter. You can alter only one tag in one statement. Before you alter a tag, make sure that the tag exists in the current working graph space. If the tag does not exist, an error will occur when you alter it.</li> </ul> <ul> <li>Multiple <code>ADD</code>, <code>DROP</code>, and <code>CHANGE</code> clauses are permitted in a single <code>ALTER TAG</code> statement, separated by commas.</li> </ul> <ul> <li>When a property value is set to <code>NOT NULL</code> using <code>ADD</code> or <code>CHANGE</code>, a default value must be specified for the property, that is, the value of <code>DEFAULT</code> must be specified.</li> </ul> <ul> <li> <p>When using <code>CHANGE</code> to modify the data type of a property:</p> <ul> <li>Only the length of a <code>FIXED_STRING</code> or an <code>INT</code> can be increased. The length of a <code>STRING</code> or an <code>INT</code> cannot be decreased.</li> </ul> <ul> <li>Only the data type conversions from FIXED_STRING to STRING and from FLOAT to DOUBLE are allowed.</li> </ul> </li> </ul>"},{"location":"3.ngql-guide/10.tag-statements/3.alter-tag/#examples","title":"Examples","text":"<pre><code>nebula&gt; CREATE TAG IF NOT EXISTS t1 (p1 string, p2 int);\nnebula&gt; ALTER TAG t1 ADD (p3 int32, fixed_string(10));\nnebula&gt; ALTER TAG t1 TTL_DURATION = 2, TTL_COL = \"p2\";\nnebula&gt; ALTER TAG t1 COMMENT = 'test1';\nnebula&gt; ALTER TAG t1 ADD (p5 double NOT NULL DEFAULT 0.4 COMMENT 'p5') COMMENT='test2';\n// Change the data type of p3 in the TAG t1 from INT32 to INT64, and that of p4 from FIXED_STRING(10) to STRING.\nnebula&gt; ALTER TAG t1 CHANGE (p3 int64, p4 string);\n[ERROR(-1005)]: Unsupported!\n</code></pre>"},{"location":"3.ngql-guide/10.tag-statements/3.alter-tag/#implementation_of_the_operation","title":"Implementation of the operation","text":"<p>Trying to use a newly altered tag may fail because the alteration of the tag is implemented asynchronously. To make sure the follow-up operations work as expected, Wait for two heartbeat cycles, i.e., 20 seconds.</p> <p>To change the heartbeat interval, modify the <code>heartbeat_interval_secs</code> parameter in the configuration files for all services.</p>"},{"location":"3.ngql-guide/10.tag-statements/4.show-tags/","title":"SHOW TAGS","text":"<p>The <code>SHOW TAGS</code> statement shows the name of all tags in the current graph space.</p> <p>You do not need any privileges for the graph space to run the <code>SHOW TAGS</code> statement. But the returned results are different based on role privileges.</p>"},{"location":"3.ngql-guide/10.tag-statements/4.show-tags/#syntax","title":"Syntax","text":"<pre><code>SHOW TAGS;\n</code></pre>"},{"location":"3.ngql-guide/10.tag-statements/4.show-tags/#examples","title":"Examples","text":"<pre><code>nebula&gt; SHOW TAGS;\n+----------+\n| Name     |\n+----------+\n| \"player\" |\n| \"team\"   |\n+----------+\n</code></pre>"},{"location":"3.ngql-guide/10.tag-statements/5.describe-tag/","title":"DESCRIBE TAG","text":"<p><code>DESCRIBE TAG</code> returns the information about a tag with the given name in a graph space, such as field names, data type, and so on.</p>"},{"location":"3.ngql-guide/10.tag-statements/5.describe-tag/#prerequisite","title":"Prerequisite","text":"<p>Running the <code>DESCRIBE TAG</code> statement requires some privileges for the graph space. Otherwise, NebulaGraph throws an error.</p>"},{"location":"3.ngql-guide/10.tag-statements/5.describe-tag/#syntax","title":"Syntax","text":"<pre><code>DESC[RIBE] TAG &lt;tag_name&gt;;\n</code></pre> <p>You can use <code>DESC</code> instead of <code>DESCRIBE</code> for short.</p>"},{"location":"3.ngql-guide/10.tag-statements/5.describe-tag/#example","title":"Example","text":"<pre><code>nebula&gt; DESCRIBE TAG player;\n+--------+----------+-------+---------+---------+\n| Field  | Type     | Null  | Default | Comment |\n+--------+----------+-------+---------+---------+\n| \"name\" | \"string\" | \"YES\" |         |         |\n| \"age\"  | \"int64\"  | \"YES\" |         |         |\n+--------+----------+-------+---------+---------+\n</code></pre>"},{"location":"3.ngql-guide/10.tag-statements/6.delete-tag/","title":"DELETE TAG","text":"<p><code>DELETE TAG</code> deletes a tag with the given name on a specified vertex.</p>"},{"location":"3.ngql-guide/10.tag-statements/6.delete-tag/#prerequisites","title":"Prerequisites","text":"<p>Running the <code>DELETE TAG</code> statement requires some privileges for the graph space. Otherwise, NebulaGraph throws an error.</p>"},{"location":"3.ngql-guide/10.tag-statements/6.delete-tag/#syntax","title":"Syntax","text":"<pre><code>DELETE TAG &lt;tag_name_list&gt; FROM &lt;VID&gt;;\n</code></pre> <ul> <li><code>tag_name_list</code>: Specifies the name of the tag. Multiple tags are separated with commas (,). <code>*</code> means all tags.</li> </ul> <ul> <li><code>VID</code>: Specifies the VID of the tag to delete.</li> </ul>"},{"location":"3.ngql-guide/10.tag-statements/6.delete-tag/#example","title":"Example","text":"<pre><code>nebula&gt; CREATE TAG IF NOT EXISTS test1(p1 string, p2 int);\nnebula&gt; CREATE TAG IF NOT EXISTS test2(p3 string, p4 int);\nnebula&gt; INSERT VERTEX test1(p1, p2),test2(p3, p4) VALUES \"test\":(\"123\", 1, \"456\", 2);\nnebula&gt; FETCH PROP ON * \"test\" YIELD vertex AS v;\n+------------------------------------------------------------+\n| v                                                          |\n+------------------------------------------------------------+\n| (\"test\" :test1{p1: \"123\", p2: 1} :test2{p3: \"456\", p4: 2}) |\n+------------------------------------------------------------+\nnebula&gt; DELETE TAG test1 FROM \"test\";\nnebula&gt; FETCH PROP ON * \"test\" YIELD vertex AS v;\n+-----------------------------------+\n| v                                 |\n+-----------------------------------+\n| (\"test\" :test2{p3: \"456\", p4: 2}) |\n+-----------------------------------+\nnebula&gt; DELETE TAG * FROM \"test\";\nnebula&gt; FETCH PROP ON * \"test\" YIELD vertex AS v;\n+---+\n| v |\n+---+\n+---+\n</code></pre> <p>Compatibility</p> <ul> <li>In openCypher, you can use the statement <code>REMOVE v:LABEL</code> to delete the tag <code>LABEL</code> of the vertex <code>v</code>.</li> <li><code>DELETE TAG</code> and <code>DROP TAG</code> have the same semantics but different syntax. In nGQL, use <code>DELETE TAG</code>.</li> </ul>"},{"location":"3.ngql-guide/10.tag-statements/improve-query-by-tag-index/","title":"Add and delete tags","text":"<p>OpenCypher has the features of <code>SET label</code> and <code>REMOVE label</code> to speed up the process of querying or labeling.</p> <p>NebulaGraph achieves the same operations by creating and inserting tags to an existing vertex, which can quickly query vertices based on the tag name. Users can also run <code>DELETE TAG</code> to delete some vertices that are no longer needed.</p>"},{"location":"3.ngql-guide/10.tag-statements/improve-query-by-tag-index/#examples","title":"Examples","text":"<p>For example, in the <code>basketballplayer</code> data set, some basketball players are also team shareholders. Users can create an index for the shareholder tag <code>shareholder</code> for quick search. If the player is no longer a shareholder, users can delete the shareholder tag of the corresponding player by <code>DELETE TAG</code>.</p> <pre><code>//This example creates the shareholder tag and index.\nnebula&gt; CREATE TAG IF NOT EXISTS shareholder();\nnebula&gt; CREATE TAG INDEX IF NOT EXISTS shareholder_tag on shareholder();\n\n//This example adds a tag on the vertex.\nnebula&gt; INSERT VERTEX shareholder() VALUES \"player100\":();\nnebula&gt; INSERT VERTEX shareholder() VALUES \"player101\":();\n\n//This example queries all the shareholders.\nnebula&gt; MATCH (v:shareholder) RETURN v;\n+--------------------------------------------------------------------+\n| v                                                                  |\n+--------------------------------------------------------------------+\n| (\"player100\" :player{age: 42, name: \"Tim Duncan\"} :shareholder{})  |\n| (\"player101\" :player{age: 36, name: \"Tony Parker\"} :shareholder{}) |\n+--------------------------------------------------------------------+\n\nnebula&gt; LOOKUP ON shareholder YIELD id(vertex);\n+-------------+\n| id(VERTEX)  |\n+-------------+\n| \"player100\" |\n| \"player101\" |\n+-------------+\n\n//In this example, the \"player100\" is no longer a shareholder.\nnebula&gt; DELETE TAG shareholder FROM \"player100\";\nnebula&gt; LOOKUP ON shareholder YIELD id(vertex);\n+-------------+\n| id(VERTEX)  |\n+-------------+\n| \"player101\" |\n+-------------+\n</code></pre> <p>Note</p> <p>If the index is created after inserting the test data, use the <code>REBUILD TAG INDEX &lt;index_name_list&gt;;</code> statement to rebuild the index.</p>"},{"location":"3.ngql-guide/11.edge-type-statements/1.create-edge/","title":"CREATE EDGE","text":"<p><code>CREATE EDGE</code> creates an edge type with the given name in a graph space.</p>"},{"location":"3.ngql-guide/11.edge-type-statements/1.create-edge/#opencypher_compatibility","title":"OpenCypher compatibility","text":"<p>Edge types in nGQL are similar to relationship types in openCypher. But they are also quite different. For example, the ways to create them are different.</p> <ul> <li>In openCypher, relationship types are created together with vertices in <code>CREATE</code> statements.</li> <li>In nGQL, edge types are created separately using <code>CREATE EDGE</code> statements. Edge types in nGQL are more like tables in MySQL.</li> </ul>"},{"location":"3.ngql-guide/11.edge-type-statements/1.create-edge/#prerequisites","title":"Prerequisites","text":"<p>Running the <code>CREATE EDGE</code> statement requires some privileges for the graph space. Otherwise, NebulaGraph throws an error.</p>"},{"location":"3.ngql-guide/11.edge-type-statements/1.create-edge/#syntax","title":"Syntax","text":"<p>To create an edge type in a specific graph space, you must specify the current working space with the <code>USE</code> statement.</p> <pre><code>CREATE EDGE [IF NOT EXISTS] &lt;edge_type_name&gt;\n    (\n      &lt;prop_name&gt; &lt;data_type&gt; [NULL | NOT NULL] [DEFAULT &lt;default_value&gt;] [COMMENT '&lt;comment&gt;']\n      [{, &lt;prop_name&gt; &lt;data_type&gt; [NULL | NOT NULL] [DEFAULT &lt;default_value&gt;] [COMMENT '&lt;comment&gt;']} ...] \n    )\n    [TTL_DURATION = &lt;ttl_duration&gt;]\n    [TTL_COL = &lt;prop_name&gt;]\n    [COMMENT = '&lt;comment&gt;'];\n</code></pre> Parameter Description <code>IF NOT EXISTS</code> Detects if the edge type that you want to create exists. If it does not exist, a new one will be created. The edge type existence detection here only compares the edge type names (excluding properties). <code>&lt;edge_type_name&gt;</code> 1. The edge type name must be unique in a graph space. 2. Once the edge type name is set, it can not be altered. 3. Edge type names cannot start with a number; they support 1-4 byte UTF-8 encoded characters, including English letters (case sensitive), numbers, Chinese characters, etc., but do not include special characters other than underscores. To use special characters, reserved keywords or starting with a number, quote them with backticks (`) and cannot use periods (<code>.</code>). For more information, see Keywords and reserved words. Note: If you name an edge type in Chinese and encounter a <code>SyntaxError</code>, you need to quote the Chinese characters with backticks (`). <code>&lt;prop_name&gt;</code> The name of the property. It must be unique for each edge type. The rules for permitted property names are the same as those for edge type names. <code>&lt;data_type&gt;</code> Shows the data type of each property. For a full description of the property data types, see Data types and Boolean. <code>NULL \\| NOT NULL</code> Specifies if the property supports <code>NULL | NOT NULL</code>. The default value is <code>NULL</code>. <code>DEFAULT</code> must be specified if <code>NOT NULL</code> is set. <code>DEFAULT</code> Specifies a default value for a property. The default value can be a literal value or an expression supported by NebulaGraph. If no value is specified, the default value is used when inserting a new edge. <code>COMMENT</code> The remarks of a certain property or the edge type itself. The maximum length is 256 bytes. By default, there will be no comments on an edge type. <code>TTL_DURATION</code> Specifies the life cycle for the property. The property that exceeds the specified TTL expires. The expiration threshold is the <code>TTL_COL</code> value plus the <code>TTL_DURATION</code>. The default value of <code>TTL_DURATION</code> is <code>0</code>. It means the data never expires. <code>TTL_COL</code> Specifies the property to set a timeout on. The data type of the property must be <code>int</code> or <code>timestamp</code>. An edge type can only specify one field as <code>TTL_COL</code>. For more information on TTL, see TTL options."},{"location":"3.ngql-guide/11.edge-type-statements/1.create-edge/#examples","title":"Examples","text":"<pre><code>nebula&gt; CREATE EDGE IF NOT EXISTS follow(degree int);\n\n# The following example creates an edge type with no properties.\nnebula&gt; CREATE EDGE IF NOT EXISTS no_property();\n\n# The following example creates an edge type with a default value.\nnebula&gt; CREATE EDGE IF NOT EXISTS follow_with_default(degree int DEFAULT 20);\n\n# In the following example, the TTL of the p2 field is set to be 100 seconds.\nnebula&gt; CREATE EDGE IF NOT EXISTS e1(p1 string, p2 int, p3 timestamp) \\\n        TTL_DURATION = 100, TTL_COL = \"p2\";\n</code></pre>"},{"location":"3.ngql-guide/11.edge-type-statements/2.drop-edge/","title":"DROP EDGE","text":"<p><code>DROP EDGE</code> drops an edge type with the given name in a graph space.</p> <p>An edge can have only one edge type. After you drop it, the edge CANNOT be accessed. The edge will be deleted in the next compaction.</p> <p>This operation only deletes the Schema data. All the files or directories in the disk will not be deleted directly until the next compaction.</p>"},{"location":"3.ngql-guide/11.edge-type-statements/2.drop-edge/#prerequisites","title":"Prerequisites","text":"<ul> <li>Running the <code>DROP EDGE</code> statement requires some privileges for the graph space. Otherwise, NebulaGraph throws an error.</li> </ul> <ul> <li>Before you drop an edge type, make sure that the edge type does not have any indexes. Otherwise, the conflict error (<code>[ERROR (-1005)]: Conflict!</code>) will be returned. To drop an index, see DROP INDEX.</li> </ul>"},{"location":"3.ngql-guide/11.edge-type-statements/2.drop-edge/#syntax","title":"Syntax","text":"<pre><code>DROP EDGE [IF EXISTS] &lt;edge_type_name&gt;\n</code></pre> <ul> <li><code>IF NOT EXISTS</code>: Detects if the edge type that you want to drop exists. Only when it exists will it be dropped.</li> </ul> <ul> <li><code>edge_type_name</code>: Specifies the edge type name that you want to drop. You can drop only one edge type in one statement.</li> </ul>"},{"location":"3.ngql-guide/11.edge-type-statements/2.drop-edge/#example","title":"Example","text":"<pre><code>nebula&gt; CREATE EDGE IF NOT EXISTS e1(p1 string, p2 int);\nnebula&gt; DROP EDGE e1;\n</code></pre>"},{"location":"3.ngql-guide/11.edge-type-statements/3.alter-edge/","title":"ALTER EDGE","text":"<p><code>ALTER EDGE</code> alters the structure of an edge type with the given name in a graph space. You can add or drop properties, and change the data type of an existing property. You can also set a TTL (Time-To-Live) on a property, or change its TTL duration.</p>"},{"location":"3.ngql-guide/11.edge-type-statements/3.alter-edge/#notes","title":"Notes","text":"<ul> <li>Running the <code>ALTER EDGE</code> statement requires some privileges for the graph space. Otherwise, NebulaGraph throws an error.</li> </ul> <ul> <li>Before you alter properties for an edge type, make sure that the properties are not indexed. If the properties contain any indexes, the conflict error <code>[ERROR (-1005)]: Conflict!</code> will occur when you <code>ALTER EDGE</code>. For more information on dropping an index, see DROP INDEX.</li> </ul> <ul> <li>The property name must be unique in an edge type. If you add a property with the same name as an existing property or a dropped property, the operation fails.</li> </ul> <ul> <li>Only the length of a <code>FIXED_STRING</code> or an <code>INT</code> can be increased.</li> </ul> <ul> <li>Only the data type conversions from FIXED_STRING to STRING and from FLOAT to DOUBLE are allowed.</li> </ul>"},{"location":"3.ngql-guide/11.edge-type-statements/3.alter-edge/#syntax","title":"Syntax","text":"<pre><code>ALTER EDGE &lt;edge_type_name&gt;\n    &lt;alter_definition&gt; [, alter_definition] ...]\n    [ttl_definition [, ttl_definition] ... ]\n    [COMMENT = '&lt;comment&gt;'];\n\nalter_definition:\n| ADD    (prop_name data_type)\n| DROP   (prop_name)\n| CHANGE (prop_name data_type)\n\nttl_definition:\n    TTL_DURATION = ttl_duration, TTL_COL = prop_name\n</code></pre> <ul> <li><code>edge_type_name</code>: Specifies the edge type name that you want to alter. You can alter only one edge type in one statement. Before you alter an edge type, make sure that the edge type exists in the graph space. If the edge type does not exist, an error occurs when you alter it.</li> </ul> <ul> <li>Multiple <code>ADD</code>, <code>DROP</code>, and <code>CHANGE</code> clauses are permitted in a single <code>ALTER EDGE</code> statement, separated by commas.</li> </ul> <ul> <li>When a property value is set to <code>NOT NULL</code> using <code>ADD</code> or <code>CHANGE</code>, a default value must be specified for the property, that is, the value of <code>DEFAULT</code> must be specified.</li> </ul>"},{"location":"3.ngql-guide/11.edge-type-statements/3.alter-edge/#example","title":"Example","text":"<pre><code>nebula&gt; CREATE EDGE IF NOT EXISTS e1(p1 string, p2 int);\nnebula&gt; ALTER EDGE e1 ADD (p3 int, p4 string);\nnebula&gt; ALTER EDGE e1 TTL_DURATION = 2, TTL_COL = \"p2\";\nnebula&gt; ALTER EDGE e1 COMMENT = 'edge1';\n</code></pre>"},{"location":"3.ngql-guide/11.edge-type-statements/3.alter-edge/#implementation_of_the_operation","title":"Implementation of the operation","text":"<p>Trying to use a newly altered edge type may fail because the alteration of the edge type is implemented asynchronously. To make sure the follow-up operations work as expected, Wait for two heartbeat cycles, i.e., 20 seconds.</p> <p>To change the heartbeat interval, modify the <code>heartbeat_interval_secs</code> parameter in the configuration files for all services.</p>"},{"location":"3.ngql-guide/11.edge-type-statements/4.show-edges/","title":"SHOW EDGES","text":"<p><code>SHOW EDGES</code> shows all edge types in the current graph space.</p> <p>You do not need any privileges for the graph space to run the <code>SHOW EDGES</code> statement. But the returned results are different based on role privileges.</p>"},{"location":"3.ngql-guide/11.edge-type-statements/4.show-edges/#syntax","title":"Syntax","text":"<pre><code>SHOW EDGES;\n</code></pre>"},{"location":"3.ngql-guide/11.edge-type-statements/4.show-edges/#example","title":"Example","text":"<pre><code>nebula&gt; SHOW EDGES;\n+----------+\n| Name     |\n+----------+\n| \"follow\" |\n| \"serve\"  |\n+----------+\n</code></pre>"},{"location":"3.ngql-guide/11.edge-type-statements/5.describe-edge/","title":"DESCRIBE EDGE","text":"<p><code>DESCRIBE EDGE</code> returns the information about an edge type with the given name in a graph space, such as field names, data type, and so on.</p>"},{"location":"3.ngql-guide/11.edge-type-statements/5.describe-edge/#prerequisites","title":"Prerequisites","text":"<p>Running the <code>DESCRIBE EDGE</code> statement requires some privileges for the graph space. Otherwise, NebulaGraph throws an error.</p>"},{"location":"3.ngql-guide/11.edge-type-statements/5.describe-edge/#syntax","title":"Syntax","text":"<pre><code>DESC[RIBE] EDGE &lt;edge_type_name&gt;\n</code></pre> <p>You can use <code>DESC</code> instead of <code>DESCRIBE</code> for short.</p>"},{"location":"3.ngql-guide/11.edge-type-statements/5.describe-edge/#example","title":"Example","text":"<pre><code>nebula&gt; DESCRIBE EDGE follow;\n+----------+---------+-------+---------+---------+\n| Field    | Type    | Null  | Default | Comment |\n+----------+---------+-------+---------+---------+\n| \"degree\" | \"int64\" | \"YES\" |         |         |\n+----------+---------+-------+---------+---------+\n</code></pre>"},{"location":"3.ngql-guide/12.vertex-statements/1.insert-vertex/","title":"INSERT VERTEX","text":"<p>The <code>INSERT VERTEX</code> statement inserts one or more vertices into a graph space in NebulaGraph.</p>"},{"location":"3.ngql-guide/12.vertex-statements/1.insert-vertex/#prerequisites","title":"Prerequisites","text":"<p>Running the <code>INSERT VERTEX</code> statement requires some privileges for the graph space. Otherwise, NebulaGraph throws an error.</p>"},{"location":"3.ngql-guide/12.vertex-statements/1.insert-vertex/#syntax","title":"Syntax","text":"<pre><code>INSERT VERTEX [IF NOT EXISTS] [tag_props, [tag_props] ...]\nVALUES VID: ([prop_value_list])\n\ntag_props:\n  tag_name ([prop_name_list])\n\nprop_name_list:\n   [prop_name [, prop_name] ...]\n\nprop_value_list:\n   [prop_value [, prop_value] ...] \n</code></pre> <ul> <li> <p><code>IF NOT EXISTS</code> detects if the VID that you want to insert exists. If it does not exist, a new one will be inserted.</p> <p>Note</p> <ul> <li><code>IF NOT EXISTS</code> only compares the names of the VID and the tag (excluding properties).</li> <li><code>IF NOT EXISTS</code> will read to check whether the data exists, which will have a significant impact on performance.</li> </ul> </li> </ul> <ul> <li> <p><code>tag_name</code> denotes the tag (vertex type), which must be created before <code>INSERT VERTEX</code>. For more information, see CREATE TAG.</p> <p>Caution</p> <p>NebulaGraph 3.5.0 supports inserting vertices without tags.</p> <p>Compatibility</p> <p>In NebulaGraph 3.5.0, inserting vertex without tag is not supported by default. If you want to use the vertex without tags, add <code>--graph_use_vertex_key=true</code> to the configuration files (<code>nebula-graphd.conf</code>) of all Graph services in the cluster, add <code>--use_vertex_key=true</code> to the configuration files (<code>nebula-storaged.conf</code>) of all Storage services in the cluster. An example of a command to insert a vertex without tag is <code>INSERT VERTEX VALUES \"1\":();</code>.</p> </li> </ul> <ul> <li><code>prop_name_list</code> contains the names of the properties on the tag.</li> </ul> <ul> <li><code>VID</code> is the vertex ID. In NebulaGraph 2.0, string and integer VID types are supported. The VID type is set when a graph space is created. For more information, see CREATE SPACE.</li> </ul> <ul> <li><code>prop_value_list</code> must provide the property values according to the <code>prop_name_list</code>. When the <code>NOT NULL</code> constraint is set for a given property, an error is returned if no property is given. When the default value for a property is <code>NULL</code>, you can omit to specify the property value. For details, see CREATE TAG.</li> </ul> <p>Caution</p> <p><code>INSERT VERTEX</code> and <code>CREATE</code> have different semantics.</p> <ul> <li>The semantics of <code>INSERT VERTEX</code> is closer to that of INSERT in NoSQL (key-value), or <code>UPSERT</code> (<code>UPDATE</code> or <code>INSERT</code>) in SQL.</li> <li>When two INSERT statements (neither uses <code>IF NOT EXISTS</code>) with the same <code>VID</code> and <code>TAG</code> are operated at the same time, the latter INSERT will overwrite the former.</li> <li>When two INSERT statements with the same <code>VID</code> but different <code>TAGS</code> are operated at the same time, the operation of different tags will not overwrite each other.</li> </ul> <p>Examples are as follows.</p>"},{"location":"3.ngql-guide/12.vertex-statements/1.insert-vertex/#examples","title":"Examples","text":"<pre><code># Insert a vertex without tag.\nnebula&gt; INSERT VERTEX VALUES \"1\":();\n\n# The following examples create tag t1 with no property and inserts vertex \"10\" with no property.\nnebula&gt; CREATE TAG IF NOT EXISTS t1();                   \nnebula&gt; INSERT VERTEX t1() VALUES \"10\":(); \n</code></pre> <pre><code>nebula&gt; CREATE TAG IF NOT EXISTS t2 (name string, age int);                \nnebula&gt; INSERT VERTEX t2 (name, age) VALUES \"11\":(\"n1\", 12);\n\n#  In the following example, the insertion fails because \"a13\" is not int.\nnebula&gt; INSERT VERTEX t2 (name, age) VALUES \"12\":(\"n1\", \"a13\"); \n\n# The following example inserts two vertices at one time.\nnebula&gt; INSERT VERTEX t2 (name, age) VALUES \"13\":(\"n3\", 12), \"14\":(\"n4\", 8); \n</code></pre> <pre><code>nebula&gt; CREATE TAG IF NOT EXISTS t3(p1 int);\nnebula&gt; CREATE TAG IF NOT EXISTS t4(p2 string);\n\n# The following example inserts vertex \"21\" with two tags.\nnebula&gt; INSERT VERTEX t3 (p1), t4(p2) VALUES \"21\": (321, \"hello\");\n</code></pre> <p>A vertex can be inserted/written with new values multiple times. Only the last written values can be read.</p> <pre><code># The following examples insert vertex \"11\" with new values for multiple times.\nnebula&gt; INSERT VERTEX t2 (name, age) VALUES \"11\":(\"n2\", 13);\nnebula&gt; INSERT VERTEX t2 (name, age) VALUES \"11\":(\"n3\", 14);\nnebula&gt; INSERT VERTEX t2 (name, age) VALUES \"11\":(\"n4\", 15);\nnebula&gt; FETCH PROP ON t2 \"11\" YIELD properties(vertex);\n+-----------------------+\n| properties(VERTEX)    |\n+-----------------------+\n| {age: 15, name: \"n4\"} |\n+-----------------------+\n</code></pre> <pre><code>nebula&gt; CREATE TAG IF NOT EXISTS t5(p1 fixed_string(5) NOT NULL, p2 int, p3 int DEFAULT NULL);\nnebula&gt; INSERT VERTEX t5(p1, p2, p3) VALUES \"001\":(\"Abe\", 2, 3);\n\n# In the following example, the insertion fails because the value of p1 cannot be NULL.\nnebula&gt; INSERT VERTEX t5(p1, p2, p3) VALUES \"002\":(NULL, 4, 5);\n[ERROR (-1009)]: SemanticError: No schema found for `t5'\n\n# In the following example, the value of p3 is the default NULL.\nnebula&gt; INSERT VERTEX t5(p1, p2) VALUES \"003\":(\"cd\", 5);\nnebula&gt; FETCH PROP ON t5 \"003\" YIELD properties(vertex);\n+---------------------------------+\n| properties(VERTEX)              |\n+---------------------------------+\n| {p1: \"cd\", p2: 5, p3: __NULL__} |\n+---------------------------------+\n\n# In the following example, the allowed maximum length of p1 is 5.\nnebula&gt; INSERT VERTEX t5(p1, p2) VALUES \"004\":(\"shalalalala\", 4);\nnebula&gt; FETCH PROP on t5 \"004\" YIELD properties(vertex);\n+------------------------------------+\n| properties(VERTEX)                 |\n+------------------------------------+\n| {p1: \"shala\", p2: 4, p3: __NULL__} |\n+------------------------------------+\n</code></pre> <p>If you insert a vertex that already exists with <code>IF NOT EXISTS</code>, there will be no modification.</p> <pre><code># The following example inserts vertex \"1\".\nnebula&gt; INSERT VERTEX t2 (name, age) VALUES \"1\":(\"n2\", 13);\n# Modify vertex \"1\" with IF NOT EXISTS. But there will be no modification as vertex \"1\" already exists.\nnebula&gt; INSERT VERTEX IF NOT EXISTS t2 (name, age) VALUES \"1\":(\"n3\", 14);\nnebula&gt; FETCH PROP ON t2 \"1\" YIELD properties(vertex);\n+-----------------------+\n| properties(VERTEX)    |\n+-----------------------+\n| {age: 13, name: \"n2\"} |\n+-----------------------+\n</code></pre>"},{"location":"3.ngql-guide/12.vertex-statements/2.update-vertex/","title":"UPDATE VERTEX","text":"<p>The <code>UPDATE VERTEX</code> statement updates properties on tags of a vertex.</p> <p>In NebulaGraph, <code>UPDATE VERTEX</code> supports compare-and-set (CAS).</p> <p>Note</p> <p>An <code>UPDATE VERTEX</code> statement can only update properties on ONE TAG of a vertex.</p>"},{"location":"3.ngql-guide/12.vertex-statements/2.update-vertex/#syntax","title":"Syntax","text":"<pre><code>UPDATE VERTEX ON &lt;tag_name&gt; &lt;vid&gt;\nSET &lt;update_prop&gt;\n[WHEN &lt;condition&gt;]\n[YIELD &lt;output&gt;]\n</code></pre> Parameter Required Description Example <code>ON &lt;tag_name&gt;</code> Yes Specifies the tag of the vertex. The properties to be updated must be on this tag. <code>ON player</code> <code>&lt;vid&gt;</code> Yes Specifies the ID of the vertex to be updated. <code>\"player100\"</code> <code>SET &lt;update_prop&gt;</code> Yes Specifies the properties to be updated and how they will be updated. <code>SET age = age +1</code> <code>WHEN &lt;condition&gt;</code> No Specifies the filter conditions. If <code>&lt;condition&gt;</code> evaluates to <code>false</code>, the <code>SET</code> clause will not take effect. <code>WHEN name == \"Tim\"</code> <code>YIELD &lt;output&gt;</code> No Specifies the output format of the statement. <code>YIELD name AS Name</code>"},{"location":"3.ngql-guide/12.vertex-statements/2.update-vertex/#example","title":"Example","text":"<pre><code>// This query checks the properties of vertex \"player101\".\nnebula&gt; FETCH PROP ON player \"player101\" YIELD properties(vertex);\n+--------------------------------+\n| properties(VERTEX)             |\n+--------------------------------+\n| {age: 36, name: \"Tony Parker\"} |\n+--------------------------------+\n\n// This query updates the age property and returns name and the new age.\nnebula&gt; UPDATE VERTEX ON player \"player101\" \\\n        SET age = age + 2 \\\n        WHEN name == \"Tony Parker\" \\\n        YIELD name AS Name, age AS Age;\n+---------------+-----+\n| Name          | Age |\n+---------------+-----+\n| \"Tony Parker\" | 38  |\n+---------------+-----+\n</code></pre>"},{"location":"3.ngql-guide/12.vertex-statements/3.upsert-vertex/","title":"UPSERT VERTEX","text":"<p>The <code>UPSERT</code> statement is a combination of <code>UPDATE</code> and <code>INSERT</code>. You can use <code>UPSERT VERTEX</code> to update the properties of a vertex if it exists or insert a new vertex if it does not exist.</p> <p>Note</p> <p>An <code>UPSERT VERTEX</code> statement can only update the properties on ONE TAG of a vertex.</p> <p>The performance of <code>UPSERT</code> is much lower than that of <code>INSERT</code> because <code>UPSERT</code> is a read-modify-write serialization operation at the partition level.</p> <p>Danger</p> <p>Don't use <code>UPSERT</code> for scenarios with highly concurrent writes. You can use <code>UPDATE</code> or <code>INSERT</code> instead.</p>"},{"location":"3.ngql-guide/12.vertex-statements/3.upsert-vertex/#syntax","title":"Syntax","text":"<pre><code>UPSERT VERTEX ON &lt;tag&gt; &lt;vid&gt;\nSET &lt;update_prop&gt;\n[WHEN &lt;condition&gt;]\n[YIELD &lt;output&gt;]\n</code></pre> Parameter Required Description Example <code>ON &lt;tag&gt;</code> Yes Specifies the tag of the vertex. The properties to be updated must be on this tag. <code>ON player</code> <code>&lt;vid&gt;</code> Yes Specifies the ID of the vertex to be updated or inserted. <code>\"player100\"</code> <code>SET &lt;update_prop&gt;</code> Yes Specifies the properties to be updated and how they will be updated. <code>SET age = age +1</code> <code>WHEN &lt;condition&gt;</code> No Specifies the filter conditions. <code>WHEN name == \"Tim\"</code> <code>YIELD &lt;output&gt;</code> No Specifies the output format of the statement. <code>YIELD name AS Name</code>"},{"location":"3.ngql-guide/12.vertex-statements/3.upsert-vertex/#insert_a_vertex_if_it_does_not_exist","title":"Insert a vertex if it does not exist","text":"<p>If a vertex does not exist, it is created no matter the conditions in the <code>WHEN</code> clause are met or not, and the <code>SET</code> clause always takes effect. The property values of the new vertex depend on:</p> <ul> <li>How the <code>SET</code> clause is defined.</li> </ul> <ul> <li>Whether the property has a default value.</li> </ul> <p>For example, if:</p> <ul> <li>The vertex to be inserted will have properties <code>name</code> and <code>age</code> based on the tag <code>player</code>.</li> </ul> <ul> <li>The <code>SET</code> clause specifies that <code>age = 30</code>.</li> </ul> <p>Then the property values in different cases are listed as follows:</p> Are <code>WHEN</code> conditions met If properties have default values Value of <code>name</code> Value of <code>age</code> Yes Yes The default value <code>30</code> Yes No <code>NULL</code> <code>30</code> No Yes The default value <code>30</code> No No <code>NULL</code> <code>30</code> <p>Here are some examples:</p> <pre><code>// This query checks if the following three vertices exist. The result \"Empty set\" indicates that the vertices do not exist.\nnebula&gt; FETCH PROP ON * \"player666\", \"player667\", \"player668\" YIELD properties(vertex);\n+--------------------+\n| properties(VERTEX) |\n+--------------------+\n+--------------------+\nEmpty set\n\nnebula&gt; UPSERT VERTEX ON player \"player666\" \\\n        SET age = 30 \\\n        WHEN name == \"Joe\" \\\n        YIELD name AS Name, age AS Age;\n+----------+----------+\n| Name     | Age      |\n+----------+----------+\n| __NULL__ | 30       |\n+----------+----------+\n\nnebula&gt; UPSERT VERTEX ON player \"player666\" \\\n        SET age = 31 \\\n        WHEN name == \"Joe\" \\\n        YIELD name AS Name, age AS Age;\n+----------+-----+\n| Name     | Age |\n+----------+-----+\n| __NULL__ | 30  |\n+----------+-----+\n\nnebula&gt; UPSERT VERTEX ON player \"player667\" \\\n        SET age = 31 \\\n        YIELD name AS Name, age AS Age;\n+----------+-----+\n| Name     | Age |\n+----------+-----+\n| __NULL__ | 31  |\n+----------+-----+\n\nnebula&gt; UPSERT VERTEX ON player \"player668\" \\\n        SET name = \"Amber\", age = age + 1 \\\n        YIELD name AS Name, age AS Age;\n+---------+----------+\n| Name    | Age      |\n+---------+----------+\n| \"Amber\" | __NULL__ |\n+---------+----------+\n</code></pre> <p>In the last query of the preceding examples, since <code>age</code> has no default value, when the vertex is created, <code>age</code> is <code>NULL</code>, and <code>age = age + 1</code> does not take effect. But if <code>age</code> has a default value, <code>age = age + 1</code> will take effect. For example:</p> <pre><code>nebula&gt; CREATE TAG IF NOT EXISTS player_with_default(name string, age int DEFAULT 20);\nExecution succeeded\n\nnebula&gt; UPSERT VERTEX ON player_with_default \"player101\" \\\n        SET age = age + 1 \\\n        YIELD name AS Name, age AS Age;\n\n+----------+-----+\n| Name     | Age |\n+----------+-----+\n| __NULL__ | 21  |\n+----------+-----+\n</code></pre>"},{"location":"3.ngql-guide/12.vertex-statements/3.upsert-vertex/#update_a_vertex_if_it_exists","title":"Update a vertex if it exists","text":"<p>If the vertex exists and the <code>WHEN</code> conditions are met, the vertex is updated.</p> <pre><code>nebula&gt; FETCH PROP ON player \"player101\" YIELD properties(vertex);\n+--------------------------------+\n| properties(VERTEX)             |\n+--------------------------------+\n| {age: 36, name: \"Tony Parker\"} |\n+--------------------------------+\n\nnebula&gt; UPSERT VERTEX ON player \"player101\" \\\n        SET age = age + 2 \\\n        WHEN name == \"Tony Parker\" \\\n        YIELD name AS Name, age AS Age;\n+---------------+-----+\n| Name          | Age |\n+---------------+-----+\n| \"Tony Parker\" | 38  |\n+---------------+-----+\n</code></pre> <p>If the vertex exists and the <code>WHEN</code> conditions are not met, the update does not take effect.</p> <pre><code>nebula&gt; FETCH PROP ON player \"player101\" YIELD properties(vertex);\n+--------------------------------+\n| properties(VERTEX)             |\n+--------------------------------+\n| {age: 38, name: \"Tony Parker\"} |\n+--------------------------------+\n\nnebula&gt; UPSERT VERTEX ON player \"player101\" \\\n        SET age = age + 2 \\\n        WHEN name == \"Someone else\" \\\n        YIELD name AS Name, age AS Age;\n+---------------+-----+\n| Name          | Age |\n+---------------+-----+\n| \"Tony Parker\" | 38  |\n+---------------+-----+\n</code></pre>"},{"location":"3.ngql-guide/12.vertex-statements/4.delete-vertex/","title":"DELETE VERTEX","text":"<p>By default, the <code>DELETE VERTEX</code> statement deletes vertices but the incoming and outgoing edges of the vertices.</p> <p>Compatibility</p> <ul> <li>NebulaGraph 2.x deletes vertices and their incoming and outgoing edges.</li> </ul> <ul> <li>NebulaGraph 3.5.0 only deletes the vertices, and does not delete the related outgoing and incoming edges of the vertices. At this time, there will be dangling edges by default.</li> </ul> <p>The <code>DELETE VERTEX</code> statement deletes one vertex or multiple vertices at a time. You can use <code>DELETE VERTEX</code> together with pipes. For more information about pipe, see Pipe operator.</p> <p>Note</p> <ul> <li><code>DELETE VERTEX</code> deletes vertices directly.</li> </ul> <ul> <li><code>DELETE TAG</code> deletes a tag with the given name on a specified vertex.</li> </ul>"},{"location":"3.ngql-guide/12.vertex-statements/4.delete-vertex/#syntax","title":"Syntax","text":"<pre><code>DELETE VERTEX &lt;vid&gt; [, &lt;vid&gt; ...] [WITH EDGE];\n</code></pre> <ul> <li>WITH EDGE: deletes vertices and the related incoming and outgoing edges of the vertices.</li> </ul>"},{"location":"3.ngql-guide/12.vertex-statements/4.delete-vertex/#examples","title":"Examples","text":"<p>This query deletes the vertex whose ID is \"team1\".</p> <pre><code># Delete the vertex whose VID is `team1` but the related incoming and outgoing edges are not deleted.\nnebula&gt; DELETE VERTEX \"team1\";\n\n# Delete the vertex whose VID is `team1` and the related incoming and outgoing edges.\nnebula&gt; DELETE VERTEX \"team1\" WITH EDGE;\n</code></pre> <p>This query shows that you can use <code>DELETE VERTEX</code> together with pipe to delete vertices.</p> <pre><code>nebula&gt; GO FROM \"player100\" OVER serve WHERE properties(edge).start_year == \"2021\" YIELD dst(edge) AS id | DELETE VERTEX $-.id;\n</code></pre>"},{"location":"3.ngql-guide/12.vertex-statements/4.delete-vertex/#process_of_deleting_vertices","title":"Process of deleting vertices","text":"<p>Once NebulaGraph deletes the vertices, all edges (incoming and outgoing edges) of the target vertex will become dangling edges. When NebulaGraph deletes the vertices <code>WITH EDGE</code>, NebulaGraph traverses the incoming and outgoing edges related to the vertices and deletes them all. Then NebulaGraph deletes the vertices.</p> <p>Caution</p> <ul> <li>Atomic deletion is not supported during the entire process for now. Please retry when a failure occurs to avoid partial deletion, which will cause pendent edges.</li> <li>Deleting a supernode takes a lot of time. To avoid connection timeout before the deletion is complete, you can modify the parameter <code>--storage_client_timeout_ms</code> in <code>nebula-graphd.conf</code> to extend the timeout period.</li> </ul>"},{"location":"3.ngql-guide/13.edge-statements/1.insert-edge/","title":"INSERT EDGE","text":"<p>The <code>INSERT EDGE</code> statement inserts an edge or multiple edges into a graph space from a source vertex (given by src_vid) to a destination vertex (given by dst_vid) with a specific rank in NebulaGraph.</p> <p>When inserting an edge that already exists, <code>INSERT EDGE</code> overrides the edge.</p>"},{"location":"3.ngql-guide/13.edge-statements/1.insert-edge/#syntax","title":"Syntax","text":"<pre><code>INSERT EDGE [IF NOT EXISTS] &lt;edge_type&gt; ( &lt;prop_name_list&gt; ) VALUES \n&lt;src_vid&gt; -&gt; &lt;dst_vid&gt;[@&lt;rank&gt;] : ( &lt;prop_value_list&gt; )\n[, &lt;src_vid&gt; -&gt; &lt;dst_vid&gt;[@&lt;rank&gt;] : ( &lt;prop_value_list&gt; ), ...];\n\n&lt;prop_name_list&gt; ::=\n  [ &lt;prop_name&gt; [, &lt;prop_name&gt; ] ...]\n\n&lt;prop_value_list&gt; ::=\n  [ &lt;prop_value&gt; [, &lt;prop_value&gt; ] ...]\n</code></pre> <ul> <li> <p><code>IF NOT EXISTS</code> detects if the edge that you want to insert exists. If it does not exist, a new one will be inserted.</p> <p>Note</p> <ul> <li><code>IF NOT EXISTS</code> only detects whether  exist and does not detect whether the property values overlap. <li><code>IF NOT EXISTS</code> will read to check whether the data exists, which will have a significant impact on performance.</li> <ul> <li><code>&lt;edge_type&gt;</code> denotes the edge type, which must be created before <code>INSERT EDGE</code>. Only one edge type can be specified in this statement.</li> </ul> <ul> <li><code>&lt;prop_name_list&gt;</code> is the property name list in the given <code>&lt;edge_type&gt;</code>.</li> </ul> <ul> <li><code>src_vid</code> is the VID of the source vertex. It specifies the start of an edge.</li> </ul> <ul> <li><code>dst_vid</code> is the VID of the destination vertex. It specifies the end of an edge.</li> </ul> <ul> <li> <p><code>rank</code> is optional. It specifies the edge rank of the same edge type. The data type is <code>int</code>. If not specified, the default value is <code>0</code>. You can insert many edges with the same edge type, source vertex, and destination vertex by using different rank values.</p> <p>OpenCypher compatibility</p> <p>OpenCypher has no such concept as rank.</p> </li> </ul> <ul> <li><code>&lt;prop_value_list&gt;</code> must provide the value list according to <code>&lt;prop_name_list&gt;</code>. If the property values do not match the data type in the edge type, an error is returned. When the <code>NOT NULL</code> constraint is set for a given property, an error is returned if no property is given. When the default value for a property is <code>NULL</code>, you can omit to specify the property value. For details, see CREATE EDGE.</li> </ul>"},{"location":"3.ngql-guide/13.edge-statements/1.insert-edge/#examples","title":"Examples","text":"<pre><code># The following example creates edge type e1 with no property and inserts an edge from vertex \"10\" to vertex \"11\" with no property.\nnebula&gt; CREATE EDGE IF NOT EXISTS e1();                 \nnebula&gt; INSERT EDGE e1 () VALUES \"10\"-&gt;\"11\":();  \n\n# The following example inserts an edge from vertex \"10\" to vertex \"11\" with no property. The edge rank is 1.\nnebula&gt; INSERT EDGE e1 () VALUES \"10\"-&gt;\"11\"@1:(); \n</code></pre> <pre><code>nebula&gt; CREATE EDGE IF NOT EXISTS e2 (name string, age int); \nnebula&gt; INSERT EDGE e2 (name, age) VALUES \"11\"-&gt;\"13\":(\"n1\", 1);\n\n# The following example creates edge type e2 with two properties.\nnebula&gt; INSERT EDGE e2 (name, age) VALUES \\\n        \"12\"-&gt;\"13\":(\"n1\", 1), \"13\"-&gt;\"14\":(\"n2\", 2); \n\n# In the following example, the insertion fails because \"a13\" is not int.\nnebula&gt; INSERT EDGE e2 (name, age) VALUES \"11\"-&gt;\"13\":(\"n1\", \"a13\");\n</code></pre> <p>An edge can be inserted/written with property values multiple times. Only the last written values can be read.</p> <pre><code>The following examples insert edge e2 with the new values for multiple times.\nnebula&gt; INSERT EDGE e2 (name, age) VALUES \"11\"-&gt;\"13\":(\"n1\", 12);\nnebula&gt; INSERT EDGE e2 (name, age) VALUES \"11\"-&gt;\"13\":(\"n1\", 13);\nnebula&gt; INSERT EDGE e2 (name, age) VALUES \"11\"-&gt;\"13\":(\"n1\", 14);\nnebula&gt; FETCH PROP ON e2 \"11\"-&gt;\"13\" YIELD edge AS e;\n+-------------------------------------------+\n| e                                         |\n+-------------------------------------------+\n| [:e2 \"11\"-&gt;\"13\" @0 {age: 14, name: \"n1\"}] |\n+-------------------------------------------+\n</code></pre> <p>If you insert an edge that already exists with <code>IF NOT EXISTS</code>, there will be no modification.</p> <pre><code># The following example inserts edge e2 from vertex \"14\" to vertex \"15\".\nnebula&gt; INSERT EDGE e2 (name, age) VALUES \"14\"-&gt;\"15\"@1:(\"n1\", 12);\n# The following example alters the edge with IF NOT EXISTS. But there will be no alteration because edge e2 already exists.\nnebula&gt; INSERT EDGE IF NOT EXISTS e2 (name, age) VALUES \"14\"-&gt;\"15\"@1:(\"n2\", 13);\nnebula&gt; FETCH PROP ON e2 \"14\"-&gt;\"15\"@1 YIELD edge AS e;\n+-------------------------------------------+\n| e                                         |\n+-------------------------------------------+\n| [:e2 \"14\"-&gt;\"15\" @1 {age: 12, name: \"n1\"}] |\n+-------------------------------------------+\n</code></pre> <p>Note</p> <ul> <li>NebulaGraph 3.5.0 allows dangling edges. Therefore, you can write the edge before the source vertex or the destination vertex exists. At this time, you can get the (not written) vertex VID through <code>&lt;edgetype&gt;._src</code> or <code>&lt;edgetype&gt;._dst</code> (which is not recommended).</li> <li>Atomic operation is not guaranteed during the entire process for now. If it fails, please try again. Otherwise, partial writing will occur. At this time, the behavior of reading the data is undefined. For example, if multiple machines are involved in the write operation, only one of the forward and reverse edges of a single edge is written successfully, or only part of the edge is written successfully when multiple edges are inserted. In this case, an error will be returned, so please execute the command again.</li> <li>Concurrently writing the same edge will cause an <code>edge conflict</code> error, so please try again later.</li> <li>The inserting speed of an edge is about half that of a vertex. Because in the storaged process, the insertion of an edge involves two tasks, while the insertion of a vertex involves only one task.</li> </ul>"},{"location":"3.ngql-guide/13.edge-statements/2.update-edge/","title":"UPDATE EDGE","text":"<p>The <code>UPDATE EDGE</code> statement updates properties on an edge.</p> <p>In NebulaGraph, <code>UPDATE EDGE</code> supports compare-and-swap (CAS).</p>"},{"location":"3.ngql-guide/13.edge-statements/2.update-edge/#syntax","title":"Syntax","text":"<pre><code>UPDATE EDGE ON &lt;edge_type&gt;\n&lt;src_vid&gt; -&gt; &lt;dst_vid&gt; [@&lt;rank&gt;]\nSET &lt;update_prop&gt;\n[WHEN &lt;condition&gt;]\n[YIELD &lt;output&gt;]\n</code></pre> Parameter Required Description Example <code>ON &lt;edge_type&gt;</code> Yes Specifies the edge type. The properties to be updated must be on this edge type. <code>ON serve</code> <code>&lt;src_vid&gt;</code> Yes Specifies the source vertex ID of the edge. <code>\"player100\"</code> <code>&lt;dst_vid&gt;</code> Yes Specifies the destination vertex ID of the edge. <code>\"team204\"</code> <code>&lt;rank&gt;</code> No Specifies the rank of the edge.  The data type is <code>int</code>. <code>10</code> <code>SET &lt;update_prop&gt;</code> Yes Specifies the properties to be updated and how they will be updated. <code>SET start_year = start_year +1</code> <code>WHEN &lt;condition&gt;</code> No Specifies the filter conditions. If <code>&lt;condition&gt;</code> evaluates to <code>false</code>, the <code>SET</code> clause does not take effect. <code>WHEN end_year &lt; 2010</code> <code>YIELD &lt;output&gt;</code> No Specifies the output format of the statement. <code>YIELD start_year AS Start_Year</code>"},{"location":"3.ngql-guide/13.edge-statements/2.update-edge/#example","title":"Example","text":"<p>The following example checks the properties of the edge with the GO statement.</p> <pre><code>nebula&gt; GO FROM \"player100\" \\\n        OVER serve \\\n        YIELD properties(edge).start_year, properties(edge).end_year;\n+-----------------------------+---------------------------+\n| properties(EDGE).start_year | properties(EDGE).end_year |\n+-----------------------------+---------------------------+\n| 1997                        | 2016                      |\n+-----------------------------+---------------------------+\n</code></pre> <p>The following example updates the <code>start_year</code> property and returns the <code>end_year</code> and the new <code>start_year</code>.</p> <pre><code>nebula&gt; UPDATE EDGE on serve \"player100\" -&gt; \"team204\"@0 \\\n        SET start_year = start_year + 1 \\\n        WHEN end_year &gt; 2010 \\\n        YIELD start_year, end_year;\n+------------+----------+\n| start_year | end_year |\n+------------+----------+\n| 1998       | 2016     |\n+------------+----------+\n</code></pre>"},{"location":"3.ngql-guide/13.edge-statements/3.upsert-edge/","title":"UPSERT EDGE","text":"<p>The <code>UPSERT</code> statement is a combination of <code>UPDATE</code> and <code>INSERT</code>. You can use <code>UPSERT EDGE</code> to update the properties of an edge if it exists or insert a new edge if it does not exist.</p> <p>The performance of <code>UPSERT</code> is much lower than that of <code>INSERT</code> because <code>UPSERT</code> is a read-modify-write serialization operation at the partition level.</p> <p>Danger</p> <p>Do not use <code>UPSERT</code> for scenarios with highly concurrent writes. You can use <code>UPDATE</code> or <code>INSERT</code> instead.</p>"},{"location":"3.ngql-guide/13.edge-statements/3.upsert-edge/#syntax","title":"Syntax","text":"<pre><code>UPSERT EDGE ON &lt;edge_type&gt;\n&lt;src_vid&gt; -&gt; &lt;dst_vid&gt; [@rank]\nSET &lt;update_prop&gt;\n[WHEN &lt;condition&gt;]\n[YIELD &lt;properties&gt;]\n</code></pre> Parameter Required Description Example <code>ON &lt;edge_type&gt;</code> Yes Specifies the edge type. The properties to be updated must be on this edge type. <code>ON serve</code> <code>&lt;src_vid&gt;</code> Yes Specifies the source vertex ID of the edge. <code>\"player100\"</code> <code>&lt;dst_vid&gt;</code> Yes Specifies the destination vertex ID of the edge. <code>\"team204\"</code> <code>&lt;rank&gt;</code> No Specifies the rank of the edge. <code>10</code> <code>SET &lt;update_prop&gt;</code> Yes Specifies the properties to be updated and how they will be updated. <code>SET start_year = start_year +1</code> <code>WHEN &lt;condition&gt;</code> No Specifies the filter conditions. <code>WHEN end_year &lt; 2010</code> <code>YIELD &lt;output&gt;</code> No Specifies the output format of the statement. <code>YIELD start_year AS Start_Year</code>"},{"location":"3.ngql-guide/13.edge-statements/3.upsert-edge/#insert_an_edge_if_it_does_not_exist","title":"Insert an edge if it does not exist","text":"<p>If an edge does not exist, it is created no matter the conditions in the <code>WHEN</code> clause are met or not, and the <code>SET</code> clause takes effect. The property values of the new edge depend on:</p> <ul> <li>How the <code>SET</code> clause is defined.</li> </ul> <ul> <li>Whether the property has a default value.</li> </ul> <p>For example, if:</p> <ul> <li>The edge to be inserted will have properties <code>start_year</code> and <code>end_year</code> based on the edge type <code>serve</code>.</li> </ul> <ul> <li>The <code>SET</code> clause specifies that <code>end_year = 2021</code>.</li> </ul> <p>Then the property values in different cases are listed as follows:</p> Are <code>WHEN</code> conditions met If properties have default values Value of <code>start_year</code> Value of <code>end_year</code> Yes Yes The default value <code>2021</code> Yes No <code>NULL</code> <code>2021</code> No Yes The default value <code>2021</code> No No <code>NULL</code> <code>2021</code> <p>Here are some examples:</p> <pre><code>// This example checks if the following three vertices have any outgoing serve edge. The result \"Empty set\" indicates that such an edge does not exist.\nnebula&gt; GO FROM \"player666\", \"player667\", \"player668\" \\\n        OVER serve \\\n        YIELD properties(edge).start_year, properties(edge).end_year;\n+-----------------------------+---------------------------+\n| properties(EDGE).start_year | properties(EDGE).end_year |\n+-----------------------------+---------------------------+\n+-----------------------------+---------------------------+\nEmpty set\n\nnebula&gt; UPSERT EDGE on serve \\\n        \"player666\" -&gt; \"team200\"@0 \\\n        SET end_year = 2021 \\\n        WHEN end_year == 2010 \\\n        YIELD start_year, end_year;\n+------------+----------+\n| start_year | end_year |\n+------------+----------+\n| __NULL__   | 2021     |\n+------------+----------+\n\nnebula&gt; UPSERT EDGE on serve \\\n        \"player666\" -&gt; \"team200\"@0 \\\n        SET end_year = 2022 \\\n        WHEN end_year == 2010 \\\n        YIELD start_year, end_year;\n+------------+----------+\n| start_year | end_year |\n+------------+----------+\n| __NULL__   | 2021     |\n+------------+----------+\n\nnebula&gt; UPSERT EDGE on serve \\\n        \"player667\" -&gt; \"team200\"@0 \\\n        SET end_year = 2022 \\\n        YIELD start_year, end_year;\n+------------+----------+\n| start_year | end_year |\n+------------+----------+\n| __NULL__   | 2022     |\n+------------+----------+\n\nnebula&gt; UPSERT EDGE on serve \\\n        \"player668\" -&gt; \"team200\"@0 \\\n        SET start_year = 2000, end_year = end_year + 1 \\\n        YIELD start_year, end_year;\n+------------+----------+\n| start_year | end_year |\n+------------+----------+\n| 2000       | __NULL__ |\n+------------+----------+\n</code></pre> <p>In the last query of the preceding example, since <code>end_year</code> has no default value, when the edge is created, <code>end_year</code> is <code>NULL</code>, and <code>end_year = end_year + 1</code> does not take effect. But if <code>end_year</code> has a default value, <code>end_year = end_year + 1</code> will take effect. For example:</p> <pre><code>nebula&gt; CREATE EDGE IF NOT EXISTS serve_with_default(start_year int, end_year int DEFAULT 2010);\nExecution succeeded\n\nnebula&gt; UPSERT EDGE on serve_with_default \\\n        \"player668\" -&gt; \"team200\" \\\n        SET end_year = end_year + 1 \\\n        YIELD start_year, end_year;\n+------------+----------+\n| start_year | end_year |\n+------------+----------+\n| __NULL__   | 2011     |\n+------------+----------+\n</code></pre>"},{"location":"3.ngql-guide/13.edge-statements/3.upsert-edge/#update_an_edge_if_it_exists","title":"Update an edge if it exists","text":"<p>If the edge exists and the <code>WHEN</code> conditions are met, the edge is updated.</p> <pre><code>nebula&gt; MATCH (v:player{name:\"Ben Simmons\"})-[e:serve]-(v2) \\\n        RETURN e;\n+-----------------------------------------------------------------------+\n| e                                                                     |\n+-----------------------------------------------------------------------+\n| [:serve \"player149\"-&gt;\"team219\" @0 {end_year: 2019, start_year: 2016}] |\n+-----------------------------------------------------------------------+\n\nnebula&gt; UPSERT EDGE on serve \\\n        \"player149\" -&gt; \"team219\" \\\n        SET end_year = end_year + 1 \\\n        WHEN start_year == 2016 \\\n        YIELD start_year, end_year;\n+------------+----------+\n| start_year | end_year |\n+------------+----------+\n| 2016       | 2020     |\n+------------+----------+\n</code></pre> <p>If the edge exists and the <code>WHEN</code> conditions are not met, the update does not take effect.</p> <pre><code>nebula&gt; MATCH (v:player{name:\"Ben Simmons\"})-[e:serve]-(v2) \\\n        RETURN e;\n+-----------------------------------------------------------------------+\n| e                                                                     |\n+-----------------------------------------------------------------------+\n| [:serve \"player149\"-&gt;\"team219\" @0 {end_year: 2020, start_year: 2016}] |\n+-----------------------------------------------------------------------+\n\n\nnebula&gt; UPSERT EDGE on serve \\\n        \"player149\" -&gt; \"team219\" \\\n        SET end_year = end_year + 1 \\\n        WHEN start_year != 2016 \\\n        YIELD start_year, end_year;\n+------------+----------+\n| start_year | end_year |\n+------------+----------+\n| 2016       | 2020     |\n+------------+----------+\n</code></pre>"},{"location":"3.ngql-guide/13.edge-statements/4.delete-edge/","title":"DELETE EDGE","text":"<p>The <code>DELETE EDGE</code> statement deletes one edge or multiple edges at a time. You can use <code>DELETE EDGE</code> together with pipe operators. For more information, see PIPE OPERATORS.</p> <p>To delete all the outgoing edges for a vertex, please delete the vertex. For more information, see DELETE VERTEX.</p>"},{"location":"3.ngql-guide/13.edge-statements/4.delete-edge/#syntax","title":"Syntax","text":"<pre><code>DELETE EDGE &lt;edge_type&gt; &lt;src_vid&gt; -&gt; &lt;dst_vid&gt;[@&lt;rank&gt;] [, &lt;src_vid&gt; -&gt; &lt;dst_vid&gt;[@&lt;rank&gt;] ...]\n</code></pre> <p>Caution</p> <p>If no rank is specified, NebulaGraph only deletes the edge with rank 0. Delete edges with all ranks, as shown in the following example.</p>"},{"location":"3.ngql-guide/13.edge-statements/4.delete-edge/#examples","title":"Examples","text":"<pre><code>nebula&gt; DELETE EDGE serve \"player100\" -&gt; \"team204\"@0;\n</code></pre> <p>The following example shows that you can use <code>DELETE EDGE</code> together with pipe operators to delete edges that meet the conditions.</p> <pre><code>nebula&gt; GO FROM \"player100\" OVER follow \\\n        WHERE dst(edge) == \"player101\" \\\n        YIELD src(edge) AS src, dst(edge) AS dst, rank(edge) AS rank \\\n        | DELETE EDGE follow $-.src-&gt;$-.dst @ $-.rank;\n</code></pre>"},{"location":"3.ngql-guide/14.native-index-statements/","title":"Index overview","text":"<p>Indexes are built to fast process graph queries. Nebula\u00a0Graph supports two kinds of indexes: native indexes and full-text indexes. This topic introduces the index types and helps choose the right index.</p>"},{"location":"3.ngql-guide/14.native-index-statements/#usage_instructions","title":"Usage Instructions","text":"<ul> <li>Indexes can improve query performance but may reduce write performance.</li> </ul> <ul> <li>An index is a prerequisite for locating data when executing a <code>LOOKUP</code>statement. If there is no index, an error will be reported when executing the <code>LOOKUP</code> statement.</li> </ul> <ul> <li>When using an index, NebulaGraph will automatically select the most optimal index.</li> </ul> <ul> <li>Indexes with high selectivity, that is, when the ratio of the number of records with unique values in the index column to the total number of records is high (for example, the ratio for <code>ID numbers</code> is <code>1</code>), can significantly improve query performance. For indexes with low selectivity (such as <code>country</code>), query performance might not experience a substantial improvement.</li> </ul>"},{"location":"3.ngql-guide/14.native-index-statements/#native_indexes","title":"Native indexes","text":"<p>Native indexes allow querying data based on a given property. Features are as follows.</p> <ul> <li>There are two kinds of native indexes: tag index and edge type index.</li> </ul> <ul> <li>Native indexes must be updated manually. You can use the <code>REBUILD INDEX</code> statement to update native indexes.</li> </ul> <ul> <li>Native indexes support indexing multiple properties on a tag or an edge type (composite indexes), but do not support indexing across multiple tags or edge types.</li> </ul>"},{"location":"3.ngql-guide/14.native-index-statements/#operations_on_native_indexes","title":"Operations on native indexes","text":"<ul> <li>CREATE INDEX</li> </ul> <ul> <li>SHOW CREATE INDEX</li> </ul> <ul> <li>SHOW INDEXES</li> </ul> <ul> <li>DESCRIBE INDEX</li> </ul> <ul> <li>REBUILD INDEX</li> </ul> <ul> <li>SHOW INDEX STATUS</li> </ul> <ul> <li>DROP INDEX</li> </ul> <ul> <li>LOOKUP</li> </ul> <ul> <li>MATCH</li> </ul> <ul> <li>Geography index</li> </ul>"},{"location":"3.ngql-guide/14.native-index-statements/#full-text_indexes","title":"Full-text indexes","text":"<p>Full-text indexes are used to do prefix, wildcard, regexp, and fuzzy search on a string property. Features are as follows.</p> <ul> <li>Full-text indexes allow indexing just one property.</li> </ul> <ul> <li>Full-text indexes do not support logical operations such as <code>AND</code>, <code>OR</code>, and <code>NOT</code>.</li> </ul> <p>Note</p> <p>To do complete string matches, use native indexes.</p>"},{"location":"3.ngql-guide/14.native-index-statements/#null_values","title":"Null values","text":"<p>Indexes do not support indexing null values.</p>"},{"location":"3.ngql-guide/14.native-index-statements/#range_queries","title":"Range queries","text":"<p>In addition to querying single results from native indexes, you can also do range queries. Not all the native indexes support range queries. You can only do range searches for numeric, date, and time type properties.</p>"},{"location":"3.ngql-guide/14.native-index-statements/1.create-native-index/","title":"CREATE INDEX","text":""},{"location":"3.ngql-guide/14.native-index-statements/1.create-native-index/#prerequisites","title":"Prerequisites","text":"<p>Before you create an index, make sure that the relative tag or edge type is created. For how to create tags or edge types, see CREATE TAG and CREATE EDGE.</p> <p>For how to create full-text indexes, see Deploy full-text index.</p>"},{"location":"3.ngql-guide/14.native-index-statements/1.create-native-index/#must-read_for_using_indexes","title":"Must-read for using indexes","text":"<p>The concept and using restrictions of indexes are comparatively complex. Before you use indexes, you must read the following sections carefully.</p> <p>You can use <code>CREATE INDEX</code> to add native indexes for the existing tags, edge types, or properties. They are usually called as tag indexes, edge type indexes, and property indexes.</p> <ul> <li>Tag indexes and edge type indexes apply to queries related to the tag and the edge type, but do not apply to queries that are based on certain properties on the tag. For example, you can use <code>LOOKUP</code> to retrieve all the vertices with the tag <code>player</code>.</li> </ul> <ul> <li>Property indexes apply to property-based queries. For example, you can use the <code>age</code> property to retrieve the VID of all vertices that meet <code>age == 19</code>.</li> </ul> <p>If a property index <code>i_TA</code> is created for the property <code>A</code> of the tag <code>T</code> and <code>i_T</code> for the tag <code>T</code>, the indexes can be replaced as follows (the same for edge type indexes):</p> <ul> <li>The query engine can use <code>i_TA</code> to replace <code>i_T</code>.</li> </ul> <ul> <li> <p>In the <code>MATCH</code> and <code>LOOKUP</code> statements, <code>i_T</code> may replace <code>i_TA</code> for querying properties.</p> <p>Legacy version compatibility</p> <p>In previous releases, the tag or edge type index in the <code>LOOKUP</code> statement cannot replace the property index for property queries.</p> </li> </ul> <p>Although the same results can be obtained by using alternative indexes for queries, the query performance varies according to the selected index.</p> <p>Caution</p> <p>Indexes can dramatically reduce the write performance. The performance can be greatly reduced. DO NOT use indexes in production environments unless you are fully aware of their influences on your service.</p> <p>Indexes cannot make queries faster. It can only locate a vertex or an edge according to properties or count the number of vertices or edges.</p> <p>Long indexes decrease the scan performance of the Storage Service and use more memory. We suggest that you set the indexing length the same as that of the longest string to be indexed. The longest index length is 256 bytes.</p>"},{"location":"3.ngql-guide/14.native-index-statements/1.create-native-index/#steps","title":"Steps","text":"<p>If you must use indexes, we suggest that you:</p> <ol> <li> <p>Import the data into NebulaGraph.</p> </li> <li> <p>Create indexes.</p> </li> <li> <p>Rebuild indexes.</p> </li> <li> <p>After the index is created and the data is imported, you can use LOOKUP or MATCH to retrieve the data. You do not need to specify which indexes to use in a query, NebulaGraph figures that out by itself.</p> </li> </ol> <p>Note</p> <p>If you create an index before importing the data, the importing speed will be extremely slow due to the reduction in the write performance.</p> <p>Keep <code>--disable_auto_compaction = false</code> during daily incremental writing.</p> <p>The newly created index will not take effect immediately. Trying to use a newly created index (such as <code>LOOKUP</code> or<code>REBUILD INDEX</code>) may fail and return <code>can't find xxx in the space</code> because the creation is implemented asynchronously. To make sure the follow-up operations work as expected, Wait for two heartbeat cycles, i.e., 20 seconds. To change the heartbeat interval, modify the <code>heartbeat_interval_secs</code> in the configuration files for all services.</p> <p>Danger</p> <p>After creating a new index, or dropping the old index and creating a new one with the same name again, you must <code>REBUILD INDEX</code>. Otherwise, these data cannot be returned in the <code>MATCH</code> and <code>LOOKUP</code> statements.</p>"},{"location":"3.ngql-guide/14.native-index-statements/1.create-native-index/#syntax","title":"Syntax","text":"<pre><code>CREATE {TAG | EDGE} INDEX [IF NOT EXISTS] &lt;index_name&gt; ON {&lt;tag_name&gt; | &lt;edge_name&gt;} ([&lt;prop_name_list&gt;]) [COMMENT '&lt;comment&gt;'];\n</code></pre> Parameter Description <code>TAG | EDGE</code> Specifies the index type that you want to create. <code>IF NOT EXISTS</code> Detects if the index that you want to create exists. If it does not exist, a new one will be created. <code>&lt;index_name&gt;</code> The name of the index. It must be unique in a graph space. A recommended way of naming is <code>i_tagName_propName</code>. Index names cannot start with a number. They supports 1 to 4 bytes UTF-8 encoded characters, such as English letters (case-sensitive), numbers, and Chinese characters, but does not support special characters except underscores. To use special characters, reserved keywords, or starting with a number, quote them with backticks. For more information, see Keywords and reserved words.Note: If you name an index in Chinese and encounter a <code>SyntaxError</code>, you need to quote the Chinese characters with backticks (`). <code>&lt;tag_name&gt; | &lt;edge_name&gt;</code> Specifies the name of the tag or edge associated with the index. <code>&lt;prop_name_list&gt;</code> To index a variable-length string property, you must use <code>prop_name(length)</code> to specify the index length. To index a tag or an edge type, ignore the <code>prop_name_list</code>. <code>COMMENT</code> The remarks of the index. The maximum length is 256 bytes. By default, there will be no comments on an index."},{"location":"3.ngql-guide/14.native-index-statements/1.create-native-index/#create_tagedge_type_indexes","title":"Create tag/edge type indexes","text":"<pre><code>nebula&gt; CREATE TAG INDEX IF NOT EXISTS player_index on player();\n</code></pre> <pre><code>nebula&gt; CREATE EDGE INDEX IF NOT EXISTS follow_index on follow();\n</code></pre> <p>After indexing a tag or an edge type, you can use the <code>LOOKUP</code> statement to retrieve the VID of all vertices <code>with the tag</code>, or <code>the source vertex ID, destination vertex ID, and ranks</code> of <code>all edges with the edge type</code>. For more information, see LOOKUP.</p>"},{"location":"3.ngql-guide/14.native-index-statements/1.create-native-index/#create_single-property_indexes","title":"Create single-property indexes","text":"<pre><code>nebula&gt; CREATE TAG INDEX IF NOT EXISTS player_index_0 on player(name(10));\n</code></pre> <p>The preceding example creates an index for the <code>name</code> property on all vertices carrying the <code>player</code> tag. This example creates an index using the first 10 characters of the <code>name</code> property.</p> <pre><code># To index a variable-length string property, you need to specify the index length.\nnebula&gt; CREATE TAG IF NOT EXISTS var_string(p1 string);\nnebula&gt; CREATE TAG INDEX IF NOT EXISTS var ON var_string(p1(10));\n\n# To index a fixed-length string property, you do not need to specify the index length.\nnebula&gt; CREATE TAG IF NOT EXISTS fix_string(p1 FIXED_STRING(10));\nnebula&gt; CREATE TAG INDEX IF NOT EXISTS fix ON fix_string(p1);\n</code></pre> <pre><code>nebula&gt; CREATE EDGE INDEX IF NOT EXISTS follow_index_0 on follow(degree);\n</code></pre>"},{"location":"3.ngql-guide/14.native-index-statements/1.create-native-index/#create_composite_property_indexes","title":"Create composite property indexes","text":"<p>An index on multiple properties on a tag (or an edge type) is called a composite property index.</p> <pre><code>nebula&gt; CREATE TAG INDEX IF NOT EXISTS player_index_1 on player(name(10), age);\n</code></pre> <p>Caution</p> <p>Creating composite property indexes across multiple tags or edge types is not supported.</p> <p>Note</p> <p>NebulaGraph follows the left matching principle to select indexes.</p>"},{"location":"3.ngql-guide/14.native-index-statements/2.1.show-create-index/","title":"SHOW CREATE INDEX","text":"<p><code>SHOW CREATE INDEX</code> shows the statement used when creating a tag or an edge type. It contains detailed information about the index, such as its associated properties.</p>"},{"location":"3.ngql-guide/14.native-index-statements/2.1.show-create-index/#syntax","title":"Syntax","text":"<pre><code>SHOW CREATE {TAG | EDGE} INDEX &lt;index_name&gt;;\n</code></pre>"},{"location":"3.ngql-guide/14.native-index-statements/2.1.show-create-index/#examples","title":"Examples","text":"<p>You can run <code>SHOW TAG INDEXES</code> to list all tag indexes, and then use <code>SHOW CREATE TAG INDEX</code> to show the information about the creation of the specified index.</p> <pre><code>nebula&gt; SHOW TAG INDEXES;\n+------------------+----------+----------+\n| Index Name       | By Tag   | Columns  |\n+------------------+----------+----------+\n| \"player_index_0\" | \"player\" | []       |\n| \"player_index_1\" | \"player\" | [\"name\"] |\n+------------------+----------+----------+\n\nnebula&gt; SHOW CREATE TAG INDEX player_index_1;\n+------------------+--------------------------------------------------+\n| Tag Index Name   | Create Tag Index                                 |\n+------------------+--------------------------------------------------+\n| \"player_index_1\" | \"CREATE TAG INDEX `player_index_1` ON `player` ( |\n|                  |  `name`(20)                                      |\n|                  | )\"                                               |\n+------------------+--------------------------------------------------+\n</code></pre> <p>Edge indexes can be queried through a similar approach.</p> <pre><code>nebula&gt; SHOW EDGE INDEXES;\n+----------------+----------+---------+\n| Index Name     | By Edge  | Columns |\n+----------------+----------+---------+\n| \"follow_index\" | \"follow\" | []      |\n+----------------+----------+---------+\n\nnebula&gt; SHOW CREATE EDGE INDEX follow_index;\n+-----------------+-------------------------------------------------+\n| Edge Index Name | Create Edge Index                               |\n+-----------------+-------------------------------------------------+\n| \"follow_index\"  | \"CREATE EDGE INDEX `follow_index` ON `follow` ( |\n|                 | )\"                                              |\n+-----------------+-------------------------------------------------+\n</code></pre>"},{"location":"3.ngql-guide/14.native-index-statements/2.show-native-indexes/","title":"SHOW INDEXES","text":"<p><code>SHOW INDEXES</code> shows the defined tag or edge type indexes names in the current graph space.</p>"},{"location":"3.ngql-guide/14.native-index-statements/2.show-native-indexes/#syntax","title":"Syntax","text":"<pre><code>SHOW {TAG | EDGE} INDEXES\n</code></pre>"},{"location":"3.ngql-guide/14.native-index-statements/2.show-native-indexes/#examples","title":"Examples","text":"<pre><code>nebula&gt; SHOW TAG INDEXES;\n+------------------+--------------+-----------------+\n| Index Name       | By Tag       | Columns         |\n+------------------+--------------+-----------------+\n| \"fix\"            | \"fix_string\" | [\"p1\"]          |\n| \"player_index_0\" | \"player\"     | [\"name\"]        |\n| \"player_index_1\" | \"player\"     | [\"name\", \"age\"] |\n| \"var\"            | \"var_string\" | [\"p1\"]          |\n+------------------+--------------+-----------------+\n\nnebula&gt; SHOW EDGE INDEXES;\n+----------------+----------+---------+\n| Index Name     | By Edge  | Columns |\n| \"follow_index\" | \"follow\" | []      |\n+----------------+----------+---------+\n</code></pre> <p>Legacy version compatibility</p> <p>In NebulaGraph 2.x, the <code>SHOW TAG/EDGE INDEXES</code> statement only returns <code>Names</code>.</p>"},{"location":"3.ngql-guide/14.native-index-statements/3.describe-native-index/","title":"DESCRIBE INDEX","text":"<p><code>DESCRIBE INDEX</code> can get the information about the index with a given name, including the property name (Field) and the property type (Type) of the index.</p>"},{"location":"3.ngql-guide/14.native-index-statements/3.describe-native-index/#syntax","title":"Syntax","text":"<pre><code>DESCRIBE {TAG | EDGE} INDEX &lt;index_name&gt;;\n</code></pre>"},{"location":"3.ngql-guide/14.native-index-statements/3.describe-native-index/#examples","title":"Examples","text":"<pre><code>nebula&gt; DESCRIBE TAG INDEX player_index_0;\n+--------+--------------------+\n| Field  | Type               |\n+--------+--------------------+\n| \"name\" | \"fixed_string(30)\" |\n+--------+--------------------+\n\nnebula&gt; DESCRIBE TAG INDEX player_index_1;\n+--------+--------------------+\n| Field  | Type               |\n+--------+--------------------+\n| \"name\" | \"fixed_string(10)\" |\n| \"age\"  | \"int64\"            |\n+--------+--------------------+\n</code></pre>"},{"location":"3.ngql-guide/14.native-index-statements/4.rebuild-native-index/","title":"REBUILD INDEX","text":"<p>Danger</p> <ul> <li>If data is updated or inserted before the creation of the index, you must rebuild the indexes manually to make sure that the indexes contain the previously added data. Otherwise, you cannot use <code>LOOKUP</code> and <code>MATCH</code> to query the data based on the index. If the index is created before any data insertion, there is no need to rebuild the index.</li> <li>When the rebuild of an index is incomplete, queries that rely on the index can use only part of the index and therefore cannot obtain accurate results.</li> </ul> <p>You can use <code>REBUILD INDEX</code> to rebuild the created tag or edge type index. For details on how to create an index, see CREATE INDEX.</p> <p>Performance</p> <p>The speed of rebuilding indexes can be optimized by modifying the <code>rebuild_index_part_rate_limit</code> and <code>snapshot_batch_size</code> parameters in the configuration file. In addition, greater parameter values may result in higher memory and network usage, see Storage Service configurations for details.</p>"},{"location":"3.ngql-guide/14.native-index-statements/4.rebuild-native-index/#syntax","title":"Syntax","text":"<pre><code>REBUILD {TAG | EDGE} INDEX [&lt;index_name_list&gt;];\n\n&lt;index_name_list&gt;::=\n    [index_name [, index_name] ...]\n</code></pre> <ul> <li>Multiple indexes are permitted in a single <code>REBUILD</code> statement, separated by commas. When the index name is not specified, all tag or edge indexes are rebuilt.</li> </ul> <ul> <li>After the rebuilding is complete, you can use the <code>SHOW {TAG | EDGE} INDEX STATUS</code> command to check if the index is successfully rebuilt. For details on index status, see SHOW INDEX STATUS.</li> </ul>"},{"location":"3.ngql-guide/14.native-index-statements/4.rebuild-native-index/#examples","title":"Examples","text":"<pre><code>nebula&gt; CREATE TAG IF NOT EXISTS person(name string, age int, gender string, email string);\nnebula&gt; CREATE TAG INDEX IF NOT EXISTS single_person_index ON person(name(10));\n\n# The following example rebuilds an index and returns the job ID.\nnebula&gt; REBUILD TAG INDEX single_person_index;\n+------------+\n| New Job Id |\n+------------+\n| 31         |\n+------------+\n\n# The following example checks the index status.\nnebula&gt; SHOW TAG INDEX STATUS;\n+-----------------------+--------------+\n| Name                  | Index Status |\n+-----------------------+--------------+\n| \"single_person_index\" | \"FINISHED\"   |\n+-----------------------+--------------+\n\n# You can also use \"SHOW JOB &lt;job_id&gt;\" to check if the rebuilding process is complete.\nnebula&gt; SHOW JOB 31;\n+----------------+---------------------+------------+-------------------------+-------------------------+-------------+\n| Job Id(TaskId) | Command(Dest)       | Status     | Start Time              | Stop Time               | Error Code  |\n+----------------+---------------------+------------+-------------------------+-------------------------+-------------+\n| 31             | \"REBUILD_TAG_INDEX\" | \"FINISHED\" | 2021-07-07T09:04:24.000 | 2021-07-07T09:04:24.000 | \"SUCCEEDED\" |\n| 0              | \"storaged1\"         | \"FINISHED\" | 2021-07-07T09:04:24.000 | 2021-07-07T09:04:28.000 | \"SUCCEEDED\" |\n| 1              | \"storaged2\"         | \"FINISHED\" | 2021-07-07T09:04:24.000 | 2021-07-07T09:04:28.000 | \"SUCCEEDED\" |\n| 2              | \"storaged0\"         | \"FINISHED\" | 2021-07-07T09:04:24.000 | 2021-07-07T09:04:28.000 | \"SUCCEEDED\" |\n| \"Total:3\"      | \"Succeeded:3\"       | \"Failed:0\" | \"In Progress:0\"         | \"\"                      | \"\"          |\n+----------------+---------------------+------------+-------------------------+-------------------------+-------------+\n</code></pre> <p>NebulaGraph creates a job to rebuild the index. The job ID is displayed in the preceding return message. To check if the rebuilding process is complete, use the <code>SHOW JOB &lt;job_id&gt;</code> statement. For more information, see SHOW JOB.</p>"},{"location":"3.ngql-guide/14.native-index-statements/5.show-native-index-status/","title":"SHOW INDEX STATUS","text":"<p><code>SHOW INDEX STATUS</code> returns the name of the created tag or edge type index and its status of job.</p> <p>The index status includes:</p> <ul> <li><code>QUEUE</code>: The job is in a queue.</li> <li><code>RUNNING</code>: The job is running.</li> <li><code>FINISHED</code>: The job is finished.</li> <li><code>FAILED</code>: The job has failed.</li> <li><code>STOPPED</code>: The job has stopped.</li> <li><code>INVALID</code>: The job is invalid.</li> </ul> <p>Note</p> <p>For details on how to create an index, see CREATE INDEX.</p>"},{"location":"3.ngql-guide/14.native-index-statements/5.show-native-index-status/#syntax","title":"Syntax","text":"<pre><code>SHOW {TAG | EDGE} INDEX STATUS;\n</code></pre>"},{"location":"3.ngql-guide/14.native-index-statements/5.show-native-index-status/#example","title":"Example","text":"<pre><code>nebula&gt; SHOW TAG INDEX STATUS;\n+----------------------+--------------+\n| Name                 | Index Status |\n+----------------------+--------------+\n| \"player_index_0\"     | \"FINISHED\"   |\n| \"player_index_1\"     | \"FINISHED\"   |\n+----------------------+--------------+\n</code></pre>"},{"location":"3.ngql-guide/14.native-index-statements/6.drop-native-index/","title":"DROP INDEX","text":"<p><code>DROP INDEX</code> removes an existing index from the current graph space.</p>"},{"location":"3.ngql-guide/14.native-index-statements/6.drop-native-index/#prerequisite","title":"Prerequisite","text":"<p>Running the <code>DROP INDEX</code> statement requires some privileges of <code>DROP TAG INDEX</code> and <code>DROP EDGE INDEX</code> in the given graph space. Otherwise, NebulaGraph throws an error.</p>"},{"location":"3.ngql-guide/14.native-index-statements/6.drop-native-index/#syntax","title":"Syntax","text":"<pre><code>DROP {TAG | EDGE} INDEX [IF EXISTS] &lt;index_name&gt;;\n</code></pre> <p><code>IF EXISTS</code>: Detects whether the index that you want to drop exists. If it exists, it will be dropped.</p>"},{"location":"3.ngql-guide/14.native-index-statements/6.drop-native-index/#example","title":"Example","text":"<pre><code>nebula&gt; DROP TAG INDEX player_index_0;\n</code></pre>"},{"location":"3.ngql-guide/15.full-text-index-statements/1.search-with-text-based-index/","title":"Full-text indexes","text":"NebulaGraph CommunityNebulaGraph Enterprise <p>Full-text indexes are used to do prefix, wildcard, regexp, and fuzzy search on a string property.</p> <p>You can use the <code>WHERE</code> clause to specify the search strings in <code>LOOKUP</code> statements.</p> <p>Full-text indexes are used to do prefix, wildcard, regexp, and fuzzy search on a string property.</p> <p>You can use the <code>WHERE</code> clause to specify the search strings in <code>LOOKUP</code> statements.</p>"},{"location":"3.ngql-guide/15.full-text-index-statements/1.search-with-text-based-index/#prerequisite","title":"Prerequisite","text":"<p>Before using the full-text index, make sure that you have deployed a Elasticsearch cluster and a Listener cluster. For more information, see Deploy Elasticsearch and Deploy Listener.</p>"},{"location":"3.ngql-guide/15.full-text-index-statements/1.search-with-text-based-index/#precaution","title":"Precaution","text":"<p>Before using the full-text index, make sure that you know the restrictions.</p>"},{"location":"3.ngql-guide/15.full-text-index-statements/1.search-with-text-based-index/#natural_language_full-text_search","title":"Natural language full-text search","text":"<p>A natural language search interprets the search string as a phrase in natural human language. The search is case-sensitive and by default prefixes the string with a match. For example, there are three vertices with the tag <code>player</code>. The tag <code>player</code> contains the property <code>name</code>. The <code>name</code> of these three vertices are <code>Kevin Durant</code>, <code>Tim Duncan</code>, and <code>David Beckham</code>. Now that the full-text index of <code>player.name</code> is established, only <code>David Beckham</code> will be queried when using the prefix search statement <code>LOOKUP ON player WHERE PREFIX(player.name,\"D\");</code>.</p>"},{"location":"3.ngql-guide/15.full-text-index-statements/1.search-with-text-based-index/#syntax","title":"Syntax","text":""},{"location":"3.ngql-guide/15.full-text-index-statements/1.search-with-text-based-index/#create_full-text_indexes","title":"Create full-text indexes","text":"<pre><code>CREATE FULLTEXT {TAG | EDGE} INDEX &lt;index_name&gt; ON {&lt;tag_name&gt; | &lt;edge_name&gt;} ([&lt;prop_name&gt;]);\n</code></pre>"},{"location":"3.ngql-guide/15.full-text-index-statements/1.search-with-text-based-index/#show_full-text_indexes","title":"Show full-text indexes","text":"<pre><code>SHOW FULLTEXT INDEXES;\n</code></pre>"},{"location":"3.ngql-guide/15.full-text-index-statements/1.search-with-text-based-index/#rebuild_full-text_indexes","title":"Rebuild full-text indexes","text":"<pre><code>REBUILD FULLTEXT INDEX;\n</code></pre> <p>Caution</p> <p>When there is a large amount of data, rebuilding full-text index is slow, you can modify <code>snapshot_send_files=false</code> in the configuration file of Storage service(<code>nebula-storaged.conf</code>).</p>"},{"location":"3.ngql-guide/15.full-text-index-statements/1.search-with-text-based-index/#drop_full-text_indexes","title":"Drop full-text indexes","text":"<pre><code>DROP FULLTEXT INDEX &lt;index_name&gt;;\n</code></pre>"},{"location":"3.ngql-guide/15.full-text-index-statements/1.search-with-text-based-index/#use_query_options","title":"Use query options","text":"<pre><code>LOOKUP ON {&lt;tag&gt; | &lt;edge_type&gt;} WHERE &lt;expression&gt; [YIELD &lt;return_list&gt;];\n\n&lt;expression&gt; ::=\n    PREFIX | WILDCARD | REGEXP | FUZZY\n\n&lt;return_list&gt;\n    &lt;prop_name&gt; [AS &lt;prop_alias&gt;] [, &lt;prop_name&gt; [AS &lt;prop_alias&gt;] ...]\n</code></pre> <ul> <li>PREFIX(schema_name.prop_name, prefix_string, row_limit, timeout)</li> </ul> <ul> <li>WILDCARD(schema_name.prop_name, wildcard_string, row_limit, timeout)</li> </ul> <ul> <li>REGEXP(schema_name.prop_name, regexp_string, row_limit, timeout)</li> </ul> <ul> <li> <p>FUZZY(schema_name.prop_name, fuzzy_string, fuzziness, operator, row_limit, timeout)</p> <ul> <li><code>fuzziness</code> (optional): Maximum edit distance allowed for matching. The default value is <code>AUTO</code>. For other valid values and more information, see Elasticsearch document.</li> </ul> <ul> <li><code>operator</code> (optional): Boolean logic used to interpret the text. Valid values are <code>OR</code> (default) and <code>AND</code>.</li> </ul> </li> </ul> <ul> <li><code>row_limit</code> (optional): Specifies the number of rows to return. The default value is <code>100</code>.</li> </ul> <ul> <li><code>timeout</code> (optional): Specifies the timeout time. The default value is <code>200ms</code>.</li> </ul>"},{"location":"3.ngql-guide/15.full-text-index-statements/1.search-with-text-based-index/#examples","title":"Examples","text":"<pre><code>// This example creates the graph space.\nnebula&gt; CREATE SPACE IF NOT EXISTS basketballplayer (partition_num=3,replica_factor=1, vid_type=fixed_string(30));\n\n// This example signs in the text service.\nnebula&gt; SIGN IN TEXT SERVICE (127.0.0.1:9200, HTTP);\n\n// This example checks the text service status.\nnebula&gt; SHOW TEXT SEARCH CLIENTS;\n\n// This example switches the graph space.\nnebula&gt; USE basketballplayer;\n\n// This example adds the listener to the NebulaGraph cluster.\nnebula&gt; ADD LISTENER ELASTICSEARCH 192.168.8.5:9789;\n\n// This example checks the listener status. When the status is `Online`, the listener is ready.\nnebula&gt; SHOW LISTENER;\n\n// This example creates the tag.\nnebula&gt; CREATE TAG IF NOT EXISTS player(name string, age int);\n\n// This example creates the full-text index. The index name starts with \"nebula_\".\nnebula&gt; CREATE FULLTEXT TAG INDEX nebula_index_1 ON player(name);\n\n// This example rebuilds the full-text index.\nnebula&gt; REBUILD FULLTEXT INDEX;\n\n// This example shows the full-text index.\nnebula&gt; SHOW FULLTEXT INDEXES;\n+------------------+-------------+-------------+--------+\n| Name             | Schema Type | Schema Name | Fields |\n+------------------+-------------+-------------+--------+\n| \"nebula_index_1\" | \"Tag\"       | \"player\"    | \"name\" |\n+------------------+-------------+-------------+--------+\n\n// This example inserts the test data.\nnebula&gt; INSERT VERTEX player(name, age) VALUES \\\n  \"Russell Westbrook\": (\"Russell Westbrook\", 30), \\\n  \"Chris Paul\": (\"Chris Paul\", 33),\\\n  \"Boris Diaw\": (\"Boris Diaw\", 36),\\\n  \"David West\": (\"David West\", 38),\\\n  \"Danny Green\": (\"Danny Green\", 31),\\\n  \"Tim Duncan\": (\"Tim Duncan\", 42),\\\n  \"James Harden\": (\"James Harden\", 29),\\\n  \"Tony Parker\": (\"Tony Parker\", 36),\\\n  \"Aron Baynes\": (\"Aron Baynes\", 32),\\\n  \"Ben Simmons\": (\"Ben Simmons\", 22),\\\n  \"Blake Griffin\": (\"Blake Griffin\", 30);\n\n// These examples run test queries.\nnebula&gt; LOOKUP ON player WHERE PREFIX(player.name, \"B\") YIELD id(vertex);\n+-----------------+\n| id(VERTEX)      |\n+-----------------+\n| \"Boris Diaw\"    |\n| \"Ben Simmons\"   |\n| \"Blake Griffin\" |\n+-----------------+\n\nnebula&gt; LOOKUP ON player WHERE WILDCARD(player.name, \"*ri*\") YIELD player.name, player.age;\n+-----------------+-----+\n| name            | age |\n+-----------------+-----+\n| \"Chris Paul\"    | 33  |\n| \"Boris Diaw\"    | 36  |\n| \"Blake Griffin\" | 30  |\n+-----------------+-----+\n\nnebula&gt; LOOKUP ON player WHERE WILDCARD(player.name, \"*ri*\") YIELD player.name, player.age | YIELD count(*);\n+----------+\n| count(*) |\n+----------+\n| 3        |\n+----------+\n\nnebula&gt; LOOKUP ON player WHERE REGEXP(player.name, \"R.*\") YIELD player.name, player.age;\n+---------------------+-----+\n| name                | age |\n+---------------------+-----+\n| \"Russell Westbrook\" | 30  |\n+---------------------+-----+\n\nnebula&gt; LOOKUP ON player WHERE REGEXP(player.name, \".*\") YIELD id(vertex);\n+---------------------+\n| id(VERTEX)          |\n+---------------------+\n| \"Danny Green\"       |\n| \"David West\"        |\n...\n\nnebula&gt; LOOKUP ON player WHERE FUZZY(player.name, \"Tim Dunncan\", AUTO, OR) YIELD player.name;\n+--------------+\n| name         |\n+--------------+\n| \"Tim Duncan\" |\n+--------------+\n\n// This example drops the full-text index.\nnebula&gt; DROP FULLTEXT INDEX nebula_index_1;\n</code></pre>"},{"location":"3.ngql-guide/15.full-text-index-statements/1.search-with-text-based-index/#prerequisite_1","title":"Prerequisite","text":"<p>Before using the full-text index, make sure that you have deployed a Elasticsearch cluster and a Listener cluster. For more information, see Deploy Elasticsearch and Deploy Listener.</p>"},{"location":"3.ngql-guide/15.full-text-index-statements/1.search-with-text-based-index/#precaution_1","title":"Precaution","text":"<p>Before using the full-text index, make sure that you know the restrictions.</p>"},{"location":"3.ngql-guide/15.full-text-index-statements/1.search-with-text-based-index/#full_text_queries","title":"Full Text Queries","text":"<p>Full-text queries enable you to search for parsed text fields, using a parser with strict syntax to return content based on the query string provided. For details, see Query string query.</p>"},{"location":"3.ngql-guide/15.full-text-index-statements/1.search-with-text-based-index/#syntax_1","title":"Syntax","text":""},{"location":"3.ngql-guide/15.full-text-index-statements/1.search-with-text-based-index/#create_full-text_indexes_1","title":"Create full-text indexes","text":"<pre><code>CREATE FULLTEXT {TAG | EDGE} INDEX &lt;index_name&gt; ON {&lt;tag_name&gt; | &lt;edge_name&gt;} (&lt;prop_name&gt; [,&lt;prop_name&gt;]...) [ANALYZER=\"&lt;analyzer_name&gt;\"];\n</code></pre> <ul> <li>Composite indexes with multiple properties are supported when creating full-text indexes.</li> <li><code>&lt;analyzer_name&gt;</code> is the name of the analyzer. The default value is <code>standard</code>. To use other analyzers (e.g. IK Analysis), you need to make sure that the corresponding analyzer is installed in Elasticsearch in advance.</li> </ul>"},{"location":"3.ngql-guide/15.full-text-index-statements/1.search-with-text-based-index/#show_full-text_indexes_1","title":"Show full-text indexes","text":"<pre><code>SHOW FULLTEXT INDEXES;\n</code></pre>"},{"location":"3.ngql-guide/15.full-text-index-statements/1.search-with-text-based-index/#rebuild_full-text_indexes_1","title":"Rebuild full-text indexes","text":"<pre><code>REBUILD FULLTEXT INDEX;\n</code></pre> <p>Caution</p> <p>When there is a large amount of data, rebuilding full-text index is slow, you can modify <code>snapshot_send_files=false</code> in the configuration file of Storage service(<code>nebula-storaged.conf</code>).</p>"},{"location":"3.ngql-guide/15.full-text-index-statements/1.search-with-text-based-index/#drop_full-text_indexes_1","title":"Drop full-text indexes","text":"<pre><code>DROP FULLTEXT INDEX &lt;index_name&gt;;\n</code></pre>"},{"location":"3.ngql-guide/15.full-text-index-statements/1.search-with-text-based-index/#use_query_options_1","title":"Use query options","text":"<pre><code>LOOKUP ON {&lt;tag&gt; | &lt;edge_type&gt;} WHERE ES_QUERY(&lt;index_name&gt;, \"&lt;text&gt;\") YIELD &lt;return_list&gt; [| LIMIT [&lt;offset&gt;,] &lt;number_rows&gt;];\n\n&lt;return_list&gt;\n    &lt;prop_name&gt; [AS &lt;prop_alias&gt;] [, &lt;prop_name&gt; [AS &lt;prop_alias&gt;] ...] [, id(vertex)  [AS &lt;prop_alias&gt;]] [, score() AS &lt;score_alias&gt;]\n</code></pre> <ul> <li><code>index_name</code>: The name of the full-text index.</li> </ul> <ul> <li><code>text</code>: Search conditions. For supported syntax, see Query string syntax.</li> </ul> <ul> <li><code>score()</code>: The score calculated by doing N degree expansion for the eligible vertices. The default value is <code>1.0</code>. The higher the score, the higher the degree of match. The return value is sorted by default from highest to lowest score. For details, see Search and Scoring in Lucene.</li> </ul>"},{"location":"3.ngql-guide/15.full-text-index-statements/1.search-with-text-based-index/#examples_1","title":"Examples","text":"<pre><code>// This example creates the graph space.\nnebula&gt; CREATE SPACE IF NOT EXISTS basketballplayer (partition_num=3,replica_factor=1, vid_type=fixed_string(30));\n\n// This example signs in the text service.\nnebula&gt; SIGN IN TEXT SERVICE (192.168.8.100:9200, HTTP);\n\n// This example checks the text service status.\nnebula&gt; SHOW TEXT SEARCH CLIENTS;\n+-----------------+-----------------+------+\n| Type            | Host            | Port |\n+-----------------+-----------------+------+\n| \"ELASTICSEARCH\" | \"192.168.8.100\" | 9200 |\n+-----------------+-----------------+------+\n\n// This example switches the graph space.\nnebula&gt; USE basketballplayer;\n\n// This example adds the listener to the NebulaGraph cluster.\nnebula&gt; ADD LISTENER ELASTICSEARCH 192.168.8.100:9789;\n\n// This example checks the listener status. When the status is `Online`, the listener is ready.\nnebula&gt; SHOW LISTENER;\n+--------+-----------------+------------------------+-------------+\n| PartId | Type            | Host                   | Host Status |\n+--------+-----------------+------------------------+-------------+\n| 1      | \"ELASTICSEARCH\" | \"\"192.168.8.100\":9789\" | \"ONLINE\"    |\n| 2      | \"ELASTICSEARCH\" | \"\"192.168.8.100\":9789\" | \"ONLINE\"    |\n| 3      | \"ELASTICSEARCH\" | \"\"192.168.8.100\":9789\" | \"ONLINE\"    |\n+--------+-----------------+------------------------+-------------+\n\n// This example creates the tag.\nnebula&gt; CREATE TAG IF NOT EXISTS player(name string, city string);\n\n// This example creates a single-attribute full-text index.\nnebula&gt; CREATE FULLTEXT TAG INDEX fulltext_index_1 ON player(name) ANALYZER=\"standard\";\n\n// This example creates a multi-attribute full-text indexe.\nnebula&gt; CREATE FULLTEXT TAG INDEX fulltext_index_2 ON player(name,city) ANALYZER=\"standard\";\n\n// This example rebuilds the full-text index.\nnebula&gt; REBUILD FULLTEXT INDEX;\n\n// This example shows the full-text index.\nnebula&gt; SHOW FULLTEXT INDEXES;\n+--------------------+-------------+-------------+--------------+------------+\n| Name               | Schema Type | Schema Name | Fields       | Analyzer   |\n+--------------------+-------------+-------------+--------------+------------+\n| \"fulltext_index_1\" | \"Tag\"       | \"player\"    | \"name\"       | \"standard\" |\n| \"fulltext_index_2\" | \"Tag\"       | \"player\"    | \"name, city\" | \"standard\" |\n+--------------------+-------------+-------------+--------------+------------+\n\n// This example inserts the test data.\nnebula&gt; INSERT VERTEX player(name, city) VALUES \\\n  \"Russell Westbrook\": (\"Russell Westbrook\", \"Los Angeles\"), \\\n  \"Chris Paul\": (\"Chris Paul\", \"Houston\"),\\\n  \"Boris Diaw\": (\"Boris Diaw\", \"Houston\"),\\\n  \"David West\": (\"David West\", \"Philadelphia\"),\\\n  \"Danny Green\": (\"Danny Green\", \"Philadelphia\"),\\\n  \"Tim Duncan\": (\"Tim Duncan\", \"New York\"),\\\n  \"James Harden\": (\"James Harden\", \"New York\"),\\\n  \"Tony Parker\": (\"Tony Parker\", \"Chicago\"),\\\n  \"Aron Baynes\": (\"Aron Baynes\", \"Chicago\"),\\\n  \"Ben Simmons\": (\"Ben Simmons\", \"Phoenix\"),\\\n  \"Blake Griffin\": (\"Blake Griffin\", \"Phoenix\");\n\n// These examples run test queries.\nnebula&gt; LOOKUP ON player WHERE ES_QUERY(fulltext_index_1,\"Chris\") YIELD id(vertex);\n+--------------+\n| id(VERTEX)   |\n+--------------+\n| \"Chris Paul\" |\n+--------------+\n\nnebula&gt; LOOKUP ON player WHERE ES_QUERY(fulltext_index_1,\"Harden\") YIELD properties(vertex);\n+----------------------------------------------------------------+\n| properties(VERTEX)                                             |\n+----------------------------------------------------------------+\n| {_vid: \"James Harden\", city: \"New York\", name: \"James Harden\"} |\n+----------------------------------------------------------------+\n\nnebula&gt; LOOKUP ON player WHERE ES_QUERY(fulltext_index_1,\"Da*\") YIELD properties(vertex);\n+------------------------------------------------------------------+\n| properties(VERTEX)                                               |\n+------------------------------------------------------------------+\n| {_vid: \"David West\", city: \"Philadelphia\", name: \"David West\"}   |\n| {_vid: \"Danny Green\", city: \"Philadelphia\", name: \"Danny Green\"} |\n+------------------------------------------------------------------+\n\nnebula&gt; LOOKUP ON player WHERE ES_QUERY(fulltext_index_1,\"*b*\") YIELD id(vertex);\n+---------------------+\n| id(VERTEX)          |\n+---------------------+\n| \"Russell Westbrook\" |\n| \"Boris Diaw\"        |\n| \"Aron Baynes\"       |\n| \"Ben Simmons\"       |\n| \"Blake Griffin\"     |\n+---------------------+\n\nnebula&gt; LOOKUP ON player WHERE ES_QUERY(fulltext_index_1,\"*b*\") YIELD id(vertex) | LIMIT 2,3;\n+-----------------+\n| id(VERTEX)      |\n+-----------------+\n| \"Aron Baynes\"   |\n| \"Ben Simmons\"   |\n| \"Blake Griffin\" |\n+-----------------+\n\nnebula&gt; LOOKUP ON player WHERE ES_QUERY(fulltext_index_1,\"*b*\") YIELD id(vertex) | YIELD count(*);\n+----------+\n| count(*) |\n+----------+\n| 5        |\n+----------+\n\nnebula&gt; LOOKUP ON player WHERE ES_QUERY(fulltext_index_1,\"*b*\") YIELD id(vertex), score() AS score;\n+---------------------+-------+\n| id(VERTEX)          | score |\n+---------------------+-------+\n| \"Russell Westbrook\" | 1.0   |\n| \"Boris Diaw\"        | 1.0   |\n| \"Aron Baynes\"       | 1.0   |\n| \"Ben Simmons\"       | 1.0   |\n| \"Blake Griffin\"     | 1.0   |\n+---------------------+-------+\n\n// For documents containing a word `b`, its score will be multiplied by a weighting factor of 4, while for documents containing a word `c`, the default weighting factor of 1 is used.\nnebula&gt; LOOKUP ON player WHERE ES_QUERY(fulltext_index_1,\"*b*^4 OR *c*\") YIELD id(vertex), score() AS score;\n+---------------------+-------+\n| id(VERTEX)          | score |\n+---------------------+-------+\n| \"Russell Westbrook\" | 4.0   |\n| \"Boris Diaw\"        | 4.0   |\n| \"Aron Baynes\"       | 4.0   |\n| \"Ben Simmons\"       | 4.0   |\n| \"Blake Griffin\"     | 4.0   |\n| \"Chris Paul\"        | 1.0   |\n| \"Tim Duncan\"        | 1.0   |\n+---------------------+-------+\n\n// When using a multi-attribute full-text index query, the conditions are matched within all properties of the index.\nnebula&gt; LOOKUP ON player WHERE ES_QUERY(fulltext_index_2,\"*h*\") YIELD properties(vertex);\n+------------------------------------------------------------------+\n| properties(VERTEX)                                               |\n+------------------------------------------------------------------+\n| {_vid: \"Chris Paul\", city: \"Houston\", name: \"Chris Paul\"}        |\n| {_vid: \"Boris Diaw\", city: \"Houston\", name: \"Boris Diaw\"}        |\n| {_vid: \"David West\", city: \"Philadelphia\", name: \"David West\"}   |\n| {_vid: \"James Harden\", city: \"New York\", name: \"James Harden\"}   |\n| {_vid: \"Tony Parker\", city: \"Chicago\", name: \"Tony Parker\"}      |\n| {_vid: \"Aron Baynes\", city: \"Chicago\", name: \"Aron Baynes\"}      |\n| {_vid: \"Ben Simmons\", city: \"Phoenix\", name: \"Ben Simmons\"}      |\n| {_vid: \"Blake Griffin\", city: \"Phoenix\", name: \"Blake Griffin\"}  |\n| {_vid: \"Danny Green\", city: \"Philadelphia\", name: \"Danny Green\"} |\n+------------------------------------------------------------------+\n\n// When using multi-attribute full-text index queries, you can specify different text for different properties for the query.\nnebula&gt; LOOKUP ON player WHERE ES_QUERY(fulltext_index_2,\"name:*b* AND city:Houston\") YIELD properties(vertex);\n+-----------------------------------------------------------+\n| properties(VERTEX)                                        |\n+-----------------------------------------------------------+\n| {_vid: \"Boris Diaw\", city: \"Houston\", name: \"Boris Diaw\"} |\n+-----------------------------------------------------------+\n\n// Delete single-attribute full-text index.\nnebula&gt; DROP FULLTEXT INDEX fulltext_index_1;\n</code></pre>"},{"location":"3.ngql-guide/16.subgraph-and-path/1.get-subgraph/","title":"GET SUBGRAPH","text":"<p>The <code>GET SUBGRAPH</code> statement returns a subgraph that is generated by traversing a graph starting from a specified vertex. <code>GET SUBGRAPH</code> statements allow you to specify the number of steps and the type or direction of edges during the traversal.</p>"},{"location":"3.ngql-guide/16.subgraph-and-path/1.get-subgraph/#syntax","title":"Syntax","text":"<pre><code>GET SUBGRAPH [WITH PROP] [&lt;step_count&gt; {STEP|STEPS}] FROM {&lt;vid&gt;, &lt;vid&gt;...}\n[{IN | OUT | BOTH} &lt;edge_type&gt;, &lt;edge_type&gt;...]\n[WHERE &lt;expression&gt; [AND &lt;expression&gt; ...]]\nYIELD {[VERTICES AS &lt;vertex_alias&gt;] [,EDGES AS &lt;edge_alias&gt;]};\n</code></pre> <ul> <li><code>WITH PROP</code> shows the properties. If not specified, the properties will be hidden.</li> </ul> <ul> <li><code>step_count</code> specifies the number of hops from the source vertices and returns the subgraph from 0 to <code>step_count</code> hops. It must be a non-negative integer. Its default value is 1.</li> </ul> <ul> <li><code>vid</code> specifies the vertex IDs. </li> </ul> <ul> <li><code>edge_type</code> specifies the edge type. You can use <code>IN</code>, <code>OUT</code>, and <code>BOTH</code> to specify the traversal direction of the edge type. The default is <code>BOTH</code>.</li> </ul> <ul> <li><code>&lt;WHERE clause&gt;</code> specifies the filter conditions for the traversal, which can be used with the boolean operator <code>AND</code>.</li> </ul> <ul> <li><code>YIELD</code> defines the output that needs to be returned. You can return only vertices or edges. A column alias must be set.</li> </ul> <p>Note</p> <p>The path type of <code>GET SUBGRAPH</code> is <code>trail</code>. Only vertices can be repeatedly visited in graph traversal. For more information, see Path.</p>"},{"location":"3.ngql-guide/16.subgraph-and-path/1.get-subgraph/#limitations","title":"Limitations","text":"<p>While using the <code>WHERE</code> clause in a <code>GET SUBGRAPH</code> statement, note the following restrictions:</p> <ul> <li>Only support the <code>AND</code> operator.</li> <li>Only support filter destination vertex, the vertex format must be <code>$$.tagName.propName</code>.</li> <li>Support filter edge, the edge format must be <code>edge_type.propName</code>.</li> <li>Support math functions, aggregate functions, string functions, datetime functions, type conversion functions and general functions in list functions.</li> <li>Not support aggregate functions, schema-related functions, conditional expression, predicate functions, geography function and user-defined functions.</li> </ul>"},{"location":"3.ngql-guide/16.subgraph-and-path/1.get-subgraph/#examples","title":"Examples","text":"<p>The following graph is used as the sample.</p> <p></p> <p>Insert the test data:</p> <pre><code>nebula&gt; CREATE SPACE IF NOT EXISTS subgraph(partition_num=15, replica_factor=1, vid_type=fixed_string(30));\nnebula&gt; USE subgraph;\nnebula&gt; CREATE TAG IF NOT EXISTS player(name string, age int);\nnebula&gt; CREATE TAG IF NOT EXISTS team(name string);\nnebula&gt; CREATE EDGE IF NOT EXISTS follow(degree int);\nnebula&gt; CREATE EDGE IF NOT EXISTS serve(start_year int, end_year int);\nnebula&gt; INSERT VERTEX player(name, age) VALUES \"player100\":(\"Tim Duncan\", 42);\nnebula&gt; INSERT VERTEX player(name, age) VALUES \"player101\":(\"Tony Parker\", 36);\nnebula&gt; INSERT VERTEX player(name, age) VALUES \"player102\":(\"LaMarcus Aldridge\", 33);\nnebula&gt; INSERT VERTEX team(name) VALUES \"team203\":(\"Trail Blazers\"), \"team204\":(\"Spurs\");\nnebula&gt; INSERT EDGE follow(degree) VALUES \"player101\" -&gt; \"player100\":(95);\nnebula&gt; INSERT EDGE follow(degree) VALUES \"player101\" -&gt; \"player102\":(90);\nnebula&gt; INSERT EDGE follow(degree) VALUES \"player102\" -&gt; \"player100\":(75);\nnebula&gt; INSERT EDGE serve(start_year, end_year) VALUES \"player101\" -&gt; \"team204\":(1999, 2018),\"player102\" -&gt; \"team203\":(2006,  2015);\n</code></pre> <ul> <li>This example goes one step from the vertex <code>player101</code> over all edge types and gets the subgraph.<pre><code>nebula&gt; GET SUBGRAPH 1 STEPS FROM \"player101\" YIELD VERTICES AS nodes, EDGES AS relationships;\n+-------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------+\n| nodes                                                                   | relationships                                                                                                               |\n+-------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------+\n| [(\"player101\" :player{})]                                               | [[:serve \"player101\"-&gt;\"team204\" @0 {}], [:follow \"player101\"-&gt;\"player100\" @0 {}], [:follow \"player101\"-&gt;\"player102\" @0 {}]] |\n| [(\"team204\" :team{}), (\"player100\" :player{}), (\"player102\" :player{})] | [[:follow \"player102\"-&gt;\"player100\" @0 {}]]                                                                                  |\n+-------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------+\n</code></pre> <p>The returned subgraph is as follows.</p> <p></p> </li> </ul> <ul> <li>This example goes one step from the vertex <code>player101</code> over incoming <code>follow</code> edges and gets the subgraph.<pre><code>nebula&gt; GET SUBGRAPH 1 STEPS FROM \"player101\" IN follow YIELD VERTICES AS nodes, EDGES AS relationships;\n+---------------------------+---------------+\n| nodes                     | relationships |\n+---------------------------+---------------+\n| [(\"player101\" :player{})] | []            |\n+---------------------------+---------------+\n</code></pre> <p>There is no incoming <code>follow</code> edge to <code>player101</code>, so only the vertex <code>player101</code> is returned.</p> </li> </ul> <ul> <li>This example goes one step from the vertex <code>player101</code> over outgoing <code>serve</code> edges, gets the subgraph, and shows the property of the edge.<pre><code>nebula&gt; GET SUBGRAPH WITH PROP 1 STEPS FROM \"player101\" OUT serve YIELD VERTICES AS nodes, EDGES AS relationships;\n+-------------------------------------------------------+-------------------------------------------------------------------------+\n| nodes                                                 | relationships                                                           |\n+-------------------------------------------------------+-------------------------------------------------------------------------+\n| [(\"player101\" :player{age: 36, name: \"Tony Parker\"})] | [[:serve \"player101\"-&gt;\"team204\" @0 {end_year: 2018, start_year: 1999}]] |\n| [(\"team204\" :team{name: \"Spurs\"})]                    | []                                                                      |\n+-------------------------------------------------------+-------------------------------------------------------------------------+\n</code></pre> <p>The returned subgraph is as follows.</p> <p></p> </li> </ul> <ul> <li>This example goes two steps from the vertex <code>player101</code> over <code>follow</code> edges, filters by degree &gt; 90 and age &gt; 30, and shows the properties of edges.<pre><code>nebula&gt; GET SUBGRAPH WITH PROP 2 STEPS FROM \"player101\" \\\n    WHERE follow.degree &gt; 90 AND $$.player.age &gt; 30 \\\n    YIELD VERTICES AS nodes, EDGES AS relationships;\n+-------------------------------------------------------+------------------------------------------------------+\n| nodes                                                 | relationships                                        |\n+-------------------------------------------------------+------------------------------------------------------+\n| [(\"player101\" :player{age: 36, name: \"Tony Parker\"})] | [[:follow \"player101\"-&gt;\"player100\" @0 {degree: 95}]] |\n| [(\"player100\" :player{age: 42, name: \"Tim Duncan\"})]  | []                                                   |\n+-------------------------------------------------------+------------------------------------------------------+\n</code></pre> </li> </ul>"},{"location":"3.ngql-guide/16.subgraph-and-path/1.get-subgraph/#faq","title":"FAQ","text":""},{"location":"3.ngql-guide/16.subgraph-and-path/1.get-subgraph/#why_is_the_number_of_hops_in_the_returned_result_greater_than_step_count","title":"Why is the number of hops in the returned result greater than <code>step_count</code>?","text":"<p>To show the completeness of the subgraph, an additional hop is made on all vertices that meet the conditions. The following graph is used as the sample.</p> <p></p> <ul> <li>The returned paths of <code>GET SUBGRAPH 1 STEPS FROM \"A\";</code> are <code>A-&gt;B</code>, <code>B-&gt;A</code>, and <code>A-&gt;C</code>. To show the completeness of the subgraph, an additional hop is made on all vertices that meet the conditions, namely <code>B-&gt;C</code>.</li> </ul> <ul> <li>The returned path of <code>GET SUBGRAPH 1 STEPS FROM \"A\" IN follow;</code> is <code>B-&gt;A</code>. To show the completeness of the subgraph, an additional hop is made on all vertices that meet the conditions, namely <code>A-&gt;B</code>.</li> </ul> <p>If you only query paths or vertices that meet the conditions, we suggest you use MATCH or GO. The example is as follows.</p> <pre><code>nebula&gt; MATCH p= (v:player) -- (v2) WHERE id(v)==\"A\" RETURN p;\nnebula&gt; GO 1 STEPS FROM \"A\" OVER follow YIELD src(edge),dst(edge);\n</code></pre>"},{"location":"3.ngql-guide/16.subgraph-and-path/1.get-subgraph/#why_is_the_number_of_hops_in_the_returned_result_lower_than_step_count","title":"Why is the number of hops in the returned result lower than <code>step_count</code>?","text":"<p>The query stops when there is not enough subgraph data and will not return the null value.</p> <pre><code>nebula&gt; GET SUBGRAPH 100 STEPS FROM \"player101\" OUT follow YIELD VERTICES AS nodes, EDGES AS relationships;\n+----------------------------------------------------+--------------------------------------------------------------------------------------+\n| nodes                                              | relationships                                                                        |\n+----------------------------------------------------+--------------------------------------------------------------------------------------+\n| [(\"player101\" :player{})]                          | [[:follow \"player101\"-&gt;\"player100\" @0 {}], [:follow \"player101\"-&gt;\"player102\" @0 {}]] |\n| [(\"player100\" :player{}), (\"player102\" :player{})] | [[:follow \"player102\"-&gt;\"player100\" @0 {}]]                                           |\n+----------------------------------------------------+--------------------------------------------------------------------------------------+\n</code></pre>"},{"location":"3.ngql-guide/16.subgraph-and-path/2.find-path/","title":"FIND PATH","text":"<p>The <code>FIND PATH</code> statement finds the paths between the selected source vertices and destination vertices.</p> <p>Note</p> <p>To improve the query performance with the <code>FIND PATH</code> statement, you can add the <code>num_operator_threads</code> parameter in the <code>nebula-graphd.conf</code> configuration file. The value range of the <code>num_operator_threads</code> parameter is [2, 10] and make sure that the value is not greater than the number of CPU cores of the machine where the <code>graphd</code> service is deployed. It is recommended to set the value to the number of CPU cores of the machine where the <code>graphd</code> service is deployed. For more information about the <code>nebula-graphd.conf</code> configuration file, see nebula-graphd.conf.</p>"},{"location":"3.ngql-guide/16.subgraph-and-path/2.find-path/#syntax","title":"Syntax","text":"<pre><code>FIND { SHORTEST | ALL | NOLOOP } PATH [WITH PROP] FROM &lt;vertex_id_list&gt; TO &lt;vertex_id_list&gt;\nOVER &lt;edge_type_list&gt; [REVERSELY | BIDIRECT] \n[&lt;WHERE clause&gt;] [UPTO &lt;N&gt; {STEP|STEPS}] \nYIELD path as &lt;alias&gt;\n[| ORDER BY $-.path] [| LIMIT &lt;M&gt;];\n\n&lt;vertex_id_list&gt; ::=\n    [vertex_id [, vertex_id] ...]\n</code></pre> <ul> <li><code>SHORTEST</code> finds the shortest path.</li> </ul> <ul> <li><code>ALL</code> finds all the paths.</li> </ul> <ul> <li><code>NOLOOP</code> finds the paths without circles.</li> </ul> <ul> <li><code>WITH PROP</code> shows properties of vertices and edges. If not specified, properties will be hidden.</li> </ul> <ul> <li><code>&lt;vertex_id_list&gt;</code> is a list of vertex IDs separated with commas (,). It supports <code>$-</code> and <code>$var</code>.</li> </ul> <ul> <li><code>&lt;edge_type_list&gt;</code> is a list of edge types separated with commas (,). <code>*</code> is all edge types.</li> </ul> <ul> <li><code>REVERSELY | BIDIRECT</code> specifies the direction. <code>REVERSELY</code> is reverse graph traversal while <code>BIDIRECT</code> is bidirectional graph traversal.</li> </ul> <ul> <li><code>&lt;WHERE clause&gt;</code> filters properties of edges.</li> </ul> <ul> <li><code>&lt;N&gt;</code> is the maximum hop number of the path. The default value is <code>5</code>.</li> </ul> <ul> <li><code>&lt;M&gt;</code> specifies the maximum number of rows to return.</li> </ul> <p>Note</p> <p>The path type of <code>FIND PATH</code> is <code>trail</code>. Only vertices can be repeatedly visited in graph traversal. For more information, see Path.</p>"},{"location":"3.ngql-guide/16.subgraph-and-path/2.find-path/#limitations","title":"Limitations","text":"<ul> <li>When a list of source and/or destination vertex IDs are specified, the paths between any source vertices and the destination vertices will be returned.</li> </ul> <ul> <li>There can be cycles when searching all paths.</li> </ul> <ul> <li><code>FIND PATH</code> only supports filtering properties of edges with <code>WHERE</code> clauses. Filtering properties of vertices and functions are not supported for now.</li> </ul> <ul> <li><code>FIND PATH</code> is a single-thread procedure, so it uses much memory.</li> </ul>"},{"location":"3.ngql-guide/16.subgraph-and-path/2.find-path/#examples","title":"Examples","text":"<p>A returned path is like <code>(&lt;vertex_id&gt;)-[:&lt;edge_type_name&gt;@&lt;rank&gt;]-&gt;(&lt;vertex_id)</code>.</p> <pre><code>nebula&gt; FIND SHORTEST PATH FROM \"player102\" TO \"team204\" OVER * YIELD path AS p;\n+--------------------------------------------+\n| p                                          |\n+--------------------------------------------+\n| &lt;(\"player102\")-[:serve@0 {}]-&gt;(\"team204\")&gt; |\n+--------------------------------------------+\n</code></pre> <pre><code>nebula&gt; FIND SHORTEST PATH WITH PROP FROM \"team204\" TO \"player100\" OVER * REVERSELY YIELD path AS p;\n+--------------------------------------------------------------------------------------------------------------------------------------+\n| p                                                                                                                                    |\n+--------------------------------------------------------------------------------------------------------------------------------------+\n| &lt;(\"team204\" :team{name: \"Spurs\"})&lt;-[:serve@0 {end_year: 2016, start_year: 1997}]-(\"player100\" :player{age: 42, name: \"Tim Duncan\"})&gt; |\n+--------------------------------------------------------------------------------------------------------------------------------------+\n</code></pre> <pre><code>nebula&gt; FIND ALL PATH FROM \"player100\" TO \"team204\" OVER * WHERE follow.degree is EMPTY or follow.degree &gt;=0 YIELD path AS p;\n+------------------------------------------------------------------------------+\n| p                                                                            |\n+------------------------------------------------------------------------------+\n| &lt;(\"player100\")-[:serve@0 {}]-&gt;(\"team204\")&gt;                                   |\n| &lt;(\"player100\")-[:follow@0 {}]-&gt;(\"player125\")-[:serve@0 {}]-&gt;(\"team204\")&gt;     |\n| &lt;(\"player100\")-[:follow@0 {}]-&gt;(\"player101\")-[:serve@0 {}]-&gt;(\"team204\")&gt;     |\n|...                                                                           |\n+------------------------------------------------------------------------------+\n</code></pre> <pre><code>nebula&gt; FIND NOLOOP PATH FROM \"player100\" TO \"team204\" OVER * YIELD path AS p;\n+--------------------------------------------------------------------------------------------------------+\n| p                                                                                                      |\n+--------------------------------------------------------------------------------------------------------+\n| &lt;(\"player100\")-[:serve@0 {}]-&gt;(\"team204\")&gt;                                                             |\n| &lt;(\"player100\")-[:follow@0 {}]-&gt;(\"player125\")-[:serve@0 {}]-&gt;(\"team204\")&gt;                               |\n| &lt;(\"player100\")-[:follow@0 {}]-&gt;(\"player101\")-[:serve@0 {}]-&gt;(\"team204\")&gt;                               |\n| &lt;(\"player100\")-[:follow@0 {}]-&gt;(\"player101\")-[:follow@0 {}]-&gt;(\"player125\")-[:serve@0 {}]-&gt;(\"team204\")&gt; |\n| &lt;(\"player100\")-[:follow@0 {}]-&gt;(\"player101\")-[:follow@0 {}]-&gt;(\"player102\")-[:serve@0 {}]-&gt;(\"team204\")&gt; |\n+--------------------------------------------------------------------------------------------------------+\n</code></pre>"},{"location":"3.ngql-guide/16.subgraph-and-path/2.find-path/#faq","title":"FAQ","text":""},{"location":"3.ngql-guide/16.subgraph-and-path/2.find-path/#does_it_support_the_where_clause_to_achieve_conditional_filtering_during_graph_traversal","title":"Does it support the WHERE clause to achieve conditional filtering during graph traversal?","text":"<p><code>FIND PATH</code> only supports filtering properties of edges with <code>WHERE</code> clauses, such as <code>WHERE follow.degree is EMPTY or follow.degree &gt;=0</code>.</p> <p>Filtering properties of vertices is not supported for now.</p>"},{"location":"3.ngql-guide/17.query-tuning-statements/1.explain-and-profile/","title":"EXPLAIN and PROFILE","text":"<p><code>EXPLAIN</code> helps output the execution plan of an nGQL statement without executing the statement.</p> <p><code>PROFILE</code> executes the statement, then outputs the execution plan as well as the execution profile. You can optimize the queries for better performance according to the execution plan and profile.</p>"},{"location":"3.ngql-guide/17.query-tuning-statements/1.explain-and-profile/#execution_plan","title":"Execution Plan","text":"<p>The execution plan is determined by the execution planner in the NebulaGraph query engine.</p> <p>The execution planner processes the parsed nGQL statements into <code>actions</code>. An <code>action</code> is the smallest unit that can be executed. A typical <code>action</code> fetches all neighbors of a given vertex, gets the properties of an edge, and filters vertices or edges based on the given conditions. Each <code>action</code> is assigned to an <code>operator</code> that performs the action.</p> <p>For example, a <code>SHOW TAGS</code> statement is processed into two <code>actions</code> and assigned to a <code>Start operator</code> and a <code>ShowTags operator</code>, while a more complex <code>GO</code> statement may be processed into more than 10 <code>actions</code> and assigned to 10 operators.</p>"},{"location":"3.ngql-guide/17.query-tuning-statements/1.explain-and-profile/#syntax","title":"Syntax","text":"<ul> <li><code>EXPLAIN</code><pre><code>EXPLAIN [format= {\"row\" | \"dot\" | \"tck\"}] &lt;your_nGQL_statement&gt;;\n</code></pre> </li> </ul> <ul> <li><code>PROFILE</code><pre><code>PROFILE [format= {\"row\" | \"dot\" | \"tck\"}] &lt;your_nGQL_statement&gt;;\n</code></pre> </li> </ul>"},{"location":"3.ngql-guide/17.query-tuning-statements/1.explain-and-profile/#output_formats","title":"Output formats","text":"<p>The output of an <code>EXPLAIN</code> or a <code>PROFILE</code> statement has three formats, the default <code>row</code> format, the <code>dot</code> format, and the <code>tck</code> format. You can use the <code>format</code> option to modify the output format. Omitting the <code>format</code> option indicates using the default <code>row</code> format.</p>"},{"location":"3.ngql-guide/17.query-tuning-statements/1.explain-and-profile/#the_row_format","title":"The <code>row</code> format","text":"<p>The <code>row</code> format outputs the return message in a table as follows.</p> <ul> <li><code>EXPLAIN</code><pre><code>nebula&gt; EXPLAIN format=\"row\" SHOW TAGS;\nExecution succeeded (time spent 327/892 us)\n\nExecution Plan\n\n-----+----------+--------------+----------------+----------------------------------------------------------------------\n| id | name     | dependencies | profiling data | operator info                                                       |\n-----+----------+--------------+----------------+----------------------------------------------------------------------\n|  1 | ShowTags | 0            |                | outputVar: [{\"colNames\":[],\"name\":\"__ShowTags_1\",\"type\":\"DATASET\"}] |\n|    |          |              |                | inputVar:                                                           |\n-----+----------+--------------+----------------+----------------------------------------------------------------------\n|  0 | Start    |              |                | outputVar: [{\"colNames\":[],\"name\":\"__Start_0\",\"type\":\"DATASET\"}]    |\n-----+----------+--------------+----------------+----------------------------------------------------------------------\n</code></pre> </li> </ul> <ul> <li><code>PROFILE</code><pre><code>nebula&gt; PROFILE format=\"row\" SHOW TAGS;\n+--------+\n| Name   |\n+--------+\n| player |\n+--------+\n| team   |\n+--------+\nGot 2 rows (time spent 2038/2728 us)\n\nExecution Plan\n\n-----+----------+--------------+----------------------------------------------------+----------------------------------------------------------------------\n| id | name     | dependencies | profiling data                                     | operator info                                                       |\n-----+----------+--------------+----------------------------------------------------+----------------------------------------------------------------------\n|  1 | ShowTags | 0            | ver: 0, rows: 1, execTime: 42us, totalTime: 1177us | outputVar: [{\"colNames\":[],\"name\":\"__ShowTags_1\",\"type\":\"DATASET\"}] |\n|    |          |              |                                                    | inputVar:                                                           |\n-----+----------+--------------+----------------------------------------------------+----------------------------------------------------------------------\n|  0 | Start    |              | ver: 0, rows: 0, execTime: 1us, totalTime: 57us    | outputVar: [{\"colNames\":[],\"name\":\"__Start_0\",\"type\":\"DATASET\"}]    |\n-----+----------+--------------+----------------------------------------------------+----------------------------------------------------------------------\n</code></pre> </li> </ul> <p>The descriptions are as follows.</p> Parameter Description <code>id</code> The ID of the <code>operator</code>. <code>name</code> The name of the <code>operator</code>. <code>dependencies</code> The ID of the <code>operator</code> that the current <code>operator</code> depends on. <code>profiling data</code> The content of the execution profile. <code>ver</code> is the version of the <code>operator</code>. <code>rows</code> shows the number of rows to be output by the <code>operator</code>. <code>execTime</code> shows the execution time of <code>action</code>. <code>totalTime</code> is the sum of the execution time, the system scheduling time, and the queueing time. <code>operator info</code> The detailed information of the <code>operator</code>."},{"location":"3.ngql-guide/17.query-tuning-statements/1.explain-and-profile/#the_dot_format","title":"The <code>dot</code> format","text":"<p>You can use the <code>format=\"dot\"</code> option to output the return message in the <code>dot</code> language, and then use Graphviz to generate a graph of the plan.</p> <p>Note</p> <p>Graphviz is open source graph visualization software. Graphviz provides an online tool for previewing DOT language files and exporting them to other formats such as SVG or JSON. For more information, see Graphviz Online.</p> <pre><code>nebula&gt; EXPLAIN format=\"dot\" SHOW TAGS;\nExecution succeeded (time spent 161/665 us)\nExecution Plan\n---------------------------------------------------------------------------------------------------------------------------------------------  -------------\n  plan\n---------------------------------------------------------------------------------------------------------------------------------------------  -------------\n  digraph exec_plan {\n      rankdir=LR;\n      \"ShowTags_0\"[label=\"ShowTags_0|outputVar: \\[\\{\\\"colNames\\\":\\[\\],\\\"name\\\":\\\"__ShowTags_0\\\",\\\"type\\\":\\\"DATASET\\\"\\}\\]\\l|inputVar:\\l\",   shape=Mrecord];\n      \"Start_2\"-&gt;\"ShowTags_0\";\n      \"Start_2\"[label=\"Start_2|outputVar: \\[\\{\\\"colNames\\\":\\[\\],\\\"name\\\":\\\"__Start_2\\\",\\\"type\\\":\\\"DATASET\\\"\\}\\]\\l|inputVar: \\l\",   shape=Mrecord];\n  }\n---------------------------------------------------------------------------------------------------------------------------------------------  -------------\n</code></pre> <p>The Graphviz graph transformed from the above DOT statement is as follows.</p> <p></p>"},{"location":"3.ngql-guide/17.query-tuning-statements/1.explain-and-profile/#the_tck_format","title":"The <code>tck</code> format","text":"<p>The tck format is similar to a table, but without borders and dividing lines between rows. You can use the results as test cases for unit testing.  For information on tck format test cases, see TCK cases.</p> <ul> <li><code>EXPLAIN</code><pre><code>nebula&gt; EXPLAIN format=\"tck\" FETCH PROP ON player \"player_1\",\"player_2\",\"player_3\" YIELD properties(vertex).name as name, properties(vertex).age as age;\nExecution succeeded (time spent 261\u00b5s/613.718\u00b5s)\nExecution Plan (optimize time 28 us)\n| id | name        | dependencies | profiling data | operator info |\n|  2 | Project     | 1            |                |               |\n|  1 | GetVertices | 0            |                |               |\n|  0 | Start       |              |                |               |\n\nWed, 22 Mar 2023 23:15:52 CST\n</code></pre> </li> </ul> <ul> <li><code>PROFILE</code><pre><code>nebula&gt; PROFILE format=\"tck\" FETCH PROP ON player \"player_1\",\"player_2\",\"player_3\" YIELD properties(vertex).name as name, properties(vertex).age as age;\n| name         | age |\n| \"Piter Park\" | 24  |\n| \"aaa\"        | 24  |\n| \"ccc\"        | 24  |\nGot 3 rows (time spent 1.474ms/2.19677ms)\nExecution Plan (optimize time 41 us)\n| id | name        | dependencies | profiling data                                                                                                      | operator info |\n|  2 | Project     | 1            | {\"rows\":3,\"version\":0}                                                                                              |               |\n|  1 | GetVertices | 0            | {\"resp[0]\":{\"exec\":\"232(us)\",\"host\":\"127.0.0.1:9779\",\"total\":\"758(us)\"},\"rows\":3,\"total_rpc\":\"875(us)\",\"version\":0} |               |\n|  0 | Start       |              | {\"rows\":0,\"version\":0}                                                                                              |               |\nWed, 22 Mar 2023 23:16:13 CST\n</code></pre> </li> </ul>"},{"location":"3.ngql-guide/17.query-tuning-statements/2.kill-session/","title":"Kill sessions","text":"<p>The <code>KILL SESSION</code> command is to terminate running sessions.</p> <p>Note</p> <ul> <li>Only the NebulaGraph <code>root</code> user can terminate sessions.</li> <li>After executing the <code>KILL SESSION</code> command, all Graph services synchronize the latest session information after <code>2* session_reclaim_interval_secs</code> seconds (<code>120</code> seconds by default).</li> </ul>"},{"location":"3.ngql-guide/17.query-tuning-statements/2.kill-session/#syntax","title":"Syntax","text":"<p>You can run the <code>KILL SESSION</code> command to terminate one or multiple sessions. The syntax is as follows:</p> <ul> <li> <p>To terminate one session</p> <pre><code>KILL {SESSION|SESSIONS} &lt;SessionId&gt;\n</code></pre> <ul> <li><code>{SESSION|SESSIONS}</code>: <code>SESSION</code> or <code>SESSIONS</code>, both are supported. </li> <li><code>&lt;SessionId&gt;</code>: Specifies the ID of one session. You can run the SHOW SESSIONS command to view the IDs of sessions.</li> </ul> </li> </ul> <ul> <li> <p>To terminate multiple sessions</p> <pre><code>SHOW SESSIONS \n| YIELD $-.SessionId AS sid [WHERE &lt;filter_clause&gt;]\n| KILL {SESSION|SESSIONS} $-.sid\n</code></pre> <p>Note</p> <p>The <code>KILL SESSION</code> command supports the pipeline operation, combining the <code>SHOW SESSIONS</code> command with the <code>KILL SESSION</code> command to terminate multiple sessions.</p> <ul> <li><code>[WHERE &lt;filter_clause&gt;]</code>\uff1a<ul> <li>Optional, the <code>WHERE</code> clause is used to filter sessions. <code>&lt;filter_expression&gt;</code> specifies a session filtering expression, for example, <code>WHERE $-.CreateTime &lt; datetime(\"2022-12-14T18:00:00\")</code>. If the <code>WHERE</code> clause is not specified, all sessions are terminated.</li> <li>Filtering conditions in a <code>WHERE</code> clause include: <code>SessionId</code>, <code>UserName</code>, <code>SpaceName</code>, <code>CreateTime</code>, <code>UpdateTime</code>, <code>GraphAddr</code>, <code>Timezone</code>, and <code>ClientIp</code>. You can run the SHOW SESSIONS command to view descriptions of these conditions.</li> </ul> </li> </ul> <ul> <li><code>{SESSION|SESSIONS}</code>: <code>SESSION</code> or <code>SESSIONS</code>, both are supported.</li> </ul> <p>Caution</p> <p>Please use filtering conditions with caution to avoid deleting sessions by mistake.</p> </li> </ul>"},{"location":"3.ngql-guide/17.query-tuning-statements/2.kill-session/#examples","title":"Examples","text":"<ul> <li> <p>To terminate one session</p> <pre><code>nebula&gt; KILL SESSION 1672887983842984 \n</code></pre> </li> </ul> <ul> <li> <p>To terminate multiple sessions</p> <ul> <li> <p>Terminate all sessions whose creation time is less than <code>2023-01-05T18:00:00</code>.</p> <pre><code>nebula&gt; SHOW SESSIONS | YIELD $-.SessionId AS sid WHERE $-.CreateTime &lt; datetime(\"2023-01-05T18:00:00\") | KILL SESSIONS $-.sid\n</code></pre> </li> </ul> <ul> <li> <p>Terminates the two sessions with the earliest creation times.</p> <pre><code>nebula&gt; SHOW SESSIONS | YIELD $-.SessionId AS sid, $-.CreateTime as CreateTime | ORDER BY $-.CreateTime ASC | LIMIT 2 | KILL SESSIONS $-.sid\n</code></pre> </li> </ul> <ul> <li> <p>Terminates all sessions created by the username <code>session_user1</code>.</p> <pre><code>nebula&gt; SHOW SESSIONS | YIELD $-.SessionId as sid WHERE $-.UserName == \"session_user1\" | KILL SESSIONS $-.sid\n</code></pre> </li> </ul> <ul> <li> <p>Terminate all sessions.</p> <pre><code>nebula&gt; SHOW SESSIONS | YIELD $-.SessionId as sid | KILL SESSION $-.sid\n\n// Or\nnebula&gt; SHOW SESSIONS | KILL SESSIONS $-.SessionId\n</code></pre> <p>Caution</p> <p>When you terminate all sessions, the current session is terminated. Please use it with caution.</p> </li> </ul> </li> </ul>"},{"location":"3.ngql-guide/17.query-tuning-statements/6.kill-query/","title":"Kill queries","text":"<p><code>KILL QUERY</code> can terminate the query being executed, and is often used to terminate slow queries.</p> <p>Note</p> <p>Users with the God role can kill any query. Other roles can only kill their own queries.</p>"},{"location":"3.ngql-guide/17.query-tuning-statements/6.kill-query/#syntax","title":"Syntax","text":"<pre><code>KILL QUERY (session=&lt;session_id&gt;, plan=&lt;plan_id&gt;);\n</code></pre> <ul> <li><code>session_id</code>: The ID of the session.</li> <li><code>plan_id</code>: The ID of the execution plan.</li> </ul> <p>The ID of the session and the ID of the execution plan can uniquely determine a query. Both can be obtained through the SHOW QUERIES statement.</p>"},{"location":"3.ngql-guide/17.query-tuning-statements/6.kill-query/#examples","title":"Examples","text":"<p>This example executes <code>KILL QUERY</code> in one session to terminate the query in another session.</p> <pre><code>nebula&gt; KILL QUERY(SESSION=1625553545984255,PLAN=163);\n</code></pre> <p>The query will be terminated and the following information will be returned.</p> <pre><code>[ERROR (-1005)]: ExecutionPlanId[1001] does not exist in current Session.\n</code></pre>"},{"location":"3.ngql-guide/3.data-types/1.numeric/","title":"Numeric types","text":"<p>nGQL supports both integer and floating-point number.</p>"},{"location":"3.ngql-guide/3.data-types/1.numeric/#integer","title":"Integer","text":"<p>Signed 64-bit integer (INT64), 32-bit integer (INT32), 16-bit integer (INT16), and 8-bit integer (INT8) are supported.</p> Type Declared keywords Range INT64 <code>INT64</code> or<code>INT</code> -9,223,372,036,854,775,808 ~ 9,223,372,036,854,775,807 INT32 <code>INT32</code> -2,147,483,648 ~ 2,147,483,647 INT16 <code>INT16</code> -32,768 ~ 32,767 INT8 <code>INT8</code> -128 ~ 127"},{"location":"3.ngql-guide/3.data-types/1.numeric/#floating-point_number","title":"Floating-point number","text":"<p>Both single-precision floating-point format (FLOAT) and double-precision floating-point format (DOUBLE) are supported.</p> Type Declared keywords Range Precision FLOAT <code>FLOAT</code> 3.4E +/- 38 6~7 bits DOUBLE <code>DOUBLE</code> 1.7E +/- 308 15~16 bits <p>Scientific notation is also supported, such as <code>1e2</code>, <code>1.1e2</code>, <code>.3e4</code>, <code>1.e4</code>, and <code>-1234E-10</code>.</p> <p>Note</p> <p>The data type of DECIMAL in MySQL is not supported.</p>"},{"location":"3.ngql-guide/3.data-types/1.numeric/#reading_and_writing_of_data_values","title":"Reading and writing of data values","text":"<p>When writing and reading different types of data, nGQL complies with the following rules:</p> Data type Set as VID Set as property Resulted data type INT64 Supported Supported INT64 INT32 Not supported Supported INT64 INT16 Not supported Supported INT64 INT8 Not supported Supported INT64 FLOAT Not supported Supported DOUBLE DOUBLE Not supported Supported DOUBLE <p>For example, nGQL does not support setting VID as INT8, but supports setting a certain property type of TAG or Edge type as INT8. When using the nGQL statement to read the property of INT8, the resulted type is INT64.</p> <ul> <li> <p>Multiple formats are supported:</p> <ul> <li>Decimal, such as <code>123456</code>.</li> <li>Hexadecimal, such as <code>0x1e240</code>.</li> <li>Octal, such as <code>0361100</code>.</li> </ul> <p>However, NebulaGraph will parse the written non-decimal value into a decimal value and save it. The value read is decimal.</p> <p>For example, the type of the property <code>score</code> is <code>INT</code>. The value of <code>0xb</code> is assigned to it through the INSERT statement. If querying the property value with statements such as FETCH, you will get the result <code>11</code>, which is the decimal result of the hexadecimal <code>0xb</code>.</p> </li> </ul> <ul> <li>Round a FLOAT/DOUBLE value when inserting it to an INT column.</li> </ul>"},{"location":"3.ngql-guide/3.data-types/10.geography/","title":"Geography","text":"<p>Geography is a data type composed of latitude and longitude that represents geospatial information. NebulaGraph currently supports Point, LineString, and Polygon in Simple Features and some functions in SQL-MM 3, such as part of the core geo parsing, construction, formatting, conversion, predicates, and dimensions.</p>"},{"location":"3.ngql-guide/3.data-types/10.geography/#type_description","title":"Type description","text":"<p>A point is the basic data type of geography, which is determined by a latitude and a longitude. For example, <code>\"POINT(3 8)\"</code> means that the longitude is <code>3\u00b0</code> and the latitude is <code>8\u00b0</code>. Multiple points can form a linestring or a polygon.</p> <p>Note</p> <p>You cannot directly insert geographic data of the following types, such as <code>INSERT VERTEX any_shape(geo) VALUES \"1\":(\"POINT(1 1)\")</code>. Instead, you need to use a geography function to specify the data type before inserting, such as <code>INSERT VERTEX any_shape(geo) VALUES \"1\":(ST_GeogFromText(\"POINT(1 1)\"));</code>.</p> Shape Example Description Point <code>\"POINT(3 8)\"</code> Specifies the data type as a point. LineString <code>\"LINESTRING(3 8, 4.7 73.23)\"</code> Specifies the data type as a linestring. Polygon <code>\"POLYGON((0 1, 1 2, 2 3, 0 1))\"</code> Specifies the data type as a polygon."},{"location":"3.ngql-guide/3.data-types/10.geography/#examples","title":"Examples","text":"<pre><code>//Create a Tag to allow storing any geography data type.\nnebula&gt; CREATE TAG IF NOT EXISTS any_shape(geo geography);\n\n//Create a Tag to allow storing a point only.\nnebula&gt; CREATE TAG IF NOT EXISTS only_point(geo geography(point));\n\n//Create a Tag to allow storing a linestring only.\nnebula&gt; CREATE TAG IF NOT EXISTS only_linestring(geo geography(linestring));\n\n//Create a Tag to allow storing a polygon only.\nnebula&gt; CREATE TAG IF NOT EXISTS only_polygon(geo geography(polygon));\n\n//Create an Edge type to allow storing any geography data type.\nnebula&gt; CREATE EDGE IF NOT EXISTS any_shape_edge(geo geography);\n\n//Create a vertex to store the geography of a polygon.\nnebula&gt; INSERT VERTEX any_shape(geo) VALUES \"103\":(ST_GeogFromText(\"POLYGON((0 1, 1 2, 2 3, 0 1))\"));\n\n//Create an edge to store the geography of a polygon.\nnebula&gt; INSERT EDGE any_shape_edge(geo) VALUES \"201\"-&gt;\"302\":(ST_GeogFromText(\"POLYGON((0 1, 1 2, 2 3, 0 1))\"));\n\n//Query the geography of Vertex 103.\nnebula&gt; FETCH PROP ON any_shape \"103\" YIELD ST_ASText(any_shape.geo);\n+---------------------------------+\n| ST_ASText(any_shape.geo)        |\n+---------------------------------+\n| \"POLYGON((0 1, 1 2, 2 3, 0 1))\" |\n+---------------------------------+\n\n//Query the geography of the edge which traverses from Vertex 201 to Vertex 302.\nnebula&gt; FETCH PROP ON any_shape_edge \"201\"-&gt;\"302\" YIELD ST_ASText(any_shape_edge.geo);\n+---------------------------------+\n| ST_ASText(any_shape_edge.geo)   |\n+---------------------------------+\n| \"POLYGON((0 1, 1 2, 2 3, 0 1))\" |\n+---------------------------------+\n\n//Create an index for the geography of the Tag any_shape and run LOOKUP.\nnebula&gt; CREATE TAG INDEX IF NOT EXISTS any_shape_geo_index ON any_shape(geo);\nnebula&gt; REBUILD TAG INDEX any_shape_geo_index;\nnebula&gt; LOOKUP ON any_shape YIELD ST_ASText(any_shape.geo);\n+---------------------------------+\n| ST_ASText(any_shape.geo)        |\n+---------------------------------+\n| \"POLYGON((0 1, 1 2, 2 3, 0 1))\" |\n+---------------------------------+\n</code></pre> <p>When creating an index for geography properties, you can specify the parameters for the index.</p> Parameter Default value Description <code>s2_max_level</code> <code>30</code> The maximum level of S2 cell used in the covering. Allowed values: <code>1</code>~<code>30</code>. Setting it to less than the default means that NebulaGraph will be forced to generate coverings using larger cells. <code>s2_max_cells</code> <code>8</code> The maximum number of S2 cells used in the covering. Provides a limit on how much work is done exploring the possible coverings. Allowed values: <code>1</code>~<code>30</code>. You may want to use higher values for odd-shaped regions such as skinny rectangles. <p>Note</p> <p>Specifying the above two parameters does not affect the Point type of property. The <code>s2_max_level</code> value of the Point type is forced to be <code>30</code>.</p> <pre><code>nebula&gt; CREATE TAG INDEX IF NOT EXISTS any_shape_geo_index ON any_shape(geo) with (s2_max_level=30, s2_max_cells=8);\n</code></pre> <p>For more index information, see Index overview.</p>"},{"location":"3.ngql-guide/3.data-types/2.boolean/","title":"Boolean","text":"<p>A boolean data type is declared with the <code>bool</code> keyword and can only take the values <code>true</code> or <code>false</code>.</p> <p>nGQL supports using boolean in the following ways:</p> <ul> <li>Define the data type of the property value as a boolean.</li> <li>Use boolean as judgment conditions in the <code>WHERE</code> clause.</li> </ul>"},{"location":"3.ngql-guide/3.data-types/3.string/","title":"String","text":"<p>Fixed-length strings and variable-length strings are supported.</p>"},{"location":"3.ngql-guide/3.data-types/3.string/#declaration_and_literal_representation","title":"Declaration and literal representation","text":"<p>The string type is declared with the keywords of:</p> <ul> <li><code>STRING</code>: Variable-length strings.</li> <li><code>FIXED_STRING(&lt;length&gt;)</code>: Fixed-length strings. <code>&lt;length&gt;</code> is the length of the string, such as <code>FIXED_STRING(32)</code>.</li> </ul> <p>A string type is used to store a sequence of characters (text). The literal constant is a sequence of characters of any length surrounded by double or single quotes. For example, <code>\"Hello, Cooper\"</code> or <code>'Hello, Cooper'</code>.</p>"},{"location":"3.ngql-guide/3.data-types/3.string/#string_reading_and_writing","title":"String reading and writing","text":"<p>Nebula\u00a0Graph supports using string types in the following ways:</p> <ul> <li>Define the data type of VID as a fixed-length string.</li> <li>Set the variable-length string as the Schema name, including the names of the graph space, tag, edge type, and property.</li> <li>Define the data type of the property as a fixed-length or variable-length string.</li> </ul> <p>For example:</p> <ul> <li>Define the data type of the property as a fixed-length string<pre><code>nebula&gt; CREATE TAG IF NOT EXISTS t1 (p1 FIXED_STRING(10)); \n</code></pre> </li> </ul> <ul> <li>Define the data type of the property as a variable-length string<pre><code>nebula&gt; CREATE TAG IF NOT EXISTS t2 (p2 STRING); \n</code></pre> </li> </ul> <p>When the fixed-length string you try to write exceeds the length limit:</p> <ul> <li>If the fixed-length string is a property, the writing will succeed, and NebulaGraph will truncate the string and only store the part that meets the length limit.</li> <li>If the fixed-length string is a VID, the writing will fail and NebulaGraph will return an error.</li> </ul>"},{"location":"3.ngql-guide/3.data-types/3.string/#escape_characters","title":"Escape characters","text":"<p>Line breaks are not allowed in a string. Escape characters are supported within strings, for example:</p> <ul> <li><code>\"\\n\\t\\r\\b\\f\"</code></li> </ul> <ul> <li><code>\"\\110ello world\"</code></li> </ul>"},{"location":"3.ngql-guide/3.data-types/3.string/#opencypher_compatibility","title":"OpenCypher compatibility","text":"<p>There are some tiny differences between openCypher and Cypher, as well as nGQL. The following is what openCypher requires. Single quotes cannot be converted to double quotes.</p> <pre><code># File: Literals.feature\nFeature: Literals\n\nBackground:\n    Given any graph\n Scenario: Return a single-quoted string\n    When executing query:\n      \"\"\"\n      RETURN '' AS literal\n      \"\"\"\n    Then the result should be, in any order:\n      | literal |\n      | ''      |    # Note: it should return single-quotes as openCypher required.\n    And no side effects\n</code></pre> <p>While Cypher accepts both single quotes and double quotes as the return results. nGQL follows the Cypher way.</p> <pre><code>nebula &gt; YIELD '' AS quote1, \"\" AS quote2, \"'\" AS quote3, '\"' AS quote4\n+--------+--------+--------+--------+\n| quote1 | quote2 | quote3 | quote4 |\n+--------+--------+--------+--------+\n| \"\"     | \"\"     | \"'\"    | \"\"\"    |\n+--------+--------+--------+--------+\n</code></pre>"},{"location":"3.ngql-guide/3.data-types/4.date-and-time/","title":"Date and time types","text":"<p>This topic will describe the <code>DATE</code>, <code>TIME</code>, <code>DATETIME</code>, <code>TIMESTAMP</code>, and <code>DURATION</code> types.</p>"},{"location":"3.ngql-guide/3.data-types/4.date-and-time/#precautions","title":"Precautions","text":"<ul> <li> <p>While inserting time-type property values with <code>DATE</code>, <code>TIME</code>, and <code>DATETIME</code>, NebulaGraph transforms them to a UTC time according to the timezone specified with the <code>timezone_name</code> parameter in the configuration files. </p> <p>Note</p> <p>To change the timezone, modify the <code>timezone_name</code> value in the configuration files of all NebulaGraph services.</p> </li> </ul> <ul> <li><code>date()</code>, <code>time()</code>, and <code>datetime()</code> can convert a time-type property with a specified timezone. For example, <code>datetime(\"2017-03-04 22:30:40.003000+08:00\")</code> or <code>datetime(\"2017-03-04T22:30:40.003000[Asia/Shanghai]\")</code>.</li> </ul> <ul> <li><code>date()</code>, <code>time()</code>, <code>datetime()</code>, and <code>timestamp()</code> all accept empty parameters to return the current date, time, and datetime.</li> </ul> <ul> <li><code>date()</code>, <code>time()</code>, and <code>datetime()</code> all accept the property name to return a specific property value of itself. For example, <code>date().month</code> returns the current month, while <code>time(\"02:59:40\").minute</code> returns the minutes of the importing time.</li> </ul> <ul> <li>For time operations it is recommended to use <code>duration()</code> to calculate the offset of the moment. Addition and subtraction of <code>date()</code> and <code>date()</code>, <code>timestamp()</code> and <code>timestamp()</code> are also supported.</li> </ul> <ul> <li>When setting the year of the time as a negative number, you need to use Map type data.</li> </ul>"},{"location":"3.ngql-guide/3.data-types/4.date-and-time/#opencypher_compatibility","title":"OpenCypher Compatibility","text":"<p>In nGQL:</p> <ul> <li>Year, month, day, hour, minute, second, millisecond, and microsecond are supported, while the nanosecond is not supported.</li> </ul> <ul> <li><code>localdatetime()</code> is not supported.</li> </ul> <ul> <li>Most string time formats are not supported. The exceptions are <code>YYYY-MM-DDThh:mm:ss</code> and <code>YYYY-MM-DD hh:mm:ss</code>.</li> </ul> <ul> <li>The single-digit string time format is supported. For example, <code>time(\"1:1:1\")</code>.</li> </ul>"},{"location":"3.ngql-guide/3.data-types/4.date-and-time/#date","title":"DATE","text":"<p>The <code>DATE</code> type is used for values with a date part but no time part. Nebula\u00a0Graph retrieves and displays <code>DATE</code> values in the <code>YYYY-MM-DD</code> format. The supported range is <code>-32768-01-01</code> to <code>32767-12-31</code>.</p> <p>The properties of <code>date()</code> include <code>year</code>, <code>month</code>, and <code>day</code>. <code>date()</code> supports the input of <code>YYYYY</code>, <code>YYYYY-MM</code> or <code>YYYYY-MM-DD</code>, and defaults to <code>01</code> for an untyped month or day.</p> <pre><code>nebula&gt; RETURN DATE({year:-123, month:12, day:3});\n+------------------------------------+\n| date({year:-(123),month:12,day:3}) |\n+------------------------------------+\n| -123-12-03                         |\n+------------------------------------+\n\nnebula&gt; RETURN DATE(\"23333\");\n+---------------+\n| date(\"23333\") |\n+---------------+\n| 23333-01-01   |\n+---------------+\n\nnebula&gt; RETURN DATE(\"2023-12-12\") - DATE(\"2023-12-11\");\n+-----------------------------------------+\n| (date(\"2023-12-12\")-date(\"2023-12-11\")) |\n+-----------------------------------------+\n| 1                                       |\n+-----------------------------------------+\n</code></pre>"},{"location":"3.ngql-guide/3.data-types/4.date-and-time/#time","title":"TIME","text":"<p>The <code>TIME</code> type is used for values with a time part but no date part. Nebula\u00a0Graph retrieves and displays <code>TIME</code> values in <code>hh:mm:ss.msmsmsususus</code> format. The supported range is <code>00:00:00.000000</code> to <code>23:59:59.999999</code>.</p> <p>The properties of <code>time()</code> include <code>hour</code>, <code>minute</code>, and <code>second</code>.</p>"},{"location":"3.ngql-guide/3.data-types/4.date-and-time/#datetime","title":"DATETIME","text":"<p>The <code>DATETIME</code> type is used for values that contain both date and time parts. Nebula\u00a0Graph retrieves and displays <code>DATETIME</code> values in <code>YYYY-MM-DDThh:mm:ss.msmsmsususus</code> format. The supported range is <code>-32768-01-01T00:00:00.000000</code> to <code>32767-12-31T23:59:59.999999</code>.</p> <ul> <li>The properties of <code>datetime()</code> include <code>year</code>, <code>month</code>, <code>day</code>, <code>hour</code>, <code>minute</code>, and <code>second</code>.</li> </ul> <ul> <li><code>datetime()</code> can convert <code>TIMESTAMP</code> to <code>DATETIME</code>. The value range of <code>TIMESTAMP</code> is <code>0~9223372036</code>.</li> </ul> <ul> <li><code>datetime()</code> supports an <code>int</code> argument. The <code>int</code> argument specifies a timestamp.</li> </ul> <pre><code># To get the current date and time.\nnebula&gt; RETURN datetime();\n+----------------------------+\n| datetime()                 |\n+----------------------------+\n| 2022-08-29T06:37:08.933000 |\n+----------------------------+\n\n# To get the current hour.\nnebula&gt; RETURN datetime().hour;\n+-----------------+\n| datetime().hour |\n+-----------------+\n| 6               |\n+-----------------+\n\n# To get date time from a given timestamp.\nnebula&gt; RETURN datetime(timestamp(1625469277));\n+---------------------------------+\n| datetime(timestamp(1625469277)) |\n+---------------------------------+\n| 2021-07-05T07:14:37.000000      |\n+---------------------------------+\n\nnebula&gt; RETURN datetime(1625469277);\n+----------------------------+\n| datetime(1625469277)       |\n+----------------------------+\n| 2021-07-05T07:14:37.000000 |\n+----------------------------+\n</code></pre>"},{"location":"3.ngql-guide/3.data-types/4.date-and-time/#timestamp","title":"TIMESTAMP","text":"<p>The <code>TIMESTAMP</code> data type is used for values that contain both date and time parts. It has a range of <code>1970-01-01T00:00:01</code> UTC to <code>2262-04-11T23:47:16</code> UTC.</p> <p><code>TIMESTAMP</code> has the following features:</p> <ul> <li>Stored and displayed in the form of a timestamp, such as <code>1615974839</code>, which means <code>2021-03-17T17:53:59</code>.</li> </ul> <ul> <li>Supported <code>TIMESTAMP</code> querying methods: timestamp and <code>timestamp()</code> function.</li> </ul> <ul> <li>Supported <code>TIMESTAMP</code> inserting methods: timestamp, <code>timestamp()</code> function, and <code>now()</code> function.</li> </ul> <ul> <li><code>timestamp()</code> function accepts empty arguments to get the current timestamp. It can pass an integer arguments to identify the integer as a timestamp and the range of passed integer is: <code>0~9223372036</code>\u3002</li> </ul> <ul> <li><code>timestamp()</code> function can convert <code>DATETIME</code> to <code>TIMESTAMP</code>, and the data type of <code>DATETIME</code> should be a <code>string</code>. </li> </ul> <ul> <li>The underlying storage data type is int64.</li> </ul> <pre><code># To get the current timestamp.\nnebula&gt; RETURN timestamp();\n+-------------+\n| timestamp() |\n+-------------+\n| 1625469277  |\n+-------------+\n\n# To get a timestamp from given date and time.\nnebula&gt; RETURN timestamp(\"2022-01-05T06:18:43\");\n+----------------------------------+\n| timestamp(\"2022-01-05T06:18:43\") |\n+----------------------------------+\n| 1641363523                       |\n+----------------------------------+\n\n# To get a timestamp using datetime().\nnebula&gt; RETURN timestamp(datetime(\"2022-08-29T07:53:10.939000\"));\n+---------------------------------------------------+\n| timestamp(datetime(\"2022-08-29T07:53:10.939000\")) |\n+---------------------------------------------------+\n| 1661759590                                        |\n+---------------------------------------------------+    \n</code></pre> <p>Note</p> <p>The date and time format string passed into <code>timestamp()</code> cannot include any millisecond and microsecond, but the date and time format string passed into <code>timestamp(datetime())</code> can include a millisecond and a microsecond.</p>"},{"location":"3.ngql-guide/3.data-types/4.date-and-time/#duration","title":"DURATION","text":"<p>The <code>DURATION</code> data type is used to indicate a period of time. Map data that are freely combined by <code>years</code>, <code>months</code>, <code>days</code>, <code>hours</code>, <code>minutes</code>, and <code>seconds</code> indicates the <code>DURATION</code>.</p> <p><code>DURATION</code> has the following features:</p> <ul> <li>Creating indexes for <code>DURATION</code> is not supported.</li> </ul> <ul> <li><code>DURATION</code> can be used to calculate the specified time.</li> </ul>"},{"location":"3.ngql-guide/3.data-types/4.date-and-time/#examples","title":"Examples","text":"<ol> <li> <p>Create a tag named <code>date1</code> with three properties: <code>DATE</code>, <code>TIME</code>, and <code>DATETIME</code>.</p> <pre><code>nebula&gt; CREATE TAG IF NOT EXISTS date1(p1 date, p2 time, p3 datetime);\n</code></pre> </li> <li> <p>Insert a vertex named <code>test1</code>.</p> <pre><code>nebula&gt; INSERT VERTEX date1(p1, p2, p3) VALUES \"test1\":(date(\"2021-03-17\"), time(\"17:53:59\"), datetime(\"2017-03-04T22:30:40.003000[Asia/Shanghai]\"));\n</code></pre> </li> <li> <p>Query whether the value of property <code>p1</code> on the <code>test1</code> tag is <code>2021-03-17</code>.</p> <pre><code>nebula&gt; MATCH (v:date1)  RETURN v.date1.p1 == date(\"2021-03-17\");\n+----------------------------------+\n| (v.date1.p1==date(\"2021-03-17\")) |\n+----------------------------------+\n| true                             |\n+----------------------------------+\n</code></pre> </li> <li> <p>Return the content of the property <code>p1</code> on <code>test1</code>.</p> <pre><code>nebula&gt; CREATE TAG INDEX IF NOT EXISTS date1_index ON date1(p1);\nnebula&gt; REBUILD TAG INDEX date1_index;\nnebula&gt; MATCH (v:date1) RETURN v.date1.p1;\n+------------------+\n| v.date1.p1.month |\n+------------------+\n| 3                |\n+------------------+\n</code></pre> </li> <li> <p>Search for vertices with <code>p3</code> property values less than <code>2023-01-01T00:00:00.000000</code>, and return the <code>p3</code> values.</p> <pre><code>nebula&gt; MATCH (v:date1)  \\\nWHERE v.date1.p3 &lt; datetime(\"2023-01-01T00:00:00.000000\") \\\nRETURN v.date1.p3;\n+----------------------------+\n| v.date1.p3                 |\n+----------------------------+\n| 2017-03-04T14:30:40.003000 |\n+----------------------------+\n</code></pre> </li> <li> <p>Create a tag named <code>school</code> with the property of <code>TIMESTAMP</code>.</p> <pre><code>nebula&gt; CREATE TAG IF NOT EXISTS school(name string , found_time timestamp);\n</code></pre> </li> <li> <p>Insert a vertex named <code>DUT</code> with a found-time timestamp of <code>\"1988-03-01T08:00:00\"</code>.</p> <pre><code># Insert as a timestamp. The corresponding timestamp of 1988-03-01T08:00:00 is 573177600, or 573206400 UTC.\nnebula&gt; INSERT VERTEX school(name, found_time) VALUES \"DUT\":(\"DUT\", 573206400);\n\n# Insert in the form of date and time.\nnebula&gt; INSERT VERTEX school(name, found_time) VALUES \"DUT\":(\"DUT\", timestamp(\"1988-03-01T08:00:00\"));\n</code></pre> </li> <li> <p>Insert a vertex named <code>dut</code> and store time with <code>now()</code> or <code>timestamp()</code> functions.</p> <pre><code># Use now() function to store time\nnebula&gt; INSERT VERTEX school(name, found_time) VALUES \"dut\":(\"dut\", now());\n\n# Use timestamp() function to store time\nnebula&gt; INSERT VERTEX school(name, found_time) VALUES \"dut\":(\"dut\", timestamp());\n</code></pre> </li> </ol> <p>You can also use <code>WITH</code> statement to set a specific date and time, or to perform calculations. For example:</p> <pre><code>nebula&gt; WITH time({hour: 12, minute: 31, second: 14, millisecond:111, microsecond: 222}) AS d RETURN d;\n+-----------------+\n| d               |\n+-----------------+\n| 12:31:14.111222 |\n+-----------------+\n\nnebula&gt; WITH date({year: 1984, month: 10, day: 11}) AS x RETURN x + 1;\n+------------+\n| (x+1)      |\n+------------+\n| 1984-10-12 |\n+------------+\n\nnebula&gt; WITH date('1984-10-11') as x, duration({years: 12, days: 14, hours: 99, minutes: 12}) as d \\\n        RETURN x + d AS sum, x - d AS diff;\n+------------+------------+\n| sum        | diff       |\n+------------+------------+\n| 1996-10-29 | 1972-09-23 |\n+------------+------------+\n</code></pre>"},{"location":"3.ngql-guide/3.data-types/5.null/","title":"NULL","text":"<p>You can set the properties for vertices or edges to <code>NULL</code>. Also, you can set the <code>NOT NULL</code> constraint to make sure that the property values are <code>NOT NULL</code>. If not specified, the property is set to <code>NULL</code> by default.</p>"},{"location":"3.ngql-guide/3.data-types/5.null/#logical_operations_with_null","title":"Logical operations with NULL","text":"<p>Here is the truth table for <code>AND</code>, <code>OR</code>, <code>XOR</code>, and <code>NOT</code>.</p> a b a AND b a OR b a XOR b NOT a false false false false false true false null false null null true false true false true true true true false false true true false true null null true null false true true true true false false null false false null null null null null null null null null null true null true null null"},{"location":"3.ngql-guide/3.data-types/5.null/#opencypher_compatibility","title":"OpenCypher compatibility","text":"<p>The comparisons and operations about NULL are different from openCypher. There may be changes later.</p>"},{"location":"3.ngql-guide/3.data-types/5.null/#comparisons_with_null","title":"Comparisons with NULL","text":"<p>The comparison operations with NULL are incompatible with openCypher.</p>"},{"location":"3.ngql-guide/3.data-types/5.null/#operations_and_return_with_null","title":"Operations and RETURN with NULL","text":"<p>The NULL operations and RETURN with NULL are incompatible with openCypher.</p>"},{"location":"3.ngql-guide/3.data-types/5.null/#examples","title":"Examples","text":""},{"location":"3.ngql-guide/3.data-types/5.null/#use_not_null","title":"Use NOT NULL","text":"<p>Create a tag named <code>player</code>. Specify the property <code>name</code> as <code>NOT NULL</code>.</p> <pre><code>nebula&gt; CREATE TAG IF NOT EXISTS player(name string NOT NULL, age int);\n</code></pre> <p>Use <code>SHOW</code> to create tag statements. The property <code>name</code> is <code>NOT NULL</code>. The property <code>age</code> is <code>NULL</code> by default.</p> <pre><code>nebula&gt; SHOW CREATE TAG player;\n+-----------+-----------------------------------+\n| Tag       | Create Tag                        |\n+-----------+-----------------------------------+\n| \"student\" | \"CREATE TAG `player` (            |\n|           |  `name` string NOT NULL,          |\n|           |  `age` int64 NULL                 |\n|           | ) ttl_duration = 0, ttl_col = \"\"\" |\n+-----------+-----------------------------------+\n</code></pre> <p>Insert the vertex <code>Kobe</code>. The property <code>age</code> can be <code>NULL</code>.</p> <pre><code>nebula&gt; INSERT VERTEX player(name, age) VALUES \"Kobe\":(\"Kobe\",null);\n</code></pre>"},{"location":"3.ngql-guide/3.data-types/5.null/#use_not_null_and_set_the_default","title":"Use NOT NULL and set the default","text":"<p>Create a tag named <code>player</code>. Specify the property <code>age</code> as <code>NOT NULL</code>. The default value is <code>18</code>.</p> <pre><code>nebula&gt; CREATE TAG IF NOT EXISTS player(name string, age int NOT NULL DEFAULT 18);\n</code></pre> <p>Insert the vertex <code>Kobe</code>. Specify the property <code>name</code> only.</p> <pre><code>nebula&gt; INSERT VERTEX player(name) VALUES \"Kobe\":(\"Kobe\");\n</code></pre> <p>Query the vertex <code>Kobe</code>. The property <code>age</code> is <code>18</code> by default.</p> <pre><code>nebula&gt; FETCH PROP ON player \"Kobe\" YIELD properties(vertex);\n+--------------------------+\n| properties(VERTEX)       |\n+--------------------------+\n| {age: 18, name: \"Kobe\"}  |\n+--------------------------+\n</code></pre>"},{"location":"3.ngql-guide/3.data-types/6.list/","title":"Lists","text":"<p>The list is a composite data type. A list is a sequence of values. Individual elements in a list can be accessed by their positions.</p> <p>A list starts with a left square bracket <code>[</code> and ends with a right square bracket <code>]</code>. A list contains zero, one, or more expressions. List elements are separated from each other with commas (<code>,</code>). Whitespace around elements is ignored in the list, thus line breaks, tab stops, and blanks can be used for formatting.</p>"},{"location":"3.ngql-guide/3.data-types/6.list/#opencypher_compatibility","title":"OpenCypher compatibility","text":"<p>A composite data type (i.e. set, map, and list) CANNOT be stored as properties of vertices or edges.</p>"},{"location":"3.ngql-guide/3.data-types/6.list/#list_operations","title":"List operations","text":"<p>You can use the preset list function to operate the list, or use the index to filter the elements in the list.</p>"},{"location":"3.ngql-guide/3.data-types/6.list/#index_syntax","title":"Index syntax","text":"<pre><code>[M]\n[M..N]\n[M..]\n[..N]\n</code></pre> <p>The index of nGQL supports queries from front to back, starting from 0. 0 means the first element, 1 means the second element, and so on. It also supports queries from back to front, starting from -1. -1 means the last element, -2 means the penultimate element, and so on.</p> <ul> <li>[M]: represents the element whose index is M.</li> <li>[M..N]: represents the elements whose indexes are <code>greater or equal to M but smaller than N</code>. Return empty when <code>N</code> is 0.</li> <li>[M..]: represents the elements whose indexes are <code>greater or equal to M</code>.</li> <li>[..N]: represents the elements whose indexes are <code>smaller than N</code>. Return empty when <code>N</code> is 0.</li> </ul> <p>Note</p> <ul> <li>Return empty if the index is out of bounds, while return normally if the index is within the bound.</li> <li>Return empty if <code>M</code>\u2265<code>N</code>.</li> <li>When querying a single element, if <code>M</code> is null, return <code>BAD_TYPE</code>. When conducting a range query, if <code>M</code> or <code>N</code> is null, return <code>null</code>.</li> </ul>"},{"location":"3.ngql-guide/3.data-types/6.list/#examples","title":"Examples","text":"<pre><code># The following query returns the list [1,2,3].\nnebula&gt; RETURN list[1, 2, 3] AS a;\n+-----------+\n| a         |\n+-----------+\n| [1, 2, 3] |\n+-----------+\n\n# The following query returns the element whose index is 3 in the list [1,2,3,4,5]. In a list, the index starts from 0, and thus the return element is 4.\nnebula&gt; RETURN range(1,5)[3];\n+---------------+\n| range(1,5)[3] |\n+---------------+\n| 4             |\n+---------------+\n\n# The following query returns the element whose index is -2 in the list [1,2,3,4,5]. The index of the last element in a list is -1, and thus the return element is 4.\nnebula&gt; RETURN range(1,5)[-2];\n+------------------+\n| range(1,5)[-(2)] |\n+------------------+\n| 4                |\n+------------------+\n\n# The following query returns the elements whose indexes are from 0 to 3 (not including 3) in the list [1,2,3,4,5].\nnebula&gt; RETURN range(1,5)[0..3];\n+------------------+\n| range(1,5)[0..3] |\n+------------------+\n| [1, 2, 3]        |\n+------------------+\n\n# The following query returns the elements whose indexes are greater than 2 in the list [1,2,3,4,5].\nnebula&gt; RETURN range(1,5)[3..] AS a;\n+--------+\n| a      |\n+--------+\n| [4, 5] |\n+--------+\n\n# The following query returns the elements whose indexes are smaller than 3.\nnebula&gt; WITH list[1, 2, 3, 4, 5] AS a \\\n        RETURN a[..3] AS r;\n+-----------+\n| r         |\n+-----------+\n| [1, 2, 3] |\n+-----------+\n\n# The following query filters the elements whose indexes are greater than 2 in the list [1,2,3,4,5], calculate them respectively, and returns them.\nnebula&gt; RETURN [n IN range(1,5) WHERE n &gt; 2 | n + 10] AS a;\n+--------------+\n| a            |\n+--------------+\n| [13, 14, 15] |\n+--------------+\n\n# The following query returns the elements from the first to the penultimate (inclusive) in the list [1, 2, 3].\nnebula&gt; YIELD list[1, 2, 3][0..-1] AS a;\n+--------+\n| a      |\n+--------+\n| [1, 2] |\n+--------+\n\n# The following query returns the elements from the first (exclusive) to the third backward in the list [1, 2, 3, 4, 5].\nnebula&gt; YIELD list[1, 2, 3, 4, 5][-3..-1] AS a;\n+--------+\n| a      |\n+--------+\n| [3, 4] |\n+--------+\n\n# The following query sets the variables and returns the elements whose indexes are 1 and 2.\nnebula&gt; $var = YIELD 1 AS f, 3 AS t; \\\n        YIELD list[1, 2, 3][$var.f..$var.t] AS a;\n+--------+\n| a      |\n+--------+\n| [2, 3] |\n+--------+\n\n# The following query returns empty because the index is out of bound. It will return normally when the index is within the bound.\nnebula&gt; RETURN list[1, 2, 3, 4, 5] [0..10] AS a;\n+-----------------+\n| a               |\n+-----------------+\n| [1, 2, 3, 4, 5] |\n+-----------------+\n\nnebula&gt; RETURN list[1, 2, 3] [-5..5] AS a;\n+-----------+\n| a         |\n+-----------+\n| [1, 2, 3] |\n+-----------+\n\n# The following query returns empty because there is a [0..0].\nnebula&gt; RETURN list[1, 2, 3, 4, 5] [0..0] AS a;\n+----+\n| a  |\n+----+\n| [] |\n+----+\n\n# The following query returns empty because of M \u2265 N.\nnebula&gt; RETURN list[1, 2, 3, 4, 5] [3..1] AS a;\n+----+\n| a  |\n+----+\n| [] |\n+----+\n\n# When conduct a range query, if `M` or `N` is null, return `null`.\nnebula&gt; WITH list[1,2,3] AS a \\\n        RETURN a[0..null] as r;\n+----------+\n| r        |\n+----------+\n| __NULL__ |\n+----------+\n\n# The following query calculates the elements in the list [1,2,3,4,5] respectively and returns them without the list head.\nnebula&gt; RETURN tail([n IN range(1, 5) | 2 * n - 10]) AS a;\n+-----------------+\n| a               |\n+-----------------+\n| [-6, -4, -2, 0] |\n+-----------------+\n\n# The following query takes the elements in the list [1,2,3] as true and return.\nnebula&gt; RETURN [n IN range(1, 3) WHERE true | n] AS r;\n+-----------+\n| r         |\n+-----------+\n| [1, 2, 3] |\n+-----------+\n\n# The following query returns the length of the list [1,2,3].\nnebula&gt; RETURN size(list[1,2,3]);\n+-------------------+\n| size(list[1,2,3]) |\n+-------------------+\n| 3                 |\n+-------------------+\n\n# The following query calculates the elements in the list [92,90] and runs a conditional judgment in a where clause.\nnebula&gt; GO FROM \"player100\" OVER follow WHERE properties(edge).degree NOT IN [x IN [92, 90] | x + $$.player.age] \\\n        YIELD dst(edge) AS id, properties(edge).degree AS degree;\n+-------------+--------+\n| id          | degree |\n+-------------+--------+\n| \"player101\" | 95     |\n| \"player102\" | 90     |\n+-------------+--------+\n\n# The following query takes the query result of the MATCH statement as the elements in a list. Then it calculates and returns them.\nnebula&gt; MATCH p = (n:player{name:\"Tim Duncan\"})-[:follow]-&gt;(m) \\\n        RETURN [n IN nodes(p) | n.player.age + 100] AS r;\n+------------+\n| r          |\n+------------+\n| [142, 136] |\n| [142, 141] |\n+------------+\n</code></pre>"},{"location":"3.ngql-guide/3.data-types/6.list/#opencypher_compatibility_1","title":"OpenCypher compatibility","text":"<ul> <li>In openCypher, return <code>null</code> when querying a single out-of-bound element. However, in nGQL, return <code>OUT_OF_RANGE</code> when querying a single out-of-bound element.<pre><code>nebula&gt; RETURN range(0,5)[-12];\n+-------------------+\n| range(0,5)[-(12)] |\n+-------------------+\n| OUT_OF_RANGE      |\n+-------------------+\n</code></pre> </li> </ul> <ul> <li> <p>A composite data type (i.e., set, map, and list) CAN NOT be stored as properties for vertices or edges.</p> <p>It is recommended to modify the graph modeling method. The composite data type should be modeled as an adjacent edge of a vertex, rather than its property. Each adjacent edge can be dynamically added or deleted. The rank values of the adjacent edges can be used for sequencing.</p> </li> </ul> <ul> <li>Patterns are not supported in the list. For example, <code>[(src)-[]-&gt;(m) | m.name]</code>.</li> </ul>"},{"location":"3.ngql-guide/3.data-types/7.set/","title":"Sets","text":"<p>The set is a composite data type. A set is a set of values. Unlike a List, values in a set are unordered and each value must be unique.</p> <p>A set starts with a left curly bracket <code>{</code> and ends with a right curly bracket <code>}</code>. A set contains zero, one, or more expressions. Set elements are separated from each other with commas (<code>,</code>). Whitespace around elements is ignored in the set, thus line breaks, tab stops, and blanks can be used for formatting.</p>"},{"location":"3.ngql-guide/3.data-types/7.set/#opencypher_compatibility","title":"OpenCypher compatibility","text":"<ul> <li>A composite data type (i.e. set, map, and list) CANNOT be stored as properties of vertices or edges.</li> </ul> <ul> <li>A set is not a data type in openCypher, but in nGQL, users can use the set.</li> </ul>"},{"location":"3.ngql-guide/3.data-types/7.set/#examples","title":"Examples","text":"<pre><code># The following query returns the set {1,2,3}.\nnebula&gt; RETURN set{1, 2, 3} AS a;\n+-----------+\n| a         |\n+-----------+\n| {3, 2, 1} |\n+-----------+\n\n# The following query returns the set {1,2}, Because the set does not allow repeating elements, and the order is unordered.\nnebula&gt; RETURN set{1, 2, 1} AS a;\n+--------+\n| a      |\n+--------+\n| {2, 1} |\n+--------+\n\n# The following query checks whether the set has the specified element 1.\nnebula&gt; RETURN 1 IN set{1, 2} AS a;\n+------+\n| a    |\n+------+\n| true |\n+------+\n\n# The following query counts the number of elements in the set.\nnebula&gt; YIELD size(set{1, 2, 1}) AS a;\n+---+\n| a |\n+---+\n| 2 |\n+---+\n\n# The following query returns a set of target vertex property values.\nnebula&gt; GO FROM \"player100\" OVER follow \\\n        YIELD set{properties($$).name,properties($$).age} as a;\n+-----------------------+\n| a                     |\n+-----------------------+\n| {36, \"Tony Parker\"}   |\n| {41, \"Manu Ginobili\"} |\n+-----------------------+\n</code></pre>"},{"location":"3.ngql-guide/3.data-types/8.map/","title":"Maps","text":"<p>The map is a composite data type. Maps are unordered collections of key-value pairs. In maps, the key is a string. The value can have any data type. You can get the map element by using <code>map['key']</code>.</p> <p>A map starts with a left curly bracket <code>{</code> and ends with a right curly bracket <code>}</code>. A map contains zero, one, or more key-value pairs. Map elements are separated from each other with commas (<code>,</code>). Whitespace around elements is ignored in the map, thus line breaks, tab stops, and blanks can be used for formatting.</p>"},{"location":"3.ngql-guide/3.data-types/8.map/#opencypher_compatibility","title":"OpenCypher compatibility","text":"<ul> <li>A composite data type (i.e. set, map, and list) CANNOT be stored as properties of vertices or edges.</li> </ul> <ul> <li>Map projection is not supported.</li> </ul>"},{"location":"3.ngql-guide/3.data-types/8.map/#examples","title":"Examples","text":"<pre><code># The following query returns the simple map.\nnebula&gt; YIELD map{key1: 'Value1', Key2: 'Value2'} as a;\n+----------------------------------+\n| a                                |\n+----------------------------------+\n| {Key2: \"Value2\", key1: \"Value1\"} |\n+----------------------------------+\n\n# The following query returns the list type map.\nnebula&gt; YIELD map{listKey: [{inner: 'Map1'}, {inner: 'Map2'}]} as a;\n+-----------------------------------------------+\n| a                                             |\n+-----------------------------------------------+\n| {listKey: [{inner: \"Map1\"}, {inner: \"Map2\"}]} |\n+-----------------------------------------------+\n\n# The following query returns the hybrid type map.\nnebula&gt; RETURN map{a: LIST[1,2], b: SET{1,2,1}, c: \"hee\"} as a;\n+----------------------------------+\n| a                                |\n+----------------------------------+\n| {a: [1, 2], b: {2, 1}, c: \"hee\"} |\n+----------------------------------+\n\n# The following query returns the specified element in a map.\nnebula&gt; RETURN map{a: LIST[1,2], b: SET{1,2,1}, c: \"hee\"}[\"b\"] AS b;\n+--------+\n| b      |\n+--------+\n| {2, 1} |\n+--------+\n\n# The following query checks whether the map has the specified key, not support checks whether the map has the specified value yet.\nnebula&gt; RETURN \"a\" IN MAP{a:1, b:2} AS a;\n+------+\n| a    |\n+------+\n| true |\n+------+\n</code></pre>"},{"location":"3.ngql-guide/3.data-types/9.type-conversion/","title":"Type Conversion/Type coercions","text":"<p>Converting an expression of a given type to another type is known as type conversion.</p> <p>NebulaGraph supports converting expressions explicit to other types. For details, see Type conversion functions.</p>"},{"location":"3.ngql-guide/3.data-types/9.type-conversion/#examples","title":"Examples","text":"<pre><code>nebula&gt; UNWIND [true, false, 'true', 'false', NULL] AS b \\\n        RETURN toBoolean(b) AS b;\n+----------+\n| b        |\n+----------+\n| true     |\n| false    |\n| true     |\n| false    |\n| __NULL__ |\n+----------+\n\nnebula&gt; RETURN toFloat(1), toFloat('1.3'), toFloat('1e3'), toFloat('not a number');\n+------------+----------------+----------------+-------------------------+\n| toFloat(1) | toFloat(\"1.3\") | toFloat(\"1e3\") | toFloat(\"not a number\") |\n+------------+----------------+----------------+-------------------------+\n| 1.0        | 1.3            | 1000.0         | __NULL__                |\n+------------+----------------+----------------+-------------------------+\n</code></pre>"},{"location":"3.ngql-guide/4.variable-and-composite-queries/1.composite-queries/","title":"Composite queries (clause structure)","text":"<p>Composite queries put data from different queries together. They then use filters, group-bys, or sorting before returning the combined return results.</p> <p>Nebula\u00a0Graph supports three methods to run composite queries (or sub-queries):</p> <ul> <li>(openCypher) Clauses are chained together, and they feed intermediate result sets between each other.</li> </ul> <ul> <li>(Native nGQL) More than one query can be batched together, separated by semicolons (;). The result of the last query is returned as the result of the batch.</li> </ul> <ul> <li>(Native nGQL) Queries can be piped together by using the pipe (<code>|</code>). The result of the previous query can be used as the input of the next query.</li> </ul>"},{"location":"3.ngql-guide/4.variable-and-composite-queries/1.composite-queries/#opencypher_compatibility","title":"OpenCypher compatibility","text":"<p>In a composite query, do not put together openCypher and native nGQL clauses in one statement. For example, this statement is undefined: <code>MATCH ... | GO ... | YIELD ...</code>.</p> <ul> <li>If you are in the openCypher way (<code>MATCH</code>, <code>RETURN</code>, <code>WITH</code>, etc), do not introduce any pipe or semicolons to combine the sub-clauses.</li> </ul> <ul> <li>If you are in the native nGQL way (<code>FETCH</code>, <code>GO</code>, <code>LOOKUP</code>, etc), you must use pipe or semicolons to combine the sub-clauses.</li> </ul>"},{"location":"3.ngql-guide/4.variable-and-composite-queries/1.composite-queries/#composite_queries_are_not_transactional_queries_as_in_sqlcypher","title":"Composite queries are not <code>transactional</code> queries (as in SQL/Cypher)","text":"<p>For example, a query is composed of three sub-queries: <code>A B C</code>, <code>A | B | C</code> or <code>A; B; C</code>. In that A is a read operation, B is a computation operation, and C is a write operation. If any part fails in the execution, the whole result will be undefined. There is no rollback. What is written depends on the query executor.</p> <p>Note</p> <p>OpenCypher has no requirement of <code>transaction</code>.</p>"},{"location":"3.ngql-guide/4.variable-and-composite-queries/1.composite-queries/#examples","title":"Examples","text":"<ul> <li>OpenCypher compatibility statement<pre><code># Connect multiple queries with clauses.\nnebula&gt; MATCH p=(v:player{name:\"Tim Duncan\"})--() \\\n        WITH nodes(p) AS n \\\n        UNWIND n AS n1 \\\n        RETURN DISTINCT n1;\n</code></pre> </li> </ul> <ul> <li>Native nGQL (Semicolon queries)<pre><code># Only return edges.\nnebula&gt; SHOW TAGS; SHOW EDGES;\n\n# Insert multiple vertices.\nnebula&gt; INSERT VERTEX player(name, age) VALUES \"player100\":(\"Tim Duncan\", 42); \\\n        INSERT VERTEX player(name, age) VALUES \"player101\":(\"Tony Parker\", 36); \\\n        INSERT VERTEX player(name, age) VALUES \"player102\":(\"LaMarcus Aldridge\", 33);\n</code></pre> </li> </ul> <ul> <li>Native nGQL (Pipe queries)<pre><code># Connect multiple queries with pipes.\nnebula&gt; GO FROM \"player100\" OVER follow YIELD dst(edge) AS id | \\\n        GO FROM $-.id OVER serve YIELD properties($$).name AS Team, \\\n        properties($^).name AS Player;\n+-----------+-----------------+\n| Team      | Player          |\n+-----------+-----------------+\n| \"Spurs\"   | \"Tony Parker\"   |\n| \"Hornets\" | \"Tony Parker\"   |\n| \"Spurs\"   | \"Manu Ginobili\" |\n+-----------+-----------------+\n</code></pre> </li> </ul>"},{"location":"3.ngql-guide/4.variable-and-composite-queries/2.user-defined-variables/","title":"User-defined variables","text":"<p>User-defined variables allow passing the result of one statement to another.</p>"},{"location":"3.ngql-guide/4.variable-and-composite-queries/2.user-defined-variables/#opencypher_compatibility","title":"OpenCypher compatibility","text":"<p>In openCypher, when you refer to the vertex, edge, or path of a variable, you need to name it first. For example:</p> <pre><code>nebula&gt; MATCH (v:player{name:\"Tim Duncan\"}) RETURN v;\n+----------------------------------------------------+\n| v                                                  |\n+----------------------------------------------------+\n| (\"player100\" :player{name: \"Tim Duncan\", age: 42}) |\n+----------------------------------------------------+\n</code></pre> <p>The user-defined variable in the preceding query is <code>v</code>.</p> <p>Caution</p> <p>In a pattern of a MATCH statement, you cannot use the same edge variable repeatedly. For example, <code>e</code> cannot be written in the pattern <code>p=(v1)-[e*2..2]-&gt;(v2)-[e*2..2]-&gt;(v3)</code>.</p>"},{"location":"3.ngql-guide/4.variable-and-composite-queries/2.user-defined-variables/#native_ngql","title":"Native nGQL","text":"<p>User-defined variables are written as <code>$var_name</code>. The <code>var_name</code> consists of letters, numbers, or underline characters. Any other characters are not permitted.</p> <p>The user-defined variables are valid only at the current execution (namely, in this composite query). When the execution ends, the user-defined variables will be automatically expired. The user-defined variables in one statement CANNOT be used in any other clients, executions, or sessions.</p> <p>You can use user-defined variables in composite queries. Details about composite queries, see Composite queries.</p> <p>Note</p> <ul> <li>User-defined variables are case-sensitive.</li> <li>To define a user-defined variable in a compound statement, end the statement with a semicolon (;). For details, please refer to the nGQL Style Guide.</li> </ul>"},{"location":"3.ngql-guide/4.variable-and-composite-queries/2.user-defined-variables/#example","title":"Example","text":"<pre><code>nebula&gt; $var = GO FROM \"player100\" OVER follow YIELD dst(edge) AS id; \\\n        GO FROM $var.id OVER serve YIELD properties($$).name AS Team, \\\n        properties($^).name AS Player;\n+-----------+-----------------+\n| Team      | Player          |\n+-----------+-----------------+\n| \"Spurs\"   | \"Tony Parker\"   |\n| \"Hornets\" | \"Tony Parker\"   |\n| \"Spurs\"   | \"Manu Ginobili\" |\n+-----------+-----------------+\n</code></pre>"},{"location":"3.ngql-guide/4.variable-and-composite-queries/3.property-reference/","title":"Property reference","text":"<p>You can refer to the properties of a vertex or an edge in <code>WHERE</code> and <code>YIELD</code> syntax.</p> <p>Note</p> <p>This function applies to native nGQL only.</p>"},{"location":"3.ngql-guide/4.variable-and-composite-queries/3.property-reference/#property_reference_for_vertex","title":"Property reference for vertex","text":""},{"location":"3.ngql-guide/4.variable-and-composite-queries/3.property-reference/#for_source_vertex","title":"For source vertex","text":"<pre><code>$^.&lt;tag_name&gt;.&lt;prop_name&gt;\n</code></pre> Parameter Description <code>$^</code> is used to get the property of the source vertex. <code>tag_name</code> is the tag name of the vertex. <code>prop_name</code> specifies the property name."},{"location":"3.ngql-guide/4.variable-and-composite-queries/3.property-reference/#for_destination_vertex","title":"For destination vertex","text":"<pre><code>$$.&lt;tag_name&gt;.&lt;prop_name&gt;\n</code></pre> Parameter Description <code>$$</code> is used to get the property of the destination vertex. <code>tag_name</code> is the tag name of the vertex. <code>prop_name</code> specifies the property name."},{"location":"3.ngql-guide/4.variable-and-composite-queries/3.property-reference/#property_reference_for_edge","title":"Property reference for edge","text":""},{"location":"3.ngql-guide/4.variable-and-composite-queries/3.property-reference/#for_user-defined_edge_property","title":"For user-defined edge property","text":"<pre><code>&lt;edge_type&gt;.&lt;prop_name&gt;\n</code></pre> Parameter Description <code>edge_type</code> is the edge type of the edge. <code>prop_name</code> specifies the property name of the edge type."},{"location":"3.ngql-guide/4.variable-and-composite-queries/3.property-reference/#for_built-in_properties","title":"For built-in properties","text":"<p>Apart from the user-defined edge property, there are four built-in properties in each edge:</p> Parameter Description <code>_src</code> source vertex ID of the edge <code>_dst</code> destination vertex ID of the edge <code>_type</code> edge type <code>_rank</code> the rank value for the edge"},{"location":"3.ngql-guide/4.variable-and-composite-queries/3.property-reference/#examples","title":"Examples","text":"<p>The following query returns the <code>name</code> property of the <code>player</code> tag on the source vertex and the <code>age</code> property of the <code>player</code> tag on the destination vertex.</p> <pre><code>nebula&gt; GO FROM \"player100\" OVER follow YIELD $^.player.name AS startName, $$.player.age AS endAge;\n+--------------+--------+\n| startName    | endAge |\n+--------------+--------+\n| \"Tim Duncan\" | 36     |\n| \"Tim Duncan\" | 41     |\n+--------------+--------+\n</code></pre> <p>The following query returns the <code>degree</code> property of the edge type <code>follow</code>.</p> <pre><code>nebula&gt; GO FROM \"player100\" OVER follow YIELD follow.degree;\n+---------------+\n| follow.degree |\n+---------------+\n| 95            |\n+---------------+\n</code></pre> <p>The following query returns the source vertex, the destination vertex, the edge type, and the edge rank value of the edge type <code>follow</code>.</p> <pre><code>nebula&gt; GO FROM \"player100\" OVER follow YIELD follow._src, follow._dst, follow._type, follow._rank;\n+-------------+-------------+--------------+--------------+\n| follow._src | follow._dst | follow._type | follow._rank |\n+-------------+-------------+--------------+--------------+\n| \"player100\" | \"player101\" | 17           | 0            |\n| \"player100\" | \"player125\" | 17           | 0            |\n+-------------+-------------+--------------+--------------+\n</code></pre> <p>Legacy version compatibility</p> <p>NebulaGraph 2.6.0 and later versions support the new Schema-related functions. Similar statements as the above examples are written as follows in 3.5.0.</p> <pre><code>GO FROM \"player100\" OVER follow YIELD properties($^).name AS startName, properties($$).age AS endAge;\nGO FROM \"player100\" OVER follow YIELD properties(edge).degree;\nGO FROM \"player100\" OVER follow YIELD src(edge), dst(edge), type(edge), rank(edge);\n</code></pre> <p>In 3.5.0, NebulaGraph is still compatible with the old syntax.</p>"},{"location":"3.ngql-guide/5.operators/1.comparison/","title":"Comparison operators","text":"<p>NebulaGraph supports the following comparison operators.</p> Name Description <code>==</code> Equal operator <code>!=</code>, <code>&lt;&gt;</code> Not equal operator <code>&gt;</code> Greater than operator <code>&gt;=</code> Greater than or equal operator <code>&lt;</code> Less than operator <code>&lt;=</code> Less than or equal operator <code>IS NULL</code> NULL check <code>IS NOT NULL</code> Not NULL check <code>IS EMPTY</code> EMPTY check <code>IS NOT EMPTY</code> Not EMPTY check <p>The result of the comparison operation is <code>true</code> or <code>false</code>.</p> <p>Note</p> <ul> <li>Comparability between values of different types is often undefined. The result could be <code>NULL</code> or others.</li> </ul> <ul> <li><code>EMPTY</code> is currently used only for checking, and does not support functions or operations such as <code>GROUP BY</code>, <code>count()</code>, <code>sum()</code>, <code>max()</code>, <code>hash()</code>, <code>collect()</code>, <code>+</code> or <code>*</code>.</li> </ul>"},{"location":"3.ngql-guide/5.operators/1.comparison/#opencypher_compatibility","title":"OpenCypher compatibility","text":"<p>openCypher does not have <code>EMPTY</code>. Thus <code>EMPTY</code> is not supported in MATCH statements.</p>"},{"location":"3.ngql-guide/5.operators/1.comparison/#examples","title":"Examples","text":""},{"location":"3.ngql-guide/5.operators/1.comparison/#_1","title":"<code>==</code>","text":"<p>String comparisons are case-sensitive. Values of different types are not equal.</p> <p>Note</p> <p>The equal operator is <code>==</code> in nGQL, while in openCypher it is <code>=</code>.</p> <pre><code>nebula&gt; RETURN 'A' == 'a', toUpper('A') == toUpper('a'), toLower('A') == toLower('a');\n+------------+------------------------------+------------------------------+\n| (\"A\"==\"a\") | (toUpper(\"A\")==toUpper(\"a\")) | (toLower(\"A\")==toLower(\"a\")) |\n+------------+------------------------------+------------------------------+\n| false      | true                         | true                         |\n+------------+------------------------------+------------------------------+\n\nnebula&gt; RETURN '2' == 2, toInteger('2') == 2;\n+----------+---------------------+\n| (\"2\"==2) | (toInteger(\"2\")==2) |\n+----------+---------------------+\n| false    | true                |\n+----------+---------------------+\n</code></pre>"},{"location":"3.ngql-guide/5.operators/1.comparison/#_2","title":"<code>&gt;</code>","text":"<pre><code>nebula&gt; RETURN 3 &gt; 2;\n+-------+\n| (3&gt;2) |\n+-------+\n| true  |\n+-------+\n\nnebula&gt; WITH 4 AS one, 3 AS two \\\n        RETURN one &gt; two AS result;\n+--------+\n| result |\n+--------+\n| true   |\n+--------+\n</code></pre>"},{"location":"3.ngql-guide/5.operators/1.comparison/#_3","title":"<code>&gt;=</code>","text":"<pre><code>nebula&gt; RETURN 2 &gt;= \"2\", 2 &gt;= 2;\n+----------+--------+\n| (2&gt;=\"2\") | (2&gt;=2) |\n+----------+--------+\n| __NULL__ | true   |\n+----------+--------+\n</code></pre>"},{"location":"3.ngql-guide/5.operators/1.comparison/#_4","title":"<code>&lt;</code>","text":"<pre><code>nebula&gt; YIELD 2.0 &lt; 1.9;\n+---------+\n| (2&lt;1.9) |\n+---------+\n| false   |\n+---------+\n</code></pre>"},{"location":"3.ngql-guide/5.operators/1.comparison/#_5","title":"<code>&lt;=</code>","text":"<pre><code>nebula&gt; YIELD 0.11 &lt;= 0.11;\n+--------------+\n| (0.11&lt;=0.11) |\n+--------------+\n| true         |\n+--------------+\n</code></pre>"},{"location":"3.ngql-guide/5.operators/1.comparison/#_6","title":"<code>!=</code>","text":"<pre><code>nebula&gt; YIELD 1 != '1';\n+----------+\n| (1!=\"1\") |\n+----------+\n| true     |\n+----------+\n</code></pre>"},{"location":"3.ngql-guide/5.operators/1.comparison/#is_not_null","title":"<code>IS [NOT] NULL</code>","text":"<pre><code>nebula&gt; RETURN null IS NULL AS value1, null == null AS value2, null != null AS value3;\n+--------+----------+----------+\n| value1 | value2   | value3   |\n+--------+----------+----------+\n| true   | __NULL__ | __NULL__ |\n+--------+----------+----------+\n\nnebula&gt; RETURN length(NULL), size(NULL), count(NULL), NULL IS NULL, NULL IS NOT NULL, sin(NULL), NULL + NULL, [1, NULL] IS NULL;\n+--------------+------------+-------------+--------------+------------------+-----------+-------------+------------------+\n| length(NULL) | size(NULL) | count(NULL) | NULL IS NULL | NULL IS NOT NULL | sin(NULL) | (NULL+NULL) | [1,NULL] IS NULL |\n+--------------+------------+-------------+--------------+------------------+-----------+-------------+------------------+\n| __NULL__     | __NULL__   | 0           | true         | false            | __NULL__  | __NULL__    | false            |\n+--------------+------------+-------------+--------------+------------------+-----------+-------------+------------------+\n\nnebula&gt; WITH {name: null} AS `map` \\\n        RETURN `map`.name IS NOT NULL;\n+----------------------+\n| map.name IS NOT NULL |\n+----------------------+\n| false                |\n+----------------------+\n\nnebula&gt; WITH {name: 'Mats', name2: 'Pontus'} AS map1, \\\n        {name: null} AS map2, {notName: 0, notName2: null } AS map3 \\\n        RETURN map1.name IS NULL, map2.name IS NOT NULL, map3.name IS NULL;\n+-------------------+-----------------------+-------------------+\n| map1.name IS NULL | map2.name IS NOT NULL | map3.name IS NULL |\n+-------------------+-----------------------+-------------------+\n| false             | false                 | true              |\n+-------------------+-----------------------+-------------------+\n\nnebula&gt; MATCH (n:player) \\\n        RETURN n.player.age IS NULL, n.player.name IS NOT NULL, n.player.empty IS NULL;\n+----------------------+---------------------------+------------------------+\n| n.player.age IS NULL | n.player.name IS NOT NULL | n.player.empty IS NULL |\n+----------------------+---------------------------+------------------------+\n| false                | true                      | true                   |\n| false                | true                      | true                   |\n...\n</code></pre>"},{"location":"3.ngql-guide/5.operators/1.comparison/#is_not_empty","title":"<code>IS [NOT] EMPTY</code>","text":"<pre><code>nebula&gt; RETURN null IS EMPTY;\n+---------------+\n| NULL IS EMPTY |\n+---------------+\n| false         |\n+---------------+\n\nnebula&gt; RETURN \"a\" IS NOT EMPTY;\n+------------------+\n| \"a\" IS NOT EMPTY |\n+------------------+\n| true             |\n+------------------+\n\nnebula&gt; GO FROM \"player100\" OVER * WHERE properties($$).name IS NOT EMPTY YIELD dst(edge);\n+-------------+\n| dst(EDGE)   |\n+-------------+\n| \"team204\"   |\n| \"player101\" |\n| \"player125\" |\n+-------------+\n</code></pre>"},{"location":"3.ngql-guide/5.operators/10.arithmetic/","title":"Arithmetic operators","text":"<p>NebulaGraph supports the following arithmetic operators.</p> Name Description <code>+</code> Addition operator <code>-</code> Minus operator <code>*</code> Multiplication operator <code>/</code> Division operator <code>%</code> Modulo operator <code>-</code> Changes the sign of the argument"},{"location":"3.ngql-guide/5.operators/10.arithmetic/#examples","title":"Examples","text":"<pre><code>nebula&gt; RETURN 1+2 AS result;\n+--------+\n| result |\n+--------+\n| 3      |\n+--------+\n\nnebula&gt; RETURN -10+5 AS result;\n+--------+\n| result |\n+--------+\n| -5     |\n+--------+\n\nnebula&gt; RETURN (3*8)%5 AS result;\n+--------+\n| result |\n+--------+\n| 4      |\n+--------+\n</code></pre>"},{"location":"3.ngql-guide/5.operators/2.boolean/","title":"Boolean operators","text":"<p>NebulaGraph supports the following boolean operators.</p> Name Description AND Logical AND NOT Logical NOT OR Logical OR XOR Logical XOR <p>For the precedence of the operators, refer to Operator Precedence.</p> <p>For the logical operations with <code>NULL</code>, refer to NULL.</p>"},{"location":"3.ngql-guide/5.operators/2.boolean/#legacy_version_compatibility","title":"Legacy version compatibility","text":"<ul> <li>Non-zero numbers cannot be converted to boolean values.</li> </ul>"},{"location":"3.ngql-guide/5.operators/4.pipe/","title":"Pipe operators","text":"<p>Multiple queries can be combined using pipe operators in nGQL.</p>"},{"location":"3.ngql-guide/5.operators/4.pipe/#opencypher_compatibility","title":"OpenCypher compatibility","text":"<p>Pipe operators apply to native nGQL only.</p>"},{"location":"3.ngql-guide/5.operators/4.pipe/#syntax","title":"Syntax","text":"<p>One major difference between nGQL and SQL is how sub-queries are composed.</p> <ul> <li>In SQL, sub-queries are nested in the query statements.</li> </ul> <ul> <li>In nGQL, the shell style <code>PIPE (|)</code> is introduced into the sub-queries.</li> </ul>"},{"location":"3.ngql-guide/5.operators/4.pipe/#examples","title":"Examples","text":"<pre><code>nebula&gt; GO FROM \"player100\" OVER follow \\\n        YIELD dst(edge) AS dstid, properties($$).name AS Name | \\\n        GO FROM $-.dstid OVER follow YIELD dst(edge);\n\n+-------------+\n| dst(EDGE)   |\n+-------------+\n| \"player100\" |\n| \"player102\" |\n| \"player125\" |\n| \"player100\" |\n+-------------+\n</code></pre> <p>Users must define aliases in the <code>YIELD</code> clause for the reference operator <code>$-</code> to use, just like <code>$-.dstid</code> in the preceding example.</p>"},{"location":"3.ngql-guide/5.operators/4.pipe/#performance_tips","title":"Performance tips","text":"<p>In NebulaGraph, pipes will affect the performance. Take <code>A | B</code> as an example, the effects are as follows:</p> <ol> <li> <p>Pipe operators operate synchronously. That is, the data can enter the pipe clause as a whole after the execution of clause <code>A</code> before the pipe operator is completed.</p> </li> <li> <p>Pipe operators need to be serialized and deserialized, which is executed in a single thread.</p> </li> <li> <p>If <code>A</code> sends a large amount of data to <code>|</code>, the entire query request may be very slow. You can try to split this statement.</p> <ol> <li> <p>Send <code>A</code> from the application,</p> </li> <li> <p>Split the return results on the application,</p> </li> <li> <p>Send to multiple graphd processes concurrently,</p> </li> <li> <p>Every graphd process executes part of B.</p> </li> </ol> <p>This is usually much faster than executing a complete <code>A | B</code> with a single graphd process.</p> </li> </ol>"},{"location":"3.ngql-guide/5.operators/5.property-reference/","title":"Reference operators","text":"<p>NGQL provides reference operators to represent a property in a <code>WHERE</code> or <code>YIELD</code> clause, or the output of the statement before the pipe operator in a composite query.</p>"},{"location":"3.ngql-guide/5.operators/5.property-reference/#opencypher_compatibility","title":"OpenCypher compatibility","text":"<p>Reference operators apply to native nGQL only.</p>"},{"location":"3.ngql-guide/5.operators/5.property-reference/#reference_operator_list","title":"Reference operator List","text":"Reference operator Description <code>$^</code> Refers to a source vertex property. For more information, see Property reference. <code>$$</code> Refers to a destination vertex property. For more information, see Property reference. <code>$-</code> Refers to the output of the statement before the pipe operator in a composite query. For more information, see Pipe."},{"location":"3.ngql-guide/5.operators/5.property-reference/#examples","title":"Examples","text":"<pre><code># The following example returns the age of the source vertex and the destination vertex.\nnebula&gt; GO FROM \"player100\" OVER follow YIELD properties($^).age AS SrcAge, properties($$).age AS DestAge;\n+--------+---------+\n| SrcAge | DestAge |\n+--------+---------+\n| 42     | 36      |\n| 42     | 41      |\n+--------+---------+\n\n# The following example returns the name and team of the players that player100 follows.\nnebula&gt; GO FROM \"player100\" OVER follow \\\n        YIELD dst(edge) AS id | \\\n        GO FROM $-.id OVER serve \\\n        YIELD $^.player.name AS Player, properties($$).name AS Team;\n+-----------------+-----------+\n| Player          | Team      |\n+-----------------+-----------+\n| \"Tony Parker\"   | \"Spurs\"   |\n| \"Tony Parker\"   | \"Hornets\" |\n| \"Manu Ginobili\" | \"Spurs\"   |\n+-----------------+-----------+\n</code></pre>"},{"location":"3.ngql-guide/5.operators/6.set/","title":"Set operators","text":"<p>This topic will describe the set operators, including <code>UNION</code>, <code>UNION ALL</code>, <code>INTERSECT</code>, and <code>MINUS</code>. To combine multiple queries, use these set operators.</p> <p>All set operators have equal precedence. If a nGQL statement contains multiple set operators, NebulaGraph will evaluate them from left to right unless parentheses explicitly specify another order.</p> <p>Caution</p> <p>The names and order of the variables defined in the query statements before and after the set operator must be consistent. For example, the names and order of <code>a,b,c</code> in <code>RETURN a,b,c UNION RETURN a,b,c</code> need to be consistent.</p>"},{"location":"3.ngql-guide/5.operators/6.set/#union_union_distinct_and_union_all","title":"UNION, UNION DISTINCT, and UNION ALL","text":"<pre><code>&lt;left&gt; UNION [DISTINCT | ALL] &lt;right&gt; [ UNION [DISTINCT | ALL] &lt;right&gt; ...]\n</code></pre> <ul> <li>Operator <code>UNION DISTINCT</code> (or by short <code>UNION</code>) returns the union of two sets A and B without duplicated elements.</li> </ul> <ul> <li>Operator <code>UNION ALL</code> returns the union of two sets A and B with duplicated elements.</li> </ul> <ul> <li>The <code>&lt;left&gt;</code> and <code>&lt;right&gt;</code> must have the same number of columns and data types. Different data types are converted according to the Type Conversion.</li> </ul>"},{"location":"3.ngql-guide/5.operators/6.set/#examples","title":"Examples","text":"<pre><code># The following statement returns the union of two query results without duplicated elements.\nnebula&gt; GO FROM \"player102\" OVER follow YIELD dst(edge) \\\n        UNION \\\n        GO FROM \"player100\" OVER follow YIELD dst(edge);\n+-------------+\n| dst(EDGE)   |\n+-------------+\n| \"player100\" |\n| \"player101\" |\n| \"player125\" |\n+-------------+\n\nnebula&gt; MATCH (v:player) \\\n        WITH v.player.name AS v \\\n        RETURN n ORDER BY n LIMIT 3 \\\n        UNION \\\n        UNWIND [\"Tony Parker\", \"Ben Simmons\"] AS n \\\n        RETURN n;\n+---------------------+\n| n                   |\n+---------------------+\n| \"Amar'e Stoudemire\" |\n| \"Aron Baynes\"       |\n| \"Ben Simmons\"       |\n| \"Tony Parker\"       |\n+---------------------+\n\n# The following statement returns the union of two query results with duplicated elements.\nnebula&gt; GO FROM \"player102\" OVER follow YIELD dst(edge) \\\n        UNION ALL \\\n        GO FROM \"player100\" OVER follow YIELD dst(edge);\n+-------------+\n| dst(EDGE)   |\n+-------------+\n| \"player100\" |\n| \"player101\" |\n| \"player101\" |\n| \"player125\" |\n+-------------+\n\nnebula&gt; MATCH (v:player) \\\n        WITH v.player.name AS n \\\n        RETURN n ORDER BY n LIMIT 3 \\\n        UNION ALL \\\n        UNWIND [\"Tony Parker\", \"Ben Simmons\"] AS n \\\n        RETURN n;\n+---------------------+\n| n                   |\n+---------------------+\n| \"Amar'e Stoudemire\" |\n| \"Aron Baynes\"       |\n| \"Ben Simmons\"       |\n| \"Tony Parker\"       |\n| \"Ben Simmons\"       |\n+---------------------+\n\n# UNION can also work with the YIELD statement. The DISTINCT keyword will check duplication by all the columns for every line, and remove duplicated lines if every column is the same.\nnebula&gt; GO FROM \"player102\" OVER follow \\\n        YIELD dst(edge) AS id, properties(edge).degree AS Degree, properties($$).age AS Age \\\n        UNION /* DISTINCT */ \\\n        GO FROM \"player100\" OVER follow \\\n        YIELD dst(edge) AS id, properties(edge).degree AS Degree, properties($$).age AS Age;\n+-------------+--------+-----+\n| id          | Degree | Age |\n+-------------+--------+-----+\n| \"player100\" | 75     | 42  |\n| \"player101\" | 75     | 36  |\n| \"player101\" | 95     | 36  |\n| \"player125\" | 95     | 41  |\n+-------------+--------+-----+\n</code></pre>"},{"location":"3.ngql-guide/5.operators/6.set/#intersect","title":"INTERSECT","text":"<pre><code>&lt;left&gt; INTERSECT &lt;right&gt;\n</code></pre> <ul> <li>Operator <code>INTERSECT</code> returns the intersection of two sets A and B (denoted by A \u22c2 B).</li> </ul> <ul> <li>Similar to <code>UNION</code>, the <code>left</code> and <code>right</code> must have the same number of columns and data types. Different data types are converted according to the Type Conversion.</li> </ul>"},{"location":"3.ngql-guide/5.operators/6.set/#example","title":"Example","text":"<pre><code># The following statement returns the intersection of two query results.\nnebula&gt; GO FROM \"player102\" OVER follow \\\n        YIELD dst(edge) AS id, properties(edge).degree AS Degree, properties($$).age AS Age \\\n        INTERSECT \\\n        GO FROM \"player100\" OVER follow \\\n        YIELD dst(edge) AS id, properties(edge).degree AS Degree, properties($$).age AS Age;\n+----+--------+-----+\n| id | Degree | Age |\n+----+--------+-----+\n+----+--------+-----+\n\nnebula&gt; MATCH (v:player)-[e:follow]-&gt;(v2) \\\n        WHERE id(v) == \"player102\" \\\n        RETURN id(v2) As id, e.degree As Degree, v2.player.age AS Age \\\n        INTERSECT \\\n        MATCH (v:player)-[e:follow]-&gt;(v2) \\\n        WHERE id(v) == \"player100\" \\\n        RETURN id(v2) As id, e.degree As Degree, v2.player.age AS Age;\n+----+--------+-----+\n| id | Degree | Age |\n+----+--------+-----+\n+----+--------+-----+\n\nnebula&gt; UNWIND [1,2] AS a RETURN a \\\n        INTERSECT \\\n        UNWIND [1,2,3,4] AS a \\\n        RETURN a;\n+---+\n| a |\n+---+\n| 1 |\n| 2 |\n+---+\n</code></pre>"},{"location":"3.ngql-guide/5.operators/6.set/#minus","title":"MINUS","text":"<pre><code>&lt;left&gt; MINUS &lt;right&gt;\n</code></pre> <p>Operator <code>MINUS</code> returns the subtraction (or difference) of two sets A and B (denoted by <code>A-B</code>). Always pay attention to the order of <code>left</code> and <code>right</code>. The set <code>A-B</code> consists of elements that are in A but not in B.</p>"},{"location":"3.ngql-guide/5.operators/6.set/#example_1","title":"Example","text":"<pre><code># The following statement returns the elements in the first query result but not in the second query result.\nnebula&gt; GO FROM \"player100\" OVER follow YIELD dst(edge) \\\n        MINUS \\\n        GO FROM \"player102\" OVER follow YIELD dst(edge);\n+-------------+\n| dst(EDGE)   |\n+-------------+\n| \"player125\" |\n+-------------+\n\nnebula&gt; GO FROM \"player102\" OVER follow YIELD dst(edge) AS id\\\n        MINUS \\\n        GO FROM \"player100\" OVER follow YIELD dst(edge) AS id;\n+-------------+\n| id          |\n+-------------+\n| \"player100\" |\n+-------------+\n\nnebula&gt; MATCH (v:player)-[e:follow]-&gt;(v2) \\\n        WHERE id(v) ==\"player102\" \\\n        RETURN id(v2) AS id\\\n        MINUS \\\n        MATCH (v:player)-[e:follow]-&gt;(v2) \\\n        WHERE id(v) ==\"player100\" \\\n        RETURN id(v2) AS id;\n+-------------+\n| id          |\n+-------------+\n| \"player100\" |\n+-------------+\n\nnebula&gt; UNWIND [1,2,3] AS a RETURN a \\\n        MINUS \\\n        WITH 4 AS a \\\n        RETURN a;\n+---+\n| a |\n+---+\n| 1 |\n| 2 |\n| 3 |\n+---+\n</code></pre>"},{"location":"3.ngql-guide/5.operators/6.set/#precedence_of_the_set_operators_and_pipe_operators","title":"Precedence of the set operators and pipe operators","text":"<p>Please note that when a query contains a pipe <code>|</code> and a set operator, the pipe takes precedence. Refer to Pipe for details. The query <code>GO FROM 1 UNION GO FROM 2 | GO FROM 3</code> is the same as the query <code>GO FROM 1 UNION (GO FROM 2 | GO FROM 3)</code>.</p>"},{"location":"3.ngql-guide/5.operators/6.set/#examples_1","title":"Examples","text":"<pre><code>nebula&gt; GO FROM \"player102\" OVER follow \\\n        YIELD dst(edge) AS play_dst  \\\n        UNION \\\n        GO FROM \"team200\" OVER serve REVERSELY \\\n        YIELD src(edge) AS play_src \\\n        | GO FROM $-.play_src OVER follow YIELD dst(edge) AS play_dst;\n\n+-------------+\n| play_dst    |\n+-------------+\n| \"player100\" |\n| \"player101\" |\n| \"player117\" |\n| \"player105\" |\n+-------------+\n</code></pre> <p>The above query executes the statements in the red bar first and then executes the statement in the green box.</p> <p>The parentheses can change the execution priority. For example:</p> <pre><code>nebula&gt; (GO FROM \"player102\" OVER follow \\\n        YIELD dst(edge) AS play_dst  \\\n        UNION \\\n        GO FROM \"team200\" OVER serve REVERSELY \\\n        YIELD src(edge) AS play_dst) \\\n        | GO FROM $-.play_dst OVER follow YIELD dst(edge) AS play_dst;\n</code></pre> <p>In the above query, the statements within the parentheses take precedence. That is, the <code>UNION</code> operation will be executed first, and its output will be executed as the input of the next operation with pipes.</p>"},{"location":"3.ngql-guide/5.operators/7.string/","title":"String operators","text":"<p>You can use the following string operators for concatenating, querying, and matching.</p> Name Description + Concatenates strings. CONTAINS Performs searchings in strings. (NOT) IN Checks whether a value is within a set of values. (NOT) STARTS WITH Performs matchings at the beginning of a string. (NOT) ENDS WITH Performs matchings at the end of a string. Regular expressions Perform string matchings using regular expressions. <p>Note</p> <p>All the string searchings or matchings are case-sensitive.</p>"},{"location":"3.ngql-guide/5.operators/7.string/#examples","title":"Examples","text":""},{"location":"3.ngql-guide/5.operators/7.string/#_1","title":"<code>+</code>","text":"<pre><code>nebula&gt; RETURN 'a' + 'b';\n+-----------+\n| (\"a\"+\"b\") |\n+-----------+\n| \"ab\"      |\n+-----------+\nnebula&gt; UNWIND 'a' AS a UNWIND 'b' AS b RETURN a + b;\n+-------+\n| (a+b) |\n+-------+\n| \"ab\"  |\n+-------+\n</code></pre>"},{"location":"3.ngql-guide/5.operators/7.string/#contains","title":"<code>CONTAINS</code>","text":"<p>The <code>CONTAINS</code> operator requires string types on both left and right sides.</p> <pre><code>nebula&gt; MATCH (s:player)-[e:serve]-&gt;(t:team) WHERE id(s) == \"player101\" \\\n        AND t.team.name CONTAINS \"ets\" RETURN s.player.name, e.start_year, e.end_year, t.team.name;\n+---------------+--------------+------------+-------------+\n| s.player.name | e.start_year | e.end_year | t.team.name |\n+---------------+--------------+------------+-------------+\n| \"Tony Parker\" | 2018         | 2019       | \"Hornets\"   |\n+---------------+--------------+------------+-------------+\n\nnebula&gt; GO FROM \"player101\" OVER serve WHERE (STRING)properties(edge).start_year CONTAINS \"19\" AND \\\n        properties($^).name CONTAINS \"ny\" \\\n        YIELD properties($^).name, properties(edge).start_year, properties(edge).end_year, properties($$).name;\n+---------------------+-----------------------------+---------------------------+---------------------+\n| properties($^).name | properties(EDGE).start_year | properties(EDGE).end_year | properties($$).name |\n+---------------------+-----------------------------+---------------------------+---------------------+\n| \"Tony Parker\"       | 1999                        | 2018                      | \"Spurs\"             |\n+---------------------+-----------------------------+---------------------------+---------------------+\n\nnebula&gt; GO FROM \"player101\" OVER serve WHERE !(properties($$).name CONTAINS \"ets\") \\\n        YIELD properties($^).name, properties(edge).start_year, properties(edge).end_year, properties($$).name;\n+---------------------+-----------------------------+---------------------------+---------------------+\n| properties($^).name | properties(EDGE).start_year | properties(EDGE).end_year | properties($$).name |\n+---------------------+-----------------------------+---------------------------+---------------------+\n| \"Tony Parker\"       | 1999                        | 2018                      | \"Spurs\"             |\n+---------------------+-----------------------------+---------------------------+---------------------+\n</code></pre>"},{"location":"3.ngql-guide/5.operators/7.string/#not_in","title":"<code>(NOT) IN</code>","text":"<pre><code>nebula&gt; RETURN  1 IN [1,2,3], \"Yao\" NOT IN [\"Yi\", \"Tim\", \"Kobe\"], NULL IN [\"Yi\", \"Tim\", \"Kobe\"];\n+----------------+------------------------------------+-------------------------------+\n| (1 IN [1,2,3]) | (\"Yao\" NOT IN [\"Yi\",\"Tim\",\"Kobe\"]) | (NULL IN [\"Yi\",\"Tim\",\"Kobe\"]) |\n+----------------+------------------------------------+-------------------------------+\n| true           | true                               | __NULL__                      |\n+----------------+------------------------------------+-------------------------------+\n</code></pre>"},{"location":"3.ngql-guide/5.operators/7.string/#not_starts_with","title":"<code>(NOT) STARTS WITH</code>","text":"<pre><code>nebula&gt; RETURN 'apple' STARTS WITH 'app', 'apple' STARTS WITH 'a', 'apple' STARTS WITH toUpper('a');\n+-----------------------------+---------------------------+------------------------------------+\n| (\"apple\" STARTS WITH \"app\") | (\"apple\" STARTS WITH \"a\") | (\"apple\" STARTS WITH toUpper(\"a\")) |\n+-----------------------------+---------------------------+------------------------------------+\n| true                        | true                      | false                              |\n+-----------------------------+---------------------------+------------------------------------+\n\nnebula&gt; RETURN 'apple' STARTS WITH 'b','apple' NOT STARTS WITH 'app';\n+---------------------------+---------------------------------+\n| (\"apple\" STARTS WITH \"b\") | (\"apple\" NOT STARTS WITH \"app\") |\n+---------------------------+---------------------------------+\n| false                     | false                           |\n+---------------------------+---------------------------------+\n</code></pre>"},{"location":"3.ngql-guide/5.operators/7.string/#not_ends_with","title":"<code>(NOT) ENDS WITH</code>","text":"<pre><code>nebula&gt; RETURN 'apple' ENDS WITH 'app', 'apple' ENDS WITH 'e', 'apple' ENDS WITH 'E', 'apple' ENDS WITH 'b';\n+---------------------------+-------------------------+-------------------------+-------------------------+\n| (\"apple\" ENDS WITH \"app\") | (\"apple\" ENDS WITH \"e\") | (\"apple\" ENDS WITH \"E\") | (\"apple\" ENDS WITH \"b\") |\n+---------------------------+-------------------------+-------------------------+-------------------------+\n| false                     | true                    | false                   | false                   |\n+---------------------------+-------------------------+-------------------------+-------------------------+\n</code></pre>"},{"location":"3.ngql-guide/5.operators/7.string/#regular_expressions","title":"Regular expressions","text":"<p>Note</p> <p>Regular expressions cannot work with native nGQL statements (<code>GO</code>, <code>FETCH</code>, <code>LOOKUP</code>, etc.). Use it in openCypher only (<code>MATCH</code>, <code>WHERE</code>, etc.).</p> <p>NebulaGraph supports filtering by using regular expressions. The regular expression syntax is inherited from <code>std::regex</code>. You can match on regular expressions by using <code>=~ 'regexp'</code>. For example:</p> <pre><code>nebula&gt; RETURN \"384748.39\" =~ \"\\\\d+(\\\\.\\\\d{2})?\";\n+--------------------------------+\n| (\"384748.39\"=~\"\\d+(\\.\\d{2})?\") |\n+--------------------------------+\n| true                           |\n+--------------------------------+\n\nnebula&gt; MATCH (v:player) WHERE v.player.name =~ 'Tony.*' RETURN v.player.name;\n+---------------+\n| v.player.name |\n+---------------+\n| \"Tony Parker\" |\n+---------------+\n</code></pre>"},{"location":"3.ngql-guide/5.operators/8.list/","title":"List operators","text":"<p>NebulaGraph supports the following list operators:</p> List operator Description + Concatenates lists. IN Checks if an element exists in a list. [] Accesses an element(s) in a list using the index operator."},{"location":"3.ngql-guide/5.operators/8.list/#examples","title":"Examples","text":"<pre><code>nebula&gt; YIELD [1,2,3,4,5]+[6,7] AS myList;\n+-----------------------+\n| myList                |\n+-----------------------+\n| [1, 2, 3, 4, 5, 6, 7] |\n+-----------------------+\n\nnebula&gt; RETURN size([NULL, 1, 2]);\n+------------------+\n| size([NULL,1,2]) |\n+------------------+\n| 3                |\n+------------------+\n\nnebula&gt; RETURN NULL IN [NULL, 1];\n+--------------------+\n| (NULL IN [NULL,1]) |\n+--------------------+\n| __NULL__           |\n+--------------------+\n\nnebula&gt; WITH [2, 3, 4, 5] AS numberlist \\\n    UNWIND numberlist AS number \\\n    WITH number \\\n    WHERE number IN [2, 3, 8] \\\n    RETURN number;\n+--------+\n| number |\n+--------+\n| 2      |\n| 3      |\n+--------+\n\nnebula&gt; WITH ['Anne', 'John', 'Bill', 'Diane', 'Eve'] AS names RETURN names[1] AS result;\n+--------+\n| result |\n+--------+\n| \"John\" |\n+--------+\n</code></pre>"},{"location":"3.ngql-guide/5.operators/9.precedence/","title":"Operator precedence","text":"<p>The following list shows the precedence of nGQL operators in descending order. Operators that are shown together on a line have the same precedence.</p> <ul> <li><code>-</code> (negative number)</li> <li><code>!</code>, <code>NOT</code></li> <li><code>*</code>, <code>/</code>, <code>%</code></li> <li><code>-</code>, <code>+</code></li> <li><code>==</code>, <code>&gt;=</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&lt;</code>, <code>&lt;&gt;</code>, <code>!=</code></li> <li><code>AND</code></li> <li><code>OR</code>, <code>XOR</code></li> <li><code>=</code> (assignment)</li> </ul> <p>For operators that occur at the same precedence level within an expression, evaluation proceeds left to right, with the exception that assignments evaluate right to left.</p> <p>The precedence of operators determines the order of evaluation of terms in an expression. To modify this order and group terms explicitly, use parentheses.</p>"},{"location":"3.ngql-guide/5.operators/9.precedence/#examples","title":"Examples","text":"<pre><code>nebula&gt; RETURN 2+3*5;\n+-----------+\n| (2+(3*5)) |\n+-----------+\n| 17        |\n+-----------+\n\nnebula&gt; RETURN (2+3)*5;\n+-----------+\n| ((2+3)*5) |\n+-----------+\n| 25        |\n+-----------+\n</code></pre>"},{"location":"3.ngql-guide/5.operators/9.precedence/#opencypher_compatibility","title":"OpenCypher compatibility","text":"<p>In openCypher, comparisons can be chained arbitrarily, e.g., <code>x &lt; y &lt;= z</code> is equivalent to <code>x &lt; y AND y &lt;= z</code> in openCypher.</p> <p>But in nGQL, <code>x &lt; y &lt;= z</code> is equivalent to <code>(x &lt; y) &lt;= z</code>. The result of <code>(x &lt; y)</code> is a boolean. Compare it with an integer <code>z</code>, and you will get the final result <code>NULL</code>.</p>"},{"location":"3.ngql-guide/6.functions-and-expressions/1.math/","title":"Built-in math functions","text":"<p>This topic describes the built-in math functions supported by NebulaGraph.</p>"},{"location":"3.ngql-guide/6.functions-and-expressions/1.math/#abs","title":"abs()","text":"<p>abs() returns the absolute value of the argument.</p> <p>Syntax: <code>abs(&lt;expression&gt;)</code></p> <ul> <li><code>expression</code>: An expression of which the result type is double.</li> </ul> <ul> <li>Result type: Double</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN abs(-10);\n+------------+\n| abs(-(10)) |\n+------------+\n| 10         |\n+------------+\nnebula&gt; RETURN abs(5-6);\n+------------+\n| abs((5-6)) |\n+------------+\n| 1          |\n+------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/1.math/#floor","title":"floor()","text":"<p>floor() returns the largest integer value smaller than or equal to the argument.(Rounds down)</p> <p>Syntax: <code>floor(&lt;expression&gt;)</code></p> <ul> <li><code>expression</code>: An expression of which the result type is double.</li> </ul> <ul> <li>Result type: Double</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN floor(9.9);\n+------------+\n| floor(9.9) |\n+------------+\n| 9.0        |\n+------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/1.math/#ceil","title":"ceil()","text":"<p>ceil() returns the smallest integer greater than or equal to the argument.(Rounds up)</p> <p>Syntax: <code>ceil(&lt;expression&gt;)</code></p> <ul> <li><code>expression</code>: An expression of which the result type is double.</li> </ul> <ul> <li>Result type: Double</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN ceil(9.1);\n+-----------+\n| ceil(9.1) |\n+-----------+\n| 10.0      |\n+-----------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/1.math/#round","title":"round()","text":"<p>round() returns the rounded value of the specified number. Pay attention to the floating-point precision when using this function.</p> <p>Syntax: <code>round(&lt;expression&gt;, &lt;digit&gt;)</code></p> <ul> <li><code>expression</code>: An expression of which the result type is double.</li> </ul> <ul> <li><code>digit</code>: Decimal digits. If <code>digit</code> is less than 0, round at the left of the decimal point.</li> </ul> <ul> <li>Result type: Double</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN round(314.15926, 2);\n+--------------------+\n| round(314.15926,2) |\n+--------------------+\n| 314.16             |\n+--------------------+\nnebula&gt; RETURN round(314.15926, -1);\n+-----------------------+\n| round(314.15926,-(1)) |\n+-----------------------+\n| 310.0                 |\n+-----------------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/1.math/#sqrt","title":"sqrt()","text":"<p>sqrt() returns the square root of the argument.</p> <p>Syntax: <code>sqrt(&lt;expression&gt;)</code></p> <ul> <li><code>expression</code>: An expression of which the result type is double.</li> </ul> <ul> <li>Result type: Double</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN sqrt(9);\n+---------+\n| sqrt(9) |\n+---------+\n| 3.0     |\n+---------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/1.math/#cbrt","title":"cbrt()","text":"<p>cbrt() returns the cubic root of the argument.</p> <p>Syntax: <code>cbrt(&lt;expression&gt;)</code></p> <ul> <li><code>expression</code>: An expression of which the result type is double.</li> </ul> <ul> <li>Result type: Double</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN cbrt(8);\n+---------+\n| cbrt(8) |\n+---------+\n| 2.0     |\n+---------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/1.math/#hypot","title":"hypot()","text":"<p>hypot() returns the hypotenuse of a right-angled triangle.</p> <p>Syntax: <code>hypot(&lt;expression_x&gt;,&lt;expression_y&gt;)</code></p> <ul> <li><code>expression_x</code>, <code>expression_y</code>: An expression of which the result type is double. They represent the side lengths x and y of a right triangle.</li> </ul> <ul> <li>Result type: Double</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN hypot(3,2*2);\n+----------------+\n| hypot(3,(2*2)) |\n+----------------+\n| 5.0            |\n+----------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/1.math/#pow","title":"pow()","text":"<p>pow() returns the result of x<sup>y</sup>.</p> <p>Syntax: <code>pow(&lt;expression_x&gt;,&lt;expression_y&gt;,)</code></p> <ul> <li><code>expression_x</code>: An expression of which the result type is double. It represents the base <code>x</code>.</li> </ul> <ul> <li><code>expression_y</code>: An expression of which the result type is double. It represents the exponential <code>y</code>.</li> </ul> <ul> <li>Result type: Double</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN pow(3,3);\n+----------+\n| pow(3,3) |\n+----------+\n| 27       |\n+----------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/1.math/#exp","title":"exp()","text":"<p>exp() returns the result of e<sup>x</sup>.</p> <p>Syntax: <code>exp(&lt;expression&gt;)</code></p> <ul> <li><code>expression</code>: An expression of which the result type is double. It represents the exponential <code>x</code>.</li> </ul> <ul> <li>Result type: Double</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN exp(2);\n+------------------+\n| exp(2)           |\n+------------------+\n| 7.38905609893065 |\n+------------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/1.math/#exp2","title":"exp2()","text":"<p>exp2() returns the result of 2<sup>x</sup>.</p> <p>Syntax: <code>exp2(&lt;expression&gt;)</code></p> <ul> <li><code>expression</code>: An expression of which the result type is double. It represents the exponential <code>x</code>.</li> </ul> <ul> <li>Result type: Double</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN exp2(3);\n+---------+\n| exp2(3) |\n+---------+\n| 8.0     |\n+---------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/1.math/#log","title":"log()","text":"<p>log() returns the base-e logarithm of the argument. (\\(log_{e}{N}\\))</p> <p>Syntax: <code>log(&lt;expression&gt;)</code></p> <ul> <li><code>expression</code>: An expression of which the result type is double. It represents the antilogarithm <code>N</code>.</li> </ul> <ul> <li>Result type: Double</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN log(8);\n+--------------------+\n| log(8)             |\n+--------------------+\n| 2.0794415416798357 |\n+--------------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/1.math/#log2","title":"log2()","text":"<p>log2() returns the base-2 logarithm of the argument. (\\(log_{2}{N}\\))</p> <p>Syntax: <code>log2(&lt;expression&gt;)</code></p> <ul> <li><code>expression</code>: An expression of which the result type is double. It represents the antilogarithm <code>N</code>.</li> </ul> <ul> <li>Result type: Double</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN log2(8);\n+---------+\n| log2(8) |\n+---------+\n| 3.0     |\n+---------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/1.math/#log10","title":"log10()","text":"<p>log10() returns the base-10 logarithm of the argument. (\\(log_{10}{N}\\))</p> <p>Syntax: <code>log10(&lt;expression&gt;)</code></p> <ul> <li><code>expression</code>: An expression of which the result type is double. It represents the antilogarithm <code>N</code>.</li> </ul> <ul> <li>Result type: Double</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN log10(100);\n+------------+\n| log10(100) |\n+------------+\n| 2.0        |\n+------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/1.math/#sin","title":"sin()","text":"<p>sin() returns the sine of the argument. Users can convert angles to radians using the function <code>radians()</code>.</p> <p>Syntax: <code>sin(&lt;expression&gt;)</code></p> <ul> <li><code>expression</code>: An expression of which the result type is double.</li> </ul> <ul> <li>Result type: Double</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN sin(3);\n+--------------------+\n| sin(3)             |\n+--------------------+\n| 0.1411200080598672 |\n+--------------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/1.math/#asin","title":"asin()","text":"<p>asin() returns the inverse sine of the argument. Users can convert angles to radians using the function <code>radians()</code>.</p> <p>Syntax: <code>asin(&lt;expression&gt;)</code></p> <ul> <li><code>expression</code>: An expression of which the result type is double.</li> </ul> <ul> <li>Result type: Double</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN asin(0.5);\n+--------------------+\n| asin(0.5)          |\n+--------------------+\n| 0.5235987755982989 |\n+--------------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/1.math/#cos","title":"cos()","text":"<p>cos() returns the cosine of the argument. Users can convert angles to radians using the function <code>radians()</code>.</p> <p>Syntax: <code>cos(&lt;expression&gt;)</code></p> <ul> <li><code>expression</code>: An expression of which the result type is double.</li> </ul> <ul> <li>Result type: Double</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN cos(0.5);\n+--------------------+\n| cos(0.5)           |\n+--------------------+\n| 0.8775825618903728 |\n+--------------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/1.math/#acos","title":"acos()","text":"<p>acos() returns the inverse cosine of the argument. Users can convert angles to radians using the function <code>radians()</code>.</p> <p>Syntax: <code>acos(&lt;expression&gt;)</code></p> <ul> <li><code>expression</code>: An expression of which the result type is double.</li> </ul> <ul> <li>Result type: Double</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN acos(0.5);\n+--------------------+\n| acos(0.5)          |\n+--------------------+\n| 1.0471975511965979 |\n+--------------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/1.math/#tan","title":"tan()","text":"<p>tan() returns the tangent of the argument. Users can convert angles to radians using the function <code>radians()</code>.</p> <p>Syntax: <code>tan(&lt;expression&gt;)</code></p> <ul> <li><code>expression</code>: An expression of which the result type is double.</li> </ul> <ul> <li>Result type: Double</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN tan(0.5);\n+--------------------+\n| tan(0.5)           |\n+--------------------+\n| 0.5463024898437905 |\n+--------------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/1.math/#atan","title":"atan()","text":"<p>atan() returns the inverse tangent of the argument. Users can convert angles to radians using the function <code>radians()</code>.</p> <p>Syntax: <code>atan(&lt;expression&gt;)</code></p> <ul> <li><code>expression</code>: An expression of which the result type is double.</li> </ul> <ul> <li>Result type: Double</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN atan(0.5);\n+--------------------+\n| atan(0.5)          |\n+--------------------+\n| 0.4636476090008061 |\n+--------------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/1.math/#rand","title":"rand()","text":"<p>rand() returns a random floating point number in the range from 0 (inclusive) to 1 (exclusive); i.e.[0,1).</p> <p>Syntax: <code>rand()</code></p> <ul> <li>Result type: Double</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN rand();\n+--------------------+\n| rand()             |\n+--------------------+\n| 0.6545837172298736 |\n+--------------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/1.math/#rand32","title":"rand32()","text":"<p>rand32() returns a random 32-bit integer in <code>[min, max)</code>.</p> <p>Syntax: <code>rand32(&lt;expression_min&gt;,&lt;expression_max&gt;)</code></p> <ul> <li><code>expression_min</code>: An expression of which the result type is int. It represents the minimum <code>min</code>.</li> </ul> <ul> <li><code>expression_max</code>: An expression of which the result type is int. It represents the maximum <code>max</code>.</li> </ul> <ul> <li>Result type: Int</li> </ul> <ul> <li>If you set only one argument, it is parsed as <code>max</code> and <code>min</code> is <code>0</code> by default. If you set no argument, the system returns a random signed 32-bit integer.</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN rand32(1,100);\n+---------------+\n| rand32(1,100) |\n+---------------+\n| 63            |\n+---------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/1.math/#rand64","title":"rand64()","text":"<p>rand64() returns a random 64-bit integer in <code>[min, max)</code>.</p> <p>Syntax: <code>rand64(&lt;expression_min&gt;,&lt;expression_max&gt;)</code></p> <ul> <li><code>expression_min</code>: An expression of which the result type is int. It represents the minimum <code>min</code>.</li> </ul> <ul> <li><code>expression_max</code>: An expression of which the result type is int. It represents the maximum <code>max</code>.</li> </ul> <ul> <li>Result type: Int</li> </ul> <ul> <li>If you set only one argument, it is parsed as <code>max</code> and <code>min</code> is <code>0</code> by default. If you set no argument, the system returns a random signed 64-bit integer.</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN rand64(1,100);\n+---------------+\n| rand64(1,100) |\n+---------------+\n| 34            |\n+---------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/1.math/#bit_and","title":"bit_and()","text":"<p>bit_and() returns the result of bitwise AND.</p> <p>Syntax: <code>bit_and(&lt;expression_1&gt;,&lt;expression_2&gt;)</code></p> <ul> <li><code>expression_1</code>, <code>expression_2</code>: An expression of which the result type is int.</li> </ul> <ul> <li>Result type: Int</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN bit_and(5,6);\n+--------------+\n| bit_and(5,6) |\n+--------------+\n| 4            |\n+--------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/1.math/#bit_or","title":"bit_or()","text":"<p>bit_or() returns the result of bitwise OR.</p> <p>Syntax: <code>bit_or(&lt;expression_1&gt;,&lt;expression_2&gt;)</code></p> <ul> <li><code>expression_1</code>, <code>expression_2</code>: An expression of which the result type is int.</li> </ul> <ul> <li>Result type: Int</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN bit_or(5,6);\n+-------------+\n| bit_or(5,6) |\n+-------------+\n| 7           |\n+-------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/1.math/#bit_xor","title":"bit_xor()","text":"<p>bit_xor() returns the result of bitwise XOR.</p> <p>Syntax: <code>bit_xor(&lt;expression_1&gt;,&lt;expression_2&gt;)</code></p> <ul> <li><code>expression_1</code>, <code>expression_2</code>: An expression of which the result type is int.</li> </ul> <ul> <li>Result type: Int</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN bit_xor(5,6);\n+--------------+\n| bit_xor(5,6) |\n+--------------+\n| 3            |\n+--------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/1.math/#size","title":"size()","text":"<p>size() returns the number of elements in a list or a map, or the length of a string.</p> <p>Syntax: <code>size({&lt;expression&gt;|&lt;string&gt;})</code></p> <ul> <li><code>expression</code>: An expression for a list or map.</li> <li><code>string</code>: A specified string.</li> </ul> <ul> <li>Result type: Int</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN size([1,2,3,4]);\n+-----------------+\n| size([1,2,3,4]) |\n+-----------------+\n| 4               |\n+-----------------+\n</code></pre> <pre><code>nebula&gt; RETURN size(\"basketballplayer\") as size;\n+------+\n| size |\n+------+\n| 16   |\n+------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/1.math/#range","title":"range()","text":"<p>range() returns a list of integers from <code>[start,end]</code> in the specified steps.</p> <p>Syntax: <code>range(&lt;expression_start&gt;,&lt;expression_end&gt;[,&lt;expression_step&gt;])</code></p> <ul> <li><code>expression_start</code>: An expression of which the result type is int. It represents the starting value <code>start</code>.</li> </ul> <ul> <li><code>expression_end</code>: An expression of which the result type is int. It represents the end value <code>end</code>.</li> </ul> <ul> <li><code>expression_step</code>: An expression of which the result type is int. It represents the step size <code>step</code>, <code>step</code> is 1 by default.</li> </ul> <ul> <li>Result type: List</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN range(1,3*3,2);\n+------------------+\n| range(1,(3*3),2) |\n+------------------+\n| [1, 3, 5, 7, 9]  |\n+------------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/1.math/#sign","title":"sign()","text":"<p>sign() returns the signum of the given number. If the number is <code>0</code>, the system returns <code>0</code>. If the number is negative, the system returns <code>-1</code>. If the number is positive, the system returns <code>1</code>.</p> <p>Syntax: <code>sign(&lt;expression&gt;)</code></p> <ul> <li><code>expression</code>: An expression of which the result type is double.</li> </ul> <ul> <li>Result type: Int</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN sign(10);\n+----------+\n| sign(10) |\n+----------+\n| 1        |\n+----------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/1.math/#e","title":"e()","text":"<p>e() returns the base of the natural logarithm, e (2.718281828459045).</p> <p>Syntax: <code>e()</code></p> <ul> <li>Result type: Double</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN e();\n+-------------------+\n| e()               |\n+-------------------+\n| 2.718281828459045 |\n+-------------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/1.math/#pi","title":"pi()","text":"<p>pi() returns the mathematical constant pi (3.141592653589793).</p> <p>Syntax: <code>pi()</code></p> <ul> <li>Result type: Double</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN pi();\n+-------------------+\n| pi()              |\n+-------------------+\n| 3.141592653589793 |\n+-------------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/1.math/#radians","title":"radians()","text":"<p>radians() converts angles to radians.</p> <p>Syntax: <code>radians(&lt;angle&gt;)</code></p> <ul> <li>Result type: Double</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN radians(180);\n+-------------------+\n| radians(180)      |\n+-------------------+\n| 3.141592653589793 |\n+-------------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/14.geo/","title":"Geography functions","text":"<p>Geography functions are used to generate or perform operations on the value of the geography data type.</p> <p>For descriptions of the geography data types, see Geography.</p>"},{"location":"3.ngql-guide/6.functions-and-expressions/14.geo/#descriptions","title":"Descriptions","text":"Function Return Type Description ST_Point(longitude, latitude) <code>GEOGRAPHY</code> Creates the geography that contains a point. ST_GeogFromText(wkt_string) <code>GEOGRAPHY</code> Returns the geography corresponding to the input WKT string. ST_ASText(geography) <code>STRING</code> Returns the WKT string of the input geography. ST_Centroid(geography) <code>GEOGRAPHY</code> Returns the centroid of the input geography in the form of the single point geography. ST_ISValid(geography) <code>BOOL</code> Returns whether the input geography is valid. ST_Intersects(geography_1, geography_2) <code>BOOL</code> Returns whether geography_1 and geography_2 have intersections. ST_Covers(geography_1, geography_2) <code>BOOL</code> Returns whether geography_1 completely contains geography_2. If there is no point outside geography_1 in geography_2, return True. ST_CoveredBy(geography_1, geography_2) <code>BOOL</code> Returns whether geography_2 completely contains geography_1.If there is no point outside geography_2 in geography_1, return True. ST_DWithin(geography_1, geography_2, distance) <code>BOOL</code> If the distance between one point (at least) in geography_1 and one point in geography_2 is less than or equal to the distance specified by the distance parameter (measured by meters), return True. ST_Distance(geography_1, geography_2) <code>FLOAT</code> Returns the smallest possible distance (measured by meters) between two non-empty geographies. S2_CellIdFromPoint(point_geography) <code>INT</code> Returns the S2 Cell ID that covers the point geography. S2_CoveringCellIds(geography) <code>ARRAY&lt;INT64&gt;</code> Returns an array of S2 Cell IDs that cover the input geography."},{"location":"3.ngql-guide/6.functions-and-expressions/14.geo/#examples","title":"Examples","text":"<pre><code>nebula&gt; RETURN ST_ASText(ST_Point(1,1));\n+--------------------------+\n| ST_ASText(ST_Point(1,1)) |\n+--------------------------+\n| \"POINT(1 1)\"             |\n+--------------------------+\n\nnebula&gt; RETURN ST_ASText(ST_GeogFromText(\"POINT(3 8)\"));\n+------------------------------------------+\n| ST_ASText(ST_GeogFromText(\"POINT(3 8)\")) |\n+------------------------------------------+\n| \"POINT(3 8)\"                             |\n+------------------------------------------+\n\nnebula&gt; RETURN ST_ASTEXT(ST_Centroid(ST_GeogFromText(\"LineString(0 1,1 0)\")));\n+----------------------------------------------------------------+\n| ST_ASTEXT(ST_Centroid(ST_GeogFromText(\"LineString(0 1,1 0)\"))) |\n+----------------------------------------------------------------+\n| \"POINT(0.5000380800773782 0.5000190382261059)\"                 |\n+----------------------------------------------------------------+\n\nnebula&gt; RETURN ST_ISValid(ST_GeogFromText(\"POINT(3 8)\"));\n+-------------------------------------------+\n| ST_ISValid(ST_GeogFromText(\"POINT(3 8)\")) |\n+-------------------------------------------+\n| true                                      |\n+-------------------------------------------+\n\nnebula&gt; RETURN ST_Intersects(ST_GeogFromText(\"LineString(0 1,1 0)\"),ST_GeogFromText(\"LineString(0 0,1 1)\"));\n+----------------------------------------------------------------------------------------------+\n| ST_Intersects(ST_GeogFromText(\"LineString(0 1,1 0)\"),ST_GeogFromText(\"LineString(0 0,1 1)\")) |\n+----------------------------------------------------------------------------------------------+\n| true                                                                                         |\n+----------------------------------------------------------------------------------------------+\n\nnebula&gt; RETURN ST_Covers(ST_GeogFromText(\"POLYGON((0 0,10 0,10 10,0 10,0 0))\"),ST_Point(1,2));\n+--------------------------------------------------------------------------------+\n| ST_Covers(ST_GeogFromText(\"POLYGON((0 0,10 0,10 10,0 10,0 0))\"),ST_Point(1,2)) |\n+--------------------------------------------------------------------------------+\n| true                                                                           |\n+--------------------------------------------------------------------------------+\n\nnebula&gt; RETURN ST_CoveredBy(ST_Point(1,2),ST_GeogFromText(\"POLYGON((0 0,10 0,10 10,0 10,0 0))\"));\n+-----------------------------------------------------------------------------------+\n| ST_CoveredBy(ST_Point(1,2),ST_GeogFromText(\"POLYGON((0 0,10 0,10 10,0 10,0 0))\")) |\n+-----------------------------------------------------------------------------------+\n| true                                                                              |\n+-----------------------------------------------------------------------------------+\n\nnebula&gt; RETURN ST_dwithin(ST_GeogFromText(\"Point(0 0)\"),ST_GeogFromText(\"Point(10 10)\"),20000000000.0);\n+---------------------------------------------------------------------------------------+\n| ST_dwithin(ST_GeogFromText(\"Point(0 0)\"),ST_GeogFromText(\"Point(10 10)\"),20000000000) |\n+---------------------------------------------------------------------------------------+\n| true                                                                                  |\n+---------------------------------------------------------------------------------------+\n\nnebula&gt; RETURN ST_Distance(ST_GeogFromText(\"Point(0 0)\"),ST_GeogFromText(\"Point(10 10)\"));\n+----------------------------------------------------------------------------+\n| ST_Distance(ST_GeogFromText(\"Point(0 0)\"),ST_GeogFromText(\"Point(10 10)\")) |\n+----------------------------------------------------------------------------+\n| 1.5685230187677438e+06                                                     |\n+----------------------------------------------------------------------------+\n\nnebula&gt; RETURN S2_CellIdFromPoint(ST_GeogFromText(\"Point(1 1)\"));\n+---------------------------------------------------+\n| S2_CellIdFromPoint(ST_GeogFromText(\"Point(1 1)\")) |\n+---------------------------------------------------+\n| 1153277837650709461                               |\n+---------------------------------------------------+\n\nnebula&gt; RETURN S2_CoveringCellIds(ST_GeogFromText(\"POLYGON((0 1, 1 2, 2 3, 0 1))\"));\n+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| S2_CoveringCellIds(ST_GeogFromText(\"POLYGON((0 1, 1 2, 2 3, 0 1))\"))                                                                                                     |\n+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| [1152391494368201343, 1153466862374223872, 1153554823304445952, 1153836298281156608, 1153959443583467520, 1154240918560178176, 1160503736791990272, 1160591697722212352] |\n+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/15.aggregating/","title":"Aggregating functions","text":"<p>This topic describes the aggregating functions supported by NebulaGraph.</p>"},{"location":"3.ngql-guide/6.functions-and-expressions/15.aggregating/#avg","title":"avg()","text":"<p>avg() returns the average value of the argument.</p> <p>Syntax: <code>avg(&lt;expression&gt;)</code></p> <ul> <li>Result type: Double</li> </ul> <p>Example:</p> <pre><code>nebula&gt; MATCH (v:player) RETURN avg(v.player.age);\n+--------------------+\n| avg(v.player.age)  |\n+--------------------+\n| 33.294117647058826 |\n+--------------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/15.aggregating/#count","title":"count()","text":"<p>count() returns the number of records.</p> <ul> <li>(Native nGQL) You can use <code>count()</code> and <code>GROUP BY</code> together to group and count the number of parameters. Use <code>YIELD</code> to return.</li> </ul> <ul> <li>(OpenCypher style) You can use <code>count()</code> and <code>RETURN</code>. <code>GROUP BY</code> is not necessary.</li> </ul> <p>Syntax: <code>count({&lt;expression&gt; | *})</code></p> <ul> <li>count(*) returns the number of rows (including NULL).</li> </ul> <ul> <li>Result type: Int</li> </ul> <p>Example:</p> <pre><code>nebula&gt; WITH [NULL, 1, 1, 2, 2] As a UNWIND a AS b \\\n        RETURN count(b), count(*), count(DISTINCT b);\n+----------+----------+-------------------+\n| count(b) | count(*) | count(distinct b) |\n+----------+----------+-------------------+\n| 4        | 5        | 2                 |\n+----------+----------+-------------------+\n</code></pre> <pre><code># The statement in the following example searches for the people whom `player101` follows and people who follow `player101`, i.e. a bidirectional query.\n# Group and count the number of parameters.\nnebula&gt; GO FROM \"player101\" OVER follow BIDIRECT \\\n        YIELD properties($$).name AS Name \\\n        | GROUP BY $-.Name YIELD $-.Name, count(*);\n+---------------------+----------+\n| $-.Name             | count(*) |\n+---------------------+----------+\n| \"LaMarcus Aldridge\" | 2        |\n| \"Tim Duncan\"        | 2        |\n| \"Marco Belinelli\"   | 1        |\n| \"Manu Ginobili\"     | 1        |\n| \"Boris Diaw\"        | 1        |\n| \"Dejounte Murray\"   | 1        |\n+---------------------+----------+\n\n# Count the number of parameters.\nnebula&gt; MATCH (v1:player)-[:follow]-(v2:player) \\\n        WHERE id(v1)== \"player101\" \\\n        RETURN v2.player.name AS Name, count(*) as cnt ORDER BY cnt DESC;\n+---------------------+-----+\n| Name                | cnt |\n+---------------------+-----+\n| \"LaMarcus Aldridge\" | 2   |\n| \"Tim Duncan\"        | 2   |\n| \"Boris Diaw\"        | 1   |\n| \"Manu Ginobili\"     | 1   |\n| \"Dejounte Murray\"   | 1   |\n| \"Marco Belinelli\"   | 1   |\n+---------------------+-----+\n</code></pre> <p>The preceding example retrieves two columns:</p> <ul> <li><code>$-.Name</code>: the names of the people.</li> </ul> <ul> <li><code>count(*)</code>: how many times the names show up.</li> </ul> <p>Because there are no duplicate names in the <code>basketballplayer</code> dataset, the number <code>2</code> in the column <code>count(*)</code> shows that the person in that row and <code>player101</code> have followed each other.</p> <pre><code># a: The statement in the following example retrieves the age distribution of the players in the dataset.\nnebula&gt; LOOKUP ON player \\\n        YIELD player.age As playerage \\\n        | GROUP BY $-.playerage \\\n        YIELD $-.playerage as age, count(*) AS number \\\n        | ORDER BY $-.number DESC, $-.age DESC;\n+-----+--------+\n| age | number |\n+-----+--------+\n| 34  | 4      |\n| 33  | 4      |\n| 30  | 4      |\n| 29  | 4      |\n| 38  | 3      |\n+-----+--------+\n...\n# b: The statement in the following example retrieves the age distribution of the players in the dataset.\nnebula&gt; MATCH (n:player) \\\n        RETURN n.player.age as age, count(*) as number \\\n        ORDER BY number DESC, age DESC;\n+-----+--------+\n| age | number |\n+-----+--------+\n| 34  | 4      |\n| 33  | 4      |\n| 30  | 4      |\n| 29  | 4      |\n| 38  | 3      |\n+-----+--------+\n...\n</code></pre> <pre><code># The statement in the following example counts the number of edges that Tim Duncan relates.\nnebula&gt; MATCH (v:player{name:\"Tim Duncan\"}) -[e]- (v2) \\\n        RETURN count(e);\n+----------+\n| count(e) |\n+----------+\n| 13       |\n+----------+\n\n# The statement in the following example counts the number of edges that Tim Duncan relates and returns two columns (no DISTINCT and DISTINCT) in multi-hop queries.\nnebula&gt; MATCH (n:player {name : \"Tim Duncan\"})-[]-&gt;(friend:player)-[]-&gt;(fof:player) \\\n        RETURN count(fof), count(DISTINCT fof);\n+------------+---------------------+\n| count(fof) | count(distinct fof) |\n+------------+---------------------+\n| 4          | 3                   |\n+------------+---------------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/15.aggregating/#max","title":"max()","text":"<p>max() returns the maximum value.</p> <p>Syntax: <code>max(&lt;expression&gt;)</code></p> <ul> <li>Result type: Same as the original argument.</li> </ul> <p>Example:</p> <pre><code>nebula&gt; MATCH (v:player) RETURN max(v.player.age);\n+-------------------+\n| max(v.player.age) |\n+-------------------+\n| 47                |\n+-------------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/15.aggregating/#min","title":"min()","text":"<p>min() returns the minimum value.</p> <p>Syntax: <code>min(&lt;expression&gt;)</code></p> <ul> <li>Result type: Same as the original argument.</li> </ul> <p>Example:</p> <pre><code>nebula&gt; MATCH (v:player) RETURN min(v.player.age);\n+-------------------+\n| min(v.player.age) |\n+-------------------+\n| 20                |\n+-------------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/15.aggregating/#collect","title":"collect()","text":"<p>collect() returns a list containing the values returned by an expression. Using this function aggregates data by merging multiple records or values into a single list.</p> <p>Syntax: <code>collect(&lt;expression&gt;)</code></p> <ul> <li>Result type: List</li> </ul> <p>Example:</p> <pre><code>nebula&gt; UNWIND [1, 2, 1] AS a \\\n        RETURN a;\n+---+\n| a |\n+---+\n| 1 |\n| 2 |\n| 1 |\n+---+\n\nnebula&gt; UNWIND [1, 2, 1] AS a \\\n        RETURN collect(a);\n+------------+\n| collect(a) |\n+------------+\n| [1, 2, 1]  |\n+------------+\n\nnebula&gt; UNWIND [1, 2, 1] AS a \\\n        RETURN a, collect(a), size(collect(a));\n+---+------------+------------------+\n| a | collect(a) | size(collect(a)) |\n+---+------------+------------------+\n| 2 | [2]        | 1                |\n| 1 | [1, 1]     | 2                |\n+---+------------+------------------+\n\n# The following examples sort the results in descending order, limit output rows to 3, and collect the output into a list.\nnebula&gt; UNWIND [\"c\", \"b\", \"a\", \"d\" ] AS p \\\n        WITH p AS q \\\n        ORDER BY q DESC LIMIT 3 \\\n        RETURN collect(q);\n+-----------------+\n| collect(q)      |\n+-----------------+\n| [\"d\", \"c\", \"b\"] |\n+-----------------+\n\nnebula&gt; WITH [1, 1, 2, 2] AS coll \\\n        UNWIND coll AS x \\\n        WITH DISTINCT x \\\n        RETURN collect(x) AS ss;\n+--------+\n| ss     |\n+--------+\n| [1, 2] |\n+--------+\n\nnebula&gt; MATCH (n:player) \\\n        RETURN collect(n.player.age);\n+---------------------------------------------------------------+\n| collect(n.player.age)                                         |\n+---------------------------------------------------------------+\n| [32, 32, 34, 29, 41, 40, 33, 25, 40, 37, ...\n...\n\n# The following example aggregates all the players' names by their ages.\nnebula&gt; MATCH (n:player) \\\n        RETURN n.player.age AS age, collect(n.player.name);\n+-----+--------------------------------------------------------------------------+\n| age | collect(n.player.name)                                                   |\n+-----+--------------------------------------------------------------------------+\n| 24  | [\"Giannis Antetokounmpo\"]                                                |\n| 20  | [\"Luka Doncic\"]                                                          |\n| 25  | [\"Joel Embiid\", \"Kyle Anderson\"]                                         |\n+-----+--------------------------------------------------------------------------+\n...\n\nnebula&gt; GO FROM \"player100\" OVER serve \\\n        YIELD properties($$).name AS name \\\n        | GROUP BY $-.name \\\n        YIELD collect($-.name) AS name;\n+-----------+\n| name      |\n+-----------+\n| [\"Spurs\"] |\n+-----------+\n\nnebula&gt; LOOKUP ON player \\\n        YIELD player.age As playerage \\\n        | GROUP BY $-.playerage \\\n        YIELD collect($-.playerage) AS playerage;\n+------------------+\n| playerage        |\n+------------------+\n| [22]             |\n| [47]             |\n| [43]             |\n| [25, 25]         |\n+------------------+\n...\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/15.aggregating/#std","title":"std()","text":"<p>std() returns the population standard deviation.</p> <p>Syntax: <code>std(&lt;expression&gt;)</code></p> <ul> <li>Result type: Double</li> </ul> <p>Example:</p> <pre><code>nebula&gt; MATCH (v:player) RETURN std(v.player.age);\n+-------------------+\n| std(v.player.age) |\n+-------------------+\n| 6.423895701687502 |\n+-------------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/15.aggregating/#sum","title":"sum()","text":"<p>sum() returns the sum value.</p> <p>Syntax: <code>sum(&lt;expression&gt;)</code></p> <ul> <li>Result type: Same as the original argument.</li> </ul> <p>Example:</p> <pre><code>nebula&gt; MATCH (v:player) RETURN sum(v.player.age);\n+-------------------+\n| sum(v.player.age) |\n+-------------------+\n| 1698              |\n+-------------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/15.aggregating/#aggregating_example","title":"Aggregating example","text":"<pre><code>nebula&gt;  GO FROM \"player100\" OVER follow YIELD dst(edge) AS dst, properties($$).age AS age \\\n         | GROUP BY $-.dst \\\n         YIELD \\\n         $-.dst AS dst, \\\n         toInteger((sum($-.age)/count($-.age)))+avg(distinct $-.age+1)+1 AS statistics;\n+-------------+------------+\n| dst         | statistics |\n+-------------+------------+\n| \"player125\" | 84.0       |\n| \"player101\" | 74.0       |\n+-------------+------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/16.type-conversion/","title":"Type conversion functions","text":"<p>This topic describes the type conversion functions supported by NebulaGraph.</p>"},{"location":"3.ngql-guide/6.functions-and-expressions/16.type-conversion/#toboolean","title":"toBoolean()","text":"<p>toBoolean() converts a string value to a boolean value.</p> <p>Syntax: <code>toBoolean(&lt;value&gt;)</code></p> <ul> <li>Result type: Bool</li> </ul> <p>Example:</p> <pre><code>nebula&gt; UNWIND [true, false, 'true', 'false', NULL] AS b \\\n        RETURN toBoolean(b) AS b;\n+----------+\n| b        |\n+----------+\n| true     |\n| false    |\n| true     |\n| false    |\n| __NULL__ |\n+----------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/16.type-conversion/#tofloat","title":"toFloat()","text":"<p>toFloat() converts an integer or string value to a floating point number.</p> <p>Syntax: <code>toFloat(&lt;value&gt;)</code></p> <ul> <li>Result type: Float</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN toFloat(1), toFloat('1.3'), toFloat('1e3'), toFloat('not a number');\n+------------+----------------+----------------+-------------------------+\n| toFloat(1) | toFloat(\"1.3\") | toFloat(\"1e3\") | toFloat(\"not a number\") |\n+------------+----------------+----------------+-------------------------+\n| 1.0        | 1.3            | 1000.0         | __NULL__                |\n+------------+----------------+----------------+-------------------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/16.type-conversion/#tostring","title":"toString()","text":"<p>toString() converts non-compound types of data, such as numbers, booleans, and so on, to strings.</p> <p>Syntax: <code>toString(&lt;value&gt;)</code></p> <ul> <li>Result type: String</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN toString(9669) AS int2str, toString(null) AS null2str;\n+---------+----------+\n| int2str | null2str |\n+---------+----------+\n| \"9669\"  | __NULL__ |\n+---------+----------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/16.type-conversion/#tointeger","title":"toInteger()","text":"<p>toInteger() converts a floating point or string value to an integer value.</p> <p>Syntax: <code>toInteger(&lt;value&gt;)</code></p> <ul> <li>Result type: Int</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN toInteger(1), toInteger('1'), toInteger('1e3'), toInteger('not a number');\n+--------------+----------------+------------------+---------------------------+\n| toInteger(1) | toInteger(\"1\") | toInteger(\"1e3\") | toInteger(\"not a number\") |\n+--------------+----------------+------------------+---------------------------+\n| 1            | 1              | 1000             | __NULL__                  |\n+--------------+----------------+------------------+---------------------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/16.type-conversion/#toset","title":"toSet()","text":"<p>toSet() converts a list or set value to a set value.</p> <p>Syntax: <code>toSet(&lt;value&gt;)</code></p> <ul> <li>Result type: Set</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN toSet(list[1,2,3,1,2]) AS list2set;\n+-----------+\n| list2set  |\n+-----------+\n| {3, 1, 2} |\n+-----------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/16.type-conversion/#hash","title":"hash()","text":"<p>hash() returns the hash value of the argument. The argument can be a number, a string, a list, a boolean, null, or an expression that evaluates to a value of the preceding data types.</p> <p>The source code of the <code>hash()</code> function (MurmurHash2), seed (<code>0xc70f6907UL</code>), and other parameters can be found in <code>MurmurHash2.h</code>.</p> <p>For Java, the hash function operates as follows.</p> <pre><code>MurmurHash2.hash64(\"to_be_hashed\".getBytes(),\"to_be_hashed\".getBytes().length, 0xc70f6907)\n</code></pre> <p>Syntax: <code>hash(&lt;string&gt;)</code></p> <ul> <li>Result type: Int</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN hash(\"abcde\");\n+--------------------+\n| hash(\"abcde\")      |\n+--------------------+\n| 811036730794841393 |\n+--------------------+\nnebula&gt; YIELD hash([1,2,3]);\n+----------------+\n| hash([1,2,3])  |\n+----------------+\n| 11093822460243 |\n+----------------+\nnebula&gt; YIELD hash(NULL);\n+------------+\n| hash(NULL) |\n+------------+\n| -1         |\n+------------+\nnebula&gt; YIELD hash(toLower(\"HELLO NEBULA\"));\n+-------------------------------+\n| hash(toLower(\"HELLO NEBULA\")) |\n+-------------------------------+\n| -8481157362655072082          |\n+-------------------------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/2.string/","title":"Built-in string functions","text":"<p>This topic describes the built-in string functions supported by NebulaGraph.</p>"},{"location":"3.ngql-guide/6.functions-and-expressions/2.string/#precautions","title":"Precautions","text":"<ul> <li>A string type is used to store a sequence of characters (text). The literal constant is a sequence of characters of any length surrounded by double or single quotes.</li> </ul> <ul> <li>Like SQL, the position index of nGQL starts from <code>1</code>, while in C language it starts from <code>0</code>.</li> </ul>"},{"location":"3.ngql-guide/6.functions-and-expressions/2.string/#strcasecmp","title":"strcasecmp()","text":"<p>strcasecmp() compares string a and b without case sensitivity.</p> <p>Syntax: <code>strcasecmp(&lt;string_a&gt;,&lt;string_b&gt;)</code></p> <ul> <li><code>string_a</code>, <code>string_b</code>: Strings to compare.</li> </ul> <ul> <li>Result type: Int</li> </ul> <ul> <li>When <code>string_a = string_b</code>, the return value is <code>0</code>. When <code>string_a &gt; string_b</code>, the return value is greater than <code>0</code>. When <code>string_a &lt; string_b</code>, the return value is less than <code>0</code>.</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN strcasecmp(\"a\",\"aa\");\n+----------------------+\n| strcasecmp(\"a\",\"aa\") |\n+----------------------+\n| -97                  |\n+----------------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/2.string/#lower_and_tolower","title":"lower() and toLower()","text":"<p>lower() and toLower() can both returns the argument in lowercase.</p> <p>Syntax: <code>lower(&lt;string&gt;)</code>, <code>toLower(&lt;string&gt;)</code></p> <ul> <li><code>string</code>: A specified string.</li> </ul> <ul> <li>Result type: String</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN lower(\"Basketball_Player\");\n+----------------------------+\n| lower(\"Basketball_Player\") |\n+----------------------------+\n| \"basketball_player\"        |\n+----------------------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/2.string/#upper_and_toupper","title":"upper() and toUpper()","text":"<p>upper() and toUpper() can both returns the argument in uppercase.</p> <p>Syntax: <code>upper(&lt;string&gt;)</code>, <code>toUpper(&lt;string&gt;)</code></p> <ul> <li><code>string</code>: A specified string.</li> </ul> <ul> <li>Result type: String</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN upper(\"Basketball_Player\");\n+----------------------------+\n| upper(\"Basketball_Player\") |\n+----------------------------+\n| \"BASKETBALL_PLAYER\"        |\n+----------------------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/2.string/#length","title":"length()","text":"<p>length() returns the length of the given string in bytes.</p> <p>Syntax: <code>length({&lt;string&gt;|&lt;path&gt;})</code></p> <ul> <li><code>string</code>: A specified string.</li> <li><code>path</code>: A specified path represented by a variable.</li> <li>Result type: Int</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN length(\"basketball\");\n+----------------------+\n| length(\"basketball\") |\n+----------------------+\n| 10                   |\n+----------------------+\n</code></pre> <pre><code>nebula&gt; MATCH p=(v:player{name:\"Tim Duncan\"})--&gt;(v2) return length(p);\n+-----------+\n| length(p) |\n+-----------+\n| 1         |\n| 1         |\n| 1         |\n+-----------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/2.string/#trim","title":"trim()","text":"<p>trim() removes the spaces at the leading and trailing of the string.</p> <p>Syntax: <code>trim(&lt;string&gt;)</code></p> <ul> <li><code>string</code>: A specified string.</li> </ul> <ul> <li>Result type: String</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN trim(\" basketball player \");\n+-----------------------------+\n| trim(\" basketball player \") |\n+-----------------------------+\n| \"basketball player\"         |\n+-----------------------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/2.string/#ltrim","title":"ltrim()","text":"<p>ltrim() removes the spaces at the leading of the string.</p> <p>Syntax: <code>ltrim(&lt;string&gt;)</code></p> <ul> <li><code>string</code>: A specified string.</li> </ul> <ul> <li>Result type: String</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN ltrim(\" basketball player \");\n+------------------------------+\n| ltrim(\" basketball player \") |\n+------------------------------+\n| \"basketball player \"         |\n+------------------------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/2.string/#rtrim","title":"rtrim()","text":"<p>rtrim() removes the spaces at the trailing of the string.</p> <p>Syntax: <code>rtrim(&lt;string&gt;)</code></p> <ul> <li><code>string</code>: A specified string.</li> </ul> <ul> <li>Result type: String</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN rtrim(\" basketball player \");\n+------------------------------+\n| rtrim(\" basketball player \") |\n+------------------------------+\n| \" basketball player\"         |\n+------------------------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/2.string/#left","title":"left()","text":"<p>left() returns a substring consisting of several characters from the leading of a string.</p> <p>Syntax: <code>left(&lt;string&gt;,&lt;count&gt;)</code></p> <ul> <li><code>string</code>: A specified string.</li> </ul> <ul> <li><code>count</code>: The number of characters from the leading of the string. If the string is shorter than <code>count</code>, the system returns the string itself.</li> </ul> <ul> <li>Result type: String</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN left(\"basketball_player\",6);\n+-----------------------------+\n| left(\"basketball_player\",6) |\n+-----------------------------+\n| \"basket\"                    |\n+-----------------------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/2.string/#right","title":"right()","text":"<p>right() returns a substring consisting of several characters from the trailing of a string.</p> <p>Syntax: <code>right(&lt;string&gt;,&lt;count&gt;)</code></p> <ul> <li><code>string</code>: A specified string.</li> </ul> <ul> <li><code>count</code>: The number of characters from the trailing of the string. If the string is shorter than <code>count</code>, the system returns the string itself.</li> </ul> <ul> <li>Result type: String</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN right(\"basketball_player\",6);\n+------------------------------+\n| right(\"basketball_player\",6) |\n+------------------------------+\n| \"player\"                     |\n+------------------------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/2.string/#lpad","title":"lpad()","text":"<p>lpad() pads a specified string from the left-side to the specified length and returns the result string.</p> <p>Syntax: <code>lpad(&lt;string&gt;,&lt;count&gt;,&lt;letters&gt;)</code></p> <ul> <li><code>string</code>: A specified string.</li> </ul> <ul> <li><code>count</code>: The length of the string after it has been left-padded. If the length is less than that of <code>string</code>, only the length of <code>string</code> characters from front to back will be returned.</li> </ul> <ul> <li><code>letters</code>: A string to be padding from the leading.</li> </ul> <ul> <li>Result type: String</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN lpad(\"abcd\",10,\"b\");\n+---------------------+\n| lpad(\"abcd\",10,\"b\") |\n+---------------------+\n| \"bbbbbbabcd\"        |\n+---------------------+\nnebula&gt; RETURN lpad(\"abcd\",3,\"b\");\n+--------------------+\n| lpad(\"abcd\",3,\"b\") |\n+--------------------+\n| \"abc\"              |\n+--------------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/2.string/#rpad","title":"rpad()","text":"<p>rpad() pads a specified string from the right-side to the specified length and returns the result string.</p> <p>Syntax: <code>rpad(&lt;string&gt;,&lt;count&gt;,&lt;letters&gt;)</code></p> <ul> <li><code>string</code>: A specified string.</li> </ul> <ul> <li><code>count</code>: The length of the string after it has been right-padded. If the length is less than that of <code>string</code>, only the length of <code>string</code> characters from front to back will be returned.</li> </ul> <ul> <li><code>letters</code>: A string to be padding from the trailing.</li> </ul> <ul> <li>Result type: String</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN rpad(\"abcd\",10,\"b\");\n+---------------------+\n| rpad(\"abcd\",10,\"b\") |\n+---------------------+\n| \"abcdbbbbbb\"        |\n+---------------------+\nnebula&gt; RETURN rpad(\"abcd\",3,\"b\");\n+--------------------+\n| rpad(\"abcd\",3,\"b\") |\n+--------------------+\n| \"abc\"              |\n+--------------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/2.string/#substr_and_substring","title":"substr() and substring()","text":"<p>substr() and substring() return a substring extracting <code>count</code> characters starting from the specified position <code>pos</code> of a specified string.</p> <p>Syntax: <code>substr(&lt;string&gt;,&lt;pos&gt;,&lt;count&gt;)</code>, <code>substring(&lt;string&gt;,&lt;pos&gt;,&lt;count&gt;)</code></p> <ul> <li><code>string</code>: A specified string.</li> </ul> <ul> <li><code>pos</code>: The position of starting extract (character index). Data type is int.</li> </ul> <ul> <li><code>count</code>: The number of characters extracted from the start position onwards.</li> </ul> <ul> <li>Result type: String</li> </ul>"},{"location":"3.ngql-guide/6.functions-and-expressions/2.string/#explanations_for_the_return_of_substr_and_substring","title":"Explanations for the return of <code>substr()</code> and <code>substring()</code>","text":"<ul> <li>If <code>pos</code> is 0, it extracts from the specified string leading (including the first character).</li> </ul> <ul> <li>If <code>pos</code> is greater than the maximum string index, an empty string is returned.</li> </ul> <ul> <li>If <code>pos</code> is a negative number, <code>BAD_DATA</code> is returned.</li> </ul> <ul> <li>If <code>count</code> is omitted, the function returns the substring starting at the position given by <code>pos</code> and extending to the end of the string.</li> </ul> <ul> <li>If <code>count</code> is 0, an empty string is returned.</li> </ul> <ul> <li>Using <code>NULL</code> as any of the argument of <code>substr()</code> will cause an issue.</li> </ul> <p>OpenCypher compatibility</p> <p>In openCypher, if <code>a</code> is <code>null</code>, <code>null</code> is returned.</p> <p>Example:</p> <pre><code>nebula&gt; RETURN substr(\"abcdefg\",2,4);\n+-----------------------+\n| substr(\"abcdefg\",2,4) |\n+-----------------------+\n| \"cdef\"                |\n+-----------------------+\nnebula&gt; RETURN substr(\"abcdefg\",0,4);\n+-----------------------+\n| substr(\"abcdefg\",0,4) |\n+-----------------------+\n| \"abcd\"                |\n+-----------------------+\nnebula&gt; RETURN substr(\"abcdefg\",2);\n+---------------------+\n| substr(\"abcdefg\",2) |\n+---------------------+\n| \"cdefg\"             |\n+---------------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/2.string/#reverse","title":"reverse()","text":"<p>reverse() returns a string in reverse order.</p> <p>Syntax: <code>reverse(&lt;string&gt;)</code></p> <ul> <li><code>string</code>: A specified string.</li> </ul> <ul> <li>Result type: String</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN reverse(\"abcdefg\");\n+--------------------+\n| reverse(\"abcdefg\") |\n+--------------------+\n| \"gfedcba\"          |\n+--------------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/2.string/#replace","title":"replace()","text":"<p>replace() replaces string a in a specified string with string b.</p> <p>Syntax: <code>replace(&lt;string&gt;,&lt;substr_a&gt;,&lt;string_b&gt;)</code></p> <ul> <li><code>string</code>: A specified string.</li> </ul> <ul> <li><code>substr_a</code>: String a.</li> </ul> <ul> <li><code>string_b</code>: String b.</li> </ul> <ul> <li>Result type: String</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN replace(\"abcdefg\",\"cd\",\"AAAAA\");\n+---------------------------------+\n| replace(\"abcdefg\",\"cd\",\"AAAAA\") |\n+---------------------------------+\n| \"abAAAAAefg\"                    |\n+---------------------------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/2.string/#split","title":"split()","text":"<p>split() splits a specified string at string b and returns a list of strings.</p> <p>Syntax: <code>split(&lt;string&gt;,&lt;substr&gt;)</code></p> <ul> <li><code>string</code>: A specified string.</li> </ul> <ul> <li><code>substr</code>: String b.</li> </ul> <ul> <li>Result type: List</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN split(\"basketballplayer\",\"a\");\n+-------------------------------+\n| split(\"basketballplayer\",\"a\") |\n+-------------------------------+\n| [\"b\", \"sketb\", \"llpl\", \"yer\"] |\n+-------------------------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/2.string/#concat","title":"concat()","text":"<p>concat() returns strings concatenated by all strings.</p> <p>Syntax: <code>concat(&lt;string1&gt;,&lt;string2&gt;,...)</code></p> <ul> <li>The function requires at least two or more strings. If there is only one string, the string itself is returned.</li> </ul> <ul> <li>If any one of the strings is <code>NULL</code>, <code>NULL</code> is returned.</li> </ul> <ul> <li>Result type: String</li> </ul> <p>Example:</p> <pre><code>//This example concatenates 1, 2, and 3.\nnebula&gt; RETURN concat(\"1\",\"2\",\"3\") AS r;\n+-------+\n| r     |\n+-------+\n| \"123\" |\n+-------+\n\n//In this example, one of the string is NULL.\nnebula&gt; RETURN concat(\"1\",\"2\",NULL) AS r;\n+----------+\n| r        |\n+----------+\n| __NULL__ |\n+----------+\n\nnebula&gt; GO FROM \"player100\" over follow \\\n        YIELD concat(src(edge), properties($^).age, properties($$).name, properties(edge).degree) AS A;\n+------------------------------+\n| A                            |\n+------------------------------+\n| \"player10042Tony Parker95\"   |\n| \"player10042Manu Ginobili95\" |\n+------------------------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/2.string/#concat_ws","title":"concat_ws()","text":"<p>concat_ws() returns strings concatenated by all strings that are delimited with a separator.</p> <p>Syntax: <code>concat_ws(&lt;separator&gt;,&lt;string1&gt;,&lt;string2&gt;,... )</code></p> <ul> <li>The function requires at least two or more strings.</li> </ul> <ul> <li>If the separator is <code>NULL</code>, the <code>concat_ws()</code> function returns <code>NULL</code>.</li> </ul> <ul> <li>If the separator is not <code>NULL</code> and there is only one string, the string itself is returned.</li> </ul> <ul> <li>If there is a <code>NULL</code> in the strings, <code>NULL</code> is ignored during the concatenation.</li> </ul> <p>Example:</p> <pre><code>//This example concatenates a, b, and c with the separator +.\nnebula&gt; RETURN concat_ws(\"+\",\"a\",\"b\",\"c\") AS r;\n+---------+\n| r       |\n+---------+\n| \"a+b+c\" |\n+---------+\n\n//In this example, the separator is NULL.\nneubla&gt; RETURN concat_ws(NULL,\"a\",\"b\",\"c\") AS r;\n+----------+\n| r        |\n+----------+\n| __NULL__ |\n+----------+\n\n//In this example, the separator is + and there is a NULL in the strings.\nnebula&gt; RETURN concat_ws(\"+\",\"a\",NULL,\"b\",\"c\") AS r;\n+---------+\n| r       |\n+---------+\n| \"a+b+c\" |\n+---------+\n\n//In this example, the separator is + and there is only one string.\nnebula&gt; RETURN concat_ws(\"+\",\"a\") AS r;\n+-----+\n| r   |\n+-----+\n| \"a\" |\n+-----+\nnebula&gt; GO FROM \"player100\" over follow \\\n        YIELD concat_ws(\" \",src(edge), properties($^).age, properties($$).name, properties(edge).degree) AS A;\n+---------------------------------+\n| A                               |\n+---------------------------------+\n| \"player100 42 Tony Parker 95\"   |\n| \"player100 42 Manu Ginobili 95\" |\n+---------------------------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/2.string/#extract","title":"extract()","text":"<p>extract() uses regular expression matching to retrieve a single substring or all substrings from a string.</p> <p>Syntax: <code>extract(&lt;string&gt;,\"&lt;regular_expression&gt;\")</code></p> <ul> <li><code>string</code>: A specified string</li> <li><code>regular_expression</code>: A regular expression</li> <li>Result type: List</li> </ul> <p>Example:</p> <pre><code>nebula&gt; MATCH (a:player)-[b:serve]-(c:team{name: \"Lakers\"}) \\\n        WHERE a.player.age &gt; 45 \\\n        RETURN extract(a.player.name, \"\\\\w+\") AS result;\n+----------------------------+\n| result                     |\n+----------------------------+\n| [\"Shaquille\", \"O\", \"Neal\"] |\n+----------------------------+\n\nnebula&gt; MATCH (a:player)-[b:serve]-(c:team{name: \"Lakers\"}) \\\n        WHERE a.player.age &gt; 45 \\\n        RETURN extract(a.player.name, \"hello\") AS result;\n+--------+\n| result |\n+--------+\n| []     |\n+--------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/2.string/#json_extract","title":"json_extract()","text":"<p>json_extract() converts the specified JSON string to a map.</p> <p>Syntax: <code>extract(&lt;string&gt;)</code></p> <ul> <li><code>string</code>:A specified string, must be JSON string.</li> <li>Result type: Map</li> </ul> <p>Caution</p> <ul> <li>Only Bool, Double, Int, String value and NULL are supported.</li> <li>Only depth-1 nested Map is supported now. If nested Map depth is greater than 1, the nested item is left as an empty Map().</li> </ul> <p>Example:</p> <pre><code>nebula&gt; YIELD json_extract('{\"a\": 1, \"b\": {}, \"c\": {\"d\": true}}') AS result;\n+-----------------------------+\n| result                      |\n+-----------------------------+\n| {a: 1, b: {}, c: {d: true}} |\n+-----------------------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/3.date-and-time/","title":"Built-in date and time functions","text":"<p>NebulaGraph supports the following built-in date and time functions:</p> Function Description int now() Returns the current timestamp of the system. timestamp timestamp() Returns the current timestamp of the system. date date() Returns the current UTC date based on the current system. time time() Returns the current UTC time based on the current system. datetime datetime() Returns the current UTC date and time based on the current system. map duration() Returns the period of time. It can be used to calculate the specified time. <p>For more information, see Date and time types.</p>"},{"location":"3.ngql-guide/6.functions-and-expressions/3.date-and-time/#examples","title":"Examples","text":"<pre><code>nebula&gt;  RETURN now(), timestamp(), date(), time(), datetime();\n+------------+-------------+------------+-----------------+----------------------------+\n| now()      | timestamp() | date()     | time()          | datetime()                 |\n+------------+-------------+------------+-----------------+----------------------------+\n| 1640057560 | 1640057560  | 2021-12-21 | 03:32:40.351000 | 2021-12-21T03:32:40.351000 |\n+------------+-------------+------------+-----------------+----------------------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/4.schema/","title":"Schema-related functions","text":"<p>This topic describes the schema-related functions supported by NebulaGraph. There are two types of schema-related functions, one for native nGQL statements and the other for openCypher-compatible statements.</p>"},{"location":"3.ngql-guide/6.functions-and-expressions/4.schema/#for_ngql_statements","title":"For nGQL statements","text":"<p>The following functions are available in <code>YIELD</code> and <code>WHERE</code> clauses of nGQL statements.</p> <p>Note</p> <p>Since vertex, edge, vertices, edges, and path are keywords, you need to use <code>AS &lt;alias&gt;</code> to set the alias, such as <code>GO FROM \"player100\" OVER follow YIELD edge AS e;</code>.</p>"},{"location":"3.ngql-guide/6.functions-and-expressions/4.schema/#idvertex","title":"id(vertex)","text":"<p>id(vertex) returns the ID of a vertex.</p> <p>Syntax: <code>id(vertex)</code></p> <ul> <li>Result type: Same as the vertex ID.</li> </ul> <p>Example:</p> <pre><code>nebula&gt; LOOKUP ON player WHERE player.age  &gt; 45 YIELD id(vertex);\n+-------------+\n| id(VERTEX)  |\n+-------------+\n| \"player144\" |\n| \"player140\" |\n+-------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/4.schema/#propertiesvertex","title":"properties(vertex)","text":"<p>properties(vertex) returns the properties of a vertex.</p> <p>Syntax: <code>properties(vertex)</code></p> <ul> <li>Result type: Map</li> </ul> <p>Example:</p> <pre><code>nebula&gt; LOOKUP ON player WHERE player.age  &gt; 45 \\\n        YIELD properties(vertex);\n+-------------------------------------+\n| properties(VERTEX)                  |\n+-------------------------------------+\n| {age: 47, name: \"Shaquille O'Neal\"} |\n| {age: 46, name: \"Grant Hill\"}       |\n+-------------------------------------+\n</code></pre> <p>You can also use the property reference symbols (<code>$^</code> and <code>$$</code>) instead of the <code>vertex</code> field in the <code>properties()</code> function to get all properties of a vertex.</p> <ul> <li><code>$^</code> represents the data of the starting vertex at the beginning of exploration. For example, in <code>GO FROM \"player100\" OVER follow reversely YIELD properties($^)</code>, <code>$^</code> refers to the vertex <code>player100</code>.</li> </ul> <ul> <li><code>$$</code> represents the data of the end vertex at the end of exploration.</li> </ul> <p><code>properties($^)</code> and <code>properties($$)</code> are generally used in <code>GO</code> statements. For more information, see Property reference.</p> <p>Caution</p> <p>You can use <code>properties().&lt;property_name&gt;</code> to get a specific property of a vertex. However, it is not recommended to use this method to obtain specific properties because the <code>properties()</code> function returns all properties, which can decrease query performance.</p>"},{"location":"3.ngql-guide/6.functions-and-expressions/4.schema/#propertiesedge","title":"properties(edge)","text":"<p>properties(edge) returns the properties of an edge.</p> <p>Syntax: <code>properties(edge)</code></p> <ul> <li>Result type: Map</li> </ul> <p>Example:</p> <pre><code>nebula&gt; GO FROM \"player100\" OVER follow \\\n        YIELD properties(edge);\n+------------------+\n| properties(EDGE) |\n+------------------+\n| {degree: 95}     |\n| {degree: 95}     |\n+------------------+\n</code></pre> <p>Caution</p> <p>You can use <code>properties(edge).&lt;property_name&gt;</code> to get a specific property of an edge. However, it is not recommended to use this method to obtain specific properties because the <code>properties(edge)</code> function returns all properties, which can decrease query performance.</p>"},{"location":"3.ngql-guide/6.functions-and-expressions/4.schema/#typeedge","title":"type(edge)","text":"<p>type(edge) returns the edge type of an edge.</p> <p>Syntax: <code>type(edge)</code></p> <ul> <li>Result type: String</li> </ul> <p>Example:</p> <pre><code>nebula&gt; GO FROM \"player100\" OVER follow \\\n        YIELD src(edge), dst(edge), type(edge), rank(edge);\n+-------------+-------------+------------+------------+\n| src(EDGE)   | dst(EDGE)   | type(EDGE) | rank(EDGE) |\n+-------------+-------------+------------+------------+\n| \"player100\" | \"player101\" | \"follow\"   | 0          |\n| \"player100\" | \"player125\" | \"follow\"   | 0          |\n+-------------+-------------+------------+------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/4.schema/#srcedge","title":"src(edge)","text":"<p>src(edge) returns the source vertex ID of an edge.</p> <p>Syntax: <code>src(edge)</code></p> <ul> <li>Result type: Same as the vertex ID.</li> </ul> <p>Example:</p> <pre><code>nebula&gt; GO FROM \"player100\" OVER follow \\\n        YIELD src(edge), dst(edge);\n+-------------+-------------+\n| src(EDGE)   | dst(EDGE)   |\n+-------------+-------------+\n| \"player100\" | \"player101\" |\n| \"player100\" | \"player125\" |\n+-------------+-------------+\n</code></pre> <p>Note</p> <p>The semantics of the query for the starting vertex with src(edge) and properties(<code>$^</code>) are different. src(edge) indicates the starting vertex ID of the edge in the graph database, while properties(<code>$^</code>) indicates the data of the starting vertex where you start to expand the graph, such as the data of the starting vertex <code>player100</code> in the above GO statement. </p>"},{"location":"3.ngql-guide/6.functions-and-expressions/4.schema/#dstedge","title":"dst(edge)","text":"<p>dst(edge) returns the destination vertex ID of an edge.</p> <p>Syntax: <code>dst(edge)</code></p> <ul> <li>Result type: Same as the vertex ID.</li> </ul> <p>Example:</p> <pre><code>nebula&gt; GO FROM \"player100\" OVER follow \\\n        YIELD src(edge), dst(edge);\n+-------------+-------------+\n| src(EDGE)   | dst(EDGE)   |\n+-------------+-------------+\n| \"player100\" | \"player101\" |\n| \"player100\" | \"player125\" |\n+-------------+-------------+\n</code></pre> <p>Note</p> <p>dst(edge) indicates the destination vertex ID of the edge in the graph database.</p>"},{"location":"3.ngql-guide/6.functions-and-expressions/4.schema/#rankedge","title":"rank(edge)","text":"<p>rank(edge) returns the rank value of an edge.</p> <p>Syntax: <code>rank(edge)</code></p> <ul> <li>Result type: Int</li> </ul> <p>Example:</p> <pre><code>nebula&gt; GO FROM \"player100\" OVER follow \\\n        YIELD src(edge), dst(edge), rank(edge);\n+-------------+-------------+------------+\n| src(EDGE)   | dst(EDGE)   | rank(EDGE) |\n+-------------+-------------+------------+\n| \"player100\" | \"player101\" | 0          |\n| \"player100\" | \"player125\" | 0          |\n+-------------+-------------+------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/4.schema/#vertex","title":"vertex","text":"<p>vertex returns the information of vertices, including VIDs, tags, properties, and values. You need to use <code>AS &lt;alias&gt;</code> to set the alias.</p> <p>Syntax: <code>vertex</code></p> <p>Example:</p> <pre><code>nebula&gt; LOOKUP ON player WHERE player.age &gt; 45 YIELD vertex AS v;\n+----------------------------------------------------------+\n| v                                                        |\n+----------------------------------------------------------+\n| (\"player144\" :player{age: 47, name: \"Shaquille O'Neal\"}) |\n| (\"player140\" :player{age: 46, name: \"Grant Hill\"})       |\n+----------------------------------------------------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/4.schema/#edge","title":"edge","text":"<p>edge returns the information of edges, including edge types, source vertices, destination vertices, ranks, properties, and values. You need to use <code>AS &lt;alias&gt;</code> to set the alias.</p> <p>Syntax: <code>edge</code></p> <p>Example:</p> <pre><code>nebula&gt; GO FROM \"player100\" OVER follow YIELD edge AS e;\n+----------------------------------------------------+\n| e                                                  |\n+----------------------------------------------------+\n| [:follow \"player100\"-&gt;\"player101\" @0 {degree: 95}] |\n| [:follow \"player100\"-&gt;\"player125\" @0 {degree: 95}] |\n+----------------------------------------------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/4.schema/#vertices","title":"vertices","text":"<p>vertices returns the information of vertices in a subgraph. For more information, see GET SUBGRAPH.</p>"},{"location":"3.ngql-guide/6.functions-and-expressions/4.schema/#edges","title":"edges","text":"<p>edges returns the information of edges in a subgraph. For more information, see GET SUBGRAPH.</p>"},{"location":"3.ngql-guide/6.functions-and-expressions/4.schema/#path","title":"path","text":"<p>path returns the information of a path. For more information, see FIND PATH.</p>"},{"location":"3.ngql-guide/6.functions-and-expressions/4.schema/#for_statements_compatible_with_opencypher","title":"For statements compatible with openCypher","text":"<p>The following functions are available in <code>RETURN</code> and <code>WHERE</code> clauses of openCypher-compatible statements.</p>"},{"location":"3.ngql-guide/6.functions-and-expressions/4.schema/#id","title":"id()","text":"<p>id() returns the ID of a vertex.</p> <p>Syntax: <code>id(&lt;vertex&gt;)</code></p> <ul> <li>Result type: Same as the vertex ID.</li> </ul> <p>Example:</p> <pre><code>nebula&gt; MATCH (v:player) RETURN id(v); \n+-------------+\n| id(v)       |\n+-------------+\n| \"player129\" |\n| \"player115\" |\n| \"player106\" |\n| \"player102\" |\n...\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/4.schema/#tags_and_labels","title":"tags() and labels()","text":"<p>tags() and labels() return the Tag of a vertex.</p> <p>Syntax: <code>tags(&lt;vertex&gt;)</code>, <code>labels(&lt;vertex&gt;)</code></p> <ul> <li>Result type: List</li> </ul> <p>Example:</p> <pre><code>nebula&gt; MATCH (v) WHERE id(v) == \"player100\" \\\n        RETURN tags(v);\n+------------+\n| tags(v)    |\n+------------+\n| [\"player\"] |\n+------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/4.schema/#properties","title":"properties()","text":"<p>properties() returns the properties of a vertex or an edge.</p> <p>Syntax: <code>properties(&lt;vertex_or_edge&gt;)</code></p> <ul> <li>Result type: Map</li> </ul> <p>Example:</p> <pre><code>nebula&gt; MATCH (v:player)-[e:follow]-() RETURN properties(v),properties(e);\n+---------------------------------------+---------------+\n| properties(v)                         | properties(e) |\n+---------------------------------------+---------------+\n| {age: 31, name: \"Stephen Curry\"}      | {degree: 90}  |\n| {age: 47, name: \"Shaquille O'Neal\"}   | {degree: 100} |\n| {age: 34, name: \"LeBron James\"}       | {degree: 13}  |\n...\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/4.schema/#type","title":"type()","text":"<p>type() returns the edge type of an edge.</p> <p>Syntax: <code>type(&lt;edge&gt;)</code></p> <ul> <li>Result type: String</li> </ul> <p>Example:</p> <pre><code>nebula&gt; MATCH (v:player{name:\"Tim Duncan\"})-[e]-&gt;() \\\n        RETURN type(e);\n+----------+\n| type(e)  |\n+----------+\n| \"serve\"  |\n| \"follow\" |\n| \"follow\" |\n+----------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/4.schema/#typeid","title":"typeid()","text":"<p>typeid() returns the internal ID value of the Edge type of the edge, which can be used to determine the direction by positive or negative.</p> <p>Syntax: <code>typeid(&lt;edge&gt;)</code></p> <ul> <li>Result type: Int</li> </ul> <p>Example:</p> <pre><code>nebula&gt; MATCH (v:player)-[e:follow]-(v2) RETURN e,typeid(e), \\\n        CASE WHEN typeid(e) &gt; 0 \\\n        THEN \"Forward\" ELSE \"Reverse\" END AS direction \\\n        LIMIT 5;\n+----------------------------------------------------+-----------+-----------+\n| e                                                  | typeid(e) | direction |\n+----------------------------------------------------+-----------+-----------+\n| [:follow \"player127\"-&gt;\"player114\" @0 {degree: 90}] | 5         | \"Forward\" |\n| [:follow \"player127\"-&gt;\"player148\" @0 {degree: 70}] | 5         | \"Forward\" |\n| [:follow \"player148\"-&gt;\"player127\" @0 {degree: 80}] | -5        | \"Reverse\" |\n| [:follow \"player147\"-&gt;\"player136\" @0 {degree: 90}] | 5         | \"Forward\" |\n| [:follow \"player136\"-&gt;\"player147\" @0 {degree: 90}] | -5        | \"Reverse\" |\n+----------------------------------------------------+-----------+-----------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/4.schema/#src","title":"src()","text":"<p>src() returns the source vertex ID of an edge.</p> <p>Syntax: <code>src(&lt;edge&gt;)</code></p> <ul> <li>Result type: Same as the vertex ID.</li> </ul> <p>Example:</p> <pre><code>nebula&gt; MATCH ()-[e]-&gt;(v:player{name:\"Tim Duncan\"}) \\\n        RETURN src(e);\n+-------------+\n| src(e)      |\n+-------------+\n| \"player125\" |\n| \"player113\" |\n| \"player102\" |\n...\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/4.schema/#dst","title":"dst()","text":"<p>dst() returns the destination vertex ID of an edge.</p> <p>Syntax: <code>dst(&lt;edge&gt;)</code></p> <ul> <li>Result type: Same as the vertex ID.</li> </ul> <p>Example:</p> <pre><code>nebula&gt; MATCH (v:player{name:\"Tim Duncan\"})-[e]-&gt;() \\\n        RETURN dst(e);\n+-------------+\n| dst(e)      |\n+-------------+\n| \"team204\"   |\n| \"player101\" |\n| \"player125\" |\n+-------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/4.schema/#startnode","title":"startNode()","text":"<p>startNode() visits a path and returns its information of source vertex ID, including VIDs, tags, properties, and values.</p> <p>Syntax: <code>startNode(&lt;path&gt;)</code></p> <p>Example:</p> <pre><code>nebula&gt; MATCH p = (a :player {name : \"Tim Duncan\"})-[r:serve]-(t) \\\n        RETURN startNode(p);\n+----------------------------------------------------+\n| startNode(p)                                       |\n+----------------------------------------------------+\n| (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) |\n+----------------------------------------------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/4.schema/#endnode","title":"endNode()","text":"<p>endNode() visits a path and returns its information of destination vertex ID, including VIDs, tags, properties, and values.</p> <p>Syntax: <code>endNode(&lt;path&gt;)</code></p> <p>Example:</p> <pre><code>nebula&gt; MATCH p = (a :player {name : \"Tim Duncan\"})-[r:serve]-(t) \\\n        RETURN endNode(p);\n+----------------------------------+\n| endNode(p)                       |\n+----------------------------------+\n| (\"team204\" :team{name: \"Spurs\"}) |\n+----------------------------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/4.schema/#rank","title":"rank()","text":"<p>rank() returns the rank value of an edge.</p> <p>Syntax: <code>rank(&lt;edge&gt;)</code></p> <ul> <li>Result type: Int</li> </ul> <p>Example:</p> <pre><code>nebula&gt; MATCH (v:player{name:\"Tim Duncan\"})-[e]-&gt;() \\\n        RETURN rank(e);\n+---------+\n| rank(e) |\n+---------+\n| 0       |\n| 0       |\n| 0       |\n+---------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/5.conditional-expressions/","title":"Conditional expressions","text":"<p>This topic describes the conditional functions supported by NebulaGraph.</p>"},{"location":"3.ngql-guide/6.functions-and-expressions/5.conditional-expressions/#case","title":"CASE","text":"<p>The <code>CASE</code> expression uses conditions to filter the parameters. nGQL provides two forms of <code>CASE</code> expressions just like openCypher: the simple form and the generic form.</p> <p>The <code>CASE</code> expression will traverse all the conditions. When the first condition is met, the <code>CASE</code> expression stops reading the conditions and returns the result. If no conditions are met, it returns the result in the <code>ELSE</code> clause. If there is no <code>ELSE</code> clause and no conditions are met, it returns <code>NULL</code>.</p>"},{"location":"3.ngql-guide/6.functions-and-expressions/5.conditional-expressions/#the_simple_form_of_case_expressions","title":"The simple form of CASE expressions","text":"<ul> <li>Syntax</li> </ul> <pre><code>CASE &lt;comparer&gt;\nWHEN &lt;value&gt; THEN &lt;result&gt;\n[WHEN ...]\n[ELSE &lt;default&gt;]\nEND\n</code></pre> <p>Caution</p> <p>Always remember to end the <code>CASE</code> expression with an <code>END</code>.</p> Parameter Description <code>comparer</code> A value or a valid expression that outputs a value. This value is used to compare with the <code>value</code>. <code>value</code> It will be compared with the <code>comparer</code>. If the <code>value</code> matches the <code>comparer</code>, then this condition is met. <code>result</code> The <code>result</code> is returned by the <code>CASE</code> expression if the <code>value</code> matches the <code>comparer</code>. <code>default</code> The <code>default</code> is returned by the <code>CASE</code> expression if no conditions are met. <ul> <li>Examples</li> </ul> <pre><code>nebula&gt; RETURN \\\n        CASE 2+3 \\\n        WHEN 4 THEN 0 \\\n        WHEN 5 THEN 1 \\\n        ELSE -1 \\\n        END \\\n        AS result;\n+--------+\n| result |\n+--------+\n| 1      |\n+--------+\n</code></pre> <pre><code>nebula&gt; GO FROM \"player100\" OVER follow \\\n        YIELD properties($$).name AS Name, \\\n        CASE properties($$).age &gt; 35 \\\n        WHEN true THEN \"Yes\" \\\n        WHEN false THEN \"No\" \\\n        ELSE \"Nah\" \\\n        END \\\n        AS Age_above_35;\n+-----------------+--------------+\n| Name            | Age_above_35 |\n+-----------------+--------------+\n| \"Tony Parker\"   | \"Yes\"        |\n| \"Manu Ginobili\" | \"Yes\"        |\n+-----------------+--------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/5.conditional-expressions/#the_generic_form_of_case_expressions","title":"The generic form of CASE expressions","text":"<ul> <li>Syntax</li> </ul> <pre><code>CASE\nWHEN &lt;condition&gt; THEN &lt;result&gt;\n[WHEN ...]\n[ELSE &lt;default&gt;]\nEND\n</code></pre> Parameter Description <code>condition</code> If the <code>condition</code> is evaluated as true, the <code>result</code> is returned by the <code>CASE</code> expression. <code>result</code> The <code>result</code> is returned by the <code>CASE</code> expression if the <code>condition</code> is evaluated as true. <code>default</code> The <code>default</code> is returned by the <code>CASE</code> expression if no conditions are met. <ul> <li>Examples</li> </ul> <pre><code>nebula&gt; YIELD \\\n        CASE WHEN 4 &gt; 5 THEN 0 \\\n        WHEN 3+4==7 THEN 1 \\\n        ELSE 2 \\\n        END \\\n        AS result;\n+--------+\n| result |\n+--------+\n| 1      |\n+--------+\n</code></pre> <pre><code>nebula&gt; MATCH (v:player) WHERE v.player.age &gt; 30 \\\n        RETURN v.player.name AS Name,  \\\n        CASE \\\n        WHEN v.player.name STARTS WITH \"T\" THEN \"Yes\" \\\n        ELSE \"No\" \\\n        END \\\n        AS Starts_with_T;\n+---------------------+---------------+\n| Name                | Starts_with_T |\n+---------------------+---------------+\n| \"Tim Duncan\"        | \"Yes\"         |\n| \"LaMarcus Aldridge\" | \"No\"          |\n| \"Tony Parker\"       | \"Yes\"         |\n+---------------------+---------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/5.conditional-expressions/#differences_between_the_simple_form_and_the_generic_form","title":"Differences between the simple form and the generic form","text":"<p>To avoid the misuse of the simple form and the generic form, it is important to understand their differences. The following example can help explain them.</p> <pre><code>nebula&gt; GO FROM \"player100\" OVER follow \\\n        YIELD properties($$).name AS Name, properties($$).age AS Age, \\\n        CASE properties($$).age \\\n        WHEN properties($$).age &gt; 35 THEN \"Yes\" \\\n        ELSE \"No\" \\\n        END \\\n        AS Age_above_35;\n+-----------------+-----+--------------+\n| Name            | Age | Age_above_35 |\n+-----------------+-----+--------------+\n| \"Tony Parker\"   | 36  | \"No\"         |\n| \"Manu Ginobili\" | 41  | \"No\"         |\n+-----------------+-----+--------------+\n</code></pre> <p>The preceding <code>GO</code> query is intended to output <code>Yes</code> when the player's age is above 35. However, in this example, when the player's age is 36, the actual output is not as expected: It is <code>No</code> instead of <code>Yes</code>.</p> <p>This is because the query uses the <code>CASE</code> expression in the simple form, and a comparison between the values of <code>$$.player.age</code> and <code>$$.player.age &gt; 35</code> is made. When the player age is 36:</p> <ul> <li>The value of <code>$$.player.age</code> is <code>36</code>. It is an integer.</li> </ul> <ul> <li><code>$$.player.age &gt; 35</code> is evaluated to be <code>true</code>. It is a boolean.</li> </ul> <p>The values of <code>$$.player.age</code> and <code>$$.player.age &gt; 35</code> do not match. Therefore, the condition is not met and <code>No</code> is returned.</p>"},{"location":"3.ngql-guide/6.functions-and-expressions/5.conditional-expressions/#coalesce","title":"coalesce()","text":"<p>coalesce() returns the first not null value in all expressions.</p> <p>Syntax: <code>coalesce(&lt;expression_1&gt;[,&lt;expression_2&gt;...])</code></p> <ul> <li>Result type: Same as the original element.</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN coalesce(null,[1,2,3]) as result;\n+-----------+\n| result    |\n+-----------+\n| [1, 2, 3] |\n+-----------+\nnebula&gt; RETURN coalesce(null) as result;\n+----------+\n| result   |\n+----------+\n| __NULL__ |\n+----------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/6.list/","title":"List functions","text":"<p>This topic describes the list functions supported by NebulaGraph. Some of the functions have different syntax in native nGQL statements and openCypher-compatible statements.</p>"},{"location":"3.ngql-guide/6.functions-and-expressions/6.list/#precautions","title":"Precautions","text":"<p>Like SQL, the position index in nGQL starts from <code>1</code>, while in the C language it starts from <code>0</code>.</p>"},{"location":"3.ngql-guide/6.functions-and-expressions/6.list/#general","title":"General","text":""},{"location":"3.ngql-guide/6.functions-and-expressions/6.list/#range","title":"range()","text":"<p>range() returns the list containing all the fixed-length steps in <code>[start,end]</code>.</p> <p>Syntax: <code>range(start, end [, step])</code></p> <ul> <li><code>step</code>: Optional parameters. <code>step</code> is 1 by default.</li> </ul> <ul> <li>Result type: List</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN range(1,9,2);\n+-----------------+\n| range(1,9,2)    |\n+-----------------+\n| [1, 3, 5, 7, 9] |\n+-----------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/6.list/#reverse","title":"reverse()","text":"<p>reverse() returns the list reversing the order of all elements in the original list.</p> <p>Syntax: <code>reverse(&lt;list&gt;)</code></p> <ul> <li>Result type: List</li> </ul> <p>Example:</p> <pre><code>nebula&gt; WITH [NULL, 4923, 'abc', 521, 487] AS ids \\\n        RETURN reverse(ids);\n+-----------------------------------+\n| reverse(ids)                      |\n+-----------------------------------+\n| [487, 521, \"abc\", 4923, __NULL__] |\n+-----------------------------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/6.list/#tail","title":"tail()","text":"<p>tail() returns all the elements of the original list, excluding the first one.</p> <p>Syntax: <code>tail(&lt;list&gt;)</code></p> <ul> <li>Result type: List</li> </ul> <p>Example:</p> <pre><code>nebula&gt; WITH [NULL, 4923, 'abc', 521, 487] AS ids \\\n        RETURN tail(ids);\n+-------------------------+\n| tail(ids)               |\n+-------------------------+\n| [4923, \"abc\", 521, 487] |\n+-------------------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/6.list/#head","title":"head()","text":"<p>head() returns the first element of a list.</p> <p>Syntax: <code>head(&lt;list&gt;)</code></p> <ul> <li>Result type: Same as the element in the original list.</li> </ul> <p>Example:</p> <pre><code>nebula&gt; WITH [NULL, 4923, 'abc', 521, 487] AS ids \\\n        RETURN head(ids);\n+-----------+\n| head(ids) |\n+-----------+\n| __NULL__  |\n+-----------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/6.list/#last","title":"last()","text":"<p>last() returns the last element of a list.</p> <p>Syntax: <code>last(&lt;list&gt;)</code></p> <ul> <li>Result type: Same as the element in the original list.</li> </ul> <p>Example:</p> <pre><code>nebula&gt; WITH [NULL, 4923, 'abc', 521, 487] AS ids \\\n        RETURN last(ids);\n+-----------+\n| last(ids) |\n+-----------+\n| 487       |\n+-----------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/6.list/#reduce","title":"reduce()","text":"<p>reduce() applies an expression to each element in a list one by one, chains the result to the next iteration by taking it as the initial value, and returns the final result. This function iterates each element <code>e</code> in the given list, runs the expression on <code>e</code>, accumulates the result with the initial value, and store the new result in the accumulator as the initial value of the next iteration. It works like the fold or reduce method in functional languages such as Lisp and Scala.</p> <p>openCypher compatibility</p> <p>In openCypher, the <code>reduce()</code> function is not defined. nGQL will implement the <code>reduce()</code> function in the Cypher way.</p> <p>Syntax: <code>reduce(&lt;accumulator&gt; = &lt;initial&gt;, &lt;variable&gt; IN &lt;list&gt; | &lt;expression&gt;)</code></p> <ul> <li><code>accumulator</code>: A variable that will hold the accumulated results as the list is iterated.</li> </ul> <ul> <li><code>initial</code>: An expression that runs once to give an initial value to the <code>accumulator</code>.</li> </ul> <ul> <li><code>variable</code>: A variable in the list that will be applied to the expression successively.</li> </ul> <ul> <li><code>list</code>: A list or a list of expressions.</li> </ul> <ul> <li><code>expression</code>: This expression will be run on each element in the list once and store the result value in the <code>accumulator</code>.</li> </ul> <ul> <li>Result type: Depends on the parameters provided, along with the semantics of the expression.</li> </ul> <p>Example:</p> <pre><code>nebula&gt; RETURN reduce(totalNum = -4 * 5, n IN [1, 2] | totalNum + n * 2) AS r;\n+-----+\n| r   |\n+-----+\n| -14 |\n+-----+\n\nnebula&gt; MATCH p = (n:player{name:\"LeBron James\"})&lt;-[:follow]-(m) \\\n        RETURN  nodes(p)[0].player.age AS src1,  nodes(p)[1].player.age AS dst2,  \\\n        reduce(totalAge = 100, n IN nodes(p) | totalAge + n.player.age) AS sum;\n+------+------+-----+\n| src1 | dst2 | sum |\n+------+------+-----+\n| 34   | 31   | 165 |\n| 34   | 29   | 163 |\n| 34   | 33   | 167 |\n| 34   | 26   | 160 |\n| 34   | 34   | 168 |\n| 34   | 37   | 171 |\n+------+------+-----+\n\nnebula&gt; LOOKUP ON player WHERE player.name == \"Tony Parker\" YIELD id(vertex) AS VertexID \\\n        |  GO FROM $-.VertexID over follow \\\n        WHERE properties(edge).degree != reduce(totalNum = 5, n IN range(1, 3) | properties($$).age + totalNum + n) \\\n        YIELD properties($$).name AS id, properties($$).age AS age, properties(edge).degree AS degree;\n+---------------------+-----+--------+\n| id                  | age | degree |\n+---------------------+-----+--------+\n| \"Tim Duncan\"        | 42  | 95     |\n| \"LaMarcus Aldridge\" | 33  | 90     |\n| \"Manu Ginobili\"     | 41  | 95     |\n+---------------------+-----+--------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/6.list/#for_ngql_statements","title":"For nGQL statements","text":""},{"location":"3.ngql-guide/6.functions-and-expressions/6.list/#keys","title":"keys()","text":"<p>keys() returns a list containing the string representations for all the property names of vertices or edges.</p> <p>Syntax: <code>keys({vertex | edge})</code></p> <ul> <li>Result type: List</li> </ul> <p>Example:</p> <pre><code>nebula&gt; LOOKUP ON player \\\n        WHERE player.age  &gt; 45 \\\n        YIELD keys(vertex);\n+-----------------+\n| keys(VERTEX)    |\n+-----------------+\n| [\"age\", \"name\"] |\n| [\"age\", \"name\"] |\n+-----------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/6.list/#labels","title":"labels()","text":"<p>labels() returns the list containing all the tags of a vertex.</p> <p>Syntax: <code>labels(verte)</code></p> <ul> <li>Result type: List</li> </ul> <p>Example:</p> <pre><code>nebula&gt; FETCH PROP ON * \"player101\", \"player102\", \"team204\" \\\n        YIELD labels(vertex);\n+----------------+\n| labels(VERTEX) |\n+----------------+\n| [\"player\"]     |\n| [\"player\"]     |\n| [\"team\"]       |\n+----------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/6.list/#for_statements_compatible_with_opencypher","title":"For statements compatible with openCypher","text":""},{"location":"3.ngql-guide/6.functions-and-expressions/6.list/#keys_1","title":"keys()","text":"<p>keys() returns a list containing the string representations for all the property names of vertices, edges, or maps.</p> <p>Syntax: <code>keys(&lt;vertex_or_edge&gt;)</code></p> <ul> <li>Result type: List</li> </ul> <p>Example:</p> <pre><code>nebula&gt; MATCH (v:player{name:\"Tim Duncan\"})-[e]-&gt;() \\\n        RETURN keys(e);\n+----------------------------+\n| keys(e)                    |\n+----------------------------+\n| [\"end_year\", \"start_year\"] |\n| [\"degree\"]                 |\n| [\"degree\"]                 |\n+----------------------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/6.list/#labels_1","title":"labels()","text":"<p>labels() returns the list containing all the tags of a vertex.</p> <p>Syntax: <code>labels(&lt;vertex&gt;)</code></p> <ul> <li>Result type: List</li> </ul> <p>Example:</p> <pre><code>nebula&gt; MATCH (v)-[e:serve]-&gt;() \\\n        WHERE id(v)==\"player100\" \\\n        RETURN labels(v);\n+------------+\n| labels(v)  |\n+------------+\n| [\"player\"] |\n+------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/6.list/#nodes","title":"nodes()","text":"<p>nodes() returns the list containing all the vertices in a path.</p> <p>Syntax: <code>nodes(&lt;path&gt;)</code></p> <ul> <li>Result type: List</li> </ul> <p>Example:</p> <pre><code>nebula&gt; MATCH p=(v:player{name:\"Tim Duncan\"})--&gt;(v2) \\\n        RETURN nodes(p);\n+-------------------------------------------------------------------------------------------------------------+\n| nodes(p)                                                                                                    |\n+-------------------------------------------------------------------------------------------------------------+\n| [(\"player100\" :player{age: 42, name: \"Tim Duncan\"}), (\"team204\" :team{name: \"Spurs\"})]                      |\n| [(\"player100\" :player{age: 42, name: \"Tim Duncan\"}), (\"player101\" :player{age: 36, name: \"Tony Parker\"})]   |\n| [(\"player100\" :player{age: 42, name: \"Tim Duncan\"}), (\"player125\" :player{age: 41, name: \"Manu Ginobili\"})] |\n+-------------------------------------------------------------------------------------------------------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/6.list/#relationships","title":"relationships()","text":"<p>relationships() returns the list containing all the relationships in a path.</p> <p>Syntax: <code>relationships(&lt;path&gt;)</code></p> <ul> <li>Result type: List</li> </ul> <p>Example:</p> <pre><code>nebula&gt; MATCH p=(v:player{name:\"Tim Duncan\"})--&gt;(v2) \\\n        RETURN relationships(p);\n+-------------------------------------------------------------------------+\n| relationships(p)                                                        |\n+-------------------------------------------------------------------------+\n| [[:serve \"player100\"-&gt;\"team204\" @0 {end_year: 2016, start_year: 1997}]] |\n| [[:follow \"player100\"-&gt;\"player101\" @0 {degree: 95}]]                    |\n| [[:follow \"player100\"-&gt;\"player125\" @0 {degree: 95}]]                    |\n+-------------------------------------------------------------------------+\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/8.predicate/","title":"Predicate functions","text":"<p>Predicate functions return <code>true</code> or <code>false</code>. They are most commonly used in <code>WHERE</code> clauses.</p> <p>NebulaGraph supports the following predicate functions:</p> Functions Description exists() Returns <code>true</code> if the specified property exists in the vertex, edge or map. Otherwise, returns <code>false</code>. any() Returns <code>true</code> if the specified predicate holds for at least one element in the given list. Otherwise, returns <code>false</code>. all() Returns <code>true</code> if the specified predicate holds for all elements in the given list. Otherwise, returns <code>false</code>. none() Returns <code>true</code> if the specified predicate holds for no element in the given list. Otherwise, returns <code>false</code>. single() Returns <code>true</code> if the specified predicate holds for exactly one of the elements in the given list. Otherwise, returns <code>false</code>. <p>Note</p> <p>NULL is returned if the list is NULL or all of its elements are NULL.</p> <p>Compatibility</p> <p>In openCypher, only function <code>exists()</code> is defined and specified. The other functions are implement-dependent.</p>"},{"location":"3.ngql-guide/6.functions-and-expressions/8.predicate/#syntax","title":"Syntax","text":"<pre><code>&lt;predicate&gt;(&lt;variable&gt; IN &lt;list&gt; WHERE &lt;condition&gt;)\n</code></pre>"},{"location":"3.ngql-guide/6.functions-and-expressions/8.predicate/#examples","title":"Examples","text":"<pre><code>nebula&gt; RETURN any(n IN [1, 2, 3, 4, 5, NULL] \\\n        WHERE n &gt; 2) AS r;\n+------+\n| r    |\n+------+\n| true |\n+------+\n\nnebula&gt; RETURN single(n IN range(1, 5) \\\n        WHERE n == 3) AS r;\n+------+\n| r    |\n+------+\n| true |\n+------+\n\nnebula&gt; RETURN none(n IN range(1, 3) \\\n        WHERE n == 0) AS r;\n+------+\n| r    |\n+------+\n| true |\n+------+\n\nnebula&gt; WITH [1, 2, 3, 4, 5, NULL] AS a \\\n        RETURN any(n IN a WHERE n &gt; 2);\n+-------------------------+\n| any(n IN a WHERE (n&gt;2)) |\n+-------------------------+\n| true                    |\n+-------------------------+\n\nnebula&gt; MATCH p = (n:player{name:\"LeBron James\"})&lt;-[:follow]-(m) \\\n        RETURN nodes(p)[0].player.name AS n1, nodes(p)[1].player.name AS n2, \\\n        all(n IN nodes(p) WHERE n.player.name NOT STARTS WITH \"D\") AS b;\n+----------------+-------------------+-------+\n| n1             | n2                | b     |\n+----------------+-------------------+-------+\n| \"LeBron James\" | \"Danny Green\"     | false |\n| \"LeBron James\" | \"Dejounte Murray\" | false |\n| \"LeBron James\" | \"Chris Paul\"      | true  |\n| \"LeBron James\" | \"Kyrie Irving\"    | true  |\n| \"LeBron James\" | \"Carmelo Anthony\" | true  |\n| \"LeBron James\" | \"Dwyane Wade\"     | false |\n+----------------+-------------------+-------+\n\nnebula&gt; MATCH p = (n:player{name:\"LeBron James\"})-[:follow]-&gt;(m) \\\n        RETURN single(n IN nodes(p) WHERE n.player.age &gt; 40) AS b;\n+------+\n| b    |\n+------+\n| true |\n+------+\n\nnebula&gt; MATCH (n:player) \\\n        RETURN exists(n.player.id), n IS NOT NULL;\n+---------------------+---------------+\n| exists(n.player.id) | n IS NOT NULL |\n+---------------------+---------------+\n| false               | true          |\n...\n\nnebula&gt; MATCH (n:player) \\\n        WHERE exists(n['name']) RETURN n;\n+-------------------------------------------------------------------------------------------------------------+\n| n                                                                                                           |\n+-------------------------------------------------------------------------------------------------------------+\n| (\"Grant Hill\" :player{age: 46, name: \"Grant Hill\"})                                                         |\n| (\"Marc Gasol\" :player{age: 34, name: \"Marc Gasol\"})                                                         |\n+-------------------------------------------------------------------------------------------------------------+\n...\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/2.match/","title":"MATCH","text":"<p>The <code>MATCH</code> statement provides pattern-based search functionality, allowing you to retrieve data that matches one or more patterns in NebulaGraph. By defining one or more patterns, you can search for data that matches the patterns in NebulaGraph. Once the matching data is retrieved, you can use the <code>RETURN</code> clause to return it as a result.</p> <p>The examples in this topic use the basketballplayer dataset as the sample dataset.</p>"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#syntax","title":"Syntax","text":"<p>The syntax of <code>MATCH</code> is relatively more flexible compared with that of other query statements such as <code>GO</code> or <code>LOOKUP</code>. The path type of the <code>MATCH</code> statement is <code>trail</code>. That is, only vertices can be repeatedly visited in the graph traversal. Edges cannot be repeatedly visited. For details, see path. But generally, it can be summarized as follows.</p> <pre><code>MATCH &lt;pattern&gt; [&lt;clause_1&gt;] RETURN &lt;output&gt; [&lt;clause_2&gt;];\n</code></pre> <ul> <li><code>pattern</code>: The <code>MATCH</code> statement supports matching one or multiple patterns. Multiple patterns are separated by commas (,). For example: <code>(a)-[]-&gt;(b),(c)-[]-&gt;(d)</code>. For the detailed description of patterns, see Patterns. </li> </ul> <ul> <li><code>clause_1</code>: The <code>WHERE</code>, <code>WITH</code>, <code>UNWIND</code>, and <code>OPTIONAL MATCH</code> clauses are supported, and the <code>MATCH</code> clause can also be used.</li> </ul> <ul> <li><code>output</code>: Define the list name for the output results to be returned. You can use <code>AS</code> to set an alias for the list.</li> </ul> <ul> <li><code>clause_2</code>: The <code>ORDER BY</code> and <code>LIMIT</code> clauses are supported.</li> </ul> <p>Legacy version compatibility</p> <ul> <li>Starting from version 3.5.0, the <code>MATCH</code> statement supports full table scans. It can traverse vertices or edges in the graph without using any indexes or filter conditions. In previous versions, the <code>MATCH</code> statement required an index for certain queries or needed to use <code>LIMIT</code> to restrict the number of output results.</li> </ul> <ul> <li>Starting from NebulaGraph version 3.0.0, in order to distinguish the properties of different tags, you need to specify a tag name when querying properties. The original statement <code>RETURN &lt;variable_name&gt;.&lt;property_name&gt;</code> is changed to <code>RETURN &lt;variable_name&gt;.&lt;tag_name&gt;.&lt;property_name&gt;</code>.</li> </ul>"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#notes","title":"Notes","text":"<ul> <li>Avoid full table scans, as they may result in decreased query performance, and if there is insufficient memory during a full table scan, the query may fail, and the system will report an error. It is recommended to use queries with filter conditions or specifying tags and edge types, such as <code>v:player</code> and <code>v.player.name</code> in the statement <code>MATCH (v:player) RETURN v.player.name AS Name</code>.</li> </ul> <ul> <li>You can create an index for a tag, edge type, or a specific property of a tag or edge type to improve query performance. For example, you can create an index for the <code>player</code> tag or the name property of the <code>player</code> tag. For more information about the usage and considerations for indexes, see Must-read for using indexes.</li> </ul> <ul> <li>The <code>MATCH</code> statement cannot query dangling edges.</li> </ul>"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#using_patterns_in_match_statements","title":"Using patterns in MATCH statements","text":""},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_vertices","title":"Match vertices","text":"<p>You can use a user-defined variable in a pair of parentheses to represent a vertex in a pattern. For example: <code>(v)</code>.</p> <pre><code>nebula&gt; MATCH (v) \\\n        RETURN v \\\n        LIMIT 3;\n+-----------------------------------------------------------+\n| v                                                         |\n+-----------------------------------------------------------+\n| (\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"}) |\n| (\"player106\" :player{age: 25, name: \"Kyle Anderson\"})     |\n| (\"player115\" :player{age: 40, name: \"Kobe Bryant\"})       |\n+-----------------------------------------------------------+\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_tags","title":"Match tags","text":"<p>Legacy version compatibility</p> <ul> <li>In NebulaGraph versions earlier than 3.0.0, the prerequisite for matching a tag is that the tag itself has an index or a certain property of the tag has an index. </li> </ul> <ul> <li>Starting from NebulaGraph 3.0.0, you can match tags without creating an index, but you need to use <code>LIMIT</code> to restrict the number of output results.</li> </ul> <ul> <li>Starting from NebulaGraph 3.5.0, the <code>MATCH</code> statement supports full table scans. There is no need to create an index for a tag or a specific property of a tag, nor use <code>LIMIT</code> to restrict the number of output results in order to execute the <code>MATCH</code> statement.</li> </ul> <p>You can specify a tag with <code>:&lt;tag_name&gt;</code> after the vertex in a pattern.</p> <pre><code>nebula&gt; MATCH (v:player) \\\n        RETURN v;\n+---------------------------------------------------------------+\n| v                                                             |\n+---------------------------------------------------------------+\n| (\"player105\" :player{age: 31, name: \"Danny Green\"})           |\n| (\"player109\" :player{age: 34, name: \"Tiago Splitter\"})        |\n| (\"player111\" :player{age: 38, name: \"David West\"})            |\n...\n</code></pre> <p>To match vertices with multiple tags, use colons (:).</p> <pre><code>nebula&gt; CREATE TAG actor (name string, age int);\nnebula&gt; INSERT VERTEX actor(name, age) VALUES \"player100\":(\"Tim Duncan\", 42);\nnebula&gt; MATCH (v:player:actor) \\\n        RETURN v \\\n+----------------------------------------------------------------------------------------+\n| v                                                                                      |\n+----------------------------------------------------------------------------------------+\n| (\"player100\" :actor{age: 42, name: \"Tim Duncan\"} :player{age: 42, name: \"Tim Duncan\"}) |\n+----------------------------------------------------------------------------------------+\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_vertex_properties","title":"Match vertex properties","text":"<p>Note</p> <p>The prerequisite for matching a vertex property is that the tag itself has an index of the corresponding property. Otherwise, you cannot execute the <code>MATCH</code> statement to match the property.</p> <p>You can specify a vertex property with <code>{&lt;prop_name&gt;: &lt;prop_value&gt;}</code> after the tag in a pattern.</p> <pre><code># The following example uses the name property to match a vertex.\nnebula&gt; MATCH (v:player{name:\"Tim Duncan\"}) \\\n        RETURN v;\n+----------------------------------------------------+\n| v                                                  |\n+----------------------------------------------------+\n| (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) |\n+----------------------------------------------------+\n</code></pre> <p>The <code>WHERE</code> clause can do the same thing:</p> <pre><code>nebula&gt; MATCH (v:player) \\\n        WHERE v.player.name == \"Tim Duncan\" \\\n        RETURN v;\n+----------------------------------------------------+\n| v                                                  |\n+----------------------------------------------------+\n| (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) |\n+----------------------------------------------------+\n</code></pre> <p>OpenCypher compatibility</p> <p>In openCypher 9, <code>=</code> is the equality operator. However, in nGQL, <code>==</code> is the equality operator and <code>=</code> is the assignment operator (as in C++ or Java).</p> <p>Use the <code>WHERE</code> clause to directly get all the vertices with the vertex property value Tim Duncan.</p> <pre><code>nebula&gt; MATCH (v) \\\n        WITH v, properties(v) as props, keys(properties(v)) as kk \\\n        WHERE [i in kk where props[i] == \"Tim Duncan\"] \\\n        RETURN v;\n+----------------------------------------------------+\n| v                                                  |\n+----------------------------------------------------+\n| (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) |\n+----------------------------------------------------+\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_vids","title":"Match VIDs","text":"<p>You can use the VID to match a vertex. The <code>id()</code> function can retrieve the VID of a vertex.</p> <pre><code>nebula&gt; MATCH (v) \\\n        WHERE id(v) == 'player101' \\\n        RETURN v;\n+-----------------------------------------------------+\n| v                                                   |\n+-----------------------------------------------------+\n| (\"player101\" :player{age: 36, name: \"Tony Parker\"}) |\n+-----------------------------------------------------+\n</code></pre> <p>To match multiple VIDs, use <code>WHERE id(v) IN [vid_list]</code> or <code>WHERE id(v) IN {vid_list}</code>.</p> <pre><code>nebula&gt; MATCH (v:player { name: 'Tim Duncan' })--(v2) \\\n        WHERE id(v2) IN [\"player101\", \"player102\"] \\\n        RETURN v2;\n+-----------------------------------------------------------+\n| v2                                                        |\n+-----------------------------------------------------------+\n| (\"player101\" :player{age: 36, name: \"Tony Parker\"})       |\n| (\"player101\" :player{age: 36, name: \"Tony Parker\"})       |\n| (\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"}) |\n+-----------------------------------------------------------+\n\nnebula&gt; MATCH (v) WHERE id(v) IN {\"player100\", \"player101\"} \\\n        RETURN v.player.name AS name;\n+---------------+\n| name          |\n+---------------+\n| \"Tony Parker\" |\n| \"Tim Duncan\"  |\n+---------------+\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_connected_vertices","title":"Match connected vertices","text":"<p>You can use the <code>--</code> symbol to represent edges of both directions and match vertices connected by these edges.</p> <p>Legacy version compatibility</p> <p>In nGQL 1.x, the <code>--</code> symbol is used for inline comments. Starting from nGQL 2.x, the <code>--</code> symbol represents an incoming or outgoing edge.</p> <pre><code>nebula&gt; MATCH (v:player{name:\"Tim Duncan\"})--(v2) \\\n        RETURN v2.player.name AS Name;\n+---------------------+\n| Name                |\n+---------------------+\n| \"Manu Ginobili\"     |\n| \"Manu Ginobili\"     |\n| \"Tiago Splitter\"    |\n...\n</code></pre> <p>You can add a <code>&gt;</code> or <code>&lt;</code> to the <code>--</code> symbol to specify the direction of an edge.</p> <p>In the following example, <code>--&gt;</code> represents an edge that starts from <code>v</code> and points to <code>v2</code>. To <code>v</code>, this is an outgoing edge, and to <code>v2</code> this is an incoming edge.</p> <pre><code>nebula&gt; MATCH (v:player{name:\"Tim Duncan\"})--&gt;(v2:player) \\\n        RETURN v2.player.name AS Name;\n+-----------------+\n| Name            |\n+-----------------+\n| \"Manu Ginobili\" |\n| \"Tony Parker\"   |\n+-----------------+\n</code></pre> <p>To query the properties of the target vertices, use the <code>CASE</code> expression.</p> <pre><code>nebula&gt; MATCH (v:player{name:\"Tim Duncan\"})--(v2) \\\n        RETURN \\\n        CASE WHEN v2.team.name IS NOT NULL \\\n        THEN v2.team.name  \\\n        WHEN v2.player.name IS NOT NULL \\\n        THEN v2.player.name END AS Name;\n+---------------------+\n| Name                |\n+---------------------+\n| \"Manu Ginobili\"     |\n| \"Manu Ginobili\"     |\n| \"Spurs\"             |\n| \"Dejounte Murray\"   |\n...\n</code></pre> <p>To extend the pattern, you can add more vertices and edges.</p> <pre><code>nebula&gt; MATCH (v:player{name:\"Tim Duncan\"})--&gt;(v2)&lt;--(v3) \\\n        RETURN v3.player.name AS Name;\n+---------------------+\n| Name                |\n+---------------------+\n| \"Dejounte Murray\"   |\n| \"LaMarcus Aldridge\" |\n| \"Marco Belinelli\"   |\n...\n</code></pre> <p>If you do not need to refer to a vertex, you can omit the variable representing it in the parentheses.</p> <pre><code>nebula&gt; MATCH (v:player{name:\"Tim Duncan\"})--&gt;()&lt;--(v3) \\\n        RETURN v3.player.name AS Name;\n+---------------------+\n| Name                |\n+---------------------+\n| \"Dejounte Murray\"   |\n| \"LaMarcus Aldridge\" |\n| \"Marco Belinelli\"   |\n...\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_paths","title":"Match paths","text":"<p>Connected vertices and edges form a path. You can use a user-defined variable to name a path as follows.</p> <pre><code>nebula&gt; MATCH p=(v:player{name:\"Tim Duncan\"})--&gt;(v2) \\\n        RETURN p;\n+--------------------------------------------------------------------------------------------------------------------------------------+\n| p                                                                                                                                    |\n+--------------------------------------------------------------------------------------------------------------------------------------+\n| &lt;(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:serve@0 {end_year: 2016, start_year: 1997}]-&gt;(\"team204\" :team{name: \"Spurs\"})&gt; |\n| &lt;(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]-&gt;(\"player101\" :player{age: 36, name: \"Tony Parker\"})&gt;   |\n| &lt;(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]-&gt;(\"player125\" :player{age: 41, name: \"Manu Ginobili\"})&gt; |\n+--------------------------------------------------------------------------------------------------------------------------------------+\n</code></pre> <p>OpenCypher compatibility</p> <p>In nGQL, the <code>@</code> symbol represents the rank of an edge, but openCypher has no such concept.</p>"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_edges","title":"Match edges","text":"<pre><code>nebula&gt; MATCH ()&lt;-[e]-() \\\n        RETURN e \\\n        LIMIT 3;\n+----------------------------------------------------+\n| e                                                  |\n+----------------------------------------------------+\n| [:follow \"player101\"-&gt;\"player102\" @0 {degree: 90}] |\n| [:follow \"player103\"-&gt;\"player102\" @0 {degree: 70}] |\n| [:follow \"player135\"-&gt;\"player102\" @0 {degree: 80}] |\n+----------------------------------------------------+\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_edge_types","title":"Match edge types","text":"<p>Just like vertices, you can specify edge types with <code>:&lt;edge_type&gt;</code> in a pattern. For example: <code>-[e:follow]-</code>.</p> <p>OpenCypher compatibility</p> <ul> <li>In NebulaGraph versions earlier than 3.0.0, the prerequisite for matching a edge type is that the edge type itself has an index or a certain property of the edge type has an index. </li> </ul> <ul> <li>Starting from version 3.0.0, there is no need to create an index for matching a edge type, but you need to use <code>LIMIT</code> to limit the number of output results and you must specify the direction of the edge.</li> </ul> <ul> <li>Starting from NebulaGraph 3.5.0, you can use the <code>MATCH</code> statement to match edges without creating an index for edge type or using <code>LIMIT</code> to restrict the number of output results.</li> </ul> <pre><code>nebula&gt; MATCH ()-[e:follow]-&gt;() \\\n        RETURN e;\n+----------------------------------------------------+\n| e                                                  |\n+----------------------------------------------------+\n| [:follow \"player102\"-&gt;\"player100\" @0 {degree: 75}] |\n| [:follow \"player102\"-&gt;\"player101\" @0 {degree: 75}] |\n| [:follow \"player129\"-&gt;\"player116\" @0 {degree: 90}] |\n...\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_edge_type_properties","title":"Match edge type properties","text":"<p>Note</p> <p>The prerequisite for matching an edge type property is that the edge type itself has an index of the corresponding property. Otherwise, you cannot execute the <code>MATCH</code> statement to match the property.</p> <p>You can specify edge type properties with <code>{&lt;prop_name&gt;: &lt;prop_value&gt;}</code> in a pattern. For example: <code>[e:follow{likeness:95}]</code>.</p> <pre><code>nebula&gt; MATCH (v:player{name:\"Tim Duncan\"})-[e:follow{degree:95}]-&gt;(v2) \\\n        RETURN e;\n+--------------------------------------------------------+\n| e                                                      |\n+--------------------------------------------------------+\n| [:follow \"player100\"-&gt;\"player101\" @0 {degree: 95}]     |\n| [:follow \"player100\"-&gt;\"player125\" @0 {degree: 95}]     |\n+--------------------------------------------------------+\n</code></pre> <p>Use the <code>WHERE</code> clause to directly get all the edges with the edge property value 90.</p> <pre><code>nebula&gt; MATCH ()-[e]-&gt;() \\\n        WITH e, properties(e) as props, keys(properties(e)) as kk \\\n        WHERE [i in kk where props[i] == 90] \\\n        RETURN e;\n+----------------------------------------------------+\n| e                                                  |\n+----------------------------------------------------+\n| [:follow \"player125\"-&gt;\"player100\" @0 {degree: 90}] |\n| [:follow \"player140\"-&gt;\"player114\" @0 {degree: 90}] |\n| [:follow \"player133\"-&gt;\"player144\" @0 {degree: 90}] |\n| [:follow \"player133\"-&gt;\"player114\" @0 {degree: 90}] |\n...\n+----------------------------------------------------+\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_multiple_edge_types","title":"Match multiple edge types","text":"<p>The <code>|</code> symbol can help matching multiple edge types. For example: <code>[e:follow|:serve]</code>. The English colon (:) before the first edge type cannot be omitted, but the English colon before the subsequent edge type can be omitted, such as <code>[e:follow|serve]</code>.</p> <pre><code>nebula&gt; MATCH (v:player{name:\"Tim Duncan\"})-[e:follow|:serve]-&gt;(v2) \\\n        RETURN e;\n+---------------------------------------------------------------------------+\n| e                                                                         |\n+---------------------------------------------------------------------------+\n| [:follow \"player100\"-&gt;\"player101\" @0 {degree: 95}]                        |\n| [:follow \"player100\"-&gt;\"player125\" @0 {degree: 95}]                        |\n| [:serve \"player100\"-&gt;\"team204\" @0 {end_year: 2016, start_year: 1997}]     |\n+---------------------------------------------------------------------------+\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_multiple_edges","title":"Match multiple edges","text":"<p>You can extend a pattern to match multiple edges in a path.</p> <pre><code>nebula&gt; MATCH (v:player{name:\"Tim Duncan\"})-[]-&gt;(v2)&lt;-[e:serve]-(v3) \\\n        RETURN v2, v3;\n+----------------------------------+-----------------------------------------------------------+\n| v2                               | v3                                                        |\n+----------------------------------+-----------------------------------------------------------+\n| (\"team204\" :team{name: \"Spurs\"}) | (\"player104\" :player{age: 32, name: \"Marco Belinelli\"})   |\n| (\"team204\" :team{name: \"Spurs\"}) | (\"player101\" :player{age: 36, name: \"Tony Parker\"})       |\n| (\"team204\" :team{name: \"Spurs\"}) | (\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"}) |\n...\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_fixed-length_paths","title":"Match fixed-length paths","text":"<p>You can use the <code>:&lt;edge_type&gt;*&lt;hop&gt;</code> pattern to match a fixed-length path. <code>hop</code> must be a non-negative integer.</p> <pre><code>nebula&gt; MATCH p=(v:player{name:\"Tim Duncan\"})-[e:follow*2]-&gt;(v2) \\\n        RETURN DISTINCT v2 AS Friends;\n+-----------------------------------------------------------+\n| Friends                                                   |\n+-----------------------------------------------------------+\n| (\"player100\" :player{age: 42, name: \"Tim Duncan\"})        |\n| (\"player125\" :player{age: 41, name: \"Manu Ginobili\"})     |\n| (\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"}) |\n+-----------------------------------------------------------+\n</code></pre> <p>If <code>hop</code> is 0, the pattern will match the source vertex of the path.</p> <pre><code>nebula&gt; MATCH (v:player{name:\"Tim Duncan\"}) -[*0]-&gt; (v2) \\\n        RETURN v2;\n+----------------------------------------------------+\n| v2                                                 |\n+----------------------------------------------------+\n| (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) |\n+----------------------------------------------------+\n</code></pre> <p>Note</p> <p>When you conditionally filter on multi-hop edges, such as <code>-[e:follow*2]-&gt;</code>, note that the <code>e</code> is a list of edges instead of a single edge. </p> <p>For example, the following statement is correct from the syntax point of view which may not get your expected query result, because the <code>e</code> is a list without the <code>.degree</code> property. </p> <pre><code>nebula&gt; MATCH p=(v:player{name:\"Tim Duncan\"})-[e:follow*2]-&gt;(v2) \\\n        WHERE e.degree &gt; 1 \\\n        RETURN DISTINCT v2 AS Friends;\n</code></pre> <p>The correct statement is as follows:</p> <pre><code>nebula&gt; MATCH p=(v:player{name:\"Tim Duncan\"})-[e:follow*2]-&gt;(v2) \\\n        WHERE ALL(e_ in e WHERE e_.degree &gt; 0) \\\n        RETURN DISTINCT v2 AS Friends;\n</code></pre> <p>Further, the following statement is for filtering the properties of the first-hop edge in multi-hop edges:</p> <pre><code>nebula&gt; MATCH p=(v:player{name:\"Tim Duncan\"})-[e:follow*2]-&gt;(v2) \\\n        WHERE e[0].degree &gt; 98 \\\n        RETURN DISTINCT v2 AS Friends;\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_variable-length_paths","title":"Match variable-length paths","text":"<p>You can use the <code>:&lt;edge_type&gt;*[minHop..maxHop]</code> pattern to match variable-length paths.<code>minHop</code> and <code>maxHop</code> are optional and default to 1 and infinity respectively.</p> <p>Note</p> <p>When setting bounds, at least one of <code>minHop</code> and <code>maxHop</code> exists.</p> <p>Caution</p> <p>If <code>maxHop</code> is not set, it may cause the Graph service to OOM, execute this command with caution.</p> <pre><code>nebula&gt; MATCH p=(v:player{name:\"Tim Duncan\"})-[e:follow*]-&gt;(v2) \\\n        RETURN v2 AS Friends;\n+-----------------------------------------------------------+\n| Friends                                                   |\n+-----------------------------------------------------------+\n| (\"player125\" :player{age: 41, name: \"Manu Ginobili\"})     |\n| (\"player101\" :player{age: 36, name: \"Tony Parker\"})       |\n...\n\nnebula&gt; MATCH p=(v:player{name:\"Tim Duncan\"})-[e:follow*1..3]-&gt;(v2) \\\n        RETURN v2 AS Friends;\n+-----------------------------------------------------------+\n| Friends                                                   |\n+-----------------------------------------------------------+\n| (\"player101\" :player{age: 36, name: \"Tony Parker\"})       |\n| (\"player125\" :player{age: 41, name: \"Manu Ginobili\"})     |\n| (\"player100\" :player{age: 42, name: \"Tim Duncan\"})        |\n...\n\nnebula&gt; MATCH p=(v:player{name:\"Tim Duncan\"})-[e:follow*1..]-&gt;(v2) \\\n        RETURN v2 AS Friends;\n+-----------------------------------------------------------+\n| Friends                                                   |\n+-----------------------------------------------------------+\n| (\"player125\" :player{age: 41, name: \"Manu Ginobili\"})     |\n| (\"player101\" :player{age: 36, name: \"Tony Parker\"})       |\n| (\"player100\" :player{age: 42, name: \"Tim Duncan\"})        |\n...\n</code></pre> <p>You can use the <code>DISTINCT</code> keyword to aggregate duplicate results.</p> <pre><code>nebula&gt; MATCH p=(v:player{name:\"Tim Duncan\"})-[e:follow*1..3]-&gt;(v2:player) \\\n        RETURN DISTINCT v2 AS Friends, count(v2);\n+-----------------------------------------------------------+-----------+\n| Friends                                                   | count(v2) |\n+-----------------------------------------------------------+-----------+\n| (\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"}) | 1         |\n| (\"player100\" :player{age: 42, name: \"Tim Duncan\"})        | 4         |\n| (\"player101\" :player{age: 36, name: \"Tony Parker\"})       | 3         |\n| (\"player125\" :player{age: 41, name: \"Manu Ginobili\"})     | 3         |\n+-----------------------------------------------------------+-----------+\n</code></pre> <p>If <code>minHop</code> is <code>0</code>, the pattern will match the source vertex of the path. Compared to the preceding statement, the following example uses <code>0</code> as the <code>minHop</code>. So in the following result set, <code>\"Tim Duncan\"</code> is counted one more time than it is in the preceding result set because it is the source vertex.</p> <pre><code>nebula&gt; MATCH p=(v:player{name:\"Tim Duncan\"})-[e:follow*0..3]-&gt;(v2:player) \\\n        RETURN DISTINCT v2 AS Friends, count(v2);\n+-----------------------------------------------------------+-----------+\n| Friends                                                   | count(v2) |\n+-----------------------------------------------------------+-----------+\n| (\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"}) | 1         |\n| (\"player100\" :player{age: 42, name: \"Tim Duncan\"})        | 5         |\n| (\"player125\" :player{age: 41, name: \"Manu Ginobili\"})     | 3         |\n| (\"player101\" :player{age: 36, name: \"Tony Parker\"})       | 3         |\n+-----------------------------------------------------------+-----------+\n</code></pre> <p>Note</p> <p>When using the variable <code>e</code> to match fixed-length or variable-length paths in a pattern, such as <code>-[e:follow*0..3]-&gt;</code>, it is not supported to reference <code>e</code> in other patterns. For example, the following statement is not supported.</p> <pre><code>nebula&gt; MATCH (v:player)-[e:like*1..3]-&gt;(n) \\\n        WHERE (n)-[e*1..4]-&gt;(:player) \\\n        RETURN v;\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_variable-length_paths_with_multiple_edge_types","title":"Match variable-length paths with multiple edge types","text":"<p>You can specify multiple edge types in a fixed-length or variable-length pattern. In this case, <code>hop</code>, <code>minHop</code>, and <code>maxHop</code> take effect on all edge types.</p> <pre><code>nebula&gt; MATCH p=(v:player{name:\"Tim Duncan\"})-[e:follow|serve*2]-&gt;(v2) \\\n        RETURN DISTINCT v2;\n+-----------------------------------------------------------+\n| v2                                                        |\n+-----------------------------------------------------------+\n| (\"team204\" :team{name: \"Spurs\"})                          |\n| (\"player100\" :player{age: 42, name: \"Tim Duncan\"})        |\n| (\"team215\" :team{name: \"Hornets\"})                        |\n| (\"player125\" :player{age: 41, name: \"Manu Ginobili\"})     |\n| (\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"}) |\n+-----------------------------------------------------------+\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_multiple_patterns","title":"Match multiple patterns","text":"<p>You can separate multiple patterns with commas (,).</p> <pre><code>nebula&gt; CREATE TAG INDEX IF NOT EXISTS team_index ON team(name(20));\nnebula&gt; REBUILD TAG INDEX team_index;\nnebula&gt; MATCH (v1:player{name:\"Tim Duncan\"}), (v2:team{name:\"Spurs\"}) \\\n        RETURN v1,v2;\n+----------------------------------------------------+----------------------------------+\n| v1                                                 | v2                               |\n+----------------------------------------------------+----------------------------------+\n| (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | (\"team204\" :team{name: \"Spurs\"}) |\n+----------------------------------------------------+----------------------------------+\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_shortest_paths","title":"Match shortest paths","text":"<p>The <code>allShortestPaths</code> function can be used to find all shortest paths between two vertices.</p> <pre><code>nebula&gt; MATCH p = allShortestPaths((a:player{name:\"Tim Duncan\"})-[e*..5]-(b:player{name:\"Tony Parker\"})) \\\n        RETURN p;\n+------------------------------------------------------------------------------------------------------------------------------------+\n| p                                                                                                                                  |\n+------------------------------------------------------------------------------------------------------------------------------------+\n| &lt;(\"player100\" :player{age: 42, name: \"Tim Duncan\"})&lt;-[:follow@0 {degree: 95}]-(\"player101\" :player{age: 36, name: \"Tony Parker\"})&gt; |\n| &lt;(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]-&gt;(\"player101\" :player{age: 36, name: \"Tony Parker\"})&gt; |\n+------------------------------------------------------------------------------------------------------------------------------------+\n</code></pre> <p>The <code>shortestPath</code> function can be used to find a single shortest path between two vertices.</p> <pre><code>nebula&gt; MATCH p = shortestPath((a:player{name:\"Tim Duncan\"})-[e*..5]-(b:player{name:\"Tony Parker\"})) \\\n        RETURN p;\n+------------------------------------------------------------------------------------------------------------------------------------+\n| p                                                                                                                                  |\n+------------------------------------------------------------------------------------------------------------------------------------+\n| &lt;(\"player100\" :player{age: 42, name: \"Tim Duncan\"})&lt;-[:follow@0 {degree: 95}]-(\"player101\" :player{age: 36, name: \"Tony Parker\"})&gt; |\n+------------------------------------------------------------------------------------------------------------------------------------+\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#retrieve_with_multiple_match","title":"Retrieve with multiple match","text":"<p>Multiple <code>MATCH</code> can be used when different patterns have different filtering criteria and return the rows that exactly match the pattern.</p> <pre><code>nebula&gt; MATCH (m)-[]-&gt;(n) WHERE id(m)==\"player100\" \\\n        MATCH (n)-[]-&gt;(l) WHERE id(n)==\"player125\" \\\n        RETURN id(m),id(n),id(l);\n+-------------+-------------+-------------+\n| id(m)       | id(n)       | id(l)       |\n+-------------+-------------+-------------+\n| \"player100\" | \"player125\" | \"team204\"   |\n| \"player100\" | \"player125\" | \"player100\" |\n+-------------+-------------+-------------+\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#retrieve_with_optional_match","title":"Retrieve with optional match","text":"<p>See OPTIONAL MATCH.</p> <p>Performance</p> <p>In NebulaGraph, the performance and resource usage of the <code>MATCH</code> statement have been optimized. But we still recommend to use <code>GO</code>, <code>LOOKUP</code>, <code>|</code>, and <code>FETCH</code> instead of <code>MATCH</code> when high performance is required.</p>"},{"location":"3.ngql-guide/7.general-query-statements/3.go/","title":"GO","text":"<p>The <code>GO</code> statement is used in the NebulaGraph database to traverse the graph starting from a given starting vertex with specified filters and return results.</p>"},{"location":"3.ngql-guide/7.general-query-statements/3.go/#opencypher_compatibility","title":"OpenCypher compatibility","text":"<p>This topic applies to native nGQL only.</p>"},{"location":"3.ngql-guide/7.general-query-statements/3.go/#syntax","title":"Syntax","text":"<pre><code>GO [[&lt;M&gt; TO] &lt;N&gt; {STEP|STEPS}] FROM &lt;vertex_list&gt;\nOVER &lt;edge_type_list&gt; [{REVERSELY | BIDIRECT}]\n[ WHERE &lt;conditions&gt;\u00a0]\nYIELD\u00a0[DISTINCT] &lt;return_list&gt;\n[{SAMPLE &lt;sample_list&gt; | &lt;limit_by_list_clause&gt;}]\n[| GROUP BY {col_name | expr | position} YIELD &lt;col_name&gt;]\n[| ORDER BY &lt;expression&gt; [{ASC | DESC}]]\n[| LIMIT [&lt;offset&gt;,] &lt;number_rows&gt;];\n\n&lt;vertex_list&gt; ::=\n    &lt;vid&gt; [, &lt;vid&gt; ...]\n\n&lt;edge_type_list&gt; ::=\n   edge_type [, edge_type ...]\n   | *\n\n&lt;return_list&gt; ::=\n    &lt;col_name&gt; [AS &lt;col_alias&gt;] [, &lt;col_name&gt; [AS &lt;col_alias&gt;] ...]\n</code></pre> <ul> <li> <p><code>&lt;N&gt; {STEP|STEPS}</code>: specifies the hop number. If not specified, the default value for <code>N</code> is <code>one</code>. When <code>N</code> is <code>zero</code>, NebulaGraph does not traverse any edges and returns nothing.</p> <p>Note</p> <p>The path type of the <code>GO</code> statement is <code>walk</code>, which means both vertices and edges can be repeatedly visited in graph traversal. For more information, see Path.</p> </li> </ul> <ul> <li><code>M TO N {STEP|STEPS}</code>: traverses <code>from M to N</code> hops. When <code>M</code> is <code>zero</code>, the output is the same as that of <code>M</code> is <code>one</code>. That is, the output of <code>GO 0 TO 2</code> and <code>GO 1 TO 2</code> are the same.</li> </ul> <ul> <li><code>&lt;vertex_list&gt;</code>: represents a list of vertex IDs separated by commas.</li> </ul> <ul> <li><code>&lt;edge_type_list&gt;</code>: represents a list of edge types which the traversal can go through.</li> </ul> <ul> <li><code>REVERSELY | BIDIRECT</code>: defines the direction of the query. By default, the <code>GO</code> statement searches for outgoing edges of <code>&lt;vertex_list&gt;</code>. If <code>REVERSELY</code> is set, <code>GO</code> searches for incoming edges. If <code>BIDIRECT</code> is set, <code>GO</code> searches for edges of both directions. The direction of the query can be checked by returning the <code>&lt;edge_type&gt;._type</code> field using <code>YIELD</code>. A positive value indicates an outgoing edge, while a negative value indicates an incoming edge.</li> </ul> <ul> <li> <p><code>WHERE &lt;expression&gt;</code>: specifies the traversal filters. You can use the <code>WHERE</code> clause for the source vertices, the edges, and the destination vertices. You can use it together with <code>AND</code>, <code>OR</code>, <code>NOT</code>, and <code>XOR</code>. For more information, see WHERE.</p> <p>Note</p> <ul> <li>There are some restrictions for the <code>WHERE</code> clause when you traverse along with multiple edge types. For example, <code>WHERE edge1.prop1 &gt; edge2.prop2</code> is not supported.</li> <li>The GO statement is executed by traversing all the vertices and then filtering according to the filter condition.</li> </ul> </li> </ul> <ul> <li><code>YIELD [DISTINCT] &lt;return_list&gt;</code>: defines the output to be returned. It is recommended to use the Schema-related functions to fill in <code>&lt;return_list&gt;</code>. <code>src(edge)</code>, <code>dst(edge)</code>, <code>type(edge) )</code>, <code>rank(edge)</code>, etc., are currently supported, while nested functions are not. For more information, see YIELD.</li> </ul> <ul> <li><code>SAMPLE &lt;sample_list&gt;</code>: takes samples from the result set. For more information, see SAMPLE.</li> </ul> <ul> <li><code>&lt;limit_by_list_clause&gt;</code>: limits the number of outputs during the traversal process. For more information, see LIMIT.</li> </ul> <ul> <li><code>GROUP BY</code>: groups the output into subgroups based on the value of the specified property. For more information, see GROUP BY. After grouping, you need to use <code>YIELD</code> again to define the output that needs to be returned.</li> </ul> <ul> <li> <p><code>ORDER BY</code>: sorts outputs with specified orders. For more information, see ORDER BY.</p> <p>Note</p> <p>When the sorting method is not specified, the output orders can be different for the same query.</p> </li> </ul> <ul> <li><code>LIMIT  [&lt;offset&gt;,] &lt;number_rows&gt;]</code>: limits the number of rows of the output. For more information, see LIMIT.</li> </ul>"},{"location":"3.ngql-guide/7.general-query-statements/3.go/#notes","title":"Notes","text":"<ul> <li>The <code>WHERE</code> and <code>YIELD</code> clauses in <code>GO</code> statements usually utilize property reference symbols (<code>$^</code> and <code>$$</code>) or the <code>properties($^)</code> and <code>properties($$)</code> functions to specify the properties of a vertex; use the <code>properties(edge)</code> function to specify the properties of an edge. For details, see Property Reference Symbols and Schema-related Functions.</li> </ul> <ul> <li>When referring to the result of a subquery in a compound <code>GO</code> statement, you need to set a name for the result and pass it to the next subquery using the pipe symbol <code>|</code>, and reference the name of the result in the next subquery using <code>$-</code>. See the Pipe Operator for details.</li> </ul> <ul> <li>When the queried property has no value, the returned result displays <code>NULL</code>.</li> </ul>"},{"location":"3.ngql-guide/7.general-query-statements/3.go/#cases_and_examples","title":"Cases and examples","text":""},{"location":"3.ngql-guide/7.general-query-statements/3.go/#to_query_the_immediate_neighbors_of_a_vertex","title":"To query the immediate neighbors of a vertex","text":"<p>For example, to query the team that a person belongs to, assuming that the person is connected to the team by the <code>serve</code> edge and the person's ID is <code>player102</code>.</p> <pre><code>nebula&gt;\u00a0GO FROM \"player102\" OVER serve YIELD dst(edge);\n+-----------+\n| dst(EDGE) |\n+-----------+\n| \"team203\" |\n| \"team204\" |\n+-----------+\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/3.go/#to_query_all_vertices_within_a_specified_number_of_hops_from_a_starting_vertex","title":"To query all vertices within a specified number of hops from a starting vertex","text":"<p>For example, to query all vertices within two hops of a person vertex, assuming that the person is connected to other people by the <code>follow</code> edge and the person's ID is <code>player102</code>.</p> <pre><code># Return all vertices that are 2 hops away from the player102 vertex.\nnebula&gt; GO 2 STEPS FROM \"player102\" OVER follow YIELD dst(edge);\n+-------------+\n| dst(EDGE)   |\n+-------------+\n| \"player101\" |\n| \"player125\" |\n| \"player100\" |\n| \"player102\" |\n| \"player125\" |\n+-------------+\n</code></pre> <pre><code># Return all vertices within 1 or 2 hops away from the player102 vertex.\nnebula&gt; GO 1 TO 2 STEPS FROM \"player100\" OVER follow \\\n        YIELD dst(edge) AS destination;\n+-------------+\n| destination |\n+-------------+\n| \"player101\" |\n| \"player125\" |\n...\n\n# The following MATCH query has the same semantics as the previous GO query.\nnebula&gt; MATCH (v) -[e:follow*1..2]-&gt;(v2) \\\n        WHERE id(v) == \"player100\" \\\n        RETURN id(v2) AS destination;\n+-------------+\n| destination |\n+-------------+\n| \"player100\" |\n| \"player102\" |\n...\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/3.go/#to_add_filtering_conditions","title":"To add filtering conditions","text":"<p>Case: To query the vertices and edges that meet specific conditions.</p> <p>For example, use the <code>WHERE</code> clause to query the edges with specific properties between the starting vertex and the destination vertex.</p> <pre><code>nebula&gt;\u00a0GO FROM \"player100\", \"player102\" OVER serve \\\n        WHERE properties(edge).start_year &gt; 1995 \\\n        YIELD DISTINCT properties($$).name AS team_name, properties(edge).start_year AS start_year, properties($^).name AS player_name;\n\n+-----------------+------------+---------------------+\n| team_name       | start_year | player_name         |\n+-----------------+------------+---------------------+\n| \"Spurs\"         | 1997       | \"Tim Duncan\"        |\n| \"Trail Blazers\" | 2006       | \"LaMarcus Aldridge\" |\n| \"Spurs\"         | 2015       | \"LaMarcus Aldridge\" |\n+-----------------+------------+---------------------+\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/3.go/#to_query_multiple_edge_types","title":"To query multiple edge types","text":"<p>Case: To query multiple edge types that are connected to the starting vertex. You can specify multiple edge types or the <code>*</code> symbol to query multiple edge types.</p> <p>For example, to query the <code>follow</code> and <code>serve</code> edges that are connected to the starting vertex.</p> <pre><code>nebula&gt; GO FROM \"player100\" OVER follow, serve \\\n        YIELD properties(edge).degree, properties(edge).start_year;\n+-------------------------+-----------------------------+\n| properties(EDGE).degree | properties(EDGE).start_year |\n+-------------------------+-----------------------------+\n| 95                      | __NULL__                    |\n| 95                      | __NULL__                    |\n| __NULL__                | 1997                        |\n+-------------------------+-----------------------------+\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/3.go/#to_query_incoming_vertices_using_the_reversely_keyword","title":"To query incoming vertices using the REVERSELY keyword","text":"<pre><code># Return the vertices that follow the player100 vertex.\nnebula&gt; GO FROM \"player100\" OVER follow REVERSELY \\\n        YIELD src(edge) AS destination;\n+-------------+\n| destination |\n+-------------+\n| \"player101\" |\n| \"player102\" |\n...\n\n# The following MATCH query has the same semantics as the previous GO query.\nnebula&gt; MATCH (v)&lt;-[e:follow]- (v2) WHERE id(v) == 'player100' \\\n        RETURN id(v2) AS destination;\n+-------------+\n| destination |\n+-------------+\n| \"player101\" |\n| \"player102\" |\n...\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/3.go/#to_use_subqueries_as_the_starting_vertice_of_a_graph_traversal","title":"To use subqueries as the starting vertice of a graph traversal","text":"<pre><code># Return the friends of the player100 vertex and the teams that the friends belong to.\nnebula&gt; GO FROM \"player100\" OVER follow REVERSELY \\\n        YIELD src(edge) AS id | \\\n        GO FROM $-.id OVER serve \\\n        WHERE properties($^).age &gt; 20 \\\n        YIELD properties($^).name AS FriendOf, properties($$).name AS Team;\n+---------------------+-----------------+\n| FriendOf            | Team            |\n+---------------------+-----------------+\n| \"Boris Diaw\"        | \"Spurs\"         |\n| \"Boris Diaw\"        | \"Jazz\"          |\n| \"Boris Diaw\"        | \"Suns\"          |\n...\n\n# The following MATCH query has the same semantics as the previous GO query.\nnebula&gt; MATCH (v)&lt;-[e:follow]- (v2)-[e2:serve]-&gt;(v3)  \\\n        WHERE id(v) == 'player100' \\\n        RETURN v2.player.name AS FriendOf, v3.team.name AS Team;\n+---------------------+-----------------+\n| FriendOf            | Team            |\n+---------------------+-----------------+\n| \"Boris Diaw\"        | \"Spurs\"         |\n| \"Boris Diaw\"        | \"Jazz\"          |\n| \"Boris Diaw\"        | \"Suns\"          |\n...\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/3.go/#to_use_group_by_to_group_the_output","title":"To use <code>GROUP BY</code> to group the output","text":"<p>You need to use <code>YIELD</code> to define the output that needs to be returned after grouping.</p> <pre><code># The following example collects the outputs according to age.\nnebula&gt; GO 2 STEPS FROM \"player100\" OVER follow \\\n        YIELD src(edge) AS src, dst(edge) AS dst, properties($$).age AS age \\\n        | GROUP BY $-.dst \\\n        YIELD $-.dst AS dst, collect_set($-.src) AS src, collect($-.age) AS age;\n+-------------+----------------------------+----------+\n| dst         | src                        | age      |\n+-------------+----------------------------+----------+\n| \"player125\" | {\"player101\"}              | [41]     |\n| \"player100\" | {\"player125\", \"player101\"} | [42, 42] |\n| \"player102\" | {\"player101\"}              | [33]     |\n+-------------+----------------------------+----------+\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/3.go/#to_use_order_by_and_limit_to_sort_and_limit_the_output","title":"To use <code>ORDER BY</code> and <code>LIMIT</code> to sort and limit the output","text":"<pre><code># The following example groups the outputs and restricts the number of rows of the outputs.\nnebula&gt; $a = GO FROM \"player100\" OVER follow YIELD src(edge) AS src, dst(edge) AS dst; \\\n        GO 2 STEPS FROM $a.dst OVER follow \\\n        YIELD $a.src AS src, $a.dst, src(edge), dst(edge) \\\n        | ORDER BY $-.src | OFFSET 1 LIMIT 2;\n+-------------+-------------+-------------+-------------+\n| src         | $a.dst      | src(EDGE)   | dst(EDGE)   |\n+-------------+-------------+-------------+-------------+\n| \"player100\" | \"player101\" | \"player100\" | \"player101\" |\n| \"player100\" | \"player125\" | \"player100\" | \"player125\" |\n+-------------+-------------+-------------+-------------+\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/3.go/#other_examples","title":"Other examples","text":"<pre><code># The following example determines if $$.player.name IS NOT EMPTY.\nnebula&gt; GO FROM \"player100\" OVER follow WHERE properties($$).name IS NOT EMPTY YIELD dst(edge);\n+-------------+\n| dst(EDGE)   |\n+-------------+\n| \"player125\" |\n| \"player101\" |\n+-------------+\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/4.fetch/","title":"FETCH","text":"<p>The <code>FETCH</code> statement retrieves the properties of the specified vertices or edges.</p>"},{"location":"3.ngql-guide/7.general-query-statements/4.fetch/#opencypher_compatibility","title":"OpenCypher Compatibility","text":"<p>This topic applies to native nGQL only.</p>"},{"location":"3.ngql-guide/7.general-query-statements/4.fetch/#fetch_vertex_properties","title":"Fetch vertex properties","text":""},{"location":"3.ngql-guide/7.general-query-statements/4.fetch/#syntax","title":"Syntax","text":"<pre><code>FETCH PROP ON {&lt;tag_name&gt;[, tag_name ...] | *}\n&lt;vid&gt; [, vid ...]\nYIELD [DISTINCT] &lt;return_list&gt; [AS &lt;alias&gt;];\n</code></pre> Parameter Description <code>tag_name</code> The name of the tag. <code>*</code> Represents all the tags in the current graph space. <code>vid</code> The vertex ID. <code>YIELD</code> Define the output to be returned. For details, see <code>YIELD</code>. <code>AS</code> Set an alias."},{"location":"3.ngql-guide/7.general-query-statements/4.fetch/#fetch_vertex_properties_by_one_tag","title":"Fetch vertex properties by one tag","text":"<p>Specify a tag in the <code>FETCH</code> statement to fetch the vertex properties by that tag.</p> <pre><code>nebula&gt; FETCH PROP ON player \"player100\" YIELD properties(vertex);\n+-------------------------------+\n| properties(VERTEX)            |\n+-------------------------------+\n| {age: 42, name: \"Tim Duncan\"} |\n+-------------------------------+\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/4.fetch/#fetch_specific_properties_of_a_vertex","title":"Fetch specific properties of a vertex","text":"<p>Use a <code>YIELD</code> clause to specify the properties to be returned.</p> <pre><code>nebula&gt; FETCH PROP ON player \"player100\" \\\n        YIELD properties(vertex).name AS name;\n+--------------+\n| name         |\n+--------------+\n| \"Tim Duncan\" |\n+--------------+\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/4.fetch/#fetch_properties_of_multiple_vertices","title":"Fetch properties of multiple vertices","text":"<p>Specify multiple VIDs (vertex IDs) to fetch properties of multiple vertices. Separate the VIDs with commas.</p> <pre><code>nebula&gt; FETCH PROP ON player \"player101\", \"player102\", \"player103\" YIELD properties(vertex);\n+--------------------------------------+\n| properties(VERTEX)                   |\n+--------------------------------------+\n| {age: 33, name: \"LaMarcus Aldridge\"} |\n| {age: 36, name: \"Tony Parker\"}       |\n| {age: 32, name: \"Rudy Gay\"}          |\n+--------------------------------------+\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/4.fetch/#fetch_vertex_properties_by_multiple_tags","title":"Fetch vertex properties by multiple tags","text":"<p>Specify multiple tags in the <code>FETCH</code> statement to fetch the vertex properties by the tags. Separate the tags with commas.</p> <pre><code># The following example creates a new tag t1.\nnebula&gt; CREATE TAG IF NOT EXISTS t1(a string, b int);\n\n# The following example attaches t1 to the vertex \"player100\".\nnebula&gt; INSERT VERTEX t1(a, b) VALUES \"player100\":(\"Hello\", 100);\n\n# The following example fetches the properties of vertex \"player100\" by the tags player and t1.\nnebula&gt; FETCH PROP ON player, t1 \"player100\" YIELD vertex AS v;\n+----------------------------------------------------------------------------+\n| v                                                                          |\n+----------------------------------------------------------------------------+\n| (\"player100\" :player{age: 42, name: \"Tim Duncan\"} :t1{a: \"Hello\", b: 100}) |\n+----------------------------------------------------------------------------+\n</code></pre> <p>You can combine multiple tags with multiple VIDs in a <code>FETCH</code> statement.</p> <pre><code>nebula&gt; FETCH PROP ON player, t1 \"player100\", \"player103\" YIELD vertex AS v;\n+----------------------------------------------------------------------------+\n| v                                                                          |\n+----------------------------------------------------------------------------+\n| (\"player100\" :player{age: 42, name: \"Tim Duncan\"} :t1{a: \"Hello\", b: 100}) |\n| (\"player103\" :player{age: 32, name: \"Rudy Gay\"})                           |\n+----------------------------------------------------------------------------+\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/4.fetch/#fetch_vertex_properties_by_all_tags","title":"Fetch vertex properties by all tags","text":"<p>Set an asterisk symbol <code>*</code> to fetch properties by all tags in the current graph space.</p> <pre><code>nebula&gt; FETCH PROP ON * \"player100\", \"player106\", \"team200\" YIELD vertex AS v;\n+----------------------------------------------------------------------------+\n| v                                                                          |\n+----------------------------------------------------------------------------+\n| (\"player100\" :player{age: 42, name: \"Tim Duncan\"} :t1{a: \"Hello\", b: 100}) |\n| (\"player106\" :player{age: 25, name: \"Kyle Anderson\"})                      |\n| (\"team200\" :team{name: \"Warriors\"})                                        |\n+----------------------------------------------------------------------------+\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/4.fetch/#fetch_edge_properties","title":"Fetch edge properties","text":""},{"location":"3.ngql-guide/7.general-query-statements/4.fetch/#syntax_1","title":"Syntax","text":"<pre><code>FETCH PROP ON &lt;edge_type&gt; &lt;src_vid&gt; -&gt; &lt;dst_vid&gt;[@&lt;rank&gt;] [, &lt;src_vid&gt; -&gt; &lt;dst_vid&gt; ...]\nYIELD &lt;output&gt;;\n</code></pre> Parameter Description <code>edge_type</code> The name of the edge type. <code>src_vid</code> The VID of the source vertex. It specifies the start of an edge. <code>dst_vid</code> The VID of the destination vertex. It specifies the end of an edge. <code>rank</code> The rank of the edge. It is optional and defaults to <code>0</code>. It distinguishes an edge from other edges with the same edge type, source vertex, destination vertex, and rank. <code>YIELD</code> Define the output to be returned. For details, see <code>YIELD</code>."},{"location":"3.ngql-guide/7.general-query-statements/4.fetch/#fetch_all_properties_of_an_edge","title":"Fetch all properties of an edge","text":"<p>The following statement fetches all the properties of the <code>serve</code> edge that connects vertex <code>\"player100\"</code> and vertex <code>\"team204\"</code>.</p> <pre><code>nebula&gt; FETCH PROP ON serve \"player100\" -&gt; \"team204\" YIELD properties(edge);\n+------------------------------------+\n| properties(EDGE)                   |\n+------------------------------------+\n| {end_year: 2016, start_year: 1997} |\n+------------------------------------+\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/4.fetch/#fetch_specific_properties_of_an_edge","title":"Fetch specific properties of an edge","text":"<p>Use a <code>YIELD</code> clause to fetch specific properties of an edge.</p> <pre><code>nebula&gt; FETCH PROP ON serve \"player100\" -&gt; \"team204\"    \\\n        YIELD properties(edge).start_year;\n+-----------------------------+\n| properties(EDGE).start_year |\n+-----------------------------+\n| 1997                        |\n+-----------------------------+\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/4.fetch/#fetch_properties_of_multiple_edges","title":"Fetch properties of multiple edges","text":"<p>Specify multiple edge patterns (<code>&lt;src_vid&gt; -&gt; &lt;dst_vid&gt;[@&lt;rank&gt;]</code>) to fetch properties of multiple edges. Separate the edge patterns with commas.</p> <pre><code>nebula&gt; FETCH PROP ON serve \"player100\" -&gt; \"team204\", \"player133\" -&gt; \"team202\" YIELD edge AS e;\n+-----------------------------------------------------------------------+\n| e                                                                     |\n+-----------------------------------------------------------------------+\n| [:serve \"player100\"-&gt;\"team204\" @0 {end_year: 2016, start_year: 1997}] |\n| [:serve \"player133\"-&gt;\"team202\" @0 {end_year: 2011, start_year: 2002}] |\n+-----------------------------------------------------------------------+\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/4.fetch/#fetch_properties_based_on_edge_rank","title":"Fetch properties based on edge rank","text":"<p>If there are multiple edges with the same edge type, source vertex, and destination vertex, you can specify the rank to fetch the properties on the correct edge.</p> <pre><code># The following example inserts edges with different ranks and property values.\nnebula&gt; insert edge serve(start_year,end_year) \\\n        values \"player100\"-&gt;\"team204\"@1:(1998, 2017);\n\nnebula&gt; insert edge serve(start_year,end_year) \\\n        values \"player100\"-&gt;\"team204\"@2:(1990, 2018);\n\n# By default, the FETCH statement returns the edge whose rank is 0.\nnebula&gt; FETCH PROP ON serve \"player100\" -&gt; \"team204\" YIELD edge AS e;\n+-----------------------------------------------------------------------+\n| e                                                                     |\n+-----------------------------------------------------------------------+\n| [:serve \"player100\"-&gt;\"team204\" @0 {end_year: 2016, start_year: 1997}] |\n+-----------------------------------------------------------------------+\n\n# To fetch on an edge whose rank is not 0, set its rank in the FETCH statement.\nnebula&gt; FETCH PROP ON serve \"player100\" -&gt; \"team204\"@1 YIELD edge AS e;\n+-----------------------------------------------------------------------+\n| e                                                                     |\n+-----------------------------------------------------------------------+\n| [:serve \"player100\"-&gt;\"team204\" @1 {end_year: 2017, start_year: 1998}] |\n+-----------------------------------------------------------------------+\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/4.fetch/#use_fetch_in_composite_queries","title":"Use FETCH in composite queries","text":"<p>A common way to use <code>FETCH</code> is to combine it with native nGQL such as <code>GO</code>.</p> <p>The following statement returns the <code>degree</code> values of the <code>follow</code> edges that start from vertex <code>\"player101\"</code>.</p> <pre><code>nebula&gt; GO FROM \"player101\" OVER follow \\\n        YIELD src(edge) AS s, dst(edge) AS d \\\n        | FETCH PROP ON follow $-.s -&gt; $-.d \\\n        YIELD properties(edge).degree;\n+-------------------------+\n| properties(EDGE).degree |\n+-------------------------+\n| 95                      |\n| 90                      |\n| 95                      |\n+-------------------------+\n</code></pre> <p>Or you can use user-defined variables to construct similar queries.</p> <pre><code>nebula&gt; $var = GO FROM \"player101\" OVER follow \\\n        YIELD src(edge) AS s, dst(edge) AS d; \\\n        FETCH PROP ON follow $var.s -&gt; $var.d \\\n        YIELD properties(edge).degree;\n+-------------------------+\n| properties(EDGE).degree |\n+-------------------------+\n| 95                      |\n| 90                      |\n| 95                      |\n+-------------------------+\n</code></pre> <p>For more information about composite queries, see Composite queries (clause structure).</p>"},{"location":"3.ngql-guide/7.general-query-statements/5.lookup/","title":"LOOKUP","text":"<p>The <code>LOOKUP</code> statement traverses data based on indexes. You can use <code>LOOKUP</code> for the following purposes:</p> <ul> <li>Search for the specific data based on conditions defined by the <code>WHERE</code> clause.</li> </ul> <ul> <li>List vertices with a tag: retrieve the VID of all vertices with a tag.</li> </ul> <ul> <li>List edges with an edge type: retrieve the source vertex IDs, destination vertex IDs, and ranks of all edges with an edge type.</li> </ul> <ul> <li>Count the number of vertices or edges with a tag or an edge type.</li> </ul>"},{"location":"3.ngql-guide/7.general-query-statements/5.lookup/#opencypher_compatibility","title":"OpenCypher compatibility","text":"<p>This topic applies to native nGQL only.</p>"},{"location":"3.ngql-guide/7.general-query-statements/5.lookup/#precautions","title":"Precautions","text":"<ul> <li>Correct use of indexes can speed up queries, but indexes can dramatically reduce the write performance. The performance can be greatly reduced. DO NOT use indexes in production environments unless you are fully aware of their influences on your service.</li> </ul> <ul> <li> <p>If the specified property is not indexed when using the <code>LOOKUP</code> statement, NebulaGraph randomly selects one of the available indexes.</p> <p>For example, the tag <code>player</code> has two properties, <code>name</code> and <code>age</code>. Both the tag <code>player</code> itself and the property <code>name</code> have indexes, but the property <code>age</code> has no indexes. When running <code>LOOKUP ON player WHERE player.age == 36 YIELD player.name;</code>, NebulaGraph randomly uses one of the indexes of the tag <code>player</code> and the property <code>name</code>. You can use the <code>EXPLAIN</code> statement to check the selected index.</p> <p>Legacy version compatibility</p> <p>Before the release 2.5.0, if the specified property is not indexed when using the <code>LOOKUP</code> statement, NebulaGraph reports an error and does not use other indexes.</p> </li> </ul>"},{"location":"3.ngql-guide/7.general-query-statements/5.lookup/#prerequisites","title":"Prerequisites","text":"<p>Before using the <code>LOOKUP</code> statement, make sure that at least one index is created. If there are already related vertices, edges, or properties before an index is created, the user must rebuild the index after creating the index to make it valid.</p>"},{"location":"3.ngql-guide/7.general-query-statements/5.lookup/#syntax","title":"Syntax","text":"<pre><code>LOOKUP ON {&lt;vertex_tag&gt; | &lt;edge_type&gt;}\n[WHERE &lt;expression&gt; [AND &lt;expression&gt; ...]]\nYIELD [DISTINCT] &lt;return_list&gt; [AS &lt;alias&gt;];\n\n&lt;return_list&gt;\n    &lt;prop_name&gt; [AS &lt;col_alias&gt;] [, &lt;prop_name&gt; [AS &lt;prop_alias&gt;] ...];\n</code></pre> <ul> <li><code>WHERE &lt;expression&gt;</code>: filters data with specified conditions. Both <code>AND</code> and <code>OR</code> are supported between different expressions. For more information, see WHERE.</li> </ul> <ul> <li><code>YIELD</code>: Define the output to be returned. For details, see <code>YIELD</code>.</li> </ul> <ul> <li><code>DISTINCT</code>: Aggregate the output results and return the de-duplicated result set.</li> </ul> <ul> <li><code>AS</code>: Set an alias.</li> </ul>"},{"location":"3.ngql-guide/7.general-query-statements/5.lookup/#limitations_of_using_where_in_lookup","title":"Limitations of using <code>WHERE</code> in <code>LOOKUP</code>","text":"<p>The <code>WHERE</code> clause in a <code>LOOKUP</code> statement does not support the following operations:</p> <ul> <li><code>$-</code> and <code>$^</code>.</li> <li>Filter <code>rank()</code>.</li> <li>In relational expressions, operators are not supported to have field names on both sides, such as <code>tagName.prop1&gt; tagName.prop2</code>.</li> <li>Nested AliasProp expressions in operation expressions and function expressions are not supported.</li> <li>The <code>XOR</code> operation is not supported.</li> <li>String operations other than <code>STARTS WITH</code> are not supported.</li> <li>Graph patterns.</li> </ul>"},{"location":"3.ngql-guide/7.general-query-statements/5.lookup/#retrieve_vertices","title":"Retrieve vertices","text":"<p>The following example returns vertices whose <code>name</code> is <code>Tony Parker</code> and the tag is <code>player</code>.</p> <pre><code>nebula&gt; CREATE TAG INDEX IF NOT EXISTS index_player ON player(name(30), age);\n\nnebula&gt; REBUILD TAG INDEX index_player;\n+------------+\n| New Job Id |\n+------------+\n| 15         |\n+------------+\n\nnebula&gt; LOOKUP ON player \\\n        WHERE player.name == \"Tony Parker\" \\\n        YIELD id(vertex);\n+---------------+\n| id(VERTEX)    |\n+---------------+\n| \"player101\"   |\n+---------------+\n\nnebula&gt; LOOKUP ON player \\\n        WHERE player.name == \"Tony Parker\" \\\n        YIELD properties(vertex).name AS name, properties(vertex).age AS age;\n+---------------+-----+\n| name          | age |\n+---------------+-----+\n| \"Tony Parker\" | 36  |\n+---------------+-----+\n\nnebula&gt; LOOKUP ON player \\\n        WHERE player.age  &gt; 45 \\\n        YIELD id(vertex);\n+-------------+\n| id(VERTEX)  |\n+-------------+\n| \"player144\" |\n| \"player140\" |\n+-------------+\n\nnebula&gt; LOOKUP ON player \\\n        WHERE player.name STARTS WITH \"B\" \\\n        AND player.age IN [22,30] \\\n        YIELD properties(vertex).name, properties(vertex).age;\n+-------------------------+------------------------+\n| properties(VERTEX).name | properties(VERTEX).age |\n+-------------------------+------------------------+\n| \"Ben Simmons\"           | 22                     |\n| \"Blake Griffin\"         | 30                     |\n+-------------------------+------------------------+\n\nnebula&gt; LOOKUP ON player \\\n        WHERE player.name == \"Kobe Bryant\"\\\n        YIELD id(vertex) AS VertexID, properties(vertex).name AS name |\\\n        GO FROM $-.VertexID OVER serve \\\n        YIELD $-.name, properties(edge).start_year, properties(edge).end_year, properties($$).name;\n+---------------+-----------------------------+---------------------------+---------------------+\n| $-.name       | properties(EDGE).start_year | properties(EDGE).end_year | properties($$).name |\n+---------------+-----------------------------+---------------------------+---------------------+\n| \"Kobe Bryant\" | 1996                        | 2016                      | \"Lakers\"            |\n+---------------+-----------------------------+---------------------------+---------------------+\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/5.lookup/#retrieve_edges","title":"Retrieve edges","text":"<p>The following example returns edges whose <code>degree</code> is <code>90</code> and the edge type is <code>follow</code>.</p> <pre><code>nebula&gt; CREATE EDGE INDEX IF NOT EXISTS index_follow ON follow(degree);\n\nnebula&gt; REBUILD EDGE INDEX index_follow;\n+------------+\n| New Job Id |\n+------------+\n| 62         |\n+------------+\n\nnebula&gt; LOOKUP ON follow \\\n        WHERE follow.degree == 90 YIELD edge AS e;\n+----------------------------------------------------+\n| e                                                  |\n+----------------------------------------------------+\n| [:follow \"player109\"-&gt;\"player125\" @0 {degree: 90}] |\n| [:follow \"player118\"-&gt;\"player120\" @0 {degree: 90}] |\n| [:follow \"player118\"-&gt;\"player131\" @0 {degree: 90}] |\n...\n\nnebula&gt; LOOKUP ON follow \\\n        WHERE follow.degree == 90 \\\n        YIELD properties(edge).degree;\n+-------------+-------------+---------+-------------------------+\n| SrcVID      | DstVID      | Ranking | properties(EDGE).degree |\n+-------------+-------------+---------+-------------------------+\n| \"player150\" | \"player143\" | 0       | 90                      |\n| \"player150\" | \"player137\" | 0       | 90                      |\n| \"player148\" | \"player136\" | 0       | 90                      |\n...\n\nnebula&gt; LOOKUP ON follow \\\n        WHERE follow.degree == 60 \\\n        YIELD dst(edge) AS DstVID, properties(edge).degree AS Degree |\\\n        GO FROM $-.DstVID OVER serve \\\n        YIELD $-.DstVID, properties(edge).start_year, properties(edge).end_year, properties($$).name;\n+-------------+-----------------------------+---------------------------+---------------------+\n| $-.DstVID   | properties(EDGE).start_year | properties(EDGE).end_year | properties($$).name |\n+-------------+-----------------------------+---------------------------+---------------------+\n| \"player105\" | 2010                        | 2018                      | \"Spurs\"             |\n| \"player105\" | 2009                        | 2010                      | \"Cavaliers\"         |\n| \"player105\" | 2018                        | 2019                      | \"Raptors\"           |\n+-------------+-----------------------------+---------------------------+---------------------+\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/5.lookup/#list_vertices_or_edges_with_a_tag_or_an_edge_type","title":"List vertices or edges with a tag or an edge type","text":"<p>To list vertices or edges with a tag or an edge type, at least one index must exist on the tag, the edge type, or its property.</p> <p>For example, if there is a <code>player</code> tag with a <code>name</code> property and an <code>age</code> property, to retrieve the VID of all vertices tagged with <code>player</code>, there has to be an index on the <code>player</code> tag itself, the <code>name</code> property, or the <code>age</code> property.</p> <ul> <li>The following example shows how to retrieve the VID of all vertices tagged with <code>player</code>.<pre><code>nebula&gt; CREATE TAG IF NOT EXISTS player(name string,age int);\n\nnebula&gt; CREATE TAG INDEX IF NOT EXISTS player_index on player();\n\nnebula&gt; REBUILD TAG INDEX player_index;\n+------------+\n| New Job Id |\n+------------+\n| 66         |\n+------------+\n\nnebula&gt; INSERT VERTEX player(name,age) \\\n        VALUES \"player100\":(\"Tim Duncan\", 42), \"player101\":(\"Tony Parker\", 36);\n\nThe following statement retrieves the VID of all vertices with the tag `player`. It is similar to `MATCH (n:player) RETURN id(n) /*, n */`.\n\nnebula&gt; LOOKUP ON player YIELD id(vertex);\n+-------------+\n| id(VERTEX)  |\n+-------------+\n| \"player100\" |\n| \"player101\" |\n...\n</code></pre> </li> </ul> <ul> <li>The following example shows how to retrieve the source Vertex IDs, destination vertex IDs, and ranks of all edges of the <code>follow</code> edge type.<pre><code>nebula&gt; CREATE EDGE IF NOT EXISTS follow(degree int);\n\nnebula&gt; CREATE EDGE INDEX IF NOT EXISTS follow_index on follow();\n\nnebula&gt; REBUILD EDGE INDEX follow_index;\n+------------+\n| New Job Id |\n+------------+\n| 88         |\n+------------+\n\nnebula&gt; INSERT EDGE follow(degree) \\\n        VALUES \"player100\"-&gt;\"player101\":(95);\n\nThe following statement retrieves all edges with the edge type `follow`. It is similar to `MATCH (s)-[e:follow]-&gt;(d) RETURN id(s), rank(e), id(d) /*, type(e) */`.\n\nnebula)&gt; LOOKUP ON follow YIELD edge AS e;\n+-----------------------------------------------------+\n| e                                                   |\n+-----------------------------------------------------+\n| [:follow \"player105\"-&gt;\"player100\" @0 {degree: 70}]  |\n| [:follow \"player105\"-&gt;\"player116\" @0 {degree: 80}]  |\n| [:follow \"player109\"-&gt;\"player100\" @0 {degree: 80}]  |\n...\n</code></pre> </li> </ul>"},{"location":"3.ngql-guide/7.general-query-statements/5.lookup/#count_the_numbers_of_vertices_or_edges","title":"Count the numbers of vertices or edges","text":"<p>The following example shows how to count the number of vertices tagged with <code>player</code> and edges of the <code>follow</code> edge type.</p> <pre><code>nebula&gt; LOOKUP ON player YIELD id(vertex)|\\\n        YIELD COUNT(*) AS Player_Number;\n+---------------+\n| Player_Number |\n+---------------+\n| 51            |\n+---------------+\n\nnebula&gt; LOOKUP ON follow YIELD edge AS e| \\\n        YIELD COUNT(*) AS Follow_Number;\n+---------------+\n| Follow_Number |\n+---------------+\n| 81            |\n+---------------+\n</code></pre> <p>Note</p> <p>You can also use <code>SHOW STATS</code> to count the numbers of vertices or edges.</p>"},{"location":"3.ngql-guide/7.general-query-statements/optional-match/","title":"OPTIONAL MATCH","text":"<p>Caution</p> <p>The feature is still in beta. It will continue to be optimized.</p> <p>The <code>OPTIONAL MATCH</code> clause is used to search for the pattern described in it. <code>OPTIONAL MATCH</code> matches patterns against your graph database, just like <code>MATCH</code> does. The difference is that if no matches are found, <code>OPTIONAL MATCH</code> will use a null for missing parts of the pattern.</p>"},{"location":"3.ngql-guide/7.general-query-statements/optional-match/#opencypher_compatibility","title":"OpenCypher Compatibility","text":"<p>This topic applies to the openCypher syntax in nGQL only.</p>"},{"location":"3.ngql-guide/7.general-query-statements/optional-match/#limitations","title":"Limitations","text":"<p>The <code>WHERE</code> clause cannot be used in an <code>OPTIONAL MATCH</code> clause.</p>"},{"location":"3.ngql-guide/7.general-query-statements/optional-match/#example","title":"Example","text":"<p>The example of the use of <code>OPTIONAL MATCH</code> in the <code>MATCH</code> statement is as follows:</p> <pre><code>nebula&gt; MATCH (m)-[]-&gt;(n) WHERE id(m)==\"player100\" \\\n        OPTIONAL MATCH (n)-[]-&gt;(l) \\\n        RETURN id(m),id(n),id(l);\n+-------------+-------------+-------------+\n| id(m)       | id(n)       | id(l)       |\n+-------------+-------------+-------------+\n| \"player100\" | \"team204\"   | __NULL__    |\n| \"player100\" | \"player101\" | \"team204\"   |\n| \"player100\" | \"player101\" | \"team215\"   |\n| \"player100\" | \"player101\" | \"player100\" |\n| \"player100\" | \"player101\" | \"player102\" |\n| \"player100\" | \"player101\" | \"player125\" |\n| \"player100\" | \"player125\" | \"team204\"   |\n| \"player100\" | \"player125\" | \"player100\" |\n+-------------+-------------+-------------+\n</code></pre> <p>Using multiple <code>MATCH</code> instead of <code>OPTIONAL MATCH</code> returns rows that match the pattern exactly. The example is as follows:</p> <pre><code>nebula&gt; MATCH (m)-[]-&gt;(n) WHERE id(m)==\"player100\" \\\n        MATCH (n)-[]-&gt;(l) \\\n        RETURN id(m),id(n),id(l);\n+-------------+-------------+-------------+\n| id(m)       | id(n)       | id(l)       |\n+-------------+-------------+-------------+\n| \"player100\" | \"player101\" | \"team204\"   |\n| \"player100\" | \"player101\" | \"team215\"   |\n| \"player100\" | \"player101\" | \"player100\" |\n| \"player100\" | \"player101\" | \"player102\" |\n| \"player100\" | \"player101\" | \"player125\" |\n| \"player100\" | \"player125\" | \"team204\"   |\n| \"player100\" | \"player125\" | \"player100\" |\n+-------------+-------------+-------------+\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/1.show-charset/","title":"SHOW CHARSET","text":"<p>The <code>SHOW CHARSET</code> statement shows the available character sets.</p> <p>Currently available types are <code>utf8</code> and <code>utf8mb4</code>. The default charset type is <code>utf8</code>. NebulaGraph extends the <code>uft8</code> to support four-byte characters. Therefore <code>utf8</code> and <code>utf8mb4</code> are equivalent.</p>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/1.show-charset/#syntax","title":"Syntax","text":"<pre><code>SHOW CHARSET;\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/1.show-charset/#example","title":"Example","text":"<pre><code>nebula&gt; SHOW CHARSET;\n+---------+-----------------+-------------------+--------+\n| Charset | Description     | Default collation | Maxlen |\n+---------+-----------------+-------------------+--------+\n| \"utf8\"  | \"UTF-8 Unicode\" | \"utf8_bin\"        | 4      |\n+---------+-----------------+-------------------+--------+\n</code></pre> Parameter Description <code>Charset</code> The name of the character set. <code>Description</code> The description of the character set. <code>Default collation</code> The default collation of the character set. <code>Maxlen</code> The maximum number of bytes required to store one character."},{"location":"3.ngql-guide/7.general-query-statements/6.show/10.show-roles/","title":"SHOW ROLES","text":"<p>The <code>SHOW ROLES</code> statement shows the roles that are assigned to a user account.</p> <p>The return message differs according to the role of the user who is running this statement:</p> <ul> <li>If the user is a <code>GOD</code> or <code>ADMIN</code> and is granted access to the specified graph space, NebulaGraph shows all roles in this graph space except for <code>GOD</code>.</li> </ul> <ul> <li>If the user is a <code>DBA</code>, <code>USER</code>, or <code>GUEST</code> and is granted access to the specified graph space, NebulaGraph shows the user's own role in this graph space.</li> </ul> <ul> <li>If the user does not have access to the specified graph space, NebulaGraph returns <code>PermissionError</code>.</li> </ul> <p>For more information about roles, see Roles and privileges.</p>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/10.show-roles/#syntax","title":"Syntax","text":"<pre><code>SHOW ROLES IN &lt;space_name&gt;;\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/10.show-roles/#example","title":"Example","text":"<pre><code>nebula&gt; SHOW ROLES in basketballplayer;\n+---------+-----------+\n| Account | Role Type |\n+---------+-----------+\n| \"user1\" | \"ADMIN\"   |\n+---------+-----------+\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/11.show-snapshots/","title":"SHOW SNAPSHOTS","text":"<p>The <code>SHOW SNAPSHOTS</code> statement shows the information of all the snapshots.</p> <p>For how to create a snapshot and backup data, see Snapshot.</p>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/11.show-snapshots/#role_requirement","title":"Role requirement","text":"<p>Only the <code>root</code> user who has the <code>GOD</code> role can use the <code>SHOW SNAPSHOTS</code> statement.</p>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/11.show-snapshots/#syntax","title":"Syntax","text":"<pre><code>SHOW SNAPSHOTS;\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/11.show-snapshots/#example","title":"Example","text":"<pre><code>nebula&gt; SHOW SNAPSHOTS;\n+--------------------------------+---------+-----------------------------------------------------+\n| Name                           | Status  | Hosts                                               |\n+--------------------------------+---------+-----------------------------------------------------+\n| \"SNAPSHOT_2020_12_16_11_13_55\" | \"VALID\" | \"storaged0:9779, storaged1:9779, storaged2:9779\"    |\n| \"SNAPSHOT_2020_12_16_11_14_10\" | \"VALID\" | \"storaged0:9779, storaged1:9779, storaged2:9779\"    |\n+--------------------------------+---------+-----------------------------------------------------+\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/12.show-spaces/","title":"SHOW SPACES","text":"<p>The <code>SHOW SPACES</code> statement shows existing graph spaces in NebulaGraph.</p> <p>For how to create a graph space, see CREATE SPACE.</p>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/12.show-spaces/#syntax","title":"Syntax","text":"<pre><code>SHOW SPACES;\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/12.show-spaces/#example","title":"Example","text":"<pre><code>nebula&gt; SHOW SPACES;\n+---------------------+\n| Name                |\n+---------------------+\n| \"docs\"              |\n| \"basketballplayer\"  |\n+---------------------+\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/14.show-stats/","title":"SHOW STATS","text":"<p>The <code>SHOW STATS</code> statement shows the statistics of the graph space collected by the latest <code>SUBMIT JOB STATS</code> job.</p> <p>The statistics include the following information:</p> <ul> <li>The number of vertices in the graph space</li> <li>The number of edges in the graph space</li> <li>The number of vertices of each tag</li> <li>The number of edges of each edge type</li> </ul> <p>Warning</p> <p>The data returned by <code>SHOW STATS</code> is not real-time. The returned data is collected by the latest SUBMIT JOB STATS job and may include TTL-expired data. The expired data will be deleted and not included in the statistics the next time the Compaction operation  is performed.</p>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/14.show-stats/#prerequisites","title":"Prerequisites","text":"<p>You have to run the <code>SUBMIT JOB STATS</code> statement in the graph space where you want to collect statistics. For more information, see SUBMIT JOB STATS.</p> <p>Caution</p> <p>The result of the <code>SHOW STATS</code> statement is based on the last executed <code>SUBMIT JOB STATS</code> statement. If you want to update the result, run <code>SUBMIT JOB STATS</code> again. Otherwise the statistics will be wrong.</p>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/14.show-stats/#syntax","title":"Syntax","text":"<pre><code>SHOW STATS;\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/14.show-stats/#examples","title":"Examples","text":"<pre><code># Choose a graph space.\nnebula&gt; USE basketballplayer;\n\n# Start SUBMIT JOB STATS.\nnebula&gt; SUBMIT JOB STATS;\n+------------+\n| New Job Id |\n+------------+\n| 98         |\n+------------+\n\n# Make sure the job executes successfully.\nnebula&gt; SHOW JOB 98;\n+----------------+---------------+------------+----------------------------+----------------------------+-------------+\n| Job Id(TaskId) | Command(Dest) | Status     | Start Time                 | Stop Time                  | Error Code  |\n+----------------+---------------+------------+----------------------------+----------------------------+-------------+\n| 98             | \"STATS\"       | \"FINISHED\" | 2021-11-01T09:33:21.000000 | 2021-11-01T09:33:21.000000 | \"SUCCEEDED\" |\n| 0              | \"storaged2\"   | \"FINISHED\" | 2021-11-01T09:33:21.000000 | 2021-11-01T09:33:21.000000 | \"SUCCEEDED\" |\n| 1              | \"storaged0\"   | \"FINISHED\" | 2021-11-01T09:33:21.000000 | 2021-11-01T09:33:21.000000 | \"SUCCEEDED\" |\n| 2              | \"storaged1\"   | \"FINISHED\" | 2021-11-01T09:33:21.000000 | 2021-11-01T09:33:21.000000 | \"SUCCEEDED\" |\n| \"Total:3\"      | \"Succeeded:3\" | \"Failed:0\" | \"In Progress:0\"            | \"\"                         | \"\"          |\n+----------------+---------------+------------+----------------------------+----------------------------+-------------+\n\n# Show the statistics of the graph space.\nnebula&gt; SHOW STATS;\n+---------+------------+-------+\n| Type    | Name       | Count |\n+---------+------------+-------+\n| \"Tag\"   | \"player\"   | 51    |\n| \"Tag\"   | \"team\"     | 30    |\n| \"Edge\"  | \"follow\"   | 81    |\n| \"Edge\"  | \"serve\"    | 152   |\n| \"Space\" | \"vertices\" | 81    |\n| \"Space\" | \"edges\"    | 233   |\n+---------+------------+-------+\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/15.show-tags-edges/","title":"SHOW TAGS/EDGES","text":"<p>The <code>SHOW TAGS</code> statement shows all the tags in the current graph space.</p> <p>The <code>SHOW EDGES</code> statement shows all the edge types in the current graph space.</p>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/15.show-tags-edges/#syntax","title":"Syntax","text":"<pre><code>SHOW {TAGS | EDGES};\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/15.show-tags-edges/#examples","title":"Examples","text":"<pre><code>nebula&gt; SHOW TAGS;\n+----------+\n| Name     |\n+----------+\n| \"player\" |\n| \"star\"   |\n| \"team\"   |\n+----------+\n\nnebula&gt; SHOW EDGES;\n+----------+\n| Name     |\n+----------+\n| \"follow\" |\n| \"serve\"  |\n+----------+\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/16.show-users/","title":"SHOW USERS","text":"<p>The <code>SHOW USERS</code> statement shows the user information.</p>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/16.show-users/#role_requirement","title":"Role requirement","text":"<p>Only the <code>root</code> user who has the <code>GOD</code> role can use the <code>SHOW USERS</code> statement.</p>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/16.show-users/#syntax","title":"Syntax","text":"<pre><code>SHOW USERS;\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/16.show-users/#example","title":"Example","text":"<pre><code>nebula&gt; SHOW USERS;\n+---------+-----------------+\n| Account | IP Whitelist    |\n+---------+-----------------+\n| \"root\"  | \"\"              |\n| \"user1\" | \"\"              |\n| \"user2\" | \"192.168.10.10\" |\n+---------+-----------------+\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/17.show-sessions/","title":"SHOW SESSIONS","text":"<p>When a user logs in to the database, a corresponding session will be created and users can query for session information.</p> <p>The <code>SHOW SESSIONS</code> statement shows the information of all the sessions. It can also show a specified session with its ID.</p>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/17.show-sessions/#precautions","title":"Precautions","text":"<ul> <li>The client will call the API <code>release</code> to release the session and clear the session information when you run <code>exit</code> after the operation ends. If you exit the database in an unexpected way and the session timeout duration is not set via <code>session_idle_timeout_secs</code> in nebula-graphd.conf, the session will not be released automatically. For those sessions that are not automatically released, you need to delete them manually. For details, see KILL SESSIONS.</li> </ul> <ul> <li><code>SHOW SESSIONS</code> queries the session information of all the Graph services.</li> </ul> <ul> <li><code>SHOW LOCAL SESSIONS</code> queries the session information of the currently connected Graph service and does not query the session information of other Graph services.</li> </ul> <ul> <li><code>SHOW SESSION &lt;Session_Id&gt;</code> queries the session information with a specific session id.</li> </ul>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/17.show-sessions/#syntax","title":"Syntax","text":"<pre><code>SHOW [LOCAL] SESSIONS;\nSHOW SESSION &lt;Session_Id&gt;;\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/17.show-sessions/#examples","title":"Examples","text":"<pre><code>nebula&gt; SHOW SESSIONS;\n+------------------+----------+--------------------+----------------------------+----------------------------+------------------+----------+--------------------+\n| SessionId        | UserName | SpaceName          | CreateTime                 | UpdateTime                 | GraphAddr        | Timezone | ClientIp           |\n+------------------+----------+--------------------+----------------------------+----------------------------+------------------+----------+--------------------+\n| 1651220858102296 | \"root\"   | \"basketballplayer\" | 2022-04-29T08:27:38.102296 | 2022-04-29T08:50:46.282921 | \"127.0.0.1:9669\" | 0        | \"127.0.0.1\" |\n| 1651199330300991 | \"root\"   | \"basketballplayer\" | 2022-04-29T02:28:50.300991 | 2022-04-29T08:16:28.339038 | \"127.0.0.1:9669\" | 0        | \"127.0.0.1\" |\n| 1651112899847744 | \"root\"   | \"basketballplayer\" | 2022-04-28T02:28:19.847744 | 2022-04-28T08:17:44.470210 | \"127.0.0.1:9669\" | 0        | \"127.0.0.1\" |\n| 1651041092662100 | \"root\"   | \"basketballplayer\" | 2022-04-27T06:31:32.662100 | 2022-04-27T07:01:25.200978 | \"127.0.0.1:9669\" | 0        | \"127.0.0.1\" |\n| 1650959429593975 | \"root\"   | \"basketballplayer\" | 2022-04-26T07:50:29.593975 | 2022-04-26T07:51:47.184810 | \"127.0.0.1:9669\" | 0        | \"127.0.0.1\" |\n| 1650958897679595 | \"root\"   | \"\"                 | 2022-04-26T07:41:37.679595 | 2022-04-26T07:41:37.683802 | \"127.0.0.1:9669\" | 0        | \"127.0.0.1\" |\n+------------------+----------+--------------------+----------------------------+----------------------------+------------------+----------+--------------------+\n\nnebula&gt; SHOW SESSION 1635254859271703;\n+------------------+----------+--------------------+----------------------------+----------------------------+------------------+----------+--------------------+\n| SessionId        | UserName | SpaceName          | CreateTime                 | UpdateTime                 | GraphAddr        | Timezone | ClientIp           |\n+------------------+----------+--------------------+----------------------------+----------------------------+------------------+----------+--------------------+\n| 1651220858102296 | \"root\"   | \"basketballplayer\" | 2022-04-29T08:27:38.102296 | 2022-04-29T08:50:54.254384 | \"127.0.0.1:9669\" | 0        | \"127.0.0.1\" |\n+------------------+----------+--------------------+----------------------------+----------------------------+------------------+----------+--------------------+\n</code></pre> Parameter Description <code>SessionId</code> The session ID, namely the identifier of a session. <code>UserName</code> The username in a session. <code>SpaceName</code> The name of the graph space that the user uses currently. It is null (<code>\"\"</code>) when you first log in because there is no specified graph space. <code>CreateTime</code> The time when the session is created, namely the time when the user logs in. The time zone is specified by <code>timezone_name</code> in the configuration file. <code>UpdateTime</code> The system will update the time when there is an operation. The time zone is specified by <code>timezone_name</code> in the configuration file. <code>GraphAddr</code> The IP address and port of the Graph server that hosts the session. <code>Timezone</code> A reserved parameter that has no specified meaning for now. <code>ClientIp</code> The IP address of the client."},{"location":"3.ngql-guide/7.general-query-statements/6.show/18.show-queries/","title":"SHOW QUERIES","text":"<p>The <code>SHOW QUERIES</code> statement shows the information of working queries in the current session.</p> <p>Note</p> <p>To terminate queries, see Kill Query.</p>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/18.show-queries/#precautions","title":"Precautions","text":"<ul> <li>The <code>SHOW LOCAL QUERIES</code> statement gets the status of queries in the current session from the local cache with almost no latency.</li> </ul> <ul> <li>The <code>SHOW QUERIES</code> statement gets the information of queries in all the sessions from the Meta Service. The information will be synchronized to the Meta Service according to the interval defined by <code>session_reclaim_interval_secs</code>. Therefore the information that you get from the client may belong to the last synchronization interval.</li> </ul>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/18.show-queries/#syntax","title":"Syntax","text":"<pre><code>SHOW [LOCAL] QUERIES;\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/18.show-queries/#examples","title":"Examples","text":"<pre><code>nebula&gt; SHOW LOCAL QUERIES;\n+------------------+-----------------+--------+----------------------+----------------------------+----------------+-----------+-----------------------+\n| SessionID        | ExecutionPlanID | User   | Host                 | StartTime                  | DurationInUSec | Status    | Query                 |\n+------------------+-----------------+--------+----------------------+----------------------------+----------------+-----------+-----------------------+\n| 1625463842921750 | 46              | \"root\" | \"\"192.168.x.x\":9669\" | 2021-07-05T05:44:19.502903 | 0              | \"RUNNING\" | \"SHOW LOCAL QUERIES;\" |\n+------------------+-----------------+--------+----------------------+----------------------------+----------------+-----------+-----------------------+\n\nnebula&gt; SHOW QUERIES;\n+------------------+-----------------+---------+----------------------+----------------------------+----------------+-----------+---------------------------------------------------------+\n| SessionID        | ExecutionPlanID | User    | Host                 | StartTime                  | DurationInUSec | Status    | Query                                                   |\n+------------------+-----------------+---------+----------------------+----------------------------+----------------+-----------+---------------------------------------------------------+\n| 1625456037718757 | 54              | \"user1\" | \"\"192.168.x.x\":9669\" | 2021-07-05T05:51:08.691318 | 1504502        | \"RUNNING\" | \"MATCH p=(v:player)-[*1..4]-(v2) RETURN v2 AS Friends;\" |\n+------------------+-----------------+---------+----------------------+----------------------------+----------------+-----------+---------------------------------------------------------+\n\n# The following statement returns the top 10 queries that have the longest duration.\nnebula&gt; SHOW QUERIES | ORDER BY $-.DurationInUSec DESC | LIMIT 10;\n+------------------+-----------------+---------+----------------------+----------------------------+----------------+-----------+-------------------------------------------------------+\n| SessionID        | ExecutionPlanID | User    | Host                 | StartTime                  | DurationInUSec | Status    | Query                                                 |\n+------------------+-----------------+---------+----------------------+----------------------------+----------------+-----------+-------------------------------------------------------+\n| 1625471375320831 | 98              | \"user2\" | \"\"192.168.x.x\":9669\" | 2021-07-05T07:50:24.461779 | 2608176        | \"RUNNING\" | \"MATCH (v:player)-[*1..4]-(v2) RETURN v2 AS Friends;\" |\n| 1625456037718757 | 99              | \"user1\" | \"\"192.168.x.x\":9669\" | 2021-07-05T07:50:24.910616 | 2159333        | \"RUNNING\" | \"MATCH (v:player)-[*1..4]-(v2) RETURN v2 AS Friends;\" |\n+------------------+-----------------+---------+----------------------+----------------------------+----------------+-----------+-------------------------------------------------------+\n</code></pre> <p>The descriptions are as follows.</p> Parameter Description <code>SessionID</code> The session ID. <code>ExecutionPlanID</code> The ID of the execution plan. <code>User</code> The username that executes the query. <code>Host</code> The IP address and port of the Graph server that hosts the session. <code>StartTime</code> The time when the query starts. <code>DurationInUSec</code> The duration of the query. The unit is microsecond. <code>Status</code> The current status of the query. <code>Query</code> The query statement."},{"location":"3.ngql-guide/7.general-query-statements/6.show/19.show-meta-leader/","title":"SHOW META LEADER","text":"<p>The <code>SHOW META LEADER</code> statement shows the information of the leader in the current Meta cluster.</p> <p>For more information about the Meta service, see Meta service.</p>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/19.show-meta-leader/#syntax","title":"Syntax","text":"<pre><code>SHOW META LEADER;\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/19.show-meta-leader/#example","title":"Example","text":"<pre><code>nebula&gt; SHOW META LEADER;\n+------------------+---------------------------+\n| Meta Leader      | secs from last heart beat |\n+------------------+---------------------------+\n| \"127.0.0.1:9559\" | 3                         |\n+------------------+---------------------------+\n</code></pre> Parameter Description <code>Meta Leader</code> Shows the information of the leader in the Meta cluster, including the IP address and port of the server where the leader is located. <code>secs from last heart beat</code> Indicates the time interval since the last heartbeat. This parameter is measured in seconds."},{"location":"3.ngql-guide/7.general-query-statements/6.show/2.show-collation/","title":"SHOW COLLATION","text":"<p>The <code>SHOW COLLATION</code> statement shows the collations supported by NebulaGraph.</p> <p>Currently available types are: <code>utf8_bin</code> and <code>utf8mb4_bin</code>.</p> <ul> <li>When the character set is <code>utf8</code>, the default collate is <code>utf8_bin</code>.</li> </ul> <ul> <li>When the character set is <code>utf8mb4</code>, the default collate is <code>utf8mb4_bin</code>.</li> </ul>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/2.show-collation/#syntax","title":"Syntax","text":"<pre><code>SHOW COLLATION;\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/2.show-collation/#example","title":"Example","text":"<pre><code>nebula&gt; SHOW COLLATION;\n+------------+---------+\n| Collation  | Charset |\n+------------+---------+\n| \"utf8_bin\" | \"utf8\"  |\n+------------+---------+\n</code></pre> Parameter Description <code>Collation</code> The name of the collation. <code>Charset</code> The name of the character set with which the collation is associated."},{"location":"3.ngql-guide/7.general-query-statements/6.show/4.show-create-space/","title":"SHOW CREATE SPACE","text":"<p>The <code>SHOW CREATE SPACE</code> statement shows the creating statement of the specified graph space.</p> <p>For details about the graph space information, see CREATE SPACE.</p>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/4.show-create-space/#syntax","title":"Syntax","text":"<pre><code>SHOW CREATE SPACE &lt;space_name&gt;;\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/4.show-create-space/#example","title":"Example","text":"<pre><code>nebula&gt; SHOW CREATE SPACE basketballplayer;\n+--------------------+---------------------------------------------------------------------------------------------------------------------------------------------+\n| Space              | Create Space                                                                                                                                |\n+--------------------+---------------------------------------------------------------------------------------------------------------------------------------------+\n| \"basketballplayer\" | \"CREATE SPACE `basketballplayer` (partition_num = 10, replica_factor = 1, charset = utf8, collate = utf8_bin, vid_type = FIXED_STRING(32))\" |\n+--------------------+---------------------------------------------------------------------------------------------------------------------------------------------+\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/5.show-create-tag-edge/","title":"SHOW CREATE TAG/EDGE","text":"<p>The <code>SHOW CREATE TAG</code> statement shows the basic information of the specified tag. For details about the tag, see CREATE TAG.</p> <p>The <code>SHOW CREATE EDGE</code> statement shows the basic information of the specified edge type. For details about the edge type, see CREATE EDGE.</p>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/5.show-create-tag-edge/#syntax","title":"Syntax","text":"<pre><code>SHOW CREATE {TAG &lt;tag_name&gt; | EDGE &lt;edge_name&gt;};\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/5.show-create-tag-edge/#examples","title":"Examples","text":"<pre><code>nebula&gt; SHOW CREATE TAG player;\n+----------+-----------------------------------+\n| Tag      | Create Tag                        |\n+----------+-----------------------------------+\n| \"player\" | \"CREATE TAG `player` (            |\n|          |  `name` string NULL,              |\n|          |  `age` int64 NULL                 |\n|          | ) ttl_duration = 0, ttl_col = \"\"\" |\n+----------+-----------------------------------+\n\nnebula&gt; SHOW CREATE EDGE follow;\n+----------+-----------------------------------+\n| Edge     | Create Edge                       |\n+----------+-----------------------------------+\n| \"follow\" | \"CREATE EDGE `follow` (           |\n|          |  `degree` int64 NULL              |\n|          | ) ttl_duration = 0, ttl_col = \"\"\" |\n+----------+-----------------------------------+\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/6.show-hosts/","title":"SHOW HOSTS","text":"<p>The <code>SHOW HOSTS</code> statement shows the cluster information, including the port, status, leader, partition, and version information. You can also add the service type in the statement to view the information of the specific service.</p>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/6.show-hosts/#syntax","title":"Syntax","text":"<pre><code>SHOW HOSTS [GRAPH | STORAGE | META];\n</code></pre> <p>Note</p> <p>For a NebulaGraph cluster installed with the source code, the version of the cluster will not be displayed in the output after executing the command <code>SHOW HOSTS (GRAPH | STORAGE | META)</code> with the service name.</p>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/6.show-hosts/#examples","title":"Examples","text":"<pre><code>nebula&gt; SHOW HOSTS;\n+-------------+-------+----------+--------------+----------------------------------+------------------------------+---------+\n| Host        | Port  | Status   | Leader count | Leader distribution              | Partition distribution       | Version |\n+-------------+-------+----------+--------------+----------------------------------+------------------------------+---------+\n| \"storaged0\" | 9779  | \"ONLINE\" | 8            | \"docs:5, basketballplayer:3\"     | \"docs:5, basketballplayer:3\" | \"3.5.0\" |\n| \"storaged1\" | 9779  | \"ONLINE\" | 9            | \"basketballplayer:4, docs:5\"     | \"docs:5, basketballplayer:4\" | \"3.5.0\" |\n| \"storaged2\" | 9779  | \"ONLINE\" | 8            | \"basketballplayer:3, docs:5\"     | \"docs:5, basketballplayer:3\" | \"3.5.0\" |\n+-------------+-------+----------+--------------+----------------------------------+------------------------------+---------+\n\nnebula&gt; SHOW HOSTS GRAPH;\n+-----------+------+----------+---------+--------------+---------+\n| Host      | Port | Status   | Role    | Git Info Sha | Version |\n+-----------+------+----------+---------+--------------+---------+\n| \"graphd\"  | 9669 | \"ONLINE\" | \"GRAPH\" | \"3ba41bd\"    | \"3.5.0\" |\n| \"graphd1\" | 9669 | \"ONLINE\" | \"GRAPH\" | \"3ba41bd\"    | \"3.5.0\" |\n| \"graphd2\" | 9669 | \"ONLINE\" | \"GRAPH\" | \"3ba41bd\"    | \"3.5.0\" |\n+-----------+------+----------+---------+--------------+---------+\n\nnebula&gt; SHOW HOSTS STORAGE;\n+-------------+------+----------+-----------+--------------+---------+\n| Host        | Port | Status   | Role      | Git Info Sha | Version |\n+-------------+------+----------+-----------+--------------+---------+\n| \"storaged0\" | 9779 | \"ONLINE\" | \"STORAGE\" | \"3ba41bd\"    | \"3.5.0\" |\n| \"storaged1\" | 9779 | \"ONLINE\" | \"STORAGE\" | \"3ba41bd\"    | \"3.5.0\" |\n| \"storaged2\" | 9779 | \"ONLINE\" | \"STORAGE\" | \"3ba41bd\"    | \"3.5.0\" |\n+-------------+------+----------+-----------+--------------+---------+\n\nnebula&gt; SHOW HOSTS META;\n+----------+------+----------+--------+--------------+---------+\n| Host     | Port | Status   | Role   | Git Info Sha | Version |\n+----------+------+----------+--------+--------------+---------+\n| \"metad2\" | 9559 | \"ONLINE\" | \"META\" | \"3ba41bd\"    | \"3.5.0\" |\n| \"metad0\" | 9559 | \"ONLINE\" | \"META\" | \"3ba41bd\"    | \"3.5.0\" |\n| \"metad1\" | 9559 | \"ONLINE\" | \"META\" | \"3ba41bd\"    | \"3.5.0\" |\n+----------+------+----------+--------+--------------+---------+\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/7.show-index-status/","title":"SHOW INDEX STATUS","text":"<p>The <code>SHOW INDEX STATUS</code> statement shows the status of jobs that rebuild native indexes, which helps check whether a native index is successfully rebuilt or not.</p>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/7.show-index-status/#syntax","title":"Syntax","text":"<pre><code>SHOW {TAG | EDGE} INDEX STATUS;\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/7.show-index-status/#examples","title":"Examples","text":"<pre><code>nebula&gt; SHOW TAG INDEX STATUS;\n+------------------------------------+--------------+\n| Name                               | Index Status |\n+------------------------------------+--------------+\n| \"date1_index\"                      | \"FINISHED\"   |\n| \"basketballplayer_all_tag_indexes\" | \"FINISHED\"   |\n| \"any_shape_geo_index\"              | \"FINISHED\"   |\n+------------------------------------+--------------+\n\nnebula&gt; SHOW EDGE INDEX STATUS;\n+----------------+--------------+\n| Name           | Index Status |\n+----------------+--------------+\n| \"follow_index\" | \"FINISHED\"   |\n+----------------+--------------+\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/7.show-index-status/#related_topics","title":"Related topics","text":"<ul> <li>Job manager and the JOB statements</li> <li>REBUILD NATIVE INDEX</li> </ul>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/8.show-indexes/","title":"SHOW INDEXES","text":"<p>The <code>SHOW INDEXES</code> statement shows the names of existing native indexes.</p>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/8.show-indexes/#syntax","title":"Syntax","text":"<pre><code>SHOW {TAG | EDGE} INDEXES;\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/8.show-indexes/#examples","title":"Examples","text":"<pre><code>nebula&gt; SHOW TAG INDEXES;\n+------------------+----------+----------+\n| Index Name       | By Tag   | Columns  |\n+------------------+----------+----------+\n| \"player_index_0\" | \"player\" | []       |\n| \"player_index_1\" | \"player\" | [\"name\"] |\n+------------------+----------+----------+\n\nnebula&gt; SHOW EDGE INDEXES;\n+----------------+----------+---------+\n| Index Name     | By Edge  | Columns |\n+----------------+----------+---------+\n| \"follow_index\" | \"follow\" | []      |\n+----------------+----------+---------+\n</code></pre> <p>Legacy version compatibility</p> <p>In NebulaGraph 2.x, <code>SHOW TAG/EDGE INDEXES</code> only returns <code>Names</code>.</p>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/9.show-parts/","title":"SHOW PARTS","text":"<p>The <code>SHOW PARTS</code> statement shows the information of a specified partition or all partitions in a graph space.</p>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/9.show-parts/#syntax","title":"Syntax","text":"<pre><code>SHOW PARTS [&lt;part_id&gt;];\n</code></pre>"},{"location":"3.ngql-guide/7.general-query-statements/6.show/9.show-parts/#examples","title":"Examples","text":"<pre><code>nebula&gt; SHOW PARTS;\n+--------------+--------------------+--------------------+-------+\n| Partition ID | Leader             | Peers              | Losts |\n+--------------+--------------------+--------------------+-------+\n| 1            | \"192.168.2.1:9779\" | \"192.168.2.1:9779\" | \"\"    |\n| 2            | \"192.168.2.2:9779\" | \"192.168.2.2:9779\" | \"\"    |\n| 3            | \"192.168.2.3:9779\" | \"192.168.2.3:9779\" | \"\"    |\n| 4            | \"192.168.2.1:9779\" | \"192.168.2.1:9779\" | \"\"    |\n| 5            | \"192.168.2.2:9779\" | \"192.168.2.2:9779\" | \"\"    |\n| 6            | \"192.168.2.3:9779\" | \"192.168.2.3:9779\" | \"\"    |\n| 7            | \"192.168.2.1:9779\" | \"192.168.2.1:9779\" | \"\"    |\n| 8            | \"192.168.2.2:9779\" | \"192.168.2.2:9779\" | \"\"    |\n| 9            | \"192.168.2.3:9779\" | \"192.168.2.3:9779\" | \"\"    |\n| 10           | \"192.168.2.1:9779\" | \"192.168.2.1:9779\" | \"\"    |\n+--------------+--------------------+--------------------+-------+\n\nnebula&gt; SHOW PARTS 1;\n+--------------+--------------------+--------------------+-------+\n| Partition ID | Leader             | Peers              | Losts |\n+--------------+--------------------+--------------------+-------+\n| 1            | \"192.168.2.1:9779\" | \"192.168.2.1:9779\" | \"\"    |\n+--------------+--------------------+--------------------+-------+\n</code></pre> <p>The descriptions are as follows.</p> Parameter Description <code>Partition ID</code> The ID of the partition. <code>Leader</code> The IP address and the port of the leader. <code>Peers</code> The IP addresses and the ports of all the replicas. <code>Losts</code> The IP addresses and the ports of replicas at fault."},{"location":"3.ngql-guide/8.clauses-and-options/group-by/","title":"GROUP BY","text":"<p>The <code>GROUP BY</code> clause can be used to aggregate data.</p>"},{"location":"3.ngql-guide/8.clauses-and-options/group-by/#opencypher_compatibility","title":"OpenCypher Compatibility","text":"<p>This topic applies to native nGQL only.</p> <p>You can also use the count() function to aggregate data.</p> <pre><code>nebula&gt;  MATCH (v:player)&lt;-[:follow]-(:player) RETURN v.player.name AS Name, count(*) as cnt ORDER BY cnt DESC;\n+----------------------+-----+\n| Name                 | cnt |\n+----------------------+-----+\n| \"Tim Duncan\"         | 10  |\n| \"LeBron James\"       | 6   |\n| \"Tony Parker\"        | 5   |\n| \"Chris Paul\"         | 4   |\n| \"Manu Ginobili\"      | 4   |\n+----------------------+-----+\n...\n</code></pre>"},{"location":"3.ngql-guide/8.clauses-and-options/group-by/#syntax","title":"Syntax","text":"<p>The <code>GROUP BY</code> clause groups the rows with the same value. Then operations such as counting, sorting, and calculation can be applied.</p> <p>The <code>GROUP BY</code> clause works after the pipe symbol (|) and before a <code>YIELD</code> clause.</p> <pre><code>| GROUP BY &lt;var&gt; YIELD &lt;var&gt;, &lt;aggregation_function(var)&gt;\n</code></pre> <p>The <code>aggregation_function()</code> function supports <code>avg()</code>, <code>sum()</code>, <code>max()</code>, <code>min()</code>, <code>count()</code>, <code>collect()</code>, and <code>std()</code>.</p>"},{"location":"3.ngql-guide/8.clauses-and-options/group-by/#examples","title":"Examples","text":"<p>The following statement finds all the vertices connected directly to vertex <code>\"player100\"</code>, groups the result set by player names, and counts how many times the name shows up in the result set.</p> <pre><code>nebula&gt; GO FROM \"player100\" OVER follow BIDIRECT \\\n        YIELD properties($$).name as Name \\\n        | GROUP BY $-.Name \\\n        YIELD $-.Name as Player, count(*) AS Name_Count;\n+---------------------+------------+\n| Player              | Name_Count |\n+---------------------+------------+\n| \"Shaquille O'Neal\"  | 1          |\n| \"Tiago Splitter\"    | 1          |\n| \"Manu Ginobili\"     | 2          |\n| \"Boris Diaw\"        | 1          |\n| \"LaMarcus Aldridge\" | 1          |\n| \"Tony Parker\"       | 2          |\n| \"Marco Belinelli\"   | 1          |\n| \"Dejounte Murray\"   | 1          |\n| \"Danny Green\"       | 1          |\n| \"Aron Baynes\"       | 1          |\n+---------------------+------------+\n</code></pre> <p>The following statement finds all the vertices connected directly to vertex <code>\"player100\"</code>, groups the result set by source vertices, and returns the sum of degree values.</p> <pre><code>nebula&gt; GO FROM \"player100\" OVER follow \\\n        YIELD src(edge) AS player, properties(edge).degree AS degree \\\n        | GROUP BY $-.player \\\n        YIELD sum($-.degree);\n+----------------+\n| sum($-.degree) |\n+----------------+\n| 190            |\n+----------------+\n</code></pre> <p>For more information about the <code>sum()</code> function, see Built-in math functions.</p>"},{"location":"3.ngql-guide/8.clauses-and-options/group-by/#implicit_group_by","title":"Implicit GROUP BY","text":"<p>The usage of <code>GROUP BY</code> in the above nGQL statements that explicitly write <code>GROUP BY</code> and act as grouping fields is called explicit <code>GROUP BY</code>, while in openCypher, the <code>GROUP BY</code> is implicit, i.e., <code>GROUP BY</code> groups fields without explicitly writing <code>GROUP BY</code>. The explicit <code>GROUP BY</code> in nGQL is the same as the implicit <code>GROUP BY</code> in openCypher, and nGQL also supports the implicit <code>GROUP BY</code>. For the implicit usage of <code>GROUP BY</code>, see how-to-make-group-by-in-a-cypher-query.</p> <p>For example, to look up the players over 34 years old with the same length of service, you can use the following statement:</p> <pre><code>nebula&gt; LOOKUP ON player WHERE player.age &gt; 34 YIELD id(vertex) AS v | \\\n        GO FROM $-.v OVER serve YIELD serve.start_year AS start_year, serve.end_year AS end_year | \\\n        YIELD $-.start_year, $-.end_year, count(*) AS count | \\\n        ORDER BY $-.count DESC | LIMIT 5;\n+---------------+-------------+-------+\n| $-.start_year | $-.end_year | count |\n+---------------+-------------+-------+\n| 2018          | 2019        | 3     |\n| 2007          | 2012        | 2     |\n| 1998          | 2004        | 2     |\n| 2017          | 2018        | 2     |\n| 2010          | 2011        | 2     |\n+---------------+-------------+-------+ \n</code></pre>"},{"location":"3.ngql-guide/8.clauses-and-options/limit/","title":"LIMIT AND SKIP","text":"<p>The <code>LIMIT</code> clause constrains the number of rows in the output. The usage of <code>LIMIT</code> in native nGQL statements and openCypher compatible statements is different.</p> <ul> <li>Native nGQL: Generally, a pipe <code>|</code> needs to be used before the <code>LIMIT</code> clause. The offset parameter can be set or omitted directly after the <code>LIMIT</code> statement.</li> </ul> <ul> <li>OpenCypher compatible statements: No pipes are permitted before the <code>LIMIT</code> clause. And you can use <code>SKIP</code> to indicate an offset.</li> </ul> <p>Note<p>When using <code>LIMIT</code> in either syntax above, it is important to use an <code>ORDER BY</code> clause that constrains the output into a unique order. Otherwise, you will get an unpredictable subset of the output.</p> </p>"},{"location":"3.ngql-guide/8.clauses-and-options/limit/#limit_in_native_ngql_statements","title":"LIMIT in native nGQL statements","text":"<p>In native nGQL, <code>LIMIT</code> has general syntax and exclusive syntax in <code>GO</code> statements.</p>"},{"location":"3.ngql-guide/8.clauses-and-options/limit/#general_limit_syntax_in_native_ngql_statements","title":"General LIMIT syntax in native nGQL statements","text":"<p>In native nGQL,  the general <code>LIMIT</code> syntax works the same as in <code>SQL</code>. The <code>LIMIT</code> clause accepts one or two parameters. The values of both parameters must be non-negative integers and be used after a pipe. The syntax and description are as follows:</p> <pre><code>... | LIMIT [&lt;offset&gt;,] &lt;number_rows&gt;;\n</code></pre> Parameter Description <code>offset</code> The offset value. It defines the row from which to start returning. The offset starts from <code>0</code>. The default value is <code>0</code>, which returns from the first row. <code>number_rows</code> It constrains the total number of returned rows. <p>For example:</p> <pre><code># The following example returns the top 3 rows of data from the result.\nnebula&gt; LOOKUP ON player YIELD id(vertex)|\\\n        LIMIT 3;\n+-------------+\n| id(VERTEX)  |\n+-------------+\n| \"player100\" |\n| \"player101\" |\n| \"player102\" |\n+-------------+\n\n# The following example returns the 3 rows of data starting from the second row of the sorted output.\nnebula&gt; GO FROM \"player100\" OVER follow REVERSELY \\\n        YIELD properties($$).name AS Friend, properties($$).age AS Age \\\n        | ORDER BY $-.Age, $-.Friend \\\n        | LIMIT 1, 3;\n+-------------------+-----+\n| Friend            | Age |\n+-------------------+-----+\n| \"Danny Green\"     | 31  |\n| \"Aron Baynes\"     | 32  |\n| \"Marco Belinelli\" | 32  |\n+-------------------+-----+\n</code></pre>"},{"location":"3.ngql-guide/8.clauses-and-options/limit/#limit_in_go_statements","title":"LIMIT in GO statements","text":"<p>In addition to the general syntax in the native nGQL, the <code>LIMIT</code> in the <code>GO</code> statement also supports limiting the number of output results based on edges.</p> <p>Syntax:</p> <pre><code>&lt;go_statement&gt; LIMIT &lt;limit_list&gt;;\n</code></pre> <p><code>limit_list</code> is a list. Elements in the list must be natural numbers, and the number of elements must be the same as the maximum number of <code>STEPS</code> in the <code>GO</code> statement. The following takes <code>GO 1 TO 3 STEPS FROM \"A\" OVER * LIMIT &lt;limit_list&gt;</code> as an example to introduce this usage of <code>LIMIT</code> in detail.</p> <ul> <li>The list <code>limit_list</code> must contain 3 natural numbers, such as <code>GO 1 TO 3 STEPS FROM \"A\" OVER * LIMIT [1,2,4]</code>.</li> <li><code>1</code> in <code>LIMIT [1,2,4]</code> means that the system automatically selects 1 edge to continue traversal in the first step. <code>2</code> means to select 2 edges to continue traversal in the second step. <code>4</code> indicates that 4 edges are selected to continue traversal in the third step.</li> <li>Because <code>GO 1 TO 3 STEPS</code> means to return all the traversal results from the first to third steps, all the red edges and their source and destination vertices in the figure below will be matched by this <code>GO</code> statement. And the yellow edges represent there is no path selected when the GO statement traverses. If it is not <code>GO 1 TO 3 STEPS</code> but <code>GO 3 STEPS</code>, it will only match the red edges of the third step and the vertices at both ends.</li> </ul> <p></p> <p>In the basketballplayer dataset, the example is as follows:</p> <pre><code>nebula&gt; GO 3 STEPS FROM \"player100\" \\\n        OVER * \\\n        YIELD properties($$).name AS NAME, properties($$).age AS Age \\\n        LIMIT [3,3,3];\n+-----------------+----------+\n| NAME            | Age      |\n+-----------------+----------+\n| \"Tony Parker\"   | 36       |\n| \"Manu Ginobili\" | 41       |\n| \"Spurs\"         | __NULL__ |\n+-----------------+----------+\n\nnebula&gt; GO 3 STEPS FROM \"player102\" OVER * BIDIRECT\\\n        YIELD dst(edge) \\\n        LIMIT [rand32(5),rand32(5),rand32(5)];\n+-------------+\n| dst(EDGE)   |\n+-------------+\n| \"player100\" |\n| \"player100\" |\n+-------------+\n</code></pre>"},{"location":"3.ngql-guide/8.clauses-and-options/limit/#limit_in_opencypher_compatible_statements","title":"LIMIT in openCypher compatible statements","text":"<p>In openCypher compatible statements such as <code>MATCH</code>, there is no need to use a pipe when <code>LIMIT</code> is used. The syntax and description are as follows:</p> <pre><code>... [SKIP &lt;offset&gt;] [LIMIT &lt;number_rows&gt;];\n</code></pre> Parameter Description <code>offset</code> The offset value. It defines the row from which to start returning. The offset starts from <code>0</code>. The default value is <code>0</code>, which returns from the first row. <code>number_rows</code> It constrains the total number of returned rows. <p>Both <code>offset</code> and <code>number_rows</code> accept expressions, but the result of the expression must be a non-negative integer.</p> <p>Note</p> <p>Fraction expressions composed of two integers are automatically floored to integers. For example, <code>8/6</code> is floored to 1.</p>"},{"location":"3.ngql-guide/8.clauses-and-options/limit/#examples_of_limit","title":"Examples of LIMIT","text":"<p><code>LIMIT</code> can be used alone to return a specified number of results.</p> <pre><code>nebula&gt; MATCH (v:player) RETURN v.player.name AS Name, v.player.age AS Age \\\n        ORDER BY Age LIMIT 5;\n+-------------------------+-----+\n| Name                    | Age |\n+-------------------------+-----+\n| \"Luka Doncic\"           | 20  |\n| \"Ben Simmons\"           | 22  |\n| \"Kristaps Porzingis\"    | 23  |\n| \"Giannis Antetokounmpo\" | 24  |\n| \"Kyle Anderson\"         | 25  |\n+-------------------------+-----+\n</code></pre>"},{"location":"3.ngql-guide/8.clauses-and-options/limit/#examples_of_skip","title":"Examples of SKIP","text":"<p><code>SKIP</code> can be used alone to set the offset and return the data after the specified position.</p> <pre><code>nebula&gt; MATCH (v:player{name:\"Tim Duncan\"}) --&gt; (v2) \\\n        RETURN v2.player.name AS Name, v2.player.age AS Age \\\n        ORDER BY Age DESC SKIP 1;\n+-----------------+-----+\n| Name            | Age |\n+-----------------+-----+\n| \"Manu Ginobili\" | 41  |\n| \"Tony Parker\"   | 36  |\n+-----------------+-----+\n\nnebula&gt; MATCH (v:player{name:\"Tim Duncan\"}) --&gt; (v2) \\\n        RETURN v2.player.name AS Name, v2.player.age AS Age \\\n        ORDER BY Age DESC SKIP 1+1;\n+---------------+-----+\n| Name          | Age |\n+---------------+-----+\n| \"Tony Parker\" | 36  |\n+---------------+-----+\n</code></pre>"},{"location":"3.ngql-guide/8.clauses-and-options/limit/#example_of_skip_and_limit","title":"Example of SKIP and LIMIT","text":"<p><code>SKIP</code> and <code>LIMIT</code> can be used together to return the specified amount of data starting from the specified position.</p> <pre><code>nebula&gt; MATCH (v:player{name:\"Tim Duncan\"}) --&gt; (v2) \\\n        RETURN v2.player.name AS Name, v2.player.age AS Age \\\n        ORDER BY Age DESC SKIP 1 LIMIT 1;\n+-----------------+-----+\n| Name            | Age |\n+-----------------+-----+\n| \"Manu Ginobili\" | 41  |\n+-----------------+-----+\n</code></pre>"},{"location":"3.ngql-guide/8.clauses-and-options/order-by/","title":"ORDER BY","text":"<p>The <code>ORDER BY</code> clause specifies the order of the rows in the output.</p> <ul> <li>Native nGQL: You must use a pipe (<code>|</code>) and an <code>ORDER BY</code> clause after <code>YIELD</code> clause.</li> </ul> <ul> <li>OpenCypher style: No pipes are permitted. The <code>ORDER BY</code> clause follows a <code>RETURN</code> clause.</li> </ul> <p>There are two order options:</p> <ul> <li><code>ASC</code>: Ascending. <code>ASC</code> is the default order.</li> <li><code>DESC</code>: Descending.</li> </ul>"},{"location":"3.ngql-guide/8.clauses-and-options/order-by/#native_ngql_syntax","title":"Native nGQL Syntax","text":"<pre><code>&lt;YIELD clause&gt;\n| ORDER BY &lt;expression&gt; [ASC | DESC] [, &lt;expression&gt; [ASC | DESC] ...];\n</code></pre> <p>Compatibility</p> <p>In the native nGQL syntax, <code>$-.</code> must be used after <code>ORDER BY</code>. But it is not required in releases prior to 2.5.0.</p>"},{"location":"3.ngql-guide/8.clauses-and-options/order-by/#examples","title":"Examples","text":"<pre><code>nebula&gt; FETCH PROP ON player \"player100\", \"player101\", \"player102\", \"player103\" \\\n        YIELD player.age AS age, player.name AS name \\\n        | ORDER BY $-.age ASC, $-.name DESC;\n+-----+---------------------+\n| age | name                |\n+-----+---------------------+\n| 32  | \"Rudy Gay\"          |\n| 33  | \"LaMarcus Aldridge\" |\n| 36  | \"Tony Parker\"       |\n| 42  | \"Tim Duncan\"        |\n+-----+---------------------+\n\nnebula&gt; $var = GO FROM \"player100\" OVER follow \\\n        YIELD dst(edge) AS dst; \\\n        ORDER BY $var.dst DESC;\n+-------------+\n| dst         |\n+-------------+\n| \"player125\" |\n| \"player101\" |\n+-------------+\n</code></pre>"},{"location":"3.ngql-guide/8.clauses-and-options/order-by/#opencypher_syntax","title":"OpenCypher Syntax","text":"<pre><code>&lt;RETURN clause&gt;\nORDER BY &lt;expression&gt; [ASC | DESC] [, &lt;expression&gt; [ASC | DESC] ...];\n</code></pre>"},{"location":"3.ngql-guide/8.clauses-and-options/order-by/#examples_1","title":"Examples","text":"<pre><code>nebula&gt; MATCH (v:player) RETURN v.player.name AS Name, v.player.age AS Age  \\\n        ORDER BY Name DESC;\n+-----------------+-----+\n| Name            | Age |\n+-----------------+-----+\n| \"Yao Ming\"      | 38  |\n| \"Vince Carter\"  | 42  |\n| \"Tracy McGrady\" | 39  |\n| \"Tony Parker\"   | 36  |\n| \"Tim Duncan\"    | 42  |\n+-----------------+-----+\n...\n\n# In the following example, nGQL sorts the rows by age first. If multiple people are of the same age, nGQL will then sort them by name.\nnebula&gt; MATCH (v:player) RETURN v.player.age AS Age, v.player.name AS Name  \\\n        ORDER BY Age DESC, Name ASC;\n+-----+-------------------+\n| Age | Name              |\n+-----+-------------------+\n| 47  | \"Shaquille O'Neal\" |\n| 46  | \"Grant Hill\"      |\n| 45  | \"Jason Kidd\"      |\n| 45  | \"Steve Nash\"      |\n+-----+-------------------+\n...\n</code></pre>"},{"location":"3.ngql-guide/8.clauses-and-options/order-by/#order_of_null_values","title":"Order of NULL values","text":"<p>nGQL lists NULL values at the end of the output for ascending sorting, and at the start for descending sorting.</p> <pre><code>nebula&gt; MATCH (v:player{name:\"Tim Duncan\"}) --&gt; (v2) \\\n        RETURN v2.player.name AS Name, v2.player.age AS Age  \\\n        ORDER BY Age;\n+-----------------+----------+\n| Name            | Age      |\n+-----------------+----------+\n| \"Tony Parker\"   | 36       |\n| \"Manu Ginobili\" | 41       |\n| __NULL__        | __NULL__ |\n+-----------------+----------+\n\nnebula&gt; MATCH (v:player{name:\"Tim Duncan\"}) --&gt; (v2) \\\n        RETURN v2.player.name AS Name, v2.player.age AS Age  \\\n        ORDER BY Age DESC;\n+-----------------+----------+\n| Name            | Age      |\n+-----------------+----------+\n| __NULL__        | __NULL__ |\n| \"Manu Ginobili\" | 41       |\n| \"Tony Parker\"   | 36       |\n+-----------------+----------+\n</code></pre>"},{"location":"3.ngql-guide/8.clauses-and-options/return/","title":"RETURN","text":"<p>The <code>RETURN</code> clause defines the output of an nGQL query. To return multiple fields, separate them with commas.</p> <p><code>RETURN</code> can lead a clause or a statement:</p> <ul> <li>A <code>RETURN</code> clause can work in openCypher statements in nGQL, such as <code>MATCH</code> or <code>UNWIND</code>.</li> </ul> <ul> <li>A <code>RETURN</code> statement can work independently to output the result of an expression.</li> </ul>"},{"location":"3.ngql-guide/8.clauses-and-options/return/#opencypher_compatibility","title":"OpenCypher compatibility","text":"<p>This topic applies to the openCypher syntax in nGQL only. For native nGQL, use <code>YIELD</code>.</p> <p><code>RETURN</code> does not support the following openCypher features yet.</p> <ul> <li> <p>Return variables with uncommon characters, for example:</p> <pre><code>MATCH (`non-english_characters`:player) \\\nRETURN `non-english_characters`;\n</code></pre> </li> </ul> <ul> <li> <p>Set a pattern in the <code>RETURN</code> clause and return all elements that this pattern matches, for example:</p> <pre><code>MATCH (v:player) \\\nRETURN (v)-[e]-&gt;(v2);\n</code></pre> </li> </ul>"},{"location":"3.ngql-guide/8.clauses-and-options/return/#map_order_description","title":"Map order description","text":"<p>When <code>RETURN</code> returns the map data structure, the order of key-value pairs is undefined.</p> <pre><code>nebula&gt; RETURN {age: 32, name: \"Marco Belinelli\"};\n+------------------------------------+\n| {age:32,name:\"Marco Belinelli\"}    |\n+------------------------------------+\n| {age: 32, name: \"Marco Belinelli\"} |\n+------------------------------------+\n\nnebula&gt; RETURN {zage: 32, name: \"Marco Belinelli\"};\n+-------------------------------------+\n| {zage:32,name:\"Marco Belinelli\"}    |\n+-------------------------------------+\n| {name: \"Marco Belinelli\", zage: 32} |\n+-------------------------------------+\n</code></pre>"},{"location":"3.ngql-guide/8.clauses-and-options/return/#return_vertices_or_edges","title":"Return vertices or edges","text":"<p>Use the <code>RETURN {&lt;vertex_name&gt; | &lt;edge_name&gt;}</code> to return vertices and edges all information.</p> <pre><code>// Return vertices\nnebula&gt; MATCH (v:player) \\\n        RETURN v;\n+---------------------------------------------------------------+\n| v                                                             |\n+---------------------------------------------------------------+\n| (\"player104\" :player{age: 32, name: \"Marco Belinelli\"})       |\n| (\"player107\" :player{age: 32, name: \"Aron Baynes\"})           |\n| (\"player116\" :player{age: 34, name: \"LeBron James\"})          |\n| (\"player120\" :player{age: 29, name: \"James Harden\"})          |\n| (\"player125\" :player{age: 41, name: \"Manu Ginobili\"})         |\n+---------------------------------------------------------------+\n...\n\n// Return edges\nnebula&gt; MATCH (v:player)-[e]-&gt;() \\\n        RETURN e;\n+------------------------------------------------------------------------------+\n| e                                                                            |\n+------------------------------------------------------------------------------+\n| [:follow \"player104\"-&gt;\"player100\" @0 {degree: 55}]                           |\n| [:follow \"player104\"-&gt;\"player101\" @0 {degree: 50}]                           |\n| [:follow \"player104\"-&gt;\"player105\" @0 {degree: 60}]                           |\n| [:serve \"player104\"-&gt;\"team200\" @0 {end_year: 2009, start_year: 2007}]        |\n| [:serve \"player104\"-&gt;\"team208\" @0 {end_year: 2016, start_year: 2015}]        |\n+------------------------------------------------------------------------------+\n...\n</code></pre>"},{"location":"3.ngql-guide/8.clauses-and-options/return/#return_vids","title":"Return VIDs","text":"<p>Use the <code>id()</code> function to retrieve VIDs.</p> <pre><code>nebula&gt; MATCH (v:player{name:\"Tim Duncan\"}) \\\n        RETURN id(v);\n+-------------+\n| id(v)       |\n+-------------+\n| \"player100\" |\n+-------------+\n</code></pre>"},{"location":"3.ngql-guide/8.clauses-and-options/return/#return_tag","title":"Return Tag","text":"<p>Use the <code>labels()</code> function to return the list of tags on a vertex.</p> <pre><code>nebula&gt; MATCH (v:player{name:\"Tim Duncan\"}) \\\n        RETURN labels(v);\n+------------+\n| labels(v)  |\n+------------+\n| [\"player\"] |\n+------------+\n</code></pre> <p>To retrieve the nth element in the <code>labels(v)</code> list, use <code>labels(v)[n-1]</code>. The following example shows how to use <code>labels(v)[0]</code> to return the first tag in the list.</p> <pre><code>nebula&gt; MATCH (v:player{name:\"Tim Duncan\"}) \\\n        RETURN labels(v)[0];\n+--------------+\n| labels(v)[0] |\n+--------------+\n| \"player\"     |\n+--------------+\n</code></pre>"},{"location":"3.ngql-guide/8.clauses-and-options/return/#return_properties","title":"Return properties","text":"<p>When returning properties of a vertex, it is necessary to specify the tag to which the properties belong because a vertex can have multiple tags and the same property name can appear on different tags.</p> <p>It is possible to specify the tag of a vertex to return all properties of that tag, or to specify both the tag and a property name to return only that property of the tag.</p> <pre><code>nebula&gt; MATCH (v:player) \\\n        RETURN v.player, v.player.name, v.player.age \\\n        LIMIT 3;\n+--------------------------------------+---------------------+--------------+\n| v.player                             | v.player.name       | v.player.age |\n+--------------------------------------+---------------------+--------------+\n| {age: 33, name: \"LaMarcus Aldridge\"} | \"LaMarcus Aldridge\" | 33           |\n| {age: 25, name: \"Kyle Anderson\"}     | \"Kyle Anderson\"     | 25           |\n| {age: 40, name: \"Kobe Bryant\"}       | \"Kobe Bryant\"       | 40           |\n+--------------------------------------+---------------------+--------------+\n</code></pre> <p>When returning edge properties, it is not necessary to specify the edge type to which the properties belong, because an edge can only have one edge type.</p> <pre><code>// Return the property of a vertex\nnebula&gt; MATCH p=(v:player{name:\"Tim Duncan\"})-[]-&gt;(v2) \\\n        RETURN properties(v2);\n+----------------------------------+\n| properties(v2)                   |\n+----------------------------------+\n| {name: \"Spurs\"}                  |\n| {age: 36, name: \"Tony Parker\"}   |\n| {age: 41, name: \"Manu Ginobili\"} |\n+----------------------------------+\n</code></pre> <pre><code>// Return the property of an edge\nnebula&gt; MATCH (v:player{name:\"Tim Duncan\"})-[e]-&gt;() \\\n        RETURN e.start_year, e.degree \\\n+--------------+----------+\n| e.start_year | e.degree |\n+--------------+----------+\n| __NULL__     | 95       |\n| __NULL__     | 95       |\n| 1997         | __NULL__ |\n+--------------+----------+\n</code></pre>"},{"location":"3.ngql-guide/8.clauses-and-options/return/#return_edge_type","title":"Return edge type","text":"<p>Use the <code>type()</code> function to return the matched edge types.</p> <pre><code>nebula&gt; MATCH p=(v:player{name:\"Tim Duncan\"})-[e]-&gt;() \\\n        RETURN DISTINCT type(e);\n+----------+\n| type(e)  |\n+----------+\n| \"serve\"  |\n| \"follow\" |\n+----------+\n</code></pre>"},{"location":"3.ngql-guide/8.clauses-and-options/return/#return_paths","title":"Return paths","text":"<p>Use <code>RETURN &lt;path_name&gt;</code> to return all the information of the matched paths.</p> <pre><code>nebula&gt; MATCH p=(v:player{name:\"Tim Duncan\"})-[*3]-&gt;() \\\n        RETURN p;\n+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| p                                                                                                                                                                                                                                                                                                              |\n+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| &lt;(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]-&gt;(\"player101\" :player{age: 36, name: \"Tony Parker\"})-[:follow@0 {degree: 90}]-&gt;(\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"})-[:serve@0 {end_year: 2019, start_year: 2015}]-&gt;(\"team204\" :team{name: \"Spurs\"})&gt;         |\n| &lt;(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]-&gt;(\"player101\" :player{age: 36, name: \"Tony Parker\"})-[:follow@0 {degree: 90}]-&gt;(\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"})-[:serve@0 {end_year: 2015, start_year: 2006}]-&gt;(\"team203\" :team{name: \"Trail Blazers\"})&gt; |\n| &lt;(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]-&gt;(\"player101\" :player{age: 36, name: \"Tony Parker\"})-[:follow@0 {degree: 90}]-&gt;(\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"})-[:follow@0 {degree: 75}]-&gt;(\"player101\" :player{age: 36, name: \"Tony Parker\"})&gt;           |\n+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n...\n</code></pre>"},{"location":"3.ngql-guide/8.clauses-and-options/return/#return_vertices_in_a_path","title":"Return vertices in a path","text":"<p>Use the <code>nodes()</code> function to return all vertices in a path.</p> <pre><code>nebula&gt; MATCH p=(v:player{name:\"Tim Duncan\"})-[]-&gt;(v2) \\\n        RETURN nodes(p);\n+-------------------------------------------------------------------------------------------------------------+\n| nodes(p)                                                                                                    |\n+-------------------------------------------------------------------------------------------------------------+\n| [(\"player100\" :player{age: 42, name: \"Tim Duncan\"}), (\"team204\" :team{name: \"Spurs\"})]                      |\n| [(\"player100\" :player{age: 42, name: \"Tim Duncan\"}), (\"player101\" :player{age: 36, name: \"Tony Parker\"})]   |\n| [(\"player100\" :player{age: 42, name: \"Tim Duncan\"}), (\"player125\" :player{age: 41, name: \"Manu Ginobili\"})] |\n+-------------------------------------------------------------------------------------------------------------+\n</code></pre>"},{"location":"3.ngql-guide/8.clauses-and-options/return/#return_edges_in_a_path","title":"Return edges in a path","text":"<p>Use the <code>relationships()</code> function to return all edges in a path.</p> <pre><code>nebula&gt; MATCH p=(v:player{name:\"Tim Duncan\"})-[]-&gt;(v2) \\\n        RETURN relationships(p);\n+-------------------------------------------------------------------------+\n| relationships(p)                                                        |\n+-------------------------------------------------------------------------+\n| [[:serve \"player100\"-&gt;\"team204\" @0 {end_year: 2016, start_year: 1997}]] |\n| [[:follow \"player100\"-&gt;\"player101\" @0 {degree: 95}]]                    |\n| [[:follow \"player100\"-&gt;\"player125\" @0 {degree: 95}]]                    |\n+-------------------------------------------------------------------------+\n</code></pre>"},{"location":"3.ngql-guide/8.clauses-and-options/return/#return_path_length","title":"Return path length","text":"<p>Use the <code>length()</code> function to return the length of a path.</p> <pre><code>nebula&gt; MATCH p=(v:player{name:\"Tim Duncan\"})-[*..2]-&gt;(v2) \\\n        RETURN p AS Paths, length(p) AS Length;\n+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+\n| Paths                                                                                                                                                                                                                  | Length |\n+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+\n| &lt;(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:serve@0 {end_year: 2016, start_year: 1997}]-&gt;(\"team204\" :team{name: \"Spurs\"})&gt;                                                                                   | 1      |\n| &lt;(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]-&gt;(\"player101\" :player{age: 36, name: \"Tony Parker\"})&gt;                                                                                     | 1      |\n| &lt;(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]-&gt;(\"player125\" :player{age: 41, name: \"Manu Ginobili\"})&gt;                                                                                   | 1      |\n| &lt;(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]-&gt;(\"player101\" :player{age: 36, name: \"Tony Parker\"})-[:serve@0 {end_year: 2018, start_year: 1999}]-&gt;(\"team204\" :team{name: \"Spurs\"})&gt;     | 2      |\n| &lt;(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]-&gt;(\"player101\" :player{age: 36, name: \"Tony Parker\"})-[:serve@0 {end_year: 2019, start_year: 2018}]-&gt;(\"team215\" :team{name: \"Hornets\"})&gt;   | 2      |\n| &lt;(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]-&gt;(\"player101\" :player{age: 36, name: \"Tony Parker\"})-[:follow@0 {degree: 95}]-&gt;(\"player100\" :player{age: 42, name: \"Tim Duncan\"})&gt;        | 2      |\n| &lt;(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]-&gt;(\"player101\" :player{age: 36, name: \"Tony Parker\"})-[:follow@0 {degree: 90}]-&gt;(\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"})&gt; | 2      |\n| &lt;(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]-&gt;(\"player101\" :player{age: 36, name: \"Tony Parker\"})-[:follow@0 {degree: 95}]-&gt;(\"player125\" :player{age: 41, name: \"Manu Ginobili\"})&gt;     | 2      |\n| &lt;(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]-&gt;(\"player125\" :player{age: 41, name: \"Manu Ginobili\"})-[:serve@0 {end_year: 2018, start_year: 2002}]-&gt;(\"team204\" :team{name: \"Spurs\"})&gt;   | 2      |\n| &lt;(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]-&gt;(\"player125\" :player{age: 41, name: \"Manu Ginobili\"})-[:follow@0 {degree: 90}]-&gt;(\"player100\" :player{age: 42, name: \"Tim Duncan\"})&gt;      | 2      |\n+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+\n</code></pre>"},{"location":"3.ngql-guide/8.clauses-and-options/return/#return_all_elements","title":"Return all elements","text":"<p>To return all the elements that this pattern matches, use an asterisk (*).</p> <pre><code>nebula&gt; MATCH (v:player{name:\"Tim Duncan\"}) \\\n        RETURN *;\n+----------------------------------------------------+\n| v                                                  |\n+----------------------------------------------------+\n| (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) |\n+----------------------------------------------------+\n\nnebula&gt; MATCH (v:player{name:\"Tim Duncan\"})-[e]-&gt;(v2) \\\n        RETURN *;\n+----------------------------------------------------+-----------------------------------------------------------------------+-------------------------------------------------------+\n| v                                                  | e                                                                     | v2                                                    |\n+----------------------------------------------------+-----------------------------------------------------------------------+-------------------------------------------------------+\n| (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | [:follow \"player100\"-&gt;\"player101\" @0 {degree: 95}]                    | (\"player101\" :player{age: 36, name: \"Tony Parker\"})   |\n| (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | [:follow \"player100\"-&gt;\"player125\" @0 {degree: 95}]                    | (\"player125\" :player{age: 41, name: \"Manu Ginobili\"}) |\n| (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | [:serve \"player100\"-&gt;\"team204\" @0 {end_year: 2016, start_year: 1997}] | (\"team204\" :team{name: \"Spurs\"})                      |\n+----------------------------------------------------+-----------------------------------------------------------------------+-------------------------------------------------------+\n</code></pre>"},{"location":"3.ngql-guide/8.clauses-and-options/return/#rename_a_field","title":"Rename a field","text":"<p>Use the <code>AS &lt;alias&gt;</code> syntax to rename a field in the output.</p> <pre><code>nebula&gt; MATCH (v:player{name:\"Tim Duncan\"})-[:serve]-&gt;(v2) \\\n        RETURN v2.team.name AS Team;\n+---------+\n| Team    |\n+---------+\n| \"Spurs\" |\n+---------+\n\nnebula&gt; RETURN \"Amber\" AS Name;\n+---------+\n| Name    |\n+---------+\n| \"Amber\" |\n+---------+\n</code></pre>"},{"location":"3.ngql-guide/8.clauses-and-options/return/#return_a_non-existing_property","title":"Return a non-existing property","text":"<p>If a property matched does not exist, <code>NULL</code> is returned.</p> <pre><code>nebula&gt; MATCH (v:player{name:\"Tim Duncan\"})-[e]-&gt;(v2) \\\n        RETURN v2.player.name, type(e), v2.player.age;\n+-----------------+----------+---------------+\n| v2.player.name  | type(e)  | v2.player.age |\n+-----------------+----------+---------------+\n| \"Manu Ginobili\" | \"follow\" | 41            |\n| __NULL__        | \"serve\"  | __NULL__      |\n| \"Tony Parker\"   | \"follow\" | 36            |\n+-----------------+----------+---------------+\n</code></pre>"},{"location":"3.ngql-guide/8.clauses-and-options/return/#return_expression_results","title":"Return expression results","text":"<p>To return the results of expressions such as literals, functions, or predicates, set them in a <code>RETURN</code> clause.</p> <pre><code>nebula&gt; MATCH (v:player{name:\"Tony Parker\"})--&gt;(v2:player) \\\n        RETURN DISTINCT v2.player.name, \"Hello\"+\" graphs!\", v2.player.age &gt; 35;\n+---------------------+----------------------+--------------------+\n| v2.player.name      | (\"Hello\"+\" graphs!\") | (v2.player.age&gt;35) |\n+---------------------+----------------------+--------------------+\n| \"LaMarcus Aldridge\" | \"Hello graphs!\"      | false              |\n| \"Tim Duncan\"        | \"Hello graphs!\"      | true               |\n| \"Manu Ginobili\"     | \"Hello graphs!\"      | true               |\n+---------------------+----------------------+--------------------+\n\nnebula&gt; RETURN 1+1;\n+-------+\n| (1+1) |\n+-------+\n| 2     |\n+-------+\n\nnebula&gt; RETURN 1- -1;\n+----------+\n| (1--(1)) |\n+----------+\n| 2        |\n+----------+\n\nnebula&gt; RETURN 3 &gt; 1;\n+-------+\n| (3&gt;1) |\n+-------+\n| true  |\n+-------+\n\nnebula&gt; RETURN 1+1, rand32(1, 5);\n+-------+-------------+\n| (1+1) | rand32(1,5) |\n+-------+-------------+\n| 2     | 1           |\n+-------+-------------+\n</code></pre>"},{"location":"3.ngql-guide/8.clauses-and-options/return/#return_unique_fields","title":"Return unique fields","text":"<p>Use <code>DISTINCT</code> to remove duplicate fields in the result set.</p> <pre><code># Before using DISTINCT.\nnebula&gt; MATCH (v:player{name:\"Tony Parker\"})--(v2:player) \\\n        RETURN v2.player.name, v2.player.age;\n+---------------------+---------------+\n| v2.player.name      | v2.player.age |\n+---------------------+---------------+\n| \"Manu Ginobili\"     | 41            |\n| \"Boris Diaw\"        | 36            |\n| \"Marco Belinelli\"   | 32            |\n| \"Dejounte Murray\"   | 29            |\n| \"Tim Duncan\"        | 42            |\n| \"Tim Duncan\"        | 42            |\n| \"LaMarcus Aldridge\" | 33            |\n| \"LaMarcus Aldridge\" | 33            |\n+---------------------+---------------+\n\n# After using DISTINCT.\nnebula&gt; MATCH (v:player{name:\"Tony Parker\"})--(v2:player) \\\n        RETURN DISTINCT v2.player.name, v2.player.age;\n+---------------------+---------------+\n| v2.player.name      | v2.player.age |\n+---------------------+---------------+\n| \"Manu Ginobili\"     | 41            |\n| \"Boris Diaw\"        | 36            |\n| \"Marco Belinelli\"   | 32            |\n| \"Dejounte Murray\"   | 29            |\n| \"Tim Duncan\"        | 42            |\n| \"LaMarcus Aldridge\" | 33            |\n+---------------------+---------------+\n</code></pre>"},{"location":"3.ngql-guide/8.clauses-and-options/sample/","title":"SAMPLE","text":"<p>The <code>SAMPLE</code> clause takes samples evenly in the result set and returns the specified amount of data.</p> <p><code>SAMPLE</code> can be used in <code>GO</code> statements only. The syntax is as follows:</p> <pre><code>&lt;go_statement&gt; SAMPLE &lt;sample_list&gt;;\n</code></pre> <p><code>sample_list</code> is a list. Elements in the list must be natural numbers, and the number of elements must be the same as the maximum number of <code>STEPS</code> in the <code>GO</code> statement. The following takes <code>GO 1 TO 3 STEPS FROM \"A\" OVER * SAMPLE &lt;sample_list&gt;</code> as an example to introduce this usage of <code>SAMPLE</code> in detail.</p> <ul> <li>The list <code>sample_list</code> must contain 3 natural numbers, such as <code>GO 1 TO 3 STEPS FROM \"A\" OVER * SAMPLE [1,2,4]</code>.</li> <li><code>1</code> in <code>SAMPLE [1,2,4]</code> means that the system automatically selects 1 edge to continue traversal in the first step. <code>2</code> means to select 2 edges to continue traversal in the second step. <code>4</code> indicates that 4 edges are selected to continue traversal in the third step. If there is no matched edge in a certain step or the number of matched edges is less than the specified number, the actual number will be returned.</li> <li>Because <code>GO 1 TO 3 STEPS</code> means to return all the traversal results from the first to third steps, all the red edges and their source and destination vertices in the figure below will be matched by this <code>GO</code> statement. And the yellow edges represent there is no path selected when the GO statement traverses. If it is not <code>GO 1 TO 3 STEPS</code> but <code>GO 3 STEPS</code>, it will only match the red edges of the third step and the vertices at both ends.</li> </ul> <p></p> <p>In the basketballplayer dataset, the example is as follows:</p> <pre><code>nebula&gt; GO 3 STEPS FROM \"player100\" \\\n        OVER * \\\n        YIELD properties($$).name AS NAME, properties($$).age AS Age \\\n        SAMPLE [1,2,3];\n+-----------------+----------+\n| NAME            | Age      |\n+-----------------+----------+\n| \"Tony Parker\"   | 36       |\n| \"Manu Ginobili\" | 41       |\n| \"Spurs\"         | __NULL__ |\n+-----------------+----------+\n\nnebula&gt; GO 1 TO 3 STEPS FROM \"player100\" \\\n        OVER * \\\n        YIELD properties($$).name AS NAME, properties($$).age AS Age \\\n        SAMPLE [2,2,2];\n+-----------------+----------+\n| NAME            | Age      |\n+-----------------+----------+\n| \"Manu Ginobili\" | 41       |\n| \"Spurs\"         | __NULL__ |\n| \"Tim Duncan\"    | 42       |\n| \"Spurs\"         | __NULL__ |\n| \"Manu Ginobili\" | 41       |\n| \"Spurs\"         | __NULL__ |\n+-----------------+----------+\n</code></pre>"},{"location":"3.ngql-guide/8.clauses-and-options/ttl-options/","title":"TTL","text":"<p>TTL (Time To Live) specifies a timeout for a property. Once timed out, the property expires.</p>"},{"location":"3.ngql-guide/8.clauses-and-options/ttl-options/#opencypher_compatibility","title":"OpenCypher Compatibility","text":"<p>This topic applies to native nGQL only.</p>"},{"location":"3.ngql-guide/8.clauses-and-options/ttl-options/#precautions","title":"Precautions","text":"<ul> <li>You CANNOT modify a property schema with TTL options on it.</li> </ul> <ul> <li> <p>TTL options and indexes have coexistence issues.</p> <ul> <li>TTL options and indexes CANNOT coexist on a tag or an edge type. If there is an index on a property, you cannot set TTL options on other properties.</li> </ul> <ul> <li>If there are TTL options on a tag, an edge type, or a property, you can still add an index on them.</li> </ul> </li> </ul>"},{"location":"3.ngql-guide/8.clauses-and-options/ttl-options/#ttl_options","title":"TTL options","text":"<p>The native nGQL TTL feature has the following options.</p> Option Description <code>ttl_col</code> Specifies the property to set a timeout on. The data type of the property must be <code>int</code> or <code>timestamp</code>. <code>ttl_duration</code> Specifies the timeout adds-on value in seconds. The value must be a non-negative int64 number. A property expires if the sum of its value and the <code>ttl_duration</code> value is smaller than the current timestamp. If the <code>ttl_duration</code> value is <code>0</code>, the property never expires.You can set <code>ttl_use_ms</code> to <code>true</code> in the configuration file <code>nebula-storaged.conf</code> (default path: <code>/usr/local/nightly/etc/</code>) to set the default unit to milliseconds. <p>Caution</p> <ul> <li>Before setting <code>ttl_use_ms</code> to <code>true</code>, make sure that no TTL has been set for any property, as shortening the expiration time may cause data to be erroneously deleted.</li> </ul> <ul> <li>After setting <code>ttl_use_ms</code> to <code>true</code>, which sets the default TTL unit to milliseconds, the data type of the property specified by <code>ttl_col</code> must be <code>int</code>, and the property value needs to be manually converted to milliseconds. For example, when setting <code>ttl_col</code> to <code>a</code>, you need to convert the value of <code>a</code> to milliseconds, such as when the value of <code>a</code> is <code>now()</code>, you need to set the value of <code>a</code> to <code>now() * 1000</code>.</li> </ul>"},{"location":"3.ngql-guide/8.clauses-and-options/ttl-options/#data_expiration_and_deletion","title":"Data expiration and deletion","text":"<p>Caution</p> <ul> <li>When the TTL options are set for a property of a tag or an edge type and the property's value is <code>NULL</code>, the property never expires. </li> <li>If a property with a default value of <code>now()</code> is added to a tag or an edge type and the TTL options are set for the property, the history data related to the tag or the edge type will never expire because the value of that property for the history data is the current timestamp.</li> </ul>"},{"location":"3.ngql-guide/8.clauses-and-options/ttl-options/#vertex_property_expiration","title":"Vertex property expiration","text":"<p>Vertex property expiration has the following impact.</p> <ul> <li>If a vertex has only one tag, once a property of the vertex expires, the vertex expires.</li> </ul> <ul> <li>If a vertex has multiple tags, once a property of the vertex expires, properties bound to the same tag with the expired property also expire, but the vertex does not expire and other properties of it remain untouched.</li> </ul>"},{"location":"3.ngql-guide/8.clauses-and-options/ttl-options/#edge_property_expiration","title":"Edge property expiration","text":"<p>Since an edge can have only one edge type, once an edge property expires, the edge expires.</p>"},{"location":"3.ngql-guide/8.clauses-and-options/ttl-options/#data_deletion","title":"Data deletion","text":"<p>The expired data are still stored on the disk, but queries will filter them out.</p> <p>NebulaGraph automatically deletes the expired data and reclaims the disk space during the next compaction.</p> <p>Note</p> <p>If TTL is disabled, the corresponding data deleted after the last compaction can be queried again.</p>"},{"location":"3.ngql-guide/8.clauses-and-options/ttl-options/#use_ttl_options","title":"Use TTL options","text":"<p>You must use the TTL options together to set a valid timeout on a property.</p>"},{"location":"3.ngql-guide/8.clauses-and-options/ttl-options/#set_a_timeout_if_a_tag_or_an_edge_type_exists","title":"Set a timeout if a tag or an edge type exists","text":"<p>If a tag or an edge type is already created, to set a timeout on a property bound to the tag or edge type, use <code>ALTER</code> to update the tag or edge type.</p> <pre><code># Create a tag.\nnebula&gt; CREATE TAG IF NOT EXISTS t1 (a timestamp);\n\n# Use ALTER to update the tag and set the TTL options.\nnebula&gt; ALTER TAG t1 TTL_COL = \"a\", TTL_DURATION = 5;\n\n# Insert a vertex with tag t1. The vertex expires 5 seconds after the insertion.\nnebula&gt; INSERT VERTEX t1(a) VALUES \"101\":(now());\n</code></pre>"},{"location":"3.ngql-guide/8.clauses-and-options/ttl-options/#set_a_timeout_when_creating_a_tag_or_an_edge_type","title":"Set a timeout when creating a tag or an edge type","text":"<p>Use TTL options in the <code>CREATE</code> statement to set a timeout when creating a tag or an edge type. For more information, see CREATE TAG and CREATE EDGE.</p> <pre><code># Create a tag and set the TTL options.\nnebula&gt; CREATE TAG IF NOT EXISTS t2(a int, b int, c string) TTL_DURATION= 100, TTL_COL = \"a\";\n\n# Insert a vertex with tag t2. The timeout timestamp is 1648197238 (1648197138 + 100).\nnebula&gt; INSERT VERTEX t2(a, b, c) VALUES \"102\":(1648197138, 30, \"Hello\");\n</code></pre>"},{"location":"3.ngql-guide/8.clauses-and-options/ttl-options/#remove_a_timeout","title":"Remove a timeout","text":"<p>To disable TTL and remove the timeout on a property, you can use the following approaches.</p> <ul> <li>Drop the property with the timeout.<pre><code>nebula&gt; ALTER TAG t1 DROP (a);\n</code></pre> </li> </ul> <ul> <li>Set <code>ttl_col</code> to an empty string.<pre><code>nebula&gt; ALTER TAG t1 TTL_COL = \"\";\n</code></pre> </li> </ul> <ul> <li>Set <code>ttl_duration</code> to <code>0</code>. This operation keeps the TTL options and prevents the property from expiring and the property schema from being modified.<pre><code>nebula&gt; ALTER TAG t1 TTL_DURATION = 0;\n</code></pre> </li> </ul>"},{"location":"3.ngql-guide/8.clauses-and-options/unwind/","title":"UNWIND","text":"<p><code>UNWIND</code> transform a list into a sequence of rows.</p> <p><code>UNWIND</code> can be used as an individual statement or as a clause within a statement.</p>"},{"location":"3.ngql-guide/8.clauses-and-options/unwind/#unwind_statement","title":"UNWIND statement","text":""},{"location":"3.ngql-guide/8.clauses-and-options/unwind/#syntax","title":"Syntax","text":"<pre><code>UNWIND &lt;list&gt; AS &lt;alias&gt; &lt;RETURN clause&gt;;\n</code></pre>"},{"location":"3.ngql-guide/8.clauses-and-options/unwind/#examples","title":"Examples","text":"<ul> <li> <p>To transform a list.</p> <pre><code>nebula&gt; UNWIND [1,2,3] AS n RETURN n;\n+---+\n| n |\n+---+\n| 1 |\n| 2 |\n| 3 |\n+---+\n</code></pre> </li> </ul>"},{"location":"3.ngql-guide/8.clauses-and-options/unwind/#unwind_clause","title":"UNWIND clause","text":""},{"location":"3.ngql-guide/8.clauses-and-options/unwind/#syntax_1","title":"Syntax","text":"<ul> <li> <p>The <code>UNWIND</code> clause in native nGQL statements.</p> <p>Note</p> <p>To use a <code>UNWIND</code> clause in a native nGQL statement, use it after the <code>|</code> operator and use the <code>$-</code> prefix for variables. If you use a statement or clause after the <code>UNWIND</code> clause, use the <code>|</code> operator and use the <code>$-</code> prefix for variables.</p> <pre><code>&lt;statement&gt; | UNWIND $-.&lt;var&gt; AS &lt;alias&gt; &lt;|&gt; &lt;clause&gt;;\n</code></pre> </li> </ul> <ul> <li> <p>The <code>UNWIND</code> clause in openCypher statements.</p> <pre><code>&lt;statement&gt; UNWIND &lt;list&gt; AS &lt;alias&gt; &lt;RETURN clause&gt;\uff1b\n</code></pre> </li> </ul>"},{"location":"3.ngql-guide/8.clauses-and-options/unwind/#examples_1","title":"Examples","text":"<ul> <li> <p>To transform a list of duplicates into a unique set of rows using <code>WITH DISTINCT</code> in a <code>UNWIND</code> clause.</p> <p>Note</p> <p><code>WITH DISTINCT</code> is not available in native nGQL statements.  </p> <pre><code>// Transform the list `[1,1,2,2,3,3]` into a unique set of rows, sort the rows, and then transform the rows into a list of unique values.\n\nnebula&gt; WITH [1,1,2,2,3,3] AS n \\\n        UNWIND n AS r \\\n        WITH DISTINCT r AS r \\\n        ORDER BY r \\\n        RETURN collect(r);\n+------------+\n| collect(r) |\n+------------+\n| [1, 2, 3]  |\n+------------+\n</code></pre> </li> </ul> <ul> <li> <p>To use an <code>UNWIND</code> clause in a <code>MATCH</code> statement.</p> <pre><code>// Get a list of the vertices in the matched path, transform the list into a unique set of rows, and then transform the rows into a list. \n\nnebula&gt; MATCH p=(v:player{name:\"Tim Duncan\"})--(v2) \\\n        WITH nodes(p) AS n \\\n        UNWIND n AS r \\\n        WITH DISTINCT r AS r \\\n        RETURN collect(r);\n+----------------------------------------------------------------------------------------------------------------------+\n| collect(r)                                                                                                           |\n+----------------------------------------------------------------------------------------------------------------------+\n| [(\"player100\" :player{age: 42, name: \"Tim Duncan\"}), (\"player101\" :player{age: 36, name: \"Tony Parker\"}),            |\n|(\"team204\" :team{name: \"Spurs\"}), (\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"}),                          |\n|(\"player125\" :player{age: 41, name: \"Manu Ginobili\"}), (\"player104\" :player{age: 32, name: \"Marco Belinelli\"}),       |\n|(\"player144\" :player{age: 47, name: \"Shaquile O'Neal\"}), (\"player105\" :player{age: 31, name: \"Danny Green\"}),         |\n|(\"player113\" :player{age: 29, name: \"Dejounte Murray\"}), (\"player107\" :player{age: 32, name: \"Aron Baynes\"}),         |\n|(\"player109\" :player{age: 34, name: \"Tiago Splitter\"}), (\"player108\" :player{age: 36, name: \"Boris Diaw\"})]           |  \n+----------------------------------------------------------------------------------------------------------------------+\n</code></pre> </li> </ul> <ul> <li> <p>To use an <code>UNWIND</code> clause in a <code>GO</code> statement.</p> <pre><code>// Query the vertices in a list for the corresponding edges with a specified statement.\n\nnebula&gt; YIELD ['player101', 'player100'] AS a | UNWIND $-.a AS  b | GO FROM $-.b OVER follow YIELD edge AS e;\n+----------------------------------------------------+\n| e                                                  |\n+----------------------------------------------------+\n| [:follow \"player101\"-&gt;\"player100\" @0 {degree: 95}] |\n| [:follow \"player101\"-&gt;\"player102\" @0 {degree: 90}] |\n| [:follow \"player101\"-&gt;\"player125\" @0 {degree: 95}] |\n| [:follow \"player100\"-&gt;\"player101\" @0 {degree: 95}] |\n| [:follow \"player100\"-&gt;\"player125\" @0 {degree: 95}] |\n+----------------------------------------------------+\n</code></pre> </li> </ul> <ul> <li> <p>To use an <code>UNWIND</code> clause in a <code>LOOKUP</code> statement.</p> <pre><code>// Find all the properties of players whose age is greater than 46, get a list of unique properties, and then transform the list into rows. \n\nnebula&gt; LOOKUP ON player \\\n        WHERE player.age &gt; 46 \\\n        YIELD DISTINCT keys(vertex) as p | UNWIND $-.p as a | YIELD $-.a AS a;\n+--------+\n| a      |\n+--------+\n| \"age\"  |\n| \"name\" |\n+--------+\n</code></pre> </li> </ul> <ul> <li> <p>To use an <code>UNWIND</code> clause in a <code>FETCH</code> statement. </p> <pre><code>// Query player101 for all tags related to player101, get a list of the tags and then transform the list into rows.\n\nnebula&gt; CREATE TAG hero(like string, height int);\n        INSERT VERTEX hero(like, height) VALUES \"player101\":(\"deep\", 182);\n        FETCH PROP ON * \"player101\" \\\n        YIELD tags(vertex) as t | UNWIND $-.t as a | YIELD $-.a AS a;\n+----------+\n| a        |\n+----------+\n| \"hero\"   |\n| \"player\" |\n+----------+\n</code></pre> </li> </ul> <ul> <li> <p>To use an <code>UNWIND</code> clause in a <code>GET SUBGRAPH</code> statement. </p> <pre><code>// Get the subgraph including outgoing and incoming serve edges within 0~2 hops from/to player100, and transform the result into rows.\n\nnebula&gt; GET SUBGRAPH 2 STEPS FROM \"player100\" BOTH serve \\\n        YIELD edges as e | UNWIND $-.e as a | YIELD $-.a AS a;\n+----------------------------------------------+\n| a                                            |\n+----------------------------------------------+\n| [:serve \"player100\"-&gt;\"team204\" @0 {}]        |\n| [:serve \"player101\"-&gt;\"team204\" @0 {}]        |\n| [:serve \"player102\"-&gt;\"team204\" @0 {}]        |\n| [:serve \"player103\"-&gt;\"team204\" @0 {}]        |\n| [:serve \"player105\"-&gt;\"team204\" @0 {}]        |\n| [:serve \"player106\"-&gt;\"team204\" @0 {}]        |\n| [:serve \"player107\"-&gt;\"team204\" @0 {}]        |\n| [:serve \"player108\"-&gt;\"team204\" @0 {}]        |\n| [:serve \"player109\"-&gt;\"team204\" @0 {}]        |\n| [:serve \"player110\"-&gt;\"team204\" @0 {}]        |\n| [:serve \"player111\"-&gt;\"team204\" @0 {}]        |\n| [:serve \"player112\"-&gt;\"team204\" @0 {}]        |\n| [:serve \"player113\"-&gt;\"team204\" @0 {}]        |\n| [:serve \"player114\"-&gt;\"team204\" @0 {}]        |\n| [:serve \"player125\"-&gt;\"team204\" @0 {}]        |\n| [:serve \"player138\"-&gt;\"team204\" @0 {}]        |\n| [:serve \"player104\"-&gt;\"team204\" @20132015 {}] |\n| [:serve \"player104\"-&gt;\"team204\" @20182019 {}] |\n+----------------------------------------------+\n</code></pre> </li> </ul> <ul> <li> <p>To use an <code>UNWIND</code> clause in a <code>FIND PATH</code> statement.</p> <pre><code>// Find all the vertices in the shortest path from player101 to team204 along the serve edge, and transform the result into rows. \n\nnebula&gt; FIND SHORTEST PATH FROM \"player101\" TO \"team204\" OVER serve \\\n        YIELD path as p | YIELD nodes($-.p) AS nodes | UNWIND $-.nodes AS a | YIELD $-.a AS a;\n+---------------+\n| a             |\n+---------------+\n| (\"player101\") |\n| (\"team204\")   |\n+---------------+\n</code></pre> </li> </ul>"},{"location":"3.ngql-guide/8.clauses-and-options/where/","title":"WHERE","text":"<p>The <code>WHERE</code> clause filters the output by conditions.</p> <p>The <code>WHERE</code> clause usually works in the following queries:</p> <ul> <li>Native nGQL: such as <code>GO</code> and <code>LOOKUP</code>.</li> </ul> <ul> <li>OpenCypher syntax: such as <code>MATCH</code> and <code>WITH</code>.</li> </ul>"},{"location":"3.ngql-guide/8.clauses-and-options/where/#opencypher_compatibility","title":"OpenCypher compatibility","text":"<p>Filtering on edge rank is a native nGQL feature. To retrieve the rank value in openCypher statements, use the rank() function, such as <code>MATCH (:player)-[e:follow]-&gt;() RETURN rank(e);</code>.</p>"},{"location":"3.ngql-guide/8.clauses-and-options/where/#basic_usage","title":"Basic usage","text":"<p>Note</p> <p>In the following examples, <code>$$</code> and <code>$^</code> are reference operators. For more information, see Operators.</p>"},{"location":"3.ngql-guide/8.clauses-and-options/where/#define_conditions_with_boolean_operators","title":"Define conditions with boolean operators","text":"<p>Use the boolean operators <code>NOT</code>, <code>AND</code>, <code>OR</code>, and <code>XOR</code> to define conditions in <code>WHERE</code> clauses. For the precedence of the operators, see Precedence.</p> <pre><code>nebula&gt; MATCH (v:player) \\\n        WHERE v.player.name == \"Tim Duncan\" \\\n        XOR (v.player.age &lt; 30 AND v.player.name == \"Yao Ming\") \\\n        OR NOT (v.player.name == \"Yao Ming\" OR v.player.name == \"Tim Duncan\") \\\n        RETURN v.player.name, v.player.age;\n+-------------------------+--------------+\n| v.player.name           | v.player.age |\n+-------------------------+--------------+\n| \"Danny Green\"           | 31           |\n| \"Tiago Splitter\"        | 34           |\n| \"David West\"            | 38           |\n...\n</code></pre> <pre><code>nebula&gt; GO FROM \"player100\" \\\n        OVER follow \\\n        WHERE properties(edge).degree &gt; 90 \\\n        OR properties($$).age != 33 \\\n        AND properties($$).name != \"Tony Parker\" \\\n        YIELD properties($$);\n+----------------------------------+\n| properties($$)                   |\n+----------------------------------+\n| {age: 41, name: \"Manu Ginobili\"} |\n+----------------------------------+\n</code></pre>"},{"location":"3.ngql-guide/8.clauses-and-options/where/#filter_on_properties","title":"Filter on properties","text":"<p>Use vertex or edge properties to define conditions in <code>WHERE</code> clauses.</p> <ul> <li>Filter on a vertex property:<pre><code>nebula&gt; MATCH (v:player)-[e]-&gt;(v2) \\\n        WHERE v2.player.age &lt; 25 \\\n        RETURN v2.player.name, v2.player.age;\n+----------------------+---------------+\n| v2.player.name       | v2.player.age |\n+----------------------+---------------+\n| \"Ben Simmons\"        | 22            |\n| \"Luka Doncic\"        | 20            |\n| \"Kristaps Porzingis\" | 23            |\n+----------------------+---------------+\n</code></pre> <pre><code>nebula&gt; GO FROM \"player100\" OVER follow \\\n        WHERE $^.player.age &gt;= 42 \\\n        YIELD dst(edge);\n+-------------+\n| dst(EDGE)   |\n+-------------+\n+-------------+\n| \"player101\" |\n| \"player125\" |\n+-------------+\n</code></pre> </li> </ul> <ul> <li>Filter on an edge property:<pre><code>nebula&gt; MATCH (v:player)-[e]-&gt;() \\\n        WHERE e.start_year &lt; 2000 \\\n        RETURN DISTINCT v.player.name, v.player.age;\n+--------------------+--------------+\n| v.player.name      | v.player.age |\n+--------------------+--------------+\n| \"Tony Parker\"      | 36           |\n| \"Tim Duncan\"       | 42           |\n| \"Grant Hill\"       | 46           |\n...\n</code></pre> <pre><code>nebula&gt; GO FROM \"player100\" OVER follow \\\n        WHERE follow.degree &gt; 90 \\\n        YIELD dst(edge);\n+-------------+\n| dst(EDGE)   |\n+-------------+\n| \"player101\" |\n| \"player125\" |\n+-------------+\n</code></pre> </li> </ul>"},{"location":"3.ngql-guide/8.clauses-and-options/where/#filter_on_dynamically-calculated_properties","title":"Filter on dynamically-calculated properties","text":"<pre><code>nebula&gt; MATCH (v:player) \\\n        WHERE v[toLower(\"AGE\")] &lt; 21 \\\n        RETURN v.player.name, v.player.age;\n+---------------+-------+\n| v.name        | v.age |\n+---------------+-------+\n| \"Luka Doncic\" | 20    |\n+---------------+-------+\n</code></pre>"},{"location":"3.ngql-guide/8.clauses-and-options/where/#filter_on_existing_properties","title":"Filter on existing properties","text":"<pre><code>nebula&gt; MATCH (v:player) \\\n        WHERE exists(v.player.age) \\\n        RETURN v.player.name, v.player.age;\n+-------------------------+--------------+\n| v.player.name           | v.player.age |\n+-------------------------+--------------+\n| \"Danny Green\"           | 31           |\n| \"Tiago Splitter\"        | 34           |\n| \"David West\"            | 38           |\n...\n</code></pre>"},{"location":"3.ngql-guide/8.clauses-and-options/where/#filter_on_edge_rank","title":"Filter on edge rank","text":"<p>In nGQL, if a group of edges has the same source vertex, destination vertex, and properties, the only thing that distinguishes them is the rank. Use rank conditions in <code>WHERE</code> clauses to filter such edges.</p> <pre><code># The following example creates test data.\nnebula&gt; CREATE SPACE IF NOT EXISTS test (vid_type=FIXED_STRING(30));\nnebula&gt; USE test;\nnebula&gt; CREATE EDGE IF NOT EXISTS e1(p1 int);\nnebula&gt; CREATE TAG IF NOT EXISTS person(p1 int);\nnebula&gt; INSERT VERTEX person(p1) VALUES \"1\":(1);\nnebula&gt; INSERT VERTEX person(p1) VALUES \"2\":(2);\nnebula&gt; INSERT EDGE e1(p1) VALUES \"1\"-&gt;\"2\"@0:(10);\nnebula&gt; INSERT EDGE e1(p1) VALUES \"1\"-&gt;\"2\"@1:(11);\nnebula&gt; INSERT EDGE e1(p1) VALUES \"1\"-&gt;\"2\"@2:(12);\nnebula&gt; INSERT EDGE e1(p1) VALUES \"1\"-&gt;\"2\"@3:(13);\nnebula&gt; INSERT EDGE e1(p1) VALUES \"1\"-&gt;\"2\"@4:(14);\nnebula&gt; INSERT EDGE e1(p1) VALUES \"1\"-&gt;\"2\"@5:(15);\nnebula&gt; INSERT EDGE e1(p1) VALUES \"1\"-&gt;\"2\"@6:(16);\n\n# The following example use rank to filter edges and retrieves edges with a rank greater than 2.\nnebula&gt; GO FROM \"1\" \\\n        OVER e1 \\\n        WHERE rank(edge) &gt; 2 \\\n        YIELD src(edge), dst(edge), rank(edge) AS Rank, properties(edge).p1 | \\\n        ORDER BY $-.Rank DESC;\n+-----------+-----------+------+---------------------+\n| src(EDGE) | dst(EDGE) | Rank | properties(EDGE).p1 |\n+-----------+-----------+------+---------------------+\n| \"1\"       | \"2\"       | 6    | 16                  |\n| \"1\"       | \"2\"       | 5    | 15                  |\n| \"1\"       | \"2\"       | 4    | 14                  |\n| \"1\"       | \"2\"       | 3    | 13                  |\n+-----------+-----------+------+---------------------+\n\n# Filter edges by rank. Find follow edges with rank equal to 0.\nnebula&gt; MATCH (v)-[e:follow]-&gt;() \\\n         WHERE rank(e)==0 \\\n         RETURN *;\n+------------------------------------------------------------+-----------------------------------------------------+\n| v                                                          | e                                                   |\n+------------------------------------------------------------+-----------------------------------------------------+\n| (\"player142\" :player{age: 29, name: \"Klay Thompson\"})      | [:follow \"player142\"-&gt;\"player117\" @0 {degree: 90}]  |\n| (\"player139\" :player{age: 34, name: \"Marc Gasol\"})         | [:follow \"player139\"-&gt;\"player138\" @0 {degree: 99}]  |\n| (\"player108\" :player{age: 36, name: \"Boris Diaw\"})         | [:follow \"player108\"-&gt;\"player100\" @0 {degree: 80}]  |\n| (\"player108\" :player{age: 36, name: \"Boris Diaw\"})         | [:follow \"player108\"-&gt;\"player101\" @0 {degree: 80}]  |\n...\n</code></pre>"},{"location":"3.ngql-guide/8.clauses-and-options/where/#filter_on_pattern","title":"Filter on pattern","text":"<pre><code>nebula&gt; MATCH (v:player{name:\"Tim Duncan\"})-[e]-&gt;(t) \\\n        WHERE (v)-[e]-&gt;(t:team) \\\n        RETURN (v)--&gt;();\n+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| (v)--&gt;() = (v)--&gt;()                                                                                                                                                                                                                                                                                                                                                                                              |\n+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| [&lt;(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:serve@0 {end_year: 2016, start_year: 1997}]-&gt;(\"team204\" :team{name: \"Spurs\"})&gt;, &lt;(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]-&gt;(\"player101\" :player{age: 36, name: \"Tony Parker\"})&gt;, &lt;(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]-&gt;(\"player125\" :player{age: 41, name: \"Manu Ginobili\"})&gt;] |\n+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n\nnebula&gt; MATCH (v:player{name:\"Tim Duncan\"})-[e]-&gt;(t) \\\n        WHERE NOT (v)-[e]-&gt;(t:team) \\\n        RETURN (v)--&gt;();\n+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| (v)--&gt;() = (v)--&gt;()                                                                                                                                                                                                                                                                                                                                                                                              |\n+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| [&lt;(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:serve@0 {end_year: 2016, start_year: 1997}]-&gt;(\"team204\" :team{name: \"Spurs\"})&gt;, &lt;(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]-&gt;(\"player101\" :player{age: 36, name: \"Tony Parker\"})&gt;, &lt;(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]-&gt;(\"player125\" :player{age: 41, name: \"Manu Ginobili\"})&gt;] |\n| [&lt;(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:serve@0 {end_year: 2016, start_year: 1997}]-&gt;(\"team204\" :team{name: \"Spurs\"})&gt;, &lt;(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]-&gt;(\"player101\" :player{age: 36, name: \"Tony Parker\"})&gt;, &lt;(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]-&gt;(\"player125\" :player{age: 41, name: \"Manu Ginobili\"})&gt;] |\n+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n</code></pre>"},{"location":"3.ngql-guide/8.clauses-and-options/where/#filter_on_strings","title":"Filter on strings","text":"<p>Use <code>STARTS WITH</code>, <code>ENDS WITH</code>, or <code>CONTAINS</code> in <code>WHERE</code> clauses to match a specific part of a string. String matching is case-sensitive.</p>"},{"location":"3.ngql-guide/8.clauses-and-options/where/#starts_with","title":"<code>STARTS WITH</code>","text":"<p><code>STARTS WITH</code> will match the beginning of a string.</p> <p>The following example uses <code>STARTS WITH \"T\"</code> to retrieve the information of players whose name starts with <code>T</code>.</p> <pre><code>nebula&gt; MATCH (v:player) \\\n        WHERE v.player.name STARTS WITH \"T\" \\\n        RETURN v.player.name, v.player.age;\n+------------------+--------------+\n| v.player.name    | v.player.age |\n+------------------+--------------+\n| \"Tony Parker\"    | 36           |\n| \"Tiago Splitter\" | 34           |\n| \"Tim Duncan\"     | 42           |\n| \"Tracy McGrady\"  | 39           |\n+------------------+--------------+\n</code></pre> <p>If you use <code>STARTS WITH \"t\"</code> in the preceding statement, an empty set is returned because no name in the dataset starts with the lowercase <code>t</code>.</p> <pre><code>nebula&gt; MATCH (v:player) \\\n        WHERE v.player.name STARTS WITH \"t\" \\\n        RETURN v.player.name, v.player.age;\n+---------------+--------------+\n| v.player.name | v.player.age |\n+---------------+--------------+\n+---------------+--------------+\nEmpty set (time spent 5080/6474 us)\n</code></pre>"},{"location":"3.ngql-guide/8.clauses-and-options/where/#ends_with","title":"<code>ENDS WITH</code>","text":"<p><code>ENDS WITH</code> will match the ending of a string.</p> <p>The following example uses <code>ENDS WITH \"r\"</code> to retrieve the information of players whose name ends with <code>r</code>.</p> <pre><code>nebula&gt; MATCH (v:player) \\\n        WHERE v.player.name ENDS WITH \"r\" \\\n        RETURN v.player.name, v.player.age;\n+------------------+--------------+\n| v.player.name    | v.player.age |\n+------------------+--------------+\n| \"Tony Parker\"    | 36           |\n| \"Tiago Splitter\" | 34           |\n| \"Vince Carter\"   | 42           |\n+------------------+--------------+\n</code></pre>"},{"location":"3.ngql-guide/8.clauses-and-options/where/#contains","title":"<code>CONTAINS</code>","text":"<p><code>CONTAINS</code> will match a certain part of a string.</p> <p>The following example uses <code>CONTAINS \"Pa\"</code> to match the information of players whose name contains <code>Pa</code>.</p> <pre><code>nebula&gt; MATCH (v:player) \\\n        WHERE v.player.name CONTAINS \"Pa\" \\\n        RETURN v.player.name, v.player.age;\n+---------------+--------------+\n| v.player.name | v.player.age |\n+---------------+--------------+\n| \"Paul George\" | 28           |\n| \"Tony Parker\" | 36           |\n| \"Paul Gasol\"  | 38           |\n| \"Chris Paul\"  | 33           |\n+---------------+--------------+\n</code></pre>"},{"location":"3.ngql-guide/8.clauses-and-options/where/#negative_string_matching","title":"Negative string matching","text":"<p>You can use the boolean operator <code>NOT</code> to negate a string matching condition.</p> <pre><code>nebula&gt; MATCH (v:player) \\\n        WHERE NOT v.player.name ENDS WITH \"R\" \\\n        RETURN v.player.name, v.player.age;\n+-------------------------+--------------+\n| v.player.name           | v.player.age |\n+-------------------------+--------------+\n| \"Danny Green\"           | 31           |\n| \"Tiago Splitter\"        | 34           |\n| \"David West\"            | 38           |\n| \"Russell Westbrook\"     | 30           |\n...\n</code></pre>"},{"location":"3.ngql-guide/8.clauses-and-options/where/#filter_on_lists","title":"Filter on lists","text":""},{"location":"3.ngql-guide/8.clauses-and-options/where/#match_values_in_a_list","title":"Match values in a list","text":"<p>Use the <code>IN</code> operator to check if a value is in a specific list.</p> <pre><code>nebula&gt; MATCH (v:player) \\\n        WHERE v.player.age IN range(20,25) \\\n        RETURN v.player.name, v.player.age;\n+-------------------------+--------------+\n| v.player.name           | v.player.age |\n+-------------------------+--------------+\n| \"Ben Simmons\"           | 22           |\n| \"Giannis Antetokounmpo\" | 24           |\n| \"Kyle Anderson\"         | 25           |\n| \"Joel Embiid\"           | 25           |\n| \"Kristaps Porzingis\"    | 23           |\n| \"Luka Doncic\"           | 20           |\n+-------------------------+--------------+\n\nnebula&gt; LOOKUP ON player \\\n        WHERE player.age IN [25,28]  \\\n        YIELD properties(vertex).name, properties(vertex).age;\n+-------------------------+------------------------+\n| properties(VERTEX).name | properties(VERTEX).age |\n+-------------------------+------------------------+\n| \"Kyle Anderson\"         | 25                     |\n| \"Damian Lillard\"        | 28                     |\n| \"Joel Embiid\"           | 25                     |\n| \"Paul George\"           | 28                     |\n| \"Ricky Rubio\"           | 28                     |\n+-------------------------+------------------------+\n</code></pre>"},{"location":"3.ngql-guide/8.clauses-and-options/where/#match_values_not_in_a_list","title":"Match values not in a list","text":"<p>Use <code>NOT</code> before <code>IN</code> to rule out the values in a list.</p> <pre><code>nebula&gt; MATCH (v:player) \\\n        WHERE v.player.age NOT IN range(20,25) \\\n        RETURN v.player.name AS Name, v.player.age AS Age \\\n        ORDER BY Age;\n+---------------------+-----+\n| Name                | Age |\n+---------------------+-----+\n| \"Kyrie Irving\"      | 26  |\n| \"Cory Joseph\"       | 27  |\n| \"Damian Lillard\"    | 28  |\n| \"Paul George\"       | 28  |\n| \"Ricky Rubio\"       | 28  |\n+---------------------+-----+\n...\n</code></pre>"},{"location":"3.ngql-guide/8.clauses-and-options/with/","title":"WITH","text":"<p>The <code>WITH</code> clause can retrieve the output from a query part, process it, and pass it to the next query part as the input.</p>"},{"location":"3.ngql-guide/8.clauses-and-options/with/#opencypher_compatibility","title":"OpenCypher compatibility","text":"<p>This topic applies to openCypher syntax only.</p> <p>Note</p> <p><code>WITH</code> has a similar function with the Pipe symbol in native nGQL, but they work in different ways. DO NOT use pipe symbols in the openCypher syntax or use <code>WITH</code> in native nGQL statements.</p>"},{"location":"3.ngql-guide/8.clauses-and-options/with/#combine_statements_and_form_a_composite_query","title":"Combine statements and form a composite query","text":"<p>Use a <code>WITH</code> clause to combine statements and transfer the output of a statement as the input of another statement.</p>"},{"location":"3.ngql-guide/8.clauses-and-options/with/#example_1","title":"Example 1","text":"<p>The following statement:</p> <ol> <li>Matches a path.</li> <li>Outputs all the vertices on the path to a list with the <code>nodes()</code> function.</li> <li>Unwinds the list into rows.</li> <li>Removes duplicated vertices and returns a set of distinct vertices.</li> </ol> <pre><code>nebula&gt; MATCH p=(v:player{name:\"Tim Duncan\"})--() \\\n        WITH nodes(p) AS n \\\n        UNWIND n AS n1 \\\n        RETURN DISTINCT n1;\n+-----------------------------------------------------------+\n| n1                                                        |\n+-----------------------------------------------------------+\n| (\"player100\" :player{age: 42, name: \"Tim Duncan\"})        |\n| (\"player101\" :player{age: 36, name: \"Tony Parker\"})       |\n| (\"team204\" :team{name: \"Spurs\"})                          |\n| (\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"}) |\n| (\"player125\" :player{age: 41, name: \"Manu Ginobili\"})     |\n| (\"player104\" :player{age: 32, name: \"Marco Belinelli\"})   |\n| (\"player144\" :player{age: 47, name: \"Shaquille O'Neal\"})  |\n| (\"player105\" :player{age: 31, name: \"Danny Green\"})       |\n| (\"player113\" :player{age: 29, name: \"Dejounte Murray\"})   |\n| (\"player107\" :player{age: 32, name: \"Aron Baynes\"})       |\n| (\"player109\" :player{age: 34, name: \"Tiago Splitter\"})    |\n| (\"player108\" :player{age: 36, name: \"Boris Diaw\"})        |\n+-----------------------------------------------------------+\n</code></pre>"},{"location":"3.ngql-guide/8.clauses-and-options/with/#example_2","title":"Example 2","text":"<p>The following statement:</p> <ol> <li>Matches the vertex with the VID <code>player100</code>.</li> <li>Outputs all the tags of the vertex into a list with the <code>labels()</code> function.</li> <li>Unwinds the list into rows.</li> <li>Returns the output.</li> </ol> <pre><code>nebula&gt; MATCH (v) \\\n        WHERE id(v)==\"player100\" \\\n        WITH labels(v) AS tags_unf \\\n        UNWIND tags_unf AS tags_f \\\n        RETURN tags_f;\n+----------+\n| tags_f   |\n+----------+\n| \"player\" |\n+----------+\n</code></pre>"},{"location":"3.ngql-guide/8.clauses-and-options/with/#filter_composite_queries","title":"Filter composite queries","text":"<p><code>WITH</code> can work as a filter in the middle of a composite query.</p> <pre><code>nebula&gt; MATCH (v:player)--&gt;(v2:player) \\\n        WITH DISTINCT v2 AS v2, v2.player.age AS Age \\\n        ORDER BY Age \\\n        WHERE Age&lt;25 \\\n        RETURN v2.player.name AS Name, Age;\n+----------------------+-----+\n| Name                 | Age |\n+----------------------+-----+\n| \"Luka Doncic\"        | 20  |\n| \"Ben Simmons\"        | 22  |\n| \"Kristaps Porzingis\" | 23  |\n+----------------------+-----+\n</code></pre>"},{"location":"3.ngql-guide/8.clauses-and-options/with/#process_the_output_before_using_collect","title":"Process the output before using collect()","text":"<p>Use a <code>WITH</code> clause to sort and limit the output before using <code>collect()</code> to transform the output into a list.</p> <pre><code>nebula&gt; MATCH (v:player) \\\n        WITH v.player.name AS Name \\\n        ORDER BY Name DESC \\\n        LIMIT 3 \\\n        RETURN collect(Name);\n+-----------------------------------------------+\n| collect(Name)                                 |\n+-----------------------------------------------+\n| [\"Yao Ming\", \"Vince Carter\", \"Tracy McGrady\"] |\n+-----------------------------------------------+\n</code></pre>"},{"location":"3.ngql-guide/8.clauses-and-options/with/#use_with_return","title":"Use with RETURN","text":"<p>Set an alias using a <code>WITH</code> clause, and then output the result through a <code>RETURN</code> clause.</p> <pre><code>nebula&gt; WITH [1, 2, 3] AS `list` RETURN 3 IN `list` AS r;\n+------+\n| r    |\n+------+\n| true |\n+------+\n\nnebula&gt; WITH 4 AS one, 3 AS two RETURN one &gt; two AS result;\n+--------+\n| result |\n+--------+\n| true   |\n+--------+\n</code></pre>"},{"location":"3.ngql-guide/8.clauses-and-options/yield/","title":"YIELD","text":"<p><code>YIELD</code> defines the output of an nGQL query.</p> <p><code>YIELD</code> can lead a clause or a statement:</p> <ul> <li>A <code>YIELD</code> clause works in nGQL statements such as <code>GO</code>, <code>FETCH</code>, or <code>LOOKUP</code> and must be defined to return the result.</li> </ul> <ul> <li>A <code>YIELD</code> statement works in a composite query or independently.</li> </ul>"},{"location":"3.ngql-guide/8.clauses-and-options/yield/#opencypher_compatibility","title":"OpenCypher compatibility","text":"<p>This topic applies to native nGQL only. For the openCypher syntax, use <code>RETURN</code>.</p> <p><code>YIELD</code> has different functions in openCypher and nGQL.</p> <ul> <li> <p>In openCypher, <code>YIELD</code> is used in the <code>CALL[\u2026YIELD]</code> clause to specify the output of the procedure call.</p> <p>Note</p> <p>NGQL does not support <code>CALL[\u2026YIELD]</code> yet.</p> </li> </ul> <ul> <li>In nGQL, <code>YIELD</code> works like <code>RETURN</code> in openCypher.</li> </ul> <p>Note</p> <p>In the following examples, <code>$$</code> and <code>$-</code> are reference operators. For more information, see Operators.</p>"},{"location":"3.ngql-guide/8.clauses-and-options/yield/#yield_clauses","title":"YIELD clauses","text":""},{"location":"3.ngql-guide/8.clauses-and-options/yield/#syntax","title":"Syntax","text":"<pre><code>YIELD [DISTINCT] &lt;col&gt; [AS &lt;alias&gt;] [, &lt;col&gt; [AS &lt;alias&gt;] ...];\n</code></pre> Parameter Description <code>DISTINCT</code> Aggregates the output and makes the statement return a distinct result set. <code>col</code> A field to be returned. If no alias is set, <code>col</code> will be a column name in the output. <code>alias</code> An alias for <code>col</code>. It is set after the keyword <code>AS</code> and will be a column name in the output."},{"location":"3.ngql-guide/8.clauses-and-options/yield/#use_a_yield_clause_in_a_statement","title":"Use a YIELD clause in a statement","text":"<ul> <li>Use <code>YIELD</code> with <code>GO</code>:<pre><code>nebula&gt; GO FROM \"player100\" OVER follow \\\n        YIELD properties($$).name AS Friend, properties($$).age AS Age;\n+-----------------+-----+\n| Friend          | Age |\n+-----------------+-----+\n| \"Tony Parker\"   | 36  |\n| \"Manu Ginobili\" | 41  |\n+-----------------+-----+\n</code></pre> </li> </ul> <ul> <li>Use <code>YIELD</code> with <code>FETCH</code>:<pre><code>nebula&gt; FETCH PROP ON player \"player100\" \\\n        YIELD properties(vertex).name;\n+-------------------------+\n| properties(VERTEX).name |\n+-------------------------+\n| \"Tim Duncan\"            |\n+-------------------------+\n</code></pre> </li> </ul> <ul> <li>Use <code>YIELD</code> with <code>LOOKUP</code>:<pre><code>nebula&gt; LOOKUP ON player WHERE player.name == \"Tony Parker\" \\\n        YIELD properties(vertex).name, properties(vertex).age;\n+-------------------------+------------------------+\n| properties(VERTEX).name | properties(VERTEX).age |\n+-------------------------+------------------------+\n| \"Tony Parker\"           | 36                     |\n+-------------------------+------------------------+\n</code></pre> </li> </ul>"},{"location":"3.ngql-guide/8.clauses-and-options/yield/#yield_statements","title":"YIELD statements","text":""},{"location":"3.ngql-guide/8.clauses-and-options/yield/#syntax_1","title":"Syntax","text":"<pre><code>YIELD [DISTINCT] &lt;col&gt; [AS &lt;alias&gt;] [, &lt;col&gt; [AS &lt;alias&gt;] ...]\n[WHERE &lt;conditions&gt;];\n</code></pre> Parameter Description <code>DISTINCT</code> Aggregates the output and makes the statement return a distinct result set. <code>col</code> A field to be returned. If no alias is set, <code>col</code> will be a column name in the output. <code>alias</code> An alias for <code>col</code>. It is set after the keyword <code>AS</code> and will be a column name in the output. <code>conditions</code> Conditions set in a <code>WHERE</code> clause to filter the output. For more information, see <code>WHERE</code>."},{"location":"3.ngql-guide/8.clauses-and-options/yield/#use_a_yield_statement_in_a_composite_query","title":"Use a YIELD statement in a composite query","text":"<p>In a composite query, a <code>YIELD</code> statement accepts, filters, and modifies the result set of the preceding statement, and then outputs it.</p> <p>The following query finds the players that \"player100\" follows and calculates their average age.</p> <pre><code>nebula&gt; GO FROM \"player100\" OVER follow \\\n        YIELD dst(edge) AS ID \\\n        | FETCH PROP ON player $-.ID \\\n        YIELD properties(vertex).age AS Age \\\n        | YIELD AVG($-.Age) as Avg_age, count(*)as Num_friends;\n+---------+-------------+\n| Avg_age | Num_friends |\n+---------+-------------+\n| 38.5    | 2           |\n+---------+-------------+\n</code></pre> <p>The following query finds the players that \"player101\" follows with the follow degrees greater than 90.</p> <pre><code>nebula&gt; $var1 = GO FROM \"player101\" OVER follow \\\n        YIELD properties(edge).degree AS Degree, dst(edge) as ID; \\\n        YIELD $var1.ID AS ID WHERE $var1.Degree &gt; 90;\n+-------------+\n| ID          |\n+-------------+\n| \"player100\" |\n| \"player125\" |\n+-------------+\n</code></pre> <p>The following query finds the vertices in the player that are older than 30 and younger than 32, and returns the de-duplicate results.</p> <pre><code>nebula&gt; LOOKUP ON player  \\\n        WHERE player.age &lt; 32 and player.age &gt;30  \\\n        YIELD DISTINCT properties(vertex).age as v;\n+--------+\n| v      |\n+--------+\n| 31     |\n+--------+\n</code></pre>"},{"location":"3.ngql-guide/8.clauses-and-options/yield/#use_a_standalone_yield_statement","title":"Use a standalone YIELD statement","text":"<p>A <code>YIELD</code> statement can calculate a valid expression and output the result.</p> <pre><code>nebula&gt; YIELD rand32(1, 6);\n+-------------+\n| rand32(1,6) |\n+-------------+\n| 3           |\n+-------------+\n\nnebula&gt; YIELD \"Hel\" + \"\\tlo\" AS string1, \", World!\" AS string2;\n+-------------+------------+\n| string1     | string2    |\n+-------------+------------+\n| \"Hel    lo\" | \", World!\" |\n+-------------+------------+\n\nnebula&gt; YIELD hash(\"Tim\") % 100;\n+-----------------+\n| (hash(Tim)%100) |\n+-----------------+\n| 42              |\n+-----------------+\n\nnebula&gt; YIELD \\\n      CASE 2+3 \\\n      WHEN 4 THEN 0 \\\n      WHEN 5 THEN 1 \\\n      ELSE -1 \\\n      END \\\n      AS result;\n+--------+\n| result |\n+--------+\n| 1      |\n+--------+\n\nnebula&gt; YIELD 1- -1;\n+----------+\n| (1--(1)) |\n+----------+\n| 2        |\n+----------+\n</code></pre>"},{"location":"3.ngql-guide/9.space-statements/1.create-space/","title":"CREATE SPACE","text":"<p>Graph spaces are used to store data in a physically isolated way in NebulaGraph, which is similar to the database concept in MySQL. The <code>CREATE SPACE</code> statement can create a new graph space or clone the schema of an existing graph space.</p>"},{"location":"3.ngql-guide/9.space-statements/1.create-space/#prerequisites","title":"Prerequisites","text":"<p>Only the God role can use the <code>CREATE SPACE</code> statement. For more information, see AUTHENTICATION.</p>"},{"location":"3.ngql-guide/9.space-statements/1.create-space/#syntax","title":"Syntax","text":""},{"location":"3.ngql-guide/9.space-statements/1.create-space/#create_graph_spaces","title":"Create graph spaces","text":"<pre><code>CREATE SPACE [IF NOT EXISTS] &lt;graph_space_name&gt; (\n    [partition_num = &lt;partition_number&gt;,]\n    [replica_factor = &lt;replica_number&gt;,]\n    vid_type = {FIXED_STRING(&lt;N&gt;) | INT[64]}\n    )\n    [COMMENT = '&lt;comment&gt;'];\n</code></pre> Parameter Description <code>IF NOT EXISTS</code> Detects if the related graph space exists. If it does not exist, a new one will be created. The graph space existence detection here only compares the graph space name (excluding properties). <code>&lt;graph_space_name&gt;</code> 1. Uniquely identifies a graph space in a NebulaGraph instance. 2. Space names cannot be modified after they are set. 3. Space names cannot start with a number; they support 1-4 byte UTF-8 encoded characters, including English letters (case sensitive), numbers, Chinese characters, etc., but do not include special characters other than underscores. To use special characters, reserved keywords or starting with a number, quote them with backticks (`) and cannot use periods (<code>.</code>). For more information, see Keywords and reserved words. Note: If you name a space in Chinese and encounter a <code>SyntaxError</code>, you need to quote the Chinese characters with backticks (`). <code>partition_num</code> Specifies the number of partitions in each replica. The suggested value is 20 times (2 times for HDD) the number of the hard disks in the cluster. For example, if you have three hard disks in the cluster, we recommend that you set 60 partitions. The default value is 100. <code>replica_factor</code> Specifies the number of replicas in the cluster. The suggested number is 3 in a production environment and 1 in a test environment. The replica number must be an odd number for the need of quorum-based voting. The default value is 1. <code>vid_type</code> A required parameter. Specifies the VID type in a graph space. Available values are <code>FIXED_STRING(N)</code> and <code>INT64</code>. <code>INT</code> equals to <code>INT64</code>. <code>`FIXED_STRING(&lt;N&gt;)</code> specifies the VID as a string, while <code>INT64</code> specifies it as an integer. <code>N</code> represents the maximum length of the VIDs. If you set a VID that is longer than <code>N</code> bytes, NebulaGraph throws an error. Note, for UTF-8 chars, the length may vary in different cases, i.e. a UTF-8 Chinese char is 3 byte, this means 11 Chinese chars(length-33) will exeed a FIXED_STRING(32) vid defination. <code>COMMENT</code> The remarks of the graph space. The maximum length is 256 bytes. By default, there is no comments on a space. <p>Caution</p> <ul> <li>If the replica number is set to one, you will not be able to load balance or scale out the NebulaGraph Storage Service with the SUBMIT JOB BALANCE statement.</li> </ul> <ul> <li> <p>Restrictions on VID type change and VID length:</p> <ul> <li>For NebulaGraph v1.x, the type of VIDs can only be <code>INT64</code>, and the String type is not allowed. For NebulaGraph v2.x, both <code>INT64</code> and <code>FIXED_STRING(&lt;N&gt;)</code> VID types are allowed. You must specify the VID type when creating a graph space, and use the same VID type in <code>INSERT</code> statements, otherwise, an error message <code>Wrong vertex id type: 1001</code> occurs.</li> </ul> <ul> <li>The length of the VID should not be longer than <code>N</code> characters. If it exceeds <code>N</code>, NebulaGraph throws <code>The VID must be a 64-bit integer or a string fitting space vertex id length limit.</code>.</li> </ul> </li> </ul> <ul> <li> <p>If the <code>Host not enough!</code> error appears, the immediate cause is that the number of online storage hosts is less than the value of <code>replica_factor</code> specified when creating a graph space. In this case, you can use the <code>SHOW HOSTS</code> command to see if the following situations occur:</p> <ul> <li>For the case where there is only one storage host in a cluster, the value of <code>replica_factor</code> can only be specified to <code>1</code>. Or create a graph space after storage hosts are scaled out. </li> </ul> <ul> <li>A new storage host is found, but <code>ADD HOSTS</code> is not executed to activate it. In this case, run <code>SHOW HOSTS</code> to locate the new storage host information and then run <code>ADD HOSTS</code> to activate it. A graph space can be created after there are enough storage hosts.</li> </ul> <ul> <li>For offline storage hosts after running <code>SHOW HOSTS</code>, troubleshooting is needed.</li> </ul> </li> </ul> <p>Legacy version compatibility</p> <p>For NebulaGraph v2.x before v2.5.0, <code>vid_type</code> is optional and defaults to <code>FIXED_STRING(8)</code>.</p> <p>Note</p> <p><code>graph_space_name</code>, <code>partition_num</code>, <code>replica_factor</code>, <code>vid_type</code>, and <code>comment</code> cannot be modified once set. To modify them, drop the current working graph space with <code>DROP SPACE</code> and create a new one with <code>CREATE SPACE</code>.</p>"},{"location":"3.ngql-guide/9.space-statements/1.create-space/#clone_graph_spaces","title":"Clone graph spaces","text":"<pre><code>CREATE SPACE [IF NOT EXISTS] &lt;new_graph_space_name&gt; AS &lt;old_graph_space_name&gt;;\n</code></pre> Parameter Description <code>IF NOT EXISTS</code> Detects if the new graph space exists. If it does not exist, the new one will be created. The graph space existence detection here only compares the graph space name (excluding properties). <code>&lt;new_graph_space_name&gt;</code> The name of the graph space that is newly created. The name of the graph space starts with a letter, supports 1 to 4 bytes UTF-8 encoded characters, such as English letters (case-sensitive), digits, and Chinese characters, but does not support special characters except underscores. For more information, see Keywords and reserved words. When a new graph space is created, the schema of the old graph space <code>&lt;old_graph_space_name&gt;</code> will be cloned, including its parameters (the number of partitions and replicas, etc.), Tag, Edge type and native indexes. <code>&lt;old_graph_space_name&gt;</code> The name of the graph space that already exists."},{"location":"3.ngql-guide/9.space-statements/1.create-space/#examples","title":"Examples","text":"<pre><code># The following example creates a graph space with a specified VID type and the maximum length. Other fields still use the default values.\nnebula&gt; CREATE SPACE IF NOT EXISTS my_space_1 (vid_type=FIXED_STRING(30));\n\n# The following example creates a graph space with a specified partition number, replica number, and VID type.\nnebula&gt; CREATE SPACE IF NOT EXISTS my_space_2 (partition_num=15, replica_factor=1, vid_type=FIXED_STRING(30));\n\n#  The following example creates a graph space with a specified partition number, replica number, and VID type, and adds a comment on it.\nnebula&gt; CREATE SPACE IF NOT EXISTS my_space_3 (partition_num=15, replica_factor=1, vid_type=FIXED_STRING(30)) comment=\"Test the graph space\";\n\n# Clone a graph space.\nnebula&gt; CREATE SPACE IF NOT EXISTS my_space_4 as my_space_3;\nnebula&gt; SHOW CREATE SPACE my_space_4;\n+--------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Space        | Create Space                                                                                                                                                 |\n+--------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| \"my_space_4\" | \"CREATE SPACE `my_space_4` (partition_num = 15, replica_factor = 1, charset = utf8, collate = utf8_bin, vid_type = FIXED_STRING(30)) comment = '\u6d4b\u8bd5\u56fe\u7a7a\u95f4'\" |\n+--------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+\n</code></pre>"},{"location":"3.ngql-guide/9.space-statements/1.create-space/#implementation_of_the_operation","title":"Implementation of the operation","text":"<p>Caution</p> <p>Trying to use a newly created graph space may fail because the creation is implemented asynchronously. To make sure the follow-up operations work as expected, Wait for two heartbeat cycles, i.e., 20 seconds. To change the heartbeat interval, modify the <code>heartbeat_interval_secs</code> parameter in the configuration files for all services. If the heartbeat interval is too short (i.e., less than 5 seconds), disconnection between peers may happen because of the misjudgment of machines in the distributed system.</p>"},{"location":"3.ngql-guide/9.space-statements/1.create-space/#check_partition_distribution","title":"Check partition distribution","text":"<p>On some large clusters, the partition distribution is possibly unbalanced because of the different startup times. You can run the following command to do a check of the machine distribution.</p> <pre><code>nebula&gt; SHOW HOSTS;\n+-------------+------+----------+--------------+--------------------------------+--------------------------------+---------+\n| Host        | Port | Status   | Leader count | Leader distribution            | Partition distribution         | Version |\n+-------------+------+----------+--------------+--------------------------------+--------------------------------+---------+\n| \"storaged0\" | 9779 | \"ONLINE\" | 8            | \"basketballplayer:3, test:5\"   | \"basketballplayer:10, test:10\" | \"3.5.0\" |\n| \"storaged1\" | 9779 | \"ONLINE\" | 9            | \"basketballplayer:4, test:5\"   | \"basketballplayer:10, test:10\" | \"3.5.0\" |\n| \"storaged2\" | 9779 | \"ONLINE\" | 3            | \"basketballplayer:3\"           | \"basketballplayer:10, test:10\" | \"3.5.0\" |\n+-------------+------+----------+--------------+--------------------------------+--------------------------------+---------+\n</code></pre> <p>To balance the request loads, use the following command.</p> <pre><code>nebula&gt; BALANCE LEADER;\nnebula&gt; SHOW HOSTS;\n+-------------+------+----------+--------------+--------------------------------+--------------------------------+---------+\n| Host        | Port | HTTP port | Status   | Leader count | Leader distribution            | Partition distribution         | Version |\n+-------------+------+-----------+----------+--------------+--------------------------------+--------------------------------+---------+\n| \"storaged0\" | 9779 | \"ONLINE\" | 7            | \"basketballplayer:3, test:4\"   | \"basketballplayer:10, test:10\" | \"3.5.0\" |\n| \"storaged1\" | 9779 | \"ONLINE\" | 7            | \"basketballplayer:4, test:3\"   | \"basketballplayer:10, test:10\" | \"3.5.0\" |\n| \"storaged2\" | 9779 | \"ONLINE\" | 6            | \"basketballplayer:3, test:3\"   | \"basketballplayer:10, test:10\" | \"3.5.0\" |\n+-------------+------+----------+--------------+--------------------------------+--------------------------------+---------+\n</code></pre>"},{"location":"3.ngql-guide/9.space-statements/2.use-space/","title":"USE","text":"<p><code>USE</code> specifies a graph space as the current working graph space for subsequent queries.</p>"},{"location":"3.ngql-guide/9.space-statements/2.use-space/#prerequisites","title":"Prerequisites","text":"<p>Running the <code>USE</code> statement requires some privileges for the graph space. Otherwise, NebulaGraph throws an error.</p>"},{"location":"3.ngql-guide/9.space-statements/2.use-space/#syntax","title":"Syntax","text":"<pre><code>USE &lt;graph_space_name&gt;;\n</code></pre>"},{"location":"3.ngql-guide/9.space-statements/2.use-space/#examples","title":"Examples","text":"<pre><code># The following example creates two sample spaces.\nnebula&gt; CREATE SPACE IF NOT EXISTS space1 (vid_type=FIXED_STRING(30));\nnebula&gt; CREATE SPACE IF NOT EXISTS space2 (vid_type=FIXED_STRING(30));\n\n# The following example specifies space1 as the current working graph space.\nnebula&gt; USE space1;\n\n# The following example specifies space2 as the current working graph space. Hereafter, you cannot read any data from space1, because these vertices and edges being traversed have no relevance with space1.\nnebula&gt; USE space2;\n</code></pre> <p>Caution</p> <p>You cannot use two graph spaces in one statement.</p> <p>Different from Fabric Cypher, graph spaces in NebulaGraph are fully isolated from each other. Making a graph space as the working graph space prevents you from accessing other spaces. The only way to traverse in a new graph space is to switch by the <code>USE</code> statement. In Fabric Cypher, you can use two graph spaces in one statement (using the <code>USE + CALL</code> syntax). But in NebulaGraph, you can only use one graph space in one statement.</p>"},{"location":"3.ngql-guide/9.space-statements/3.show-spaces/","title":"SHOW SPACES","text":"<p><code>SHOW SPACES</code> lists all the graph spaces in the NebulaGraph examples.</p>"},{"location":"3.ngql-guide/9.space-statements/3.show-spaces/#syntax","title":"Syntax","text":"<pre><code>SHOW SPACES;\n</code></pre>"},{"location":"3.ngql-guide/9.space-statements/3.show-spaces/#example","title":"Example","text":"<pre><code>nebula&gt; SHOW SPACES;\n+--------------------+\n| Name               |\n+--------------------+\n| \"cba\"              |\n| \"basketballplayer\" |\n+--------------------+\n</code></pre> <p>To create graph spaces, see CREATE SPACE.</p>"},{"location":"3.ngql-guide/9.space-statements/4.describe-space/","title":"DESCRIBE SPACE","text":"<p><code>DESCRIBE SPACE</code> returns the information about the specified graph space.</p>"},{"location":"3.ngql-guide/9.space-statements/4.describe-space/#syntax","title":"Syntax","text":"<p>You can use <code>DESC</code> instead of <code>DESCRIBE</code> for short.</p> <pre><code>DESC[RIBE] SPACE &lt;graph_space_name&gt;;\n</code></pre> <p>The <code>DESCRIBE SPACE</code> statement is different from the <code>SHOW SPACES</code> statement. For details about <code>SHOW SPACES</code>, see SHOW SPACES.</p>"},{"location":"3.ngql-guide/9.space-statements/4.describe-space/#example","title":"Example","text":"<pre><code>nebula&gt; DESCRIBE SPACE basketballplayer;\n+----+--------------------+------------------+----------------+---------+------------+--------------------+---------+\n| ID | Name               | Partition Number | Replica Factor | Charset | Collate    | Vid Type           | Comment |\n+----+--------------------+------------------+----------------+---------+------------+--------------------+---------+\n| 1  | \"basketballplayer\" | 10               | 1              | \"utf8\"  | \"utf8_bin\" | \"FIXED_STRING(32)\" |         |\n+----+--------------------+------------------+----------------+---------+------------+--------------------+---------+\n</code></pre>"},{"location":"3.ngql-guide/9.space-statements/5.drop-space/","title":"DROP SPACE","text":"<p><code>DROP SPACE</code> deletes the specified graph space and everything in it.</p> <p>Note</p> <p><code>DROP SPACE</code> can only delete the specified logic graph space while retain all the data on the hard disk by modifying the value of <code>auto_remove_invalid_space</code> to <code>false</code> in the Storage service configuration file. For more information, see Storage configuration.</p> <p>Warning</p> <p>After you execute <code>DROP SPACE</code>, even if the snapshot contains data of the graph space, the data of the graph space cannot be recovered. But if the value of <code>auto_remove_invalid_space</code> is set to <code>false</code>, contact the sales team to recover the data of the graph space. </p>"},{"location":"3.ngql-guide/9.space-statements/5.drop-space/#prerequisites","title":"Prerequisites","text":"<p>Only the God role can use the <code>DROP SPACE</code> statement. For more information, see AUTHENTICATION.</p>"},{"location":"3.ngql-guide/9.space-statements/5.drop-space/#syntax","title":"Syntax","text":"<pre><code>DROP SPACE [IF EXISTS] &lt;graph_space_name&gt;;\n</code></pre> <p>You can use the <code>IF EXISTS</code> keywords when dropping spaces. These keywords automatically detect if the related graph space exists. If it exists, it will be deleted. Otherwise, no graph space will be deleted.</p> <p>Legacy version compatibility</p> <p>In NebulaGraph versions earlier than 3.1.0, the <code>DROP SPACE</code> statement does not remove all the files and directories from the disk by default.</p> <p>Caution<p>BE CAUTIOUS about running the <code>DROP SPACE</code> statement.</p> </p>"},{"location":"3.ngql-guide/9.space-statements/5.drop-space/#faq","title":"FAQ","text":"<p>Q: Why is my disk space not freed after executing the 'DROP SPACE' statement and deleting a graph space?</p> <p>A: For NebulaGraph version earlier than 3.1.0, <code>DROP SPACE</code> can only delete the specified logic graph space and does not delete the files and directories on the disk. To delete the files and directories on the disk, manually delete the corresponding file path. The file path is located in <code>&lt;nebula_graph_install_path&gt;/data/storage/nebula/&lt;space_id&gt;</code>. The <code>&lt;space_id&gt;</code> can be viewed via <code>DESCRIBE SPACE {space_name}</code>.</p>"},{"location":"3.ngql-guide/9.space-statements/6.clear-space/","title":"CLEAR SPACE","text":"<p><code>CLEAR SPACE</code> deletes the vertices and edges in a graph space, but does not delete the graph space itself and the schema information.</p> <p>Note</p> <p>It is recommended to execute <code>SUBMIT JOB COMPACT</code> immediately after executing the <code>CLEAR SPACE</code> operation improve the query performance. Note that the COMPACT operation may affect query performance, and it is recommended to perform this operation during low business hours (e.g., early morning).</p>"},{"location":"3.ngql-guide/9.space-statements/6.clear-space/#permission_requirements","title":"Permission requirements","text":"<p>Only the God role has the permission to run <code>CLEAR SPACE</code>.</p>"},{"location":"3.ngql-guide/9.space-statements/6.clear-space/#caution","title":"Caution","text":"<ul> <li>Once cleared, the data CANNOT be recovered. Use <code>CLEAR SPACE</code> with caution.</li> <li><code>CLEAR SPACE</code> is not an atomic operation. If an error occurs, re-run <code>CLEAR SPACE</code> to avoid data remaining.</li> <li>The larger the amount of data in the graph space, the longer it takes to clear it. If the execution fails due to client connection timeout, increase the value of the <code>storage_client_timeout_ms</code> parameter in the Graph Service configuration.</li> <li>During the execution of <code>CLEAR SPACE</code>, writing data into the graph space is not automatically prohibited. Such write operations can result in incomplete data clearing, and the residual data can be damaged.</li> </ul> <p>Note</p> <p>The NebulaGraph Community Edition does not support blocking data writing while allowing <code>CLEAR SPACE</code>.</p> <p>Enterpriseonly</p> <p>The NebulaGraph Enterprise Edition supports blocking data writing by setting <code>VARIABLE read_only=true</code> before running <code>CLEAR SPACE</code>. After the data are cleared successfully, run <code>SET VARIABLE read_only=false</code> to allow data writing again.</p>"},{"location":"3.ngql-guide/9.space-statements/6.clear-space/#syntax","title":"Syntax","text":"<pre><code>CLEAR SPACE [IF EXISTS] &lt;space_name&gt;;\n</code></pre> Parameter/Option Description <code>IF EXISTS</code> Check whether the graph space to be cleared exists. If it exists, continue to clear it. If it does not exist, the execution finishes, and a message indicating that the execution succeeded is displayed. If <code>IF EXISTS</code> is not set and the graph space does not exist, the <code>CLEAR SPACE</code> statement fails to execute, and an error occurs. <code>space_name</code> The name of the space to be cleared. <p>Example:</p> <pre><code>CLEAR SPACE basketballplayer;\n</code></pre>"},{"location":"3.ngql-guide/9.space-statements/6.clear-space/#data_reserved","title":"Data reserved","text":"<p><code>CLEAR SPACE</code> does not delete the following data in a graph space:</p> <ul> <li>Tag information.</li> <li>Edge type information.</li> <li>The metadata of native indexes and full-text indexes.</li> </ul> <p>The following example shows what <code>CLEAR SPACE</code> deletes and reserves.</p> <pre><code># Enter the graph space basketballplayer.\nnebula [(none)]&gt; use basketballplayer;\nExecution succeeded\n\n# List tags and Edge types.\nnebula[basketballplayer]&gt; SHOW TAGS;\n+----------+\n| Name     |\n+----------+\n| \"player\" |\n| \"team\"   |\n+----------+\nGot 2 rows\n\nnebula[basketballplayer]&gt; SHOW EDGES;\n+----------+\n| Name     |\n+----------+\n| \"follow\" |\n| \"serve\"  |\n+----------+\nGot 2 rows\n\n# Submit a job to make statistics of the graph space.\nnebula[basketballplayer]&gt; SUBMIT JOB STATS;\n+------------+\n| New Job Id |\n+------------+\n| 4          |\n+------------+\nGot 1 rows\n\n# Check the statistics.\nnebula[basketballplayer]&gt; SHOW STATS;\n+---------+------------+-------+\n| Type    | Name       | Count |\n+---------+------------+-------+\n| \"Tag\"   | \"player\"   | 51    |\n| \"Tag\"   | \"team\"     | 30    |\n| \"Edge\"  | \"follow\"   | 81    |\n| \"Edge\"  | \"serve\"    | 152   |\n| \"Space\" | \"vertices\" | 81    |\n| \"Space\" | \"edges\"    | 233   |\n+---------+------------+-------+\nGot 6 rows\n\n# List tag indexes.\nnebula[basketballplayer]&gt; SHOW TAG INDEXES;\n+------------------+----------+----------+\n| Index Name       | By Tag   | Columns  |\n+------------------+----------+----------+\n| \"player_index_0\" | \"player\" | []       |\n| \"player_index_1\" | \"player\" | [\"name\"] |\n+------------------+----------+----------+\nGot 2 rows\n\n# ----------------------- Dividing line for CLEAR SPACE -----------------------\n# Run CLEAR SPACE to clear the graph space basketballplayer.\nnebula[basketballplayer]&gt; CLEAR SPACE basketballplayer;\nExecution succeeded\n\n# Update the statistics.\nnebula[basketballplayer]&gt; SUBMIT JOB STATS;\n+------------+\n| New Job Id |\n+------------+\n| 5          |\n+------------+\nGot 1 rows\n\n# Check the statistics. The tags and edge types still exist, but all the vertices and edges are gone.\nnebula[basketballplayer]&gt; SHOW STATS;\n+---------+------------+-------+\n| Type    | Name       | Count |\n+---------+------------+-------+\n| \"Tag\"   | \"player\"   | 0     |\n| \"Tag\"   | \"team\"     | 0     |\n| \"Edge\"  | \"follow\"   | 0     |\n| \"Edge\"  | \"serve\"    | 0     |\n| \"Space\" | \"vertices\" | 0     |\n| \"Space\" | \"edges\"    | 0     |\n+---------+------------+-------+\nGot 6 rows\n\n# Try to list the tag indexes. They still exist.\nnebula[basketballplayer]&gt; SHOW TAG INDEXES;\n+------------------+----------+----------+\n| Index Name       | By Tag   | Columns  |\n+------------------+----------+----------+\n| \"player_index_0\" | \"player\" | []       |\n| \"player_index_1\" | \"player\" | [\"name\"] |\n+------------------+----------+----------+\nGot 2 rows (time spent 523/978 us)\n</code></pre>"},{"location":"4.deployment-and-installation/1.resource-preparations/","title":"Prepare resources for compiling, installing, and running NebulaGraph","text":"<p>This topic describes the requirements and suggestions for compiling and installing NebulaGraph, as well as how to estimate the resource you need to reserve for running a NebulaGraph cluster.</p> <p>Enterpriseonly</p> <p>In addition to installing NebulaGraph with the source code, the Dashboard Enterprise Edition tool is a better and convenient choice for installing Community and Enterprise Edition NebulaGraph. For details, see Deploy Dashboard.</p>"},{"location":"4.deployment-and-installation/1.resource-preparations/#about_storage_devices","title":"About storage devices","text":"<p>NebulaGraph is designed and implemented for NVMe SSD. All default parameters are optimized for the SSD devices and require extremely high IOPS and low latency.</p> <ul> <li>Due to the poor IOPS capability and long random seek latency, HDD is not recommended. Users may encounter many problems when using HDD.</li> </ul> <ul> <li>Do not use remote storage devices, such as NAS or SAN. Do not connect an external virtual hard disk based on HDFS or Ceph.</li> </ul> <ul> <li>Do not use RAID.</li> </ul> <ul> <li>Use local SSD devices, or AWS Provisioned IOPS SSD equivalence.</li> </ul>"},{"location":"4.deployment-and-installation/1.resource-preparations/#about_cpu_architecture","title":"About CPU architecture","text":"<p>Enterpriseonly</p> <p>You can run NebulaGraph Enterprise Edition on ARM, including Apple Mac M1 and Huawei Kunpeng. Contact us for details.</p> <p>Note</p> <p>Starting with 3.0.2, you can run containerized NebulaGraph databases on Docker Desktop for ARM macOS or on ARM Linux servers. </p>"},{"location":"4.deployment-and-installation/1.resource-preparations/#requirements_for_compiling_the_source_code","title":"Requirements for compiling the source code","text":""},{"location":"4.deployment-and-installation/1.resource-preparations/#hardware_requirements_for_compiling_nebulagraph","title":"Hardware requirements for compiling NebulaGraph","text":"Item Requirement CPU architecture x86_64 Memory 4 GB Disk 10 GB, SSD"},{"location":"4.deployment-and-installation/1.resource-preparations/#supported_operating_systems_for_compiling_nebulagraph","title":"Supported operating systems for compiling NebulaGraph","text":"<p>For now, we can only compile NebulaGraph in the Linux system. We recommend that you use any Linux system with kernel version <code>4.15</code> or above.</p> <p>Note</p> <p>To install NebulaGraph on Linux systems with kernel version lower than required, use RPM/DEB packages or TAR files.</p>"},{"location":"4.deployment-and-installation/1.resource-preparations/#software_requirements_for_compiling_nebulagraph","title":"Software requirements for compiling NebulaGraph","text":"<p>You must have the correct version of the software listed below to compile NebulaGraph. If they are not as required or you are not sure, follow the steps in Prepare software for compiling NebulaGraph to get them ready.</p> Software Version Note glibc 2.17 or above You can run <code>ldd --version</code> to check the glibc version. make Any stable version - m4 Any stable version - git Any stable version - wget Any stable version - unzip Any stable version - xz Any stable version - readline-devel Any stable version - ncurses-devel Any stable version - zlib-devel Any stable version - g++ 8.5.0 or above You can run <code>gcc -v</code> to check the gcc version. cmake 3.14.0 or above You can run <code>cmake --version</code> to check the cmake version. curl Any stable version - redhat-lsb-core Any stable version - libstdc++-static Any stable version Only needed in CentOS 8+, RedHat 8+, and Fedora systems. libasan Any stable version Only needed in CentOS 8+, RedHat 8+, and Fedora systems. bzip2 Any stable version - <p>Other third-party software will be automatically downloaded and installed to the <code>build</code> directory at the configure (cmake) stage.</p>"},{"location":"4.deployment-and-installation/1.resource-preparations/#prepare_software_for_compiling_nebulagraph","title":"Prepare software for compiling NebulaGraph","text":"<p>If part of the dependencies are missing or the versions does not meet the requirements, manually install them with the following steps. You can skip unnecessary dependencies or steps according to your needs.</p> <ol> <li> <p>Install dependencies.</p> <ul> <li>For CentOS, RedHat, and Fedora users, run the following commands.<pre><code>$ yum update\n$ yum install -y make \\\n                 m4 \\\n                 git \\\n                 wget \\\n                 unzip \\\n                 xz \\\n                 readline-devel \\\n                 ncurses-devel \\\n                 zlib-devel \\\n                 gcc \\\n                 gcc-c++ \\\n                 cmake \\\n                 curl \\\n                 redhat-lsb-core \\\n                 bzip2\n  // For CentOS 8+, RedHat 8+, and Fedora, install libstdc++-static and libasan as well\n$ yum install -y libstdc++-static libasan\n</code></pre> </li> </ul> <ul> <li>For Debian and Ubuntu users, run the following commands.<pre><code>$ apt-get update\n$ apt-get install -y make \\\n                     m4 \\\n                     git \\\n                     wget \\\n                     unzip \\\n                     xz-utils \\\n                     curl \\\n                     lsb-core \\\n                     build-essential \\\n                     libreadline-dev \\\n                     ncurses-dev \\\n                     cmake \\\n                     bzip2\n</code></pre> </li> </ul> </li> <li> <p>Check if the GCC and cmake on your host are in the right version. See Software requirements for compiling NebulaGraph for the required versions.</p> <pre><code>$ g++ --version\n$ cmake --version\n</code></pre> <p>If your GCC and CMake are in the right versions, then you are all set and you can ignore the subsequent steps. If they are not, select and perform the needed steps as follows.</p> </li> <li> <p>If the CMake version is incorrect, visit the CMake official website to install the required version.</p> </li> <li> <p>If the G++ version is incorrect, visit the G++ official website or follow the instructions below to to install the required version.</p> <ul> <li> <p>For CentOS users, run:</p> <pre><code>yum install centos-release-scl\nyum install devtoolset-11\nscl enable devtoolset-11 'bash'\n</code></pre> </li> </ul> <ul> <li> <p>For Ubuntu users, run:</p> <pre><code>add-apt-repository ppa:ubuntu-toolchain-r/test\napt install gcc-11 g++-11\n</code></pre> </li> </ul> </li> </ol>"},{"location":"4.deployment-and-installation/1.resource-preparations/#requirements_and_suggestions_for_installing_nebulagraph_in_test_environments","title":"Requirements and suggestions for installing NebulaGraph in test environments","text":""},{"location":"4.deployment-and-installation/1.resource-preparations/#hardware_requirements_for_test_environments","title":"Hardware requirements for test environments","text":"Item Requirement CPU architecture x86_64 Number of CPU core 4 Memory 8 GB Disk 100 GB, SSD"},{"location":"4.deployment-and-installation/1.resource-preparations/#supported_operating_systems_for_test_environments","title":"Supported operating systems for test environments","text":"<p>For now, we can only install NebulaGraph in the Linux system. To install NebulaGraph in a test environment, we recommend that you use any Linux system with kernel version <code>3.9</code> or above.</p>"},{"location":"4.deployment-and-installation/1.resource-preparations/#suggested_service_architecture_for_test_environments","title":"Suggested service architecture for test environments","text":"Process Suggested number metad (the metadata service process) 1 storaged (the storage service process) 1 or more graphd (the query engine service process) 1 or more <p>For example, for a single-machine test environment, you can deploy 1 metad, 1 storaged, and 1 graphd processes in the machine.</p> <p>For a more common test environment, such as a cluster of 3 machines (named as A, B, and C), you can deploy NebulaGraph as follows:</p> Machine name Number of metad Number of storaged Number of graphd A 1 1 1 B None 1 1 C None 1 1"},{"location":"4.deployment-and-installation/1.resource-preparations/#requirements_and_suggestions_for_installing_nebulagraph_in_production_environments","title":"Requirements and suggestions for installing NebulaGraph in production environments","text":""},{"location":"4.deployment-and-installation/1.resource-preparations/#hardware_requirements_for_production_environments","title":"Hardware requirements for production environments","text":"Item Requirement CPU architecture x86_64 Number of CPU core 48 Memory 256 GB Disk 2 * 1.6 TB, NVMe SSD"},{"location":"4.deployment-and-installation/1.resource-preparations/#supported_operating_systems_for_production_environments","title":"Supported operating systems for production environments","text":"<p>For now, we can only install NebulaGraph in the Linux system. To install NebulaGraph in a production environment, we recommend that you use any Linux system with kernel version 3.9 or above.</p> <p>Users can adjust some of the kernel parameters to better accommodate the need for running NebulaGraph. For more information, see kernel configuration.</p>"},{"location":"4.deployment-and-installation/1.resource-preparations/#suggested_service_architecture_for_production_environments","title":"Suggested service architecture for production environments","text":"<p>Danger</p> <p>DO NOT deploy a single cluster across IDCs (The Enterprise Edtion supports data synchronization between clusters across IDCs).</p> Process Suggested number metad (the metadata service process) 3 storaged (the storage service process) 3 or more graphd (the query engine service process) 3 or more <p>Each metad process automatically creates and maintains a replica of the metadata. Usually, you need to deploy three metad processes and only three.</p> <p>The number of storaged processes does not affect the number of graph space replicas.</p> <p>Users can deploy multiple processes on a single machine. For example, on a cluster of 5 machines (named as A, B, C, D, and E), you can deploy NebulaGraph as follows:</p> Machine name Number of metad Number of storaged Number of graphd A 1 1 1 B 1 1 1 C 1 1 1 D None 1 1 E None 1 1"},{"location":"4.deployment-and-installation/1.resource-preparations/#capacity_requirements_for_running_a_nebulagraph_cluster","title":"Capacity requirements for running a NebulaGraph cluster","text":"<p>Users can estimate the memory, disk space, and partition number needed for a NebulaGraph cluster of 3 replicas as follows.</p> Resource Unit How to estimate Description Disk space for a cluster Bytes <code>the_sum_of_edge_number_and_vertex_number</code> * <code>average_bytes_of_properties</code> * 7.5 * 120% For more information, see Edge partitioning and storage amplification. Memory for a cluster Bytes [<code>the_sum_of_edge_number_and_vertex_number</code> * 16 + <code>the_number_of_RocksDB_instances</code> * (<code>write_buffer_size</code> * <code>max_write_buffer_number</code>) + <code>rocksdb_block_cache</code>] * 120% <code>write_buffer_size</code> and <code>max_write_buffer_number</code> are RocksDB parameters. For more information, see MemTable. For details about <code>rocksdb_block_cache</code>, see Memory usage in RocksDB. Number of partitions for a graph space - <code>the_number_of_disks_in_the_cluster</code> * <code>disk_partition_num_multiplier</code> <code>disk_partition_num_multiplier</code> is an integer between 2 and 20 (both including). Its value depends on the disk performance. Use 20 for SSD and 2 for HDD. <ul> <li>Question 1: Why do I need to multiply by 7.5 in the disk space estimation formula?<p>Answer: On one hand, the data in one single replica takes up about 2.5 times more space than that of the original data file (csv) according to test values.    On the other hand, indexes take up additional space. Each indexed vertex or edge takes up 16 bytes of memory. The hard disk space occupied by the index can be empirically estimated as the total number of indexed vertices or edges * 50 bytes.</p> </li> </ul> <ul> <li>Question 2: Why do we multiply the disk space and memory by 120%?<p>Answer: The extra 20% is for buffer.</p> </li> </ul> <ul> <li> <p>Question 3: How to get the number of RocksDB instances?</p> <p>Answer: Each graph space corresponds to one RocksDB instance and each directory in the <code>--data_path</code> item in the <code>etc/nebula-storaged.conf</code> file corresponds to one RocksDB instance.    That is, the number of RocksDB instances = the number of directories * the number of graph spaces.</p> <p>Note</p> <p>Users can decrease the memory size occupied by the bloom filter by adding <code>--enable_partitioned_index_filter=true</code> in <code>etc/nebula-storaged.conf</code>. But it may decrease the read performance in some random-seek cases.</p> </li> </ul> <p>Caution</p> <p>Each RocksDB instance takes up about 70M of disk space even when no data has been written yet. One partition corresponds to one RocksDB instance, and when the partition setting is very large, for example, 100, the graph space takes up a lot of disk space after it is created.</p>"},{"location":"4.deployment-and-installation/4.uninstall-nebula-graph/","title":"Uninstall NebulaGraph","text":"<p>This topic describes how to uninstall NebulaGraph.</p> <p>Caution</p> <p>Before re-installing NebulaGraph on a machine, follow this topic to completely uninstall the old NebulaGraph, in case the remaining data interferes with the new services, including inconsistencies between Meta services.</p>"},{"location":"4.deployment-and-installation/4.uninstall-nebula-graph/#prerequisite","title":"Prerequisite","text":"<p>The NebulaGraph services should be stopped before the uninstallation. For more information, see Manage NebulaGraph services.</p>"},{"location":"4.deployment-and-installation/4.uninstall-nebula-graph/#step_1_delete_data_files_of_the_storage_and_meta_services","title":"Step 1: Delete data files of the Storage and Meta Services","text":"<p>If you have modified the <code>data_path</code> in the configuration files for the Meta Service and Storage Service, the directories where NebulaGraph stores data may not be in the installation path of NebulaGraph. Check the configuration files to confirm the data paths, and then manually delete the directories to clear all data.</p> <p>Note</p> <p>For a NebulaGraph cluster, delete the data files of all Storage and Meta servers.</p> <ol> <li> <p>Check the Storage Service disk settings. For example:</p> <pre><code>########## Disk ##########\n# Root data path. Split by comma. e.g. --data_path=/disk1/path1/,/disk2/path2/\n# One path per Rocksdb instance.\n--data_path=/nebula/data/storage\n</code></pre> </li> <li> <p>Check the Metad Service configurations and find the corresponding metadata directories.</p> </li> <li> <p>Delete the data and the directories found in step 2.</p> </li> </ol>"},{"location":"4.deployment-and-installation/4.uninstall-nebula-graph/#step_2_delete_the_installation_directories","title":"Step 2: Delete the installation directories","text":"<p>Note</p> <p>Delete all installation directories, including the <code>cluster.id</code> file in them.</p> <p>The default installation path is <code>/usr/local/nebula</code>, which is specified by <code>--prefix</code> while installing NebulaGraph.</p>"},{"location":"4.deployment-and-installation/4.uninstall-nebula-graph/#uninstall_nebulagraph_deployed_with_source_code","title":"Uninstall NebulaGraph deployed with source code","text":"<p>Find the installation directories of NebulaGraph, and delete them all.</p>"},{"location":"4.deployment-and-installation/4.uninstall-nebula-graph/#uninstall_nebulagraph_deployed_with_rpm_packages","title":"Uninstall NebulaGraph deployed with RPM packages","text":"<ol> <li> <p>Run the following command to get the NebulaGraph version.</p> <pre><code>$ rpm -qa | grep \"nebula\"\n</code></pre> <p>The return message is as follows.</p> <pre><code>nebula-graph-3.5.0-1.x86_64\n</code></pre> </li> <li> <p>Run the following command to uninstall NebulaGraph.</p> <pre><code>sudo rpm -e &lt;nebula_version&gt;\n</code></pre> <p>For example:</p> <pre><code>sudo rpm -e nebula-graph-3.5.0-1.x86_64\n</code></pre> </li> <li> <p>Delete the installation directories.</p> </li> </ol>"},{"location":"4.deployment-and-installation/4.uninstall-nebula-graph/#uninstall_nebulagraph_deployed_with_deb_packages","title":"Uninstall NebulaGraph deployed with DEB packages","text":"<ol> <li> <p>Run the following command to get the NebulaGraph version.</p> <pre><code>$ dpkg -l | grep \"nebula\"\n</code></pre> <p>The return message is as follows.</p> <pre><code>ii  nebula-graph  3.5.0  amd64     NebulaGraph Package built using CMake\n</code></pre> </li> <li> <p>Run the following command to uninstall NebulaGraph.</p> <pre><code>sudo dpkg -r &lt;nebula_version&gt;\n</code></pre> <p>For example:</p> <pre><code>sudo dpkg -r nebula-graph\n</code></pre> </li> <li> <p>Delete the installation directories.</p> </li> </ol>"},{"location":"4.deployment-and-installation/4.uninstall-nebula-graph/#uninstall_nebulagraph_deployed_with_docker_compose","title":"Uninstall NebulaGraph deployed with Docker Compose","text":"<ol> <li> <p>In the <code>nebula-docker-compose</code> directory, run the following command to stop the NebulaGraph services.</p> <pre><code>docker-compose down -v\n</code></pre> </li> <li> <p>Delete the <code>nebula-docker-compose</code> directory.</p> </li> </ol>"},{"location":"4.deployment-and-installation/connect-to-nebula-graph/","title":"Connect to NebulaGraph","text":"<p>This topic provides basic instruction on how to use the native CLI client NebulaGraph Console to connect to NebulaGraph.</p> <p>Caution</p> <p>When connecting to NebulaGraph for the first time, you must register the Storage Service before querying data.</p> <p>NebulaGraph supports multiple types of clients, including a CLI client, a GUI client, and clients developed in popular programming languages. For more information, see the client list.</p>"},{"location":"4.deployment-and-installation/connect-to-nebula-graph/#prerequisites","title":"Prerequisites","text":"<ul> <li>You have started NebulaGraph services.</li> </ul> <ul> <li>The machine on which you plan to run NebulaGraph Console has network access to the Graph Service of NebulaGraph.</li> </ul> <ul> <li> <p>The NebulaGraph Console version is compatible with the NebulaGraph version.</p> <p>Note</p> <p>NebulaGraph Console and NebulaGraph of the same version number are the most compatible. There may be compatibility issues when connecting to NebulaGraph with a different version of NebulaGraph Console. The error message <code>incompatible version between client and server</code> is displayed when there is such an issue.</p> </li> </ul>"},{"location":"4.deployment-and-installation/connect-to-nebula-graph/#steps","title":"Steps","text":"<ol> <li> <p>On the NebulaGraph Console releases page, select a NebulaGraph Console version and click Assets.</p> <p>Note</p> <p>It is recommended to select the latest version.</p> </li> <li> <p>In the Assets area, find the correct binary file for the machine where you want to run NebulaGraph Console and download the file to the machine.</p> </li> <li> <p>(Optional) Rename the binary file to <code>nebula-console</code> for convenience.</p> <p>Note</p> <p>For Windows, rename the file to <code>nebula-console.exe</code>.</p> </li> <li> <p>On the machine to run NebulaGraph Console, grant the execute permission of the nebula-console binary file to the user.</p> <p>Note</p> <p>For Windows, skip this step.</p> <pre><code>$ chmod 111 nebula-console\n</code></pre> </li> <li> <p>In the command line interface, change the working directory to the one where the nebula-console binary file is stored.</p> </li> <li> <p>Run the following command to connect to NebulaGraph.</p> <ul> <li>For Linux or macOS:</li> </ul> <pre><code>$ ./nebula-console -addr &lt;ip&gt; -port &lt;port&gt; -u &lt;username&gt; -p &lt;password&gt;\n[-t 120] [-e \"nGQL_statement\" | -f filename.nGQL]\n</code></pre> <ul> <li>For Windows:</li> </ul> <pre><code>&gt; nebula-console.exe -addr &lt;ip&gt; -port &lt;port&gt; -u &lt;username&gt; -p &lt;password&gt;\n[-t 120] [-e \"nGQL_statement\" | -f filename.nGQL]\n</code></pre> <p>Parameter descriptions are as follows:</p> Parameter Description <code>-h/-help</code> Shows the help menu. <code>-addr/-address</code> Sets the IP address of the Graph service. The default address is 127.0.0.1.  <code>-P/-port</code> Sets the port number of the graphd service. The default port number is 9669. <code>-u/-user</code> Sets the username of your NebulaGraph account. Before enabling authentication, you can use any existing username. The default username is <code>root</code>. <code>-p/-password</code> Sets the password of your NebulaGraph account. Before enabling authentication, you can use any characters as the password. <code>-t/-timeout</code> Sets an integer-type timeout threshold of the connection. The unit is millisecond. The default value is 120. <code>-e/-eval</code> Sets a string-type nGQL statement. The nGQL statement is executed once the connection succeeds. The connection stops after the result is returned. <code>-f/-file</code> Sets the path of an nGQL file. The nGQL statements in the file are executed once the connection succeeds. The result will be returned and the connection stops then. <code>-enable_ssl</code> Enables SSL encryption when connecting to NebulaGraph. <code>-ssl_root_ca_path</code> Sets the storage path of the certification authority file. <code>-ssl_cert_path</code> Sets the storage path of the certificate file. <code>-ssl_private_key_path</code> Sets the storage path of the private key file. <p>For information on more parameters, see the project repository.</p> </li> </ol>"},{"location":"4.deployment-and-installation/manage-service/","title":"Manage NebulaGraph Service","text":"<p>NebulaGraph supports managing services with scripts. </p> <p>Enterpriseonly</p> <p>You can also manage NebulaGraph with systemd in the NebulaGraph Enterprise Edition.</p> <p>Danger</p> <p>The two methods are incompatible. It is recommended to use only one method in a cluster.</p>"},{"location":"4.deployment-and-installation/manage-service/#manage_services_with_script","title":"Manage services with script","text":"<p>You can use the <code>nebula.service</code> script to start, stop, restart, terminate, and check the NebulaGraph services.</p> <p>Note</p> <p><code>nebula.service</code> is stored in the <code>/usr/local/nebula/scripts</code> directory by default. If you have customized the path, use the actual path in your environment.</p>"},{"location":"4.deployment-and-installation/manage-service/#syntax","title":"Syntax","text":"<pre><code>$ sudo /usr/local/nebula/scripts/nebula.service\n[-v] [-c &lt;config_file_path&gt;]\n&lt;start | stop | restart | kill | status&gt;\n&lt;metad | graphd | storaged | all&gt;\n</code></pre> Parameter Description <code>-v</code> Display detailed debugging information. <code>-c</code> Specify the configuration file path. The default path is <code>/usr/local/nebula/etc/</code>. <code>start</code> Start the target services. <code>stop</code> Stop the target services. <code>restart</code> Restart the target services. <code>kill</code> Terminate the target services. <code>status</code> Check the status of the target services. <code>metad</code> Set the Meta Service as the target service. <code>graphd</code> Set the Graph Service as the target service. <code>storaged</code> Set the Storage Service as the target service. <code>all</code> Set all the NebulaGraph services as the target services."},{"location":"4.deployment-and-installation/manage-service/#manage_services_with_systemd","title":"Manage services with systemd","text":"<p>For easy maintenance, NebulaGraph Enterprise Edition supports managing services with systemd. You can start, stop, restart, and check services with <code>systemctl</code> commands.</p> <p>Note</p> <ul> <li>After installing NebulaGraph Enterprise Edition, the <code>.service</code> files required by systemd are located in the <code>etc/unit</code> path in the installation directory. NebulaGraph installed with the RPM/DEB package automatically places the <code>.service</code> files into the path <code>/usr/lib/systemd/system</code> and the parameter <code>ExecStart</code> is generated based on the specified NebulaGraph installation path, so you can use <code>systemctl</code> commands directly.</li> </ul> <ul> <li>The <code>systemctl</code> commands cannot be used to manage the Enterprise Edition cluster that is created with Dashboard of the Enterprise Edition.</li> </ul> <ul> <li>Otherwise, users need to move the <code>.service</code> files manually into the directory <code>/usr/lib/systemd/system</code>, and modify the file path of the parameter <code>ExecStart</code> in the <code>.service</code> files.</li> </ul>"},{"location":"4.deployment-and-installation/manage-service/#syntax_1","title":"Syntax","text":"<pre><code>$ systemctl &lt;start | stop | restart | status &gt; &lt;nebula | nebula-metad | nebula-graphd | nebula-storaged&gt;\n</code></pre> Parameter Description <code>start</code> Start the target services. <code>stop</code> Stop the target services. <code>restart</code> Restart the target services. <code>status</code> Check the status of the target services. <code>nebula</code> Set all the NebulaGraph services as the target services. <code>nebula-metad</code> Set the Meta Service as the target service. <code>nebula-graphd</code> Set the Graph Service as the target service. <code>nebula-storaged</code> Set the Storage Service as the target service."},{"location":"4.deployment-and-installation/manage-service/#start_nebulagraph","title":"Start NebulaGraph","text":"<p>Run the following command to start NebulaGraph.</p> <pre><code>$ sudo /usr/local/nebula/scripts/nebula.service start all\n[INFO] Starting nebula-metad...\n[INFO] Done\n[INFO] Starting nebula-graphd...\n[INFO] Done\n[INFO] Starting nebula-storaged...\n[INFO] Done\n</code></pre> <p>Users can also run the following command:</p> <pre><code>$ systemctl start nebula\n</code></pre> <p>If users want to automatically start NebulaGraph when the machine starts, run the following command:</p> <pre><code>$ systemctl enable nebula\n</code></pre>"},{"location":"4.deployment-and-installation/manage-service/#stop_nebulagraph","title":"Stop NebulaGraph","text":"<p>Danger</p> <p>Do not run <code>kill -9</code> to forcibly terminate the processes. Otherwise, there is a low probability of data loss.</p> <p>Run the following command to stop NebulaGraph.</p> <pre><code>$ sudo /usr/local/nebula/scripts/nebula.service stop all\n[INFO] Stopping nebula-metad...\n[INFO] Done\n[INFO] Stopping nebula-graphd...\n[INFO] Done\n[INFO] Stopping nebula-storaged...\n[INFO] Done\n</code></pre> <p>Users can also run the following command:</p> <pre><code>$ systemctl stop nebula\n</code></pre>"},{"location":"4.deployment-and-installation/manage-service/#check_the_service_status","title":"Check the service status","text":"<p>Run the following command to check the service status of NebulaGraph.</p> <pre><code>$ sudo /usr/local/nebula/scripts/nebula.service status all\n</code></pre> <ul> <li> <p>NebulaGraph is running normally if the following information is returned.</p> <pre><code>INFO] nebula-metad(33fd35e): Running as 29020, Listening on 9559\n[INFO] nebula-graphd(33fd35e): Running as 29095, Listening on 9669\n[WARN] nebula-storaged after v3.0.0 will not start service until it is added to cluster.\n[WARN] See Manage Storage hosts:ADD HOSTS in https://docs.nebula-graph.io/\n[INFO] nebula-storaged(33fd35e): Running as 29147, Listening on 9779\n</code></pre> <p>Note</p> <p>After starting NebulaGraph, the port of the <code>nebula-storaged</code> process is shown in red. Because the <code>nebula-storaged</code> process waits for the <code>nebula-metad</code> to add the current Storage service during the startup process. The Storage works after it receives the ready signal. Starting from NebulaGraph 3.0.0, the Meta service cannot directly read or write data in the Storage service that you add in the configuration file. The configuration file only registers the Storage service to the Meta service. You must run the <code>ADD HOSTS</code> command to enable the Meta to read and write data in the Storage service. For more information, see Manage Storage hosts.</p> </li> </ul> <ul> <li>If the returned result is similar to the following one, there is a problem. You may also go to the NebulaGraph community for help.<pre><code>[INFO] nebula-metad: Running as 25600, Listening on 9559\n[INFO] nebula-graphd: Exited\n[INFO] nebula-storaged: Running as 25646, Listening on 9779\n</code></pre> </li> </ul> <p>Users can also run the following command:</p> <pre><code>$ systemctl status nebula\n\u25cf nebula.service\n   Loaded: loaded (/usr/lib/systemd/system/nebula.service; disabled; vendor preset: disabled)\n   Active: active (exited) since \u4e00 2022-03-28 04:13:24 UTC; 1h 47min ago\n  Process: 21772 ExecStart=/usr/local/ent-nightly/scripts/nebula.service start all (code=exited, status=0/SUCCESS)\n Main PID: 21772 (code=exited, status=0/SUCCESS)\n    Tasks: 325\n   Memory: 424.5M\n   CGroup: /system.slice/nebula.service\n           \u251c\u250021789 /usr/local/ent-nightly/bin/nebula-metad --flagfile /usr/local/ent-nightly/etc/nebula-metad.conf\n           \u251c\u250021827 /usr/local/ent-nightly/bin/nebula-graphd --flagfile /usr/local/ent-nightly/etc/nebula-graphd.conf\n           \u2514\u250021900 /usr/local/ent-nightly/bin/nebula-storaged --flagfile /usr/local/ent-nightly/etc/nebula-storaged.conf\n3\u6708 28 04:13:24 xxxxxx systemd[1]: Started nebula.service.\n...\n</code></pre> <p>The NebulaGraph services consist of the Meta Service, Graph Service, and Storage Service. The configuration files for all three services are stored in the <code>/usr/local/nebula/etc/</code> directory by default. You can check the configuration files according to the returned result to troubleshoot problems.</p>"},{"location":"4.deployment-and-installation/manage-service/#next_to_do","title":"Next to do","text":"<p>Connect to NebulaGraph</p>"},{"location":"4.deployment-and-installation/manage-storage-host/","title":"Manage Storage hosts","text":"<p>Starting from NebulaGraph 3.0.0, setting Storage hosts in the configuration files only registers the hosts on the Meta side, but does not add them into the cluster. You must run the <code>ADD HOSTS</code> statement to add the Storage hosts.</p> <p>Note</p> <p>NebulaGraph Cloud clusters add Storage hosts automatically. Cloud users do not need to manually run <code>ADD HOSTS</code>.</p>"},{"location":"4.deployment-and-installation/manage-storage-host/#prerequisites","title":"Prerequisites","text":"<ul> <li>You have connected to the NebulaGraph database.</li> </ul>"},{"location":"4.deployment-and-installation/manage-storage-host/#add_storage_hosts","title":"Add Storage hosts","text":"<p>Add the Storage hosts to a NebulaGraph cluster.</p> <pre><code>nebula&gt; ADD HOSTS &lt;ip&gt;:&lt;port&gt; [,&lt;ip&gt;:&lt;port&gt; ...];\nnebula&gt; ADD HOSTS \"&lt;hostname&gt;\":&lt;port&gt; [,\"&lt;hostname&gt;\":&lt;port&gt; ...];\n</code></pre> <p>Note</p> <ul> <li>To make sure the follow-up operations work as expected, wait for two heartbeat cycles, i.e., 20 seconds, and then run <code>SHOW HOSTS</code> to check whether the host is online.</li> </ul> <ul> <li>Make sure that the IP address and port number are the same as those in the configuration file. For example, the default IP address and port number in standalone deployment are <code>127.0.0.1:9779</code>.</li> </ul> <ul> <li>When using a domain name, enclose it in quotation marks, for example, <code>ADD HOSTS \"foo-bar\":9779</code>.</li> </ul> <ul> <li>Ensure that the storage host to be added is not used by any other cluster, otherwise, the storage adding operation will fail.</li> </ul>"},{"location":"4.deployment-and-installation/manage-storage-host/#drop_storage_hosts","title":"Drop Storage hosts","text":"<p>Delete the Storage hosts from cluster.</p> <p>Note</p> <p>You can not delete an in-use Storage host directly. Delete the associated graph space before deleting the Storage host.</p> <pre><code>nebula&gt; DROP HOSTS &lt;ip&gt;:&lt;port&gt; [,&lt;ip&gt;:&lt;port&gt; ...];\nnebula&gt; DROP HOSTS \"&lt;hostname&gt;\":&lt;port&gt; [,\"&lt;hostname&gt;\":&lt;port&gt; ...];\n</code></pre>"},{"location":"4.deployment-and-installation/manage-storage-host/#view_storage_hosts","title":"View Storage hosts","text":"<p>View the Storage hosts in the cluster.</p> <pre><code>nebula&gt; SHOW HOSTS STORAGE;\n+-------------+------+----------+-----------+--------------+----------------------+\n| Host        | Port | Status   | Role      | Git Info Sha | Version              |\n+-------------+------+----------+-----------+--------------+----------------------+\n| \"storaged0\" | 9779 | \"ONLINE\" | \"STORAGE\" | \"3ba41bd\"    | \"3.5.0\" |\n| \"storaged1\" | 9779 | \"ONLINE\" | \"STORAGE\" | \"3ba41bd\"    | \"3.5.0\" |\n| \"storaged2\" | 9779 | \"ONLINE\" | \"STORAGE\" | \"3ba41bd\"    | \"3.5.0\" |\n+-------------+------+----------+-----------+--------------+--------------- ------+\n</code></pre>"},{"location":"4.deployment-and-installation/standalone-deployment/","title":"Standalone NebulaGraph","text":"<p>Standalone NebulaGraph merges the Meta, Storage, and Graph services into a single process deployed on a single machine. This topic introduces scenarios, deployment steps, etc. of standalone NebulaGraph.</p> <p>Danger</p> <p>Do not use standalone NebulaGraph in production environments. </p>"},{"location":"4.deployment-and-installation/standalone-deployment/#background","title":"Background","text":"<p>The traditional NebulaGraph consists of three services, each service having executable binary files and the corresponding process. Processes communicate with each other by RPC. In standalone NebulaGraph, the three processes corresponding to the three services are combined into one process. For more information about NebulaGraph, see Architecture overview.</p>"},{"location":"4.deployment-and-installation/standalone-deployment/#scenarios","title":"Scenarios","text":"<p>Small data sizes and low availability requirements. For example, test environments that are limited by the number of machines, scenarios that are only used to verify functionality.</p>"},{"location":"4.deployment-and-installation/standalone-deployment/#limitations","title":"Limitations","text":"<ul> <li>Single service instance per machine.</li> <li>High availability and reliability not supported.</li> </ul>"},{"location":"4.deployment-and-installation/standalone-deployment/#resource_requirements","title":"Resource requirements","text":"<p>For information about the resource requirements for standalone NebulaGraph, see Software requirements for compiling NebulaGraph.</p>"},{"location":"4.deployment-and-installation/standalone-deployment/#steps","title":"Steps","text":"<p>Currently, you can only install standalone NebulaGraph with the source code. The steps are similar to those of the multi-process NebulaGraph. You only need to modify the step Generate Makefile with CMake by adding <code>-DENABLE_STANDALONE_VERSION=on</code> to the command. For example:</p> <pre><code>cmake -DCMAKE_INSTALL_PREFIX=/usr/local/nebula -DENABLE_TESTING=OFF -DENABLE_STANDALONE_VERSION=on -DCMAKE_BUILD_TYPE=Release .. \n</code></pre> <p>For more information about installation details, see Install NebulaGraph by compiling the source code.</p> <p>After installing standalone NebulaGraph, see the topic connect to Service to connect to NebulaGraph databases.</p>"},{"location":"4.deployment-and-installation/standalone-deployment/#configuration_file","title":"Configuration file","text":"<p>The path to the configuration file for standalone NebulaGraph is <code>/usr/local/nebula/etc</code> by default.</p> <p>You can run <code>sudo cat nebula-standalone.conf.default</code> to see the file content. The parameters and the corresponding descriptions in the file are generally the same as the configurations for multi-process NebulaGraph except for the following parameters.</p> Parameter Predefined value Description <code>meta_port</code> <code>9559</code> The port number of the Meta service. <code>storage_port</code> <code>9779</code> The port number of the Storage Service. <code>meta_data_path</code> <code>data/meta</code> The path to Meta data. <p>You can run commands to check configurable parameters and the corresponding descriptions. For details, see Configurations.</p>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/","title":"Install NebulaGraph by compiling the source code","text":"<p>Installing NebulaGraph from the source code allows you to customize the compiling and installation settings and test the latest features.</p>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Users have to prepare correct resources described in Prepare resources for compiling, installing, and running NebulaGraph.</p> <p>Note</p> <p>Compilation of NebulaGraph offline is not currently supported.</p> </li> </ul> <ul> <li>The host to be installed with NebulaGraph has access to the Internet.</li> </ul> <ul> <li>For NebulaGraph Enterprise, you must have the license key loaded in LM.</li> </ul>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#installation_steps","title":"Installation steps","text":"<ol> <li> <p>Use Git to clone the source code of NebulaGraph to the host.</p> <ul> <li> <p>[Recommended] To install NebulaGraph 3.5.0, run the following command.</p> <pre><code>$ git clone --branch release-3.5 https://github.com/vesoft-inc/nebula.git\n</code></pre> </li> </ul> <ul> <li> <p>To install the latest developing release, run the following command to clone the source code from the master branch.</p> <pre><code>$ git clone https://github.com/vesoft-inc/nebula.git\n</code></pre> </li> </ul> </li> <li> <p>Go to the <code>nebula/third-party</code> directory, and run the <code>install-third-party.sh</code> script to install the third-party libraries.</p> <pre><code>$ cd nebula/third-party\n$ ./install-third-party.sh\n</code></pre> </li> <li> <p>Go back to the <code>nebula</code> directory, create a directory named <code>build</code>, and enter the directory.</p> <pre><code>$ cd ..\n$ mkdir build &amp;&amp; cd build\n</code></pre> </li> <li> <p>Generate Makefile with CMake.</p> <p>Note</p> <p>The installation path is <code>/usr/local/nebula</code> by default. To customize it, add the <code>-DCMAKE_INSTALL_PREFIX=&lt;installation_path&gt;</code> CMake variable in the following command.</p> <p>For more information about CMake variables, see CMake variables.</p> <pre><code>$ cmake -DCMAKE_INSTALL_PREFIX=/usr/local/nebula -DENABLE_TESTING=OFF -DCMAKE_BUILD_TYPE=Release ..\n</code></pre> </li> <li> <p>Compile NebulaGraph.</p> <p>Note</p> <p>Check Prepare resources for compiling, installing, and running NebulaGraph.</p> <p>To speed up the compiling, use the <code>-j</code> option to set a concurrent number <code>N</code>. It should be \\(\\min(\\text{CPU}core number,\\frac{the_memory_size(GB)}{2})\\).</p> <pre><code>$ make -j{N} # E.g., make -j2\n</code></pre> </li> <li> <p>Install NebulaGraph.</p> <pre><code>$ sudo make install\n</code></pre> </li> <li> <p>(Enterprise only) For Enterprise Edition, set the value of <code>license_manager_url</code> to the host IP and port number <code>9119</code> where the license management tool is located in the Meta service configuration file of NebulaGraph (<code>nebula-metad.conf</code>), e.g. <code>192.168.8.100:9119</code>.    </p> </li> </ol> <p>Note</p> <p>The configuration files in the <code>etc/</code> directory (<code>/usr/local/nebula/etc</code> by default) are references. Users can create their own configuration files accordingly. If you want to use the scripts in the <code>script</code> directory to start, stop, restart, and kill the service, and check the service status, the configuration files have to be named as <code>nebula-graph.conf</code>, <code>nebula-metad.conf</code>, and <code>nebula-storaged.conf</code>.</p>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#update_the_master_branch","title":"Update the master branch","text":"<p>The source code of the master branch changes frequently. If the corresponding NebulaGraph release is installed, update it in the following steps.</p> <ol> <li> <p>In the <code>nebula</code> directory, run <code>git pull upstream master</code> to update the source code.</p> </li> <li> <p>In the <code>nebula/build</code> directory, run <code>make -j{N}</code> and <code>make install</code> again.</p> </li> </ol>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#next_to_do","title":"Next to do","text":"<p>Manage NebulaGraph services</p>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#cmake_variables","title":"CMake variables","text":""},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#usage_of_cmake_variables","title":"Usage of CMake variables","text":"<pre><code>$ cmake -D&lt;variable&gt;=&lt;value&gt; ...\n</code></pre> <p>The following CMake variables can be used at the configure (cmake) stage to adjust the compiling settings.</p>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#cmake_install_prefix","title":"CMAKE_INSTALL_PREFIX","text":"<p><code>CMAKE_INSTALL_PREFIX</code> specifies the path where the service modules, scripts, configuration files are installed. The default path is <code>/usr/local/nebula</code>.</p>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#enable_werror","title":"ENABLE_WERROR","text":"<p><code>ENABLE_WERROR</code> is <code>ON</code> by default and it makes all warnings into errors. You can set it to <code>OFF</code> if needed.</p>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#enable_testing","title":"ENABLE_TESTING","text":"<p><code>ENABLE_TESTING</code> is <code>ON</code> by default and unit tests are built with the NebulaGraph services. If you just need the service modules, set it to <code>OFF</code>.</p>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#enable_asan","title":"ENABLE_ASAN","text":"<p><code>ENABLE_ASAN</code> is <code>OFF</code> by default and the building of ASan (AddressSanitizer), a memory error detector, is disabled. To enable it, set <code>ENABLE_ASAN</code> to <code>ON</code>. This variable is intended for NebulaGraph developers.</p>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#cmake_build_type","title":"CMAKE_BUILD_TYPE","text":"<p>NebulaGraph supports the following building types of <code>MAKE_BUILD_TYPE</code>:</p> <ul> <li> <p><code>Debug</code></p> <p>The default value of <code>CMAKE_BUILD_TYPE</code>. It indicates building NebulaGraph with the debug info but not the optimization options.</p> </li> </ul> <ul> <li> <p><code>Release</code></p> <p>It indicates building NebulaGraph with the optimization options but not the debug info.</p> </li> </ul> <ul> <li> <p><code>RelWithDebInfo</code></p> <p>It indicates building NebulaGraph with the optimization options and the debug info.</p> </li> </ul> <ul> <li> <p><code>MinSizeRel</code></p> <p>It indicates building NebulaGraph with the optimization options for controlling the code size but not the debug info.</p> </li> </ul>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#enable_include_what_you_use","title":"ENABLE_INCLUDE_WHAT_YOU_USE","text":"<p><code>ENABLE_INCLUDE_WHAT_YOU_USE</code> is <code>OFF</code> by default. When set to <code>ON</code> and include-what-you-use is installed on the system, the system reports redundant headers contained in the project source code during makefile generation.</p>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#nebula_use_linker","title":"NEBULA_USE_LINKER","text":"<p>Specifies the program linker on the system. The available values are:</p> <ul> <li><code>bfd</code>, the default value, indicates that ld.bfd is applied as the linker.</li> <li><code>lld</code>, indicates that ld.lld, if installed on the system, is applied as the linker.</li> <li><code>gold</code>, indicates that ld.gold, if installed on the system, is applied as the linker.</li> </ul>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#cmake_c_compilercmake_cxx_compiler","title":"CMAKE_C_COMPILER/CMAKE_CXX_COMPILER","text":"<p>Usually, CMake locates and uses a C/C++ compiler installed in the host automatically. But if your compiler is not installed at the standard path, or if you want to use a different one, run the command as follows to specify the installation path of the target compiler:</p> <pre><code>$ cmake -DCMAKE_C_COMPILER=&lt;path_to_gcc/bin/gcc&gt; -DCMAKE_CXX_COMPILER=&lt;path_to_gcc/bin/g++&gt; ..\n$ cmake -DCMAKE_C_COMPILER=&lt;path_to_clang/bin/clang&gt; -DCMAKE_CXX_COMPILER=&lt;path_to_clang/bin/clang++&gt; ..\n</code></pre>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#enable_ccache","title":"ENABLE_CCACHE","text":"<p><code>ENABLE_CCACHE</code> is <code>ON</code> by default and Ccache (compiler cache) is used to speed up the compiling of NebulaGraph.</p> <p>To disable <code>ccache</code>, setting <code>ENABLE_CCACHE</code> to <code>OFF</code> is not enough. On some platforms, the <code>ccache</code> installation hooks up or precedes the compiler. In such a case, you have to set an environment variable <code>export CCACHE_DISABLE=true</code> or add a line <code>disable=true</code> in <code>~/.ccache/ccache.conf</code> as well. For more information, see the ccache official documentation.</p>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#nebula_thirdparty_root","title":"NEBULA_THIRDPARTY_ROOT","text":"<p><code>NEBULA_THIRDPARTY_ROOT</code> specifies the path where the third party software is installed. By default it is <code>/opt/vesoft/third-party</code>.</p>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#examine_problems","title":"Examine problems","text":"<p>If the compiling fails, we suggest you:</p> <ol> <li> <p>Check whether the operating system release meets the requirements and whether the memory and hard disk space are sufficient.</p> </li> <li> <p>Check whether the third-party is installed correctly.</p> </li> <li> <p>Use <code>make -j1</code> to reduce the compiling concurrency.</p> </li> </ol>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/2.install-nebula-graph-by-rpm-or-deb/","title":"Install NebulaGraph with RPM or DEB package","text":"<p>RPM and DEB are common package formats on Linux systems. This topic shows how to quickly install NebulaGraph with the RPM or DEB package.</p> <p>Note</p> <p>The console is not complied or packaged with NebulaGraph server binaries. You can install nebula-console by yourself.</p> <p>Enterpriseonly</p> <p>For NebulaGraph Enterprise, please contact us.</p>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/2.install-nebula-graph-by-rpm-or-deb/#prerequisites","title":"Prerequisites","text":"<ul> <li>The tool <code>wget</code> is installed.</li> </ul> <ul> <li>For NebulaGraph Enterprise, you must have the license key loaded in LM.</li> </ul>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/2.install-nebula-graph-by-rpm-or-deb/#step_1_download_the_package_from_cloud_service","title":"Step 1: Download the package from cloud service","text":"<p>Note</p> <p>NebulaGraph is currently only supported for installation on Linux systems, and only CentOS 7.x, CentOS 8.x, Ubuntu 16.04, Ubuntu 18.04, and Ubuntu 20.04 operating systems are supported. </p> <ul> <li>Download the released version.<p>URL:</p> <pre><code>//Centos 7\nhttps://oss-cdn.nebula-graph.io/package/&lt;release_version&gt;/nebula-graph-&lt;release_version&gt;.el7.x86_64.rpm\n\n//Centos 8\nhttps://oss-cdn.nebula-graph.io/package/&lt;release_version&gt;/nebula-graph-&lt;release_version&gt;.el8.x86_64.rpm\n\n//Ubuntu 1604\nhttps://oss-cdn.nebula-graph.io/package/&lt;release_version&gt;/nebula-graph-&lt;release_version&gt;.ubuntu1604.amd64.deb\n\n//Ubuntu 1804\nhttps://oss-cdn.nebula-graph.io/package/&lt;release_version&gt;/nebula-graph-&lt;release_version&gt;.ubuntu1804.amd64.deb\n\n//Ubuntu 2004\nhttps://oss-cdn.nebula-graph.io/package/&lt;release_version&gt;/nebula-graph-&lt;release_version&gt;.ubuntu2004.amd64.deb\n</code></pre> <p>For example, download the release package <code>3.5.0</code> for <code>Centos 7.5</code>:</p> <pre><code>wget https://oss-cdn.nebula-graph.io/package/3.5.0/nebula-graph-3.5.0.el7.x86_64.rpm\nwget https://oss-cdn.nebula-graph.io/package/3.5.0/nebula-graph-3.5.0.el7.x86_64.rpm.sha256sum.txt\n</code></pre> <p>Download the release package <code>3.5.0</code> for <code>Ubuntu 1804</code>:</p> <pre><code>wget https://oss-cdn.nebula-graph.io/package/3.5.0/nebula-graph-3.5.0.ubuntu1804.amd64.deb\nwget https://oss-cdn.nebula-graph.io/package/3.5.0/nebula-graph-3.5.0.ubuntu1804.amd64.deb.sha256sum.txt\n</code></pre> </li> </ul> <ul> <li> <p>Download the nightly version.</p> <p>Danger</p> <ul> <li>Nightly versions are usually used to test new features. Do not use it in a production environment.</li> <li>Nightly versions may not be built successfully every night. And the names may change from day to day.</li> </ul> <p>URL:</p> <pre><code>//Centos 7\nhttps://oss-cdn.nebula-graph.io/package/nightly/&lt;yyyy.mm.dd&gt;/nebula-graph-&lt;yyyy.mm.dd&gt;-nightly.el7.x86_64.rpm\n\n//Centos 8\nhttps://oss-cdn.nebula-graph.io/package/nightly/&lt;yyyy.mm.dd&gt;/nebula-graph-&lt;yyyy.mm.dd&gt;-nightly.el8.x86_64.rpm\n\n//Ubuntu 1604\nhttps://oss-cdn.nebula-graph.io/package/nightly/&lt;yyyy.mm.dd&gt;/nebula-graph-&lt;yyyy.mm.dd&gt;-nightly.ubuntu1604.amd64.deb\n\n//Ubuntu 1804\nhttps://oss-cdn.nebula-graph.io/package/nightly/&lt;yyyy.mm.dd&gt;/nebula-graph-&lt;yyyy.mm.dd&gt;-nightly.ubuntu1804.amd64.deb\n\n//Ubuntu 2004\nhttps://oss-cdn.nebula-graph.io/package/nightly/&lt;yyyy.mm.dd&gt;/nebula-graph-&lt;yyyy.mm.dd&gt;-nightly.ubuntu2004.amd64.deb\n</code></pre> <p>For example, download the <code>Centos 7.5</code> package developed and built in <code>2021.11.28</code>:</p> <pre><code>wget https://oss-cdn.nebula-graph.io/package/nightly/2021.11.28/nebula-graph-2021.11.28-nightly.el7.x86_64.rpm\nwget https://oss-cdn.nebula-graph.io/package/nightly/2021.11.28/nebula-graph-2021.11.28-nightly.el7.x86_64.rpm.sha256sum.txt\n</code></pre> <p>For example, download the <code>Ubuntu 1804</code> package developed and built in <code>2021.11.28</code>:</p> <pre><code>wget https://oss-cdn.nebula-graph.io/package/nightly/2021.11.28/nebula-graph-2021.11.28-nightly.ubuntu1804.amd64.deb\nwget https://oss-cdn.nebula-graph.io/package/nightly/2021.11.28/nebula-graph-2021.11.28-nightly.ubuntu1804.amd64.deb.sha256sum.txt\n</code></pre> </li> </ul>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/2.install-nebula-graph-by-rpm-or-deb/#step_2_install_nebulagraph","title":"Step 2: Install NebulaGraph","text":"<ul> <li> <p>Use the following syntax to install with an RPM package.</p> <pre><code>$ sudo rpm -ivh --prefix=&lt;installation_path&gt; &lt;package_name&gt;\n</code></pre> <p>The option <code>--prefix</code> indicates the installation path. The default path is <code>/usr/local/nebula/</code>.</p> <p>For example, to install an RPM package in the default path for the 3.5.0 version, run the following command.</p> <pre><code>sudo rpm -ivh nebula-graph-3.5.0.el7.x86_64.rpm\n</code></pre> </li> </ul> <ul> <li> <p>Use the following syntax to install with a DEB package.</p> <pre><code>$ sudo dpkg -i &lt;package_name&gt;\n</code></pre> <p>Note</p> <p>Customizing the installation path is not supported when installing NebulaGraph with a DEB package. The default installation path is <code>/usr/local/nebula/</code>.</p> <p>For example, to install a DEB package for the 3.5.0 version, run the following command.</p> <pre><code>sudo dpkg -i nebula-graph-3.5.0.ubuntu1804.amd64.deb\n</code></pre> <p>Note</p> <p>The default installation path is <code>/usr/local/nebula/</code>.</p> </li> </ul>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/2.install-nebula-graph-by-rpm-or-deb/#step_3_configure_the_address_of_the_license_manager","title":"Step 3: Configure the address of the License Manager","text":"<p>Enterpriseonly</p> <p>This step is required only for NebulaGraph Enterprise.</p> <p>In the Meta service configuration file (<code>nebula-metad.conf</code>) of NebulaGraph, set the value of <code>license_manager_url</code> to the host IP and port number <code>9119</code> where the License Manager (LM) is located, e.g. <code>192.168.8.100:9119</code>.</p>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/2.install-nebula-graph-by-rpm-or-deb/#next_to_do","title":"Next to do","text":"<ul> <li>Start NebulaGraph </li> </ul> <ul> <li>Connect to NebulaGraph</li> </ul>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/3.deploy-nebula-graph-with-docker-compose/","title":"Deploy NebulaGraph with Docker Compose","text":"<p>Using Docker Compose can quickly deploy NebulaGraph services based on the prepared configuration file. It is only recommended to use this method when testing functions of NebulaGraph.</p>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/3.deploy-nebula-graph-with-docker-compose/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>You have installed the following applications on your host.</p> Application Recommended version Official installation reference Docker Latest Install Docker Engine Docker Compose Latest Install Docker Compose Git Latest Download Git </li> </ul> <ul> <li>If you are deploying NebulaGraph as a non-root user, grant the user with Docker-related privileges. For detailed instructions, see Manage Docker as a non-root user.</li> </ul> <ul> <li>You have started the Docker service on your host.</li> </ul> <ul> <li>If you have already deployed another version of NebulaGraph with Docker Compose on your host, to avoid compatibility issues, you need to delete the <code>nebula-docker-compose/data</code> directory.</li> </ul> <ul> <li>For NebulaGraph Enterprise, you must have the license key loaded in LM.   </li> </ul>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/3.deploy-nebula-graph-with-docker-compose/#deploy_nebulagraph","title":"Deploy NebulaGraph","text":"<ol> <li> <p>Clone the <code>3.5.0</code> branch of the <code>nebula-docker-compose</code> repository to your host with Git.</p> <p>Danger</p> <p>The <code>master</code> branch contains the untested code for the latest NebulaGraph development release. DO NOT use this release in a production environment.</p> <pre><code>$ git clone -b release-3.5 https://github.com/vesoft-inc/nebula-docker-compose.git\n</code></pre> <p>Note</p> <p>The <code>x.y</code> version of Docker Compose aligns to the <code>x.y</code> version of NebulaGraph. For the NebulaGraph <code>z</code> version, Docker Compose does not publish the corresponding <code>z</code> version, but pulls the <code>z</code> version of the NebulaGraph image.</p> <p>Note</p> <p>For installation of the NebulaGraph enterprise version, contact us.</p> </li> <li> <p>Go to the <code>nebula-docker-compose</code> directory.</p> <pre><code>$ cd nebula-docker-compose/\n</code></pre> </li> <li> <p>Configure License Manager address.</p> <p>Enterpriseonly</p> <p>Skip this step if you are using the community version.</p> <ol> <li> <p>Edit the <code>docker-compose.yml</code> file.</p> <pre><code>$ cd nebula-docker-compose/\n$ vim docker-compose.yml\n</code></pre> </li> <li> <p>Add the <code>license_manager_url</code> field under all <code>services.metad{number}.command</code> and set its value to the access address of LM.</p> <pre><code>...\nservices:\n  metad0:\n    command:\n      - --license_manager_url=&lt;LM_ADDR&gt;:&lt;LM_PORT&gt; // &lt;LM_ADDR&gt; is the address of the LM service, and &lt;LM_PORT&gt; is the port of the LM service, which is 9119 by default.\n  metad1:\n    command:\n      - --license_manager_url=&lt;LM_ADDR&gt;:&lt;LM_PORT&gt;\n  ...\n</code></pre> </li> <li> <p>Save and exit.</p> </li> </ol> </li> <li> <p>Run the following command to start all the NebulaGraph services.</p> <p>Note</p> <ul> <li>Update the NebulaGraph images and NebulaGraph Console images first if they are out of date.</li> <li>The return result after executing the command varies depending on the installation directory.</li> </ul> <pre><code>[nebula-docker-compose]$ docker-compose up -d\nCreating nebuladockercompose_metad0_1 ... done\nCreating nebuladockercompose_metad2_1 ... done\nCreating nebuladockercompose_metad1_1 ... done\nCreating nebuladockercompose_graphd2_1   ... done\nCreating nebuladockercompose_graphd_1    ... done\nCreating nebuladockercompose_graphd1_1   ... done\nCreating nebuladockercompose_storaged0_1 ... done\nCreating nebuladockercompose_storaged2_1 ... done\nCreating nebuladockercompose_storaged1_1 ... done\n</code></pre> <p>Compatibility</p> <p>Starting from NebulaGraph version 3.1.0, nebula-docker-compose automatically starts a NebulaGraph Console docker container and adds the storage host to the cluster (i.e. <code>ADD HOSTS</code> command).</p> <p>Note</p> <p>For more information of the preceding services, see NebulaGraph architecture.</p> </li> </ol>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/3.deploy-nebula-graph-with-docker-compose/#connect_to_nebulagraph","title":"Connect to NebulaGraph","text":"<p>There are two ways to connect to NebulaGraph:</p> <ul> <li>Connected with Nebula Console outside the container. Because the external mapping port for the Graph service is also fixed as <code>9669</code> in the container's configuration file, you can connect directly through the default port. For details, see Connect to NebulaGraph.</li> </ul> <ul> <li>Log into the container installed NebulaGraph Console, then connect to the Graph service. This section describes this approach.</li> </ul> <ol> <li> <p>Run the following command to view the name of NebulaGraph Console docker container.</p> <pre><code>$ docker-compose ps\n          Name                         Command             State                 Ports\n--------------------------------------------------------------------------------------------\nnebuladockercompose_console_1     sh -c sleep 3 &amp;&amp;          Up\n                                  nebula-co ...\n......\n</code></pre> </li> <li> <p>Run the following command to enter the NebulaGraph Console docker container.</p> <pre><code>docker exec -it nebuladockercompose_console_1 /bin/sh\n/ #\n</code></pre> </li> <li> <p>Connect to NebulaGraph with NebulaGraph Console.</p> <pre><code>/ # ./usr/local/bin/nebula-console -u &lt;user_name&gt; -p &lt;password&gt; --address=graphd --port=9669\n</code></pre> <p>Note</p> <p>By default, the authentication is off, you can only log in with an existing username (the default is <code>root</code>) and any password. To turn it on, see Enable authentication.</p> </li> <li> <p>Run the following commands to view the cluster state.</p> <pre><code>nebula&gt; SHOW HOSTS;\n+-------------+------+----------+--------------+----------------------+------------------------+---------+\n| Host        | Port | Status   | Leader count | Leader distribution  | Partition distribution | Version |\n+-------------+------+----------+--------------+----------------------+------------------------+---------+\n| \"storaged0\" | 9779 | \"ONLINE\" | 0            | \"No valid partition\" | \"No valid partition\"   | \"3.5.0\" |\n| \"storaged1\" | 9779 | \"ONLINE\" | 0            | \"No valid partition\" | \"No valid partition\"   | \"3.5.0\" |\n| \"storaged2\" | 9779 | \"ONLINE\" | 0            | \"No valid partition\" | \"No valid partition\"   | \"3.5.0\" |\n+-------------+------+----------+--------------+----------------------+------------------------+---------+\n</code></pre> </li> </ol> <p>Run <code>exit</code> twice to switch back to your terminal (shell).</p>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/3.deploy-nebula-graph-with-docker-compose/#check_the_nebulagraph_service_status_and_ports","title":"Check the NebulaGraph service status and ports","text":"<p>Run <code>docker-compose ps</code> to list all the services of NebulaGraph and their status and ports.</p> <p>Note</p> <p>NebulaGraph provides services to the clients through port <code>9669</code> by default. To use other ports, modify the <code>docker-compose.yaml</code> file in the <code>nebula-docker-compose</code> directory and restart the NebulaGraph services.</p> <pre><code>$ docker-compose ps\nnebuladockercompose_console_1     sh -c sleep 3 &amp;&amp;                 Up\n                                  nebula-co ...\nnebuladockercompose_graphd1_1     /usr/local/nebula/bin/nebu ...   Up      0.0.0.0:49174-&gt;19669/tcp,:::49174-&gt;19669/tcp, 0.0.0.0:49171-&gt;19670/tcp,:::49171-&gt;19670/tcp, 0.0.0.0:49177-&gt;9669/tcp,:::49177-&gt;9669/tcp\nnebuladockercompose_graphd2_1     /usr/local/nebula/bin/nebu ...   Up      0.0.0.0:49175-&gt;19669/tcp,:::49175-&gt;19669/tcp, 0.0.0.0:49172-&gt;19670/tcp,:::49172-&gt;19670/tcp, 0.0.0.0:49178-&gt;9669/tcp,:::49178-&gt;9669/tcp\nnebuladockercompose_graphd_1      /usr/local/nebula/bin/nebu ...   Up      0.0.0.0:49180-&gt;19669/tcp,:::49180-&gt;19669/tcp, 0.0.0.0:49179-&gt;19670/tcp,:::49179-&gt;19670/tcp, 0.0.0.0:9669-&gt;9669/tcp,:::9669-&gt;9669/tcp\nnebuladockercompose_metad0_1      /usr/local/nebula/bin/nebu ...   Up      0.0.0.0:49157-&gt;19559/tcp,:::49157-&gt;19559/tcp, 0.0.0.0:49154-&gt;19560/tcp,:::49154-&gt;19560/tcp, 0.0.0.0:49160-&gt;9559/tcp,:::49160-&gt;9559/tcp, 9560/tcp\nnebuladockercompose_metad1_1      /usr/local/nebula/bin/nebu ...   Up      0.0.0.0:49156-&gt;19559/tcp,:::49156-&gt;19559/tcp, 0.0.0.0:49153-&gt;19560/tcp,:::49153-&gt;19560/tcp, 0.0.0.0:49159-&gt;9559/tcp,:::49159-&gt;9559/tcp, 9560/tcp\nnebuladockercompose_metad2_1      /usr/local/nebula/bin/nebu ...   Up      0.0.0.0:49158-&gt;19559/tcp,:::49158-&gt;19559/tcp, 0.0.0.0:49155-&gt;19560/tcp,:::49155-&gt;19560/tcp, 0.0.0.0:49161-&gt;9559/tcp,:::49161-&gt;9559/tcp, 9560/tcp\nnebuladockercompose_storaged0_1   /usr/local/nebula/bin/nebu ...   Up      0.0.0.0:49166-&gt;19779/tcp,:::49166-&gt;19779/tcp, 0.0.0.0:49163-&gt;19780/tcp,:::49163-&gt;19780/tcp, 9777/tcp, 9778/tcp, 0.0.0.0:49169-&gt;9779/tcp,:::49169-&gt;9779/tcp, 9780/tcp\nnebuladockercompose_storaged1_1   /usr/local/nebula/bin/nebu ...   Up      0.0.0.0:49165-&gt;19779/tcp,:::49165-&gt;19779/tcp, 0.0.0.0:49162-&gt;19780/tcp,:::49162-&gt;19780/tcp, 9777/tcp, 9778/tcp, 0.0.0.0:49168-&gt;9779/tcp,:::49168-&gt;9779/tcp, 9780/tcp\nnebuladockercompose_storaged2_1   /usr/local/nebula/bin/nebu ...   Up      0.0.0.0:49167-&gt;19779/tcp,:::49167-&gt;19779/tcp, 0.0.0.0:49164-&gt;19780/tcp,:::49164-&gt;19780/tcp, 9777/tcp, 9778/tcp, 0.0.0.0:49170-&gt;9779/tcp,:::49170-&gt;9779/tcp, 9780/tcp\n</code></pre> <p>If the service is abnormal, you can first confirm the abnormal container name (such as <code>nebuladockercompose_graphd2_1</code>).</p> <p>Then you can execute <code>docker ps</code> to view the corresponding <code>CONTAINER ID</code> (such as <code>2a6c56c405f5</code>).</p> <pre><code>[nebula-docker-compose]$ docker ps\nCONTAINER ID   IMAGE                               COMMAND                  CREATED          STATUS                    PORTS                                                                                                  NAMES\n2a6c56c405f5   vesoft/nebula-graphd:nightly     \"/usr/local/nebula/b\u2026\"   36 minutes ago   Up 36 minutes (healthy)   0.0.0.0:49230-&gt;9669/tcp, 0.0.0.0:49229-&gt;19669/tcp, 0.0.0.0:49228-&gt;19670/tcp                            nebuladockercompose_graphd2_1\n7042e0a8e83d   vesoft/nebula-storaged:nightly   \"./bin/nebula-storag\u2026\"   36 minutes ago   Up 36 minutes (healthy)   9777-9778/tcp, 9780/tcp, 0.0.0.0:49227-&gt;9779/tcp, 0.0.0.0:49226-&gt;19779/tcp, 0.0.0.0:49225-&gt;19780/tcp   nebuladockercompose_storaged2_1\n18e3ea63ad65   vesoft/nebula-storaged:nightly   \"./bin/nebula-storag\u2026\"   36 minutes ago   Up 36 minutes (healthy)   9777-9778/tcp, 9780/tcp, 0.0.0.0:49219-&gt;9779/tcp, 0.0.0.0:49218-&gt;19779/tcp, 0.0.0.0:49217-&gt;19780/tcp   nebuladockercompose_storaged0_1\n4dcabfe8677a   vesoft/nebula-graphd:nightly     \"/usr/local/nebula/b\u2026\"   36 minutes ago   Up 36 minutes (healthy)   0.0.0.0:49224-&gt;9669/tcp, 0.0.0.0:49223-&gt;19669/tcp, 0.0.0.0:49222-&gt;19670/tcp                            nebuladockercompose_graphd1_1\na74054c6ae25   vesoft/nebula-graphd:nightly     \"/usr/local/nebula/b\u2026\"   36 minutes ago   Up 36 minutes (healthy)   0.0.0.0:9669-&gt;9669/tcp, 0.0.0.0:49221-&gt;19669/tcp, 0.0.0.0:49220-&gt;19670/tcp                             nebuladockercompose_graphd_1\n880025a3858c   vesoft/nebula-storaged:nightly   \"./bin/nebula-storag\u2026\"   36 minutes ago   Up 36 minutes (healthy)   9777-9778/tcp, 9780/tcp, 0.0.0.0:49216-&gt;9779/tcp, 0.0.0.0:49215-&gt;19779/tcp, 0.0.0.0:49214-&gt;19780/tcp   nebuladockercompose_storaged1_1\n45736a32a23a   vesoft/nebula-metad:nightly      \"./bin/nebula-metad \u2026\"   36 minutes ago   Up 36 minutes (healthy)   9560/tcp, 0.0.0.0:49213-&gt;9559/tcp, 0.0.0.0:49212-&gt;19559/tcp, 0.0.0.0:49211-&gt;19560/tcp                  nebuladockercompose_metad0_1\n3b2c90eb073e   vesoft/nebula-metad:nightly      \"./bin/nebula-metad \u2026\"   36 minutes ago   Up 36 minutes (healthy)   9560/tcp, 0.0.0.0:49207-&gt;9559/tcp, 0.0.0.0:49206-&gt;19559/tcp, 0.0.0.0:49205-&gt;19560/tcp                  nebuladockercompose_metad2_1\n7bb31b7a5b3f   vesoft/nebula-metad:nightly      \"./bin/nebula-metad \u2026\"   36 minutes ago   Up 36 minutes (healthy)   9560/tcp, 0.0.0.0:49210-&gt;9559/tcp, 0.0.0.0:49209-&gt;19559/tcp, 0.0.0.0:49208-&gt;19560/tcp                  nebuladockercompose_metad1_1\n</code></pre> <p>Use the <code>CONTAINER ID</code> to log in the container and troubleshoot.</p> <pre><code>nebula-docker-compose]$ docker exec -it 2a6c56c405f5 bash\n[root@2a6c56c405f5 nebula]#\n</code></pre>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/3.deploy-nebula-graph-with-docker-compose/#check_the_service_data_and_logs","title":"Check the service data and logs","text":"<p>All the data and logs of NebulaGraph are stored persistently in the <code>nebula-docker-compose/data</code> and <code>nebula-docker-compose/logs</code> directories.</p> <p>The structure of the directories is as follows:</p> <pre><code>nebula-docker-compose/\n  |-- docker-compose.yaml\n  \u251c\u2500\u2500 data\n  \u2502\u00a0\u00a0 \u251c\u2500\u2500 meta0\n  \u2502\u00a0\u00a0 \u251c\u2500\u2500 meta1\n  \u2502\u00a0\u00a0 \u251c\u2500\u2500 meta2\n  \u2502\u00a0\u00a0 \u251c\u2500\u2500 storage0\n  \u2502\u00a0\u00a0 \u251c\u2500\u2500 storage1\n  \u2502\u00a0\u00a0 \u2514\u2500\u2500 storage2\n  \u2514\u2500\u2500 logs\n      \u251c\u2500\u2500 graph\n      \u251c\u2500\u2500 graph1\n      \u251c\u2500\u2500 graph2\n      \u251c\u2500\u2500 meta0\n      \u251c\u2500\u2500 meta1\n      \u251c\u2500\u2500 meta2\n      \u251c\u2500\u2500 storage0\n      \u251c\u2500\u2500 storage1\n      \u2514\u2500\u2500 storage2\n</code></pre>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/3.deploy-nebula-graph-with-docker-compose/#stop_the_nebulagraph_services","title":"Stop the NebulaGraph services","text":"<p>You can run the following command to stop the NebulaGraph services:</p> <pre><code>$ docker-compose down\n</code></pre> <p>The following information indicates you have successfully stopped the NebulaGraph services:</p> <pre><code>Stopping nebuladockercompose_console_1   ... done\nStopping nebuladockercompose_graphd1_1   ... done\nStopping nebuladockercompose_graphd_1    ... done\nStopping nebuladockercompose_graphd2_1   ... done\nStopping nebuladockercompose_storaged1_1 ... done\nStopping nebuladockercompose_storaged0_1 ... done\nStopping nebuladockercompose_storaged2_1 ... done\nStopping nebuladockercompose_metad2_1    ... done\nStopping nebuladockercompose_metad0_1    ... done\nStopping nebuladockercompose_metad1_1    ... done\nRemoving nebuladockercompose_console_1   ... done\nRemoving nebuladockercompose_graphd1_1   ... done\nRemoving nebuladockercompose_graphd_1    ... done\nRemoving nebuladockercompose_graphd2_1   ... done\nRemoving nebuladockercompose_storaged1_1 ... done\nRemoving nebuladockercompose_storaged0_1 ... done\nRemoving nebuladockercompose_storaged2_1 ... done\nRemoving nebuladockercompose_metad2_1    ... done\nRemoving nebuladockercompose_metad0_1    ... done\nRemoving nebuladockercompose_metad1_1    ... done\nRemoving network nebuladockercompose_nebula-net\n</code></pre> <p>Danger</p> <p>The parameter <code>-v</code> in the command <code>docker-compose down -v</code> will delete all your local NebulaGraph storage data. Try this command if you are using the nightly release and having some compatibility issues.</p>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/3.deploy-nebula-graph-with-docker-compose/#modify_configurations","title":"Modify configurations","text":"<p>The configuration file of NebulaGraph deployed by Docker Compose is <code>nebula-docker-compose/docker-compose.yaml</code>. To make the new configuration take effect, modify the configuration in this file and restart the service.</p> <p>For more instructions, see Configurations.</p>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/3.deploy-nebula-graph-with-docker-compose/#faq","title":"FAQ","text":""},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/3.deploy-nebula-graph-with-docker-compose/#how_to_fix_the_docker_mapping_to_external_ports","title":"How to fix the docker mapping to external ports?","text":"<p>To set the <code>ports</code> of corresponding services as fixed mapping, modify the <code>docker-compose.yaml</code> in the <code>nebula-docker-compose</code> directory. For example:</p> <pre><code>graphd:\n    image: vesoft/nebula-graphd:release-3.5\n    ...\n    ports:\n      - 9669:9669\n      - 19669\n      - 19670\n</code></pre> <p><code>9669:9669</code> indicates the internal port 9669 is uniformly mapped to external ports, while <code>19669</code> indicates the internal port 19669 is randomly mapped to external ports.</p>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/3.deploy-nebula-graph-with-docker-compose/#how_to_upgrade_or_update_the_docker_images_of_nebulagraph_services","title":"How to upgrade or update the docker images of NebulaGraph services","text":"<ol> <li> <p>In the <code>nebula-docker-compose/docker-compose.yaml</code> file, change all the <code>image</code> values to the required image version.</p> </li> <li> <p>In the <code>nebula-docker-compose</code> directory, run <code>docker-compose pull</code> to update the images of the Graph Service, Storage Service, Meta Service, and NebulaGraph Console.</p> </li> <li> <p>Run <code>docker-compose up -d</code> to start the NebulaGraph services again.</p> </li> <li> <p>After connecting to NebulaGraph with NebulaGraph Console, run <code>SHOW HOSTS GRAPH</code>, <code>SHOW HOSTS STORAGE</code>, or <code>SHOW HOSTS META</code> to check the version of the responding service respectively.</p> </li> </ol>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/3.deploy-nebula-graph-with-docker-compose/#error_toomanyrequests_when_docker-compose_pull","title":"<code>ERROR: toomanyrequests</code> when <code>docker-compose pull</code>","text":"<p>You may meet the following error.</p> <p><code>ERROR: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit</code>.</p> <p>You have met the rate limit of Docker Hub. Learn more on Understanding Docker Hub Rate Limiting.</p>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/3.deploy-nebula-graph-with-docker-compose/#how_to_update_the_nebulagraph_console_client","title":"How to update the NebulaGraph Console client","text":"<p>The command <code>docker-compose pull</code> updates both the NebulaGraph services and the NebulaGraph Console.</p>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/3.deploy-nebula-graph-with-docker-compose/#related_documents","title":"Related documents","text":"<ul> <li>Install and deploy NebulaGraph with the source code</li> <li>Install NebulaGraph by RPM or DEB</li> <li>Connect to NebulaGraph</li> </ul>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/4.install-nebula-graph-from-tar/","title":"Install NebulaGraph graph with the tar.gz file","text":"<p>You can install NebulaGraph by downloading the tar.gz file.</p> <p>Note</p> <ul> <li>NebulaGraph provides installing with the tar.gz file starting from version 2.6.0.</li> </ul> <ul> <li>NebulaGraph is currently only supported for installation on Linux systems, and only CentOS 7.x, CentOS 8.x, Ubuntu 16.04, Ubuntu 18.04, and Ubuntu 20.04 operating systems are supported. </li> </ul>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/4.install-nebula-graph-from-tar/#prerequisites","title":"Prerequisites","text":"<p>For NebulaGraph Enterprise, you must have the license key loaded in LM.</p>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/4.install-nebula-graph-from-tar/#installation_steps","title":"Installation steps","text":"<ol> <li> <p>Download the NebulaGraph tar.gz file using the following address.</p> <p>Before downloading, you need to replace <code>&lt;release_version&gt;</code> with the version you want to download.</p> <pre><code>//Centos 7\nhttps://oss-cdn.nebula-graph.com.cn/package/&lt;release_version&gt;/nebula-graph-&lt;release_version&gt;.el7.x86_64.tar.gz\n//Checksum\nhttps://oss-cdn.nebula-graph.com.cn/package/&lt;release_version&gt;/nebula-graph-&lt;release_version&gt;.el7.x86_64.tar.gz.sha256sum.txt\n\n//Centos 8\nhttps://oss-cdn.nebula-graph.com.cn/package/&lt;release_version&gt;/nebula-graph-&lt;release_version&gt;.el8.x86_64.tar.gz\n//Checksum\nhttps://oss-cdn.nebula-graph.com.cn/package/&lt;release_version&gt;/nebula-graph-&lt;release_version&gt;.el8.x86_64.tar.gz.sha256sum.txt\n\n//Ubuntu 1604\nhttps://oss-cdn.nebula-graph.com.cn/package/&lt;release_version&gt;/nebula-graph-&lt;release_version&gt;.ubuntu1604.amd64.tar.gz\n//Checksum\nhttps://oss-cdn.nebula-graph.com.cn/package/&lt;release_version&gt;/nebula-graph-&lt;release_version&gt;.ubuntu1604.amd64.tar.gz.sha256sum.txt\n\n//Ubuntu 1804\nhttps://oss-cdn.nebula-graph.com.cn/package/&lt;release_version&gt;/nebula-graph-&lt;release_version&gt;.ubuntu1804.amd64.tar.gz\n//Checksum\nhttps://oss-cdn.nebula-graph.com.cn/package/&lt;release_version&gt;/nebula-graph-&lt;release_version&gt;.ubuntu1804.amd64.tar.gz.sha256sum.txt\n\n//Ubuntu 2004\nhttps://oss-cdn.nebula-graph.com.cn/package/&lt;release_version&gt;/nebula-graph-&lt;release_version&gt;.ubuntu2004.amd64.tar.gz\n//Checksum\nhttps://oss-cdn.nebula-graph.com.cn/package/&lt;release_version&gt;/nebula-graph-&lt;release_version&gt;.ubuntu2004.amd64.tar.gz.sha256sum.txt\n</code></pre> <p>For example, to download the NebulaGraph release-3.5 tar.gz file for <code>CentOS 7.5</code>, run the following command:</p> <pre><code>wget https://oss-cdn.nebula-graph.com.cn/package/3.5.0/nebula-graph-3.5.0.el7.x86_64.tar.gz\n</code></pre> </li> <li> <p>Decompress the tar.gz file to the NebulaGraph installation directory.</p> <pre><code>tar -xvzf &lt;tar.gz_file_name&gt; -C &lt;install_path&gt;\n</code></pre> <ul> <li><code>tar.gz_file_name</code> specifies the name of the tar.gz file.</li> <li><code>install_path</code> specifies the installation path.</li> </ul> <p>For example:</p> <pre><code>tar -xvzf nebula-graph-3.5.0.el7.x86_64.tar.gz -C /home/joe/nebula/install\n</code></pre> </li> <li> <p>Modify the name of the configuration file.</p> <p>Enter the decompressed directory, rename the files <code>nebula-graphd.conf.default</code>, <code>nebula-metad.conf.default</code>, and <code>nebula-storaged.conf.default</code> in the subdirectory <code>etc</code>, and delete <code>.default</code> to apply the default configuration of NebulaGraph.</p> </li> <li> <p>(Enterprise only) For Enterprise Edition, set the value of <code>license_manager_url</code> to the host IP and port number <code>9119</code> where the license management tool is located in the Meta service configuration file of NebulaGraph (<code>nebula-metad.conf</code>), e.g. <code>192.168.8.100:9119</code>.    </p> </li> </ol> <p>Note</p> <p>To modify the configuration, see Configurations.</p> <p>So far, you have installed NebulaGraph successfully.</p>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/4.install-nebula-graph-from-tar/#next_to_do","title":"Next to do","text":"<p>Manage NebulaGraph services</p>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/6.deploy-nebula-graph-with-peripherals/","title":"Install NebulaGraph with ecosystem tools","text":"<p>You can install the Enterprise Edition and Community Edition of NebulaGraph with the following ecosystem tools:</p> <ul> <li>NebulaGraph Dashboard Enterprise Edition </li> </ul> <ul> <li>NebulaGraph Operator</li> </ul>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/6.deploy-nebula-graph-with-peripherals/#installation_details","title":"Installation details","text":"<ul> <li>To install NebulaGraph with NebulaGraph Dashboard Enterprise Edition, see Create a cluster.</li> </ul> <ul> <li>To install NebulaGraph with NebulaGraph Operator, see Deploy NebulaGraph clusters with Kubectl or Deploy NebulaGraph clusters with Helm.</li> </ul> <p>Note</p> <p>Contact our sales (inqury@vesoft.com) to get the installation package for the Enterprise Edition of NebulaGraph.</p>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/7.compile-using-docker/","title":"Compile NebulaGraph using Docker","text":"<p>NebulaGraph's source code is written in C++. Compiling NebulaGraph requires certain dependencies which might conflict with host system dependencies, potentially causing compilation failures. Docker offers a solution to this. NebulaGraph provides a Docker image containing the complete compilation environment, ensuring an efficient build process and avoiding host OS conflicts. This guide outlines the steps to compile NebulaGraph using Docker.</p>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/7.compile-using-docker/#prerequisites","title":"Prerequisites","text":"<p>Before you begin:</p> <ol> <li> <p>Docker: Ensure Docker is installed on your system.</p> </li> <li> <p>Clone NebulaGraph's Source Code: Clone the repository locally using:</p> <pre><code>git clone --branch release-3.5 https://github.com/vesoft-inc/nebula.git\n</code></pre> <p>This clones the NebulaGraph source code to a subdirectory named <code>nebula</code>.</p> </li> </ol>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/7.compile-using-docker/#compilation_steps","title":"Compilation steps","text":"<ol> <li> <p>Pull the NebulaGraph compilation image.</p> <pre><code>docker pull vesoft/nebula-dev:ubuntu2004\n</code></pre> <p>Here, we use the official NebulaGraph compilation image, <code>ubuntu2004</code>. For different versions, see nebula-dev-docker.</p> </li> <li> <p>Start the compilation container.</p> <pre><code>docker run -ti \\\n  --security-opt seccomp=unconfined \\\n  -v \"$PWD\":/home \\\n  -w /home \\\n  --name nebula_dev \\\n  vesoft/nebula-dev:ubuntu2004 \\\n  bash\n</code></pre> <ul> <li><code>--security-opt seccomp=unconfined</code>: Disables the seccomp security mechanism to avoid compilation errors.</li> </ul> <ul> <li><code>-v \"$PWD\":/home</code>: Mounts the local path of the NebulaGraph code to the container's <code>/home</code> directory.</li> </ul> <ul> <li><code>-w /home</code>: Sets the container's working directory to <code>/home</code>. Any command run inside the container will use this directory as the current directory.</li> </ul> <ul> <li><code>--name nebula_dev</code>: Assigns a name to the container, making it easier to manage and operate.</li> </ul> <ul> <li><code>vesoft/nebula-dev:ubuntu2004</code>: Uses the <code>ubuntu2004</code> version of the <code>vesoft/nebula-dev</code> compilation image.</li> </ul> <ul> <li><code>bash</code>: Executes the <code>bash</code> command inside the container, entering the container's interactive terminal.</li> </ul> <p>After executing this command, you'll enter an interactive terminal inside the container. To re-enter the container, use <code>docker exec -ti nebula_dev bash</code>.</p> </li> <li> <p>Compile NebulaGraph inside the container.</p> <ol> <li> <p>Enter the NebulaGraph source code directory.</p> <pre><code>cd nebula\n</code></pre> </li> <li> <p>Create a build directory and enter it.</p> <pre><code>mkdir build &amp;&amp; cd build\n</code></pre> </li> <li> <p>Use CMake to generate the Makefile.</p> <p><pre><code>cmake -DCMAKE_CXX_COMPILER=$TOOLSET_CLANG_DIR/bin/g++ -DCMAKE_C_COMPILER=$TOOLSET_CLANG_DIR/bin/gcc -DENABLE_WERROR=OFF -DCMAKE_BUILD_TYPE=Debug -DENABLE_TESTING=OFF ..\n</code></pre> For more on CMake, see CMake Parameters.</p> </li> <li> <p>Compile NebulaGraph.</p> <pre><code># The -j parameter specifies the number of threads to use.\n# If you have a multi-core CPU, you can use more threads to speed up compilation.\nmake -j2\n</code></pre> <p>Compilation might take some time based on your system performance. </p> </li> </ol> </li> <li> <p>Install the Executables and Libraries.</p> <p>Post successful compilation, NebulaGraph's binaries and libraries are located in <code>/home/nebula/build</code>. Install them to <code>/usr/local/nebula</code>:</p> <pre><code>make install\n</code></pre> </li> </ol> <p>Once completed, NebulaGraph is compiled and installed in the host directory <code>/usr/local/nebula</code>.</p>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/7.compile-using-docker/#next_steps","title":"Next Steps","text":"<ul> <li>Start NebulaGraph Service</li> <li>Connect to NebulaGraph</li> </ul>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/deploy-nebula-graph-cluster/","title":"Deploy a NebulaGraph cluster with RPM/DEB package on multiple servers","text":"<p>For now, NebulaGraph does not provide an official deployment tool. Users can deploy a NebulaGraph cluster with RPM or DEB package manually. This topic provides an example of deploying a NebulaGraph cluster on multiple servers (machines).</p>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/deploy-nebula-graph-cluster/#deployment","title":"Deployment","text":"Machine name IP address Number of graphd Number of storaged Number of metad A 192.168.10.111 1 1 1 B 192.168.10.112 1 1 1 C 192.168.10.113 1 1 1 D 192.168.10.114 1 1 None E 192.168.10.115 1 1 None"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/deploy-nebula-graph-cluster/#prerequisites","title":"Prerequisites","text":"<ul> <li>Prepare 5 machines for deploying the cluster.</li> <li>Use the NTP service to synchronize time in the cluster.</li> </ul> <ul> <li>For NebulaGraph Enterprise, you must have the license key loaded in LM.</li> </ul>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/deploy-nebula-graph-cluster/#manual_deployment_process","title":"Manual deployment process","text":""},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/deploy-nebula-graph-cluster/#install_nebulagraph","title":"Install NebulaGraph","text":"<p>Install NebulaGraph on each machine in the cluster. Available approaches of installation are as follows.</p> <ul> <li>Install NebulaGraph with RPM or DEB package</li> </ul> <ul> <li>Install NebulaGraph by compiling the source code</li> </ul>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/deploy-nebula-graph-cluster/#modify_the_configurations","title":"Modify the configurations","text":"<p>To deploy NebulaGraph according to your requirements, you have to modify the configuration files.</p> <p>All the configuration files for NebulaGraph, including <code>nebula-graphd.conf</code>, <code>nebula-metad.conf</code>, and <code>nebula-storaged.conf</code>, are stored in the <code>etc</code> directory in the installation path. You only need to modify the configuration for the corresponding service on the machines. The configurations that need to be modified for each machine are as follows.</p> Machine name The configuration to be modified A <code>nebula-graphd.conf</code>, <code>nebula-storaged.conf</code>, <code>nebula-metad.conf</code> B <code>nebula-graphd.conf</code>, <code>nebula-storaged.conf</code>, <code>nebula-metad.conf</code> C <code>nebula-graphd.conf</code>, <code>nebula-storaged.conf</code>, <code>nebula-metad.conf</code> D <code>nebula-graphd.conf</code>, <code>nebula-storaged.conf</code> E <code>nebula-graphd.conf</code>, <code>nebula-storaged.conf</code> <p>Users can refer to the content of the following configurations, which only show part of the cluster settings. The hidden content uses the default setting so that users can better understand the relationship between the servers in the NebulaGraph cluster.</p> <p>Note</p> <p>The main configuration to be modified is <code>meta_server_addrs</code>. All configurations need to fill in the IP addresses and ports of all Meta services. At the same time, <code>local_ip</code> needs to be modified as the network IP address of the machine itself. For detailed descriptions of the configuration parameters, see:</p> <ul> <li>Meta Service configurations</li> </ul> <ul> <li>Graph Service configurations</li> </ul> <ul> <li>Storage Service configurations</li> </ul> <p>Enterpriseonly</p> <p>For Enterprise Edition, set the value of <code>license_manager_url</code> to the host IP and port number <code>9119</code> where the license management tool is located in the Meta service configuration files of NebulaGraph (<code>nebula-metad.conf</code>), e.g. <code>192.168.8.100:9119</code>.</p> <ul> <li> <p>Deploy machine A</p> <ul> <li> <p><code>nebula-graphd.conf</code></p> <pre><code>########## networking ##########\n# Comma separated Meta Server Addresses\n--meta_server_addrs=192.168.10.111:9559,192.168.10.112:9559,192.168.10.113:9559\n# Local IP used to identify the nebula-graphd process.\n# Change it to an address other than loopback if the service is distributed or\n# will be accessed remotely.\n--local_ip=192.168.10.111\n# Network device to listen on\n--listen_netdev=any\n# Port to listen on\n--port=9669\n</code></pre> </li> </ul> <ul> <li> <p><code>nebula-storaged.conf</code></p> <pre><code>########## networking ##########\n# Comma separated Meta server addresses\n--meta_server_addrs=192.168.10.111:9559,192.168.10.112:9559,192.168.10.113:9559\n# Local IP used to identify the nebula-storaged process.\n# Change it to an address other than loopback if the service is distributed or\n# will be accessed remotely.\n--local_ip=192.168.10.111\n# Storage daemon listening port\n--port=9779\n</code></pre> </li> </ul> <ul> <li> <p><code>nebula-metad.conf</code></p> <pre><code>########## networking ##########\n# Comma separated Meta Server addresses\n--meta_server_addrs=192.168.10.111:9559,192.168.10.112:9559,192.168.10.113:9559\n# Local IP used to identify the nebula-metad process.\n# Change it to an address other than loopback if the service is distributed or\n# will be accessed remotely.\n--local_ip=192.168.10.111\n# Meta daemon listening port\n--port=9559\n</code></pre> </li> </ul> </li> </ul> <ul> <li> <p>Deploy machine B</p> <ul> <li> <p><code>nebula-graphd.conf</code></p> <pre><code>########## networking ##########\n# Comma separated Meta Server Addresses\n--meta_server_addrs=192.168.10.111:9559,192.168.10.112:9559,192.168.10.113:9559\n# Local IP used to identify the nebula-graphd process.\n# Change it to an address other than loopback if the service is distributed or\n# will be accessed remotely.\n--local_ip=192.168.10.112\n# Network device to listen on\n--listen_netdev=any\n# Port to listen on\n--port=9669\n</code></pre> </li> </ul> <ul> <li> <p><code>nebula-storaged.conf</code></p> <pre><code>########## networking ##########\n# Comma separated Meta server addresses\n--meta_server_addrs=192.168.10.111:9559,192.168.10.112:9559,192.168.10.113:9559\n# Local IP used to identify the nebula-storaged process.\n# Change it to an address other than loopback if the service is distributed or\n# will be accessed remotely.\n--local_ip=192.168.10.112\n# Storage daemon listening port\n--port=9779\n</code></pre> </li> </ul> <ul> <li> <p><code>nebula-metad.conf</code></p> <pre><code>########## networking ##########\n# Comma separated Meta Server addresses\n--meta_server_addrs=192.168.10.111:9559,192.168.10.112:9559,192.168.10.113:9559\n# Local IP used to identify the nebula-metad process.\n# Change it to an address other than loopback if the service is distributed or\n# will be accessed remotely.\n--local_ip=192.168.10.112\n# Meta daemon listening port\n--port=9559\n</code></pre> </li> </ul> </li> </ul> <ul> <li> <p>Deploy machine C</p> <ul> <li> <p><code>nebula-graphd.conf</code></p> <pre><code>########## networking ##########\n# Comma separated Meta Server Addresses\n--meta_server_addrs=192.168.10.111:9559,192.168.10.112:9559,192.168.10.113:9559\n# Local IP used to identify the nebula-graphd process.\n# Change it to an address other than loopback if the service is distributed or\n# will be accessed remotely.\n--local_ip=192.168.10.113\n# Network device to listen on\n--listen_netdev=any\n# Port to listen on\n--port=9669\n</code></pre> </li> </ul> <ul> <li> <p><code>nebula-storaged.conf</code></p> <pre><code>########## networking ##########\n# Comma separated Meta server addresses\n--meta_server_addrs=192.168.10.111:9559,192.168.10.112:9559,192.168.10.113:9559\n# Local IP used to identify the nebula-storaged process.\n# Change it to an address other than loopback if the service is distributed or\n# will be accessed remotely.\n--local_ip=192.168.10.113\n# Storage daemon listening port\n--port=9779\n</code></pre> </li> </ul> <ul> <li> <p><code>nebula-metad.conf</code></p> <pre><code>########## networking ##########\n# Comma separated Meta Server addresses\n--meta_server_addrs=192.168.10.111:9559,192.168.10.112:9559,192.168.10.113:9559\n# Local IP used to identify the nebula-metad process.\n# Change it to an address other than loopback if the service is distributed or\n# will be accessed remotely.\n--local_ip=192.168.10.113\n# Meta daemon listening port\n--port=9559\n</code></pre> </li> </ul> </li> </ul> <ul> <li> <p>Deploy machine D</p> <ul> <li> <p><code>nebula-graphd.conf</code></p> <pre><code>########## networking ##########\n# Comma separated Meta Server Addresses\n--meta_server_addrs=192.168.10.111:9559,192.168.10.112:9559,192.168.10.113:9559\n# Local IP used to identify the nebula-graphd process.\n# Change it to an address other than loopback if the service is distributed or\n# will be accessed remotely.\n--local_ip=192.168.10.114\n# Network device to listen on\n--listen_netdev=any\n# Port to listen on\n--port=9669\n</code></pre> </li> </ul> <ul> <li> <p><code>nebula-storaged.conf</code></p> <pre><code>########## networking ##########\n# Comma separated Meta server addresses\n--meta_server_addrs=192.168.10.111:9559,192.168.10.112:9559,192.168.10.113:9559\n# Local IP used to identify the nebula-storaged process.\n# Change it to an address other than loopback if the service is distributed or\n# will be accessed remotely.\n--local_ip=192.168.10.114\n# Storage daemon listening port\n--port=9779\n</code></pre> </li> </ul> </li> </ul> <ul> <li> <p>Deploy machine E</p> <ul> <li> <p><code>nebula-graphd.conf</code></p> <pre><code>########## networking ##########\n# Comma separated Meta Server Addresses\n--meta_server_addrs=192.168.10.111:9559,192.168.10.112:9559,192.168.10.113:9559\n# Local IP used to identify the nebula-graphd process.\n# Change it to an address other than loopback if the service is distributed or\n# will be accessed remotely.\n--local_ip=192.168.10.115\n# Network device to listen on\n--listen_netdev=any\n# Port to listen on\n--port=9669\n</code></pre> </li> </ul> <ul> <li> <p><code>nebula-storaged.conf</code></p> <pre><code>########## networking ##########\n# Comma separated Meta server addresses\n--meta_server_addrs=192.168.10.111:9559,192.168.10.112:9559,192.168.10.113:9559\n# Local IP used to identify the nebula-storaged process.\n# Change it to an address other than loopback if the service is distributed or\n# will be accessed remotely.\n--local_ip=192.168.10.115\n# Storage daemon listening port\n--port=9779\n</code></pre> </li> </ul> </li> </ul>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/deploy-nebula-graph-cluster/#start_the_cluster","title":"Start the cluster","text":"<p>Start the corresponding service on each machine. Descriptions are as follows.</p> Machine name The process to be started A graphd, storaged, metad B graphd, storaged, metad C graphd, storaged, metad D graphd, storaged E graphd, storaged <p>The command to start the NebulaGraph services is as follows.</p> <pre><code>sudo /usr/local/nebula/scripts/nebula.service start &lt;metad|graphd|storaged|all&gt;\n</code></pre> <p>Note</p> <ul> <li>Make sure all the processes of services on each machine are started. Otherwise, you will fail to start NebulaGraph.</li> </ul> <ul> <li>When the graphd process, the storaged process, and the metad process are all started, you can use <code>all</code> instead.</li> </ul> <ul> <li><code>/usr/local/nebula</code> is the default installation path for NebulaGraph. Use the actual path if you have customized the path. For more information about how to start and stop the services, see Manage NebulaGraph services.</li> </ul>"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/deploy-nebula-graph-cluster/#check_the_cluster_status","title":"Check the cluster status","text":"<p>Install the native CLI client NebulaGraph Console, then connect to any machine that has started the graphd process, run <code>ADD HOSTS</code> command to add storage hosts, and run <code>SHOW HOSTS</code> to check the cluster status. For example:</p> <pre><code>$ ./nebula-console --addr 192.168.10.111 --port 9669 -u root -p nebula\n\n2021/05/25 01:41:19 [INFO] connection pool is initialized successfully\nWelcome to NebulaGraph!\n\n&gt; ADD HOSTS 192.168.10.111:9779, 192.168.10.112:9779, 192.168.10.113:9779, 192.168.10.114:9779, 192.168.10.115:9779;\n&gt; SHOW HOSTS;\n+------------------+------+----------+--------------+----------------------+------------------------+---------+\n| Host             | Port | Status   | Leader count | Leader distribution  | Partition distribution | Version |\n+------------------+------+----------+--------------+----------------------+------------------------+---------+\n| \"192.168.10.111\" | 9779 | \"ONLINE\" | 0            | \"No valid partition\" | \"No valid partition\"   | \"3.5.0\" |\n| \"192.168.10.112\" | 9779 | \"ONLINE\" | 0            | \"No valid partition\" | \"No valid partition\"   | \"3.5.0\" |\n| \"192.168.10.113\" | 9779 | \"ONLINE\" | 0            | \"No valid partition\" | \"No valid partition\"   | \"3.5.0\" |\n| \"192.168.10.114\" | 9779 | \"ONLINE\" | 0            | \"No valid partition\" | \"No valid partition\"   | \"3.5.0\" |\n| \"192.168.10.115\" | 9779 | \"ONLINE\" | 0            | \"No valid partition\" | \"No valid partition\"   | \"3.5.0\" |\n+------------------+------+-----------+----------+--------------+----------------------+------------------------+---------+\n</code></pre>"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/upgrade-nebula-ent-from-3.x-3.4/","title":"Upgrade NebulaGraph Enterprise Edition from version 3.x to 3.5.0","text":"<p>This topic takes the enterprise edition of NebulaGraph v3.1.0 as an example and describes how to upgrade to v3.5.0.</p>"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/upgrade-nebula-ent-from-3.x-3.4/#notes","title":"Notes","text":"<ul> <li> <p>This upgrade is only applicable for upgrading the enterprise edition of NebulaGraph v3.x (x &lt; 4) to v3.5.0. For upgrading from version 3.4.0 and above to 3.5.0, you can directly replace the binary files for an upgrade. For more information, see Upgrade NebulaGraph to 3.5.0.  </p> <p>Note</p> <p>If your version is below 3.0.0, please upgrade to enterprise edition 3.1.0 before upgrading to v3.5.0. For details, see Upgrade NebulaGraph Enterprise Edition 2.x to 3.1.0.</p> </li> </ul> <ul> <li>The IP address of the machine performing the upgrade operation must be the same as the original machine.</li> </ul> <ul> <li>The remaining disk space on the machine must be at least 1.5 times the size of the original data directory.</li> </ul> <ul> <li> <p>Before upgrading a NebulaGraph cluster with full-text indexes deployed, you must manually delete the full-text indexes in Elasticsearch, and then run the <code>SIGN IN</code> command to log into ES and recreate the indexes after the upgrade is complete.</p> <p>Note</p> <p>To manually delete the full-text indexes in Elasticsearch, you can use the curl command <code>curl -XDELETE -u &lt;es_username&gt;:&lt;es_password&gt; '&lt;es_access_ip&gt;:&lt;port&gt;/&lt;fullindex_name&gt;'</code>, for example, <code>curl -XDELETE -u elastic:elastic 'http://192.168.8.223:9200/nebula_index_2534'</code>. If no username and password are set for Elasticsearch, you can omit the <code>-u &lt;es_username&gt;:&lt;es_password&gt;</code> part.</p> </li> </ul>"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/upgrade-nebula-ent-from-3.x-3.4/#steps","title":"Steps","text":"<ol> <li> <p>Contact us to obtain the installation package of the enterprise edition of NebulaGraph v3.5.0 and install it.</p> <p>Note</p> <p>The upgrade steps are the same for different installation packages. This article uses the RPM package and the installation directory <code>/usr/local/nebulagraph-ent-3.5.0</code> as an example. See Install with RPM packages for specific operations.</p> <p>Caution</p> <p>Please ensure that the number of storage paths set for the <code>--data_path</code> parameter in the Meta and Storage service configuration files of the 3.5.0 cluster is the same as that for the <code>--data_path</code> parameter in the configuration files of the 3.x cluster. Otherwise, the upgraded cluster will not start.</p> </li> <li> <p>Stop the enterprise edition of v3.x services. For details see Manage NebulaGraph services.</p> <p>Run the <code>nebula.service status all</code> command to confirm that all services have been stopped after running the command.</p> </li> <li> <p>In the installation directory of the Enterprise Edition NebulaGraph v3.5.0, run the following commands to upgrade the Storage and Meta services. </p> <ul> <li> <p>Upgrade the Storage service:</p> <p>Syntax:</p> <pre><code>sudo ./bin/db_upgrader  --max_concurrent_parts=&lt;num&gt; --src_db_path=&lt;source_storage_data_path&gt; --dst_db_path=&lt;destination_storage_data_path&gt;\n</code></pre> Parameter Description <code>--max_concurrent_parts</code> Specify the number of partitions to upgrade simultaneously, with the default value being 1.It is recommended to increase the value appropriately based on disk performance. <code>--src_db_path</code> Specify the absolute path to the source data directory. The following takes the source data directory <code>/usr/local/nebula-ent-3.1.0/data/storage</code> as an example. <code>--dst_db_path</code> Specify the absolute path to the target data directory. The example target data directory is <code>/usr/local/nebula-ent-3.5.0/data/storage</code>. <p>Example:</p> <pre><code>sudo ./bin/db_upgrader --max_concurrent_parts=20 --src_db_path=/usr/local/nebula-ent-3.1.0/data/storage --dst_db_path=/usr/local/nebula-ent-3.5.0/data/storage\n</code></pre> <p>If there are multiple source data directories, specify each source data directory and target data directory and run the corresponding command. For example, there are two source data directories <code>/usr/local/nebula-ent-3.1.0/data/storage</code> and <code>/usr/local/nebula-ent-3.1.0/data2/storage</code>, run the following commands:</p> <pre><code>sudo ./bin/db_upgrader --src_db_path=/usr/local/nebula-ent-3.1.0/data/storage --dst_db_path=/usr/local/nebula-ent-3.5.0/data/storage\n\nsudo ./bin/db_upgrader --src_db_path=/usr/local/nebula-ent-3.1.0/data2/storage --dst_db_path=/usr/local/nebula-ent-3.5.0/data2/storage\n</code></pre> </li> </ul> <ul> <li> <p>Upgrade the Meta service:</p> <p>Syntax:</p> <pre><code>sudo ./bin/meta_upgrader --src_meta_path=&lt;source_meta_data_path&gt; --dst_meta_path=&lt;destination_meta_data_path&gt;\n</code></pre> Parameter Description <code>--src_meta_path</code> Specify the absolute path to the source meta data directory. The following takes the source data directory <code>/usr/local/nebula-ent-3.1.0/data/meta</code> as an example. <code>--dst_meta_path</code> Specify the absolute path to the target meta data directory. The example target data directory is <code>/usr/local/nebula-ent-3.5.0/data/meta</code>. <p>Example:</p> <pre><code>sudo ./bin/meta_upgrader --src_meta_path=/usr/local/nebula-ent-3.1.0/data/meta --dst_meta_path=/usr/local/nebula-ent-3.5.0/data/meta\n</code></pre> <p>If there are multiple source meta data directories, specify each source meta data directory and target meta data directory and run the corresponding command.</p> </li> </ul> <p>After the upgrade, a <code>data</code> directory will be generated in the v3.5.0 installation directory, containing the upgraded data files.</p> </li> <li> <p>In the <code>/usr/local/nebula-ent-3.5.0/etc/nebula-metad.conf</code> file, set <code>license_manager_url</code> to the URL of LM.</p> <p>Note</p> <p>For the enterprise edition of NebulaGraph v3.5.0 or later, you need to install and configure LM to verify the license used to start NebulaGraph.</p> </li> <li> <p>Start and connect to the NebulaGraph v3.5.0 enterprise edition service and verify that the data is correct. The following commands can be used as reference:</p> <pre><code>nebula&gt; SHOW HOSTS;\nnebula&gt; SHOW HOSTS storage;\nnebula&gt; SHOW SPACES;\nnebula&gt; USE &lt;space_name&gt;\nnebula&gt; SHOW PARTS;\nnebula&gt; SUBMIT JOB STATS;\nnebula&gt; SHOW STATS;\nnebula&gt; MATCH (v) RETURN v LIMIT 5;\n</code></pre> </li> </ol>"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/upgrade-nebula-ent-from-3.x-3.4/#docker_compose_deployment","title":"Docker Compose Deployment","text":"<p>Caution</p> <p>For NebulaGraph deployed using Docker Compose, it is recommended to redeploy the new version and import data.</p>"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/upgrade-nebula-from-300-to-latest/","title":"Upgrade NebulaGraph v3.x to v3.5.0","text":"<p>To upgrade NebulaGraph v3.x to v3.5.0, you only need to use the RPM/DEB package of v3.5.0 for the upgrade, or compile it and then reinstall.</p> <p>Caution</p> <p>Before upgrading a NebulaGraph cluster with full-text indexes deployed, you must manually delete the full-text indexes in Elasticsearch, and then run the <code>SIGN IN</code> command to log into ES and recreate the indexes after the upgrade is complete. To manually delete the full-text indexes in Elasticsearch, you can use the curl command <code>curl -XDELETE -u &lt;es_username&gt;:&lt;es_password&gt; '&lt;es_access_ip&gt;:&lt;port&gt;/&lt;fullindex_name&gt;'</code>, for example, <code>curl -XDELETE -u elastic:elastic 'http://192.168.8.223:9200/nebula_index_2534'</code>. If no username and password are set for Elasticsearch, you can omit the <code>-u &lt;es_username&gt;:&lt;es_password&gt;</code> part.</p>"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/upgrade-nebula-from-300-to-latest/#upgrade_steps_with_rpmdeb_packages","title":"Upgrade steps with RPM/DEB packages","text":"<ol> <li> <p>Download the RPM/DEB package.</p> </li> <li> <p>Stop all NebulaGraph services. For details, see Manage NebulaGraph Service. It is recommended to back up the configuration file before updating.</p> <p>Caution</p> <p>If you want to use the vertex without tags, add <code>--graph_use_vertex_key=true</code> to the configuration files (<code>nebula-graphd.conf</code>) of all Graph services in the cluster, add <code>--use_vertex_key=true</code> to the configuration files (<code>nebula-storaged.conf</code>) of all Storage services in the cluster.</p> </li> <li> <p>Execute the following command to upgrade:</p> <ul> <li>RPM package<pre><code>$ sudo rpm -Uvh &lt;package_name&gt;\n</code></pre> <p>If you specify the path during installation, you also need to specify the path during upgrade.</p> <pre><code>$ sudo rpm -Uvh --prefix=&lt;installation_path&gt; &lt;package_name&gt;\n</code></pre> </li> </ul> <ul> <li>DEB package<pre><code>$ sudo dpkg -i &lt;package_name&gt;\n</code></pre> </li> </ul> </li> <li> <p>Start the required services on each server. For details, see Manage NebulaGraph Service.</p> </li> </ol>"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/upgrade-nebula-from-300-to-latest/#upgrade_steps_by_compiling_the_new_source_code","title":"Upgrade steps by compiling the new source code","text":"<ol> <li> <p>Back up the old version of the configuration file. The configuration file is saved in the <code>etc</code> directory of the NebulaGraph installation path.</p> </li> <li> <p>Update the repository and compile the source code. For details, see Install NebulaGraph by compiling the source code.</p> <p>Note</p> <p>When compiling, set the installation path, which is the same as the installation path of the old version.</p> </li> </ol>"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/upgrade-nebula-from-300-to-latest/#upgrade_steps_by_deploying_docker_compose","title":"Upgrade steps by deploying Docker Compose","text":"<ol> <li> <p>Modify the file <code>docker-compose.yaml</code> in the directory <code>nebula-docker-compose</code>, and modify all versions after <code>image</code> to <code>release-3.5</code>.</p> </li> <li> <p>Execute the command <code>docker-compose pull</code> in the directory <code>nebula-docker-compose</code> to update the images of all services.</p> </li> <li> <p>Execute the command <code>docker-compose down</code> to stop the NebulaGraph service.</p> </li> <li> <p>Execute the command <code>docker-compose up -d</code> to start the NebulaGraph service.</p> </li> </ol>"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/upgrade-nebula-graph-to-latest/","title":"Upgrade NebulaGraph to 3.5.0","text":"<p>This topic describes how to upgrade NebulaGraph from version 2.x and 3.x to 3.5.0, taking upgrading from version 2.6.1 to 3.5.0 as an example.</p>"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/upgrade-nebula-graph-to-latest/#applicable_source_versions","title":"Applicable source versions","text":"<p>This topic applies to upgrading NebulaGraph from 2.5.0 and later 2.x, and 3.x versions to 3.5.0. It does not apply to historical versions earlier than 2.5.0, including the 1.x versions.</p> <p>To upgrade NebulaGraph from historical versions to 3.5.0:</p> <ol> <li>Upgrade it to the latest 2.5 version according to the docs of that version.</li> <li>Follow this topic to upgrade it to 3.5.0.</li> </ol> <p>Caution</p> <p>To upgrade NebulaGraph from versions earlier than 2.0.0 (including the 1.x versions) to 3.5.0, you need to find the <code>date_time_zonespec.csv</code> in the <code>share/resources</code> directory of 3.5.0 files, and then copy it to the same directory in the NebulaGraph installation path.</p>"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/upgrade-nebula-graph-to-latest/#limitations","title":"Limitations","text":"<ul> <li>Rolling Upgrade is not supported. You must stop all the NebulaGraph services before the upgrade.</li> </ul> <ul> <li>There is no upgrade script. You have to manually upgrade each server in the cluster.</li> </ul> <ul> <li>This topic does not apply to scenarios where NebulaGraph is deployed with Docker, including Docker Swarm, Docker Compose, and K8s.</li> </ul> <ul> <li>You must upgrade the old NebulaGraph services on the same machines they are deployed. DO NOT change the IP addresses, configuration files of the machines, and DO NOT change the cluster topology.</li> </ul> <ul> <li>Known issues that could cause data loss are listed on GitHub known issues. The issues are all related to altering schema or default values.</li> </ul> <ul> <li>DO NOT use soft links to switch the data directories.</li> </ul> <ul> <li>You must have the sudo privileges to complete the steps in this topic.</li> </ul>"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/upgrade-nebula-graph-to-latest/#upgrade_influences","title":"Upgrade influences","text":"<ul> <li> <p>Client compatibility</p> <p>After the upgrade, you will not be able to connect to NebulaGraph from old clients. You will need to upgrade all clients to a version compatible with NebulaGraph 3.5.0.</p> </li> </ul> <ul> <li> <p>Configuration changes</p> <p>A few configuration parameters have been changed. For more information, see the release notes and configuration docs.</p> </li> </ul> <ul> <li> <p>nGQL compatibility</p> <p>The nGQL syntax is partially incompatible:</p> <ul> <li>Disable the <code>YIELD</code> clause to return custom variables.</li> </ul> <ul> <li>The <code>YIELD</code> clause is required in the <code>FETCH</code>, <code>GO</code>, <code>LOOKUP</code>, <code>FIND PATH</code> and <code>GET SUBGRAPH</code> statements.</li> </ul> <ul> <li>It is required to specify a tag to query properties of a vertex in a <code>MATCH</code> statement. For example, from <code>return v.name</code> to <code>return v.player.name</code>.</li> </ul> </li> </ul> <ul> <li> <p>Full-text indexes</p> <p>Before upgrading a NebulaGraph cluster with full-text indexes deployed, you must manually delete the full-text indexes in Elasticsearch, and then run the <code>SIGN IN</code> command to log into ES and recreate the indexes after the upgrade is complete. To manually delete the full-text indexes in Elasticsearch, you can use the curl command <code>curl -XDELETE -u &lt;es_username&gt;:&lt;es_password&gt; '&lt;es_access_ip&gt;:&lt;port&gt;/&lt;fullindex_name&gt;'</code>, for example, <code>curl -XDELETE -u elastic:elastic 'http://192.168.8.xxx:9200/nebula_index_2534'</code>. If no username and password are set for Elasticsearch, you can omit the <code>-u &lt;es_username&gt;:&lt;es_password&gt;</code> part.</p> </li> </ul> <p>Caution</p> <p>There may be other undiscovered influences. Before the upgrade, we recommend that you read the release notes and user manual carefully, and keep an eye on the posts on the forum and issues on Github.</p>"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/upgrade-nebula-graph-to-latest/#preparations_before_the_upgrade","title":"Preparations before the upgrade","text":"<ul> <li> <p>Download the package of NebulaGraph 3.5.0 according to your operating system and system architecture. You need the binary files during the upgrade. Find the package on the download page.</p> <p>Note</p> <p>You can also get the new binaries from the source code or the RPM/DEB package.</p> </li> </ul> <ul> <li> <p>Locate the data files based on the value of the <code>data_path</code> parameters in the Storage and Meta configurations, and backup the data files. The default paths are <code>nebula/data/storage</code> and <code>nebula/data/meta</code>.</p> <p>Danger</p> <p>The old data will not be automatically backed up during the upgrade. You must manually back up the data to avoid data loss.</p> </li> </ul> <ul> <li>Backup the configuration files.</li> </ul> <ul> <li> <p>Collect the statistics of all graph spaces before the upgrade. After the upgrade, you can collect again and compare the results to make sure that no data is lost. To collect the statistics:</p> <ol> <li>Run <code>SUBMIT JOB STATS</code>.</li> <li>Run <code>SHOW JOBS</code> and record the result.</li> </ol> </li> </ul>"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/upgrade-nebula-graph-to-latest/#upgrade_steps","title":"Upgrade steps","text":"<ol> <li> <p>Stop all NebulaGraph services.</p> <pre><code>&lt;nebula_install_path&gt;/scripts/nebula.service stop all\n</code></pre> <p><code>nebula_install_path</code> indicates the installation path of NebulaGraph.</p> <p>The storaged progress needs around 1 minute to flush data. You can run <code>nebula.service status all</code> to check if all services are stopped. For more information about starting and stopping services, see Manage services.</p> <p>Note</p> <p>If the services are not fully stopped in 20 minutes, stop upgrading and ask for help on the forum or Github.</p> <p>Caution</p> <p>Starting from version 3.0.0, it is possible to insert vertices without tags. If you need to keep vertices without tags, add <code>--graph_use_vertex_key=true</code> in the configuration file (<code>nebula-graphd.conf</code>) of all Graph services within the cluster; and add <code>--use_vertex_key=true</code> in the configuration file (<code>nebula-storaged.conf</code>) of all Storage services.\"</p> </li> <li> <p>In the target path where you unpacked the package, use the binaries in the <code>bin</code> directory to replace the old binaries in the <code>bin</code> directory in the NebulaGraph installation path.</p> <p>Note</p> <p>Update the binary of the corresponding service on each NebulaGraph server.</p> </li> <li> <p>Modify the following parameters in all Graph configuration files to accommodate the value range of the new version. If the parameter values are within the specified range, skip this step.</p> <ul> <li>Set a value in [1,604800] for <code>session_idle_timeout_secs</code>. The recommended value is 28800.</li> <li>Set a value in [1,604800] for <code>client_idle_timeout_secs</code>. The recommended value is 28800.</li> </ul> <p>The default values of these parameters in the 2.x versions are not within the range of the new version. If you do not change the default values, the upgrade will fail. For detailed parameter description, see Graph Service Configuration.</p> </li> <li> <p>Start all Meta services.</p> <pre><code>&lt;nebula_install_path&gt;/scripts/nebula-metad.service start\n</code></pre> <p>Once started, the Meta services take several seconds to elect a leader.</p> <p>To verify that Meta services are all started, you can start any Graph server, connect to it through NebulaGraph Console, and run <code>SHOW HOSTS meta</code> and <code>SHOW META LEADER</code>. If the status of Meta services are correctly returned, the services are successfully started.</p> <p>Note</p> <p>If the operation fails, stop the upgrade and ask for help on the forum or GitHub.</p> </li> <li> <p>Start all the Graph and Storage services.</p> <p>Note</p> <p>If the operation fails, stop the upgrade and ask for help on the forum or GitHub.</p> </li> <li> <p>Connect to the new version of NebulaGraph to verify that services are available and data are complete. For how to connect, see Connect to NebulaGraph.</p> <p>Currently, there is no official way to check whether the upgrade is successful. You can run the following reference statements to test the upgrade:</p> <pre><code>nebula&gt; SHOW HOSTS;\nnebula&gt; SHOW HOSTS storage;\nnebula&gt; SHOW SPACES;\nnebula&gt; USE &lt;space_name&gt;\nnebula&gt; SHOW PARTS;\nnebula&gt; SUBMIT JOB STATS;\nnebula&gt; SHOW STATS;\nnebula&gt; MATCH (v) RETURN v LIMIT 5;\n</code></pre> <p>You can also test against new features in version 3.5.0.</p> </li> </ol>"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/upgrade-nebula-graph-to-latest/#upgrade_failure_and_rollback","title":"Upgrade failure and rollback","text":"<p>If the upgrade fails, stop all NebulaGraph services of the new version, recover the old configuration files and binaries, and start the services of the old version.</p> <p>All NebulaGraph clients in use must be switched to the old version.</p>"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/upgrade-nebula-graph-to-latest/#faq","title":"FAQ","text":""},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/upgrade-nebula-graph-to-latest/#can_i_write_through_the_client_during_the_upgrade","title":"Can I write through the client during the upgrade?","text":"<p>A: No. You must stop all NebulaGraph services during the upgrade.</p>"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/upgrade-nebula-graph-to-latest/#the_space_0_not_found_warning_message_during_the_upgrade_process","title":"The <code>Space 0 not found</code> warning message during the upgrade process","text":"<p>When the <code>Space 0 not found</code> warning message appears during the upgrade process, you can ignore it. The space <code>0</code> is used to store meta information about the Storage service and does not contain user data, so it will not affect the upgrade.</p>"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/upgrade-nebula-graph-to-latest/#how_to_upgrade_if_a_machine_has_only_the_graph_service_but_not_the_storage_service","title":"How to upgrade if a machine has only the Graph Service, but not the Storage Service?","text":"<p>A: You only need to update the configuration files and binaries of the Graph Service.</p>"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/upgrade-nebula-graph-to-latest/#how_to_resolve_the_error_permission_denied","title":"How to resolve the error <code>Permission denied</code>?","text":"<p>A: Try again with the sudo privileges.</p>"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/upgrade-nebula-graph-to-latest/#is_there_any_change_in_gflags","title":"Is there any change in gflags?","text":"<p>A: Yes. For more information, see the release notes and configuration docs.</p>"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/upgrade-nebula-graph-to-latest/#is_there_a_tool_or_solution_for_verifying_data_consistency_after_the_upgrade","title":"Is there a tool or solution for verifying data consistency after the upgrade?","text":"<p>A: No. But if you only want to check the number of vertices and edges, run <code>SUBMIT JOB STATS</code> and <code>SHOW STATS</code> after the upgrade, and compare the result with the result that you recorded before the upgrade.</p>"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/upgrade-nebula-graph-to-latest/#how_to_solve_the_issue_that_storage_is_offline_and_leader_count_is_0","title":"How to solve the issue that Storage is <code>OFFLINE</code> and <code>Leader count</code> is <code>0</code>?","text":"<p>A: Run the following statement to add the Storage hosts into the cluster manually.</p> <pre><code>ADD HOSTS &lt;ip&gt;:&lt;port&gt;[, &lt;ip&gt;:&lt;port&gt; ...];\n</code></pre> <p>For example:</p> <pre><code>ADD HOSTS 192.168.10.100:9779, 192.168.10.101:9779, 192.168.10.102:9779;\n</code></pre> <p>If the issue persists, ask for help on the forum or GitHub.</p>"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/upgrade-nebula-graph-to-latest/#why_the_job_type_changed_after_the_upgrade_but_job_id_remains_the_same","title":"Why the job type changed after the upgrade, but job ID remains the same?","text":"<p>A: <code>SHOW JOBS</code> depends on an internal ID to identify job types, but in NebulaGraph 2.5.0 the internal ID changed in this pull request, so this issue happens after upgrading from a version earlier than 2.5.0.</p>"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/1.text-based-index-restrictions/","title":"Full-text index restrictions","text":"Restrictions for NebulaGraph CommunityRestrictions for NebulaGraph Enterprise <p>Caution</p> <ul> <li>This topic introduces the restrictions for full-text indexes. Please read the restrictions very carefully before using the full-text indexes.</li> <li>Version 3.5.0 redoes the full-text index function, which is not compatible with the previous versions, and requires deleting the previous index data and rebuilding the index.</li> </ul> <p>For now, full-text search has the following limitations:</p> <ul> <li>Currently, full-text search supports <code>LOOKUP</code> statements only.</li> </ul> <ul> <li>The full-text index name can contain only numbers, lowercase letters, and underscores.</li> </ul> <ul> <li>The names of full-text indexes within different graph spaces cannot be duplicated.</li> </ul> <ul> <li>The query returns 10 records by default. You can use the <code>LIMIT</code> clause to return more records, up to 10,000. You can modify the ElasticSearch parameters to adjust the maximum number of records returned.</li> </ul> <ul> <li>If there is a full-text index on the tag/edge type, the tag/edge type cannot be deleted or modified.</li> </ul> <ul> <li>The type of properties must be <code>STRING</code> or <code>FIXED_STRING</code>.</li> </ul> <ul> <li>Full-text index can not be applied to search multiple tags/edge types.</li> </ul> <ul> <li>Full-text index can not search properties with value <code>NULL</code>.</li> </ul> <ul> <li>Altering Elasticsearch indexes is not supported at this time.</li> </ul> <ul> <li>Modifying the analyzer is not supported. You have to delete the index data and then specify the analyzer when you rebuild the index.</li> </ul> <ul> <li><code>WHERE</code> clauses supports full-text search only working on single terms.</li> </ul> <ul> <li>Make sure that you start the Elasticsearch cluster and Nebula\u00a0Graph at the same time. If not, the data writing on the Elasticsearch cluster can be incomplete.</li> </ul> <ul> <li>It may take a while for Elasticsearch to create indexes. If Nebula\u00a0Graph warns no index is found, you can check the status of the indexing task.</li> </ul> <ul> <li>NebulaGraph clusters deployed with K8s do not have native support for the full-text search feature. However, you can manually deploy the feature yourself.</li> </ul> <p>Caution</p> <p>This topic introduces the restrictions for full-text indexes. Please read the restrictions very carefully before using the full-text indexes.</p> <p>For now, full-text search has the following limitations:</p> <ul> <li>Currently, full-text search supports <code>LOOKUP</code> statements only.</li> </ul> <ul> <li>The full-text index name can contain only numbers, lowercase letters, and underscores.</li> </ul> <ul> <li>The names of full-text indexes within different graph spaces cannot be duplicated.</li> </ul> <ul> <li>If there is a full-text index on the tag/edge type, the tag/edge type cannot be deleted or modified.</li> </ul> <ul> <li>The type of properties must be <code>STRING</code> or <code>FIXED_STRING</code>.</li> </ul> <ul> <li>Full-text index can not be applied to search multiple tags/edge types.</li> </ul> <ul> <li>Sorting for the returned results of the full-text search is not supported. Data is returned in the order of data insertion.</li> </ul> <ul> <li>Full-text index can not search properties with value <code>NULL</code>.</li> </ul> <ul> <li>Altering Elasticsearch indexes is not supported at this time.</li> </ul> <ul> <li>The pipe operator is not supported.</li> </ul> <ul> <li><code>WHERE</code> clauses supports full-text search only working on single terms.</li> </ul> <ul> <li>Make sure that you start the Elasticsearch cluster and Nebula\u00a0Graph at the same time. If not, the data writing on the Elasticsearch cluster can be incomplete.</li> </ul> <ul> <li>It may take a while for Elasticsearch to create indexes. If Nebula\u00a0Graph warns no index is found, wait for the index to take effect (however, the waiting time is unknown and there is no code to check).</li> </ul> <ul> <li>NebulaGraph clusters deployed with K8s do not support the full-text search feature.</li> </ul>"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/2.deploy-es/","title":"Deploy full-text index","text":"NebulaGraph CommunityNebulaGraph Enterprise <p>Nebula\u00a0Graph full-text indexes are powered by Elasticsearch. This means that you can use Elasticsearch full-text query language to retrieve what you want. Full-text indexes are managed through built-in procedures. They can be created only for variable <code>STRING</code> and <code>FIXED_STRING</code> properties when the listener cluster and the Elasticsearch cluster are deployed.</p> <p>Nebula\u00a0Graph full-text indexes are powered by Elasticsearch. This means that you can use Elasticsearch full-text query language to retrieve what you want. Full-text indexes are managed through built-in procedures. They can be created only for variable <code>STRING</code> and <code>FIXED_STRING</code> properties when the listener cluster and the Elasticsearch cluster are deployed.</p>"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/2.deploy-es/#precaution","title":"Precaution","text":"<p>Before you start using the full-text index, please make sure that you know the restrictions.</p>"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/2.deploy-es/#deploy_elasticsearch_cluster","title":"Deploy Elasticsearch cluster","text":"<p>To deploy an Elasticsearch cluster, see Kubernetes Elasticsearch deployment or Elasticsearch installation. Currently only <code>7.x</code> versions of <code>Elasticsearch</code> are supported.</p> <p>Compatibility</p> <p>For NebulaGraph 3.4 and later versions, no additional templates need to be created.</p> <p>You can configure the Elasticsearch to meet your business needs. To customize the Elasticsearch, see Elasticsearch Document.</p>"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/2.deploy-es/#sign_in_to_the_text_search_clients","title":"Sign in to the text search clients","text":"<p>When the Elasticsearch cluster is deployed, use the <code>SIGN IN</code> statement to sign in to the Elasticsearch clients. Multiple <code>elastic_ip:port</code> pairs are separated with commas. You must use the IPs and the port number in the configuration file for the Elasticsearch.</p>"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/2.deploy-es/#syntax","title":"Syntax","text":"<pre><code>SIGN IN TEXT SERVICE (&lt;elastic_ip:port&gt;, {HTTP | HTTPS} [,\"&lt;username&gt;\", \"&lt;password&gt;\"]) [, (&lt;elastic_ip:port&gt;, ...)];\n</code></pre>"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/2.deploy-es/#example","title":"Example","text":"<pre><code>nebula&gt; SIGN IN TEXT SERVICE (127.0.0.1:9200, HTTP);\n</code></pre> <p>Note</p> <p>Elasticsearch does not have a username or password by default. If you configured a username and password, you need to specify them in the <code>SIGN IN</code> statement.</p> <p>Caution</p> <p>The Elasticsearch client can only be logged in once, and if there are changes, you need to <code>SIGN OUT</code> and then <code>SIGN IN</code> again, and the client takes effect globally, and multiple graph spaces share the same Elasticsearch client.</p>"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/2.deploy-es/#show_text_search_clients","title":"Show text search clients","text":"<p>The <code>SHOW TEXT SEARCH CLIENTS</code> statement can list the text search clients.</p>"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/2.deploy-es/#syntax_1","title":"Syntax","text":"<pre><code>SHOW TEXT SEARCH CLIENTS;\n</code></pre>"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/2.deploy-es/#example_1","title":"Example","text":"<pre><code>nebula&gt; SHOW TEXT SEARCH CLIENTS;\n+-------------+------+\n| Host        | Port |\n+-------------+------+\n| \"127.0.0.1\" | 9200 |\n| \"127.0.0.1\" | 9200 |\n| \"127.0.0.1\" | 9200 |\n+-------------+------+\n</code></pre>"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/2.deploy-es/#sign_out_to_the_text_search_clients","title":"Sign out to the text search clients","text":"<p>The <code>SIGN OUT TEXT SERVICE</code> statement can sign out all the text search clients.</p>"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/2.deploy-es/#syntax_2","title":"Syntax","text":"<pre><code>SIGN OUT TEXT SERVICE;\n</code></pre>"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/2.deploy-es/#example_2","title":"Example","text":"<pre><code>nebula&gt; SIGN OUT TEXT SERVICE;\n</code></pre>"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/2.deploy-es/#precaution_1","title":"Precaution","text":"<p>Before you start using the full-text index, please make sure that you know the restrictions.</p>"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/2.deploy-es/#deploy_elasticsearch_cluster_1","title":"Deploy Elasticsearch cluster","text":"<p>To deploy an Elasticsearch cluster, see Kubernetes Elasticsearch deployment or Elasticsearch installation.</p> <p>Note</p> <p>To support external network access to Elasticsearch, set <code>network.host</code> to <code>0.0.0.0</code> in <code>config/elasticsearch.yml</code>.</p> <p>You can configure the Elasticsearch to meet your business needs. To customize the Elasticsearch, see Elasticsearch Document.</p>"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/2.deploy-es/#sign_in_to_the_text_search_clients_1","title":"Sign in to the text search clients","text":"<p>When the Elasticsearch cluster is deployed, use the <code>SIGN IN</code> statement to sign in to the Elasticsearch clients. Multiple <code>elastic_ip:port</code> pairs are separated with commas. You must use the IPs and the port number in the configuration file for the Elasticsearch.</p>"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/2.deploy-es/#syntax_3","title":"Syntax","text":"<pre><code>SIGN IN TEXT SERVICE (&lt;elastic_ip:port&gt;, {HTTP | HTTPS} [,\"&lt;username&gt;\", \"&lt;password&gt;\"]) [, (&lt;elastic_ip:port&gt;, ...)];\n</code></pre>"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/2.deploy-es/#example_3","title":"Example","text":"<pre><code>nebula&gt; SIGN IN TEXT SERVICE (192.168.8.100:9200, HTTP);\n</code></pre> <p>Note</p> <p>Elasticsearch does not have a username or password by default. If you configured a username and password, you need to specify them in the <code>SIGN IN</code> statement.</p> <p>Caution</p> <p>The Elasticsearch client can only be logged in once, and if there are changes, you need to <code>SIGN OUT</code> and then <code>SIGN IN</code> again, and the client takes effect globally, and multiple graph spaces share the same Elasticsearch client.</p>"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/2.deploy-es/#show_text_search_clients_1","title":"Show text search clients","text":"<p>The <code>SHOW TEXT SEARCH CLIENTS</code> statement can list the text search clients.</p>"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/2.deploy-es/#syntax_4","title":"Syntax","text":"<pre><code>SHOW TEXT SEARCH CLIENTS;\n</code></pre>"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/2.deploy-es/#example_4","title":"Example","text":"<pre><code>nebula&gt; SHOW TEXT SEARCH CLIENTS;\n+-----------------+-----------------+------+\n| Type            | Host            | Port |\n+-----------------+-----------------+------+\n| \"ELASTICSEARCH\" | \"192.168.8.100\" | 9200 |\n+-----------------+-----------------+------+\n</code></pre>"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/2.deploy-es/#sign_out_to_the_text_search_clients_1","title":"Sign out to the text search clients","text":"<p>The <code>SIGN OUT TEXT SERVICE</code> statement can sign out all the text search clients.</p>"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/2.deploy-es/#syntax_5","title":"Syntax","text":"<pre><code>SIGN OUT TEXT SERVICE;\n</code></pre>"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/2.deploy-es/#example_5","title":"Example","text":"<pre><code>nebula&gt; SIGN OUT TEXT SERVICE;\n</code></pre>"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/","title":"Deploy Raft Listener for NebulaGraph Storage service","text":"NebulaGraph CommunityNebulaGraph Enterprise <p>Full-text index data is written to the Elasticsearch cluster asynchronously. The Raft Listener (Listener for short) is a separate process that fetches data from the Storage Service and writes them into the Elasticsearch cluster.</p> <p>Full-text index data is written to the Elasticsearch cluster asynchronously. The Raft Listener (Listener for short) is a separate process that fetches data from the Storage Service and writes them into the Elasticsearch cluster.</p>"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/#prerequisites","title":"Prerequisites","text":"<ul> <li>You have read and fully understood the restrictions for using full-text indexes.</li> </ul> <ul> <li>You have deployed a NebulaGraph cluster.</li> </ul> <ul> <li>You have deploy a Elasticsearch cluster.</li> </ul> <ul> <li>You have prepared at least one extra Storage Server. To use the full-text search, you must run one or more Storage Server as the Raft Listener.</li> </ul>"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/#precautions","title":"Precautions","text":"<ul> <li>The Storage Service that you want to run as the Listener must have the same or later release with all the other Nebula\u00a0Graph services in the cluster.</li> </ul> <ul> <li>For now, you can only add all Listeners to a graph space once and for all. Trying to add a new Listener to a graph space that already has a Listener will fail. To add all Listeners, set them in one statement.</li> </ul>"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/#deployment_process","title":"Deployment process","text":""},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/#step_1_install_the_storage_service","title":"Step 1: Install the Storage service","text":"<p>The Listener process and the storaged process use the same binary file. However, their configuration files and using ports are different. You can install NebulaGraph on all servers that need to deploy a Listener, but only the Storage service can be used. For details, see Install NebulaGraph by RPM or DEB Package.</p>"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/#step_2_prepare_the_configuration_file_for_the_listener","title":"Step 2: Prepare the configuration file for the Listener","text":"<p>You have to prepare a corresponding configuration file on the machine that you want to deploy a Listener. The file must be named as <code>nebula-storaged-listener.conf</code> and stored in the <code>etc</code> directory. A template is provided for your reference. Note that the file suffix <code>.production</code> should be removed.</p> <p>Most configurations are the same as the configurations of Storage Service. This topic only introduces the differences.</p> Name Default value Description <code>daemonize</code> <code>true</code> When set to <code>true</code>, the process is a daemon process. <code>pid_file</code> <code>pids/nebula-metad-listener.pid</code> The file that records the process ID. <code>meta_server_addrs</code> - IP addresses and ports of all Meta services. Multiple Meta services are separated by commas. <code>local_ip</code> - The local IP address of the Listener service. <code>port</code> - The listening port of the RPC daemon of the Listener service. <code>heartbeat_interval_secs</code> <code>10</code> The heartbeat interval of the Meta service. The unit is second (s). <code>listener_path</code> <code>data/listener</code> The WAL directory of the Listener. Only one directory is allowed. <code>data_path</code> <code>data</code> For compatibility reasons, this parameter can be ignored. Fill in the default value <code>data</code>. <code>part_man_type</code> <code>memory</code> The type of the part manager. Optional values \u200b\u200bare <code>memory</code> and <code>meta</code>. <code>rocksdb_batch_size</code> <code>4096</code> The default reserved bytes for batch operations. <code>rocksdb_block_cache</code> <code>4</code> The default block cache size of BlockBasedTable. The unit is Megabyte (MB). <code>engine_type</code> <code>rocksdb</code> The type of the Storage engine, such as <code>rocksdb</code>, <code>memory</code>, etc. <code>part_type</code> <code>simple</code> The type of the part, such as <code>simple</code>, <code>consensus</code>, etc. <p>Note</p> <p>Use real IP addresses in the configuration file instead of domain names or loopback IP addresses such as <code>127.0.0.1</code>.</p>"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/#step_3_start_listeners","title":"Step 3: Start Listeners","text":"<p>Run the following command to start the Listener.</p> <pre><code>./bin/nebula-storaged --flagfile &lt;listener_config_path&gt;/nebula-storaged-listener.conf\n</code></pre> <p><code>${listener_config_path}</code> is the path where you store the Listener configuration file.</p>"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/#step_4_add_listeners_to_nebulagraph","title":"Step 4: Add Listeners to NebulaGraph","text":"<p>Connect to NebulaGraph and run <code>USE &lt;space&gt;</code> to enter the graph space that you want to create full-text indexes for. Then run the following statement to add a Listener into NebulaGraph.</p> <pre><code>ADD LISTENER ELASTICSEARCH &lt;listener_ip:port&gt; [,&lt;listener_ip:port&gt;, ...]\n</code></pre> <p>Warning</p> <p>You must use real IPs for a Listener.</p> <p>Add all Listeners in one statement completely.</p> <pre><code>nebula&gt; ADD LISTENER ELASTICSEARCH 192.168.8.5:9789,192.168.8.6:9789;\n</code></pre>"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/#show_listeners","title":"Show Listeners","text":"<p>Run the <code>SHOW LISTENER</code> statement to list all Listeners.</p>"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/#example","title":"Example","text":"<pre><code>nebula&gt; SHOW LISTENER;\n+--------+-----------------+-----------------------+----------+\n| PartId | Type            | Host                  | Status   |\n+--------+-----------------+-----------------------+----------+\n| 1      | \"ELASTICSEARCH\" | \"[192.168.8.5:46780]\" | \"ONLINE\" |\n| 2      | \"ELASTICSEARCH\" | \"[192.168.8.5:46780]\" | \"ONLINE\" |\n| 3      | \"ELASTICSEARCH\" | \"[192.168.8.5:46780]\" | \"ONLINE\" |\n+--------+-----------------+-----------------------+----------+\n</code></pre>"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/#remove_listeners","title":"Remove Listeners","text":"<p>Run the <code>REMOVE LISTENER ELASTICSEARCH</code> statement to remove all Listeners in a graph space.</p>"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/#example_1","title":"Example","text":"<pre><code>nebula&gt; REMOVE LISTENER ELASTICSEARCH;\n</code></pre>"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/#prerequisites_1","title":"Prerequisites","text":"<ul> <li>You have read and fully understood the restrictions for using full-text indexes.</li> </ul> <ul> <li>You have deployed a NebulaGraph cluster.</li> </ul> <ul> <li>You have deployed a Elasticsearch cluster.</li> </ul> <ul> <li>You have prepared one or multiple servers to run one or multiple raft listeners.</li> </ul>"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/#precautions_1","title":"Precautions","text":"<ul> <li>The Storage Service that you want to run as the Listener must have the same or later release with all the other Nebula\u00a0Graph services in the cluster.</li> </ul> <ul> <li>For now, you can only add all Listeners to a graph space once and for all. Trying to add a new Listener to a graph space that already has a Listener will fail. To add all Listeners, set them in one statement.</li> </ul>"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/#deployment_process_1","title":"Deployment process","text":""},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/#step_1_install_the_listener_service","title":"Step 1: Install the Listener service","text":"<p>The Listener service uses the same binary as the storaged service. However, the configuration files are different and the processes use different ports. You can install NebulaGraph on all servers that need to deploy a Listener, but only the storaged service can be used. For details, see Install NebulaGraph by RPM or DEB Package.</p>"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/#step_2_prepare_the_configuration_file_for_the_listener_1","title":"Step 2: Prepare the configuration file for the Listener","text":"<p>In the <code>etc</code> directory, remove the suffix from <code>nebula-storaged-listener.conf.default</code> or <code>nebula-storaged-listener.conf.production</code> to <code>nebula-storaged-listener.conf</code>, and then modify the configuration content.</p> <p>Most configurations are the same as the configurations of Storage Service. This topic only introduces the differences.</p> Name Default value Description <code>daemonize</code> <code>true</code> When set to <code>true</code>, the process is a daemon process. <code>pid_file</code> <code>pids/nebula-metad-listener.pid</code> The file that records the process ID. <code>meta_server_addrs</code> - IP addresses and ports of all Meta services. Multiple Meta services are separated by commas. <code>local_ip</code> - The local IP address of the Listener service. Use real IP addresses instead of domain names or loopback IP addresses such as <code>127.0.0.1</code>. <code>port</code> - The listening port of the RPC daemon of the Listener service. <code>heartbeat_interval_secs</code> <code>10</code> The heartbeat interval of the Meta service. The unit is second (s). <code>listener_path</code> <code>data/listener</code> The WAL directory of the Listener. Only one directory is allowed. <code>data_path</code> <code>data</code> For compatibility reasons, this parameter can be ignored. Fill in the default value <code>data</code>. <code>part_man_type</code> <code>memory</code> The type of the part manager. Optional values \u200b\u200bare <code>memory</code> and <code>meta</code>. <code>rocksdb_batch_size</code> <code>4096</code> The default reserved bytes for batch operations. <code>rocksdb_block_cache</code> <code>4</code> The default block cache size of BlockBasedTable. The unit is Megabyte (MB). <code>engine_type</code> <code>rocksdb</code> The type of the Storage engine, such as <code>rocksdb</code>, <code>memory</code>, etc. <code>part_type</code> <code>simple</code> The type of the part, such as <code>simple</code>, <code>consensus</code>, etc."},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/#step_3_start_listeners_1","title":"Step 3: Start Listeners","text":"<p>To initiate the Listener, navigate to the installation path of the desired cluster and execute the following command:</p> <pre><code>./bin/nebula-storaged --flagfile etc/nebula-storaged-listener.conf\n</code></pre>"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/#step_4_add_listeners_to_nebulagraph_1","title":"Step 4: Add Listeners to NebulaGraph","text":"<p>Connect to NebulaGraph and run <code>USE &lt;space&gt;</code> to enter the graph space that you want to create full-text indexes for. Then run the following statement to add a Listener into NebulaGraph.</p> <pre><code>ADD LISTENER ELASTICSEARCH &lt;listener_ip:port&gt; [,&lt;listener_ip:port&gt;, ...]\n</code></pre> <p>Warning</p> <p>You must use real IPs for a Listener.</p> <p>Add all Listeners in one statement completely.</p> <pre><code>nebula&gt; ADD LISTENER ELASTICSEARCH 192.168.8.100:9789,192.168.8.101:9789;\n</code></pre>"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/#show_listeners_1","title":"Show Listeners","text":"<p>Run the <code>SHOW LISTENER</code> statement to list all Listeners.</p>"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/#example_2","title":"Example","text":"<pre><code>nebula&gt; SHOW LISTENER;\n+--------+-----------------+------------------------+-------------+\n| PartId | Type            | Host                   | Host Status |\n+--------+-----------------+------------------------+-------------+\n| 1      | \"ELASTICSEARCH\" | \"\"192.168.8.100\":9789\" | \"ONLINE\"    |\n| 2      | \"ELASTICSEARCH\" | \"\"192.168.8.100\":9789\" | \"ONLINE\"    |\n| 3      | \"ELASTICSEARCH\" | \"\"192.168.8.100\":9789\" | \"ONLINE\"    |\n+--------+-----------------+------------------------+-------------+\n</code></pre>"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/#remove_listeners_1","title":"Remove Listeners","text":"<p>Run the <code>REMOVE LISTENER ELASTICSEARCH</code> statement to remove all Listeners in a graph space.</p>"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/#example_3","title":"Example","text":"<pre><code>nebula&gt; REMOVE LISTENER ELASTICSEARCH;\n</code></pre>"},{"location":"5.configurations-and-logs/1.configurations/1.configurations/","title":"Configurations","text":"<p>NebulaGraph builds the configurations based on the gflags repository. Most configurations are flags. When the NebulaGraph service starts, it will get the configuration information from Configuration files by default. Configurations that are not in the file apply the default values.</p> <p>Enterpriseonly</p> <p>The tuning service for performance, parameters and query statements are provided only in the Enterprise Edition.</p> <p>Note</p> <ul> <li>Because there are many configurations and they may change as NebulaGraph develops, this topic will not introduce all configurations. To get detailed descriptions of configurations, follow the instructions below.</li> <li>It is not recommended to modify the configurations that are not introduced in this topic, unless you are familiar with the source code and fully understand the function of configurations.</li> </ul> <p>Legacy version compatibility</p> <p>In the topic of 1.x, we provide a method of using the <code>CONFIGS</code> command to modify the configurations in the cache. However, using this method in a production environment can easily cause inconsistencies of configurations between clusters and the local. Therefore, this method will no longer be introduced starting with version 2.x.</p>"},{"location":"5.configurations-and-logs/1.configurations/1.configurations/#get_the_configuration_list_and_descriptions","title":"Get the configuration list and descriptions","text":"<p>Use the following command to get all the configuration information of the service corresponding to the binary file:</p> <pre><code>&lt;binary&gt; --help\n</code></pre> <p>For example:</p> <pre><code># Get the help information from Meta\n$ /usr/local/nebula/bin/nebula-metad  --help\n\n# Get the help information from Graph\n$ /usr/local/nebula/bin/nebula-graphd --help\n\n# Get the help information from Storage\n$ /usr/local/nebula/bin/nebula-storaged --help\n</code></pre> <p>The above examples use the default storage path <code>/usr/local/nebula/bin/</code>. If you modify the installation path of NebulaGraph, use the actual path to query the configurations.</p>"},{"location":"5.configurations-and-logs/1.configurations/1.configurations/#get_configurations","title":"Get configurations","text":"<p>Use the <code>curl</code> command to get the value of the running configurations.</p> <p>For example:</p> <pre><code># Get the running configurations from Meta\ncurl 127.0.0.1:19559/flags\n\n# Get the running configurations from Graph\ncurl 127.0.0.1:19669/flags\n\n# Get the running configurations from Storage\ncurl 127.0.0.1:19779/flags\n</code></pre> <p>Note</p> <p>In an actual environment, use the real host IP address instead of <code>127.0.0.1</code> in the above example.</p>"},{"location":"5.configurations-and-logs/1.configurations/1.configurations/#configuration_files","title":"Configuration files","text":""},{"location":"5.configurations-and-logs/1.configurations/1.configurations/#configuration_files_for_clusters_installed_from_source_with_an_rpmdeb_package_or_a_tar_package","title":"Configuration files for clusters installed from source, with an RPM/DEB package, or a TAR package","text":"<p>NebulaGraph provides two initial configuration files for each service, <code>&lt;service_name&gt;.conf.default</code> and <code>&lt;service_name&gt;.conf.production</code>. You can use them in different scenarios conveniently. For clusters installed from source and with a RPM/DEB package, the default path is <code>/usr/local/nebula/etc/</code>. For clusters installed with a TAR package, the path is <code>&lt;install_path&gt;/&lt;tar_package_directory&gt;/etc</code>.</p> <p>The configuration values in the initial configuration file are for reference only and can be adjusted according to actual needs. To use the initial configuration file, choose one of the above two files and delete the suffix <code>.default</code> or <code>.production</code> to make it valid.</p> <p>Note</p> <p>To ensure the availability of services, it is recommended that configurations for the same service be consistent, except for the local IP address <code>local_ip</code>. For example, three Storage servers are deployed in one NebulaGraph cluster. The configurations of the three Storage servers are recommended to be consistent, except for the IP address.</p> <p>The initial configuration files corresponding to each service are as follows.</p> NebulaGraph service Initial configuration file Description Meta <code>nebula-metad.conf.default</code> and <code>nebula-metad.conf.production</code> Meta service configuration Graph <code>nebula-graphd.conf.default</code> and <code>nebula-graphd.conf.production</code> Graph service configuration Storage <code>nebula-storaged.conf.default</code> and <code>nebula-storaged.conf.production</code> Storage service configuration <p>Each initial configuration file of all services contains <code>local_config</code>. The default value is <code>true</code>, which means that the NebulaGraph service will get configurations from its configuration files and start it.</p> <p>Caution</p> <p>It is not recommended to modify the value of <code>local_config</code> to <code>false</code>. If modified, the NebulaGraph service will first read the cached configurations, which may cause configuration inconsistencies between clusters and cause unknown risks.</p>"},{"location":"5.configurations-and-logs/1.configurations/1.configurations/#configuration_files_for_clusters_installed_with_docker_compose","title":"Configuration files for clusters installed with Docker Compose","text":"<p>For clusters installed with Docker Compose, the configuration file's default installation path of the cluster is <code>&lt;install_path&gt;/nebula-docker-compose/docker-compose.yaml</code>. The parameters in the <code>command</code> field of the file are the launch parameters for each service.  </p>"},{"location":"5.configurations-and-logs/1.configurations/1.configurations/#configuration_files_for_clusters_installed_with_nebulagraph_operator","title":"Configuration files for clusters installed with NebulaGraph Operator","text":"<p>For clusters installed with Kubectl through NebulaGraph Operator, the configuration file's path is the path of the cluster YAML file. You can modify the configuration of each service through the <code>spec.{graphd|storaged|metad}.config</code> parameter.  </p> <p>Note</p> <p>The services cannot be configured for clusters installed with Helm.</p>"},{"location":"5.configurations-and-logs/1.configurations/1.configurations/#modify_configurations","title":"Modify configurations","text":"<p>You can modify the configurations of NebulaGraph in the configuration file or use commands to dynamically modify configurations.</p> <p>Caution</p> <p>Using both methods to modify the configuration can cause the configuration information to be managed inconsistently, which may result in confusion. It is recommended to only use the configuration file to manage the configuration, or to make the same modifications to the configuration file after dynamically updating the configuration through commands to ensure consistency.</p>"},{"location":"5.configurations-and-logs/1.configurations/1.configurations/#modifying_configurations_in_the_configuration_file","title":"Modifying configurations in the configuration file","text":"<p>By default, each NebulaGraph service gets configured from its configuration files. You can modify configurations and make them valid according to the following steps:</p> <ul> <li> <p>For clusters installed from source, with a RPM/DEB, or a TAR package</p> <ol> <li> <p>Use a text editor to modify the configuration files of the target service and save the modification.</p> </li> <li> <p>Choose an appropriate time to restart all NebulaGraph services to make the modifications valid.</p> </li> </ol> </li> </ul> <ul> <li> <p>For clusters installed with Docker Compose</p> <ol> <li>In the <code>&lt;install_path&gt;/nebula-docker-compose/docker-compose.yaml</code> file, modify the configurations of the target service.</li> <li>In the <code>nebula-docker-compose</code> directory, run the command <code>docker-compose up -d</code> to restart the service involving configuration modifications.</li> </ol> </li> </ul> <ul> <li> <p>For clusters installed with Kubectl</p> <p>For details, see Customize configuration parameters for a NebulaGraph cluster.</p> </li> </ul>"},{"location":"5.configurations-and-logs/1.configurations/1.configurations/#dynamically_modifying_configurations_using_command","title":"Dynamically modifying configurations using command","text":"<p>You can dynamically modify the configuration of NebulaGraph by using the curl command. For example, to modify the <code>wal_ttl</code> parameter of the Storage service to <code>600</code>, use the following command:</p> <pre><code>curl -X PUT -H \"Content-Type: application/json\" -d'{\"wal_ttl\":\"600\"}' -s \"http://192.168.15.6:19779/flags\"\n</code></pre> <p>In this command, <code>{\"wal_ttl\":\"600\"}</code> specifies the configuration parameter and its value to be modified, and <code>192.168.15.6:19779</code> specifies the IP address and HTTP port number of the Storage service.</p> <p>Caution</p> <ul> <li>The functionality of dynamically modifying configurations is only applicable to prototype verification and testing environments. It is not recommended to use this feature in production environments. This is because when the <code>local_config</code> value is set to <code>true</code>, the dynamically modified configuration is not persisted, and the configuration will be restored to the initial configuration after the service is restarted.</li> </ul> <ul> <li>Only part of the configuration parameters can be dynamically modified. For the specific list of parameters that can be modified, see the description of Whether supports runtime dynamic modifications in the respective service configuration.</li> </ul>"},{"location":"5.configurations-and-logs/1.configurations/2.meta-config/","title":"Meta Service configuration","text":"<p>NebulaGraph provides two initial configuration files for the Meta Service, <code>nebula-metad.conf.default</code> and <code>nebula-metad.conf.production</code>. Users can use them in different scenarios conveniently. The default file path is <code>/usr/local/nebula/etc/</code>.</p> <p>Caution</p> <ul> <li>It is not recommended to modify the value of <code>local_config</code> to <code>false</code>. If modified, the NebulaGraph service will first read the cached configurations, which may cause configuration inconsistencies between clusters and cause unknown risks.</li> <li>It is not recommended to modify the configurations that are not introduced in this topic, unless you are familiar with the source code and fully understand the function of configurations.</li> </ul>"},{"location":"5.configurations-and-logs/1.configurations/2.meta-config/#how_to_use_the_configuration_files","title":"How to use the configuration files","text":"<p>To use the initial configuration file, choose one of the above two files and delete the suffix <code>.default</code> or <code>.production</code> from the initial configuration file for the Meta Service to apply the configurations defined in it.</p>"},{"location":"5.configurations-and-logs/1.configurations/2.meta-config/#about_parameter_values","title":"About parameter values","text":"<p>If a parameter is not set in the configuration file, NebulaGraph uses the default value. Not all parameters are predefined. And the predefined parameters in the two initial configuration files are different. This topic uses the parameters in <code>nebula-metad.conf.default</code>.</p> <p>Caution</p> <p>Some parameter values in the configuration file can be dynamically modified during runtime. We label these parameters as Yes that supports runtime dynamic modification in this article. When the <code>local_config</code> value is set to <code>true</code>, the dynamically modified configuration is not persisted, and the configuration will be restored to the initial configuration after the service is restarted. For more information, see Modify configurations.</p> <p>For all parameters and their current values, see Configurations.</p>"},{"location":"5.configurations-and-logs/1.configurations/2.meta-config/#basics_configurations","title":"Basics configurations","text":"Name Predefined value Description Whether supports runtime dynamic modifications <code>daemonize</code> <code>true</code> When set to <code>true</code>, the process is a daemon process. No <code>pid_file</code> <code>pids/nebula-metad.pid</code> The file that records the process ID. No <code>timezone_name</code> - Specifies the NebulaGraph time zone. This parameter is not predefined in the initial configuration files. You can manually set it if you need it. The system default value is <code>UTC+00:00:00</code>. For the format of the parameter value, see Specifying the Time Zone with TZ. For example, <code>--timezone_name=UTC+08:00</code> represents the GMT+8 time zone. No <p>Note</p> <ul> <li>While inserting property values of time types, NebulaGraph transforms time types (except TIMESTAMP) to the corresponding UTC according to the time zone specified by <code>timezone_name</code>. The time-type values returned by nGQL queries are all UTC time.</li> <li><code>timezone_name</code> is only used to transform the data stored in NebulaGraph. Other time-related data of the NebulaGraph processes still uses the default time zone of the host, such as the log printing time.</li> </ul>"},{"location":"5.configurations-and-logs/1.configurations/2.meta-config/#license_configurations","title":"License configurations","text":"<p>Enterpriseonly</p> <p>The license configurations are for the Enterprise Edition only.</p> Name Predefined value Description Whether supports runtime dynamic modifications <code>license_manager_url</code> - The address of license manager. Set the value to the host IP and port number <code>9119</code> where the license management tool is located in the Meta service configuration file of NebulaGraph (<code>nebula-metad.conf</code>), e.g. <code>192.168.8.100:9119</code>. No"},{"location":"5.configurations-and-logs/1.configurations/2.meta-config/#logging_configurations","title":"Logging configurations","text":"Name Predefined value Description Whether supports runtime dynamic modifications <code>log_dir</code> <code>logs</code> The directory that stores the Meta Service log. It is recommended to put logs on a different hard disk from the data. No <code>minloglevel</code> <code>0</code> Specifies the minimum level of the log. That is, log messages at or above this level. Optional values are <code>0</code> (INFO), <code>1</code> (WARNING), <code>2</code> (ERROR), <code>3</code> (FATAL). It is recommended to set it to <code>0</code> during debugging and <code>1</code> in a production environment. If it is set to <code>4</code>, NebulaGraph will not print any logs. Yes <code>v</code> <code>0</code> Specifies the detailed level of VLOG. That is, log all VLOG messages less or equal to the level. Optional values are <code>0</code>, <code>1</code>, <code>2</code>, <code>3</code>, <code>4</code>, <code>5</code>. The VLOG macro provided by glog allows users to define their own numeric logging levels and control verbose messages that are logged with the parameter <code>v</code>. For details, see Verbose Logging. Yes <code>logbufsecs</code> <code>0</code> Specifies the maximum time to buffer the logs. If there is a timeout, it will output the buffered log to the log file. <code>0</code> means real-time output. This configuration is measured in seconds. No <code>redirect_stdout</code> <code>true</code> When set to <code>true</code>, the process redirects the<code>stdout</code> and <code>stderr</code> to separate output files. No <code>stdout_log_file</code> <code>metad-stdout.log</code> Specifies the filename for the <code>stdout</code> log. No <code>stderr_log_file</code> <code>metad-stderr.log</code> Specifies the filename for the <code>stderr</code> log. No <code>stderrthreshold</code> <code>3</code> Specifies the <code>minloglevel</code> to be copied to the <code>stderr</code> log. No <code>timestamp_in_logfile_name</code> <code>true</code> Specifies if the log file name contains a timestamp. <code>true</code> indicates yes, <code>false</code> indicates no. No"},{"location":"5.configurations-and-logs/1.configurations/2.meta-config/#networking_configurations","title":"Networking configurations","text":"Name Predefined value Description Whether supports runtime dynamic modifications <code>meta_server_addrs</code> <code>127.0.0.1:9559</code> Specifies the IP addresses and ports of all Meta Services.  Multiple addresses are separated with commas. No <code>local_ip</code> <code>127.0.0.1</code> Specifies the local IP for the Meta Service. The local IP address is used to identify the nebula-metad process. If it is a distributed cluster or requires remote access, modify it to the corresponding address. No <code>port</code> <code>9559</code> Specifies RPC daemon listening port of the Meta service. The external port for the Meta Service is predefined to <code>9559</code>. The internal port is predefined to <code>port + 1</code>, i.e., <code>9560</code>. Nebula\u00a0Graph uses the internal port for multi-replica interactions. No <code>ws_ip</code> <code>0.0.0.0</code> Specifies the IP address for the HTTP service. No <code>ws_http_port</code> <code>19559</code> Specifies the port for the HTTP service. No <code>ws_storage_http_port</code> <code>19779</code> Specifies the Storage service listening port used by the HTTP protocol. It must be consistent with the <code>ws_http_port</code> in the Storage service configuration file. This parameter only applies to standalone NebulaGraph. No <code>heartbeat_interval_secs</code> <code>10</code> Specifies the default heartbeat interval. Make sure the <code>heartbeat_interval_secs</code> values for all services are the same, otherwise NebulaGraph CANNOT work normally. This configuration is measured in seconds. Yes <p>Caution</p> <p>The real IP address must be used in the configuration file. Otherwise, <code>127.0.0.1/0.0.0.0</code> cannot be parsed correctly in some cases.</p>"},{"location":"5.configurations-and-logs/1.configurations/2.meta-config/#storage_configurations","title":"Storage configurations","text":"Name Predefined Value Description Whether supports runtime dynamic modifications <code>data_path</code> <code>data/meta</code> The storage path for Meta data. No"},{"location":"5.configurations-and-logs/1.configurations/2.meta-config/#misc_configurations","title":"Misc configurations","text":"Name Predefined Value Description Whether supports runtime dynamic modifications <code>default_parts_num</code> <code>100</code> Specifies the default partition number when creating a new graph space. No <code>default_replica_factor</code> <code>1</code> Specifies the default replica number when creating a new graph space. No"},{"location":"5.configurations-and-logs/1.configurations/2.meta-config/#rocksdb_options_configurations","title":"RocksDB options configurations","text":"Name Predefined Value Description Whether supports runtime dynamic modifications <code>rocksdb_wal_sync</code> <code>true</code> Enables or disables RocksDB WAL synchronization. Available values are <code>true</code> (enable) and <code>false</code> (disable). No"},{"location":"5.configurations-and-logs/1.configurations/2.meta-config/#black_box_configurations","title":"Black box configurations","text":"<p>Enterpriseonly</p> <p>The Nebula-BBox configurations are for the Enterprise Edition only.</p> Name Predefined Value Description Whether supports runtime dynamic modifications <code>ng_black_box_switch</code> <code>true</code> Whether to enable the Nebula-BBox feature. No <code>ng_black_box_home</code> <code>black_box</code> The name of the directory to store Nebula-BBox file data. No <code>ng_black_box_dump_period_seconds</code> <code>5</code> The time interval for Nebula-BBox to collect metric data. Unit: Second. No <code>ng_black_box_file_lifetime_seconds</code> <code>1800</code> Storage time for Nebula-BBox files generated after collecting metric data. Unit: Second. Yes"},{"location":"5.configurations-and-logs/1.configurations/3.graph-config/","title":"Graph Service configuration","text":"<p>NebulaGraph provides two initial configuration files for the Graph Service, <code>nebula-graphd.conf.default</code> and <code>nebula-graphd.conf.production</code>. Users can use them in different scenarios conveniently. The default file path is <code>/usr/local/nebula/etc/</code>.</p> <p>Caution</p> <ul> <li>It is not recommended to modify the value of <code>local_config</code> to <code>false</code>. If modified, the NebulaGraph service will first read the cached configurations, which may cause configuration inconsistencies between clusters and cause unknown risks.</li> <li>It is not recommended to modify the configurations that are not introduced in this topic, unless you are familiar with the source code and fully understand the function of configurations.</li> </ul>"},{"location":"5.configurations-and-logs/1.configurations/3.graph-config/#how_to_use_the_configuration_files","title":"How to use the configuration files","text":"<p>To use the initial configuration file, choose one of the above two files and delete the suffix <code>.default</code> or <code>.production</code> from the initial configuration file for the Meta Service to apply the configurations defined in it.</p>"},{"location":"5.configurations-and-logs/1.configurations/3.graph-config/#about_parameter_values","title":"About parameter values","text":"<p>If a parameter is not set in the configuration file, NebulaGraph uses the default value. Not all parameters are predefined. And the predefined parameters in the two initial configuration files are different. This topic uses the parameters in <code>nebula-metad.conf.default</code>.</p> <p>Caution</p> <p>Some parameter values in the configuration file can be dynamically modified during runtime. We label these parameters as Yes that supports runtime dynamic modification in this article. When the <code>local_config</code> value is set to <code>true</code>, the dynamically modified configuration is not persisted, and the configuration will be restored to the initial configuration after the service is restarted. For more information, see Modify configurations.</p> <p>For all parameters and their current values, see Configurations.</p>"},{"location":"5.configurations-and-logs/1.configurations/3.graph-config/#basics_configurations","title":"Basics configurations","text":"Name Predefined value Description Whether supports runtime dynamic modifications <code>daemonize</code> <code>true</code> When set to <code>true</code>, the process is a daemon process. No <code>pid_file</code> <code>pids/nebula-graphd.pid</code> The file that records the process ID. No <code>enable_optimizer</code> <code>true</code> When set to <code>true</code>, the optimizer is enabled. No <code>timezone_name</code> - Specifies the NebulaGraph time zone. This parameter is not predefined in the initial configuration files. The system default value is <code>UTC+00:00:00</code>. For the format of the parameter value, see Specifying the Time Zone with TZ. For example, <code>--timezone_name=UTC+08:00</code> represents the GMT+8 time zone. No <code>local_config</code> <code>true</code> When set to <code>true</code>, the process gets configurations from the configuration files. No <p>Note</p> <ul> <li>While inserting property values of time types, NebulaGraph transforms time types (except TIMESTAMP) to the corresponding UTC according to the time zone specified by <code>timezone_name</code>. The time-type values returned by nGQL queries are all UTC time.</li> <li><code>timezone_name</code> is only used to transform the data stored in NebulaGraph. Other time-related data of the NebulaGraph processes still uses the default time zone of the host, such as the log printing time.</li> </ul>"},{"location":"5.configurations-and-logs/1.configurations/3.graph-config/#logging_configurations","title":"Logging configurations","text":"Name Predefined value Description Whether supports runtime dynamic modifications <code>log_dir</code> <code>logs</code> The directory that stores the Meta Service log. It is recommended to put logs on a different hard disk from the data. No <code>minloglevel</code> <code>0</code> Specifies the minimum level of the log. That is, log messages at or above this level. Optional values are <code>0</code> (INFO), <code>1</code> (WARNING), <code>2</code> (ERROR), <code>3</code> (FATAL). It is recommended to set it to <code>0</code> during debugging and <code>1</code> in a production environment. If it is set to <code>4</code>, NebulaGraph will not print any logs. Yes <code>v</code> <code>0</code> Specifies the detailed level of VLOG. That is, log all VLOG messages less or equal to the level. Optional values are <code>0</code>, <code>1</code>, <code>2</code>, <code>3</code>, <code>4</code>, <code>5</code>. The VLOG macro provided by glog allows users to define their own numeric logging levels and control verbose messages that are logged with the parameter <code>v</code>. For details, see Verbose Logging. Yes <code>logbufsecs</code> <code>0</code> Specifies the maximum time to buffer the logs. If there is a timeout, it will output the buffered log to the log file. <code>0</code> means real-time output. This configuration is measured in seconds. No <code>redirect_stdout</code> <code>true</code> When set to <code>true</code>, the process redirects the<code>stdout</code> and <code>stderr</code> to separate output files. No <code>stdout_log_file</code> <code>graphd-stdout.log</code> Specifies the filename for the <code>stdout</code> log. No <code>stderr_log_file</code> <code>graphd-stderr.log</code> Specifies the filename for the <code>stderr</code> log. No <code>stderrthreshold</code> <code>3</code> Specifies the <code>minloglevel</code> to be copied to the <code>stderr</code> log. No <code>timestamp_in_logfile_name</code> <code>true</code> Specifies if the log file name contains a timestamp. <code>true</code> indicates yes, <code>false</code> indicates no. No"},{"location":"5.configurations-and-logs/1.configurations/3.graph-config/#query_configurations","title":"Query configurations","text":"Name Predefined value Description Whether supports runtime dynamic modifications <code>accept_partial_success</code> <code>false</code> When set to <code>false</code>, the process treats partial success as an error. This configuration only applies to read-only requests. Write requests always treat partial success as an error. Yes <code>session_reclaim_interval_secs</code> <code>60</code> Specifies the interval that the Session information is sent to the Meta service. This configuration is measured in seconds. Yes <code>max_allowed_query_size</code> <code>4194304</code> Specifies the maximum length of queries. Unit: bytes. The default value is <code>4194304</code>, namely 4MB. Yes"},{"location":"5.configurations-and-logs/1.configurations/3.graph-config/#networking_configurations","title":"Networking configurations","text":"Name Predefined value Description Whether supports runtime dynamic modifications <code>meta_server_addrs</code> <code>127.0.0.1:9559</code> Specifies the IP addresses and ports of all Meta Services.  Multiple addresses are separated with commas. No <code>local_ip</code> <code>127.0.0.1</code> Specifies the local IP for the Graph Service. The local IP address is used to identify the nebula-graphd process. If it is a distributed cluster or requires remote access, modify it to the corresponding address. No <code>listen_netdev</code> <code>any</code> Specifies the listening network device. No <code>port</code> <code>9669</code> Specifies RPC daemon listening port of the Graph service. No <code>reuse_port</code> <code>false</code> When set to <code>false</code>, the <code>SO_REUSEPORT</code> is closed. No <code>listen_backlog</code> <code>1024</code> Specifies the maximum length of the connection queue for socket monitoring. This configuration must be modified together with the <code>net.core.somaxconn</code>. No <code>client_idle_timeout_secs</code> <code>28800</code> Specifies the time to expire an idle connection. The value ranges from 1 to 604800. The default is 8 hours. This configuration is measured in seconds. No <code>session_idle_timeout_secs</code> <code>28800</code> Specifies the time to expire an idle session. The value ranges from 1 to 604800. The default is 8 hours. This configuration is measured in seconds. No <code>num_accept_threads</code> <code>1</code> Specifies the number of threads that accept incoming connections. No <code>num_netio_threads</code> <code>0</code> Specifies the number of networking IO threads. <code>0</code> is the number of CPU cores. No <code>num_max_connections</code> <code>0</code> Max active connections for all networking threads. 0 means no limit.Max connections for each networking thread = num_max_connections / num_netio_threads No <code>num_worker_threads</code> <code>0</code> Specifies the number of threads that execute queries. <code>0</code> is the number of CPU cores. No <code>ws_ip</code> <code>0.0.0.0</code> Specifies the IP address for the HTTP service. No <code>ws_http_port</code> <code>19669</code> Specifies the port for the HTTP service. No <code>heartbeat_interval_secs</code> <code>10</code> Specifies the default heartbeat interval. Make sure the <code>heartbeat_interval_secs</code> values for all services are the same, otherwise NebulaGraph CANNOT work normally. This configuration is measured in seconds. Yes <code>storage_client_timeout_ms</code> - Specifies the RPC connection timeout threshold between the Graph Service and the Storage Service. This parameter is not predefined in the initial configuration files. You can manually set it if you need it. The system default value is <code>60000</code> ms. No <code>enable_record_slow_query</code> <code>true</code> Whether to record slow queries.  Only available in NebulaGraph Enterprise Edition. No <code>slow_query_limit</code> <code>100</code> The maximum number of slow queries that can be recorded.  Only available in NebulaGraph Enterprise Edition. No <code>slow_query_threshold_us</code> <code>200000</code> When the execution time of a query exceeds the value, the query is called a slow query. Unit: Microsecond. No <code>ws_meta_http_port</code> <code>19559</code> Specifies the Meta service listening port used by the HTTP protocol. It must be consistent with the <code>ws_http_port</code> in the Meta service configuration file. No <p>Caution</p> <p>The real IP address must be used in the configuration file. Otherwise, <code>127.0.0.1/0.0.0.0</code> cannot be parsed correctly in some cases.</p>"},{"location":"5.configurations-and-logs/1.configurations/3.graph-config/#charset_and_collate_configurations","title":"Charset and collate configurations","text":"Name Predefined value Description Whether supports runtime dynamic modifications <code>default_charset</code> <code>utf8</code> Specifies the default charset when creating a new graph space. No <code>default_collate</code> <code>utf8_bin</code> Specifies the default collate when creating a new graph space. No"},{"location":"5.configurations-and-logs/1.configurations/3.graph-config/#authorization_configurations","title":"Authorization configurations","text":"Name Predefined value Description Whether supports runtime dynamic modifications <code>enable_authorize</code> <code>false</code> When set to <code>false</code>, the system authentication is not enabled. For more information, see Authentication. No <code>auth_type</code> <code>password</code> Specifies the login method. Available values are <code>password</code>, <code>ldap</code>, and <code>cloud</code>. No"},{"location":"5.configurations-and-logs/1.configurations/3.graph-config/#memory_configurations","title":"Memory configurations","text":"Name Predefined value Description Whether supports runtime dynamic modifications <code>system_memory_high_watermark_ratio</code> <code>0.8</code> Specifies the trigger threshold of the high-level memory alarm mechanism. If the system memory usage is higher than this value, an alarm mechanism will be triggered, and NebulaGraph will stop querying. This parameter is not predefined in the initial configuration files. Yes"},{"location":"5.configurations-and-logs/1.configurations/3.graph-config/#audit_configurations","title":"Audit configurations","text":"<p>Enterpriseonly</p> <p>The audit log is only available in the Enterprise Edition. </p> <p>For more information about audit log, see Audit log.</p>"},{"location":"5.configurations-and-logs/1.configurations/3.graph-config/#metrics_configurations","title":"Metrics configurations","text":"<p>| Name | Predefined value | Description |Whether supports runtime dynamic modifications| | - | - | - | | <code>enable_space_level_metrics</code> | <code>false</code> | Enable or disable space-level metrics. Such metric names contain the name of the graph space that it monitors, for example, <code>query_latency_us{space=basketballplayer}.avg.3600</code>. You can view the supported metrics with the <code>curl</code> command. For more information, see Query NebulaGraph metrics. | No|</p>"},{"location":"5.configurations-and-logs/1.configurations/3.graph-config/#session_configurations","title":"Session configurations","text":"Name Predefined value Description Whether supports runtime dynamic modifications <code>max_sessions_per_ip_per_user</code> <code>300</code> The maximum number of active sessions that can be created from a single IP adddress for a single user. No"},{"location":"5.configurations-and-logs/1.configurations/3.graph-config/#experimental_configurations","title":"Experimental configurations","text":"<p>Enterpriseonly</p> <p>The switch of the experimental feature is only available in the Community Edition.</p> Name Predefined value Description Whether supports runtime dynamic modifications <code>enable_experimental_feature</code> <code>false</code> Specifies the experimental feature. Optional values are <code>true</code> and <code>false</code>. No <code>enable_data_balance</code> <code>true</code> Whether to enable the BALANCE DATA feature. Only works when <code>enable_experimental_feature</code> is <code>true</code>. No"},{"location":"5.configurations-and-logs/1.configurations/3.graph-config/#black_box_configurations","title":"Black box configurations","text":"<p>Enterpriseonly</p> <p>The Nebula-BBox configurations are for the Enterprise Edition only.</p> Name Predefined value Description Whether supports runtime dynamic modifications <code>ng_black_box_switch</code> <code>true</code> Whether to enable the Nebula-BBox feature. No <code>ng_black_box_home</code> <code>black_box</code> The name of the directory to store Nebula-BBox file data. No <code>ng_black_box_dump_period_seconds</code> <code>5</code> The time interval for Nebula-BBox to collect metric data. Unit: Second. No <code>ng_black_box_file_lifetime_seconds</code> <code>1800</code> Storage time for Nebula-BBox files generated after collecting metric data. Unit: Second. Yes"},{"location":"5.configurations-and-logs/1.configurations/3.graph-config/#memory_tracker_configurations","title":"Memory tracker configurations","text":"Name Predefined value Description Whether supports runtime dynamic modifications <code>memory_tracker_limit_ratio</code> <code>0.8</code> The percentage of free memory. When the free memory is lower than this value, NebulaGraph stops accepting queries.  Calculated as follows:  <code>Free memory / (Total memory - Reserved memory)</code> Note: For clusters with a mixed-used environment, the value of <code>memory_tracker_limit_ratio</code> should be set to a lower value. For example, when Graphd is expected to occupy only 50% of memory, the value can be set to less than <code>0.5</code>. Yes <code>memory_tracker_untracked_reserved_memory_mb</code> <code>50</code> The reserved memory that is not tracked by the memory tracker. Unit: MB. Yes <code>memory_tracker_detail_log</code> <code>false</code> Whether to enable the memory tracker log. When the value is <code>true</code>, the memory tracker log is generated. Yes <code>memory_tracker_detail_log_interval_ms</code> <code>60000</code> The time interval for generating the memory tracker log. Unit: Millisecond. <code>memory_tracker_detail_log</code> is <code>true</code> when this parameter takes effect. Yes <code>memory_purge_enabled</code> <code>true</code> Whether to enable the memory purge feature. When the value is <code>true</code>, the memory purge feature is enabled. Yes <code>memory_purge_interval_seconds</code> <code>10</code> The time interval for the memory purge feature to purge memory. Unit: Second. This parameter only takes effect if <code>memory_purge_enabled</code> is set to true. Yes"},{"location":"5.configurations-and-logs/1.configurations/3.graph-config/#performance_optimization_configurations","title":"performance optimization configurations","text":"Name Predefined value Description Whether supports runtime dynamic modifications <code>max_job_size</code> <code>1</code> The maximum number of concurrent jobs, i.e., the maximum number of threads used in the phase of query execution where concurrent execution is possible. It is recommended to be half of the physical CPU cores. Yes <code>min_batch_size</code> <code>8192</code> The minimum batch size for processing the dataset. Takes effect only when <code>max_job_size</code> is greater than 1. Yes <code>optimize_appendvertices</code> <code>false</code> When enabled, the <code>MATCH</code> statement is executed without filtering dangling edges. Yes <code>path_batch_size</code> <code>10000</code> The number of paths constructed per thread. Yes"},{"location":"5.configurations-and-logs/1.configurations/4.storage-config/","title":"Storage Service configurations","text":"<p>NebulaGraph provides two initial configuration files for the Storage Service, <code>nebula-storaged.conf.default</code> and <code>nebula-storaged.conf.production</code>. Users can use them in different scenarios conveniently. The default file path is <code>/usr/local/nebula/etc/</code>.</p> <p>Caution</p> <ul> <li>It is not recommended to modify the value of <code>local_config</code> to <code>false</code>. If modified, the NebulaGraph service will first read the cached configurations, which may cause configuration inconsistencies between clusters and cause unknown risks.</li> <li>It is not recommended to modify the configurations that are not introduced in this topic, unless you are familiar with the source code and fully understand the function of configurations.</li> </ul>"},{"location":"5.configurations-and-logs/1.configurations/4.storage-config/#how_to_use_the_configuration_files","title":"How to use the configuration files","text":"<p>To use the initial configuration file, choose one of the above two files and delete the suffix <code>.default</code> or <code>.production</code> from the initial configuration file for the Meta Service to apply the configurations defined in it.</p>"},{"location":"5.configurations-and-logs/1.configurations/4.storage-config/#about_parameter_values","title":"About parameter values","text":"<p>If a parameter is not set in the configuration file, NebulaGraph uses the default value. Not all parameters are predefined. And the predefined parameters in the two initial configuration files are different. This topic uses the parameters in <code>nebula-metad.conf.default</code>. For parameters that are not included in <code>nebula-metad.conf.default</code>, see <code>nebula-storaged.conf.production</code>.</p> <p>Caution</p> <p>Some parameter values in the configuration file can be dynamically modified during runtime. We label these parameters as Yes that supports runtime dynamic modification in this article. When the <code>local_config</code> value is set to <code>true</code>, the dynamically modified configuration is not persisted, and the configuration will be restored to the initial configuration after the service is restarted. For more information, see Modify configurations.</p> <p>Note</p> <p>The configurations of the Raft Listener and the Storage service are different. For details, see Deploy Raft listener.</p> <p>For all parameters and their current values, see Configurations.</p>"},{"location":"5.configurations-and-logs/1.configurations/4.storage-config/#basics_configurations","title":"Basics configurations","text":"Name Predefined value Description Whether supports runtime dynamic modifications <code>daemonize</code> <code>true</code> When set to <code>true</code>, the process is a daemon process. No <code>pid_file</code> <code>pids/nebula-storaged.pid</code> The file that records the process ID. No <code>timezone_name</code> <code>UTC+00:00:00</code> Specifies the NebulaGraph time zone. This parameter is not predefined in the initial configuration files, if you need to use this parameter, add it manually. For the format of the parameter value, see Specifying the Time Zone with TZ. For example, <code>--timezone_name=UTC+08:00</code> represents the GMT+8 time zone. No <code>local_config</code> <code>true</code> When set to <code>true</code>, the process gets configurations from the configuration files. No <p>Note</p> <ul> <li>While inserting property values of time types, NebulaGraph transforms time types (except TIMESTAMP) to the corresponding UTC according to the time zone specified by <code>timezone_name</code>. The time-type values returned by nGQL queries are all UTC.</li> <li><code>timezone_name</code> is only used to transform the data stored in NebulaGraph. Other time-related data of the NebulaGraph processes still uses the default time zone of the host, such as the log printing time.</li> </ul>"},{"location":"5.configurations-and-logs/1.configurations/4.storage-config/#logging_configurations","title":"Logging configurations","text":"Name Predefined value Description Whether supports runtime dynamic modifications <code>log_dir</code> <code>logs</code> The directory that stores the Meta Service log. It is recommended to put logs on a different hard disk from the data. No <code>minloglevel</code> <code>0</code> Specifies the minimum level of the log. That is, log messages at or above this level. Optional values are <code>0</code> (INFO), <code>1</code> (WARNING), <code>2</code> (ERROR), <code>3</code> (FATAL). It is recommended to set it to <code>0</code> during debugging and <code>1</code> in a production environment. If it is set to <code>4</code>, NebulaGraph will not print any logs. Yes <code>v</code> <code>0</code> Specifies the detailed level of VLOG. That is, log all VLOG messages less or equal to the level. Optional values are <code>0</code>, <code>1</code>, <code>2</code>, <code>3</code>, <code>4</code>, <code>5</code>. The VLOG macro provided by glog allows users to define their own numeric logging levels and control verbose messages that are logged with the parameter <code>v</code>. For details, see Verbose Logging. Yes <code>logbufsecs</code> <code>0</code> Specifies the maximum time to buffer the logs. If there is a timeout, it will output the buffered log to the log file. <code>0</code> means real-time output. This configuration is measured in seconds. No <code>redirect_stdout</code> <code>true</code> When set to <code>true</code>, the process redirects the<code>stdout</code> and <code>stderr</code> to separate output files. No <code>stdout_log_file</code> <code>graphd-stdout.log</code> Specifies the filename for the <code>stdout</code> log. No <code>stderr_log_file</code> <code>graphd-stderr.log</code> Specifies the filename for the <code>stderr</code> log. No <code>stderrthreshold</code> <code>3</code> Specifies the <code>minloglevel</code> to be copied to the <code>stderr</code> log. No <code>timestamp_in_logfile_name</code> <code>true</code> Specifies if the log file name contains a timestamp. <code>true</code> indicates yes, <code>false</code> indicates no. No"},{"location":"5.configurations-and-logs/1.configurations/4.storage-config/#networking_configurations","title":"Networking configurations","text":"Name Predefined value Description Whether supports runtime dynamic modifications <code>meta_server_addrs</code> <code>127.0.0.1:9559</code> Specifies the IP addresses and ports of all Meta Services.  Multiple addresses are separated with commas. No <code>local_ip</code> <code>127.0.0.1</code> Specifies the local IP for the Storage Service. The local IP address is used to identify the nebula-storaged process. If it is a distributed cluster or requires remote access, modify it to the corresponding address. No <code>port</code> <code>9779</code> Specifies RPC daemon listening port of the Storage service. The external port for the Meta Service is predefined to <code>9779</code>. The internal port is predefined to <code>9777</code>, <code>9778</code>, and <code>9780</code>. Nebula\u00a0Graph uses the internal port for multi-replica interactions.  <code>9777</code>: The port used by the Drainer service, which is only exposed in the Enterprise Edition cluster. <code>9778</code>: The port used by the Admin service, which receives Meta commands for Storage. <code>9780</code>: The port used for Raft communication. No <code>ws_ip</code> <code>0.0.0.0</code> Specifies the IP address for the HTTP service. No <code>ws_http_port</code> <code>19779</code> Specifies the port for the HTTP service. No <code>heartbeat_interval_secs</code> <code>10</code> Specifies the default heartbeat interval. Make sure the <code>heartbeat_interval_secs</code> values for all services are the same, otherwise NebulaGraph CANNOT work normally. This configuration is measured in seconds. Yes <p>Caution</p> <p>The real IP address must be used in the configuration file. Otherwise, <code>127.0.0.1/0.0.0.0</code> cannot be parsed correctly in some cases.</p>"},{"location":"5.configurations-and-logs/1.configurations/4.storage-config/#raft_configurations","title":"Raft configurations","text":"Name Predefined value Description Whether supports runtime dynamic modifications <code>raft_heartbeat_interval_secs</code> <code>30</code> Specifies the time to expire the Raft election. The configuration is measured in seconds. Yes <code>raft_rpc_timeout_ms</code> <code>500</code> Specifies the time to expire the Raft RPC. The configuration is measured in milliseconds. Yes <code>wal_ttl</code> <code>14400</code> Specifies the lifetime of the RAFT WAL. The configuration is measured in seconds. Yes"},{"location":"5.configurations-and-logs/1.configurations/4.storage-config/#disk_configurations","title":"Disk configurations","text":"Name Predefined value Description Whether supports runtime dynamic modifications <code>data_path</code> <code>data/storage</code> Specifies the data storage path. Multiple paths are separated with commas. For NebulaGraph of the community edition, one RocksDB instance corresponds to one path. For NebulaGraph of the enterprise edition, one RocksDB instance corresponds to one partition. No <code>minimum_reserved_bytes</code> <code>268435456</code> Specifies the minimum remaining space of each data storage path. When the value is lower than this standard, the cluster data writing may fail. This configuration is measured in bytes. No <code>rocksdb_batch_size</code> <code>4096</code> Specifies the block cache for a batch operation. The configuration is measured in bytes. No <code>rocksdb_block_cache</code> <code>4</code> Specifies the block cache for BlockBasedTable. The configuration is measured in megabytes. No <code>disable_page_cache</code> <code>false</code> Enables or disables the operating system's page cache for NebulaGraph. By default, the parameter value is <code>false</code> and page cache is enabled. If the value is set to <code>true</code>, page cache is disabled and sufficient block cache space must be configured for NebulaGraph. No <code>engine_type</code> <code>rocksdb</code> Specifies the engine type. No <code>rocksdb_compression</code> <code>lz4</code> Specifies the compression algorithm for RocksDB. Optional values are <code>no</code>, <code>snappy</code>, <code>lz4</code>, <code>lz4hc</code>, <code>zlib</code>, <code>bzip2</code>, and <code>zstd</code>.This parameter modifies the compression algorithm for each level. If you want to set different compression algorithms for each level, use the parameter <code>rocksdb_compression_per_level</code>. No <code>rocksdb_compression_per_level</code> \\ Specifies the compression algorithm for each level. The priority is higher than <code>rocksdb_compression</code>. For example, <code>no:no:lz4:lz4:snappy:zstd:snappy</code>.You can also not set certain levels of compression algorithms, for example, <code>no:no:lz4:lz4::zstd</code>, level L4 and L6 use the compression algorithm of <code>rocksdb_compression</code>. No <code>enable_rocksdb_statistics</code> <code>false</code> When set to <code>false</code>, RocksDB statistics is disabled. No <code>rocksdb_stats_level</code> <code>kExceptHistogramOrTimers</code> Specifies the stats level for RocksDB. Optional values are <code>kExceptHistogramOrTimers</code>, <code>kExceptTimers</code>, <code>kExceptDetailedTimers</code>, <code>kExceptTimeForMutex</code>, and <code>kAll</code>. No <code>enable_rocksdb_prefix_filtering</code> <code>true</code> When set to <code>true</code>, the prefix bloom filter for RocksDB is enabled. Enabling prefix bloom filter makes the graph traversal faster but occupies more memory. No <code>enable_rocksdb_whole_key_filtering</code> <code>false</code> When set to <code>true</code>, the whole key bloom filter for RocksDB is enabled. <code>rocksdb_filtering_prefix_length</code> <code>12</code> Specifies the prefix length for each key. Optional values are <code>12</code> and <code>16</code>. The configuration is measured in bytes. No <code>enable_partitioned_index_filter</code> <code>false</code> When set to <code>true</code>, it reduces the amount of memory used by the bloom filter. But in some random-seek situations, it may reduce the read performance. This parameter is not predefined in the initial configuration files, if you need to use this parameter, add it manually. No"},{"location":"5.configurations-and-logs/1.configurations/4.storage-config/#misc_configurations","title":"Misc configurations","text":"<p>Caution</p> <p>The configuration <code>snapshot</code> in the following table is different from the snapshot in NebulaGraph. The <code>snapshot</code> here refers to the stock data on the leader when synchronizing Raft.</p> Name Predefined value Description Whether supports runtime dynamic modifications <code>query_concurrently</code> <code>true</code> Whether to turn on multi-threaded queries. Enabling it can improve the latency performance of individual queries, but it will reduce the overall throughput under high pressure. Yes <code>auto_remove_invalid_space</code> <code>true</code> After executing <code>DROP SPACE</code>, the specified graph space will be deleted. This parameter sets whether to delete all the data in the specified graph space at the same time. When the value is <code>true</code>, all the data in the specified graph space will be deleted at the same time. Yes <code>num_io_threads</code> <code>16</code> The number of network I/O threads used to send RPC requests and receive responses. Yes <code>num_max_connections</code> <code>0</code> Max active connections for all networking threads. 0 means no limit.Max connections for each networking thread = num_max_connections / num_netio_threads No <code>num_worker_threads</code> <code>32</code> The number of worker threads for one RPC-based Storage service. Yes <code>max_concurrent_subtasks</code> <code>10</code> The maximum number of concurrent subtasks to be executed by the task manager. Yes <code>snapshot_part_rate_limit</code> <code>10485760</code> The rate limit when the Raft leader synchronizes the stock data with other members of the Raft group. Unit: bytes/s. Yes <code>snapshot_batch_size</code> <code>1048576</code> The amount of data sent in each batch when the Raft leader synchronizes the stock data with other members of the Raft group. Unit: bytes. Yes <code>rebuild_index_part_rate_limit</code> <code>4194304</code> The rate limit when the Raft leader synchronizes the index data rate with other members of the Raft group during the index rebuilding process. Unit: bytes/s. Yes <code>rebuild_index_batch_size</code> <code>1048576</code> The amount of data sent in each batch when the Raft leader synchronizes the index data with other members of the Raft group during the index rebuilding process. Unit: bytes. Yes"},{"location":"5.configurations-and-logs/1.configurations/4.storage-config/#rocksdb_options","title":"RocksDB options","text":"Name Predefined value Description Whether supports runtime dynamic modifications <code>rocksdb_db_options</code> <code>{}</code> Specifies the RocksDB database options. Yes <code>rocksdb_column_family_options</code> <code>{\"write_buffer_size\":\"67108864\",</code><code>\"max_write_buffer_number\":\"4\",</code><code>\"max_bytes_for_level_base\":\"268435456\"}</code> Specifies the RocksDB column family options. Yes <code>rocksdb_block_based_table_options</code> <code>{\"block_size\":\"8192\"}</code> Specifies the RocksDB block based table options. Yes <p>The format of the RocksDB option is <code>{\"&lt;option_name&gt;\":\"&lt;option_value&gt;\"}</code>. Multiple options are separated with commas.</p> <p>Supported options of <code>rocksdb_db_options</code> and <code>rocksdb_column_family_options</code> are listed as follows.</p> <ul> <li><code>rocksdb_db_options</code><pre><code>max_total_wal_size\ndelete_obsolete_files_period_micros\nmax_background_jobs\nstats_dump_period_sec\ncompaction_readahead_size\nwritable_file_max_buffer_size\nbytes_per_sync\nwal_bytes_per_sync\ndelayed_write_rate\navoid_flush_during_shutdown\nmax_open_files\nstats_persist_period_sec\nstats_history_buffer_size\nstrict_bytes_per_sync\nenable_rocksdb_prefix_filtering\nenable_rocksdb_whole_key_filtering\nrocksdb_filtering_prefix_length\nnum_compaction_threads\nrate_limit\n</code></pre> </li> </ul> <ul> <li><code>rocksdb_column_family_options</code><pre><code>write_buffer_size\nmax_write_buffer_number\nlevel0_file_num_compaction_trigger\nlevel0_slowdown_writes_trigger\nlevel0_stop_writes_trigger\ntarget_file_size_base\ntarget_file_size_multiplier\nmax_bytes_for_level_base\nmax_bytes_for_level_multiplier\ndisable_auto_compactions \n</code></pre> </li> </ul> <p>For more information, see RocksDB official documentation.</p>"},{"location":"5.configurations-and-logs/1.configurations/4.storage-config/#black_box_configurations","title":"Black Box configurations","text":"<p>Enterpriseonly</p> <p>The Nebula-BBox configurations are for the Enterprise Edition only.</p> Name Predefined value Description Whether supports runtime dynamic modifications <code>ng_black_box_switch</code> <code>true</code> Whether to enable the Nebula-BBox feature. No <code>ng_black_box_home</code> <code>black_box</code> The name of the directory to store Nebula-BBox file data. No <code>ng_black_box_dump_period_seconds</code> <code>5</code> The time interval for Nebula-BBox to collect metric data. Unit: Second. No <code>ng_black_box_file_lifetime_seconds</code> <code>1800</code> Storage time for Nebula-BBox files generated after collecting metric data. Unit: Second. Yes"},{"location":"5.configurations-and-logs/1.configurations/4.storage-config/#memory_tracker_configurations","title":"Memory Tracker configurations","text":"<p>For details about Memory Tracker, see Memory Tracker: Memory Management Practice in NebulaGraph Database</p> Name Predefined value Description Whether supports runtime dynamic modifications <code>memory_tracker_limit_ratio</code> <code>0.8</code> The value of this parameter can be set to <code>(0, 1]</code>, <code>2</code>, and <code>3</code>.<code>(0, 1]</code>: The percentage of free memory. When the free memory is lower than this value, NebulaGraph stops accepting queries.  Calculated as follows:  <code>Free memory / (Total memory - Reserved memory)</code> Note: For clusters with a mixed-used environment, the value of <code>memory_tracker_limit_ratio</code> should be set to a lower value. For example, when Graphd is expected to occupy only 50% of memory, the value can be set to less than <code>0.5</code>.<code>2</code>: Dynamic Self Adaptive mode. MemoryTracker dynamically adjusts the available memory based on the system's current available memory. Note: This feature is experimental. As memory usage cannot be monitored in real time in dynamic adaptive mode, an OOM error may still occur to handle large memory allocations. <code>3</code>: Disable MemoryTracker. MemoryTracker only logs memory usage and does not interfere with executions even if the limit is exceeded. Yes <code>memory_tracker_untracked_reserved_memory_mb</code> <code>50</code> The reserved memory that is not tracked by the Memory Tracker. Unit: MB. Yes <code>memory_tracker_detail_log</code> <code>false</code> Whether to enable the Memory Tracker log. When the value is <code>true</code>, the Memory Tracker log is generated. Yes <code>memory_tracker_detail_log_interval_ms</code> <code>60000</code> The time interval for generating the Memory Tracker log. Unit: Millisecond. <code>memory_tracker_detail_log</code> is <code>true</code> when this parameter takes effect. Yes <code>memory_purge_enabled</code> <code>true</code> Whether to enable the memory purge feature. When the value is <code>true</code>, the memory purge feature is enabled. Yes <code>memory_purge_interval_seconds</code> <code>10</code> The time interval for the memory purge feature to purge memory. Unit: Second. This parameter only takes effect if <code>memory_purge_enabled</code> is set to true. Yes"},{"location":"5.configurations-and-logs/1.configurations/4.storage-config/#for_super-large_vertices","title":"For super-Large vertices","text":"<p>When the query starting from each vertex gets an edge, truncate it directly to avoid too many neighboring edges on the super-large vertex, because a single query occupies too much hard disk and memory. Or you can truncate a certain number of edges specified in the <code>Max_edge_returned_per_vertex</code> parameter. Excess edges will not be returned. This parameter applies to all spaces.</p> Property name Default value Description Whether supports runtime dynamic modifications max_edge_returned_per_vertex <code>2147483647</code> Specifies the maximum number of edges returned for each dense vertex. Excess edges are truncated and not returned. This parameter is not predefined in the initial configuration files, if you need to use this parameter, add it manually. No"},{"location":"5.configurations-and-logs/1.configurations/4.storage-config/#storage_configurations_for_large_dataset","title":"Storage configurations for large dataset","text":"<p>Warning</p> <p>One graph space takes up at least about 300 MB of memory.</p> <p>When you have a large dataset (in the RocksDB directory) and your memory is tight, we suggest that you set the <code>enable_partitioned_index_filter</code> parameter to <code>true</code>. The performance is affected because RocksDB indexes are cached.</p>"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/","title":"Kernel configurations","text":"<p>This topic introduces the Kernel configurations in Nebula\u00a0Graph.</p>"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#resource_control","title":"Resource control","text":""},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#ulimit_precautions","title":"ulimit precautions","text":"<p>The <code>ulimit</code> command specifies the resource threshold for the current shell session. The precautions are as follows:</p> <ul> <li>The changes made by <code>ulimit</code> only take effect for the current session or child process.</li> </ul> <ul> <li>The resource threshold (soft threshold) cannot exceed the hard threshold.</li> </ul> <ul> <li>Common users cannot use commands to adjust the hard threshold, even with <code>sudo</code>.</li> </ul> <ul> <li>To modify the system level or adjust the hard threshold, edit the file <code>/etc/security/limits.conf</code>. This method requires re-login to take effect.</li> </ul>"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#ulimit_-c","title":"ulimit -c","text":"<p><code>ulimit -c</code> limits the size of the core dumps. We recommend that you set it to <code>unlimited</code>. The command is:</p> <pre><code>ulimit -c unlimited\n</code></pre>"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#ulimit_-n","title":"ulimit -n","text":"<p><code>ulimit -n</code> limits the number of open files. We recommend that you set it to more than 100,000. For example:</p> <pre><code>ulimit -n 130000\n</code></pre>"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#memory","title":"Memory","text":""},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#vmswappiness","title":"vm.swappiness","text":"<p><code>vm.swappiness</code> specifies the percentage of the available memory before starting swap. The greater the value, the more likely the swap occurs. We recommend that you set it to 0. When set to 0, the page cache is removed first. Note that when <code>vm.swappiness</code> is 0, it does not mean that there is no swap.</p>"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#vmmin_free_kbytes","title":"vm.min_free_kbytes","text":"<p><code>vm.min_free_kbytes</code> specifies the minimum number of kilobytes available kept by Linux VM. If you have a large system memory, we recommend that you increase this value. For example, if your physical memory 128GB, set it to 5GB. If the value is not big enough, the system cannot apply for enough continuous physical memory.</p>"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#vmmax_map_count","title":"vm.max_map_count","text":"<p><code>vm.max_map_count</code> limits the maximum number of vma (virtual memory area) for a process. The default value is <code>65530</code>. It is enough for most applications. If your memory application fails because the memory consumption is large, increase the <code>vm.max_map_count</code> value.</p>"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#vmdirty_","title":"vm.dirty_*","text":"<p>These values control the dirty data cache for the system. For write-intensive scenarios, you can make adjustments based on your needs (throughput priority or delay priority). We recommend that you use the system default value.</p>"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#transparent_huge_page","title":"Transparent huge page","text":"<p>For better delay performance, you must run the following commands to disable the transparent huge pages (THP).</p> <pre><code>root# echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled\nroot# echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag\nroot# swapoff -a &amp;&amp; swapon -a\n</code></pre> <p>To prevent THP from being enabled again after the system restarts, you can modify the GRUB configuration file or <code>/etc/rc.local</code> to disable THP automatically upon system startup.</p>"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#networking","title":"Networking","text":""},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#netipv4tcp_slow_start_after_idle","title":"net.ipv4.tcp_slow_start_after_idle","text":"<p>The default value of <code>net.ipv4.tcp_slow_start_after_idle</code> is <code>1</code>. If set, the congestion window is timed out after an idle period. We recommend that you set it to <code>0</code>, especially for long fat scenarios (high latency and large bandwidth).</p>"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#netcoresomaxconn","title":"net.core.somaxconn","text":"<p><code>net.core.somaxconn</code> specifies the maximum number of connection queues listened by the socket. The default value is <code>128</code>. For scenarios with a large number of burst connections, we recommend that you set it to greater than <code>1024</code>.</p>"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#netipv4tcp_max_syn_backlog","title":"net.ipv4.tcp_max_syn_backlog","text":"<p><code>net.ipv4.tcp_max_syn_backlog</code> specifies the maximum number of TCP connections in the SYN_RECV (semi-connected) state. The setting rule for this parameter is the same as that of <code>net.core.somaxconn</code>.</p>"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#netcorenetdev_max_backlog","title":"net.core.netdev_max_backlog","text":"<p><code>net.core.netdev_max_backlog</code> specifies the maximum number of packets. The default value is <code>1000</code>. We recommend that you increase it to greater than <code>10,000</code>, especially for 10G network adapters.</p>"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#netipv4tcp_keepalive_","title":"net.ipv4.tcp_keepalive_*","text":"<p>These values keep parameters alive for TCP connections. For applications that use a 4-layer transparent load balancer, if the idle connection is disconnected unexpectedly, decrease the values of <code>tcp_keepalive_time</code> and <code>tcp_keepalive_intvl</code>.</p>"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#netipv4tcp_rmemwmem","title":"net.ipv4.tcp_rmem/wmem","text":"<p><code>net.ipv4.tcp_wmem/rmem</code> specifies the minimum, default, and maximum size of the buffer pool sent/received by the TCP socket. For long fat links, we recommend that you increase the default value to <code>bandwidth (GB) * RTT (ms)</code>.</p>"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#scheduler","title":"scheduler","text":"<p>For SSD devices, we recommend that you set <code>scheduler</code> to <code>noop</code> or <code>none</code>. The path is <code>/sys/block/DEV_NAME/queue/scheduler</code>.</p>"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#other_parameters","title":"Other parameters","text":""},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#kernelcore_pattern","title":"kernel.core_pattern","text":"<p>we recommend that you set it to <code>core</code> and set <code>kernel.core_uses_pid</code> to <code>1</code>.</p>"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#modify_parameters","title":"Modify parameters","text":""},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#sysctl","title":"sysctl","text":"<ul> <li> <p><code>sysctl &lt;conf_name&gt;</code></p> <p>Checks the current parameter value.</p> </li> </ul> <ul> <li> <p><code>sysctl -w &lt;conf_name&gt;=&lt;value&gt;</code></p> <p>Modifies the parameter value. The modification takes effect immediately. The original value is restored after restarting.</p> </li> </ul> <ul> <li> <p><code>sysctl -p [&lt;file_path&gt;]</code> </p> <p>Loads Linux parameter values \u200b\u200bfrom the specified configuration file. The default path is <code>/etc/sysctl.conf</code>.</p> </li> </ul>"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#prlimit","title":"prlimit","text":"<p>The <code>prlimit</code> command gets and sets process resource limits. You can modify the hard threshold by using it and the <code>sudo</code> command. For example, <code>prlimit --nofile = 130000 --pid = $$</code> adjusts the maximum number of open files permitted by the current process to <code>14000</code>. And the modification takes effect immediately. Note that this command is only available in RedHat 7u or higher versions.</p>"},{"location":"5.configurations-and-logs/2.log-management/audit-log/","title":"Audit logs","text":"<p>The NebulaGraph audit logs store all operations received by graph service in categories, then provide the logs for users to track specific types of operations as needed.</p> <p>Enterpriseonly</p> <p>Only available for the NebulaGraph Enterprise Edition.</p>"},{"location":"5.configurations-and-logs/2.log-management/audit-log/#log_categories","title":"Log categories","text":"Category Statement Description <code>login</code> - Logs the information when the client tries to connect to graph service. <code>exit</code> - Logs the information when the client disconnect from graph service. <code>ddl</code> <code>CREATE SPACE</code>,<code>DROP SPACE</code>,<code>CREATE TAG</code>,<code>DROP TAG</code>,<code>ALTER TAG</code>,<code>DELETE TAG</code>,<code>CREATE EDGE</code>,<code>DROP EDGE</code>,<code>ALTER EDGE</code>,<code>CREATE INDEX</code>,<code>DROP INDEX</code>,<code>CREATE FULLTEXT INDEX</code>,<code>DROP FULLTEXT INDEX</code> Logs the information about DDL statements. <code>dql</code> <code>MATCH</code>,<code>LOOKUP</code>,<code>GO</code>,<code>FETCH</code>,<code>GET SUBGRAPH</code>,<code>FIND PATH</code>,<code>UNWIND</code>,<code>GROUP BY</code>,<code>ORDER BY</code>,<code>YIELD</code>,<code>LIMIT</code>,<code>RETURN</code>,<code>REBUILD INDEX</code>,<code>REBUILD FULLTEXT INDEX</code> Logs the information about DQL statements. <code>dml</code> <code>INSERT VERTEX</code>,<code>DELETE VERTEX</code>,<code>UPDATE VERTEX</code>,<code>UPSERT VERTEX</code>,<code>INSERT EDGE</code>,<code>DELETE EDGE</code>,<code>UPDATE EDGE</code>,<code>UPSERT EDGE</code> Logs the information about DML statements. <code>dcl</code> <code>CREATE USER</code>,<code>GRANT ROLE</code>,<code>REVOKE ROLE</code>,<code>CHANGE PASSWORD</code>,<code>ALTER USER</code>,<code>DROP USER</code>,<code>CREATE SNAPSHOT</code>,<code>DROP SNAPSHOT</code>,<code>ADD LISTENER</code>,<code>REMOVE LISTENER</code>,<code>BALANCE</code>,<code>SUBMIT JOB</code>,<code>STOP JOB</code>,<code>RECOVER JOB</code>,<code>ADD DRAINER</code>,<code>REMOVE DRAINER</code>,<code>SIGN IN DRAINER SERVICE</code>,<code>SIGN OUT DRAINER SERVICE</code>,<code>DOWNLOAD HDFS</code>,<code>INGEST</code> Logs the information about DCL statements. <code>util</code> <code>SHOW HOSTS</code>,<code>SHOW USERS</code>,<code>SHOW ROLES</code>,<code>SHOW SNAPSHOTS</code>,<code>SHOW SPACES</code>,<code>SHOW PARTS</code>,<code>SHOW TAGS</code>,<code>SHOW EDGES</code>,<code>SHOW INDEXES</code>,<code>SHOW CREATE SPACE</code>,<code>SHOW CREATE TAG/EDGE</code>,<code>SHOW CREATE INDEX</code>,<code>SHOW INDEX STATUS</code>,<code>SHOW LISTENER</code>,<code>SHOW TEXT SEARCH CLIENTS</code>,<code>SHOW DRAINER CLIENTS</code>,<code>SHOW FULLTEXT INDEXES</code>,<code>SHOW CONFIGS</code>,<code>SHOW CHARSET</code>,<code>SHOW COLLATION</code>,<code>SHOW STATS</code>,<code>SHOW SESSIONS</code>,<code>SHOW META LEADER</code>,<code>SHOW DRAINERS</code>,<code>SHOW QUERIES</code>,<code>SHOW JOB</code>,<code>SHOW JOBS</code>,<code>DESCRIBE INDEX</code>,<code>DESCRIBE EDGE</code>,<code>DESCRIBE TAG</code>,<code>DESCRIBE SPACE</code>,<code>DESCRIBE USER</code>,<code>USE SPACE</code>,<code>SIGN IN TEXT SERVICE</code>,<code>SIGN OUT TEXT SERVICE</code>,<code>EXPLAIN</code>,<code>PROFILE</code>,<code>KILL QUERY</code> Logs the information about util statements. <code>unknown</code> - Logs the information about unrecognized statements."},{"location":"5.configurations-and-logs/2.log-management/audit-log/#configure_audit_logs","title":"Configure audit logs","text":"<p>You need to configure the graph service file to view audit logs. The default file path of configuration is <code>/usr/local/nebula/etc/nebula-graphd.conf</code>.</p> <p>Note</p> <p>After modifying the configuration, you need to restart the graph service to take effect.</p> <p>Parameter descriptions are as follows:</p> Parameter Predefined value Description <code>enable_audit</code> <code>false</code> Whether or not to enable audit logs. <code>audit_log_handler</code> <code>file</code> Specifies the place where the audit logs will be written. Optional values are <code>file</code> (local file) and <code>es</code> (Elasticsearch). The supported Elasticsearch versions are 7.x and 8.x. <code>audit_log_file</code> <code>./logs/audit/audit.log</code> Takes effect only when <code>audit_log_handler=file</code>. The path for storing audit logs. The value can be absolute or relative. <code>audit_log_strategy</code> <code>synchronous</code> Sets the method to synchronize audit logs. Takes effect only when <code>audit_log_handler=file</code>.  Optional values are <code>asynchronous</code> and <code>synchronous</code>. When <code>asynchronous</code>, log events are cached in memory and do not block the main thread, but may result in missing logs due to insufficient cache. When <code>synchronous</code>, log events are refreshed and synchronized to the file each time. <code>audit_log_max_buffer_size</code> <code>1048576</code> Take effect only when <code>audit_log_handler=file</code> and <code>audit_log_strategy=asynchronous</code>. The size of the memory buffer used for logging. Unit: bytes. <code>audit_log_format</code> <code>xml</code> Takes effect only when <code>audit_log_handler=file</code>. The format of the the audit logs. Optional values are <code>xml</code>, <code>json</code> and <code>csv</code>. <code>audit_log_es_address</code> - Takes effect only when <code>audit_log_handler=es</code>. The address of Elasticsearch server. The format is <code>IP1:port1, IP2:port2, ...</code>. <code>audit_log_es_user</code> - Takes effect only when <code>audit_log_handler=es</code>. The user name of the Elasticsearch. <code>audit_log_es_password</code> - Takes effect only when <code>audit_log_handler=es</code>. The user password of the Elasticsearch. <code>audit_log_es_batch_size</code> <code>1000</code> Takes effect only when <code>audit_log_handler=es</code>. The number of logs sent to Elasticsearch at one time. <code>audit_log_exclude_spaces</code> - The list of spaces for not tracking. Multiple graph spaces are separated by commas. <code>audit_log_categories</code> <code>login,exit</code> The list of log categories for tracking. Multiple categories are separated by commas."},{"location":"5.configurations-and-logs/2.log-management/audit-log/#audit_logs_format","title":"Audit logs format","text":"<p>The fields of audit logs are the same for different handlers and formats. For example, when the audit logs are stored in the default path <code>/usr/local/nebula/logs/audit/audit.log</code> and in the format of XML, the fields in the audit logs are described as follows:</p> <p>Note</p> <p>If the audit log directory is deleted while NebulaGraph is running, the log would not continue to be printed and this operation will not affect the services. To recover the logs, you should restart the services.</p> <pre><code>&lt;AUDIT_RECORD\n  CATEGORY=\"util\"\n  TIMESTAMP=\"2022-04-07 02:31:38\"\n  TERMINAL=\"\"\n  CONNECTION_ID=\"1649298693144580\"\n  CONNECTION_STATUS=\"0\"\n  CONNECTION_MESSAGE=\"\"\n  USER=\"root\"\n  CLIENT_HOST=\"127.0.0.1\"\n  HOST=\"192.168.8.111\"\n  SPACE=\"\"\n  QUERY=\"use basketballplayer1\"\n  QUERY_STATUS=\"-1005\"\n  QUERY_MESSAGE=\"SpaceNotFound: \"\n/&gt;\n&lt;AUDIT_RECORD\n  CATEGORY=\"util\"\n  TIMESTAMP=\"2022-04-07 02:31:39\"\n  TERMINAL=\"\"\n  CONNECTION_ID=\"1649298693144580\"\n  CONNECTION_STATUS=\"0\"\n  CONNECTION_MESSAGE=\"\"\n  USER=\"root\"\n  CLIENT_HOST=\"127.0.0.1\"\n  HOST=\"192.168.8.111\"\n  SPACE=\"\"\n  QUERY=\"use basketballplayer\"\n  QUERY_STATUS=\"0\"\n  QUERY_MESSAGE=\"\"\n/&gt;\n</code></pre> Field Description <code>CATEGORY</code> The category of the audit logs. <code>TIMESTAMP</code> The generation time of the audit logs. <code>TERMINAL</code> The reserved field. <code>CONNECTION_ID</code> The session ID of the connection. <code>CONNECTION_STATUS</code> The status of the connection. <code>0</code> indicates success, and other numbers indicate different error messages. <code>CONNECTION_MESSAGE</code> An error message is displayed when the connection fails. <code>USER</code> The user name of the NebulaGraph connection. <code>CLIENT_HOST</code> The IP address of the client. <code>HOST</code> The IP address of the host. <code>SPACE</code> The graph space where you perform queries. <code>QUERY</code> The query statement. <code>QUERY_STATUS</code> The status of the query. <code>0</code> indicates success, and other numbers indicate different error messages. <code>QUERY_MESSAGE</code> An error message is displayed when the query fails."},{"location":"5.configurations-and-logs/2.log-management/logs/","title":"Runtime logs","text":"<p>Runtime logs are provided for DBAs and developers to locate faults when the system fails.</p> <p>NebulaGraph uses glog to print runtime logs, uses gflags to control the severity level of the log, and provides an HTTP interface to dynamically change the log level at runtime to facilitate tracking.</p>"},{"location":"5.configurations-and-logs/2.log-management/logs/#log_directory","title":"Log directory","text":"<p>The default runtime log directory is <code>/usr/local/nebula/logs/</code>.</p> <p>If the log directory is deleted while NebulaGraph is running, the log would not continue to be printed. However, this operation will not affect the services. To recover the logs, restart the services.</p>"},{"location":"5.configurations-and-logs/2.log-management/logs/#parameter_descriptions","title":"Parameter descriptions","text":"<ul> <li><code>minloglevel</code>: Specifies the minimum level of the log. That is, no logs below this level will be printed. Optional values are <code>0</code> (INFO), <code>1</code> (WARNING), <code>2</code> (ERROR), <code>3</code> (FATAL). It is recommended to set it to <code>0</code> during debugging and <code>1</code> in a production environment. If it is set to <code>4</code>, NebulaGraph will not print any logs.</li> </ul> <ul> <li><code>v</code>: Specifies the detailed level of the log. The larger the value, the more detailed the log is. Optional values are <code>0</code>, <code>1</code>, <code>2</code>, <code>3</code>.</li> </ul> <p>The default severity level for the metad, graphd, and storaged logs can be found in their respective configuration files. The default path is <code>/usr/local/nebula/etc/</code>.</p>"},{"location":"5.configurations-and-logs/2.log-management/logs/#check_the_severity_level","title":"Check the severity level","text":"<p>Check all the flag values (log values included) of the current gflags with the following command.</p> <pre><code>$ curl &lt;ws_ip&gt;:&lt;ws_port&gt;/flags\n</code></pre> Parameter Description <code>ws_ip</code> The IP address for the HTTP service, which can be found in the configuration files above. The default value is <code>127.0.0.1</code>. <code>ws_port</code> The port for the HTTP service, which can be found in the configuration files above. The default values are <code>19559</code>(Meta), <code>19669</code>(Graph), and <code>19779</code>(Storage) respectively. <p>Examples are as follows:</p> <ul> <li>Check the current <code>minloglevel</code> in the Meta service:<pre><code>$ curl 127.0.0.1:19559/flags | grep 'minloglevel'\n</code></pre> </li> </ul> <ul> <li>Check the current <code>v</code> in the Storage service:<pre><code>$ curl 127.0.0.1:19779/flags | grep -w 'v'\n</code></pre> </li> </ul>"},{"location":"5.configurations-and-logs/2.log-management/logs/#change_the_severity_level","title":"Change the severity level","text":"<p>Change the severity level of the log with the following command.</p> <pre><code>$ curl -X PUT -H \"Content-Type: application/json\" -d '{\"&lt;key&gt;\":&lt;value&gt;[,\"&lt;key&gt;\":&lt;value&gt;]}' \"&lt;ws_ip&gt;:&lt;ws_port&gt;/flags\"\n</code></pre> Parameter Description <code>key</code> The type of the log to be changed. For optional values, see Parameter descriptions. <code>value</code> The level of the log. For optional values, see Parameter descriptions. <code>ws_ip</code> The IP address for the HTTP service, which can be found in the configuration files above. The default value is <code>127.0.0.1</code>. <code>ws_port</code> The port for the HTTP service, which can be found in the configuration files above. The default values are <code>19559</code>(Meta), <code>19669</code>(Graph), and <code>19779</code>(Storage) respectively. <p>Examples are as follows:</p> <pre><code>$ curl -X PUT -H \"Content-Type: application/json\" -d '{\"minloglevel\":0,\"v\":3}' \"127.0.0.1:19779/flags\" # storaged\n$ curl -X PUT -H \"Content-Type: application/json\" -d '{\"minloglevel\":0,\"v\":3}' \"127.0.0.1:19669/flags\" # graphd\n$ curl -X PUT -H \"Content-Type: application/json\" -d '{\"minloglevel\":0,\"v\":3}' \"127.0.0.1:19559/flags\" # metad\n</code></pre> <p>If the log level is changed while NebulaGraph is running, it will be restored to the level set in the configuration file after restarting the service. To permanently modify it, see Configuration files.</p>"},{"location":"5.configurations-and-logs/2.log-management/logs/#rocksdb_runtime_logs","title":"RocksDB runtime logs","text":"<p>RocksDB runtime logs are usually used to debug RocksDB parameters and stored in <code>/usr/local/nebula/data/storage/nebula/$id/data/LOG</code>. <code>$id</code> is the ID of the example.</p>"},{"location":"6.monitor-and-metrics/1.query-performance-metrics/","title":"Query NebulaGraph metrics","text":"<p>NebulaGraph supports querying the monitoring metrics through HTTP ports.</p>"},{"location":"6.monitor-and-metrics/1.query-performance-metrics/#metrics_structure","title":"Metrics structure","text":"<p>Each metric of NebulaGraph consists of three fields: name, type, and time range. The fields are separated by periods, for example, <code>num_queries.sum.600</code>. Different NebulaGraph services (Graph, Storage, or Meta) support different metrics. The detailed description is as follows.</p> Field Example Description Metric name <code>num_queries</code> Indicates the function of the metric. Metric type <code>sum</code> Indicates how the metrics are collected. Supported types are SUM, AVG, RATE, and the P-th sample quantiles such as P75, P95, P99, and P99.9. Time range <code>600</code> The time range in seconds for the metric collection. Supported values are 5, 60, 600, and 3600, representing the last 5 seconds, 1 minute, 10 minutes, and 1 hour."},{"location":"6.monitor-and-metrics/1.query-performance-metrics/#query_metrics_over_http","title":"Query metrics over HTTP","text":""},{"location":"6.monitor-and-metrics/1.query-performance-metrics/#syntax","title":"Syntax","text":"<pre><code>curl -G \"http://&lt;ip&gt;:&lt;port&gt;/stats?stats=&lt;metric_name_list&gt; [&amp;format=json]\"\n</code></pre> Parameter Description <code>ip</code> The IP address of the server. You can find it in the configuration file in the installation directory. <code>port</code> The HTTP port of the server. You can find it in the configuration file in the installation directory. The default ports are 19559 (Meta), 19669 (Graph), and 19779 (Storage). <code>metric_name_list</code> The metrics names. Multiple metrics are separated by commas (,). <code>&amp;format=json</code> Optional. Returns the result in the JSON format. <p>Note</p> <p>If NebulaGraph is deployed with Docker Compose, run <code>docker-compose ps</code> to check the ports that are mapped from the service ports inside of the container and then query through them.</p>"},{"location":"6.monitor-and-metrics/1.query-performance-metrics/#query_a_single_metric","title":"Query a single metric","text":"<p>Query the query number in the last 10 minutes in the Graph Service.</p> <pre><code>$ curl -G \"http://192.168.8.40:19669/stats?stats=num_queries.sum.600\"\nnum_queries.sum.600=400\n</code></pre>"},{"location":"6.monitor-and-metrics/1.query-performance-metrics/#query_multiple_metrics","title":"Query multiple metrics","text":"<p>Query the following metrics together:</p> <ul> <li>The average heartbeat latency in the last 1 minute.</li> </ul> <ul> <li> <p>The average latency of the slowest 1% heartbeats, i.e., the P99 heartbeats, in the last 10 minutes.</p> <pre><code>$ curl -G \"http://192.168.8.40:19559/stats?stats=heartbeat_latency_us.avg.60,heartbeat_latency_us.p99.600\"\nheartbeat_latency_us.avg.60=281\nheartbeat_latency_us.p99.600=985\n</code></pre> </li> </ul>"},{"location":"6.monitor-and-metrics/1.query-performance-metrics/#return_a_json_result","title":"Return a JSON result.","text":"<p>Query the number of new vertices in the Storage Service in the last 10 minutes and return the result in the JSON format.</p> <pre><code>$ curl -G \"http://192.168.8.40:19779/stats?stats=num_add_vertices.sum.600&amp;format=json\"\n[{\"value\":1,\"name\":\"num_add_vertices.sum.600\"}]\n</code></pre>"},{"location":"6.monitor-and-metrics/1.query-performance-metrics/#query_all_metrics_in_a_service","title":"Query all metrics in a service.","text":"<p>If no metric is specified in the query, NebulaGraph returns all metrics in the service.</p> <pre><code>$ curl -G \"http://192.168.8.40:19559/stats\"\nheartbeat_latency_us.avg.5=304\nheartbeat_latency_us.avg.60=308\nheartbeat_latency_us.avg.600=299\nheartbeat_latency_us.avg.3600=285\nheartbeat_latency_us.p75.5=652\nheartbeat_latency_us.p75.60=669\nheartbeat_latency_us.p75.600=651\nheartbeat_latency_us.p75.3600=642\nheartbeat_latency_us.p95.5=930\nheartbeat_latency_us.p95.60=963\nheartbeat_latency_us.p95.600=933\nheartbeat_latency_us.p95.3600=929\nheartbeat_latency_us.p99.5=986\nheartbeat_latency_us.p99.60=1409\nheartbeat_latency_us.p99.600=989\nheartbeat_latency_us.p99.3600=986\nnum_heartbeats.rate.5=0\nnum_heartbeats.rate.60=0\nnum_heartbeats.rate.600=0\nnum_heartbeats.rate.3600=0\nnum_heartbeats.sum.5=2\nnum_heartbeats.sum.60=40\nnum_heartbeats.sum.600=394\nnum_heartbeats.sum.3600=2364\n...\n</code></pre>"},{"location":"6.monitor-and-metrics/1.query-performance-metrics/#space-level_metrics","title":"Space-level metrics","text":"<p>The Graph service supports a set of space-level metrics that record the information of different graph spaces separately.</p> <p>Space-level metrics can be queried only by querying all metrics. For example, run <code>curl -G \"http://192.168.8.40:19559/stats\"</code> to show all metrics. The returned result contains the graph space name in the form of '{space=space_name}', such as <code>num_active_queries{space=basketballplayer}.sum.5=0</code>.</p> <p>Caution</p> <p>To enable space-level metrics, set the value of <code>enable_space_level_metrics</code> to <code>true</code> in the Graph service configuration file before starting NebulaGraph. For details about how to modify the configuration, see Configuration Management.</p>"},{"location":"6.monitor-and-metrics/1.query-performance-metrics/#metric_description","title":"Metric description","text":""},{"location":"6.monitor-and-metrics/1.query-performance-metrics/#graph","title":"Graph","text":"Parameter Description <code>num_active_queries</code> The number of changes in the number of active queries. Formula: The number of started queries minus the number of finished queries within a specified time. <code>num_active_sessions</code> The number of changes in the number of active sessions. Formula: The number of logged in sessions minus the number of logged out sessions within a specified time.For example, when querying <code>num_active_sessions.sum.5</code>, if there were 10 sessions logged in and 30 sessions logged out in the last 5 seconds, the value of this metric is <code>-20</code> (10-30). <code>num_aggregate_executors</code> The number of executions for the Aggregation operator. <code>num_auth_failed_sessions_bad_username_password</code> The number of sessions where authentication failed due to incorrect username and password. <code>num_auth_failed_sessions_out_of_max_allowed</code> The number of sessions that failed to authenticate logins because the value of the parameter <code>FLAG_OUT_OF_MAX_ALLOWED_CONNECTIONS</code> was exceeded. <code>num_auth_failed_sessions</code> The number of sessions in which login authentication failed. <code>num_indexscan_executors</code> The number of executions for index scan operators. <code>num_killed_queries</code> The number of killed queries. <code>num_opened_sessions</code> The number of sessions connected to the server. <code>num_queries</code> The number of queries. <code>num_query_errors_leader_changes</code> The number of the raft leader changes due to query errors. <code>num_query_errors</code> The number of query errors. <code>num_reclaimed_expired_sessions</code> The number of expired sessions actively reclaimed by the server. <code>num_rpc_sent_to_metad_failed</code> The number of failed RPC requests that the Graphd service sent to the Metad service. <code>num_rpc_sent_to_metad</code> The number of RPC requests that the Graphd service sent to the Metad service. <code>num_rpc_sent_to_storaged_failed</code> The number of failed RPC requests that the Graphd service sent to the Storaged service. <code>num_rpc_sent_to_storaged</code> The number of RPC requests that the Graphd service sent to the Storaged service. <code>num_sentences</code> The number of statements received by the Graphd service. <code>num_slow_queries</code> The number of slow queries. <code>num_sort_executors</code> The number of executions for the Sort operator. <code>optimizer_latency_us</code> The latency of executing optimizer statements. <code>query_latency_us</code> The latency of queries. <code>slow_query_latency_us</code> The latency of slow queries. <code>num_queries_hit_memory_watermark</code> The number of queries reached the memory watermark."},{"location":"6.monitor-and-metrics/1.query-performance-metrics/#meta","title":"Meta","text":"Parameter Description <code>commit_log_latency_us</code> The latency of committing logs in Raft. <code>commit_snapshot_latency_us</code> The latency of committing snapshots in Raft. <code>heartbeat_latency_us</code> The latency of heartbeats. <code>num_heartbeats</code> The number of heartbeats. <code>num_raft_votes</code> The number of votes in Raft. <code>transfer_leader_latency_us</code> The latency of transferring the raft leader. <code>num_agent_heartbeats</code> The number of heartbeats for the AgentHBProcessor. <code>agent_heartbeat_latency_us</code> The latency of the AgentHBProcessor. <code>replicate_log_latency_us</code> The latency of replicating the log record to most nodes by Raft. <code>num_send_snapshot</code> The number of times that Raft sends snapshots to other nodes. <code>append_log_latency_us</code> The latency of replicating the log record to a single node by Raft. <code>append_wal_latency_us</code> The Raft write latency for a single WAL. <code>num_grant_votes</code> The number of times that Raft votes for other nodes. <code>num_start_elect</code> The number of times that Raft starts an election."},{"location":"6.monitor-and-metrics/1.query-performance-metrics/#storage","title":"Storage","text":"Parameter Description <code>add_edges_latency_us</code> The latency of adding edges. <code>add_vertices_latency_us</code> The latency of adding vertices. <code>commit_log_latency_us</code> The latency of committing logs in Raft. <code>commit_snapshot_latency_us</code> The latency of committing snapshots in Raft. <code>delete_edges_latency_us</code> The latency of deleting edges. <code>delete_vertices_latency_us</code> The latency of deleting vertices. <code>get_neighbors_latency_us</code> The latency of querying neighbor vertices. <code>get_dst_by_src_latency_us</code> The latency of querying the destination vertex by the source vertex. <code>num_get_prop</code> The number of executions for the GetPropProcessor. <code>num_get_neighbors_errors</code> The number of execution errors for the GetNeighborsProcessor. <code>num_get_dst_by_src_errors</code> The number of execution errors for the GetDstBySrcProcessor. <code>get_prop_latency_us</code> The latency of executions for the GetPropProcessor. <code>num_edges_deleted</code> The number of deleted edges. <code>num_edges_inserted</code> The number of inserted edges. <code>num_raft_votes</code> The number of votes in Raft. <code>num_rpc_sent_to_metad_failed</code> The number of failed RPC requests that the Storage service sent to the Meta service. <code>num_rpc_sent_to_metad</code> The number of RPC requests that the Storaged service sent to the Metad service. <code>num_tags_deleted</code> The number of deleted tags. <code>num_vertices_deleted</code> The number of deleted vertices. <code>num_vertices_inserted</code> The number of inserted vertices. <code>transfer_leader_latency_us</code> The latency of transferring the raft leader. <code>lookup_latency_us</code> The latency of executions for the LookupProcessor. <code>num_lookup_errors</code> The number of execution errors for the LookupProcessor. <code>num_scan_vertex</code> The number of executions for the ScanVertexProcessor. <code>num_scan_vertex_errors</code> The number of execution errors for the ScanVertexProcessor. <code>update_edge_latency_us</code> The latency of executions for the UpdateEdgeProcessor. <code>num_update_vertex</code> The number of executions for the UpdateVertexProcessor. <code>num_update_vertex_errors</code> The number of execution errors for the UpdateVertexProcessor. <code>kv_get_latency_us</code> The latency of executions for the Getprocessor. <code>kv_put_latency_us</code> The latency of executions for the PutProcessor. <code>kv_remove_latency_us</code> The latency of executions for the RemoveProcessor. <code>num_kv_get_errors</code> The number of execution errors for the GetProcessor. <code>num_kv_get</code> The number of executions for the GetProcessor. <code>num_kv_put_errors</code> The number of execution errors for the PutProcessor. <code>num_kv_put</code> The number of executions for the PutProcessor. <code>num_kv_remove_errors</code> The number of execution errors for the RemoveProcessor. <code>num_kv_remove</code> The number of executions for the RemoveProcessor. <code>forward_tranx_latency_us</code> The latency of transmission. <code>scan_edge_latency_us</code> The latency of executions for the ScanEdgeProcessor. <code>num_scan_edge_errors</code> The number of execution errors for the ScanEdgeProcessor. <code>num_scan_edge</code> The number of executions for the ScanEdgeProcessor. <code>scan_vertex_latency_us</code> The latency of executions for the ScanVertexProcessor. <code>num_add_edges</code> The number of times that edges are added. <code>num_add_edges_errors</code> The number of errors when adding edges. <code>num_add_vertices</code> The number of times that vertices are added. <code>num_start_elect</code> The number of times that Raft starts an election. <code>num_add_vertices_errors</code> The number of errors when adding vertices. <code>num_delete_vertices_errors</code> The number of errors when deleting vertices. <code>append_log_latency_us</code> The latency of replicating the log record to a single node by Raft. <code>num_grant_votes</code> The number of times that Raft votes for other nodes. <code>replicate_log_latency_us</code> The latency of replicating the log record to most nodes by Raft. <code>num_delete_tags</code> The number of times that tags are deleted. <code>num_delete_tags_errors</code> The number of errors when deleting tags. <code>num_delete_edges</code> The number of edge deletions. <code>num_delete_edges_errors</code> The number of errors when deleting edges <code>num_send_snapshot</code> The number of times that snapshots are sent. <code>update_vertex_latency_us</code> The latency of executions for the UpdateVertexProcessor. <code>append_wal_latency_us</code> The Raft write latency for a single WAL. <code>num_update_edge</code> The number of executions for the UpdateEdgeProcessor. <code>delete_tags_latency_us</code> The latency of deleting tags. <code>num_update_edge_errors</code> The number of execution errors for the UpdateEdgeProcessor. <code>num_get_neighbors</code> The number of executions for the GetNeighborsProcessor. <code>num_get_dst_by_src</code> The number of executions for the GetDstBySrcProcessor. <code>num_get_prop_errors</code> The number of execution errors for the GetPropProcessor. <code>num_delete_vertices</code> The number of times that vertices are deleted. <code>num_lookup</code> The number of executions for the LookupProcessor. <code>num_sync_data</code> The number of times the Storage service synchronizes data from the Drainer. <code>num_sync_data_errors</code> The number of errors that occur when the Storage service synchronizes data from the Drainer. <code>sync_data_latency_us</code> The latency of the Storage service synchronizing data from the Drainer."},{"location":"6.monitor-and-metrics/1.query-performance-metrics/#graph_space","title":"Graph space","text":"<p>Note</p> <p>Space-level metrics are created dynamically, so that only when the behavior is triggered in the graph space, the corresponding metric is created and can be queried by the user.</p> Parameter Description <code>num_active_queries</code> The number of queries currently being executed. <code>num_queries</code> The number of queries. <code>num_sentences</code> The number of statements received by the Graphd service. <code>optimizer_latency_us</code> The latency of executing optimizer statements. <code>query_latency_us</code> The latency of queries. <code>num_slow_queries</code> The number of slow queries. <code>num_query_errors</code> The number of query errors. <code>num_query_errors_leader_changes</code> The number of raft leader changes due to query errors. <code>num_killed_queries</code> The number of killed queries. <code>num_aggregate_executors</code> The number of executions for the Aggregation operator. <code>num_sort_executors</code> The number of executions for the Sort operator. <code>num_indexscan_executors</code> The number of executions for index scan operators. <code>num_auth_failed_sessions_bad_username_password</code> The number of sessions where authentication failed due to incorrect username and password. <code>num_auth_failed_sessions</code> The number of sessions in which login authentication failed. <code>num_opened_sessions</code> The number of sessions connected to the server. <code>num_queries_hit_memory_watermark</code> The number of queries reached the memory watermark. <code>num_reclaimed_expired_sessions</code> The number of expired sessions actively reclaimed by the server. <code>num_rpc_sent_to_metad_failed</code> The number of failed RPC requests that the Graphd service sent to the Metad service. <code>num_rpc_sent_to_metad</code> The number of RPC requests that the Graphd service sent to the Metad service. <code>num_rpc_sent_to_storaged_failed</code> The number of failed RPC requests that the Graphd service sent to the Storaged service. <code>num_rpc_sent_to_storaged</code> The number of RPC requests that the Graphd service sent to the Storaged service. <code>slow_query_latency_us</code> The latency of slow queries."},{"location":"6.monitor-and-metrics/1.query-performance-metrics/#single_process_metrics","title":"Single process metrics","text":"<p>Graph, Meta, and Storage services all have their own single process metrics.</p> Parameter Description <code>context_switches_total</code> The number of context switches. <code>cpu_seconds_total</code> The CPU usage based on user and system time. <code>memory_bytes_gauge</code> The number of bytes of memory used. <code>open_filedesc_gauge</code> The number of file descriptors. <code>read_bytes_total</code> The number of bytes read. <code>write_bytes_total</code> The number of bytes written."},{"location":"6.monitor-and-metrics/2.rocksdb-statistics/","title":"RocksDB statistics","text":"<p>NebulaGraph uses RocksDB as the underlying storage. This topic describes how to collect and show the RocksDB statistics of NebulaGraph.</p>"},{"location":"6.monitor-and-metrics/2.rocksdb-statistics/#enable_rocksdb","title":"Enable RocksDB","text":"<p>By default, the function of RocksDB statistics is disabled. To enable RocksDB statistics, you need to:</p> <ol> <li> <p>Modify the <code>--enable_rocksdb_statistics</code> parameter as <code>true</code> in the <code>nebula-storaged.conf</code> file. The default path of the configuration file is <code>/use/local/nebula/etc</code>.</p> </li> <li> <p>Restart the service to make the modification valid.</p> </li> </ol>"},{"location":"6.monitor-and-metrics/2.rocksdb-statistics/#get_rocksdb_statistics","title":"Get RocksDB statistics","text":"<p>Users can use the built-in HTTP service in the storage service to get the following types of statistics. Results in the JSON format are supported.</p> <ul> <li>All RocksDB statistics.</li> <li>Specified RocksDB statistics.</li> </ul>"},{"location":"6.monitor-and-metrics/2.rocksdb-statistics/#examples","title":"Examples","text":"<p>Use the following command to get all RocksDB statistics: <pre><code>curl -L \"http://${storage_ip}:${port}/rocksdb_stats\"\n</code></pre></p> <p>For example: <pre><code>curl -L \"http://172.28.2.1:19779/rocksdb_stats\"\n\nrocksdb.blobdb.blob.file.bytes.read=0\nrocksdb.blobdb.blob.file.bytes.written=0\nrocksdb.blobdb.blob.file.bytes.synced=0\n...\n</code></pre></p> <p>Use the following command to get specified RocksDB statistics: <pre><code>curl -L \"http://${storage_ip}:${port}/rocksdb_stats?stats=${stats_name}\"\n</code></pre></p> <p>For example, use the following command to get the information of <code>rocksdb.bytes.read</code> and <code>rocksdb.block.cache.add</code>. <pre><code>curl -L \"http://172.28.2.1:19779/rocksdb_stats?stats=rocksdb.bytes.read,rocksdb.block.cache.add\"\n\nrocksdb.block.cache.add=14\nrocksdb.bytes.read=1632\n</code></pre></p> <p>Use the following command to get specified RocksDB statistics in the JSON format: <pre><code>curl -L \"http://${storage_ip}:${port}/rocksdb_stats?stats=${stats_name}&amp;format=json\"\n</code></pre></p> <p>For example, use the following command to get the information of <code>rocksdb.bytes.read</code> and <code>rocksdb.block.cache.add</code> and return the results in the JSON format. <pre><code>curl -L \"http://172.28.2.1:19779/rocksdb_stats?stats=rocksdb.bytes.read,rocksdb.block.cache.add&amp;format=json\"\n\n[\n  {\n    \"rocksdb.block.cache.add\": 1\n  },\n  {\n    \"rocksdb.bytes.read\": 160\n  }\n]\n</code></pre></p>"},{"location":"6.monitor-and-metrics/3.bbox/3.1.bbox/","title":"What is black-box monitoring","text":"<p>NebulaGraph comes with a black-box monitoring feature that collects and archives data on the operating system and service metrics on a regular basis. When the NebulaGraph service fails, it helps you quickly locate the problem and analyze the cause without a direct network connection.</p> <p>Enterpriseoly</p> <p>The black-box monitoring feature is for the NebulaGraph Enterprise Edition only.</p>"},{"location":"6.monitor-and-metrics/3.bbox/3.1.bbox/#note","title":"Note","text":"<p>Black-box monitoring operates as a set of background processes on the server and collects metric data regularly. Currently, only operating system performance metrics are collected (e.g., CPU, Memory, Network IO, and other related metrics). In the future, we will support collecting NebulaGraph service-related metrics. For the description of metrics, see PROC.</p>"},{"location":"6.monitor-and-metrics/3.bbox/3.1.bbox/#enable_black-box_monitoring","title":"Enable black-box monitoring","text":"<p>The black-box monitoring feature is turned on by default. The <code>black-box</code> directory is created and stored in the NebulaGraph installation directory the first time NebulaGraph is started. Files of collected black-box monitoring data are stored in that directory.</p> <p>You can disable the black-box monitoring by setting the related parameters in the Black box configurations section of the configuration files of all NebulaGraph services. For details about service configurations, see Configurations.</p>"},{"location":"6.monitor-and-metrics/3.bbox/3.1.bbox/#black-box_monitoring_files","title":"Black-box monitoring files","text":""},{"location":"6.monitor-and-metrics/3.bbox/3.1.bbox/#directory_structure","title":"Directory structure","text":"<p>In the <code>black_box</code> directory, the system automatically creates sub-directories. Sub-directories are named with the corresponding process number of each NebulaGraph service running on the current machine. In each sub-directory, by default, a binary file is generated every 5 seconds to record the OS performance metric data during this time. The file name is in the format of <code>black_box.{timestamp_id}.log</code>. <code>timestamp_id</code> is the timestamp when the file is generated.</p> <pre><code>\u251c\u2500\u2500 5776 # The service process number \n\u2502   \u251c\u2500\u2500 black_box.1665472900.log # Black-box monitoring file\n\u2502   \u251c\u2500\u2500 black_box.1665472905.log\n\u2502   ...\n\u251c\u2500\u2500 5787\n\u2502   \u251c\u2500\u2500 black_box.1665728450.log\n\u2502   \u251c\u2500\u2500 black_box.1665728455.log\n\u2502   ...\n...\n</code></pre> <p>Each black-box monitoring file has 30 minutes (1800 seconds) of storage by default. Files stored for more than 30 minutes will be automatically deleted.</p> <p>The generation interval and storage time of black-box monitoring files can be configured in the Black box configuration section of the configuration file of each NebulaGraph service. For configuration file details, see Configurations.</p>"},{"location":"6.monitor-and-metrics/3.bbox/3.1.bbox/#viewing_black-box_monitoring_files","title":"Viewing black-box monitoring files","text":"<p>To view the black-box binaries you need to use the NebulaGraph Black Box tool, which can also be used to convert the binaries to CSV files and export them for viewing. For using the NebulaGraph Black Box tool, see Black Box tool Nebula-BBox.</p>"},{"location":"6.monitor-and-metrics/3.bbox/3.2.bbox-reviewer/","title":"Black-box monitoring tool - NebulaGraph Black Box","text":"<p>The black-box monitoring tool NebulaGraph Black Box (short for Nebula-BBox) helps you view black-box monitoring data. This topic introduces how to use Nebula-BBox in Linux.</p> <p>Enterpriseonly</p> <p>Nebula-BBox is only available for the NebulaGraph Enterprise Edition.</p>"},{"location":"6.monitor-and-metrics/3.bbox/3.2.bbox-reviewer/#nebula-bbox_features","title":"Nebula-BBox features","text":"<p>Nebula-BBox provides the following features:</p> <ul> <li>View monitoring metric data via TUI, Terminal User Interface.</li> <li>Export data as CSV files.</li> <li>View data in different dimensions.<ul> <li>View data for one or more metrics.</li> <li>View data for a certain time.</li> <li>View data from one or more directories or files, or mixed.</li> </ul> </li> <li>Support Linux, macOS, and Windows systems.</li> </ul>"},{"location":"6.monitor-and-metrics/3.bbox/3.2.bbox-reviewer/#version_compatibility","title":"Version compatibility","text":"<p>The version correspondence between NebulaGraph and Nebula-BBox is as follows.</p> NebulaGraph Nebula-BBox 3.5.0 3.5.0"},{"location":"6.monitor-and-metrics/3.bbox/3.2.bbox-reviewer/#deploy_nebula-bbox","title":"Deploy Nebula-BBox","text":"<p>You can deploy Nebula-BBox with RPM, DEB, or TAR packages, or with Docker. The following example uses RPM packages.</p> <ol> <li> <p>Obtain an RPM package.</p> <p>Enterpriseonly</p> <p>Contact us to get the Nebula-BBox installation package.</p> </li> <li> <p>Run <code>sudo rpm -i &lt;rpm&gt;</code> to install the package. For example:</p> <pre><code>sudo rpm -i nebula-bbox-&lt;version&gt;.x86_64.rpm\n</code></pre> <p>Nebula-BBox is installed in the default path <code>/usr/bin/</code> in the form of a binary file <code>nebula-bbox</code>.</p> </li> </ol>"},{"location":"6.monitor-and-metrics/3.bbox/3.2.bbox-reviewer/#use_nebula-bbox","title":"Use Nebula-BBox","text":"<p>Run <code>nebula-bbox -h/--help</code> to view the available commands.</p> <p>Caution</p> <p>For Nebula-BBox installed in a non-default path (default path is <code>/usr/bin/</code>), when executing <code>nebula-bbox</code> related commands, it is necessary to specify the installation path of Nebula-BBox. For example, if Nebula-BBox is installed in <code>/usr/bbox</code>, then you need to execute <code>/usr/bbox/nebula-bbox -h</code>.</p>"},{"location":"6.monitor-and-metrics/3.bbox/3.2.bbox-reviewer/#view_nebula-bbox_version","title":"View Nebula-BBox version","text":"<p>Run <code>nebula-bbox version</code> to view the version information of Nebula-BBox.</p>"},{"location":"6.monitor-and-metrics/3.bbox/3.2.bbox-reviewer/#view_black-box_monitoring_metrics","title":"View black-box monitoring metrics","text":"<p>Run <code>nebula-bbox metrics</code> to view all the metrics collected by Nebula-BBox. For details about the description of metrics, see PROC(5).</p>"},{"location":"6.monitor-and-metrics/3.bbox/3.2.bbox-reviewer/#view_black-box_monitoring_data","title":"View black-box monitoring data","text":"<p>You can use Nebula-BBox to view black-box monitoring file data. The syntax is as follows:</p> <pre><code>nebula-bbox view [(-o|--output=)tui|csv] [--metrics name[,name ...]] [flags] (FILE | DIRECTORY ...)\n</code></pre>"},{"location":"6.monitor-and-metrics/3.bbox/3.2.bbox-reviewer/#parameters","title":"Parameters","text":"Parameter Description <code>-o</code>\uff0c<code>--output</code> Specifies the output display format. Optional values are <code>tui</code> and <code>csv</code>. The default value is <code>tui</code> when this parameter is not specified. <code>tui</code>: The output display format is TUI, Terminal User Interface. <code>csv</code>: The output display format is a CSV file. <code>--metrics</code> Specifies one or multiple metrics. Optional values can be the metrics returned by running <code>nebula-bbox metrics</code>, and all metrics are displayed when this parameter is not specified.When specifying multiple metrics, separate them with commas, for example <code>--metrics &lt;name&gt;,&lt;name&gt;...</code>. <code>flags</code> You can specify other parameters: <code>--output-file</code>: When the value of <code>-o</code> or <code>--output</code> is <code>csv</code>, you need to specify it to define the storage path and file name of the CSV file.<code>--start-time</code>: View metric data from the defined start time to the current time.<code>--end-time</code>: Defines the end time to view data collected during a period, used with <code>--start-time</code>.<code>--duration</code>: Defines a duration to view data collected during the duration, used with <code>--start-time</code>, not used with <code>--end-time</code>."},{"location":"6.monitor-and-metrics/3.bbox/3.2.bbox-reviewer/#examples","title":"Examples","text":"<p>The following examples assume that the NebulaGraph cluster is installed in the default path <code>/usr/local/nebula/</code> and the black-box monitoring data is stored in the default path <code>/usr/local/nebula/data/bbox/</code>.</p> <p>Note</p> <p>Before you want to specify one or multiple metrics, run <code>nebula-bbox metrics</code> to view all the metrics that you can specify. For details, see the context above.</p> Cases Commands View the data of all metrics by specifying a single file. <code>nebula-bbox view /usr/local/nebula/black_box/&lt;pid&gt;/black_box.&lt;timestamp&gt;.log</code> View the data of all metrics by specifying multiple files. <code>nebula-bbox view /usr/local/nebula/black_box/&lt;pid&gt;/black_box.&lt;timestamp1&gt;.log /usr/local/nebula/black_box/&lt;pid&gt;/black_box.&lt;timestamp2&gt;.log</code> View the data of all metrics by specifying all files. <code>nebula-bbox view /usr/local/nebula/black_box</code> View the data of all metrics by specifying multiple subdirectories. <code>nebula-bbox view /usr/local/nebula/black_box/&lt;pid1&gt; /usr/local/nebula/black_box/&lt;pid2&gt;</code> View the data of all metrics by specifying a subdirectory and a single file. <code>nebula-bbox view /usr/local/nebula/black_box/&lt;pid1&gt; /usr/local/nebula/black_box/&lt;pid2&gt; /usr/local/nebula/black_box/&lt;pid3&gt;/black_box.&lt;timestamp&gt;.log</code> View the data of a specified metric in a single file. <code>nebula-bbox view --metrics &lt;name&gt; /usr/local/nebula/black_box/&lt;pid&gt;/black_box.&lt;timestamp&gt;.log</code> View the data of a specified metric of all files. <code>nebula-bbox view --metrics &lt;name&gt; /usr/local/nebula/black_box</code> View the data of specified metrics of all files in the form of a CSV file. <code>nebula-bbox view --metrics &lt;name1&gt;[,&lt;name2&gt; ...] --output csv --output-file &lt;csv_filename&gt;.csv /usr/local/nebula/black_box</code> View the data of multiple specified metrics of all files. <code>nebula-bbox view --metrics &lt;name1&gt;[,&lt;name2&gt; ...] /usr/local/nebula/black_box</code> View the data of all metrics of all files from noon September 6, 2022, Beijing time until now. <code>nebula-bbox view --start-time \"Tue, 06 Sep 2022 12:00:00 +0800\" /usr/local/nebula/black_box</code>The replacement of <code>Tue, 06 Sep 2022 12:00:00 +0800</code> can be <code>2022-09-06T12:00:00+08:00</code> and <code>2022-09-06 04:00:00 +0800</code>. View the data of all metrics of all files within one hour starting from noon September 6, 2022, Beijing time. <code>nebula-bbox view --start-time \"Tue, 06 Sep 2022 12:00:00 +0800\" --duration 1h /usr/local/nebula/black_box</code>. You can use <code>h</code>\u3001<code>m</code>\u3001<code>s</code> to specify a duration. View the data of all metrics of all files from noon September 6, 2022, Beijing time to 13:00 September 6, 2022, Beijing time. <code>nebula-bbox view --start-time \"2022-09-06 04:00:00 +0800\" --end-time \"2022-09-06 05:00:00 +0800\" /usr/local/nebula/black_box</code>"},{"location":"6.monitor-and-metrics/3.bbox/3.2.bbox-reviewer/#tui_mode_and_shortcuts","title":"TUI mode and shortcuts","text":"<p>The TUI mode displays monitoring data in the form of a table. The first line of the table shows the time, service PID based on which metrics are collected, service name, and metric names.</p> <p></p> <p>You can use the following shortcuts to view data in TUI mode.</p> Shortcut Description <code>F1</code> Displays help. <code>Left</code> Move left. <code>Right</code> Move right. <code>Down</code> Move down. <code>Up</code> Move up. <code>Ctrl-A</code> or <code>Home</code> Jump to the first column of the current line. <code>Ctrl-E</code> or <code>End</code> Jump to the last column of the current line. <code>Ctrl-T</code> Jump to the first line. <code>Ctrl-B</code> Jump to the last line. <code>Enter</code> View the detailed information in a table cell. <code>Escape</code> Quit displaying cell details."},{"location":"6.monitor-and-metrics/3.bbox/3.2.bbox-reviewer/#faq","title":"FAQ","text":"<p>Q: Why does my TUI interface display as follows?</p> <p></p> <p>A: The situation shown above is due to a mismatch in the Linux system character set. Run <code>export LC_CTYPE=\"en_US.UTF-8\"</code> to solve the problem.</p>"},{"location":"7.data-security/4.ssl/","title":"SSL encryption","text":"<p>NebulaGraph supports data transmission with SSL encryption between clients, the Graph service, the Meta service, and the Storage service. This topic describes how to enable SSL encryption.</p>"},{"location":"7.data-security/4.ssl/#precaution","title":"Precaution","text":"<p>Enabling SSL encryption will slightly affect the performance, such as causing operation latency.</p>"},{"location":"7.data-security/4.ssl/#parameters","title":"Parameters","text":"Parameter Default value Description <code>cert_path</code> - The path to the PEM certification. <code>key_path</code> - The path to the key certification. <code>password_path</code> - The path to the password file certification. <code>ca_path</code> - The path to the trusted CA file. <code>enable_ssl</code> <code>false</code> Whether to enable SSL encryption. <code>enable_graph_ssl</code> <code>false</code> Whether to enable SSL encryption in the Graph service only. <code>enable_meta_ssl</code> <code>false</code> Whether to enable SSL encryption in the Meta service only."},{"location":"7.data-security/4.ssl/#certificate_modes","title":"Certificate modes","text":"<p>To use SSL encryption, SSL certificates are required. NebulaGraph supports two certificate modes.</p> <ul> <li> <p>Self-signed certificate mode</p> <p>In this mode, users need to make the signed certificate by themselves and set <code>cert_path</code>, <code>key_path</code>, and <code>password_path</code> in the corresponding file according to encryption policies.</p> </li> </ul> <ul> <li> <p>CA-signed certificate mode</p> <p>In this mode, users need to apply for the signed certificate from a certificate authority and set <code>cert_path</code>, <code>key_path</code>, and <code>password_path</code> in the corresponding file according to encryption policies.</p> </li> </ul>"},{"location":"7.data-security/4.ssl/#encryption_policies","title":"Encryption policies","text":"<p>NebulaGraph supports three encryption policies. For details, see Usage explanation.</p> <ul> <li> <p>Encrypt the data transmission between clients, the Graph service, the Meta service, and the Storage service.</p> <p>Add <code>enable_ssl = true</code> to the configuration files of <code>nebula-graphd.conf</code>, <code>nebula-metad.conf</code>, and <code>nebula-storaged.conf</code>.</p> </li> </ul> <ul> <li> <p>Encrypt the data transmission between clients and the Graph service.</p> <p>This policy applies to the case that the clusters are set in the same server room. Only the port of the Graph service is open to the outside because other services can communicate over the internal network without encryption. Add <code>enable_graph_ssl = true</code> to the configuration file of <code>nebula-graphd.conf</code>.</p> </li> </ul> <ul> <li> <p>Encrypt the data transmission related to the Meta service in the cluster.</p> <p>This policy applies to transporting classified information to the Meta service. Add <code>enable_meta_ssl = true</code> to the configuration files of <code>nebula-graphd.conf</code>, <code>nebula-metad.conf</code>, and <code>nebula-storaged.conf</code>.</p> </li> </ul>"},{"location":"7.data-security/4.ssl/#steps","title":"Steps","text":"<ol> <li> <p>Ensure the certificate mode and the encryption policy.</p> </li> <li> <p>Add the certificate configuration and the policy configuration in corresponding files.</p> <p>For example, the three configuration files need to be set as follows when using a self-signed certificate and encrypt data transmission between clients, the Graph service, the Meta service, and the Storage service.</p> <pre><code>--cert_path=xxxxxx\n--key_path=xxxxx\n--password_path=xxxxxx\n--enable_ssl=true\n</code></pre> </li> <li> <p>Set the SSL and the trusted CA in clients. For code examples, see nebula-test-run.py.</p> </li> </ol>"},{"location":"7.data-security/1.authentication/1.authentication/","title":"Authentication","text":"<p>NebulaGraph replies on local authentication or LDAP authentication to implement access control.</p> <p>NebulaGraph creates a session when a client connects to it. The session stores information about the connection, including the user information. If the authentication system is enabled, the session will be mapped to corresponding users.</p> <p>Note</p> <p>By default, the authentication is disabled and NebulaGraph allows connections with the username <code>root</code> and any password.</p> <p>NebulaGraph supports local authentication and LDAP authentication.</p>"},{"location":"7.data-security/1.authentication/1.authentication/#local_authentication","title":"Local authentication","text":"<p>Local authentication indicates that usernames and passwords are stored locally on the server, with the passwords encrypted. Users will be authenticated when trying to visit NebulaGraph.</p>"},{"location":"7.data-security/1.authentication/1.authentication/#enable_local_authentication","title":"Enable local authentication","text":"<ol> <li> <p>Modify the <code>nebula-graphd.conf</code> file (<code>/usr/local/nebula/etc/</code> is the default path) to set the following parameters:</p> <ul> <li> <p><code>--enable_authorize</code>: Set its value to <code>true</code> to enable authentication.</p> <p>Note</p> <ul> <li>By default, the authentication is disabled and NebulaGraph allows connections with the username <code>root</code> and any password.</li> <li>You can use the username <code>root</code> and password <code>nebula</code> to log into NebulaGraph after enabling local authentication. This account has the build-in God role. For more information about roles, see Roles and privileges.</li> </ul> </li> </ul> <ul> <li><code>--failed_login_attempts</code>: This parameter is optional, and you need to add this parameter manually. Specify the attempts of continuously entering incorrect passwords for a single Graph service. When the number exceeds the limitation, your account will be locked. For multiple Graph services, the allowed attempts are <code>number of services * failed_login_attempts</code>.</li> </ul> <ul> <li><code>--password_lock_time_in_secs</code>: This parameter is optional, and you need to add this parameter manually. Specify the time how long your account is locked after multiple incorrect password entries are entered. Unit: second.</li> </ul> </li> <li> <p>Restart the NebulaGraph services. For how to restart, see Manage NebulaGraph services.</p> </li> </ol>"},{"location":"7.data-security/1.authentication/1.authentication/#ldap_authentication","title":"LDAP authentication","text":"<p>Lightweight Directory Access Protocol (LDAP) is a lightweight client-server protocol for accessing directories and building a centralized account management system. LDAP authentication and local authentication can be enabled at the same time, but LDAP authentication has a higher priority. If the local authentication server and the LDAP server both have the information of user <code>Amber</code>, NebulaGraph reads from the LDAP server first.</p>"},{"location":"7.data-security/1.authentication/1.authentication/#enable_ldap_authentication","title":"Enable LDAP authentication","text":"<p>Enterpriseonly</p> <p>Contact us.</p>"},{"location":"7.data-security/1.authentication/2.management-user/","title":"User management","text":"<p>User management is an indispensable part of NebulaGraph access control. This topic describes how to manage users and roles.</p> <p>After enabling authentication, only valid users can connect to NebulaGraph and access the resources according to the user roles.</p> <p>Note</p> <ul> <li>By default, the authentication is disabled. NebulaGraph allows connections with the username <code>root</code> and any password.</li> <li>Once the role of a user is modified, the user has to re-login to make the new role takes effect.</li> </ul>"},{"location":"7.data-security/1.authentication/2.management-user/#create_user","title":"CREATE USER","text":"<p>The <code>root</code> user with the GOD role can run <code>CREATE USER</code> to create a new user.</p> <ul> <li> <p>Syntax</p> <pre><code>CREATE USER [IF NOT EXISTS] &lt;user_name&gt; [WITH PASSWORD '&lt;password&gt;'];\n</code></pre> <ul> <li><code>IF NOT EXISTS</code>: Detects if the user name exists. The user will be created only if the user name does not exist.</li> <li><code>user_name</code>: Sets the name of the user. The maximum length is 16 characters.</li> <li><code>password</code>: Sets the password of the user. The default password is the empty string (<code>''</code>). The maximum length is 24 characters.</li> </ul> </li> </ul> <ul> <li> <p>Syntax with enterprise edition</p> <p><pre><code>CREATE USER [IF NOT EXISTS] &lt;user_name&gt; [WITH PASSWORD '&lt;password&gt;'][WITH IP WHITELIST &lt;ip_list&gt;];\n</code></pre> - <code>ip_list</code>: Sets the IP address whitelist. The user can connect to NebulaGraph only from IP addresses in the list. Use commas to separate multiple IP addresses.</p> </li> </ul> <ul> <li> <p>Example</p> <pre><code>nebula&gt; CREATE USER user1 WITH PASSWORD 'nebula';\nnebula&gt; SHOW USERS;\n+---------+-------------------------------+\n| Account | IP Whitelist                  |\n+---------+-------------------------------+\n| \"root\"  | \"\"                            |\n| \"user1\" | \"\"                            |\n+---------+-------------------------------+\n</code></pre> </li> </ul> <ul> <li> <p>Example with enterprise edition</p> <pre><code>nebula&gt; CREATE USER user2 WITH PASSWORD 'nebula' WITH IP WHITELIST 192.168.10.10,192.168.10.12;\nnebula&gt; SHOW USERS;\n+---------+-------------------------------+\n| Account | IP Whitelist                  |\n+---------+-------------------------------+\n| \"root\"  | \"\"                            |\n| \"user2\" | \"192.168.10.10,192.168.10.12\" |\n+---------+-------------------------------+\n</code></pre> </li> </ul>"},{"location":"7.data-security/1.authentication/2.management-user/#grant_role","title":"GRANT ROLE","text":"<p>Users with the GOD role or the ADMIN role can run <code>GRANT ROLE</code> to assign a built-in role in a graph space to a user. For more information about NebulaGraph built-in roles, see Roles and privileges.</p> <ul> <li> <p>Syntax</p> <pre><code>GRANT ROLE &lt;role_type&gt; ON &lt;space_name&gt; TO &lt;user_name&gt;;\n</code></pre> </li> </ul> <ul> <li> <p>Example</p> <pre><code>nebula&gt; GRANT ROLE USER ON basketballplayer TO user1;\n</code></pre> </li> </ul>"},{"location":"7.data-security/1.authentication/2.management-user/#revoke_role","title":"REVOKE ROLE","text":"<p>Users with the GOD role or the ADMIN role can run <code>REVOKE ROLE</code> to revoke the built-in role of a user in a graph space. For more information about NebulaGraph built-in roles, see Roles and privileges.</p> <ul> <li> <p>Syntax</p> <pre><code>REVOKE ROLE &lt;role_type&gt; ON &lt;space_name&gt; FROM &lt;user_name&gt;;\n</code></pre> </li> </ul> <ul> <li> <p>Example</p> <pre><code>nebula&gt; REVOKE ROLE USER ON basketballplayer FROM user1;\n</code></pre> </li> </ul>"},{"location":"7.data-security/1.authentication/2.management-user/#describe_user","title":"DESCRIBE USER","text":"<p>Users can run <code>DESCRIBE USER</code> to list the roles for a specified user.</p> <ul> <li> <p>Syntax</p> <pre><code>DESCRIBE USER &lt;user_name&gt;;\nDESC USER &lt;user_name&gt;;\n</code></pre> </li> </ul> <ul> <li> <p>Example</p> <pre><code>nebula&gt; DESCRIBE USER user1;\n+---------+--------------------+\n| role    | space              |\n+---------+--------------------+\n| \"ADMIN\" | \"basketballplayer\" |\n+---------+--------------------+\n</code></pre> </li> </ul>"},{"location":"7.data-security/1.authentication/2.management-user/#show_roles","title":"SHOW ROLES","text":"<p>Users can run <code>SHOW ROLES</code> to list the roles in a graph space.</p> <ul> <li> <p>Syntax</p> <pre><code>SHOW ROLES IN &lt;space_name&gt;;\n</code></pre> </li> </ul> <ul> <li> <p>Example</p> <pre><code>nebula&gt; SHOW ROLES IN basketballplayer;\n+---------+-----------+\n| Account | Role Type |\n+---------+-----------+\n| \"user1\" | \"ADMIN\"   |\n+---------+-----------+\n</code></pre> </li> </ul>"},{"location":"7.data-security/1.authentication/2.management-user/#change_password","title":"CHANGE PASSWORD","text":"<p>Users can run <code>CHANGE PASSWORD</code> to set a new password for a user. The old password is needed when setting a new one.</p> <ul> <li> <p>Syntax</p> <pre><code>CHANGE PASSWORD &lt;user_name&gt; FROM '&lt;old_password&gt;' TO '&lt;new_password&gt;';\n</code></pre> </li> </ul> <ul> <li> <p>Example</p> <pre><code>nebula&gt; CHANGE PASSWORD user1 FROM 'nebula' TO 'nebula123';\n</code></pre> </li> </ul>"},{"location":"7.data-security/1.authentication/2.management-user/#alter_user","title":"ALTER USER","text":"<p>The <code>root</code> user with the GOD role can run <code>ALTER USER</code> to set a new password. The old password is not needed when altering the user.</p> <ul> <li> <p>Syntax</p> <p><pre><code>ALTER USER &lt;user_name&gt; WITH PASSWORD '&lt;password&gt;';\n</code></pre> - Example</p> <pre><code>nebula&gt; ALTER USER user2 WITH PASSWORD 'nebula';\n</code></pre> </li> </ul> <ul> <li> <p>Syntax with enterprise edition</p> <pre><code>  ALTER USER &lt;user_name&gt; WITH PASSWORD '&lt;password&gt;' [WITH IP WHITELIST &lt;ip_list&gt;];\n</code></pre> </li> </ul> <ul> <li> <p>Example with enterprise edition</p> <p>Enterpriseonly</p> <p>When <code>WITH IP WHITELIST</code> is not used, the IP address whitelist is removed and the user can connect to the NebulaGraph by any IP address.</p> <pre><code>nebula&gt; ALTER USER user2 WITH PASSWORD 'nebula' WITH IP WHITELIST 192.168.10.10;\n</code></pre> </li> </ul>"},{"location":"7.data-security/1.authentication/2.management-user/#drop_user","title":"DROP USER","text":"<p>The <code>root</code> user with the GOD role can run <code>DROP USER</code> to remove a user.</p> <p>Note</p> <p>Removing a user does not close the current session of the user, and the user role still takes effect in the session until the session is closed.</p> <ul> <li> <p>Syntax</p> <pre><code>DROP USER [IF EXISTS] &lt;user_name&gt;;\n</code></pre> </li> </ul> <ul> <li> <p>Example</p> <pre><code>nebula&gt; DROP USER user1;\n</code></pre> </li> </ul>"},{"location":"7.data-security/1.authentication/2.management-user/#show_users","title":"SHOW USERS","text":"<p>The <code>root</code> user with the GOD role can run <code>SHOW USERS</code> to list all the users.</p> <ul> <li> <p>Syntax</p> <pre><code>SHOW USERS;\n</code></pre> </li> </ul> <ul> <li> <p>Example</p> <pre><code>nebula&gt; SHOW USERS;\n+---------+-----------------+\n| Account | IP Whitelist    |\n+---------+-----------------+\n| \"root\"  | \"\"              |\n| \"user1\" | \"\"              |\n| \"user2\" | \"192.168.10.10\" |\n+---------+-----------------+\n</code></pre> </li> </ul>"},{"location":"7.data-security/1.authentication/3.role-list/","title":"Roles and privileges","text":"<p>A role is a collection of privileges. You can assign a role to a user for access control.</p>"},{"location":"7.data-security/1.authentication/3.role-list/#built-in_roles","title":"Built-in roles","text":"<p>NebulaGraph does not support custom roles, but it has multiple built-in roles:</p> <ul> <li> <p>GOD</p> <ul> <li>GOD is the original role with all privileges not limited to graph spaces. It is similar to <code>root</code> in Linux and <code>administrator</code> in Windows.</li> </ul> <ul> <li>When the Meta Service is initialized, the one and only GOD role user <code>root</code> is automatically created with the password <code>nebula</code>.</li> </ul> <p>Caution</p> <p>Modify the password for <code>root</code> timely for security.</p> <ul> <li> <p>When the <code>--enable_authorize</code> parameter in the <code>nebula-graphd.conf</code> file (the default directory is <code>/usr/local/nebula/etc/</code>) is set to <code>true</code>:</p> <ul> <li>One cluster can only have one user with the GOD role. This user can manage all graph spaces in a cluster.</li> </ul> <ul> <li>Manual authorization of the God role is not supported. Only the <code>root</code> user with the default God role can be used.</li> </ul> </li> </ul> </li> </ul> <ul> <li> <p>ADMIN</p> <ul> <li>An ADMIN role can read and write both the Schema and the data in a specific graph space.</li> </ul> <ul> <li> <p>An ADMIN role of a graph space can grant DBA, USER, and GUEST roles in the graph space to other users.</p> <p>Note</p> <p>Only roles lower than ADMIN can be authorized to other users.</p> </li> </ul> </li> </ul> <ul> <li> <p>DBA</p> <ul> <li>A DBA role can read and write both the Schema and the data in a specific graph space.</li> </ul> <ul> <li>A DBA role of a graph space CANNOT grant roles to other users.</li> </ul> </li> </ul> <ul> <li> <p>USER</p> <ul> <li>A USER role can read and write data in a specific graph space.</li> </ul> <ul> <li>The Schema information is read-only to the USER roles in a graph space.</li> </ul> </li> </ul> <ul> <li>GUEST<ul> <li>A GUEST role can only read the Schema and the data in a specific graph space.</li> </ul> </li> </ul> <ul> <li> <p>BASIC</p> <ul> <li>A BASIC role can read the Schema in a specific graph space.</li> </ul> <ul> <li>( Additional authorization required ) A BASIC role can read and write the Tag and Edge Type in a specific graph space.</li> </ul> </li> </ul> <p>Enterpriseonly</p> <p>The Basic role is only available in the Enterprise edition.</p> <p>Note</p> <ul> <li>NebulaGraph does not support custom roles. Users can only use the default built-in roles.</li> <li>A user can have only one role in a graph space. For authenticated users, see User management.</li> </ul>"},{"location":"7.data-security/1.authentication/3.role-list/#role_privileges_and_allowed_ngql","title":"Role privileges and allowed nGQL","text":"<p>The privileges of roles and the nGQL statements that each role can use are listed as follows.</p> Privilege God Admin DBA User Guest Basic Allowed nGQL Read space Y Y Y Y Y Y <code>USE</code>, <code>DESCRIBE SPACE</code> Read schema Y Y Y Y Y Y <code>DESCRIBE TAG</code>, <code>DESCRIBE EDGE</code>, <code>DESCRIBE TAG INDEX</code>, <code>DESCRIBE EDGE INDEX</code> Write schema Y Y Y Y <code>CREATE TAG</code>, <code>ALTER TAG</code>, <code>CREATE EDGE</code>, <code>ALTER EDGE</code>, <code>DROP TAG</code>, <code>DELETE TAG</code>, <code>DROP EDGE</code>, <code>CREATE TAG INDEX</code>, <code>CREATE EDGE INDEX</code>, <code>DROP TAG INDEX</code>, <code>DROP EDGE INDEX</code> Write user Y <code>CREATE USER</code>, <code>DROP USER</code>, <code>ALTER USER</code> Write role Y Y <code>GRANT</code>, <code>REVOKE</code> Read data Y Y Y Y Y C <code>GO</code>, <code>SET</code>, <code>PIPE</code>, <code>MATCH</code>, <code>ASSIGNMENT</code>, <code>LOOKUP</code>, <code>YIELD</code>, <code>ORDER BY</code>, <code>FETCH VERTICES</code>, <code>Find</code>, <code>FETCH EDGES</code>, <code>FIND PATH</code>, <code>LIMIT</code>, <code>GROUP BY</code>, <code>RETURN</code> Write data Y Y Y Y C <code>INSERT VERTEX</code>, <code>UPDATE VERTEX</code>, <code>INSERT EDGE</code>, <code>UPDATE EDGE</code>, <code>DELETE VERTEX</code>, <code>DELETE EDGES</code>, <code>DELETE TAG</code> Show operations Y Y Y Y Y Y <code>SHOW</code>, <code>CHANGE PASSWORD</code> Job Y Y Y Y <code>SUBMIT JOB COMPACT</code>, <code>SUBMIT JOB FLUSH</code>, <code>SUBMIT JOB STATS</code>, <code>STOP JOB</code>, <code>RECOVER JOB</code>, <code>BUILD TAG INDEX</code>, <code>BUILD EDGE INDEX</code>,<code>INGEST</code>, <code>DOWNLOAD</code> Write space Y <code>CREATE SPACE</code>, <code>DROP SPACE</code>, <code>CREATE SNAPSHOT</code>, <code>DROP SNAPSHOT</code>, <code>BALANCE</code>, <code>CONFIG</code> <p>Enterpriseonly</p> <p>Only the Enterprise Edition supports fine-grain (Tag/Edge type level) permission management based on Basic roles.</p> <p>Caution</p> <ul> <li>The results of <code>SHOW</code> operations are limited to the role of a user. For example, all users can run <code>SHOW SPACES</code>, but the results only include the graph spaces that the users have privileges.</li> <li>Only the GOD role can run <code>SHOW USERS</code> and <code>SHOW SNAPSHOTS</code>.</li> </ul>"},{"location":"7.data-security/1.authentication/3.role-list/#basic_roleenterprise_edition","title":"Basic role(Enterprise Edition)","text":""},{"location":"7.data-security/1.authentication/3.role-list/#syntax","title":"Syntax","text":"<p>Caution</p> <p>The following commands can be executed only after entering the graph space.</p> <ul> <li>Grant <code>Basic</code> user Tag/Edge Permissions.</li> </ul> <pre><code>GRANT { OPTION[,OPTION] } [ TAG {  * | &lt;tag&gt;[,...] } | EDGE {  * | &lt;edge_type&gt;[, ...] }] TO &lt;user_name&gt;;\nOPTION = { READ | WRITE }\n</code></pre> <ul> <li>Revoke <code>Basic</code> user Tag/Edge Permissions.</li> </ul> <pre><code>REVOKE { OPTION[,OPTION] } [ TAG {  * | &lt;tag&gt;[,...] } | EDGE {  * | &lt;edge_type&gt;[, ...] }] TO &lt;user_name&gt;;\nOPTION = { READ | WRITE }\n</code></pre> <ul> <li>Show <code>Basic</code> user Tag/Edge Permissions.</li> </ul> <pre><code>SHOW GRANTS [&lt;user_name&gt;]\n</code></pre>"},{"location":"7.data-security/1.authentication/3.role-list/#precautions","title":"Precautions","text":"<ul> <li>The default Basic role does not have any Tag/Edge read and write permissions.</li> <li>Only GOD and ADMIN role users can perform GRANT and REVOKE operations.</li> <li>Only allow users to GRANT and REVOKE the Basic role in the specified graph space, and are not allowed to grant authorization to other role users.</li> <li>The Basic role cannot insert vertices without a tag.</li> <li>Both read and write privileges are required when performing an <code>UPDATE</code> or <code>UPSERT</code> operation.</li> </ul>"},{"location":"7.data-security/1.authentication/3.role-list/#examples","title":"Examples","text":"<pre><code># Create `test` user\nnebula&gt; CREATE USER test WITH PASSWORD 'nebula';\n\n# Grant Basic role permissions for the `test` user\nnebula&gt; GRANT ROLE BASIC ON basketballplayer TO test;\n\n# Choose graph space `basketballplayer`\nnebula&gt; use basketballplayer;\n\n# Grant read and write permissions to `test` user Tag `player` and Edge Type `follow` and `serve`\n# Granting the user the read and write permissions of the specified Tag/Edge must be after specifying the graph space\nnebula&gt; GRANT READ, WRITE TAG player EDGE follow, serve TO test;\n\n# Show `test` user permissions\nnebula&gt; &gt; SHOW GRANTS test;\n+--------+------------+---------------------+------------+---------------------+\n| user   | READ(TAG)  | READ(EDGE)          | WRITE(TAG) | WRITE(EDGE)         |\n+--------+------------+---------------------+------------+---------------------+\n| \"test\" | [\"player\"] | [\"follow\", \"serve\"] | [\"player\"] | [\"follow\", \"serve\"] |\n+--------+------------+---------------------+------------+---------------------+\n\n# Revoke `test` user all Edge Type read and write permissions\nnebula&gt; REVOKE READ,WRITE EDGE * FROM test;\n\n# Show `test` user permissions\nnebula&gt; SHOW GRANTS test;\n+--------+------------+------------+------------+-------------+\n| user   | READ(TAG)  | READ(EDGE) | WRITE(TAG) | WRITE(EDGE) |\n+--------+------------+------------+------------+-------------+\n| \"test\" | [\"player\"] | []         | [\"player\"] | []          |\n+--------+------------+------------+------------+-------------+\n\n# When Basic role users read data without permission, the following error will occur.\nnebula&gt;  MATCH (v:player)-[:likex]-() RETURN v;\n[ERROR (-1008)]: PermissionError: Edge `likex' does not exist or is not readable.\n</code></pre> <p>Caution</p> <p>For Basic role users, an error will be reported for Tag/Edge Type that explicitly specify no read permission, and no errors will be reported for Tag/Edge Types that do not explicitly specify no read permission. During the traverse process, all queries cannot read the unprivileged Tag/Edge Type and its properties. The read permission of the Edge Type can control the expansion behavior of the edge. During the traversal process, if the Edge Type has no permission, it will not be expanded; the read permission of the Tag does not control the expansion behavior of the vertex. Even if the Tag has no permission during the expansion process, also can be expanded.</p>"},{"location":"7.data-security/1.authentication/4.ldap/","title":"OpenLDAP authentication","text":"<p>This topic introduces how to connect NebulaGraph to the OpenLDAP server and use the DN (Distinguished Name) and password defined in OpenLDAP for authentication.</p> <p>Enterpriseonly</p> <p>This feature is supported by the Enterprise Edition only.</p>"},{"location":"7.data-security/1.authentication/4.ldap/#authentication_method","title":"Authentication method","text":"<p>After the OpenLDAP authentication is enabled and users log into NebulaGraph with the account and password, NebulaGraph checks whether the login account exists in the Meta service. If the account exists, NebulaGraph finds the corresponding DN in OpenLDAP according to the authentication method and verifies the password.</p> <p>OpenLDAP supports two authentication methods: simple bind authentication (SimpleBindAuth) and search bind authentication (SearchBindAuth).</p>"},{"location":"7.data-security/1.authentication/4.ldap/#simplebindauth","title":"SimpleBindAuth","text":"<p>Simple bind authentication splices the login account and the configuration information of Graph services into a DN that can be recognized by OpenLDAP, and then authenticates on OpenLDAP based on the DN and password.</p>"},{"location":"7.data-security/1.authentication/4.ldap/#searchbindauth","title":"SearchBindAuth","text":"<p>Search bind authentication reads the Graph service configuration information and queries whether the <code>uid</code> in the configuration matches the login account. If they match, search bind authentication reads the DN, and then uses the DN and password to verify on OpenLDAP.</p> <p>Caution</p> <p>Only the <code>uid</code> attribute in OpenLDAP can be used to specify a username for SearchBindAuth.</p>"},{"location":"7.data-security/1.authentication/4.ldap/#prerequisites","title":"Prerequisites","text":"<ul> <li>OpenLDAP is installed.</li> </ul> <ul> <li>The account and password are imported on OpenLDAP.</li> </ul> <ul> <li>The server where OpenLDAP is located has opened the corresponding authentication port.</li> </ul>"},{"location":"7.data-security/1.authentication/4.ldap/#procedures","title":"Procedures","text":"<p>Take the existing account <code>test2</code> and password <code>passwdtest2</code> on OpenLDAP as an example.</p> <ol> <li> <p>Connect to NebulaGraph, create and authorize the shadow account <code>test2</code> corresponding to OpenLDAP.</p> <pre><code>nebula&gt; CREATE USER test2 WITH PASSWORD '';\nnebula&gt; GRANT ROLE ADMIN ON basketballplayer TO test2;\n</code></pre> <p>Note</p> <p>When creating an account in NebulaGraph, the password can be set arbitrarily.</p> </li> <li> <p>Edit the configuration file <code>nebula-graphd.conf</code> (The default path is<code>/usr/local/nebula/etc/</code>):</p> <ul> <li> <p>SimpleBindAuth (Recommended)</p> <pre><code># Whether to get the configuration information from the configuration file.\n--local_config=true\n# Whether to enable authentication.\n--enable_authorize=true\n# Authentication methods include password, ldap, and cloud.\n--auth_type=ldap\n# The address of the OpenLDAP server.\n--ldap_server=192.168.8.211\n# The port of the OpenLDAP server.\n--ldap_port=389\n# The name of the Schema in OpenLDAP.\n--ldap_scheme=ldap\n# The prefix of DN.\n--ldap_prefix=uid=\n# The suffix of DN.\n--ldap_suffix=,ou=it,dc=sys,dc=com\n</code></pre> </li> </ul> <ul> <li> <p>SearchBindAuth</p> <pre><code># Whether to get the configuration information from the configuration file.\n--local_config=true\n# Whether to enable authentication.\n--enable_authorize=true\n# Authentication methods include password, ldap, and cloud.\n--auth_type=ldap\n# The address of the OpenLDAP server.\n--ldap_server=192.168.8.211\n# The port of the OpenLDAP server.\n--ldap_port=389\n# The name of the Schema in OpenLDAP.\n--ldap_scheme=ldap\n# The DN that binds the target.\n--ldap_basedn=ou=it,dc=sys,dc=com\n# The OpenLDAP login username. If anonymous access is supported, this parameter is optional. Otherwise, it is required.\n--ldap_binddn=cn=admin,dc=example,dc=org\n# The OpenLDAP login password. If anonymous access is supported, this parameter is optional. Otherwise, it is required.\n--ldap_bindpasswd=admin\n</code></pre> </li> </ul> </li> <li> <p>Restart NebulaGraph services to make the new configuration valid.</p> </li> <li> <p>Run the login test.</p> <pre><code>$ ./nebula-console --addr 127.0.0.1 --port 9669 -u test2 -p passwdtest2\n2021/09/08 03:49:39 [INFO] connection pool is initialized successfully\n\nWelcome to NebulaGraph!\n</code></pre> <p>Note</p> <p>After you turn on OpenLDAP for authentication, you can log in with the account and password set in OpenLDAP. However, you need to make sure that NebulaGraph has the same username in the local account.</p> </li> </ol>"},{"location":"8.service-tuning/2.graph-modeling/","title":"Graph data modeling suggestions","text":"<p>This topic provides general suggestions for modeling data in NebulaGraph.</p> <p>Note</p> <p>The following suggestions may not apply to some special scenarios. In these cases, find help in the NebulaGraph community.</p>"},{"location":"8.service-tuning/2.graph-modeling/#model_for_performance","title":"Model for performance","text":"<p>There is no perfect method to model in Nebula\u00a0Graph. Graph modeling depends on the questions that you want to know from the data. Your data drives your graph model. Graph data modeling is intuitive and convenient. Create your data model based on your business model. Test your model and gradually optimize it to fit your business. To get better performance, you can change or re-design your model multiple times.</p>"},{"location":"8.service-tuning/2.graph-modeling/#design_and_evaluate_the_most_important_queries","title":"Design and evaluate the most important queries","text":"<p>Usually, various types of queries are validated in test scenarios to assess the overall capabilities of the system. However, in most production scenarios, there are not many types of frequently used queries. You can optimize the data model based on key queries selected according to the Pareto (80/20) principle.</p>"},{"location":"8.service-tuning/2.graph-modeling/#full-graph_scanning_avoidance","title":"Full-graph scanning avoidance","text":"<p>Graph traversal can be performed after one or more vertices/edges are located through property indexes or VIDs. But for some query patterns, such as subgraph and path query patterns, the source vertex or edge of the traversal cannot be located through property indexes or VIDs. These queries find all the subgraphs that satisfy the query pattern by scanning the whole graph space which will have poor query performance. NebulaGraph does not implement indexing for the graph structures of subgraphs or paths. </p>"},{"location":"8.service-tuning/2.graph-modeling/#no_predefined_bonds_between_tags_and_edge_types","title":"No predefined bonds between Tags and Edge types","text":"<p>Define the bonds between Tags and Edge types in the application, not NebulaGraph. There are no statements that could get the bonds between Tags and Edge types.</p>"},{"location":"8.service-tuning/2.graph-modeling/#tagsedge_types_predefine_a_set_of_properties","title":"Tags/Edge types predefine a set of properties","text":"<p>While creating Tags or Edge types, you need to define a set of properties. Properties are part of the NebulaGraph Schema.</p>"},{"location":"8.service-tuning/2.graph-modeling/#control_changes_in_the_business_model_and_the_data_model","title":"Control changes in the business model and the data model","text":"<p>Changes here refer to changes in business models and data models (meta-information), not changes in the data itself.</p> <p>Some graph databases are designed to be Schema-free, so their data modeling, including the modeling of the graph topology and properties, can be very flexible. Properties can be re-modeled to graph topology, and vice versa. Such systems are often specifically optimized for graph topology access.</p> <p>NebulaGraph 3.5.0 is a strong-Schema (row storage) system, which means that the business model should not change frequently. For example, the property Schema should not change. It is similar to avoiding <code>ALTER TABLE</code> in MySQL.</p> <p>On the contrary, vertices and their edges can be added or deleted at low costs. Thus, the easy-to-change part of the business model should be transformed to vertices or edges, rather than properties.</p> <p>For example, in a business model, people have relatively fixed properties such as age, gender, and name. But their contact, place of visit, trade account, and login device are often changing. The former is suitable for modeling as properties and the latter as vertices or edges.</p>"},{"location":"8.service-tuning/2.graph-modeling/#set_temporary_properties_through_self-loop_edges","title":"Set temporary properties through self-loop edges","text":"<p>As a strong Schema system, NebulaGraph does not support List-type properties. And using <code>ALTER TAG</code> costs too much. If you need to add some temporary properties or List-type properties to a vertex, you can first create an edge type with the required properties, and then insert one or more edges that direct to the vertex itself. The figure is as follows.</p> <p></p> <p>To retrieve temporary properties of vertices, fetch from self-loop edges. For example:</p> <p><pre><code>//Create the edge type and insert the loop property.\nnebula&gt; CREATE EDGE IF NOT EXISTS temp(tmp int);\nnebula&gt; INSERT EDGE temp(tmp) VALUES \"player100\"-&gt;\"player100\"@1:(1);\nnebula&gt; INSERT EDGE temp(tmp) VALUES \"player100\"-&gt;\"player100\"@2:(2);\nnebula&gt; INSERT EDGE temp(tmp) VALUES \"player100\"-&gt;\"player100\"@3:(3);\n\n//After the data is inserted, you can query the loop property by general query statements, for example:\nnebula&gt; GO FROM \"player100\" OVER temp YIELD properties(edge).tmp;\n+----------------------+\n| properties(EDGE).tmp |\n+----------------------+\n| 1                    |\n| 2                    |\n| 3                    |\n+----------------------+\n\n//If you want the results to be returned in the form of a List, you can use a function, for example:\nnebula&gt; MATCH (v1:player)-[e:temp]-&gt;() return collect(e.tmp);\n+----------------+\n| collect(e.tmp) |\n+----------------+\n| [1, 2, 3]      |\n+----------------+\n</code></pre> Operations on loops are not encapsulated with any syntactic sugars and you can use them just like those on normal edges.</p>"},{"location":"8.service-tuning/2.graph-modeling/#about_dangling_edges","title":"About dangling edges","text":"<p>A dangling edge is an edge that only connects to a single vertex and only one part of the edge connects to the vertex.</p> <p>In NebulaGraph 3.5.0, dangling edges may appear in the following two cases.</p> <ol> <li> <p>Insert edges with INSERT EDGE statement before the source vertex or the destination vertex exists.</p> </li> <li> <p>Delete vertices with DELETE VERTEX statement and the <code>WITH EDGE</code> option is not used. At this time, the system does not delete the related outgoing and incoming edges of the vertices. There will be dangling edges by default.</p> </li> </ol> <p>Dangling edges may appear in NebulaGraph 3.5.0 as the design allow it to exist. And there is no MERGE statement like openCypher has. The existence of dangling edges depends entirely on the application level. You can use GO and LOOKUP statements to find a dangling edge, but cannot use the MATCH statement to find a dangling edge.</p> <p>Examples:</p> <pre><code>// Insert an edge that connects two vertices which do not exist in the graph. The source vertex's ID is '11'. The destination vertex's ID is'13'. \n\nnebula&gt; CREATE EDGE IF NOT EXISTS e1 (name string, age int);\nnebula&gt; INSERT EDGE e1 (name, age) VALUES \"11\"-&gt;\"13\":(\"n1\", 1);\n\n// Query using the `GO` statement\n\nnebula&gt; GO FROM \"11\" over e1 YIELD properties(edge);\n+----------------------+\n| properties(EDGE)     |\n+----------------------+\n| {age: 1, name: \"n1\"} |\n+----------------------+\n\n// Query using the `LOOKUP` statement\n\nnebula&gt; LOOKUP ON e1 YIELD EDGE AS r;\n+-------------------------------------------------------+\n| r                                                     |\n+-------------------------------------------------------+\n| [:e2 \"11\"-&gt;\"13\" @0 {age: 1, name: \"n1\"}]              |\n+-------------------------------------------------------+\n\n// Query using the `MATCH` statement\n\nnebula&gt; MATCH ()-[e:e1]-&gt;() RETURN e;\n+---+\n| e |\n+---+\n+---+\nEmpty set (time spent 3153/3573 us)\n</code></pre>"},{"location":"8.service-tuning/2.graph-modeling/#breadth-first_traversal_over_depth-first_traversal","title":"Breadth-first traversal over depth-first traversal","text":"<ul> <li>NebulaGraph has lower performance for depth-first traversal based on the Graph topology, and better performance for breadth-first traversal and obtaining properties. For example, if model A contains properties \"name\", \"age\", and \"eye color\", it is recommended to create a tag <code>person</code> and add properties <code>name</code>, <code>age</code>, and <code>eye_color</code> to it. If you create a tag <code>eye_color</code> and an edge type <code>has</code>, and then create an edge to represent the eye color owned by the person, the traversal performance will not be high.</li> </ul> <ul> <li>The performance of finding an edge by an edge property is close to that of finding a vertex by a vertex property. For some databases, it is recommended to re-model edge properties as those of the intermediate vertices. For example, model the pattern <code>(src)-[edge {P1, P2}]-&gt;(dst)</code> as <code>(src)-[edge1]-&gt;(i_node {P1, P2})-[edge2]-&gt;(dst)</code>. With NebulaGraph 3.5.0, you can use <code>(src)-[edge {P1, P2}]-&gt;(dst)</code> directly to decrease the depth of the traversal and increase the performance.</li> </ul>"},{"location":"8.service-tuning/2.graph-modeling/#edge_directions","title":"Edge directions","text":"<p>To query in the opposite direction of an edge, use the following syntax:</p> <p><code>(dst)&lt;-[edge]-(src)</code> or <code>GO FROM dst REVERSELY</code>.</p> <p>If you do not care about the directions or want to query against both directions, use the following syntax:</p> <p><code>(src)-[edge]-(dst)</code> or <code>GO FROM src BIDIRECT</code>.</p> <p>Therefore, there is no need to insert the same edge redundantly in the reversed direction.</p>"},{"location":"8.service-tuning/2.graph-modeling/#set_tag_properties_appropriately","title":"Set tag properties appropriately","text":"<p>Put a group of properties that are on the same level into the same tag. Different groups represent different concepts.</p>"},{"location":"8.service-tuning/2.graph-modeling/#use_indexes_correctly","title":"Use indexes correctly","text":"<p>Using property indexes helps find VIDs through properties, but can lead to great performance reduction. Only use an index when you need to find vertices or edges through their properties.</p>"},{"location":"8.service-tuning/2.graph-modeling/#design_vids_appropriately","title":"Design VIDs appropriately","text":"<p>See VID.</p>"},{"location":"8.service-tuning/2.graph-modeling/#long_texts","title":"Long texts","text":"<p>Do not use long texts to create edge properties. Edge properties are stored twice and long texts lead to greater write amplification. For how edges properties are stored, see Storage architecture. It is recommended to store long texts in HBase or Elasticsearch and store its address in NebulaGraph.</p>"},{"location":"8.service-tuning/2.graph-modeling/#dynamic_graphs_sequence_graphs_are_not_supported","title":"Dynamic graphs (sequence graphs) are not supported","text":"<p>In some scenarios, graphs need to have the time information to describe how the structure of the entire graph changes over time.<sup>1</sup></p> <p>The Rank field on Edges in NebulaGraph 3.5.0 can be used to store time in int64, but no field on vertices can do this because if you store the time information as property values, it will be covered by new insertion. Thus NebulaGraph does not support sequence graphs.</p> <p></p>"},{"location":"8.service-tuning/2.graph-modeling/#free_graph_data_modeling_tools","title":"Free graph data modeling tools","text":"<p>arrows.app</p> <ol> <li> <p>https://blog.twitter.com/engineering/en_us/topics/insights/2021/temporal-graph-networks\u00a0\u21a9</p> </li> </ol>"},{"location":"8.service-tuning/3.system-design/","title":"System design suggestions","text":""},{"location":"8.service-tuning/3.system-design/#qps_or_low-latency_first","title":"QPS or low-latency first","text":"<ul> <li>NebulaGraph 3.5.0 is good at handling small requests with high concurrency. In such scenarios, the whole graph is huge, containing maybe trillions of vertices or edges, but the subgraphs accessed by each request are not large (containing millions of vertices or edges), and the latency of a single request is low. The concurrent number of such requests, i.e., the QPS, can be huge.</li> </ul> <ul> <li>On the other hand, in interactive analysis scenarios, the request concurrency is usually not high, but the subgraphs accessed by each request are large, with thousands of millions of vertices or edges. To lower the latency of big requests in such scenarios, you can split big requests into multiple small requests in the application, and concurrently send them to multiple graphd processes. This can decrease the memory used by each graphd process as well. Besides, you can use NebulaGraph Algorithm for such scenarios.</li> </ul>"},{"location":"8.service-tuning/3.system-design/#data_transmission_and_optimization","title":"Data transmission and optimization","text":"<ul> <li>Read/write balance. NebulaGraph fits into OLTP scenarios with balanced read/write, i.e., concurrent write and read. It is not suitable for OLAP scenarios that usually need to write once and read many times.</li> <li>Select different write methods. For large batches of data writing, use SST files. For small batches of data writing, use <code>INSERT</code>.</li> <li>Run <code>COMPACTION</code> and <code>BALANCE</code> jobs to optimize data format and storage distribution at the right time.</li> <li>NebulaGraph 3.5.0 does not support transactions and isolation in the relational database and is closer to NoSQL.</li> </ul>"},{"location":"8.service-tuning/3.system-design/#query_preheating_and_data_preheating","title":"Query preheating and data preheating","text":"<p>Preheat on the application side:</p> <ul> <li>The Grapd process does not support pre-compiling queries and generating corresponding query plans, nor can it cache previous query results.</li> <li>The Storagd process does not support preheating data. Only the LSM-Tree and BloomFilter of RocksDB are loaded into memory at startup.</li> <li>Once accessed, vertices and edges are cached respectively in two types of LRU cache of the Storage Service.</li> </ul>"},{"location":"8.service-tuning/4.plan/","title":"Execution plan","text":"<p>NebulaGraph 3.5.0 applies rule-based execution plans. Users cannot change execution plans, pre-compile queries (and corresponding plan cache), or accelerate queries by specifying indexes.</p> <p>To view the execution plan and executive summary, see EXPLAIN and PROFILE.</p>"},{"location":"8.service-tuning/compaction/","title":"Compaction","text":"<p>This topic gives some information about compaction.</p> <p>In NebulaGraph, <code>Compaction</code> is the most important background process and has an important effect on performance.</p> <p><code>Compaction</code> reads the data that is written on the hard disk, then re-organizes the data structure and the indexes, and then writes back to the hard disk. The read performance can increase by times after compaction. Thus, to get high read performance, trigger <code>compaction</code> (full <code>compaction</code>) manually when writing a large amount of data into Nebula\u00a0Graph.</p> <p>Note</p> <p>Note that <code>compaction</code> leads to long-time hard disk IO. We suggest that users do compaction during off-peak hours (for example, early morning).</p> <p>NebulaGraph has two types of <code>compaction</code>: automatic <code>compaction</code> and full <code>compaction</code>.</p>"},{"location":"8.service-tuning/compaction/#automatic_compaction","title":"Automatic <code>compaction</code>","text":"<p>Automatic <code>compaction</code> is automatically triggered when the system reads data, writes data, or the system restarts. The read performance can increase in a short time. Automatic <code>compaction</code> is enabled by default. But once triggered during peak hours, it can cause unexpected IO occupancy that has an unwanted effect on the performance.</p>"},{"location":"8.service-tuning/compaction/#full_compaction","title":"Full <code>compaction</code>","text":"<p>Full <code>compaction</code> enables large-scale background operations for a graph space such as merging files, deleting the data expired by TTL. This operation needs to be initiated manually. Use the following statements to enable full <code>compaction</code>:</p> <p>Note</p> <p>We recommend you to do the full compaction during off-peak hours because full compaction has a lot of IO operations.</p> <pre><code>nebula&gt; USE &lt;your_graph_space&gt;;\nnebula&gt; SUBMIT JOB COMPACT;\n</code></pre> <p>The preceding statement returns the job ID. To show the <code>compaction</code> progress, use the following statement:</p> <pre><code>nebula&gt; SHOW JOB &lt;job_id&gt;;\n</code></pre>"},{"location":"8.service-tuning/compaction/#operation_suggestions","title":"Operation suggestions","text":"<p>These are some operation suggestions to keep Nebula\u00a0Graph performing well.</p> <ul> <li>After data import is done, run <code>SUBMIT JOB COMPACT</code>.</li> </ul> <ul> <li>Run <code>SUBMIT JOB COMPACT</code> periodically during off-peak hours (e.g. early morning).</li> </ul> <ul> <li> <p>To control the write traffic limitation for <code>compactions</code>, set the following parameter in the <code>nebula-storaged.conf</code> configuration file.</p> <p>Note</p> <p>This parameter limits the rate of all writes including normal writes and compaction writes. </p> <pre><code># Limit the write rate to 20MB/s.\n--rocksdb_rate_limit=20 (in MB/s)\n</code></pre> </li> </ul>"},{"location":"8.service-tuning/compaction/#faq","title":"FAQ","text":""},{"location":"8.service-tuning/compaction/#where_are_the_logs_related_to_compaction_stored","title":"\"Where are the logs related to <code>Compaction</code> stored?\"","text":"<p>By default, the logs are stored under the <code>LOG</code> file in the <code>/usr/local/nebula/data/storage/nebula/{1}/data/</code> directory, or similar to <code>LOG.old.1625797988509303</code>. You can find the following content.</p> <pre><code>** Compaction Stats [default] **\nLevel    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  L0      2/0    2.46 KB   0.5      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0      0.0      0.53              0.51         2    0.264       0      0\n Sum      2/0    2.46 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0      0.0      0.53              0.51         2    0.264       0      0\n Int      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0\n</code></pre> <p>If the number of <code>L0</code> files is large, the read performance will be greatly affected and compaction can be triggered.</p>"},{"location":"8.service-tuning/compaction/#can_i_do_full_compactions_for_multiple_graph_spaces_at_the_same_time","title":"\"Can I do full <code>compactions</code> for multiple graph spaces at the same time?\"","text":"<p>Yes, you can. But the IO is much larger at this time and the efficiency may be affected.</p>"},{"location":"8.service-tuning/compaction/#how_much_time_does_it_take_for_full_compactions","title":"\"How much time does it take for full <code>compactions</code>?\"","text":"<p>When <code>rocksdb_rate_limit</code> is set to <code>20</code>, you can estimate the full compaction time by dividing the hard disk usage by the <code>rocksdb_rate_limit</code>. If you do not set the <code>rocksdb_rate_limit</code> value, the empirical value is around 50 MB/s.</p>"},{"location":"8.service-tuning/compaction/#can_i_modify_--rocksdb_rate_limit_dynamically","title":"\"Can I modify <code>--rocksdb_rate_limit</code> dynamically?\"","text":"<p>No, you cannot.</p>"},{"location":"8.service-tuning/compaction/#can_i_stop_a_full_compaction_after_it_starts","title":"\"Can I stop a full <code>compaction</code> after it starts?\"","text":"<p>No, you cannot. When you start a full compaction, you have to wait till it is done. This is the limitation of RocksDB.</p>"},{"location":"8.service-tuning/enable_autofdo_for_nebulagraph/","title":"Enable AutoFDO for NebulaGraph","text":"<p>The AutoFDO can analyze the performance of an optimized program and use the program's performance information to guide the compiler to re-optimize the program. This document will help you to enable the AutoFDO for NebulaGraph.</p> <p>More information about the AutoFDO, please refer AutoFDO Wiki.</p>"},{"location":"8.service-tuning/enable_autofdo_for_nebulagraph/#resource_preparations","title":"Resource Preparations","text":""},{"location":"8.service-tuning/enable_autofdo_for_nebulagraph/#install_dependencies","title":"Install Dependencies","text":"<ul> <li> <p>Install perf</p> <pre><code>sudo apt-get update\nsudo apt-get install -y linux-tools-common \\\nlinux-tools-generic \\\nlinux-tools-`uname -r`\n</code></pre> </li> </ul> <ul> <li> <p>Install autofdo tool</p> <pre><code>sudo apt-get update\nsudo apt-get install -y autofdo\n</code></pre> <p>Or you can compile the autofdo tool from source.</p> </li> </ul>"},{"location":"8.service-tuning/enable_autofdo_for_nebulagraph/#nebulagraph_binary_with_debug_version","title":"NebulaGraph Binary with Debug Version","text":"<p>For how to build NebulaGraph from source, please refer to the official document: Install NebulaGraph by compiling the source code. In the configure step, replace <code>CMAKE_BUILD_TYPE=Release</code> with <code>CMAKE_BUILD_TYPE=RelWithDebInfo</code> as below:</p> <pre><code>$ cmake -DCMAKE_INSTALL_PREFIX=/usr/local/nebula -DENABLE_TESTING=OFF -DCMAKE_BUILD_TYPE=RelWithDebInfo ..\n</code></pre>"},{"location":"8.service-tuning/enable_autofdo_for_nebulagraph/#prepare_test_data","title":"Prepare Test Data","text":"<p>In our test environment, we use NebulaGraph Bench to prepare the test data and collect the profile data by running the FindShortestPath, Go1Step, Go2Step, Go3Step, InsertPersonScenario 5 scenarios. </p> <p>Note</p> <p>You can use your TopN queries in your production environment to collect the profile data, the performance can gain more in your environment.</p>"},{"location":"8.service-tuning/enable_autofdo_for_nebulagraph/#prepare_profile_data","title":"Prepare Profile Data","text":""},{"location":"8.service-tuning/enable_autofdo_for_nebulagraph/#collect_perf_data_for_autofdo_tool","title":"Collect Perf Data For AutoFdo Tool","text":"<ol> <li> <p>After the test data preparation work done. Collect the perf data for different scenarios. Get the pid of <code>storaged</code>, <code>graphd</code>, <code>metad</code>.</p> <pre><code>$ nebula.service status all\n[INFO] nebula-metad: Running as 305422, Listening on 9559\n[INFO] nebula-graphd: Running as 305516, Listening on 9669\n[INFO] nebula-storaged: Running as 305707, Listening on 9779\n</code></pre> </li> <li> <p>Start the perf record for nebula-graphd and nebula-storaged. </p> <pre><code>perf record -p 305516,305707 -b -e br_inst_retired.near_taken:pp -o ~/FindShortestPath.data\n</code></pre> <p>Note</p> <p>Because the <code>nebula-metad</code> service contribution percent is small compared with <code>nebula-graphd</code> and <code>nebula-storaged</code> services. To reduce effort, we didn't collect the perf data for <code>nebula-metad</code> service.</p> </li> <li> <p>Start the benchmark test for FindShortestPath scenario.</p> <pre><code>cd NebulaGraph-Bench \npython3 run.py stress run -s benchmark -scenario find_path.FindShortestPath -a localhost:9669 --args='-u 100 -i 100000'\n</code></pre> </li> <li> <p>After the benchmark finished, end the perf record by Ctrl + c.</p> </li> <li> <p>Repeat above steps to collect corresponding profile data for the rest  Go1Step, Go2Step, Go3Step and InsertPersonScenario scenarios.</p> </li> </ol>"},{"location":"8.service-tuning/enable_autofdo_for_nebulagraph/#create_gcov_file","title":"Create Gcov File","text":"<pre><code>create_gcov --binary=$NEBULA_HOME/bin/nebula-storaged \\\n--profile=~/FindShortestPath.data \\\n--gcov=~/FindShortestPath-storaged.gcov \\\n-gcov_version=1\n\ncreate_gcov --binary=$NEBULA_HOME/bin/nebula-graphd \\\n--profile=~/FindShortestPath.data \\\n--gcov=~/FindShortestPath-graphd.gcov \\\n-gcov_version=1\n</code></pre> <p>Repeat for Go1Step, Go2Step, Go3Step and InsertPersonScenario scenarios.</p>"},{"location":"8.service-tuning/enable_autofdo_for_nebulagraph/#merge_the_profile_data","title":"Merge the Profile Data","text":"<pre><code>profile_merger ~/FindShortestPath-graphd.gcov \\\n~/FindShortestPath-storaged.gcov \\\n~/go1step-storaged.gcov \\\n~/go1step-graphd.gcov \\\n~/go2step-storaged.gcov \\\n~/go2step-graphd.gcov \\\n~/go3step-storaged.gcov \\\n~/go3step-master-graphd.gcov \\\n~/InsertPersonScenario-storaged.gcov \\\n~/InsertPersonScenario-graphd.gcov\n</code></pre> <p>You will get a merged profile which is named <code>fbdata.afdo</code> after that.</p>"},{"location":"8.service-tuning/enable_autofdo_for_nebulagraph/#recompile_graphnebula_binary_with_the_merged_profile","title":"Recompile GraphNebula Binary with the Merged Profile","text":"<p>Recompile the GraphNebula Binary by passing the profile with compile option <code>-fauto-profile</code>.</p> <pre><code>diff --git a/cmake/nebula/GeneralCompilerConfig.cmake b/cmake/nebula/GeneralCompilerConfig.cmake\n@@ -20,6 +20,8 @@ add_compile_options(-Wshadow)\n add_compile_options(-Wnon-virtual-dtor)\n add_compile_options(-Woverloaded-virtual)\n add_compile_options(-Wignored-qualifiers)\n+add_compile_options(-fauto-profile=~/fbdata.afdo)\n</code></pre> <p>Note</p> <p>When you use multiple fbdata.afdo to compile multiple times, please remember to <code>make clean</code> before re-compile, baucase only change the fbdata.afdo will not trigger re-compile.</p>"},{"location":"8.service-tuning/enable_autofdo_for_nebulagraph/#performance_test_result","title":"Performance Test Result","text":""},{"location":"8.service-tuning/enable_autofdo_for_nebulagraph/#hardware_software_environment","title":"Hardware &amp; Software Environment","text":"Key Value CPU Processor# 2 Sockets 2 NUMA 2 CPU Type Intel(R) Xeon(R) Platinum 8380 CPU @ 2.30GHz Cores per Processor 40C80T Cache L1 data: 48KB L1 i: 32KB  L2: 1.25MB per physical core  L3: shared 60MB per processor Memory Micron DDR4 3200MT/s 16GB16Micron DDR4 3200MT/s 16GB16 SSD Disk INTEL SSDPE2KE016T8 SSD R/W Sequential 3200 MB/s (read) / 2100 MB/s(write) Nebula Version master with commit id 51d84a4ed7d2a032a337e3b996c927e3bc5d1415 Kernel 4.18.0-408.el8.x86_64"},{"location":"8.service-tuning/enable_autofdo_for_nebulagraph/#test_results","title":"Test Results","text":"Scenario Average Latency(LiB) Default Binary Optimized Binary with AutoFDO P95 Latency (LiB) Default Binary Optimized Binary with AutoFDO FindShortestPath 1 8072.52 7260.10 1 22102.00 19108.00 2 8034.32 7218.59 2 22060.85 19006.00 3 8079.27 7257.24 3 22147.00 19053.00 4 8087.66 7221.39 4 22143.00 19050.00 5 8044.77 7239.85 5 22181.00 19055.00 STDDEVP 20.57 17.34 STDDEVP 41.41 32.36 Mean 8063.71 7239.43 Mean 22126.77 19054.40 STDDEVP/Mean 0.26% 0.24% STDDEVP/Mean 0.19% 0.17% Opt/Default 100.00% 10.22% Opt/Default 100.00% 13.89% Go1Step 1 422.53 418.37 1 838.00 850.00 2 432.37 402.44 2 866.00 815.00 3 437.45 407.98 3 874.00 836.00 4 429.16 408.38 4 858.00 838.00 5 446.38 411.32 5 901.00 837.00 STDDEVP 8.02 5.20 STDDEVP 20.63 11.30 Mean 433.58 409.70 Mean 867.40 835.20 STDDEVP/Mean 1.85% 1.27% STDDEVP/Mean 2.38% 1.35% Opt/Default 100.00% 5.51% Opt/Default 100.00% 3.71% Go2Step 1 2989.93 2824.29 1 10202.00 9656.95 2 2957.22 2834.55 2 10129.00 9632.40 3 2962.74 2818.62 3 10168.40 9624.70 4 2992.39 2817.27 4 10285.10 9647.50 5 2934.85 2834.91 5 10025.00 9699.65 STDDEVP 21.53 7.57 STDDEVP 85.62 26.25 Mean 2967.43 2825.93 Mean 10161.90 9652.24 STDDEVP/Mean 0.73% 0.27% STDDEVP/Mean 0.84% 0.27% Opt/Default 100.00% 4.77% Opt/Default 100.00% 5.02% Go3Step 1 93551.97 89406.96 1 371359.55 345433.50 2 92418.43 89977.25 2 368868.00 352375.20 3 92587.67 90339.25 3 365390.15 356198.55 4 93371.64 92458.95 4 373578.15 365177.75 5 94046.05 89943.44 5 373392.25 352576.00 STDDEVP 609.07 1059.54 STDDEVP 3077.38 6437.52 Mean 93195.15 90425.17 Mean 370517.62 354352.20 STDDEVP/Mean 0.65% 1.17% STDDEVP/Mean 0.83% 1.82% Opt/Default 100.00% 2.97% Opt/Default 100.00% 4.36% InsertPerson 1 2022.86 1937.36 1 2689.00 2633.45 2 1966.05 1935.41 2 2620.45 2555.00 3 1985.25 1953.58 3 2546.00 2593.00 4 2026.73 1887.28 4 2564.00 2394.00 5 2007.55 1964.41 5 2676.00 2581.00 STDDEVP 23.02 26.42 STDDEVP 57.45 82.62 Mean 2001.69 1935.61 Mean 2619.09 2551.29 STDDEVP/Mean 1.15% 1.37% STDDEVP/Mean 2.19% 3.24% Opt/Default 100.00% 3.30% Opt/Default 100.00% 2.59%"},{"location":"8.service-tuning/load-balance/","title":"Storage load balance","text":"<p>You can use the <code>SUBMIT JOB BALANCE</code> statement to balance the distribution of partitions and Raft leaders, or clear some Storage servers for easy maintenance. For details, see SUBMIT JOB BALANCE.</p> <p>Danger</p> <p>The <code>BALANCE</code> commands migrate data and balance the distribution of partitions by creating and executing a set of subtasks. DO NOT stop any machine in the cluster or change its IP address until all the subtasks finish. Otherwise, the follow-up subtasks fail.</p>"},{"location":"8.service-tuning/load-balance/#balance_partition_distribution","title":"Balance partition distribution","text":"<p>Enterpriseonly</p> <p>Only available for the NebulaGraph Enterprise Edition.</p> <p>Note</p> <p>If the current graph space already has a <code>SUBMIT JOB BALANCE DATA</code> job in the <code>FAILED</code> status, you can restore the <code>FAILED</code> job, but cannot start a new <code>SUBMIT JOB BALANCE DATA</code> job. If the job continues to fail, manually stop it, and then you can start a new one.</p> <p>The <code>SUBMIT JOB BALANCE DATA</code> commands starts a job to balance the distribution of storage partitions in the current graph space by creating and executing a set of subtasks.</p>"},{"location":"8.service-tuning/load-balance/#examples","title":"Examples","text":"<p>After you add new storage hosts into the cluster, no partition is deployed on the new hosts.</p> <ol> <li> <p>Run <code>SHOW HOSTS</code> to check the partition distribution.</p> <pre><code>nebual&gt; SHOW HOSTS;\n+-----------------+------+----------+--------------+-----------------------+------------------------+----------------------+\n| Host            | Port | Status   | Leader count | Leader distribution   | Partition distribution | Version              |\n+-----------------+------+----------+--------------+-----------------------+------------------------+----------------------+\n| \"192.168.8.101\" | 9779 | \"ONLINE\" | 0            | \"No valid partition\"  | \"No valid partition\"   | \"3.5.0\" |\n| \"192.168.8.100\" | 9779 | \"ONLINE\" | 15           | \"basketballplayer:15\" | \"basketballplayer:15\"  | \"3.5.0\" |\n+-----------------+------+----------+--------------+-----------------------+------------------------+----------------------+\n</code></pre> </li> <li> <p>Enter the graph space <code>basketballplayer</code>, and execute the command <code>SUBMIT JOB BALANCE DATA</code> to balance the distribution of storage partitions.</p> <pre><code>nebula&gt; USE basketballplayer;\nnebula&gt; SUBMIT JOB BALANCE DATA;\n+------------+\n| New Job Id |\n+------------+\n| 25         |\n+------------+\n</code></pre> </li> <li> <p>The job ID is returned after running <code>SUBMIT JOB BALANCE DATA</code>. Run <code>SHOW JOB &lt;job_id&gt;</code> to check the status of the job.</p> <pre><code>nebula&gt; SHOW JOB 25;\n+------------------------+-------------------+------------+----------------------------+----------------------------+-------------+\n| Job Id(spaceId:partId) | Command(src-&gt;dst) | Status     | Start Time                 | Stop Time                  | State       |\n+------------------------+-------------------+------------+----------------------------+----------------------------+-------------+\n| 25                     | \"DATA_BALANCE\"    | \"FINISHED\" | 2023-01-17T06:24:35.000000 | 2023-01-17T06:24:35.000000 | \"SUCCEEDED\" |\n| \"Total:0\"              | \"Succeeded:0\"     | \"Failed:0\" | \"In Progress:0\"            | \"Invalid:0\"                | \"\"          |\n+------------------------+-------------------+------------+----------------------------+----------------------------+-------------+\n</code></pre> </li> <li> <p>When all the subtasks succeed, the load balancing process finishes. Run <code>SHOW HOSTS</code> again to make sure the partition distribution is balanced.</p> <p>Note</p> <p><code>SUBMIT JOB BALANCE DATA</code> does not balance the leader distribution. For more information, see Balance leader distribution.</p> <pre><code>nebula&gt; SHOW HOSTS;\n+-----------------+------+----------+--------------+----------------------+------------------------+----------------------+\n| Host            | Port | Status   | Leader count | Leader distribution  | Partition distribution | Version              |\n+-----------------+------+----------+--------------+----------------------+------------------------+----------------------+\n| \"192.168.8.101\" | 9779 | \"ONLINE\" | 7            | \"basketballplayer:7\" | \"basketballplayer:7\"   | \"3.5.0\" |\n| \"192.168.8.100\" | 9779 | \"ONLINE\" | 8            | \"basketballplayer:8\" | \"basketballplayer:8\"   | \"3.5.0\" |\n+-----------------+------+----------+--------------+----------------------+------------------------+----------------------+\n</code></pre> </li> </ol> <p>If any subtask fails, run <code>RECOVER JOB &lt;job_id&gt;</code> to recover the failed jobs. If redoing load balancing does not solve the problem, ask for help in the NebulaGraph community.</p>"},{"location":"8.service-tuning/load-balance/#stop_data_balancing","title":"Stop data balancing","text":"<p>To stop a balance job, run <code>STOP JOB &lt;job_id&gt;</code>.</p> <ul> <li>If no balance job is running, an error is returned.</li> </ul> <ul> <li>If a balance job is running, <code>Job stopped</code> is returned.</li> </ul> <p>Note</p> <p><code>STOP JOB &lt;job_id&gt;</code> does not stop the running subtasks but cancels all follow-up subtasks. The status of follow-up subtasks is set to <code>INVALID</code>. The status of ongoing subtasks is set to <code>SUCCEEDED</code> or <code>FAILED</code> based on the result. You can run the <code>SHOW JOB &lt;job_id&gt;</code> command to check the stopped job status.</p> <p>Once all the subtasks are finished or stopped, you can run <code>RECOVER JOB &lt;job_id&gt;</code> again to balance the partitions again, the subtasks continue to be executed in the original state.</p>"},{"location":"8.service-tuning/load-balance/#restore_a_balance_job","title":"Restore a balance job","text":"<p>To restore a balance job in the <code>FAILED</code> or <code>STOPPED</code> status, run <code>RECOVER JOB &lt;job_id&gt;</code>.</p> <p>Note</p> <p>For a <code>STOPPED</code> <code>SUBMIT JOB BALANCE DATA</code> job, NebulaGraph detects whether the same type of <code>FAILED</code> jobs or <code>FINISHED</code> jobs have been created since the start time of the job. If so, the <code>STOPPED</code> job cannot be restored. For example, if chronologically there are STOPPED job1, FINISHED job2, and STOPPED Job3, only job3 can be restored, and job1 cannot.</p>"},{"location":"8.service-tuning/load-balance/#migrate_partition","title":"Migrate partition","text":"<p>To migrate specified partitions and scale in the cluster, you can run <code>SUBMIT JOB BALANCE DATA REMOVE &lt;ip:port&gt; [,&lt;ip&gt;:&lt;port&gt; ...]</code>.</p> <p>For example, to migrate the partitions in server <code>192.168.8.100:9779</code>, the command as following:</p> <pre><code>nebula&gt; SUBMIT JOB BALANCE DATA REMOVE 192.168.8.100:9779;\nnebula&gt; SHOW HOSTS;\n+-----------------+------+----------+--------------+-----------------------+------------------------+----------------------+\n| Host            | Port | Status   | Leader count | Leader distribution   | Partition distribution | Version              |\n+-----------------+------+----------+--------------+-----------------------+------------------------+----------------------+\n| \"192.168.8.101\" | 9779 | \"ONLINE\" | 15           | \"basketballplayer:15\" | \"basketballplayer:15\"  | \"3.5.0\" |\n| \"192.168.8.100\" | 9779 | \"ONLINE\" | 0            | \"No valid partition\"  | \"No valid partition\"   | \"3.5.0\" |\n+-----------------+------+----------+--------------+-----------------------+------------------------+----------------------+\n</code></pre> <p>Note</p> <p>This command migrates partitions to other storage hosts but does not delete the current storage host from the cluster. To delete the Storage hosts from cluster, see Manage Storage hosts.</p>"},{"location":"8.service-tuning/load-balance/#balance_leader_distribution","title":"Balance leader distribution","text":"<p>To balance the raft leaders, run <code>SUBMIT JOB BALANCE LEADER</code>. It will start a job to balance the distribution of all the storage leaders in all graph spaces.</p>"},{"location":"8.service-tuning/load-balance/#example","title":"Example","text":"<pre><code>nebula&gt; SUBMIT JOB BALANCE LEADER;\n</code></pre> <p>Run <code>SHOW HOSTS</code> to check the balance result.</p> <pre><code>nebula&gt; SHOW HOSTS;\n+------------------+------+----------+--------------+-----------------------------------+------------------------+----------------------+\n| Host             | Port | Status   | Leader count | Leader distribution               | Partition distribution | Version              |\n+------------------+------+----------+--------------+-----------------------------------+------------------------+----------------------+\n| \"192.168.10.101\" | 9779 | \"ONLINE\" | 8            | \"basketballplayer:3\"              | \"basketballplayer:8\"   | \"3.5.0\" |\n| \"192.168.10.102\" | 9779 | \"ONLINE\" | 3            | \"basketballplayer:3\"              | \"basketballplayer:8\"   | \"3.5.0\" |\n| \"192.168.10.103\" | 9779 | \"ONLINE\" | 0            | \"basketballplayer:2\"              | \"basketballplayer:7\"   | \"3.5.0\" |\n| \"192.168.10.104\" | 9779 | \"ONLINE\" | 0            | \"basketballplayer:2\"              | \"basketballplayer:7\"   | \"3.5.0\" |\n| \"192.168.10.105\" | 9779 | \"ONLINE\" | 0            | \"basketballplayer:2\"              | \"basketballplayer:7\"   | \"3.5.0\" |\n+------------------+------+----------+--------------+-----------------------------------+------------------------+----------------------+\n</code></pre> <p>Caution</p> <p>In NebulaGraph 3.5.0, switching leaders will cause a large number of short-term request errors (Storage Error <code>E_RPC_FAILURE</code>). For solutions, see FAQ.</p>"},{"location":"8.service-tuning/practice/","title":"Best practices","text":"<p>NebulaGraph is used in a variety of industries. This topic presents a few best practices for using NebulaGraph. For more best practices, see Blog.</p>"},{"location":"8.service-tuning/practice/#scenarios","title":"Scenarios","text":"<ul> <li>Use cases</li> </ul> <ul> <li>User review</li> </ul> <ul> <li>Performance</li> </ul>"},{"location":"8.service-tuning/practice/#kernel","title":"Kernel","text":"<ul> <li>What is a graph database and what are its use cases - Definition, examples &amp; trends</li> </ul> <ul> <li>NebulaGraph Source Code Explained: Variable-Length Pattern Matching</li> </ul> <ul> <li>Adding a Test Case for NebulaGraph</li> </ul> <ul> <li>BDD-Based Integration Testing Framework for NebulaGraph: Part \u2160</li> </ul> <ul> <li>BDD-Based Integration Testing Framework for NebulaGraph: Part II</li> </ul> <ul> <li>Understanding Subgraph in NebulaGraph</li> </ul> <ul> <li>Full-Text Indexing in NebulaGraph</li> </ul>"},{"location":"8.service-tuning/practice/#ecosystem_tool","title":"Ecosystem tool","text":"<ul> <li>Validating Import Performance of NebulaGraph Importer</li> </ul> <ul> <li>Ecosystem Tools: NebulaGraph Dashboard for Monitoring</li> </ul> <ul> <li>Visualizing Graph Data with NebulaGraph Explorer</li> </ul>"},{"location":"8.service-tuning/super-node/","title":"Processing super vertices","text":""},{"location":"8.service-tuning/super-node/#principle_introduction","title":"Principle introduction","text":"<p>In graph theory, a super vertex, also known as a dense vertex, is a vertex with an extremely high number of adjacent edges. The edges can be outgoing or incoming.</p> <p>Super vertices are very common because of the power-law distribution. For example, popular leaders in social networks (Internet celebrities), top stocks in the stock market, Big Four in the banking system, hubs in transportation networks, websites with high clicking rates on the Internet, and best sellers in E-commerce.</p> <p>In NebulaGraph 3.5.0, a <code>vertex</code> and its <code>properties</code> form a <code>key-value pair</code>, with its <code>VID</code> and other meta information as the <code>key</code>. Its <code>Out-Edge Key-Value</code> and <code>In-Edge Key-Value</code> are stored in the same partition in the form of LSM-trees in hard disks and caches.</p> <p>Therefore, <code>directed traversals from this vertex</code> and <code>directed traversals ending at this vertex</code> both involve either <code>a large number of sequential IO scans</code> (ideally, after Compaction or a large number of <code>random IO</code> (frequent writes to <code>the vertex</code> and its <code>ingoing and outgoing edges</code>).</p> <p>As a rule of thumb, a vertex is considered dense when the number of its edges exceeds 10,000. Some special cases require additional consideration.</p> <p>Note</p> <p>In NebulaGraph 3.5.0, there is not any data structure to store the out/in degree for each vertex. Therefore, there is no direct method to know whether it is a super vertex or not. You can try to use Spark to count the degrees periodically. </p>"},{"location":"8.service-tuning/super-node/#indexes_for_duplicate_properties","title":"Indexes for duplicate properties","text":"<p>In a property graph, there is another class of cases similar to super vertices: a property has a very high duplication rate, i.e., many vertices with the same <code>tag</code> but different <code>VIDs</code> have identical property and property values.</p> <p>Property indexes in NebulaGraph 3.5.0 are designed to reuse the functionality of RocksDB in the Storage Service, in which case indexes are modeled as <code>keys with the same prefix</code>. If the lookup of a property fails to hit the cache, it is processed as a random seek and a sequential prefix scan on the hard disk to find the corresponding VID. After that, the graph is usually traversed from this vertex, so that another random read and sequential scan for the corresponding key-value of this vertex will be triggered. The higher the duplication rate, the larger the scan range.</p> <p>For more information about property indexes, see How indexing works in NebulaGraph.</p> <p>Usually, special design and processing are required when the number of duplicate property values exceeds 10,000.</p>"},{"location":"8.service-tuning/super-node/#suggested_solutions","title":"Suggested solutions","text":""},{"location":"8.service-tuning/super-node/#solutions_at_the_database_end","title":"Solutions at the database end","text":"<ol> <li>Truncation: Only return a certain number (a threshold) of edges, and do not return other edges exceeding this threshold.</li> <li>Compact: Reorganize the order of data in RocksDB to reduce random reads and increase sequential reads.</li> </ol>"},{"location":"8.service-tuning/super-node/#solutions_at_the_application_end","title":"Solutions at the application end","text":"<p>Break up some of the super vertices according to their business significance:</p> <ul> <li> <p>Delete multiple edges and merge them into one.</p> <p>For example, in the transfer scenario <code>(Account_A)-[TRANSFER]-&gt;(Account_B)</code>, each transfer record is modeled as an edge between account A and account B, then there may be tens of thousands of transfer records between <code>(Account_A)</code> and <code>(Account_B)</code>.</p> <p>In such scenarios, merge obsolete transfer details on a daily, weekly, or monthly basis. That is, batch-delete old edges and replace them with a small number of edges representing <code>monthly total</code> and <code>times</code>. And keep the transfer details of the latest month.</p> </li> </ul> <ul> <li> <p>Split an edge into multiple edges of different types.</p> <p>For example, in the <code>(Airport)&lt;-[DEPART]-(Flight)</code> scenario, the departure of each flight is modeled as an edge between a flight and an airport. Departures from a big airport might be enormous.</p> <p>According to different airlines, divide the <code>DEPART</code> edge type into finer edge types, such as <code>DEPART_CEAIR</code>, <code>DEPART_CSAIR</code>, etc. Specify the departing airline in queries (graph traversal).</p> </li> </ul> <ul> <li> <p>Split vertices.</p> <p>For example, in the loan network <code>(person)-[BORROW]-&gt;(bank)</code>, large bank A will have a very large number of loans and borrowers.</p> <p>In such scenarios, you can split the large vertex A into connected sub-vertices A1, A2, and A3.</p> <pre><code>(Person1)-[BORROW]-&gt;(BankA1), (Person2)-[BORROW]-&gt;(BankA2), (Person2)-[BORROW]-&gt;(BankA3);\n(BankA1)-[BELONGS_TO]-&gt;(BankA), (BankA2)-[BELONGS_TO]-&gt;(BankA), (BankA3)-[BELONGS_TO]-&gt;(BankA).\n</code></pre> <p>A1, A2, and A3 can either be three real branches of bank A, such as Beijing branch, Shanghai branch, and Zhejiang branch, or three virtual branches set up according to certain rules, such as <code>A1: 1-1000, A2: 1001-10000 and A3: 10000+</code> according to the number of loans. In this way, any operation on A is converted into three separate operations on A1, A2, and A3.</p> </li> </ul>"},{"location":"9.about-license/1.license-overview/","title":"About NebulaGraph licenses","text":"<p>Enterpriseonly</p> <p>NebulaGraph licenses applies only to the NebulaGraph Enterprise Edition.</p>"},{"location":"9.about-license/1.license-overview/#what_nebulagraph_licenses_do","title":"What NebulaGraph licenses do","text":"<p>NebulaGraph licenses are the legal permissions granted by Vesoft Co., Ltd., allowing you to utilize the capabilities of a NebulaGraph Enterprise Edition database and its associated software. You can buy a NebulaGraph license on a cloud marketplace or by contacting Vesoft's sales team. Currently, the only cloud marketplace available is the AWS Marketplace. You can purchase a NebulaGraph license from NebulaGraph Enterprise (by Node) on the AWS Marketplace.</p> <p>After purchasing a NebulaGraph license, you must obtain a license key by binding an LMID through the LC. Once the license key is obtained, you need to use the LM service to load the license key. When starting the NebulaGraph Enterprise and associated software, the LM service will check the validity of the license. If the license is valid, then the graph database and associated software will function normally. Otherwise, the graph database and associated software will not be functional.</p> <p>You can view the license information, including the expiration date, nodes purchased, and license key on the LC or by using the LM client to query the license information via the command line.</p>"},{"location":"9.about-license/1.license-overview/#license_key","title":"License key","text":"<p>A license key is an encrypted string containing authorization information and serves as the unique credential for you to obtain access to the NebulaGraph Enterprise and its associated software features. There are two forms of license keys: online license keys and offline license keys. For more information, see License key.</p>"},{"location":"9.about-license/1.license-overview/#licensing_process_flowchart","title":"Licensing process flowchart","text":""},{"location":"9.about-license/1.license-overview/#licensing_process","title":"Licensing process","text":""},{"location":"9.about-license/1.license-overview/#purchasing_licenses_on_cloud_marketplaces","title":"Purchasing licenses on cloud marketplaces","text":"<ol> <li>Create a contract for the purchase of a NebulaGraph license through a cloud marketplace service.</li> <li>Follow the setup account link to set up your LC account.</li> <li>Receive email attachments that contain NebulaGraph Enterprise and LM installation packages.</li> <li>View and copy the LMID on your LM.</li> <li>Bind the LMID to generate a license key on LC.</li> <li>Load the license key on LM.</li> <li>Configure the LM address in the NebulaGraph and associated software.</li> <li>Start NebulaGraph and associated software.</li> </ol>"},{"location":"9.about-license/1.license-overview/#purchasing_licenses_through_vesoft_sales_personnel","title":"Purchasing licenses through Vesoft sales personnel","text":"<ol> <li>Contact Vesoft's sales personnel to purchase a NebulaGraph license and obtain NebulaGraph and LM installation packages.</li> <li>Receive an email to set up your LC account.</li> <li>View and copy the LMID on your LM.</li> <li>Bind the LMID to generate a license key on LC.</li> <li>Load the license key on LM.</li> <li>Configure the LM address in the NebulaGraph and associated software.</li> <li>Start NebulaGraph and associated software.</li> </ol>"},{"location":"9.about-license/3.purchase-license/","title":"Purchase a NebulaGraph license","text":"<p>To utilize the features of the NebulaGraph database and associated software, you must obtain a NebulaGraph license. The license can be procured either via the cloud marketplace or by directly contacting Vesoft sales.</p> <p>Currently, AWS Marketplace is the only cloud marketplace from which a license can be obtained. This article assists you in purchasing a NebulaGraph license on the AWS Marketplace.</p>"},{"location":"9.about-license/3.purchase-license/#preparations","title":"Preparations","text":"<p>You have registered an AWS Marketplace account and logged in.</p>"},{"location":"9.about-license/3.purchase-license/#steps","title":"Steps","text":"<p>Note</p> <p>Before purchasing a license on the AWS Marketplace, it is recommended that you contact Vesoft sales for detailed information about the license.</p> <ol> <li>Open the AWS Marketplace NebulaGraph Enterprise (by Node) service page.</li> <li>Click View purchase options to enter the license purchase contract signing page.</li> <li> <p>Configure the contract items, which include the license validity period, auto-renewal setting, and the number of nodes to be purchased.</p> <ul> <li>How long do you want your contract to run: Choose the validity period for the license, either 1 month or 1 year.</li> <li>Renewal Settings: Whether to automatically renew the license after its validity period.<ul> <li>Yes: The license will be automatically renewed after its validity period. </li> <li>No: The license will not be automatically renewed after its validity period.</li> </ul> </li> <li>Contract Options: Select the number of resources to purchase, currently supporting the purchase of query nodes and storage nodes.</li> </ul> </li> <li> <p>Click Create contract.</p> </li> <li>In the pop-up panel, confirm purchase information and click Pay now.</li> <li> <p>Click Set up your account for LC registration and to start managing the license. For details, see set up an LC account. </p> <p>You can view the license information on LC.</p> <p>Note</p> <p>Once the registration process is finished, you will receive an email from Vesoft within one business day containing the complete NebulaGraph packages, which include not only the database, but also other software such as LM, NebulaGraph Explorer, and more.</p> </li> </ol>"},{"location":"9.about-license/3.purchase-license/#next_to_do","title":"Next to do","text":"<p>After purchasing a license, you must generate a license key and then load it into the LM service. Following this, the NebulaGraph database and its associated software will verify the validity of the license key through the LM service at startup. If the license key is valid, the NebulaGraph database and associated software can operate normally. The following steps describe how to generate and load a license key:</p> <ul> <li>Install LM</li> <li>Generate the license key</li> <li>Load the license key</li> </ul> <p>For more information about how to use a license, see Licensing process.</p>"},{"location":"9.about-license/4.manage-license/","title":"Manage licenses","text":"<p>This article provides instructions on managing licenses, including license renewal, license node expansion, and viewing online and offline license keys.</p>"},{"location":"9.about-license/4.manage-license/#preparations","title":"Preparations","text":"<ul> <li>You have generated a license key on LC</li> <li>You have loaded the license key into the LM</li> </ul>"},{"location":"9.about-license/4.manage-license/#renew_licenses","title":"Renew licenses","text":"<ul> <li>For licenses purchased through Vesoft's sales team, you need to contact the sales team to renew them.</li> <li> <p>For licenses purchased on the cloud marketplace platform, follow these steps for renewal:</p> <ol> <li>On the LC homepage, navigate to the LICENSES LIST section, and find the target license card.</li> <li>Click RENEW to enter the cloud marketplace renewal page.</li> <li>Click Modify renewal terms and select the renewal period, which can be either 1 month or 1 year.</li> <li>Click Modify renewal.</li> </ol> <p>After a successful renewal, if the license key loaded in the LM is online, your LM will automatically synchronize the license information. If it's offline, you need to copy the new offline license key from LC, and then reload this new offline license key into your LM.</p> </li> </ul>"},{"location":"9.about-license/4.manage-license/#expand_license_node_count","title":"Expand license node count","text":"<ul> <li>For licenses purchased through sales team, you need to contact the sales team to increase the number of nodes.</li> <li> <p>For licenses purchased on the cloud marketplace platform,  follow these steps to expand the license node count:</p> <ol> <li>In the LICENSES LIST section of the LC homepage, find the target license card.</li> <li>Click RENEW to enter the cloud marketplace renewal page.</li> <li>Click Upgrade current contract and select the number of nodes to be added.</li> <li>Click Modify current contract.</li> </ol> <p>After successfully expanding the node count, if the license key loaded in the LM is online, the LM will automatically synchronize the license information. If it's an offline license key, you need to copy the new offline license key from LC, and then reload this new offline license key into the LM.</p> </li> </ul>"},{"location":"9.about-license/4.manage-license/#view_online_and_offline_license_keys","title":"View online and offline license keys","text":"<ol> <li>In the LICENSES LIST section of the LC homepage, find the target license card.</li> <li>Click VIEW DETAILS to enter the license information page.</li> <li>In the Basic Info section, click EDIT LM ID.</li> <li>In the pop-up panel, select Online or Offline, and click CONFIRM.</li> <li>In the LICENSES KEY section, view the corresponding license key in the selected mode.</li> </ol>"},{"location":"9.about-license/2.license-management-suite/1.suite-overview/","title":"License management suites overview","text":"<p>The license management suites are a combination of a platform and services designed to enable you to obtain authorized access to the NebulaGraph Enterprise Edition database and its associated software. These suites include license purchase services, the publicly accessible license management platform known as the License Center (LC), and the client-side license management service called the License Manager (LM).</p>"},{"location":"9.about-license/2.license-management-suite/1.suite-overview/#nebulagraph_enterprise_by_node","title":"NebulaGraph Enterprise (by Node)","text":"<p>NebulaGraph Enterprise (by Node) is a service offered by Vesoft on AWS Marketplace, which allows you to easily sign contracts, purchase, or update Vesoft licenses. For more information on this service, see Purchase Licenses.</p> <p>Note</p> <p>Currently, the license purchase service is available exclusively on AWS Marketplace. However, Vesoft plans to expand to more cloud marketplaces for license purchases in the future.</p>"},{"location":"9.about-license/2.license-management-suite/1.suite-overview/#license_center","title":"License Center","text":"<p>License Center (LC) by Vesoft is a publicly accessible platform that is used to record and manage all purchased licenses. Its main purpose is to enable you to view your license information through public access, including the license key, valid duration, number of querying nodes, number of storage nodes, and other relevant details. For more information, see License Center (LC).</p>"},{"location":"9.about-license/2.license-management-suite/1.suite-overview/#license_manager","title":"License Manager","text":"<p>Vesoft's License Manager (LM) is an essential service that operates in the background to manage your NebulaGraph licenses and license the NebulaGraph Enterprise Edition database and associated software. The LM client tool enables you to query and view your license information conveniently from the client side. This information includes the license key, validation period, and the number of resources purchased. For more information, see License Manager (LM).</p>"},{"location":"9.about-license/2.license-management-suite/2.license-center/","title":"License Center","text":"<p>License Center (LC) provided by Vesoft is an online platform for managing licenses that is accessible through public networks. On the LC platform, you can track all your purchased license information, including details such as license type, number of purchased resources, the status of the license, and expiration date. </p> <p></p> <p>To generate a license key, you need to bind the ID of your License Manager (LM) on LC. The license key must then be loaded into the installed LM service. And after specifying the LM access address in the software, you can authorize the license which enables you to use NebulaGraph Enterprise.</p> <p>This article introduces how to set up an LC account, bind the LMID, and generate the license key.</p>"},{"location":"9.about-license/2.license-management-suite/2.license-center/#preparations","title":"Preparations","text":"<p>To use LC, you must first purchase a NebulaGraph license. For more information, see Purchase a license.</p>"},{"location":"9.about-license/2.license-management-suite/2.license-center/#set_up_an_lc_account","title":"Set up an LC account","text":"<p>To use LC, you must first set up an LC account. </p> <ol> <li> <p>Go to the LC account setup page.</p> <p></p> <p>The entry to the LC account setup page varies depending on how you purchase your license:</p> <ul> <li>For purchasing a license on a cloud marketplace, go to the cloud marketplace service page and then click Click here to set up your account. </li> <li>For purchasing a license through Vesoft sales personnel, go to the email sent by Vesoft and then click Setup License.</li> </ul> </li> <li> <p>Click Register.</p> <p></p> </li> <li> <p>Fill in your email address, password, and company name, and tick the I have read and agreed to the Terms of Use and Privacy Policy box.</p> <p>Caution</p> <ul> <li>Make sure the email address is valid, as you will receive a verification email after registration.</li> <li>The password must be between 12 and 30 characters long and contain numbers, letters, and special characters.</li> </ul> </li> <li> <p>Click Register to complete the registration.</p> </li> <li>Open the verification email you received, and click on Activate to go to the LC login page.</li> <li>Enter your email address and password, and click Login to log in to LC.</li> </ol>"},{"location":"9.about-license/2.license-management-suite/2.license-center/#bind_lmid_to_generate_a_license_key","title":"Bind LMID to generate a license key","text":"<p>After you log in to LC, you need bind the ID of your LM to generate a license key. </p> <p>Caution</p> <p>Each license can only be bound to one LMID, and the unbinding of LMIDs is not supported.</p>"},{"location":"9.about-license/2.license-management-suite/2.license-center/#quickly_bind_lmid","title":"Quickly bind LMID","text":"<p>You are guided to bind the LMID every time you log in to LC after you buy a new license. Binding the LMID is a prerequisite to generate a license key for using the license. You can also skip the quick binding and bind the LMID on the license information page.</p> <p>The following describes how to quickly bind the LMID:</p> <ol> <li> <p>On the quick binding page, check the information of the purchased license, and click Next.</p> </li> <li> <p>Bind the LMID by the following steps and then click Next.</p> <ol> <li>Install the LM service. For how to install the LM service, see LM.</li> <li>View the LMID. For how to view the LMID, see LM.</li> <li> <p>Fill in the LMID and select Online or Offline. </p> <ul> <li> <p>Online </p> <p>Select the Online mode to generate an online license key. </p> </li> </ul> <ul> <li> <p>Offline </p> <p>Select the Offline mode to generate an offline license key. After you enter the offline license key into your LM, the LM service stores fixed license information.</p> </li> </ul> <p>For more information, see License key.</p> </li> <li> <p>Click BIND LMID to complete the binding.</p> </li> </ol> </li> <li> <p>View the license key generated after binding the LMID and click Close to complete the binding.</p> </li> <li> <p>(Optional) Copy the license key and load it into the LM service. For how to load the license key, see LM.</p> </li> </ol> <p>Note</p> <p>You can choose a license key type based on your LM accessibility. </p> <ul> <li>If your LM is accessible from the internet, you can select either Online or Offline mode. The Online mode is recommended, as it generates an online license key.</li> <li>If your LM is not accessible from the internet, then Offline mode is the only option available for generating an offline license key, as it can't reach out to the license server to validate the key itself.</li> </ul>"},{"location":"9.about-license/2.license-management-suite/2.license-center/#bind_lmid_on_the_license_information_page","title":"Bind LMID on the license information page","text":"<p>If you skip the quick binding, you can still bind the LMID on the license information page.</p> <ol> <li>On the targeted license details page, click Bind License Manager ID.</li> <li>In the pop-up panel, enter the ID of your LM. For how to view LMID, see LM.</li> <li> <p>Select Online or Offline, and then click CONFIRM to bind the LMID.</p> <ul> <li>Select Online to generate an online license key, so that LM can get the latest license information from LC every 1 ~ 2 hours.</li> </ul> <ul> <li>Select Offline to generate an offline license key, which means LM obtains fixed license information. If you need to update the license information, you must obtain a new offline license key.</li> </ul> </li> <li> <p>In the License Key section, view the license key generated after binding the LMID.</p> </li> <li>(Optional) Copy the license key and load it into the LM service. For how to load the license key, see LM.</li> </ol>"},{"location":"9.about-license/2.license-management-suite/2.license-center/#license_information","title":"License information","text":"<p>In the LICENSES LIST section of the LC homepage, click VIEW DETAILS to access the License Info page. </p>"},{"location":"9.about-license/2.license-management-suite/2.license-center/#basic_information","title":"Basic information","text":"<ul> <li>LMID\uff1aIndicates the ID of the LM service that you installed (If not bound, this field will be empty).</li> <li>License Type: Currently limited to the purchase of node-based resources.</li> <li>Start At and Expire Time: Indicates the active and expiry dates of the license.</li> </ul>"},{"location":"9.about-license/2.license-management-suite/2.license-center/#resources","title":"Resources","text":"<p>In the Purchased Resources section, you can view the purchased query and storage node quantities and statuses, as well as the complimentary software names and statuses.</p>"},{"location":"9.about-license/2.license-management-suite/2.license-center/#license_key","title":"License key","text":"<p>After you bind the LMID, a license key is automatically generated and the License Key section displays the license key information.</p> <ul> <li> <p>Online license keys</p> <p>An online license allows you to obtain the latest license information from LC. </p> <p>When binding your LMID, select the Online mode to generate an online license key. After you load the key into the LM service, the LM can retrieve the latest license information regularly.</p> </li> </ul> <ul> <li> <p>Offline license keys</p> <p>Compared to an online license key, an offline license key contains fixed license information. If the license information is updated, a new offline license key must be obtained.</p> <p>When binding your LMID, select the Offline mode to generate an offline license key. Compared to an online license key, after you load an offline license key into your LM, the LM service stores fixed license information. If the license information is updated, a new offline license key must be obtained.</p> </li> </ul>"},{"location":"9.about-license/2.license-management-suite/2.license-center/#subscription","title":"Subscription","text":"<p>This section is only displayed when you purchase a license on a cloud marketplace. In this section, you can view the subscription ID of the cloud marketplace where your license is purchased, your subscription platform account, product ID, and subscription details.</p>"},{"location":"9.about-license/2.license-management-suite/3.license-manager/","title":"License Manager","text":"<p>A License Manager (LM) is an essential service that runs on a server for you to manage your license and license the NebulaGraph enterprise edition database and its associated software. You can use an LM client that communicates with the LM service to load license keys and view license information, including the license validity period and purchased nodes. By configuring the LM service address in the NebulaGraph database and its associated software, the validity of the license can be verified to ensure the normal use of the NebulaGraph database and its associated software.</p> <p>This article introduces how to deploy and use an LM service in a Linux environment and how to configure it within the Nebula Graph database and its associated software. For information on how to deploy the LM in a K8s cluster, see Deploy LM.</p>"},{"location":"9.about-license/2.license-management-suite/3.license-manager/#preparations","title":"Preparations","text":"<p>To use an LM, you need to make sure the following:</p> <ul> <li>You have purchased a NebulaGraph license.</li> </ul> <ul> <li> <p>You have obtained the desired LM installation package.</p> <p>Note</p> <p>LM installation packages are sent to you by email after you purchase a license.</p> </li> </ul> <ul> <li>LM uses <code>9119</code> as the default port, make sure that port is not occupied.</li> </ul>"},{"location":"9.about-license/2.license-management-suite/3.license-manager/#notes","title":"Notes","text":"<ul> <li>An LM is a single-process service. To ensure the reliability and continuity of the LM, it is recommended that you use systemd to manage the LM and set a restart policy for the LM.</li> <li>The time on the LM server must be synchronized with the services (including the NebulaGraph database and associated software) connected to the LM. If the times are not in sync, license verification will fail, which can prevent the service from being used.</li> </ul>"},{"location":"9.about-license/2.license-management-suite/3.license-manager/#install_and_start_lm","title":"Install and start LM","text":"<p>An LM can be installed on Linux amd64 or arm64 systems, or installed through Dashboard.</p>"},{"location":"9.about-license/2.license-management-suite/3.license-manager/#using_the_tar_package","title":"Using the TAR Package","text":"<ol> <li> <p>Unpack the LM TAR package.</p> <pre><code>tar -zxvf &lt;name.tar.gz&gt; -C &lt;path&gt;\n</code></pre> <ul> <li><code>&lt;name.tar.gz&gt;</code>: The name of the LM TAR package.</li> <li><code>&lt;path&gt;</code>: The installation path for the unpacked LM. If the <code>-C</code> parameter is not specified, it defaults to the current directory.</li> </ul> </li> <li> <p>Start the LM service using systemd.</p> <ol> <li> <p>Create the LM service file <code>/etc/systemd/system/nebula-license-manager.service</code> with the following contents:</p> <pre><code>[Unit]\nDescription=License Manager\n[Service]\nType=simple\nExecStart=&lt;path&gt;/nebula-license-manager/nebula-license-manager\nWorkingDirectory=&lt;path&gt;/nebula-license-manager\nRestart=always\n[Install]\nWantedBy=multi-user.target\n</code></pre> <ul> <li><code>&lt;path&gt;</code>: Refers to the directory where the LM package is extracted.</li> </ul> </li> <li> <p>Start the LM service:</p> <pre><code>sudo systemctl start nebula-license-manager\n</code></pre> </li> </ol> </li> <li> <p>Set up LM to start automatically on boot.</p> <pre><code>sudo systemctl enable nebula-license-manager\n</code></pre> </li> </ol>"},{"location":"9.about-license/2.license-management-suite/3.license-manager/#using_the_rpm_package","title":"Using the RPM package","text":"<ol> <li> <p>Unpack the LM RPM package.</p> <pre><code>sudo rpm -ivh &lt;name.rpm&gt;\n</code></pre> <ul> <li><code>&lt;name.rpm&gt;</code>: The name of the LM RPM package.</li> <li>The default installation path is <code>/usr/local/nebula-license-manager</code>, which cannot be changed.</li> </ul> </li> <li> <p>Start LM.</p> <pre><code>sudo systemctl start nebula-license-manager\n</code></pre> </li> <li> <p>Set up LM to start automatically on boot.</p> <pre><code>sudo systemctl enable nebula-license-manager\n</code></pre> </li> </ol>"},{"location":"9.about-license/2.license-management-suite/3.license-manager/#using_the_deb_package","title":"Using the DEB package","text":"<ol> <li> <p>Unpack the LM DEB package.</p> <pre><code>sudo dpkg -i &lt;name.deb&gt;\n</code></pre> <ul> <li><code>&lt;name.deb&gt;</code>: The name of the LM DEB package.</li> <li>The default installation path is <code>/usr/local/nebula-license-manager</code>, which cannot be changed.</li> </ul> </li> <li> <p>Start LM.</p> <pre><code>sudo systemctl start nebula-license-manager\n</code></pre> </li> <li> <p>Set up LM to start automatically on boot.</p> <pre><code>sudo systemctl enable nebula-license-manager\n</code></pre> </li> </ol>"},{"location":"9.about-license/2.license-management-suite/3.license-manager/#using_dashboard","title":"Using Dashboard","text":"<p>LM can be installed and started through Dashboard. For more information, see Connect to Dashboard.</p>"},{"location":"9.about-license/2.license-management-suite/3.license-manager/#view_lm_configuration_file","title":"View LM configuration file","text":"<p>The configuration file name for LM is <code>nebula-license-manager.yaml</code>.</p> <ul> <li>For RPM and DEB packages, the default path is <code>/usr/local/nebula-license-manager/etc/nebula-license-manager.yaml</code>.</li> </ul> <ul> <li>For the TAR package, the path is <code>nebula-license-manager/etc/nebula-license-manager.yaml</code> under the LM installation directory.</li> </ul> <p>The contents of the LM configuration file are as follows:</p> <pre><code>Name: nebula-license-manager  \nHost: 0.0.0.0                 # The host address LM binds to.   \nPort: 9119                    # The port number LM listens on. The default is 9119.\nTimeout: 3000                 # The timeout period for LM waiting for client requests, in milliseconds. The default is 3000 milliseconds.   \nDataPath: data                # The path for storing LM status data.\nNotify:                       # LM notification related configuration. \n  Mail:                       # Mail notification related configuration.\n    Host: \"\"                  # SMTP server address.\n    Port: 465                 # SMTP server port number.\n    User: \"\"                  # SMTP email.\n    Password: \"\"              # SMTP email password.  \n    To: []                    # The list of emails to receive notifications.   \nLog:                          # Logging related configuration.\n  Mode: file                  # The logging mode. It can be file (output to file) or console (output to console). The default is file.\n  Path: logs                  # The path for storing log files.\n  Level: info                 # The log level. It can be debug, info, error, or severe. The default is info. \n  KeepDays: 30                # The longest number of days to keep logs. The default is 30 days.\n</code></pre>"},{"location":"9.about-license/2.license-management-suite/3.license-manager/#use_lm","title":"Use LM","text":"<p>After your LM starts, in the LM installation path you can use the LM CLI to view license information.</p>"},{"location":"9.about-license/2.license-management-suite/3.license-manager/#view_lm_cli_version","title":"View LM CLI version","text":"<pre><code>./nebula-license-manager-cli version\n</code></pre> <p>Note</p> <p>When LM starts, its version information is printed in the logs.</p>"},{"location":"9.about-license/2.license-management-suite/3.license-manager/#load_a_license_key","title":"Load a license key","text":"<p>After generating a license key, you need to use the LM client tool to load the license key.</p> <pre><code>./nebula-license-manager-cli load --key &lt;license-key&gt; --force\n</code></pre> <ul> <li><code>&lt;license-key&gt;</code>: The license key string, such as <code>MSY2-LGQ6O-69521-XXXXX-XXXXX</code>.</li> <li><code>--force</code>: Loads the license key without checking the current license status. This flag is optional.</li> </ul>"},{"location":"9.about-license/2.license-management-suite/3.license-manager/#view_license_information","title":"View license information","text":"<pre><code>./nebula-license-manager-cli info\n</code></pre> <ul> <li> <p>When the license key is not loaded, the output is as follows:</p> <pre><code>          LMID:  RUZB-XXXX                     \n LicenseStatus:  NotExist\n</code></pre> </li> </ul> <ul> <li> <p>When the license key is loaded, the output is as follows:</p> <pre><code>          LMID:  RUZB-XXXX                    \n LicenseStatus:  Normal                      \n    LicenseKey:  MM90U-9H4Q0-W093M-XXXXX-XXXXX \n          Type:  NODE                          \n    Query Node:  3                             \n  Storage Node:  3                             \n      ExpireAt:  2023-06-25 12:00:00 +0800 CST \n</code></pre> </li> </ul> <p>The information items of the license in the output are described as follows:</p> Items Description <code>LMID</code> The ID of your LM. When you obtain a license key, this LMID needs to be bound. For more information, see Generate a license key. <code>LicenseStatus</code> The status of the license. It includes:<code>Normal</code>: The license can be used normally.<code>NotExist</code>: The license key does not exist.<code>Invalid</code>: The license key is invalid.<code>Syncing</code>: Synchronizing the license information from LC.<code>Expiring</code>: The license is about to expire.<code>Expired</code>: The license has expired. <code>LicenseKey</code> An encrypted string containing authorization information, which is the only credential for you to obtain the authorization of the NebulaGraph database and its associated software. For details, see License key. <code>Type</code> The type of resources purchased. Currently, only node-based resources can be purchased. <code>Query Node</code> The number of query nodes purchased <code>Storage Node</code> The number of storage nodes purchased <code>ExpireAt</code> The expiration time of the license."},{"location":"9.about-license/2.license-management-suite/3.license-manager/#synchronize_license_info","title":"Synchronize license info","text":"<p>When the license key loaded into LM is in online mode, the LM periodically synchronizes the license information from LC every one to two hours. You can also manually synchronize the license key using the following command.</p> <pre><code>./nebula-license-manager-cli sync\n</code></pre>"},{"location":"9.about-license/2.license-management-suite/3.license-manager/#check_the_license_quota_usage","title":"Check the license quota usage","text":"<p>You can run the following command to check the current usage of the license quota (the number of nodes purchased) and the usage status of the associated software.</p> <pre><code>./nebula-license-manager-cli usage\n</code></pre>"},{"location":"9.about-license/2.license-management-suite/3.license-manager/#view_the_license_information_on_a_specified_lm","title":"View the license information on a specified LM","text":"<p>For a database administrator (DBA), there may be a need to view the license information on a specified LM. To achieve this, run the following command:</p> <pre><code>./nebula-license-manager-cli &lt;command&gt; --addr &lt;host&gt;:9119\n</code></pre> <ul> <li><code>&lt;command&gt;</code>: The command to be executed. Options include <code>info</code>, <code>usage</code>, <code>sync</code>, and <code>load --key &lt;license-key&gt; --force</code>.</li> <li><code>&lt;host&gt;</code>: The IP address of the host where the specified LM is located.</li> </ul>"},{"location":"9.about-license/2.license-management-suite/3.license-manager/#monitor_lm","title":"Monitor LM","text":""},{"location":"9.about-license/2.license-management-suite/3.license-manager/#monitor_lm_status","title":"Monitor LM status","text":"<p>You can use monitoring tools to monitor the status of the LM service.</p> <ul> <li> <p>Use Dashboard Enterprise</p> <p>When LM is running normally, the License Manager page displays the status of LM as Running, otherwise, it displays Exited. For more information, see License Manager.</p> </li> </ul> <ul> <li> <p>Use Prometheus</p> <p>You need to configure the Prometheus server before monitoring LM. </p> <p>Add the following configuration to the Prometheus configuration file. For more information about the Prometheus configuration file, see Prometheus Configuration.</p> <pre><code>...\n- job_name: license-manager\n  scrape_interval: 15s                # The interval for pulling data.\n  metrics_path: /metrics              # The monitoring metrics path of LM, which is `/metrics`.\n  scheme: http                        # The protocol type of LM, which is HTTP.\n  static_configs:                     # The address and port of LM (default 9119).\n  - targets: \n    - [&lt;ip:lm_port&gt;]\n...\n</code></pre> <p>After the configuration is complete, you can monitor the status of LM through its built-in metric <code>up</code>. If the value is <code>1</code>, it means that LM is running normally; if the value is <code>0</code>, it means that LM is not running. To view the metric, enter <code>up</code> in the Prometheus query box as shown below:</p> <pre><code>up{instance=\"&lt;ip:lm_port&gt;\", job=\"job_name\"}\n</code></pre> <ul> <li><code>&lt;ip:lm_port&gt;</code>: The IP address and port of LM.</li> <li><code>job_name</code>: The name of the job.</li> </ul> <p>For example, up{instance=\"192.168.8.xxx:9119\", job=\"license-manager\"}.</p> <p>Note</p> <p>By default, LM uses port <code>9119</code>. If you need to change the port number, you can modify the value of the <code>Port</code> field in the LM configuration file above, or modify the value of <code>port</code> in the YAML file of Deploying LM in K8s.</p> </li> </ul>"},{"location":"9.about-license/2.license-management-suite/3.license-manager/#view_lm_metrics","title":"View LM metrics","text":"<p>You can view the built-in metrics of LM through the following URL:</p> <pre><code>http://&lt;ip:lm_port&gt;/metrics\n</code></pre> <ul> <li><code>&lt;ip:lm_port&gt;</code>: The IP address and port of LM.</li> </ul> <p>You can also collect LM metrics data through monitoring tools. To use Prometheus, you need to configure the Prometheus server before collecting LM metrics. For more information, see the above section Monitor LM status.</p>"},{"location":"9.about-license/2.license-management-suite/3.license-manager/#configure_connection_to_lm","title":"Configure connection to LM","text":""},{"location":"9.about-license/2.license-management-suite/3.license-manager/#configure_lm_in_nebulagraph","title":"Configure LM in NebulaGraph","text":"<p>In the NebulaGraph database Meta service configuration file (<code>nebula-metad.conf</code>), set the <code>license_manager_url</code> value to reflect the IP address of the LM host and port number <code>9119</code> in the format like <code>192.168.8.xxx:9119</code>. For more information, see Meta service configuration.</p> <p>After the configuration is complete, restart the Meta service. </p>"},{"location":"9.about-license/2.license-management-suite/3.license-manager/#configure_lm_in_explorer","title":"Configure LM in Explorer","text":"<p>In the Explorer installation directory, enter the <code>config</code> folder and modify the <code>app-config.yaml</code> file. Set the value of <code>LicenseManagerURL</code> to reflect the IP address of the LM host and port number <code>9119</code> in the format like <code>192.168.8.xxx:9119</code>.</p> <p>After the configuration is complete, restart Explorer. For more information, see Deploy Explorer.</p>"},{"location":"9.about-license/2.license-management-suite/3.license-manager/#configure_lm_in_dashboard","title":"Configure LM in Dashboard","text":"<p>In the Dashboard installation directory, enter the <code>etc</code> folder and modify the <code>config.yaml</code> file. Set the value of <code>LicenseManagerURL</code> to reflect the IP address of the LM host and port number <code>9119</code> in the format like <code>192.168.8.xxx:9119</code>.</p> <p>After the configuration is complete, restart Dashboard. For more information, see Deploy Dashboard.</p>"},{"location":"9.about-license/2.license-management-suite/3.license-manager/#configure_lm_in_analytics","title":"Configure LM in Analytics","text":"<p>In the Analytics installation directory, enter the <code>scripts</code> folder and modify the <code>analytics.conf</code> file. Set the value of <code>license_manager_url</code> to the IP address of the LM host and the port number <code>9119</code>, for example, <code>192.168.8.xxx:9119</code>.</p> <p>After the configuration is complete, run <code>./run_pagerank.sh</code> in the <code>scripts</code> folder to start the Analytics service. For more information, see NebulaGraph Analytics.</p>"},{"location":"9.about-license/2.license-management-suite/3.license-manager/#configure_lm_in_nebulagraph_operator","title":"Configure LM in NebulaGraph Operator","text":"<ul> <li>When deploying the cluster using Kubectl, configure the address and port of the LM through the <code>spec.metad.licenseManagerURL</code> field in the cluster configuration file. For more details, see Deploying with Kubectl.</li> </ul> <ul> <li>When deploying the cluster using Helm, specify the address and port of the LM with <code>--set nebula.metad.licenseManagerURL</code>. For more details, see Deploying with Helm.</li> </ul>"},{"location":"9.about-license/2.license-management-suite/3.license-manager/#faq","title":"FAQ","text":"<p>Q: Can I change the host on which my LM is located?</p> <p>A: No. The LM is bound to the host where it is installed. If you need to change the host, or the host is unable to be used, you need to contact Vesoft sales to rebind the LMID.</p>"},{"location":"backup-and-restore/3.manage-snapshot/","title":"Backup and restore data with snapshots","text":"<p>NebulaGraph supports using snapshots to back up and restore data. When data loss or misoperation occurs, the data will be restored through the snapshot.</p>"},{"location":"backup-and-restore/3.manage-snapshot/#prerequisites","title":"Prerequisites","text":"<p>NebulaGraph authentication is disabled by default. In this case, all users can use the snapshot feature.</p> <p>If authentication is enabled, only the GOD role user can use the snapshot feature. For more information about roles, see Roles and privileges.</p>"},{"location":"backup-and-restore/3.manage-snapshot/#precautions","title":"Precautions","text":"<ul> <li>To prevent data loss, create a snapshot as soon as the system structure changes, for example, after operations such as <code>ADD HOST</code>, <code>DROP HOST</code>, <code>CREATE SPACE</code>, <code>DROP SPACE</code>, and <code>BALANCE</code> are performed.</li> </ul> <ul> <li>NebulaGraph cannot automatically delete the invalid files created by a failed snapshot task. You have to manually delete them by using <code>DROP SNAPSHOT</code>.</li> </ul> <ul> <li>Customizing the storage path for snapshots is not supported for now. The default path is <code>/usr/local/nebula/data</code>.</li> </ul>"},{"location":"backup-and-restore/3.manage-snapshot/#snapshot_form_and_path","title":"Snapshot form and path","text":"<p>NebulaGraph snapshots are stored in the form of directories with names like <code>SNAPSHOT_2021_03_09_08_43_12</code>. The suffix <code>2021_03_09_08_43_12</code> is generated automatically based on the creation time (UTC).</p> <p>When a snapshot is created, snapshot directories will be automatically created in the <code>checkpoints</code> directory on the leader Meta server and each Storage server.</p> <p>To fast locate the path where the snapshots are stored, you can use the Linux command <code>find</code>. For example:</p> <pre><code>$ find |grep 'SNAPSHOT_2021_03_09_08_43_12'\n./data/meta2/nebula/0/checkpoints/SNAPSHOT_2021_03_09_08_43_12\n./data/meta2/nebula/0/checkpoints/SNAPSHOT_2021_03_09_08_43_12/data\n./data/meta2/nebula/0/checkpoints/SNAPSHOT_2021_03_09_08_43_12/data/000081.sst\n...\n</code></pre>"},{"location":"backup-and-restore/3.manage-snapshot/#create_snapshots","title":"Create snapshots","text":"<p>Run <code>CREATE SNAPSHOT</code> to create a snapshot for all the graph spaces based on the current time for NebulaGraph. Creating a snapshot for a specific graph space is not supported yet.</p> <p>Note</p> <p>If the creation fails, delete the snapshot and try again.</p> <pre><code>nebula&gt; CREATE SNAPSHOT;\n</code></pre>"},{"location":"backup-and-restore/3.manage-snapshot/#view_snapshots","title":"View snapshots","text":"<p>To view all existing snapshots, run <code>SHOW SNAPSHOTS</code>.</p> <pre><code>nebula&gt; SHOW SNAPSHOTS;\n+--------------------------------+---------+------------------+\n| Name                           | Status  | Hosts            |\n+--------------------------------+---------+------------------+\n| \"SNAPSHOT_2021_03_09_08_43_12\" | \"VALID\" | \"127.0.0.1:9779\" |\n| \"SNAPSHOT_2021_03_09_09_10_52\" | \"VALID\" | \"127.0.0.1:9779\" |\n+--------------------------------+---------+------------------+\n</code></pre> <p>The parameters in the return information are described as follows.</p> Parameter Description <code>Name</code> The name of the snapshot directory. The prefix <code>SNAPSHOT</code> indicates that the file is a snapshot file, and the suffix indicates the time the snapshot was created (UTC). <code>Status</code> The status of the snapshot. <code>VALID</code> indicates that the creation succeeded, while <code>INVALID</code> indicates that it failed. <code>Hosts</code> IP addresses and ports of all Storage servers at the time the snapshot was created."},{"location":"backup-and-restore/3.manage-snapshot/#delete_snapshots","title":"Delete snapshots","text":"<p>To delete a snapshot with the given name, run <code>DROP SNAPSHOT</code>.</p> <pre><code>DROP SNAPSHOT &lt;snapshot_name&gt;;\n</code></pre> <p>Example:</p> <pre><code>nebula&gt; DROP SNAPSHOT SNAPSHOT_2021_03_09_08_43_12;\nnebula&gt; SHOW SNAPSHOTS;\n+--------------------------------+---------+------------------+\n| Name                           | Status  | Hosts            |\n+--------------------------------+---------+------------------+\n| \"SNAPSHOT_2021_03_09_09_10_52\" | \"VALID\" | \"127.0.0.1:9779\" |\n+--------------------------------+---------+------------------+\n</code></pre>"},{"location":"backup-and-restore/3.manage-snapshot/#restore_data_with_snapshots","title":"Restore data with snapshots","text":"<p>Warning</p> <p>When you restore data with snapshots, make sure that the graph spaces backed up in the snapshot have not been dropped. Otherwise, the data of the graph spaces cannot be restored.</p> <p>Currently, there is no command to restore data with snapshots. You need to manually copy the snapshot file to the corresponding folder, or you can make it by using a shell script. The logic implements as follows:</p> <ol> <li> <p>After the snapshot is created, the <code>checkpoints</code> directory is generated in the installation directory of the leader Meta server and all Storage servers, and saves the created snapshot. Taking this topic as an example, when there are two graph spaces, the snapshots created are saved in <code>/usr/local/nebula/data/meta/nebula/0/checkpoints</code>, <code>/usr/local/nebula/data/storage/ nebula/3/checkpoints</code> and <code>/usr/local/nebula/data/storage/nebula/4/checkpoints</code>.</p> <pre><code>$ ls /usr/local/nebula/data/meta/nebula/0/checkpoints/\nSNAPSHOT_2021_03_09_09_10_52\n$ ls /usr/local/nebula/data/storage/nebula/3/checkpoints/\nSNAPSHOT_2021_03_09_09_10_52\n$ ls /usr/local/nebula/data/storage/nebula/4/checkpoints/\nSNAPSHOT_2021_03_09_09_10_52\n</code></pre> </li> <li> <p>To restore the lost data through snapshots, you can take a snapshot at an appropriate time, copy the folders <code>data</code> and <code>wal</code> in the corresponding snapshot directory to its parent directory (at the same level with <code>checkpoints</code>) to overwrite the previous <code>data</code> and <code>wal</code>, and then restart the cluster.</p> <p>Caution</p> <p>The data and wal directories of all Meta servers should be overwritten at the same time. Otherwise, the new leader Meta server will use the latest Meta data after a cluster is restarted. </p> </li> </ol>"},{"location":"backup-and-restore/nebula-br/1.what-is-br/","title":"What is Backup &amp; Restore","text":"<p>Backup &amp; Restore (BR for short) is a Command-Line Interface (CLI) tool to back up data of graph spaces of NebulaGraph and to restore data from the backup files.</p>"},{"location":"backup-and-restore/nebula-br/1.what-is-br/#features","title":"Features","text":"<p>The BR has the following features. It supports:</p> <ul> <li>Backing up and restoring data in a one-click operation.</li> <li>Restoring data in the following backup file types:<ul> <li>Local Disk (SSD or HDD). It is recommend to use local disk in test environment only.</li> <li>Amazon S3 compatible interface, such as Alibaba Cloud OSS, MinIO,Ceph RGW, etc.</li> </ul> </li> <li>Backing up and restoring the entire NebulaGraph cluster.</li> <li>Backing up data of specified graph spaces (experimental).</li> </ul>"},{"location":"backup-and-restore/nebula-br/1.what-is-br/#limitations","title":"Limitations","text":"<ul> <li>Supports NebulaGraph v3.x only.</li> <li>Supports full backup, but not incremental backup.</li> <li>Currently, NebulaGraph Listener and full-text indexes do not support backup.</li> <li>If you back up data to the local disk, the backup files will be saved in the local path of each server. You can also mount the NFS on your host to restore the backup data to a different host.</li> <li>During the backup process, both DDL and DML statements in any specified graph spaces are blocked. We recommend that you do the operation within the low peak period of the business, for example, from 2:00 AM to 5:00 AM.</li> <li>The backup graph space can be restored to the original cluster only. Cross clusters restoration is not supported. Make sure the number of hosts in the cluster is not changed. Restoring a specified graph space will delete all other graph spaces in the cluster.</li> <li>Restoration requires that the number of the storage servers in the original cluster is the same as that of the storage servers in the target cluster and storage server IPs must be the same. Restoring the specified space will clear all the remaining spaces in the cluster.</li> <li>During the restoration process, there is a time when NebulaGraph stops running.</li> <li>Using BR in a container-based NebulaGraph cluster is not supported.</li> </ul>"},{"location":"backup-and-restore/nebula-br/1.what-is-br/#how_to_use_br","title":"How to use BR","text":"<p>To use the BR, follow these steps:</p> <ol> <li>Install BR.</li> <li>Use BR to back up data.</li> <li>Use BR to restore data from backup files.</li> </ol>"},{"location":"backup-and-restore/nebula-br/2.compile-br/","title":"Install BR","text":"<p>This topic introduces the installation of BR in bare-metal deployment scenarios. </p>"},{"location":"backup-and-restore/nebula-br/2.compile-br/#notes","title":"Notes","text":"<p>To use the BR (Enterprise Edition) tool, you need to install the NebulaGraph Agent service, which is taken as a daemon for each machine in the cluster that starts and stops the NebulaGraph service, and uploads and downloads backup files. The BR (Enterprise Edition) tool and the Agent plug-in are installed as described below.</p>"},{"location":"backup-and-restore/nebula-br/2.compile-br/#version_compatibility","title":"Version compatibility","text":"NebulaGraph BR Agent 3.5.x 3.5.0 0.2.0 ~ 3.4.0 3.3.0 ~ 3.4.1 3.3.0 0.2.0 ~ 3.4.0 3.0.x ~ 3.2.x 0.6.1 0.1.0 ~ 0.2.0"},{"location":"backup-and-restore/nebula-br/2.compile-br/#install_br_with_a_binary_file","title":"Install BR with a binary file","text":"<ol> <li> <p>Install BR.</p> <pre><code>wget https://github.com/vesoft-inc/nebula-br/releases/download/v3.5.0/br-3.5.0-linux-amd64\n</code></pre> </li> <li> <p>Change the binary file name to <code>br</code>.</p> <pre><code>sudo mv br-3.5.0-linux-amd64 br\n</code></pre> </li> <li> <p>Grand execute permission to BR.</p> <pre><code>sudo chmod +x br\n</code></pre> </li> <li> <p>Run <code>./br version</code> to check BR version.</p> <pre><code>[nebula-br]$ ./br version\nNebula Backup And Restore Utility Tool,V-3.5.0\n</code></pre> </li> </ol>"},{"location":"backup-and-restore/nebula-br/2.compile-br/#install_br_with_the_source_code","title":"Install BR with the source code","text":"<p>Before compiling the BR, do a check of these:</p> <ul> <li>Go 1.14.x or a later version is installed.</li> <li>make is installed.</li> </ul> <p>To compile the BR, follow these steps:</p> <ol> <li> <p>Clone the <code>nebula-br</code> repository to your machine.</p> <pre><code>git clone https://github.com/vesoft-inc/nebula-br.git\n</code></pre> </li> <li> <p>Change to the <code>br</code> directory.</p> <pre><code>cd nebula-br\n</code></pre> </li> <li> <p>Compile the BR.</p> <pre><code>make\n</code></pre> </li> </ol> <p>Users can enter <code>bin/br version</code> on the command line. If the following results are returned, the BR is compiled successfully.</p> <pre><code>[nebula-br]$ bin/br version\nNebulaGraph Backup And Restore Utility Tool,V-3.5.0\n</code></pre>"},{"location":"backup-and-restore/nebula-br/2.compile-br/#install_agent","title":"Install Agent","text":"<p>NebulaGraph Agent is installed as a binary file in each machine and serves the BR tool with the RPC protocol.</p> <p>In each machine, follow these steps:</p> <ol> <li> <p>Install Agent.</p> <pre><code>wget https://github.com/vesoft-inc/nebula-agent/releases/download/v3.4.0/agent-3.4.0-linux-amd64\n</code></pre> </li> <li> <p>Rename the Agent file to <code>agent</code>.</p> <pre><code>sudo mv agent-3.4.0-linux-amd64 agent\n</code></pre> </li> <li> <p>Add execute permission to Agent. </p> <pre><code>sudo chmod +x agent\n</code></pre> </li> <li> <p>Start Agent.</p> <p>Note</p> <p>Before starting Agent, make sure that the Meta service has been started and Agent has read and write access to the corresponding NebulaGraph cluster directory and backup directory. </p> <pre><code>sudo nohup ./agent --agent=\"&lt;agent_node_ip&gt;:8888\" --meta=\"&lt;metad_node_ip&gt;:9559\" &gt; nebula_agent.log 2&gt;&amp;1 &amp;\n</code></pre> <ul> <li><code>--agent</code>: The IP address and port number of Agent.</li> <li><code>--meta</code>: The IP address and access port of any Meta service in the cluster.</li> <li><code>--ratelimit</code>: (Optional) Limits the speed of file uploads and downloads to prevent bandwidth from being filled up and making other services unavailable. Unit: Bytes.</li> </ul> <p>For example: </p> <pre><code>sudo nohup ./agent --agent=\"192.168.8.129:8888\" --meta=\"192.168.8.129:9559\" --ratelimit=1048576 &gt; nebula_agent.log 2&gt;&amp;1 &amp;\n</code></pre> <p>Caution</p> <p>The IP address format for <code>--agent</code>should be the same as that of Meta and Storage services set in the configuration files. That is, use the real IP addresses or use <code>127.0.0.1</code>. Otherwise Agent does not run.</p> </li> <li> <p>Log into NebulaGraph and then run the following command to view the status of Agent.</p> <pre><code>nebula&gt; SHOW HOSTS AGENT;\n+-----------------+------+----------+---------+--------------+---------+\n| Host            | Port | Status   | Role    | Git Info Sha | Version |\n+-----------------+------+----------+---------+--------------+---------+\n| \"192.168.8.129\" | 8888 | \"ONLINE\" | \"AGENT\" | \"96646b8\"    |         |\n+-----------------+------+----------+---------+--------------+---------+  \n</code></pre> </li> </ol>"},{"location":"backup-and-restore/nebula-br/2.compile-br/#faq","title":"FAQ","text":""},{"location":"backup-and-restore/nebula-br/2.compile-br/#the_error_e_list_cluster_no_agent_failure","title":"The error `E_LIST_CLUSTER_NO_AGENT_FAILURE","text":"<p>If you encounter <code>E_LIST_CLUSTER_NO_AGENT_FAILURE</code> error, it may be due to the Agent service is not started or the Agent service is not registered to Meta service. First, execute <code>SHOW HOSTS AGENT</code> to check the status of the Agent service on all nodes in the cluster, when the status shows <code>OFFLINE</code>, it means the registration of Agent failed, then check whether the value of the <code>--meta</code> option in the command to start the Agent service is correct.</p>"},{"location":"backup-and-restore/nebula-br/3.br-backup-data/","title":"Use BR to back up data","text":"<p>After the BR is installed, you can back up data of the entire graph space. This topic introduces how to use the BR to back up data.</p>"},{"location":"backup-and-restore/nebula-br/3.br-backup-data/#prerequisites","title":"Prerequisites","text":"<p>To back up data with the BR, do a check of these:</p> <ul> <li>Install BR and Agent and run Agent on each host in the cluster.</li> </ul> <ul> <li>The NebulaGraph services are running.</li> </ul> <ul> <li> <p>If you store the backup files locally, create a directory with the same absolute path on the meta servers, the storage servers, and the BR machine for the backup files and get the absolute path. Make sure the account has write privileges for this directory.</p> <p>Note</p> <p>In the production environment, we recommend that you mount Network File System (NFS) storage to the meta servers, the storage servers, and the BR machine for local backup, or use Amazon S3 or Alibaba Cloud OSS for remote backup. When you restore the data from local files, you must manually move these backup files to a specified directory, which causes redundant data and troubles. For more information, see Restore data from backup files.</p> </li> </ul>"},{"location":"backup-and-restore/nebula-br/3.br-backup-data/#procedure","title":"Procedure","text":"<p>In the BR installation directory (the default path of the compiled BR is <code>./bin/br</code>), run the following command to perform a full backup for the entire cluster.</p> <p>Note</p> <p>Make sure that the local path where the backup file is stored exists.</p> <pre><code>$ ./br backup full --meta &lt;ip_address&gt; --storage &lt;storage_path&gt;\n</code></pre> <p>For example: </p> <ul> <li> <p>Run the following command to perform a full backup for the entire cluster whose meta service address is <code>192.168.8.129:9559</code>, and save the backup file to <code>/home/nebula/backup/</code>.</p> <p>Caution</p> <p>If there are multiple metad addresses, you can use any one of them.</p> <p>Caution</p> <p>If you back up data to a local disk, only the data of the leader metad is backed up by default. So if there are multiple metad processes, you need to manually copy the directory of the leader metad (path <code>&lt;storage_path&gt;/meta</code>) and overwrite the corresponding directory of other follower meatd processes.</p> <pre><code>$ ./br backup full --meta \"192.168.8.129:9559\" --storage \"local:///home/nebula/backup/\"\n</code></pre> </li> </ul> <ul> <li> <p>Run the following command to perform a full backup for the entire cluster whose meta service address is <code>192.168.8.129:9559</code>, and save the backup file to <code>backup</code> in the <code>br-test</code> bucket of the object storage service compatible with S3 protocol.</p> <pre><code>$ ./br backup full --meta \"192.168.8.129:9559\" --s3.endpoint \"http://192.168.8.129:9000\" --storage=\"s3://br-test/backup/\" --s3.access_key=minioadmin --s3.secret_key=minioadmin --s3.region=default\n</code></pre> </li> </ul> <p>The parameters are as follows.</p> Parameter Data type Required Default value Description <code>-h,-help</code> - No None Checks help for restoration. <code>--debug</code> - No None Checks for more log information. <code>--log</code> string No <code>\"br.log\"</code> Specifies detailed log path for restoration and backup. <code>--meta</code> string Yes None The IP address and port of the meta service. <code>--space</code> string Yes None (Experimental feature) Specifies the names of the spaces to be backed up. All spaces will be backed up if not specified. Multiple spaces can be specified, and format is <code>--spaces nba_01 --spaces nba_02</code>. <code>--storage</code> string Yes None The target storage URL of BR backup data. The format is: \\&lt;Schema&gt;://\\&lt;PATH&gt;. Schema: Optional values are <code>local</code> and <code>s3</code>. When selecting s3, you need to fill in <code>s3.access_key</code>, <code>s3.endpoint</code>, <code>s3.region</code>, and <code>s3.secret_key</code>.PATH: The path of the storage location. <code>--s3.access_key</code> string No None Sets AccessKey ID. <code>--s3.endpoint</code> string No None Sets the S3 endpoint URL, please specify the HTTP or HTTPS scheme explicitly. <code>--s3.region</code> string No None Sets the region or location to upload or download the backup. <code>--s3.secret_key</code> string No None Sets SecretKey for AccessKey ID."},{"location":"backup-and-restore/nebula-br/3.br-backup-data/#next_to_do","title":"Next to do","text":"<p>After the backup files are generated, you can use the BR to restore them for NebulaGraph. For more information, see Use BR to restore data.</p>"},{"location":"backup-and-restore/nebula-br/4.br-restore-data/","title":"Use BR to restore data","text":"<p>If you use the BR to back up data, you can use it to restore the data to NebulaGraph. This topic introduces how to use the BR to restore data from backup files.</p> <p>Caution</p> <p>During the restoration process, the data on the target NebulaGraph cluster is removed and then is replaced with the data from the backup files. If necessary, back up the data on the target cluster.</p> <p>Caution</p> <p>The restoration process is performed OFFLINE.</p>"},{"location":"backup-and-restore/nebula-br/4.br-restore-data/#prerequisites","title":"Prerequisites","text":"<p>To restore data with the BR, do a check of these:</p> <ul> <li>Install BR and Agent and run Agent on each host in the cluster.</li> </ul> <ul> <li>Download nebula-agent and start the agent service in each cluster(including metad, storaged, graphd) host.</li> </ul> <ul> <li>No application is connected to the target NebulaGraph cluster.</li> </ul> <ul> <li>Make sure that the target and the source NebulaGraph clusters have the same topology, which means that they have exactly the same number of hosts. The number of data folders for each host is consistently distributed.</li> </ul>"},{"location":"backup-and-restore/nebula-br/4.br-restore-data/#procedures","title":"Procedures","text":"<p>In the BR installation directory (the default path of the compiled BR is <code>./br</code>), run the following command to perform a full backup for the entire cluster.</p> <ol> <li> <p>Users can use the following command to list the existing backup information:</p> <p><pre><code>$ ./br show --storage &lt;storage_path&gt;\n</code></pre> For example, run the following command to list the backup information in the local <code>/home/nebula/backup</code> path. <pre><code>$ ./br show --storage \"local:///home/nebula/backup\"\n+----------------------------+---------------------+------------------------+-------------+------------+\n|            NAME            |     CREATE TIME     |         SPACES         | FULL BACKUP | ALL SPACES |\n+----------------------------+---------------------+------------------------+-------------+------------+\n| BACKUP_2022_02_10_07_40_41 | 2022-02-10 07:40:41 | basketballplayer       | true        | true       |\n| BACKUP_2022_02_11_08_26_43 | 2022-02-11 08:26:47 | basketballplayer,foesa | true        | true       |\n+----------------------------+---------------------+------------------------+-------------+------------+\n</code></pre></p> <p>Or, you can run the following command to list the backup information stored in S3 URL <code>s3://192.168.8.129:9000/br-test/backup</code>. <pre><code>$ ./br show --s3.endpoint \"http://192.168.8.129:9000\" --storage=\"s3://br-test/backup/\" --s3.access_key=minioadmin --s3.secret_key=minioadmin --s3.region=default\n</code></pre></p> Parameter Data type Required Default value Description <code>-h,-help</code> - No None Checks help for restoration. <code>-debug</code> - No None Checks for more log information. <code>-log</code> string No <code>\"br.log\"</code> Specifies detailed log path for restoration and backup. <code>--storage</code> string Yes None The target storage URL of BR backup data. The format is: &lt;Schema&gt;://&lt;PATH&gt;. Schema: Optional values are <code>local</code> and <code>s3</code>. When selecting s3, you need to fill in <code>s3.access_key</code>, <code>s3.endpoint</code>, <code>s3.region</code>, and <code>s3.secret_key</code>.PATH: The path of the storage location. <code>--s3.access_key</code> string No None Sets AccessKey ID. <code>--s3.endpoint</code> string No None Sets the S3 endpoint URL, please specify the HTTP or HTTPS scheme explicitly. <code>--s3.region</code> string No None Sets the region or location to upload or download the backup. <code>--s3.secret_key</code> string No None Sets SecretKey for AccessKey ID. </li> <li> <p>Run the following command to restore data.</p> <pre><code>$ ./br restore full --meta &lt;ip_address&gt; --storage &lt;storage_path&gt; --name &lt;backup_name&gt;\n</code></pre> <p>For example, run the following command to upload the backup files from the local <code>/home/nebula/backup/</code> to the cluster where the meta service's address is <code>192.168.8.129:9559</code>.</p> <pre><code>$ ./br restore full --meta \"192.168.8.129:9559\" --storage \"local:///home/nebula/backup/\" --name BACKUP_2021_12_08_18_38_08\n</code></pre> <p>Or, you can run the following command to upload the backup files from the S3 URL <code>s3://192.168.8.129:9000/br-test/backup</code>. <pre><code>$ ./br restore full --meta \"192.168.8.129:9559\" --s3.endpoint \"http://192.168.8.129:9000\" --storage=\"s3://br-test/backup/\" --s3.access_key=minioadmin --s3.secret_key=minioadmin --s3.region=\"default\" --name BACKUP_2021_12_08_18_38_08\n</code></pre></p> <p>If the following information is returned, the data is restored successfully. <pre><code>Restore succeed.\n</code></pre></p> <p>Caution</p> <p>If your new cluster hosts' IPs are not all the same as the backup cluster, after restoration, you should run <code>add hosts</code> to add the Storage host IPs in the new cluster one by one.</p> <p>The parameters are as follows.</p> Parameter Data type Required Default value Description <code>-h,-help</code> - No None Checks help for restoration. <code>-debug</code> - No None Checks for more log information. <code>-log</code> string No <code>\"br.log\"</code> Specifies detailed log path for restoration and backup. <code>-meta</code> string Yes None The IP address and port of the meta service. <code>-name</code> string Yes None The name of backup. <code>--storage</code> string Yes None The target storage URL of BR backup data. The format is: \\&lt;Schema&gt;://\\&lt;PATH&gt;. Schema: Optional values are <code>local</code> and <code>s3</code>. When selecting s3, you need to fill in <code>s3.access_key</code>, <code>s3.endpoint</code>, <code>s3.region</code>, and <code>s3.secret_key</code>.PATH: The path of the storage location. <code>--s3.access_key</code> string No None Sets AccessKey ID. <code>--s3.endpoint</code> string No None Sets the S3 endpoint URL, please specify the HTTP or HTTPS scheme explicitly. <code>--s3.region</code> string No None Sets the region or location to upload or download the backup. <code>--s3.secret_key</code> string No None Sets SecretKey for AccessKey ID. </li> <li> <p>Run the following command to clean up temporary files if any error occurred during backup. It will clean the files in cluster and external storage. You could also use it to clean up old backups files in external storage.</p> <pre><code>$ ./br cleanup --meta &lt;ip_address&gt; --storage &lt;storage_path&gt; --name &lt;backup_name&gt;\n</code></pre> <p>The parameters are as follows.</p> Parameter Data type Required Default value Description <code>-h,-help</code> - No None Checks help for restoration. <code>-debug</code> - No None Checks for more log information. <code>-log</code> string No <code>\"br.log\"</code> Specifies detailed log path for restoration and backup. <code>-meta</code> string Yes None The IP address and port of the meta service. <code>-name</code> string Yes None The name of backup. <code>--storage</code> string Yes None The target storage URL of BR backup data. The format is: \\&lt;Schema&gt;://\\&lt;PATH&gt;. Schema: Optional values are <code>local</code> and <code>s3</code>. When selecting s3, you need to fill in <code>s3.access_key</code>, <code>s3.endpoint</code>, <code>s3.region</code>, and <code>s3.secret_key</code>.PATH: The path of the storage location. <code>--s3.access_key</code> string No None Sets AccessKey ID. <code>--s3.endpoint</code> string No None Sets the S3 endpoint URL, please specify the HTTP or HTTPS scheme explicitly. <code>--s3.region</code> string No None Sets the region or location to upload or download the backup. <code>--s3.secret_key</code> string No None Sets SecretKey for AccessKey ID. </li> </ol>"},{"location":"backup-and-restore/nebula-br-ent/1.br-ent-overview/","title":"What is Backup Restore (Enterprise Edition)","text":"<p>Backup Restore (BR for short) Enterprise Edition is a Command-Line Interface (CLI) tool. With BR Enterprise Edition, you can back up and restore NebulaGraph data.</p> <p>For the deployment of BR in K8s Operator, see Backup and restore data using NebulaGraph Operator.</p> <p>Enterpriseonly</p> <p>The BR Enterprise Edition tool is for NebulaGraph Enterprise Edition only.</p>"},{"location":"backup-and-restore/nebula-br-ent/1.br-ent-overview/#features","title":"Features","text":"<ul> <li>Backups and restoration of NebulaGraph data with a single command:<ul> <li>Support full and incremental backups.</li> <li>Support the restoration of a full backup.</li> <li>Support restoration across NebulaGraph clusters.</li> </ul> </li> <li>Cloud and on-premises backup and restoration:<ul> <li>Local disks (SSD or HDD). It is recommended to use local disks with the shared storage service NFS.</li> <li>Cloud services compatible with the Amazon S3 interface, such as Alibaba Cloud OSS, MinIO, Ceph RGW, etc.</li> </ul> </li> <li>Support to view the progress of backup or restoration.</li> </ul>"},{"location":"backup-and-restore/nebula-br-ent/1.br-ent-overview/#limits","title":"Limits","text":"<ul> <li>The version of NebulaGraph Enterprise Edition clusters must be equal to or greater than v3.5.0.</li> <li>Using BR Enterprise Edition in a container-based NebulaGraph cluster is not supported.</li> </ul>"},{"location":"backup-and-restore/nebula-br-ent/1.br-ent-overview/#usage_process","title":"Usage process","text":"<p>To use BR Enterprise Edition, follow these steps:</p> <ol> <li>Install BR (Enterprise Edition)</li> <li>Back up data</li> <li>Restore data</li> </ol>"},{"location":"backup-and-restore/nebula-br-ent/2.install-tools/","title":"Install BR (Enterprise Edition)","text":"<p>The BR Enterprise Edition tool is used to back up and restore NebulaGraph Enterprise Edition data. This topic describes how to install this tool.</p>"},{"location":"backup-and-restore/nebula-br-ent/2.install-tools/#notes","title":"Notes","text":"<p>To use the BR (Enterprise Edition) tool, you need to install the NebulaGraph Agent plug-in, which is taken as a daemon for each machine in the cluster that starts and stops the NebulaGraph service, and uploads and downloads backup files. The BR (Enterprise Edition) tool and the Agent plug-in are installed as described below.</p>"},{"location":"backup-and-restore/nebula-br-ent/2.install-tools/#version_compatibility","title":"Version compatibility","text":"NebulaGraph Enterprise Edition BR Enterprise Edition Agent 3.5.x 3.5.1 3.4.0 3.4.1 3.4.0 ~ 3.4.1 3.4.0"},{"location":"backup-and-restore/nebula-br-ent/2.install-tools/#install_br_enterprise_edition_1","title":"Install BR (Enterprise Edition)","text":"<p>The BR (Enterprise Edition) is a command-line interface (CLI) tool that helps to back up NebulaGraph data or restore the data through backup files.</p> <p>Follow these steps:</p> <ol> <li> <p>Obtain the RPM package.</p> <p>Enterpriseonly</p> <p>Contact us to obtain the BR Enterprise Edition installation package.</p> </li> <li> <p>Run <code>sudo rpm -i &lt;rpm&gt;</code> to install the RPM package.   </p> <p>For example, install BR Enterprise Edition with the following command, and the default installation path is <code>/usr/local/br-ent/</code>:</p> <p><code>sudo rpm -i nebula-br-ent-&lt;version&gt;.x86_64.rpm</code></p> </li> </ol> <p>In the BR Enterprise Edition installation directory, run <code>./br version</code> to view version information. The following information is returned:</p> <pre><code>[br-ent]$ ./br version\nNebula Backup And Restore Utility Tool,V-3.5.1\n</code></pre>"},{"location":"backup-and-restore/nebula-br-ent/2.install-tools/#install_agent","title":"Install Agent","text":"<p>NebulaGraph Agent is installed as a binary file in each machine and serves the BR tool with the RPC protocol.</p> <p>In each machine, follow these steps:</p> <ol> <li> <p>Install Agent.</p> <pre><code>wget https://github.com/vesoft-inc/nebula-agent/releases/download/v3.4.0/agent-3.4.0-linux-amd64\n</code></pre> </li> <li> <p>Rename the Agent file to <code>agent</code>.</p> <pre><code>sudo mv agent-3.4.0-linux-amd64 agent\n</code></pre> </li> <li> <p>Add execute permission to Agent. </p> <pre><code>sudo chmod +x agent\n</code></pre> </li> <li> <p>Start Agent.</p> <p>Note</p> <p>Before starting Agent, make sure that the Meta service has been started and Agent has read and write access to the corresponding NebulaGraph cluster directory and backup directory. </p> <pre><code>sudo nohup ./agent --agent=\"&lt;agent_node_ip&gt;:8888\" --meta=\"&lt;metad_node_ip&gt;:9559\" &gt; nebula_agent.log 2&gt;&amp;1 &amp;\n</code></pre> <ul> <li><code>--agent</code>: The IP address and port number of Agent.</li> <li><code>--meta</code>: The IP address and access port of any Meta service in the cluster.</li> <li><code>--ratelimit</code>: (Optional) Limits the speed of file uploads and downloads to prevent bandwidth from being filled up and making other services unavailable. Unit: Bytes.</li> </ul> <p>For example: </p> <pre><code>sudo nohup ./agent --agent=\"192.168.8.129:8888\" --meta=\"192.168.8.129:9559\" --ratelimit=1048576 &gt; nebula_agent.log 2&gt;&amp;1 &amp;\n</code></pre> </li> <li> <p>Log into NebulaGraph and then run the following command to view the status of Agent.</p> <pre><code>nebula&gt; SHOW HOSTS AGENT;\n+-----------------+------+----------+---------+--------------+---------+\n| Host            | Port | Status   | Role    | Git Info Sha | Version |\n+-----------------+------+----------+---------+--------------+---------+\n| \"192.168.8.129\" | 8888 | \"ONLINE\" | \"AGENT\" | \"96646b8\"    |         |\n+-----------------+------+----------+---------+--------------+---------+  \n</code></pre> </li> </ol>"},{"location":"backup-and-restore/nebula-br-ent/3.backup-data/","title":"Back up data with BR (Enterprise Edition)","text":"<p>You can use BR (Enterprise Edition) to back up NebulaGraph data. You can perform full backups and incremental backups. You can also back up data to your local disks and to cloud storage services compatible with the Amazon S3 interface. This topic introduces how to back up NebulaGraph data.</p>"},{"location":"backup-and-restore/nebula-br-ent/3.backup-data/#background","title":"Background","text":"<ul> <li>A full backup is a backup of your NebulaGraph database's entire.</li> <li>An incremental backup covers all files that have changed or been modified since the last backup was made. The last backup can be either a full backup or an incremental backup.</li> <li>For the data directory structure of NebulaGraph, see the (default) path <code>usr/local/nebula-ent/data</code>.</li> </ul>"},{"location":"backup-and-restore/nebula-br-ent/3.backup-data/#notes","title":"Notes","text":"<ul> <li>During the process of data backup, DDL and DML statements in all specified graph spaces are blocked. We recommend that you perform the backup operation during the low business peak period.</li> <li>If you back up data to a local disk, the backup files will be saved in the local storage path of each server in a NebulaGraph cluster. You can also mount the NFS on your server to restore the backup data to a different server.</li> <li>During the process of a full backup, it may take a long time if the amount of the data to be backed up is large.</li> <li>The cluster performing an incremental backup needs to be the same as the cluster specified for the previous backup. The storage path of the incremental backup should also be the same as the path of the previous backup.</li> <li>Make sure that the time between each incremental backup and the last backup is less than a <code>wal_ttl</code> time.</li> <li>Make sure that Agent has read and write permissions for the corresponding NebulaGraph cluster installation directory and backup directory.</li> <li>Not support backup for Listener.</li> <li>Not support backup for full-text indexes.</li> </ul> <ul> <li>Not support backup for data of a specified graph space.</li> </ul>"},{"location":"backup-and-restore/nebula-br-ent/3.backup-data/#prerequisites","title":"Prerequisites","text":"<ul> <li>The NebulaGraph services are running.</li> </ul> <ul> <li>You have installed BR Enterprise Edition and Agent, and have started the Agent(s) that are installed in each machine.</li> </ul> <ul> <li>You have created a directory with the same absolute path on the machines where the meta service, storage service, and the BR Enterprise Edition tool are located. Make sure your account has write privileges for this directory.</li> </ul>"},{"location":"backup-and-restore/nebula-br-ent/3.backup-data/#full_backups","title":"Full backups","text":""},{"location":"backup-and-restore/nebula-br-ent/3.backup-data/#to_a_cloud_storage_service","title":"To a cloud storage service","text":"<p>Note</p> <p>You can back up data to the cloud storage services that are compatible with the Amazon S3 interface.</p> <p>In the directory of the BR Enterprise Edition installation tool, run the following command to back up the entire data of your NebulaGraph database to a cloud storage service:</p> <pre><code>./br backup full --meta &lt;ip_address:port&gt; --s3.access_key &lt;access_key&gt; --s3.secret_key &lt;secret_key&gt;  --s3.region &lt;region_name&gt; --storage s3://&lt;storage_path&gt; --s3.endpoint &lt;endpoint_url&gt;\n</code></pre> <p>For example, perform a full backup of the entire cluster where one of the meta service addresses is <code>192.168.8.129:9559</code>, and back up data to the <code>/</code> directory in the <code>nebula-br-test</code> bucket of Amazon S3.</p> <pre><code>./br backup full --meta 192.168.8.129:9559 --s3.access_key QImbbGDjfQExxx --s3.secret_key dVSJZfl7tnoFq7Z5zt6sfxxxx  --s3.region us-east-1 --storage s3://nebula-br-test/ --s3.endpoint http://192.168.8.xxx:9000/\n</code></pre>"},{"location":"backup-and-restore/nebula-br-ent/3.backup-data/#to_a_local_disk","title":"To a local disk","text":"<p>Caution</p> <p>In the production environment, we recommend that you mount NFS (Network File System) to the machines where the meta service, the storage service, and the BR Enterprise Edition tool are located for local backups, or use Alibaba Cloud OSS, and Amazon S3 for cloud backups. Otherwise, you must manually move these backup files to the specified directory before restoring the data.</p> <p>For local backups, only the data of the leader mated and the leader partitions are backed up. When the shared storage service is not used and there are multiple metads or the replica number of partitions is greater than 1, you need to manually copy the directory of the backed-up leader metad (<code>&lt;storage_path&gt;/meta</code>) and overwrite the corresponding directories of other follower metads, and copy the corresponding partition data from the directory of the leader partitions (<code>&lt;storage_path&gt;/&lt;partition_id&gt;</code>) to the corresponding directories of other follower partitions. It is not recommended for you to have the copy operation.</p> <p>In the directory of the BR Enterprise Edition installation tool, run the following command to back up the entire data of your NebulaGraph database to a local disk:</p> <p>Note</p> <p>Make sure you have created a directory where your data to be backed up.</p> <pre><code>./br backup full --meta &lt;ip_address:port&gt; --storage local://&lt;storage_path&gt;\n</code></pre> <p>For example, perform a full backup of the entire cluster where one of the meta server addresses is <code>192.168.8.129:9559</code>, and back up data to the <code>/backup/</code> directory on the local disk.</p> <pre><code>./br backup full --meta \"192.168.8.129:9559\" --storage \"local:///backup/\"\n</code></pre>"},{"location":"backup-and-restore/nebula-br-ent/3.backup-data/#directory_structure","title":"Directory structure","text":"<p>Full backups back up all the data of the leader metad and leader partitions. The structure of a backed-up directory is as follows:</p> <pre><code>\u251c\u2500\u2500 BACKUP_2022_08_12_08_41_19.meta\uff08Backup of metadata, such as host information and the storage path of partitions\uff09\n\u251c\u2500\u2500 data\uff08Backup of the storage data\uff09\n\u2502   \u251c\u2500\u2500 1\uff08Graph space ID\uff09\n\u2502   \u2502   \u251c\u2500\u2500 1\uff08Partition ID\uff09\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 data \n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 000033.sst\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 000035.sst\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 000037.sst\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 000039.sst\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 CURRENT\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 MANIFEST-000004\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 OPTIONS-000007\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 wal \n\u2502   \u2502   \u2502       \u251c\u2500\u2500 0000000000000000001.wal\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 0000000000000000267.wal\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 0000000000000000324.wal\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 commitlog.id\n\u2502   \u2502   \u251c\u2500\u2500 10\uff08Partition ID\uff09\n        ...\n\u2514\u2500\u2500 meta\uff08Backup of the meta data\uff09\n    ...\n</code></pre>"},{"location":"backup-and-restore/nebula-br-ent/3.backup-data/#incremental_backups","title":"Incremental backups","text":"<p>Caution</p> <p>Before performing an incremental backup, make sure that the data of the previous full backup or incremental backup is not changed. Otherwise, the incremental backup may fail. </p>"},{"location":"backup-and-restore/nebula-br-ent/3.backup-data/#to_a_cloud_storage_service_1","title":"To a cloud storage service","text":"<p>Note</p> <p>You can back up data to the cloud storage services that are compatible with the Amazon S3 interface.</p> <p>In the directory of the BR Enterprise Edition installation tool, run the following command to back up the incremental data of your NebulaGraph database to a cloud storage service:</p> <pre><code>./br backup incr --meta &lt;ip_address:port&gt; --s3.access_key &lt;access_key&gt; --s3.secret_key &lt;secret_key&gt;  --s3.region &lt;region_name&gt; --storage s3://&lt;storage_path&gt; --s3.endpoint &lt;endpoint_url&gt; --base &lt;backup_file_name&gt;\n</code></pre> <p>For example, based on the file <code>BACKUP_2022_08_11_09_11_07</code>, perform an incremental backup for the cluster where one of the meta server addresses is <code>192.168.8.129:9559</code>, and back up data to the <code>/</code> directory in the <code>nebula-br-test</code> bucket of Amazon S3.</p> <pre><code>./br backup incr --meta 192.168.8.129:9559 --s3.access_key QImbbGDjfQExxx --s3.secret_key dVSJZfl7tnoFq7Z5zt6sfxxxx  --s3.region us-east-1 --storage s3://nebula-br-test/ --s3.endpoint http://192.168.8.xxx:9000/ --base BACKUP_2022_08_11_09_11_07\n</code></pre>"},{"location":"backup-and-restore/nebula-br-ent/3.backup-data/#to_a_local_disk_1","title":"To a local disk","text":"<p>In the directory of the BR Enterprise Edition installation tool, run the following command to back up the incremental data of your NebulaGraph database to a local disk:</p> <p>Note</p> <p>Make sure that the local path where the backup file is stored exists.</p> <pre><code>./br backup incr --meta &lt;ip_address:port&gt; --storage local://&lt;storage_path&gt; --base &lt;backup_file_name&gt;\n</code></pre> <p>For example, based on the file <code>BACKUP_2022_08_11_09_11_07</code>, perform an incremental backup for the cluster where one of the meta server addresses is <code>192.168.8.129:9559</code>, and back up data to the <code>/backup/</code> directory on the local disk.</p> <pre><code>./br backup incr --meta \"192.168.8.129:9559\" --storage \"local:///backup/\" --base BACKUP_2022_08_11_09_11_07\n</code></pre>"},{"location":"backup-and-restore/nebula-br-ent/3.backup-data/#directory_structure_1","title":"Directory structure","text":"<p>In addition to the leader meta data, for the leader partition data of an existing graph space (graph space ID <code>1</code> in the following code block), Incremental backups only back up the <code>wal</code> directory of the leader partition; for the leader partition data of the newly added graph space (graph space ID <code>4</code> in the following code block), the full <code>data</code> and <code>wal</code> directories are backed up. The incremental backup directory structure may be different compared to the full backup data structure.</p> <p>The structure of an incremental backup is as follows:</p> <pre><code>\u251c\u2500\u2500 BACKUP_2022_08_12_08_58_23.meta\uff08Backup of metadata, such as host information and the storage path of partitions\uff09\n\u251c\u2500\u2500 data\uff08Backup of the storage data\uff09\n\u2502   \u251c\u2500\u2500 1 (Graph space ID)\n\u2502   \u2502   \u251c\u2500\u2500 1 (Partition ID)\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 wal \uff08wal directory\uff09\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 0000000000000000671.wal\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 0000000000000000700.wal\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 commitlog.id\n...\n\u2502   \u2514\u2500\u2500 4 (Graph space ID)\n\u2502       \u251c\u2500\u2500 1\n\u2502       \u2502   \u251c\u2500\u2500 data\uff08data directory\uff09\n\u2502       \u2502   \u2502   \u251c\u2500\u2500 000009.sst\n\u2502       \u2502   \u2502   \u251c\u2500\u2500 CURRENT\n\u2502       \u2502   \u2502   \u251c\u2500\u2500 MANIFEST-000004\n\u2502       \u2502   \u2502   \u2514\u2500\u2500 OPTIONS-000007\n\u2502       \u2502   \u2514\u2500\u2500 wal (wal directory)\n\u2502       \u2502       \u251c\u2500\u2500 0000000000000000001.wal\n\u2502       \u2502       \u2514\u2500\u2500 commitlog.id\n\u2514\u2500\u2500 meta\uff08Backup of the meta data\uff09\n    ...\n</code></pre>"},{"location":"backup-and-restore/nebula-br-ent/3.backup-data/#options","title":"Options","text":"Option Data type Required Default value Description <code>-h</code>,<code>--help</code> - No None View more information about BR commands. <code>--debug</code> - No None View more information for BR logs. <code>--log</code> string No <code>\"br.log\"</code> The path of BR logs. <code>--concurrency</code> int No None Used to control the number of concurrent file uploads during data backup. The default value is <code>5</code>. <code>--meta</code> string Yes None The IP address and port number of any Meta service in the cluster. <code>--storage</code> string Yes None The storage path of the data to be backed up in the form of <code>&lt;schema&gt;://&lt;storage_path&gt;</code>.<code>schema</code>: Two options, <code>s3</code> and <code>local</code>.When <code>schema</code> is <code>s3</code>, you still need to add options of <code>s3.access_key</code>\u3001<code>s3.endpoint</code>\u3001<code>s3.region</code>, and <code>s3.secret_key</code>.<code>&lt;storage_path&gt;</code>: The storage path. <code>--s3.access_key</code> string No None The Access Key ID that is used to identify a user. <code>--s3.endpoint</code> string No None The S3 endpoint URL. You need to specify the HTTP or HTTPS scheme explicitly. <code>--s3.region</code> string No None The physical location of a data center. <code>--s3.secret_key</code> string No None The Access Key Secret that is used to verify the identity of the user. <code>--base</code> string Yes None The name of the directory of any previous backup. Based on this backup, perform an incremental backup."},{"location":"backup-and-restore/nebula-br-ent/3.backup-data/#view_backup_progress","title":"View backup progress","text":"<p>The log file <code>br.log</code> of BR can be viewed in the installation directory. The log file will record the backup progress and will look like as follows:</p> <pre><code>{\"level\":\"info\",\"msg\":\"full upload storaged partition finished, progress: 1/20\",\"time\":\"2023-03-15T02:13:20.946Z\"}\n{\"level\":\"info\",\"msg\":\"full upload storaged partition finished, progress: 2/20\",\"time\":\"2023-03-15T02:13:21.154Z\"}\n{\"level\":\"info\",\"msg\":\"full upload storaged partition finished, progress: 3/20\",\"time\":\"2023-03-15T02:13:21.537Z\"}\n</code></pre>"},{"location":"backup-and-restore/nebula-br-ent/3.backup-data/#what_is_next","title":"What is next?","text":"<p>After your NebulaGraph data is backed up, you can restore the data to NebulaGraph. For details, see Restore data.</p> <p>Caution</p> <p>DO NOT change the name and path of the backed-up files. Otherwise, data restoration will fail.</p>"},{"location":"backup-and-restore/nebula-br-ent/4.restore-data/","title":"Restore data with BR (Enterprise Edition)","text":"<p>For the backed-up NebulaGraph data with the BR tool, you can restore the data. This topic describes how to restore data from backup files.</p>"},{"location":"backup-and-restore/nebula-br-ent/4.restore-data/#notes","title":"Notes","text":"<ul> <li>There will be a period of NebulaGraph service unavailability for data restoration, so it is recommended to operate during the low business peak period.</li> <li>Restoration requires that the number of the storage servers in the original cluster is the same as that of the storage servers in the target cluster. </li> <li>After the data restoration is executed successfully, the existing data on the target cluster will be deleted and then replaced with the data in the backup directory. It is recommended to back up the data on the target cluster in advance.</li> </ul>"},{"location":"backup-and-restore/nebula-br-ent/4.restore-data/#prerequisites","title":"Prerequisites","text":"<ul> <li>You have installed BR Enterprise Edition and Agent, and have started the Agent(s) that are installed in each machine.</li> </ul> <ul> <li>No application is connected to the target NebulaGraph cluster.</li> </ul> <ul> <li>Make sure the number of the storage servers in the original cluster is the same as that in the target cluster.</li> </ul>"},{"location":"backup-and-restore/nebula-br-ent/4.restore-data/#steps","title":"Steps","text":"<p>In the BR Enterprise Edition installation directory, follow these steps:</p> <ol> <li> <p>View backup files.</p> <ul> <li> <p>List the backup files in a local path. </p> <pre><code>./br show --storage local://&lt;storage_path&gt;\n</code></pre> <p>For example, you can view the backup files in the local <code>/backup/</code> path with the following command.</p> <pre><code>./br show --storage \"local:///backup/\"\n+----------------------------+---------------------+------------------------+-------------+------------+----------------------------+\n|            NAME            |     CREATE TIME     |         SPACES         | FULL BACKUP | ALL SPACES |      BASE BACKUP NAME      |\n+----------------------------+---------------------+------------------------+-------------+------------+----------------------------+\n| BACKUP_2022_08_11_06_12_43 | 2022-08-11 06:12:43 | basketballplayer       | true        | true       |                            |\n| BACKUP_2022_08_11_08_27_14 | 2022-08-11 08:27:15 | basketballplayer,br    | false       | true       | BACKUP_2022_08_11_06_12_43 |\n</code></pre> </li> </ul> <ul> <li> <p>Lists backups in <code>/</code> under the <code>nebula-br-test</code> bucket of the S3 protocol-compliant object storage service.</p> <pre><code>./br show --s3.access_key QImbbGDjfQEYxxxx --s3.secret_key dVSJZfl7tnoFq7Z5zt6sfYnvi63bxxxx  --s3.region us-east-1 --storage s3://nebula-br-test/ --s3.endpoint http://192.168.8.xxx:9000/\n</code></pre> </li> </ul> </li> <li> <p>Restore data.</p> <ul> <li> <p>Restore data based on local backups.</p> <pre><code>./br restore full --meta &lt;ip_address&gt; --storage &lt;storage_path&gt; --name &lt;backup_name&gt;\n</code></pre> <p>For example, restore the <code>BACKUP_2022_08_11_09_11_07</code> file data from the local <code>/backup/</code> path to the cluster with the meta address <code>192.168.8.129:9559</code>:</p> <pre><code>./br restore full --meta \"192.168.8.129:9559\" --storage \"local:///backup/\" --name BACKUP_2022_08_11_09_11_07\n</code></pre> </li> </ul> <ul> <li> <p>Restore data based on cloud backups.</p> <p>Restore the <code>BACKUP_2022_08_12_07_37_02</code> backup data from the <code>/</code> path under the <code>nebula-br-test</code> bucket of the S3-compliant object storage service to the cluster with the meta address <code>192.168.8.129:9559</code>, and specify the restoration log path.</p> <pre><code>./br restore full --meta 192.168.8.129:9559 --s3.accesskey QImbbGDjfQEYxxxx --s3.secretkey dVSJZfl7tnoFq7Z5zt6sfYnvi63bxxxx  --s3.region us-east-1 --storage s3://nebula-br-test/ --s3.endpoint http://192.168.8.xxx:9000/ --log \"3.log\" --name BACKUP_2022_08_12_07_37_02\n</code></pre> </li> </ul> <p>If the following information is returned, the data is restored successfully.</p> <pre><code>Restore succeed.\n</code></pre> <p>Note</p> <p>If the data restoration fails, BR Enterprise Edition automatically performs a rollback and the cluster's data is automatically recovered back to the pre-restoration data.</p> </li> <li> <p>Run the following command to clean temporary files. This command will clean up temporary files in the cluster or external storage, and you can also use this command to clean up old backup directories. The example is as follows.</p> <p>Note</p> <p>By default, BR automatically cleans up the temporary files when an error occurs during data restoration. If the automatic cleanup fails, you need to execute the command to clean up the temporary files manually. </p> <ul> <li> <p>Clear a local backup directory.</p> <pre><code>./br cleanup --meta 192.168.8.129:9559 --storage \"local:///backup/\" --name BACKUP_2022_08_11_09_11_07\n</code></pre> </li> </ul> <ul> <li> <p>Clear the backup directory in the cloud storage service.</p> <pre><code>./br cleanup --meta 192.168.8.129:9559 --s3.accesskey QImbbGDjfQEYxxxx --s3.secretkey dVSJZfl7tnoFq7Z5zt6sfYnvi63bxxxx  --s3.region us-east-1 --storage s3://nebula-br-test/ --s3.endpoint http://192.168.8.xxx:9000/ --name BACKUP_2022_08_12_07_37_02\n</code></pre> </li> </ul> </li> </ol>"},{"location":"backup-and-restore/nebula-br-ent/4.restore-data/#options","title":"Options","text":"<p>The following options are for data restorations.</p> Option Data type Required Default value Description <code>-h,-help</code> - No - View more information about BR commands. <code>--debug</code> - No None View more information for BR logs. <code>--log</code> string No \"br.log\" The path of BR logs. <code>--concurrency</code> int No None Used to control the number of concurrent file downloads during data restoration. The default value is <code>5</code>. <code>--meta</code> string Yes None The IP address and port number of any Meta service in the cluster. <code>--name</code> string Yes None The name of the backup file. <code>--storage</code> string Yes None The storage URL of the backup is to be restored. The format is <code>&lt;schema&gt;://&lt;storage_path&gt;</code>.<code>schema</code>: Two options, <code>s3</code> and <code>local</code>.When <code>schema</code> is <code>s3</code>, you still need to add options of <code>s3.access_key</code>\u3001<code>s3.endpoint</code>\u3001<code>s3.region</code>, and <code>s3.secret_key</code>.<code>&lt;storage_path&gt;</code>: The storage path of the backup to be restored. <code>--s3.access_key</code> string No None The Access Key ID that is used to identify a user. <code>--s3.endpoint</code> string No None The S3 endpoint URL. You need to specify the HTTP or HTTPS scheme explicitly. <code>--s3.region</code> string No None The physical location of a data center. <code>--s3.secret_key</code> string No None The Access Key Secret that is used to verify the identity of the user."},{"location":"backup-and-restore/nebula-br-ent/4.restore-data/#view_restoration_progress","title":"View restoration progress","text":"<p>The log file <code>br.log</code> of BR can be viewed in the installation directory. The log file will record the restoration progress and will look like as follows:</p> <pre><code>{\"level\":\"info\",\"msg\":\"download storaged partition finished, progress: 1/20\",\"time\":\"2023-03-15T02:16:43.430Z\"}\n{\"level\":\"info\",\"msg\":\"download storaged partition finished, progress: 2/20\",\"time\":\"2023-03-15T02:16:43.431Z\"}\n{\"level\":\"info\",\"msg\":\"download storaged partition finished, progress: 3/20\",\"time\":\"2023-03-15T02:16:43.763Z\"}\n</code></pre>"},{"location":"graph-computing/algorithm-description/","title":"Algorithm overview","text":"<p>Graph computing can detect the graph structure, such as the communities in a graph and the division of a graph. It can also reveal the inherent characteristics of the correlation between various vertexes, such as the centrality and similarity of the vertices. This topic introduces the algorithms and parameters supported by NebulaGraph.</p> <p>Note</p> <p>This topic only introduces the parameters of NebulaGraph Analytics. For details about the parameters of NebulaGraph Algorithm, see algorithm.</p> <p>Note</p> <p>The algorithm parameters need to be set when performing graph computing, and there are requirements for data sources. The data source needs to contain source vertexes and destination vertexes. PageRank, DegreeWithTime, SSSP, APSP, LPA, HANP, and Louvain algorithms must include weight.</p> <ul> <li>If the data source comes from HDFS, users need to specify a CSV file that contains <code>src</code> and <code>dst</code> columns. Some algorithms also need to contain a <code>weight</code> column.</li> </ul> <ul> <li>If the data source comes from NebulaGraph, users need to specify the edge types that provide <code>src</code> and <code>dst</code> columns. Some algorithms also need to specify the properties of the edge types as <code>weight</code> columns.</li> </ul>"},{"location":"graph-computing/algorithm-description/#node_importance_measurement","title":"Node importance measurement","text":""},{"location":"graph-computing/algorithm-description/#pagerank","title":"PageRank","text":"<p>The PageRank algorithm calculates the relevance and importance of vertices based on their relationships. It is commonly used in search engine page rankings. If a page is linked by many other pages, the page is more important (PageRank value is higher). If a page with a high PageRank value links to other pages, the PageRank value of the linked pages will increase.</p> <p>Parameter descriptions are as follows:</p> <ul> <li> <p>NebulaGraph Analytics</p> <ul> <li> <p>Input parameters</p> Parameter Predefined value Description <code>ITERATIONS</code> <code>10</code> The maximum number of iterations. <code>IS_DIRECTED</code> <code>true</code> Whether to consider the direction of the edges. If set to <code>false</code>, the system automatically adds the reverse edge. <code>EPS</code> <code>0.0001</code> The convergence accuracy. When the difference between the result of two iterations is less than the <code>EPS</code> value, the iteration is not continued. <code>DAMPING</code> <code>0.85</code> The damping coefficient. It is the jump probability after visiting a page. </li> </ul> <ul> <li> <p>Output parameters</p> Parameter Type Description <code>VID</code> Determined by <code>vid_type</code> The vertex ID. <code>VALUE</code> double The PageRank value of the vertex. </li> </ul> </li> </ul>"},{"location":"graph-computing/algorithm-description/#kcore","title":"KCore","text":"<p>The KCore algorithm is used to calculate the subgraph composed of no vertexes less than K degree, usually used in community discovery, financial risk control and other scenarios. The calculation result is one of the most commonly used reference values to judge the importance of a vertex, which reflects the propagation ability of a vertex.</p> <p>Parameter descriptions are as follows:</p> <ul> <li> <p>NebulaGraph Analytics</p> <ul> <li> <p>Input parameters</p> Parameter Predefined value Description <code>TYPE</code> <code>vertex</code> The calculation type. Available values are <code>vertex</code> and <code>subgraph</code>. When set to <code>vertex</code>, the system calculates the number of cores for each vertex. <code>KMIN</code> <code>1</code> Set the minimum value of K when performing the range calculation. Takes effect only when <code>TYPE</code>=<code>subgraph</code>. <code>KMAX</code> <code>1000000</code> Set the maximum value of K when performing the range calculation. Takes effect only when <code>TYPE</code>=<code>subgraph</code>. </li> </ul> <ul> <li> <p>Output parameters when <code>TYPE=vertex</code></p> Parameter Type Description <code>VID</code> Determined by <code>vid_type</code> The vertex ID. <code>VALUE</code> int Outputs the core degree of the vertex. </li> </ul> <ul> <li> <p>Output parameters when <code>TYPE=subgraph</code></p> Parameter Type Description <code>VID</code> Determined by <code>vid_type</code> The vertex ID. <code>VALUE</code> The same with <code>VID</code> Outputs the neighbors of the vertex. </li> </ul> </li> </ul>"},{"location":"graph-computing/algorithm-description/#degreecentrality_nstepdegree","title":"DegreeCentrality (NStepDegree)","text":"<p>The DegreeCentrality algorithm is used to find the popular vertexes in a graph. Degree centrality measures the number of incoming or outgoing (or both) relationships from a vertex, depending on the direction of the projection of the relationship. The greater the degree of a vertex is, the higher the degree centrality of the vertex is, and the more important the vertex is in the network.</p> <p>Note</p> <p>NebulaGraph Analytics only estimates DegreeCentrality roughly.</p> <p>Parameter descriptions are as follows:</p> <ul> <li> <p>NebulaGraph Analytics</p> <ul> <li> <p>Input parameters</p> Parameter Predefined value Description <code>IS_DIRECTED</code> <code>true</code> Whether to consider the direction of the edges. If set to <code>false</code>, the system automatically adds the reverse edge. <code>STEP</code> <code>3</code> The degree of calculation. <code>-1</code> means infinity. <code>BITS</code> <code>6</code> The hyperloglog bit width for cardinality estimation. <code>TYPE</code> <code>both</code> The direction of the edges for calculation. Optional values are <code>in</code>, <code>out</code> and <code>both</code>. </li> </ul> <ul> <li> <p>Output parameters when <code>TYPE=both</code></p> Parameter Type Description <code>VID</code> Determined by <code>vid_type</code> The vertex ID. <code>BOTH_DEGREE</code> int Outputs the bidirectional degree centrality of the vertex. <code>OUT_DEGREE</code> int Outputs the outbound degree centrality of the vertex. <code>IN_DEGREE</code> int Outputs the inbound degree centrality of the vertex. </li> </ul> <ul> <li> <p>Output parameters when <code>TYPE=out</code></p> Parameter Type Description <code>VID</code> Determined by <code>vid_type</code> The vertex ID. <code>OUT_DEGREE</code> int Outputs the outbound degree centrality of the vertex. </li> </ul> <ul> <li> <p>Output parameters when <code>TYPE=in</code></p> Parameter Type Description <code>VID</code> Determined by <code>vid_type</code> The vertex ID. <code>IN_DEGREE</code> int Outputs the inbound degree centrality of the vertex. </li> </ul> </li> </ul>"},{"location":"graph-computing/algorithm-description/#degreewithtime","title":"DegreeWithTime","text":"<p>The DegreeWithTime algorithm is used to count neighbors based on the time range of edges to find out the popular vertexes in a graph.</p> <p>Note</p> <p>This algorithm is supported by NebulaGraph Analytics only.</p> <p>Parameter descriptions are as follows:</p> <ul> <li> <p>Input parameters</p> Parameter Predefined value Description <code>ITERATIONS</code> <code>10</code> The maximum number of iterations. <code>IS_DIRECTED</code> <code>true</code> Whether to consider the direction of the edges. If set to <code>false</code>, the system automatically adds the reverse edge. <code>BEGIN_TIME</code> - The begin time. <code>END_TIME</code> - The end time. </li> </ul> <ul> <li> <p>Output parameters when <code>TYPE=both</code></p> Parameter Type Description <code>VID</code> Determined by <code>vid_type</code> The vertex ID. <code>BOTH_DEGREE</code> int Outputs the bidirectional popularity of the vertex. <code>OUT_DEGREE</code> int Outputs the outbound popularity of the vertex. <code>IN_DEGREE</code> int Outputs the inbound popularity of the vertex. </li> </ul> <ul> <li> <p>Output parameters when <code>TYPE=out</code></p> Parameter Type Description <code>VID</code> Determined by <code>vid_type</code> The vertex ID. <code>OUT_DEGREE</code> int Outputs the outbound popularity of the vertex. </li> </ul> <ul> <li> <p>Output parameters when <code>TYPE=in</code></p> Parameter Type Description <code>VID</code> Determined by <code>vid_type</code> The vertex ID. <code>IN_DEGREE</code> int Outputs the inbound popularity of the vertex. </li> </ul>"},{"location":"graph-computing/algorithm-description/#betweennesscentrality","title":"BetweennessCentrality","text":"<p>The BetweennessCentrality algorithm is used to detect the amount of influence a vertex has on the flow of information in a graph. It is used to find the vertexes that act as bridges between one part of the graph and another. Each vertex is given a score, the betweenness centrality score, based on the number of shortest paths through that vertex.</p> <p>Parameter descriptions are as follows:</p> <ul> <li> <p>NebulaGraph Analytics</p> <ul> <li> <p>Input parameters</p> Parameter Predefined value Description <code>ITERATIONS</code> <code>10</code> The maximum number of iterations. <code>IS_DIRECTED</code> <code>true</code> Whether to consider the direction of the edges. If set to <code>false</code>, the system automatically adds the reverse edge. <code>CHOSEN</code> <code>-1</code> The selected vertex ID, <code>-1</code> means random selection. <code>CONSTANT</code> <code>2</code> The constant. </li> </ul> <ul> <li> <p>Output parameters</p> Parameter Type Description <code>VID</code> Determined by <code>vid_type</code> The vertex ID. <code>VALUE</code> double The betweenness centrality score of the vertex. </li> </ul> </li> </ul>"},{"location":"graph-computing/algorithm-description/#closenesscentrality","title":"ClosenessCentrality","text":"<p>The ClosenessCentrality algorithm is used to calculate the reciprocal of the average of the shortest distance from one vertex to all other reachable vertexes. The larger the value is, the closer the vertex is to the center of the graph, and it can also be used to measure how long it takes for information to be transmitted from that vertex to other vertexes.</p> <p>Parameter descriptions are as follows:</p> <ul> <li> <p>NebulaGraph Analytics</p> <ul> <li> <p>Input parameters</p> Parameter Predefined value Description <code>IS_DIRECTED</code> <code>true</code> Whether to consider the direction of the edges. If set to <code>false</code>, the system automatically adds the reverse edge. <code>NUM_SAMPLES</code> <code>10</code> The number of sample vertices. </li> </ul> <ul> <li> <p>Output parameters</p> Parameter Type Description <code>VID</code> Determined by <code>vid_type</code> The vertex ID. <code>VALUE</code> double The closeness centrality score of the vertex. </li> </ul> </li> </ul>"},{"location":"graph-computing/algorithm-description/#path","title":"Path","text":""},{"location":"graph-computing/algorithm-description/#apsp","title":"APSP","text":"<p>The APSP (Full Graph Shortest Path) algorithm is used to find all shortest paths between two vertexes in a graph.</p> <p>Note</p> <p>This algorithm is supported by NebulaGraph Analytics only.</p> <p>Parameter descriptions are as follows:</p> <ul> <li> <p>Output parameters</p> Parameter Type Description <code>VID1</code> Determined by <code>vid_type</code> The VID of the source vertex. <code>VID2</code> Determined by <code>vid_type</code> The VID of the destination vertex. <code>DISTANCE</code> double Outputs the distance from <code>VID1</code> to <code>VID2</code>. </li> </ul>"},{"location":"graph-computing/algorithm-description/#sssp","title":"SSSP","text":"<p>The SSSP (Single source shortest Path) algorithm is used to calculate the shortest path length from a given vertex (source vertex) to other vertexes. It is usually used in scenarios such as network routing and path designing.</p> <p>Parameter descriptions are as follows:</p> <ul> <li> <p>NebulaGraph Analytics</p> <ul> <li> <p>Input parameters</p> Parameter Predefined value Description <code>ROOT</code> - The VID of the source vertex. </li> </ul> <ul> <li> <p>Output parameters</p> Parameter Type Description <code>VID</code> Determined by <code>vid_type</code> The VID of the source vertex. <code>DISTANCE</code> double Outputs the distance from <code>ROOT</code> to <code>VID</code>. </li> </ul> </li> </ul>"},{"location":"graph-computing/algorithm-description/#bfs","title":"BFS","text":"<p>The BFS (Breadth First traversal) algorithm is a basic graph traversal algorithm. It gives a source vertex and accesses other vertexes with increasing hops, that is, it traverses all the adjacent vertexes of the vertex first and then extends to the adjacent vertexes of the adjacent vertexes.</p> <p>Parameter descriptions are as follows:</p> <ul> <li> <p>NebulaGraph Analytics</p> <ul> <li> <p>Input parameters</p> Parameter Predefined value Description <code>IS_DIRECTED</code> <code>true</code> Whether to consider the direction of the edges. If set to <code>false</code>, the system automatically adds the reverse edge. <code>ROOT</code> - The VID of the source vertex. </li> </ul> <ul> <li> <p>Output parameters</p> Parameter Type Description <code>ROOT</code> Determined by <code>vid_type</code> The VID of the source vertex. <code>VISITED</code> int Outputs the number of the vertex accessed by <code>ROOT</code>. </li> </ul> </li> </ul>"},{"location":"graph-computing/algorithm-description/#shortestpath","title":"ShortestPath","text":"<p>The ShortestPath algorithm is used to find the shortest path between any two vertices in the graph, which is frequently applied in scenarios such as path design and network planning.</p> <ul> <li> <p>NebulaGraph Analytics</p> <ul> <li> <p>Input parameters</p> Parameter Predefined value Description <code>src</code> <code>\"100\"</code> Starting vertices. Multiple VIDs are separated by commas (,). <code>dst</code> <code>\"200\"</code> Destination vertices. Multiple VIDs are separated by commas (,). </li> </ul> <ul> <li> <p>Output parameters</p> Parameter Type Description <code>VALUE</code> list Returns the vertices in the shortest path. The format is<code>src, vid1,vid2...dst</code>. If there are multiple shortest paths between two vertices, only one path is returned. </li> </ul> </li> </ul>"},{"location":"graph-computing/algorithm-description/#community_discovery","title":"Community discovery","text":""},{"location":"graph-computing/algorithm-description/#lpa","title":"LPA","text":"<p>The LPA (label propagation) algorithm is a semi-supervised learning method based on graph. Its basic idea is to use label information of labeled vertexes to predict label information of unlabeled vertexes. vertexes include labeled and unlabeled data, and their edges represent the similarity of two vertexes. The labels of vertexes are transferred to other vertexes according to the similarity. Label data is like a source that can be labeled for unlabeled data. The greater the similarity of vertexes is, the easier the label is to spread.</p> <p>Parameter descriptions are as follows:</p> <ul> <li> <p>NebulaGraph Analytics</p> <ul> <li> <p>Input parameters</p> Parameter Predefined value Description <code>ITERATIONS</code> <code>10</code> The maximum number of iterations. <code>IS_DIRECTED</code> <code>true</code> Whether to consider the direction of the edges. If set to <code>false</code>, the system automatically adds the reverse edge. <code>IS_CALC_MODULARITY</code> <code>false</code> Whether to calculate modularity. <code>IS_OUTPUT_MODULARITY</code> <code>false</code> Whether to calculate and output module degrees. When set to <code>true</code>, the default output is to the third column of the file, but it can also be output to NebulaGraph with options <code>-nebula_output_props</code> and <code>-nebula_output_types</code>. Output to NebulaGraph is not yet supported when using Explorer. <code>IS_STAT_COMMUNITY</code> <code>false</code> Whether to count the number of communities. </li> </ul> <ul> <li> <p>Output parameters</p> Parameter Type Description <code>VID</code> Determined by <code>vid_type</code> The vertex ID. <code>LABEL</code> The same with <code>VID</code> Outputs the vertex IDs that have the same label. </li> </ul> </li> </ul>"},{"location":"graph-computing/algorithm-description/#hanp","title":"HANP","text":"<p>The HANP (Hop Preference &amp; Node Preference) algorithm is an optimization algorithm of LPA algorithm, which considers other information of labels, such as degree information, distance information, etc., and introduces attenuation coefficient during propagation to prevent transition propagation.</p> <p>Parameter descriptions are as follows:</p> <ul> <li> <p>NebulaGraph Analytics</p> <ul> <li> <p>Input parameters</p> Parameter Predefined value Description <code>ITERATIONS</code> <code>10</code> The maximum number of iterations. <code>IS_DIRECTED</code> <code>true</code> Whether to consider the direction of the edges. If set to <code>false</code>, the system automatically adds the reverse edge. <code>PREFERENCE</code> <code>1.0</code> The bias of the neighbor vertex degree. <code>m&gt;0</code>indicates biasing the neighbor with high vertex degree, <code>m&lt;0</code> indicates biasing the neighbor with low vertex degree, and <code>m=0</code> indicates ignoring the neighbor vertex degree. <code>HOP_ATT</code> <code>0.1</code> The attenuation coefficient. The value ranges from <code>0</code> to <code>1</code>. The larger the value, the faster it decays and the fewer times it can be passed. <code>IS_OUTPUT_MODULARITY</code> <code>false</code> Whether to calculate and output module degrees. When set to <code>true</code>, the default output is to the third column of the file, but it can also be output to NebulaGraph with options <code>-nebula_output_props</code> and <code>-nebula_output_types</code>. Output to NebulaGraph is not yet supported when using Explorer. <code>IS_STAT_COMMUNITY</code> <code>false</code> Whether to count the number of communities. </li> </ul> <ul> <li> <p>Output parameters</p> Parameter Type Description <code>VID</code> Determined by <code>vid_type</code> The vertex ID. <code>LABEL</code> The same with <code>VID</code> Outputs the vertex IDs that have the same label. </li> </ul> </li> </ul>"},{"location":"graph-computing/algorithm-description/#connectedcomponent","title":"ConnectedComponent","text":"<p>The ConnectedComponent algorithm is used to calculate a subgraph of a graph in which all vertexes are connected to each other. Strongly Connected Component takes the path direction into account, while Weakly Connected Component does not.</p> <p>Note</p> <p>NebulaGraph Analytics only supports Weakly Connected Component.</p> <p>Parameter descriptions are as follows:</p> <ul> <li> <p>NebulaGraph Analytics</p> <ul> <li> <p>Input parameters</p> Parameter Predefined value Description <code>IS_DIRECTED</code> <code>true</code> Whether to consider the direction of the edges. If set to <code>false</code>, the system automatically adds the reverse edge. <code>IS_CALC_MODULARITY</code> <code>false</code> Whether to calculate modularity. <code>IS_OUTPUT_MODULARITY</code> <code>false</code> Whether to calculate and output module degrees. When set to <code>true</code>, the default output is to the third column of the file, but it can also be output to NebulaGraph with options <code>-nebula_output_props</code> and <code>-nebula_output_types</code>. Output to NebulaGraph is not yet supported when using Explorer. <code>IS_STAT_COMMUNITY</code> <code>false</code> Whether to count the number of communities. </li> </ul> <ul> <li> <p>Output parameters</p> Parameter Type Description <code>VID</code> Determined by <code>vid_type</code> The vertex ID. <code>LABEL</code> The same with <code>VID</code> Outputs the vertex IDs that have the same label. </li> </ul> </li> </ul>"},{"location":"graph-computing/algorithm-description/#louvain","title":"Louvain","text":"<p>The Louvain algorithm is a community discovery algorithm based on modularity. This algorithm performs well in efficiency and effect, and can be used to find hierarchical community structures. Its optimization goal is to maximize the modularity of the whole community network. Modularity is used to distinguish the differences in link density within and between communities, and to measure how well each vertex divides the community. In general, a good clustering approach will result in more modularity within communities than between communities.</p> <p>Parameter descriptions are as follows:</p> <ul> <li> <p>NebulaGraph Analytics</p> <ul> <li> <p>Input parameters</p> Parameter Predefined value Description <code>IS_DIRECTED</code> <code>true</code> Whether to consider the direction of the edges. If set to <code>false</code>, the system automatically adds the reverse edge. <code>OUTER_ITERATION</code> <code>20</code> The maximum number of iterations in the first phase. <code>INNER_ITERATION</code> <code>10</code> The maximum number of iterations in the second phase. <code>IS_CALC_MODULARITY</code> <code>false</code> Whether to calculate modularity. <code>IS_OUTPUT_MODULARITY</code> <code>false</code> Whether to calculate and output module degrees. When set to <code>true</code>, the default output is to the third column of the file, but it can also be output to NebulaGraph with options <code>-nebula_output_props</code> and <code>-nebula_output_types</code>. Output to NebulaGraph is not yet supported when using Explorer. <code>IS_STAT_COMMUNITY</code> <code>false</code> Whether to count the number of communities. </li> </ul> <ul> <li> <p>Output parameters</p> Parameter Type Description <code>VID</code> Determined by <code>vid_type</code> The vertex ID. <code>LABEL</code> The same with <code>VID</code> Outputs the vertex IDs that have the same label. </li> </ul> </li> </ul>"},{"location":"graph-computing/algorithm-description/#infomap","title":"InfoMap","text":"<p>The InfoMap algorithm uses double encoding to classify directed graphs into communities. The encoding reuse of nodes in different communities can greatly shorten the length of description information. In terms of implementation, the algorithm includes the PageRank algorithm, which converts a random walk into a random surf.</p> <p>Note</p> <p>This algorithm is supported by NebulaGraph Analytics only.</p> <ul> <li> <p>NebulaGraph Analytics</p> <ul> <li> <p>Input parameters</p> Parameter Predefined value Description <code>pagerank_iter</code> <code>10</code> The maximum number of iterations of the internal PageRank algorithm. <code>pagerank_threshold</code> <code>0.0001</code> The convergence accuracy of the internal PageRank algorithm. <code>teleport_prob</code> <code>0.15</code> The teleportation probability. <code>inner_iter</code> <code>3</code> The number of inner iterations. <code>outer_iter</code> <code>2</code> The number of outer iterations. <code>comm_info_num</code> <code>100</code> The number of communities exported. </li> </ul> <ul> <li> <p>Output parameters</p> Parameter Type Description <code>VID</code> Determined by <code>vid_type</code> The vertex ID. <code>LABEL</code> The same with <code>VID</code> Outputs the vertex IDs that have the same label. </li> </ul> </li> </ul>"},{"location":"graph-computing/algorithm-description/#graph_feature","title":"Graph feature","text":""},{"location":"graph-computing/algorithm-description/#trianglecount","title":"TriangleCount","text":"<p>The TriangleCount algorithm is used to count the number of triangles in a graph. The more triangles, the higher the degree of vertex association in the graph, the tighter the organizational relationship.</p> <p>Parameter descriptions are as follows:</p> <ul> <li> <p>NebulaGraph Analytics</p> <ul> <li> <p>Input parameters</p> Parameter Predefined value Description <code>OPT</code> <code>3</code> The calculation type. Optional values are <code>1</code>, <code>2</code> and <code>3</code>. <code>1</code> indicates counting the entire graph, <code>2</code> indicates counting through each vertex, <code>3</code> indicates listing all triangles. <code>REMOVED_DUPLICATION_EDGE</code> <code>true</code> Whether to exclude repeated edges. <code>REMOVED_SELF_EDGE</code> <code>true</code> Whether to exclude self-loop edge. </li> </ul> <ul> <li> <p>Output parameters when <code>OPT=1</code></p> Parameter Type Description <code>COUNT</code> int Outputs the number of the triangles in the full graph space. </li> </ul> <ul> <li> <p>Output parameters when <code>OPT=2</code></p> Parameter Type Description <code>VID</code> Determined by <code>vid_type</code> The vertex ID. <code>COUNT</code> int Outputs the number of the triangles based on the vertex. </li> </ul> <ul> <li> <p>Output parameters when <code>OPT=3</code></p> Parameter Type Description <code>VID1</code> The same with <code>VID</code> Outputs the ID of the vertex A that forms the triangle. <code>VID2</code> The same with <code>VID</code> Outputs the ID of the vertex B that forms the triangle. <code>VID3</code> The same with <code>VID</code> Outputs the ID of the vertex C that forms the triangle. </li> </ul> </li> </ul>"},{"location":"graph-computing/algorithm-description/#node2vec","title":"Node2Vec","text":"<p>The Node2Vec algorithm proposed a more reasonable graph feature learning method based on DeepWalk, and proposed a semi-supervised algorithm for scalable feature learning in networks. SGD was used to optimize a custom graph-based objective function, which could maximize the network domain information of nodes reserved in d-dimensional feature space. Based on the random walk, a second order random walk process is designed, which is equivalent to an extension of DeepWalk algorithm, and preserves the graph characteristics of neighbor nodes. Applicable to node function similarity comparison, node structure similarity comparison, community clustering and other scenarios.R</p> <p>Parameter descriptions are as follows:</p> <ul> <li> <p>NebulaGraph Analytics</p> <ul> <li>Input parameters   |Parameter|Predefined value|Description|   |:--|:--|:--|   |<code>is_weighted</code>|<code>false</code>|  Random walk with bias or not.|   |<code>p</code>|<code>1.0</code>| The backward bias for random walk.|   |<code>q</code>|<code>0.5</code>| The forward bias for random walk.|   |<code>epoch</code>|<code>1</code>| The number of iterations.|   |<code>step</code>|<code>10</code>| The number of steps per iteration.|   |<code>rate</code>|<code>0.02</code>| The rate of the random walk.|</li> </ul> <ul> <li>Output parameters   Output multiple columns where vertices in the same column are associated.</li> </ul> </li> </ul>"},{"location":"graph-computing/algorithm-description/#tree_stat","title":"Tree_stat","text":"<p>The Tree_stat algorithm counts the width or depth of a subgraph with a specified root vertex.</p> <p>Note</p> <p>This algorithm is supported by NebulaGraph Analytics only.</p> <ul> <li> <p>NebulaGraph Analytics</p> <ul> <li> <p>Input parameters</p> Parameter Predefined value Description <code>root</code> <code>100</code> The VID of the root vertex. <code>stat</code> <code>width,depth</code> Counts width or depth. Multiple values are separated by commas (,). </li> </ul> <ul> <li> <p>Output parameters</p> Parameter Type Description <code>VALUE</code> list Returns a row of statistics in the same format as the <code>stat</code> parameter. </li> </ul> </li> </ul>"},{"location":"graph-computing/algorithm-description/#hyperanf","title":"HyperANF","text":"<p>The HyperANF algorithm is used to evaluate the average distance between any two vertices in a graph.</p> <p>Note</p> <p>This algorithm is supported by NebulaGraph Analytics only.</p> <ul> <li> <p>NebulaGraph Analytics</p> <ul> <li> <p>Input parameters</p> Parameter Predefined value Description <code>bits</code> <code>6</code> The bit length of the HyperLogLog counter. The value ranges from 6 to 16. </li> </ul> <ul> <li> <p>Output parameters</p> Parameter Type Description <code>VALUE</code> double The average distance. </li> </ul> </li> </ul>"},{"location":"graph-computing/algorithm-description/#clustering","title":"Clustering","text":""},{"location":"graph-computing/algorithm-description/#clusteringcoefficient","title":"ClusteringCoefficient","text":"<p>The ClusteringCoefficient algorithm is used to calculate the clustering degree of vertexes in a graph. In all kinds of network structures reflecting the real world, especially social network structures, network groups with relatively high density tend to be formed between various vertexes. In other words, compared with the networks randomly connected between two vertexes, the aggregation coefficient of the real world network is higher.</p> <p>Parameter descriptions are as follows:</p> <ul> <li> <p>NebulaGraph Analytics</p> <ul> <li> <p>Input parameters</p> Parameter Predefined value Description <code>TYPE</code> <code>local</code> The clustering type. Optional values are <code>local</code> and <code>global</code>. <code>local</code> indicates counting through each vertex, <code>global</code> indicates counting the entire graph. <code>REMOVED_DUPLICATION_EDGE</code> <code>true</code> Whether to exclude repeated edges. <code>REMOVED_SELF_EDGE</code> <code>true</code> Whether to exclude self-loop edge. </li> </ul> <ul> <li> <p>Output parameters when <code>TYPE=local</code></p> Parameter Type Description <code>VID</code> Determined by <code>vid_type</code> The vertex ID. <code>VALUE</code> double Outputs the clustering coefficient of the vertex. </li> </ul> <ul> <li> <p>Output parameters when <code>TYPE=global</code></p> Parameter Type Description <code>VID</code> Determined by <code>vid_type</code> The vertex ID. <code>VALUE</code> double Outputs the clustering coefficient of the full graph space. There is only one line of data. </li> </ul> </li> </ul>"},{"location":"graph-computing/algorithm-description/#similarity","title":"Similarity","text":""},{"location":"graph-computing/algorithm-description/#jaccard","title":"Jaccard","text":"<p>The Jaccard algorithm is used to calculate the similarity of two vertexes (or sets) and predict the relationship between them. It is suitable for social network friend recommendation, relationship prediction and other scenarios.</p> <p>Parameter descriptions are as follows:</p> <ul> <li> <p>NebulaGraph Analytics</p> <ul> <li> <p>Input parameters</p> Parameter Predefined value Description <code>IDS1</code> - A set of VIDs. Multiple VIDs are separated by commas (,). It is not allowed to be empty. <code>IDS2</code> - A set of VIDs. Multiple VIDs are separated by commas (,). It can be empty, and empty represents all vertexes. <code>REMOVED_SELF_EDGE</code> <code>true</code> Whether to exclude self-loop edges. </li> </ul> <ul> <li> <p>Output parameters</p> Parameter Type Description <code>VID1</code> Determined by <code>vid_type</code> The ID of the first vertex. <code>VID2</code> Determined by <code>vid_type</code> The ID of the second vertex. <code>VALUE</code> double The similarity between <code>VID1</code> and <code>VID2</code>. </li> </ul> </li> </ul>"},{"location":"graph-computing/nebula-algorithm/","title":"NebulaGraph Algorithm","text":"<p>NebulaGraph Algorithm (Algorithm) is a Spark application based on GraphX. It uses a complete algorithm tool to perform graph computing on the data in the NebulaGraph database by submitting a Spark task. You can also programmatically use the algorithm under the lib repository to perform graph computing on DataFrame.</p>"},{"location":"graph-computing/nebula-algorithm/#version_compatibility","title":"Version compatibility","text":"<p>The correspondence between the NebulaGraph Algorithm release and the NebulaGraph core release is as follows.</p> NebulaGraph NebulaGraph Algorithm nightly 3.0-SNAPSHOT 3.0.0 ~ 3.4.x 3.x.0 2.6.x 2.6.x 2.5.0\u30012.5.1 2.5.0 2.0.0\u30012.0.1 2.1.0"},{"location":"graph-computing/nebula-algorithm/#prerequisites","title":"Prerequisites","text":"<p>Before using the NebulaGraph Algorithm, users need to confirm the following information:</p> <ul> <li>The NebulaGraph services have been deployed and started. For details, see NebulaGraph Installation.</li> </ul> <ul> <li>The Spark version is 2.4.x.</li> </ul> <ul> <li>The Scala version is 2.11.</li> </ul> <ul> <li>(Optional) If users need to clone, compile, and package the latest Algorithm in Github, install Maven.</li> </ul>"},{"location":"graph-computing/nebula-algorithm/#limitations","title":"Limitations","text":"<p>Graph computing outputs vertex datasets, and the algorithm results are stored in DataFrames as the properties of vertices. You can do further operations such as statistics and filtering according to your business requirements.</p> <p>!!!</p> <pre><code>Before Algorithm v3.1.0, when submitting the algorithm package directly, the data of the vertex ID must be an integer. That is, the vertex ID can be INT or String, but the data itself is an integer.\n</code></pre>"},{"location":"graph-computing/nebula-algorithm/#supported_algorithms","title":"Supported algorithms","text":"<p>The graph computing algorithms supported by NebulaGraph Algorithm are as follows.</p> Algorithm Description Scenario Properties name Properties type PageRank The rank of pages Web page ranking, key node mining pagerank double/string Louvain Louvain Community mining, hierarchical clustering louvain int/string KCore K core Community discovery, financial risk control kcore int/string LabelPropagation Label propagation Information spreading, advertising, and community discovery lpa int/string Hanp Label propagation advanced Community discovery, recommendation system hanp int/string ConnectedComponent Weakly connected component Community discovery, island discovery cc int/string StronglyConnectedComponent Strongly connected component Community discovery scc int/string ShortestPath The shortest path Path planning, network planning shortestpath string TriangleCount Triangle counting Network structure analysis trianglecount int/string GraphTriangleCount Graph triangle counting Network structure and tightness analysis count int BetweennessCentrality Intermediate centrality Key node mining, node influence computing betweenness double/string ClosenessCentrality Closeness centrality Key node mining, node influence computing closeness double/string DegreeStatic Degree of statistical Graph structure analysis degree,inDegree,outDegree int/string ClusteringCoefficient Aggregation coefficient Recommendation system, telecom fraud analysis clustercoefficient double/string Jaccard Jaccard similarity Similarity computing, recommendation system jaccard string BFS Breadth-First Search Sequence traversal, shortest path planning bfs string DFS Depth-First Search Sequence traversal, shortest path planning dfs string Node2Vec - Graph classification node2vec string <p>Note</p> <p>When writing the algorithm results into the NebulaGraph, make sure that the tag in the corresponding graph space has properties names and data types corresponding to the table above.</p>"},{"location":"graph-computing/nebula-algorithm/#implementation_methods","title":"Implementation methods","text":"<p>NebulaGraph Algorithm implements the graph calculating as follows:</p> <ol> <li> <p>Read the graph data of DataFrame from the NebulaGraph database using the NebulaGraph Spark Connector.</p> </li> <li> <p>Transform the graph data of DataFrame to the GraphX graph.</p> </li> <li> <p>Use graph algorithms provided by GraphX (such as PageRank) or self-implemented algorithms (such as Louvain).</p> </li> </ol> <p>For detailed implementation methods, see Scala file.</p>"},{"location":"graph-computing/nebula-algorithm/#get_nebulagraph_algorithm","title":"Get NebulaGraph Algorithm","text":""},{"location":"graph-computing/nebula-algorithm/#compile_and_package","title":"Compile and package","text":"<ol> <li> <p>Clone the repository <code>nebula-algorithm</code>.</p> <pre><code>$ git clone -b v3.0.0 https://github.com/vesoft-inc/nebula-algorithm.git\n</code></pre> </li> <li> <p>Enter the directory <code>nebula-algorithm</code>.</p> <pre><code>$ cd nebula-algorithm\n</code></pre> </li> <li> <p>Compile and package.</p> <pre><code>$ mvn clean package -Dgpg.skip -Dmaven.javadoc.skip=true -Dmaven.test.skip=true\n</code></pre> </li> </ol> <p>After the compilation, a similar file <code>nebula-algorithm-3.x.x.jar</code> is generated in the directory <code>nebula-algorithm/target</code>.</p>"},{"location":"graph-computing/nebula-algorithm/#download_maven_from_the_remote_repository","title":"Download maven from the remote repository","text":"<p>Download address</p>"},{"location":"graph-computing/nebula-algorithm/#how_to_use","title":"How to use","text":""},{"location":"graph-computing/nebula-algorithm/#use_algorithm_interface_recommended","title":"Use algorithm interface (recommended)","text":"<p>The <code>lib</code> repository provides 10 common graph algorithms.</p> <ol> <li> <p>Add dependencies to the file <code>pom.xml</code>.</p> <pre><code>&lt;dependency&gt;\n     &lt;groupId&gt;com.vesoft&lt;/groupId&gt;\n     &lt;artifactId&gt;nebula-algorithm&lt;/artifactId&gt;\n     &lt;version&gt;3.0.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> </li> <li> <p>Use the algorithm (take PageRank as an example) by filling in parameters. For more examples, see example.</p> <p>Note</p> <p>By default, the DataFrame that executes the algorithm sets the first column as the starting vertex, the second column as the destination vertex, and the third column as the edge weights (not the rank in the NebulaGraph).</p> <pre><code>val prConfig = new PRConfig(5, 1.0)\nval prResult = PageRankAlgo.apply(spark, data, prConfig, false)\n</code></pre> <p>If your vertex IDs are Strings, see Pagerank Example for how to encoding and decoding them.</p> </li> </ol>"},{"location":"graph-computing/nebula-algorithm/#submit_the_algorithm_package_directly","title":"Submit the algorithm package directly","text":"<ol> <li> <p>Set the Configuration file.</p> <pre><code>{\n  # Configurations related to Spark\n  spark: {\n    app: {\n        name: LPA\n        # The number of partitions of Spark\n        partitionNum:100\n    }\n    master:local\n  }\n\n  data: {\n    # Data source. Optional values are nebula, csv, and json.\n    source: csv\n    # Data sink. The algorithm result will be written into this sink. Optional values are nebula, csv, and text.\n    sink: nebula\n    # Whether the algorithm has a weight.\n    hasWeight: false\n  }\n\n  # Configurations related to NebulaGraph\n  nebula: {\n    # Data source. When NebulaGraph is the data source of the graph computing, the configuration of `nebula.read` is valid.\n    read: {\n        # The IP addresses and ports of all Meta services. Multiple addresses are separated by commas (,). Example: \"ip1:port1,ip2:port2\".\n        # To deploy NebulaGraph by using Docker Compose, fill in the port with which Docker Compose maps to the outside.\n        # Check the status with `docker-compose ps`.\n        metaAddress: \"192.168.*.10:9559\"\n        # The name of the graph space in NebulaGraph.\n        space: basketballplayer\n        # Edge types in NebulaGraph. When there are multiple labels, the data of multiple edges will be merged.\n        labels: [\"serve\"]\n        # The property name of each edge type in NebulaGraph. This property will be used as the weight column of the algorithm. Make sure that it corresponds to the edge type.\n        weightCols: [\"start_year\"]\n    }\n\n    # Data sink. When the graph computing result sinks into NebulaGraph, the configuration of `nebula.write` is valid.\n    write:{\n        # The IP addresses and ports of all Graph services. Multiple addresses are separated by commas (,). Example: \"ip1:port1,ip2:port2\".\n        # To deploy by using Docker Compose, fill in the port with which Docker Compose maps to the outside.\n        # Check the status with `docker-compose ps`.\n        graphAddress: \"192.168.*.11:9669\"\n        # The IP addresses and ports of all Meta services. Multiple addresses are separated by commas (,). Example: \"ip1:port1,ip2:port2\".\n        # To deploy NebulaGraph by using Docker Compose, fill in the port with which Docker Compose maps to the outside.\n        # Check the staus with `docker-compose ps`.\n        metaAddress: \"192.168.*.12:9559\"\n        user:root\n        pswd:nebula\n        # Before submitting the graph computing task, create the graph space and tag.\n        # The name of the graph space in NebulaGraph.\n        space:nb\n        # The name of the tag in NebulaGraph. The graph computing result will be written into this tag. The property name of this tag is as follows.\n        # PageRank: pagerank\n        # Louvain: louvain\n        # ConnectedComponent: cc\n        # StronglyConnectedComponent: scc\n        # LabelPropagation: lpa\n        # ShortestPath: shortestpath\n        # DegreeStatic: degree,inDegree,outDegree\n        # KCore: kcore\n        # TriangleCount: tranglecpunt\n        # BetweennessCentrality: betweennedss\n        tag:pagerank\n    }\n    }  \n\n  local: {\n    # Data source. When the data source is csv or json, the configuration of `local.read` is valid.\n    read:{\n        filePath: \"hdfs://127.0.0.1:9000/edge/work_for.csv\"\n        # If the CSV file has a header or it is a json file, use the header. If not, use [_c0, _c1, _c2, ..., _cn] instead.\n        # The header of the source VID column.\n        srcId:\"_c0\"\n        # The header of the destination VID column.\n        dstId:\"_c1\"\n        # The header of the weight column.\n        weight: \"_c2\"\n        # Whether the csv file has a header.\n        header: false\n        # The delimiter in the csv file.\n        delimiter:\",\"\n    }\n\n    # Data sink. When the graph computing result sinks to the csv or text file, the configuration of `local.write` is valid.\n    write:{\n        resultPath:/tmp/\n    }\n    }\n\n\n  algorithm: {\n    # The algorithm to execute. Optional values are as follow: \n    # pagerank, louvain, connectedcomponent, labelpropagation, shortestpaths, \n    # degreestatic, kcore, stronglyconnectedcomponent, trianglecount ,\n    # betweenness, graphtriangleCount.\n    executeAlgo: pagerank\n\n    # PageRank\n    pagerank: {\n        maxIter: 10\n        resetProb: 0.15 \n        encodeId:false # Configure true if the VID is of string type.\n    }\n\n    # Louvain\n    louvain: {\n        maxIter: 20\n        internalIter: 10\n        tol: 0.5\n        encodeId:false # Configure true if the VID is of string type.\n    }\n\n   # ...\n\n}\n}\n</code></pre> <p>Note</p> <p>When <code>sink: nebula</code> is configured, it means that the algorithm results will be written back to the NebulaGraph cluster. The property names of the tag have implicit conventions. For details, see Supported algorithms section of this topic.</p> </li> <li> <p>Submit the graph computing task.</p> <pre><code>${SPARK_HOME}/bin/spark-submit --master &lt;mode&gt; --class com.vesoft.nebula.algorithm.Main &lt;nebula-algorithm-3.0.0.jar_path&gt; -p &lt;application.conf_path&gt;\n</code></pre> <p>Example:</p> <pre><code>${SPARK_HOME}/bin/spark-submit --master \"local\" --class com.vesoft.nebula.algorithm.Main /root/nebula-algorithm/target/nebula-algorithm-3.0-SNAPSHOT.jar -p /root/nebula-algorithm/src/main/resources/application.conf\n</code></pre> </li> </ol>"},{"location":"graph-computing/nebula-analytics/","title":"NebulaGraph Analytics","text":"<p>NebulaGraph Analytics is a high-performance graph computing framework tool that performs graph analysis of data in the NebulaGraph database.</p>"},{"location":"graph-computing/nebula-analytics/#prerequisites","title":"Prerequisites","text":"<ul> <li>The NebulaGraph Analytics installation package has been obtained. Contact us to apply.</li> </ul> <ul> <li>The license key is loaded.</li> </ul> <ul> <li>The HDFS 2.2.x or later has been deployed.</li> </ul> <ul> <li>The JDK 1.8 has been deployed.</li> </ul>"},{"location":"graph-computing/nebula-analytics/#scenarios","title":"Scenarios","text":"<p>You can import data from data sources as NebulaGraph clusters, CSV files on HDFS, or local CSV files into NebulaGraph Analytics and export the graph computation results to NebulaGraph clusters, CSV files on HDFS, or local CSV files from NebulaGraph Analytics.</p>"},{"location":"graph-computing/nebula-analytics/#limitations","title":"Limitations","text":"<p>When you import NebulaGraph cluster data into NebulaGraph Analytics and export the graph computation results from NebulaGraph Analytics to a NebulaGraph cluster, the graph computation results can only be exported to the graph space where the data source is located.</p>"},{"location":"graph-computing/nebula-analytics/#version_compatibility","title":"Version compatibility","text":"<p>The version correspondence between NebulaGraph Analytics and NebulaGraph is as follows.</p> NebulaGraph NebulaGraph Analytics 3.5.0 3.5.0 3.4.0 ~ 3.4.1 3.5.0\u30013.4.0 3.3.0 3.3.0 3.1.0 ~ 3.2.x 3.2.0 3.0.x 1.0.x 2.6.x 0.9.0"},{"location":"graph-computing/nebula-analytics/#graph_algorithms","title":"Graph algorithms","text":"<p>NebulaGraph Analytics supports the following graph algorithms.</p> Algorithm Description Category APSP All Pair Shortest Path Path SSSP Single Source Shortest Path Path BFS Breadth-first search Path ShortestPath The shortest path Path PageRank It is used to rank web pages. Node importance measurement KCore k-Cores Node importance measurement DegreeCentrality It is a simple count of the total number of connections linked to a vertex. Node importance measurement DegreeWithTime Neighbor statistics based on the time range of edge ranks Node importance measurement BetweennessCentrality Intermediate centrality Node importance measurement ClosenessCentrality Closeness centrality Node importance measurement TriangleCount It counts the number of triangles. Graph feature Node2Vec Graph neural network Graph feature Tree_stat Tree structure statistics Graph feature HyperANF Estimate the average distance of the graph Graph feature LPA Label Propagation Algorithm Community discovery WCC Weakly connected component Community discovery LOUVAIN It detects communities in large networks. Community discovery InfoMap Community classification Community discovery HANP Hop attenuation &amp; Node Preference Community discovery Clustering Coefficient It is a measure of the degree to which nodes in a graph tend to cluster together. Clustering Jaccard Jaccard similarity Similarity"},{"location":"graph-computing/nebula-analytics/#install_nebulagraph_analytics","title":"Install NebulaGraph Analytics","text":"<ol> <li> <p>Install the NebulaGraph Analytics. When installing a cluster of multiple NebulaGraph Analytics on multiple nodes, you need to install NebulaGraph Analytics to the same path and set up SSH-free login between nodes.</p> <pre><code>sudo rpm -ivh &lt;analytics_package_name&gt; --prefix &lt;install_path&gt;\nsudo chown &lt;user&gt;:&lt;user&gt; -R &lt;install path&gt;\n</code></pre> <p>For example:</p> <pre><code>sudo rpm -ivh nebula-analytics-3.5.0-centos.x86_64.rpm --prefix=/home/vesoft/nebula-analytics\nsudo chown vesoft:vesoft -R /home/vesoft/nebula-analytics\n</code></pre> </li> <li> <p>Configure the correct Hadoop path and JDK path in the file <code>set_env.sh</code>, the file path is <code>nebula-analytics/scripts/set_env.sh</code>. If there are multiple machines, ensure that the paths are the same.</p> <p>Note</p> <p>The default TCP port range used by the MPICH process manager and MPICH library is 10000 to 10100. To adjust this, modify the value of the environment variable <code>MPIR_CVAR_CH3_PORT_RANGE</code> in the <code>set_env.sh</code> file.</p> <pre><code>export HADOOP_HOME=&lt;hadoop_path&gt;\nexport JAVA_HOME=&lt;java_path&gt;\n</code></pre> </li> <li> <p>Configure the <code>analytics.conf</code> file with the path <code>nebula-analytics/scripts/analytics.conf</code>. Set the value of <code>license_manager_url</code> to the host IP and port number <code>9119</code> where the license management tool is located, e.g. <code>192.168.8.100:9119</code>.</p> </li> </ol>"},{"location":"graph-computing/nebula-analytics/#how_to_use_nebulagraph_analytics","title":"How to use NebulaGraph Analytics","text":"<p>After installation, you can set parameters of different algorithms and then execute a script to obtain the results of the algorithms and export them to the specified format.</p> <ol> <li> <p>Select one node from the NebulaGraph Analytics cluster and then access the <code>scripts</code> directory.</p> <pre><code>$ cd scripts\n</code></pre> </li> <li> <p>Confirm the data source and export path. Configuration steps are as follows.</p> <ul> <li> <p>NebulaGraph clusters as the data source</p> <ol> <li> <p>Modify the configuration file <code>nebula.conf</code> to configure the NebulaGraph cluster. </p> <pre><code># The number of retries connecting to NebulaGraph.\n--retry=3  \n# The name of the graph space where you read or write data.\n--space=baskeyballplayer  \n\n# Read data from NebulaGraph.\n# The name of edges.\n--edges=LIKES  \n# The name of the property to be read as the weight of the edge. Can be either the attribute name or _rank.\n#--edge_data_fields \n# The number of rows read per scan.\n--read_batch_size=10000  \n\n# Write data to NebulaGraph.\n# The graphd process address.\n--graph_server_addrs=192.168.8.100:9669  \n# The account to log into NebulaGraph.\n--user=root  \n# The password to log into NebulaGraph.\n--password=nebula  \n# The pattern used to write data back to NebulaGraph: insert or update.\n--mode=insert  \n# The tag name written back to NebulaGraph.\n--tag=pagerank  \n# The property name corresponding to the tag.\n--prop=pr  \n# The property type corresponding the the tag.\n--type=double \n# The number of rows per write. \n--write_batch_size=1000 \n# The file path where the data failed to be written back to NebulaGraph is stored.\n--err_file=/home/xxx/analytics/err.txt \n\n# other\n# The access timeout period of the service.\n--graphd_timeout=60000\n--metad_timeout=60000\n--storaged_timeout=60000\n</code></pre> </li> <li> <p>Modify the related parameters in the script to be used, such as <code>run_pagerank.sh</code>. </p> <pre><code># The sum of the number of processes running on all machines in the cluster. It is recommended to be the number of machines or the number of nodes in the NUMA architecture.\nWNUM=3 \n# The number of threads per process. It is recommended to set the maximum value to be the number of hardware threads of the machine.\nWCORES=4  \n# The path to the data source.\n# Set to read data from NebulaGraph via the nebula.conf file.\nINPUT=${INPUT:=\"nebula:$PROJECT/scripts/nebula.conf\"}  \n# Set to read data from the CSV files on HDFS or on local directories.\n# #INPUT=${INPUT:=\"$PROJECT/data/graph/v100_e2150_ua_c3.csv\"}\n\n# The export path to the graph computation results.\n# Data can be exported to a NebulaGraph. If the data source is also a NebulaGraph, the results will be exported to the graph space specified in nebula.conf.\nOUTPUT=${OUTPUT:=\"nebula:$PROJECT/scripts/nebula.conf\"}\n# Data can also be exported to the CSV files on HDFS or on local directories.\n# OUTPUT=${OUTPUT:='hdfs://192.168.8.100:9000/_test/output'}\n\n# If the value is true, it is a directed graph, if false, it is an undirected graph.\nIS_DIRECTED=${IS_DIRECTED:=true}\n# Set whether to encode ID or not.\nNEED_ENCODE=${NEED_ENCODE:=true}\n# The ID type of the data source vertices. For example string, int32, and int64.\nVTYPE=${VTYPE:=int32}\n# Encoding type. The value distributed specifies the distributed vertex ID encoding. The value single specifies the single-machine vertex ID encoding. \nENCODER=${ENCODER:=\"distributed\"}\n# The parameter for the PageRank algorithm. Algorithms differ in parameters.\nEPS=${EPS:=0.0001}\nDAMPING=${DAMPING:=0.85}\n# The number of iterations.\nITERATIONS=${ITERATIONS:=100}\n</code></pre> </li> </ol> </li> </ul> <ul> <li> <p>Local or HDFS CSV files as the data source</p> <p>Modify parameters in the script to be used, such as <code>run_pagerank.sh</code>.</p> <pre><code># The sum of the number of processes running on all machines in the cluster. It is recommended to be the number of machines or the number of nodes in the NUMA architecture.\nWNUM=3 \n# The number of threads per process. It is recommended to set the maximum value to be the number of hardware threads of the machine.\nWCORES=4  \n# The path to the data source.\n# Set to read data from NebulaGraph via the nebula.conf file.\n# INPUT=${INPUT:=\"nebula:$PROJECT/scripts/nebula.conf\"}  \n# Set to read data from the CSV files on HDFS or on local directories.\nINPUT=${INPUT:=\"$PROJECT/data/graph/v100_e2150_ua_c3.csv\"}\n\n# The export path to the graph computation results.\n# Data can be exported to a NebulaGraph. If the data source is also a NebulaGraph, the results will be exported to the graph space specified in nebula.conf.\n# OUTPUT=${OUTPUT:=\"nebula:$PROJECT/scripts/nebula.conf\"}\n# Data can also be exported to the CSV files on HDFS or on local directories.\nOUTPUT=${OUTPUT:='hdfs://192.168.8.100:9000/_test/output'}\n\n# If the value is true, it is a directed graph, if false, it is an undirected graph.\nIS_DIRECTED=${IS_DIRECTED:=true}\n# Set whether to encode ID or not.\nNEED_ENCODE=${NEED_ENCODE:=true}\n# The ID type of the data source vertices. For example string, int32, and int64.\nVTYPE=${VTYPE:=int32}\n# The value distributed specifies the distributed vertex ID encoding. The value single specifies the single-machine vertex ID encoding. \nENCODER=${ENCODER:=\"distributed\"}\n# The parameter for the PageRank algorithm. Algorithms differ in parameters.\nEPS=${EPS:=0.0001}\nDAMPING=${DAMPING:=0.85}\n# The number of iterations.\nITERATIONS=${ITERATIONS:=100}\n</code></pre> </li> </ul> </li> <li> <p>Modify the configuration file <code>cluster</code> to set the NebulaGraph Analytics cluster nodes and task assignment weights for executing the algorithm.</p> <pre><code># NebulaGraph Analytics Cluster Node IP Addresses: Task Assignment Weights\n192.168.8.200:1\n192.168.8.201:1\n192.168.8.202:1\n</code></pre> </li> <li> <p>Run the algorithm script. For example:</p> <pre><code>./run_pagerank.sh\n</code></pre> </li> <li> <p>View the graph computation results in the export path.</p> <ul> <li>For exporting to a NebulaGraph cluster, check the results according to the settings in <code>nebula.conf</code>.</li> </ul> <ul> <li>For exporting the results to the CSV files on HDFS or on local directories, check the results according to the settings in <code>OUTPUT</code>, which is a compressed file in the <code>.gz</code> format.</li> </ul> </li> </ol>"},{"location":"graph-computing/use-explorer/","title":"NebulaGraph Explorer Workflow","text":"<p>NebulaGraph Explorer provides workflows for visual calculations.</p> <p>For more details, see Workflows.</p> <p>Enterpriseonly</p> <p>To apply for the NebulaGraph Explorer installation package, contact us. </p>"},{"location":"nebula-dashboard/1.what-is-dashboard/","title":"What is NebulaGraph Dashboard Community Edition","text":"<p>NebulaGraph Dashboard Community Edition (Dashboard for short) is a visualization tool that monitors the status of machines and services in NebulaGraph clusters.</p> <p>Enterpriseonly</p> <p>Dashboard Enterprise Edition adds features such as visual cluster creation, batch import of clusters, fast scaling, etc. For more information, see Pricing.</p>"},{"location":"nebula-dashboard/1.what-is-dashboard/#features","title":"Features","text":"<p>Dashboard monitors:</p> <ul> <li>The status of all the machines in clusters, including CPU, memory, load, disk, and network.</li> </ul> <ul> <li>The information of all the services in clusters, including the IP addresses, versions, and monitoring metrics (such as the number of queries, the latency of queries, the latency of heartbeats, and so on).</li> </ul> <ul> <li>The information of clusters, including the information of services, partitions, configurations, and long-term tasks.</li> </ul> <ul> <li>Set how often the metrics page refreshes.</li> </ul>"},{"location":"nebula-dashboard/1.what-is-dashboard/#scenarios","title":"Scenarios","text":"<p>You can use Dashboard in one of the following scenarios:</p> <ul> <li>You want to monitor key metrics conveniently and quickly, and present multiple key information of the business to ensure the business operates normally.</li> </ul> <ul> <li>You want to monitor clusters from multiple dimensions (such as the time, aggregate rules, and metrics).</li> </ul> <ul> <li>After a failure occurs, you need to review it and confirm its occurrence time and unexpected phenomena.</li> </ul>"},{"location":"nebula-dashboard/1.what-is-dashboard/#precautions","title":"Precautions","text":"<p>The monitoring data will be retained for 14 days by default, that is, only the monitoring data within the last 14 days can be queried.</p> <p>Note</p> <p>The monitoring service is supported by Prometheus. The update frequency and retention intervals can be modified. For details, see Prometheus.</p>"},{"location":"nebula-dashboard/1.what-is-dashboard/#version_compatibility","title":"Version compatibility","text":"<p>The version correspondence between NebulaGraph and Dashboard Community Edition is as follows.</p> NebulaGraph version Dashboard version 3.5.0 3.4.0 3.4.0 ~ 3.4.1 3.4.0\u30013.2.0 3.3.0 3.2.0 2.5.0 ~ 3.2.0 3.1.0 2.5.x ~ 3.1.0 1.1.1 2.0.1~2.5.1 1.0.2 2.0.1~2.5.1 1.0.1"},{"location":"nebula-dashboard/1.what-is-dashboard/#release_note","title":"Release note","text":"<p>Release</p>"},{"location":"nebula-dashboard/2.deploy-dashboard/","title":"Deploy Dashboard Community Edition","text":"<p>This topic will describe how to deploy NebulaGraph Dashboard in detail.</p> <p>To download and compile the latest source code of Dashboard, follow the instructions on the nebula dashboard GitHub page.</p>"},{"location":"nebula-dashboard/2.deploy-dashboard/#prerequisites","title":"Prerequisites","text":"<p>Before you deploy Dashboard, you must confirm that:</p> <ul> <li>The NebulaGraph services are deployed and started. For more information, see NebulaGraph Database Manual.</li> </ul> <ul> <li> <p>Before the installation starts, the following ports are not occupied.</p> <ul> <li>9200</li> </ul> <ul> <li>9100</li> </ul> <ul> <li>9090</li> </ul> <ul> <li>8090</li> </ul> <ul> <li>7003</li> </ul> </li> </ul> <ul> <li>The node-exporter is installed on the machines to be monitored. For details on installation, see Prometheus document.</li> </ul>"},{"location":"nebula-dashboard/2.deploy-dashboard/#steps","title":"Steps","text":"<ol> <li> <p>Download the tar packagenebula-dashboard-3.4.0.x86_64.tar.gz  as needed.</p> </li> <li> <p>Run <code>tar -xvf nebula-dashboard-3.4.0.x86_64.tar.gz</code> to decompress the installation package.</p> </li> <li> <p>Modify the <code>config.yaml</code> file in <code>nebula-dashboard</code>.</p> <p>The configuration file contains the configurations of four dependent services and configurations of clusters. The descriptions of the dependent services are as follows.</p> Service Default port Description nebula-http-gateway 8090 Provides HTTP ports for cluster services to execute nGQL statements to interact with the NebulaGraph database. nebula-stats-exporter 9200 Collects the performance metrics in the cluster, including the IP addresses, versions, and monitoring metrics (such as the number of queries, the latency of queries, the latency of heartbeats, and so on). node-exporter 9100 Collects the source information of nodes in the cluster, including the CPU, memory, load, disk, and network. prometheus 9090 The time series database that stores monitoring data. <p>The descriptions of the configuration file are as follows.</p> <pre><code>port: 7003   # Web service port.\ngateway:\n  ip: hostIP   # The IP of the machine where the Dashboard is deployed.\n  port: 8090\n  https: false  # Whether to enable HTTPS.\n  runmode: dev  # Program running mode, including dev, test, and prod. It is used to distinguish between different running environments generally.\nstats-exporter:\n  ip: hostIP   # The IP of the machine where the Dashboard is deployed.\n  nebulaPort: 9200\n  https: false  # Whether to enable HTTPS.\nnode-exporter:\n  - ip: nebulaHostIP_1 # The IP of the machine where the NebulaGraph is deployed.\n    port: 9100\n    https: false # Whether to enable HTTPS.\n# - ip: nebulaHostIP_2\n#   port: 9100\n#   https: false\nprometheus:\n  ip: hostIP    # The IP of the machine where the Dashboard is deployed.\n  prometheusPort: 9090\n  https: false  # Whether to enable HTTPS.\n  scrape_interval: 5s  # The interval for collecting the monitoring data, which is 1 minute by default.\n  evaluation_interval: 5s  # The interval for running alert rules, which is 1 minute by default.\n# Cluster node info\nnebula-cluster:\n  name: 'default' # Cluster name\n  metad:\n    - name: metad0\n      endpointIP: nebulaMetadIP  # The IP of the machine where the Meta service is deployed.\n      port: 9559\n      endpointPort: 19559\n  # - name: metad1\n  #   endpointIP: nebulaMetadIP\n  #   port: 9559\n  #   endpointPort: 19559  \n  graphd:\n    - name: graphd0\n      endpointIP: nebulaGraphdIP  # The IP of the machine where the Graph service is deployed.\n      port: 9669\n      endpointPort: 19669\n  # - name: graphd1\n  #   endpointIP: nebulaGraphdIP\n  #   port: 9669\n  #   endpointPort: 19669  \n  storaged:\n    - name: storaged0\n      endpointIP: nebulaStoragedIP  # The IP of the machine where the Storage service is deployed.\n      port: 9779\n      endpointPort: 19779\n  # - name: storaged1\n  #   endpointIP: nebulaStoragedIP\n  #   port: 9779\n  #   endpointPort: 19779  \n</code></pre> </li> <li> <p>Run <code>./dashboard.service start all</code> to start the services.</p> </li> </ol>"},{"location":"nebula-dashboard/2.deploy-dashboard/#deploy_dashboard_with_docker_compose","title":"Deploy Dashboard with Docker Compose","text":"<p>If you are deploying Dashboard using docker, you should also modify the configuration file <code>config.yaml</code>, and then run <code>docker-compose up -d</code> to start the container.</p> <p>Note</p> <p>If you change the port number in <code>config.yaml</code>, the port number in <code>docker-compose.yaml</code> needs to be consistent as well.</p> <p>Run <code>docker-compose stop</code> to stop the container.</p>"},{"location":"nebula-dashboard/2.deploy-dashboard/#manage_services_in_dashboard","title":"Manage services in Dashboard","text":"<p>You can use the <code>dashboard.service</code> script to start, restart, stop, and check the Dashboard services.</p> <pre><code>sudo &lt;dashboard_path&gt;/dashboard.service\n[-v] [-h]\n&lt;start|restart|stop|status&gt;  &lt;prometheus|webserver|exporter|gateway|all&gt;\n</code></pre> Parameter Description <code>dashboard_path</code> Dashboard installation path. <code>-v</code> Display detailed debugging information. <code>-h</code> Display help information. <code>start</code> Start the target services. <code>restart</code> Restart the target services. <code>stop</code> Stop the target services. <code>status</code> Check the status of the target services. <code>prometheus</code> Set the prometheus service as the target service. <code>webserver</code> Set the webserver Service as the target service. <code>exporter</code> Set the exporter Service as the target service. <code>gateway</code> Set the gateway Service as the target service. <code>all</code> Set all the Dashboard services as the target services. <p>Note</p> <p>To view the Dashboard version, run the command <code>./dashboard.service -version</code>.</p>"},{"location":"nebula-dashboard/2.deploy-dashboard/#next_to_do","title":"Next to do","text":"<p>Connect to Dashboard</p>"},{"location":"nebula-dashboard/3.connect-dashboard/","title":"Connect Dashboard","text":"<p>After Dashboard is deployed, you can log in and use Dashboard on the browser.</p>"},{"location":"nebula-dashboard/3.connect-dashboard/#prerequisites","title":"Prerequisites","text":"<ul> <li>The Dashboard services are started. For more information, see Deploy Dashboard.</li> </ul> <ul> <li>We recommend you to use the Chrome browser of the version above 89. Otherwise, there may be compatibility issues.</li> </ul>"},{"location":"nebula-dashboard/3.connect-dashboard/#procedures","title":"Procedures","text":"<ol> <li> <p>Confirm the IP address of the machine where the Dashboard service is installed. Enter <code>&lt;IP&gt;:7003</code> in the browser to open the login page.</p> </li> <li> <p>Enter the username and the passwords of the NebulaGraph database.</p> <ul> <li>If authentication is enabled, you can log in with the created accounts.</li> </ul> <ul> <li>If authentication is not enabled, you can only log in using <code>root</code> as the username and random characters as the password.</li> </ul> <p>To enable authentication, see Authentication.</p> </li> <li> <p>Select the NebulaGraph version to be used.</p> </li> <li> <p>Click Login.</p> </li> </ol>"},{"location":"nebula-dashboard/4.use-dashboard/","title":"Dashboard","text":"<p>NebulaGraph Dashboard consists of three parts: Machine, Service, and Management. This topic will describe them in detail.</p>"},{"location":"nebula-dashboard/4.use-dashboard/#overview","title":"Overview","text":""},{"location":"nebula-dashboard/4.use-dashboard/#machine","title":"Machine","text":"<p>Click Machine-&gt;Overview to enter the machine overview page.</p> <p>On this page, you can view the variation of CPU, Memory, Load, Disk, and Network In/Out quickly.</p> <ul> <li>By default, you can view the monitoring data for a maximum of 14 days. You can also select a time range or quickly select the latest 1 hour, 6 hours, 12 hours, 1 day, 3 days, 7 days, or 14 days.</li> <li>By default, you can view the monitoring data of all the instances in clusters. You can select the instances you want to view in the instance box.</li> <li>By default, the monitoring information page will not be updated automatically. You can set the update frequency of the monitoring information page globally or click the  button to update the page manually.</li> <li>To set a base line, click the  button.</li> <li> <p>To view the detailed monitoring information, click the  button. In this example, select <code>Load</code> for details. The figure is as follows.</p> <p></p> <ul> <li>You can set the monitoring time range, instance, update frequency and base line.</li> <li>You can search for or select the target metric. For details about monitoring metrics, see Metrics.</li> <li>You can temporarily hide nodes that you do not need to view.</li> <li>You can click the  button to view the detailed monitoring information.</li> </ul> </li> </ul>"},{"location":"nebula-dashboard/4.use-dashboard/#service","title":"Service","text":"<p>Click Service-&gt;Overview to enter the service overview page.</p> <p>On this page, you can view the information of Graph, Meta, and Storage services quickly. In the upper right corner, the number of normal services and abnormal services will be displayed.</p> <p>Note</p> <p>In the  Service page, only two monitoring metrics can be set for each service, which can be adjusted by clicking the Set up button.</p> <ul> <li>By default, you can view the monitoring data for a maximum of 14 days. You can also select a time range or quickly select the latest 1 hour, 6 hours, 12 hours, 1 day, 3 days, 7 days, or 14 days.</li> <li>By default, you can view the monitoring data of all the instances in clusters. You can select the instances you want to view in the instance box.</li> <li>By default, the monitoring information page will not be updated automatically. You can set the update frequency of the monitoring information page globally or click the  button to update the page manually.</li> <li>You can view the status of all the services in a cluster.</li> <li> <p>To view the detailed monitoring information, click the  button. In this example, select <code>Graph</code> for details. The figure is as follows.</p> <p></p> <ul> <li>You can set the monitoring time range, instance, update frequency, period, aggregation and base line.</li> <li>You can search for or select the target metric. For details of monitoring metrics, see Monitor parameter.</li> <li>You can temporarily hide nodes that you do not need to view.</li> <li>You can click the  button to view the detailed monitoring information.</li> <li>The Graph service supports a set of space-level metrics. For more information, see the following section Graph space.</li> </ul> </li> </ul>"},{"location":"nebula-dashboard/4.use-dashboard/#graph_space","title":"Graph space","text":"<p>Note</p> <p>Before using graph space metrics, you need to set <code>enable_space_level_metrics</code> to <code>true</code> in the Graph service. For details, see [Graph Service configuration](../5.configurations-and-logs/1.configurations/3.graph-config.md.</p> <p>Space-level metric incompatibility</p> <p>If a graph space name contains special characters, the corresponding metric data of that graph space may not be displayed. </p> <p>The service monitoring page can also monitor graph space level metrics. Only when the behavior of a graph space metric is triggered, you can specify the graph space to view information about the corresponding graph space metric.</p> <p>Space graph metrics record the information of different graph spaces separately. Currently, only the Graph service supports a set of space-level metrics.</p> <p>For information about the space graph metrics, see Graph space.</p> <p></p>"},{"location":"nebula-dashboard/4.use-dashboard/#management","title":"Management","text":""},{"location":"nebula-dashboard/4.use-dashboard/#overview_info","title":"Overview info","text":"<p>On the Overview Info page, you can see the information of the NebulaGraph cluster, including Storage leader distribution, Storage service details, versions and hosts information of each NebulaGraph service, and partition distribution and details.</p> <p></p>"},{"location":"nebula-dashboard/4.use-dashboard/#storage_leader_distribution","title":"Storage Leader Distribution","text":"<p>In this section, the number of Leaders and the Leader distribution will be shown.</p> <ul> <li>Click the Balance Leader button in the upper right corner to distribute Leaders evenly and quickly in the NebulaGraph cluster. For details about the Leader, see Storage Service.</li> </ul> <ul> <li>Click Detail in the upper right corner to view the details of the Leader distribution.</li> </ul>"},{"location":"nebula-dashboard/4.use-dashboard/#version","title":"Version","text":"<p>In this section, the version and host information of each NebulaGraph service will be shown. Click Detail in the upper right corner to view the details of the version and host information.</p>"},{"location":"nebula-dashboard/4.use-dashboard/#service_information","title":"Service information","text":"<p>In this section, the information on Storage services will be shown. The parameter description is as follows:</p> Parameter Description <code>Host</code> The IP address of the host. <code>Port</code> The port of the host. <code>Status</code> The host status. <code>Git Info Sha</code> The commit ID of the current version. <code>Leader Count</code> The number of Leaders. <code>Partition Distribution</code> The distribution of partitions. <code>Leader Distribution</code> The distribution of Leaders. <p>Click Detail in the upper right corner to view the details of the Storage service information.</p>"},{"location":"nebula-dashboard/4.use-dashboard/#partition_distribution","title":"Partition Distribution","text":"<p>Select the specified graph space in the upper left corner, you can view the distribution of partitions in the specified graph space. You can see the IP addresses and ports of all Storage services in the cluster, and the number of partitions in each Storage service.</p> <p>Click Detail in the upper right corner to view more details.</p>"},{"location":"nebula-dashboard/4.use-dashboard/#partition_information","title":"Partition information","text":"<p>In this section, the information on partitions will be shown. Before viewing the partition information, you need to select a graph space in the upper left corner. The parameter description is as follows:</p> Parameter Description <code>Partition ID</code> The ID of the partition. <code>Leader</code> The IP address and port of the leader. <code>Peers</code> The IP addresses and ports of all the replicas. <code>Losts</code> The IP addresses and ports of faulty replicas. <p>Click Detail in the upper right corner to view details. You can also enter the partition ID into the input box in the upper right corner of the details page to filter the shown data. </p>"},{"location":"nebula-dashboard/4.use-dashboard/#config","title":"Config","text":"<p>It shows the configuration of the NebulaGraph service. NebulaGraph Dashboard Community Edition does not support online modification of configurations for now.</p>"},{"location":"nebula-dashboard/4.use-dashboard/#others","title":"Others","text":"<p>In the lower left corner of the page, you can:</p> <ul> <li>Sign out</li> </ul> <ul> <li>Switch between Chinese and English</li> </ul> <ul> <li>View the current Dashboard release</li> </ul> <ul> <li>View the user manual and forum</li> </ul> <ul> <li>Fold the sidebar</li> </ul>"},{"location":"nebula-dashboard/6.monitor-parameter/","title":"Metrics","text":"<p>This topic will describe the monitoring metrics in NebulaGraph Dashboard.</p>"},{"location":"nebula-dashboard/6.monitor-parameter/#machine","title":"Machine","text":"<p>Note</p> <ul> <li>All the machine metrics listed below are for the Linux operating system.</li> <li>The default unit in Disk and Network is byte. The unit will change with the data magnitude as the page displays. For example, when the flow is less than 1 KB/s, the unit will be Bytes/s.</li> <li>For versions of Dashboard Community Edition greater than v1.0.2, the memory occupied by Buff and Cache will not be counted in the memory usage.</li> </ul>"},{"location":"nebula-dashboard/6.monitor-parameter/#cpu","title":"CPU","text":"Parameter Description <code>cpu_utilization</code> The percentage of used CPU. <code>cpu_idle</code> The percentage of idled CPU. <code>cpu_wait</code> The percentage of CPU waiting for IO operations. <code>cpu_user</code> The percentage of CPU used by users. <code>cpu_system</code> The percentage of CPU used by the system."},{"location":"nebula-dashboard/6.monitor-parameter/#memory","title":"Memory","text":"Parameter Description <code>memory_utilization</code> The percentage of used memory. <code>memory_used</code> The memory space used (not including caches). <code>memory_free</code> The memory space available."},{"location":"nebula-dashboard/6.monitor-parameter/#load","title":"Load","text":"Parameter Description <code>load_1m</code> The average load of the system in the last 1 minute. <code>load_5m</code> The average load of the system in the last 5 minutes. <code>load_15m</code> The average load of the system in the last 15 minutes."},{"location":"nebula-dashboard/6.monitor-parameter/#disk","title":"Disk","text":"Parameter Description <code>disk_used_percentage</code> The disk utilization percentage. <code>disk_used</code> The disk space used. <code>disk_free</code> The disk space available. <code>disk_readbytes</code> The number of bytes that the system reads in the disk per second. <code>disk_writebytes</code> The number of bytes that the system writes in the disk per second. <code>disk_readiops</code> The number of read queries that the disk receives per second. <code>disk_writeiops</code> The number of write queries that the disk receives per second. <code>inode_utilization</code> The percentage of used inode."},{"location":"nebula-dashboard/6.monitor-parameter/#network","title":"Network","text":"Parameter Description <code>network_in_rate</code> The number of bytes that the network card receives per second. <code>network_out_rate</code> The number of bytes that the network card sends out per second. <code>network_in_errs</code> The number of wrong bytes that the network card receives per second. <code>network_out_errs</code> The number of wrong bytes that the network card sends out per second. <code>network_in_packets</code> The number of data packages that the network card receives per second. <code>network_out_packets</code> The number of data packages that the network card sends out per second."},{"location":"nebula-dashboard/6.monitor-parameter/#service","title":"Service","text":""},{"location":"nebula-dashboard/6.monitor-parameter/#period","title":"Period","text":"<p>The period is the time range of counting metrics. It currently supports 5 seconds, 60 seconds, 600 seconds, and 3600 seconds, which respectively represent the last 5 seconds, the last 1 minute, the last 10 minutes, and the last 1 hour.</p>"},{"location":"nebula-dashboard/6.monitor-parameter/#metric_methods","title":"Metric methods","text":"Parameter Description <code>rate</code> The average rate of operations per second in a period. <code>sum</code> The sum of operations in the period. <code>avg</code> The average latency in the cycle. <code>P75</code> The 75th percentile latency. <code>P95</code> The 95th percentile latency. <code>P99</code> The 99th percentile latency. <code>P999</code> The 99.9th percentile latency. <p>Note</p> <p>Dashboard collects the following metrics from the NebulaGraph core, but only shows the metrics that are important to it. </p>"},{"location":"nebula-dashboard/6.monitor-parameter/#graph","title":"Graph","text":"Parameter Description <code>num_active_queries</code> The number of changes in the number of active queries. Formula: The number of started queries minus the number of finished queries within a specified time. <code>num_active_sessions</code> The number of changes in the number of active sessions. Formula: The number of logged in sessions minus the number of logged out sessions within a specified time.For example, when querying <code>num_active_sessions.sum.5</code>, if there were 10 sessions logged in and 30 sessions logged out in the last 5 seconds, the value of this metric is <code>-20</code> (10-30). <code>num_aggregate_executors</code> The number of executions for the Aggregation operator. <code>num_auth_failed_sessions_bad_username_password</code> The number of sessions where authentication failed due to incorrect username and password. <code>num_auth_failed_sessions_out_of_max_allowed</code> The number of sessions that failed to authenticate logins because the value of the parameter <code>FLAG_OUT_OF_MAX_ALLOWED_CONNECTIONS</code> was exceeded. <code>num_auth_failed_sessions</code> The number of sessions in which login authentication failed. <code>num_indexscan_executors</code> The number of executions for index scan operators. <code>num_killed_queries</code> The number of killed queries. <code>num_opened_sessions</code> The number of sessions connected to the server. <code>num_queries</code> The number of queries. <code>num_query_errors_leader_changes</code> The number of the raft leader changes due to query errors. <code>num_query_errors</code> The number of query errors. <code>num_reclaimed_expired_sessions</code> The number of expired sessions actively reclaimed by the server. <code>num_rpc_sent_to_metad_failed</code> The number of failed RPC requests that the Graphd service sent to the Metad service. <code>num_rpc_sent_to_metad</code> The number of RPC requests that the Graphd service sent to the Metad service. <code>num_rpc_sent_to_storaged_failed</code> The number of failed RPC requests that the Graphd service sent to the Storaged service. <code>num_rpc_sent_to_storaged</code> The number of RPC requests that the Graphd service sent to the Storaged service. <code>num_sentences</code> The number of statements received by the Graphd service. <code>num_slow_queries</code> The number of slow queries. <code>num_sort_executors</code> The number of executions for the Sort operator. <code>optimizer_latency_us</code> The latency of executing optimizer statements. <code>query_latency_us</code> The latency of queries. <code>slow_query_latency_us</code> The latency of slow queries. <code>num_queries_hit_memory_watermark</code> The number of queries reached the memory watermark."},{"location":"nebula-dashboard/6.monitor-parameter/#meta","title":"Meta","text":"Parameter Description <code>commit_log_latency_us</code> The latency of committing logs in Raft. <code>commit_snapshot_latency_us</code> The latency of committing snapshots in Raft. <code>heartbeat_latency_us</code> The latency of heartbeats. <code>num_heartbeats</code> The number of heartbeats. <code>num_raft_votes</code> The number of votes in Raft. <code>transfer_leader_latency_us</code> The latency of transferring the raft leader. <code>num_agent_heartbeats</code> The number of heartbeats for the AgentHBProcessor. <code>agent_heartbeat_latency_us</code> The latency of the AgentHBProcessor. <code>replicate_log_latency_us</code> The latency of replicating the log record to most nodes by Raft. <code>num_send_snapshot</code> The number of times that Raft sends snapshots to other nodes. <code>append_log_latency_us</code> The latency of replicating the log record to a single node by Raft. <code>append_wal_latency_us</code> The Raft write latency for a single WAL. <code>num_grant_votes</code> The number of times that Raft votes for other nodes. <code>num_start_elect</code> The number of times that Raft starts an election."},{"location":"nebula-dashboard/6.monitor-parameter/#storage","title":"Storage","text":"Parameter Description <code>add_edges_latency_us</code> The latency of adding edges. <code>add_vertices_latency_us</code> The latency of adding vertices. <code>commit_log_latency_us</code> The latency of committing logs in Raft. <code>commit_snapshot_latency_us</code> The latency of committing snapshots in Raft. <code>delete_edges_latency_us</code> The latency of deleting edges. <code>delete_vertices_latency_us</code> The latency of deleting vertices. <code>get_neighbors_latency_us</code> The latency of querying neighbor vertices. <code>get_dst_by_src_latency_us</code> The latency of querying the destination vertex by the source vertex. <code>num_get_prop</code> The number of executions for the GetPropProcessor. <code>num_get_neighbors_errors</code> The number of execution errors for the GetNeighborsProcessor. <code>num_get_dst_by_src_errors</code> The number of execution errors for the GetDstBySrcProcessor. <code>get_prop_latency_us</code> The latency of executions for the GetPropProcessor. <code>num_edges_deleted</code> The number of deleted edges. <code>num_edges_inserted</code> The number of inserted edges. <code>num_raft_votes</code> The number of votes in Raft. <code>num_rpc_sent_to_metad_failed</code> The number of failed RPC requests that the Storage service sent to the Meta service. <code>num_rpc_sent_to_metad</code> The number of RPC requests that the Storaged service sent to the Metad service. <code>num_tags_deleted</code> The number of deleted tags. <code>num_vertices_deleted</code> The number of deleted vertices. <code>num_vertices_inserted</code> The number of inserted vertices. <code>transfer_leader_latency_us</code> The latency of transferring the raft leader. <code>lookup_latency_us</code> The latency of executions for the LookupProcessor. <code>num_lookup_errors</code> The number of execution errors for the LookupProcessor. <code>num_scan_vertex</code> The number of executions for the ScanVertexProcessor. <code>num_scan_vertex_errors</code> The number of execution errors for the ScanVertexProcessor. <code>update_edge_latency_us</code> The latency of executions for the UpdateEdgeProcessor. <code>num_update_vertex</code> The number of executions for the UpdateVertexProcessor. <code>num_update_vertex_errors</code> The number of execution errors for the UpdateVertexProcessor. <code>kv_get_latency_us</code> The latency of executions for the Getprocessor. <code>kv_put_latency_us</code> The latency of executions for the PutProcessor. <code>kv_remove_latency_us</code> The latency of executions for the RemoveProcessor. <code>num_kv_get_errors</code> The number of execution errors for the GetProcessor. <code>num_kv_get</code> The number of executions for the GetProcessor. <code>num_kv_put_errors</code> The number of execution errors for the PutProcessor. <code>num_kv_put</code> The number of executions for the PutProcessor. <code>num_kv_remove_errors</code> The number of execution errors for the RemoveProcessor. <code>num_kv_remove</code> The number of executions for the RemoveProcessor. <code>forward_tranx_latency_us</code> The latency of transmission. <code>scan_edge_latency_us</code> The latency of executions for the ScanEdgeProcessor. <code>num_scan_edge_errors</code> The number of execution errors for the ScanEdgeProcessor. <code>num_scan_edge</code> The number of executions for the ScanEdgeProcessor. <code>scan_vertex_latency_us</code> The latency of executions for the ScanVertexProcessor. <code>num_add_edges</code> The number of times that edges are added. <code>num_add_edges_errors</code> The number of errors when adding edges. <code>num_add_vertices</code> The number of times that vertices are added. <code>num_start_elect</code> The number of times that Raft starts an election. <code>num_add_vertices_errors</code> The number of errors when adding vertices. <code>num_delete_vertices_errors</code> The number of errors when deleting vertices. <code>append_log_latency_us</code> The latency of replicating the log record to a single node by Raft. <code>num_grant_votes</code> The number of times that Raft votes for other nodes. <code>replicate_log_latency_us</code> The latency of replicating the log record to most nodes by Raft. <code>num_delete_tags</code> The number of times that tags are deleted. <code>num_delete_tags_errors</code> The number of errors when deleting tags. <code>num_delete_edges</code> The number of edge deletions. <code>num_delete_edges_errors</code> The number of errors when deleting edges <code>num_send_snapshot</code> The number of times that snapshots are sent. <code>update_vertex_latency_us</code> The latency of executions for the UpdateVertexProcessor. <code>append_wal_latency_us</code> The Raft write latency for a single WAL. <code>num_update_edge</code> The number of executions for the UpdateEdgeProcessor. <code>delete_tags_latency_us</code> The latency of deleting tags. <code>num_update_edge_errors</code> The number of execution errors for the UpdateEdgeProcessor. <code>num_get_neighbors</code> The number of executions for the GetNeighborsProcessor. <code>num_get_dst_by_src</code> The number of executions for the GetDstBySrcProcessor. <code>num_get_prop_errors</code> The number of execution errors for the GetPropProcessor. <code>num_delete_vertices</code> The number of times that vertices are deleted. <code>num_lookup</code> The number of executions for the LookupProcessor. <code>num_sync_data</code> The number of times the Storage service synchronizes data from the Drainer. <code>num_sync_data_errors</code> The number of errors that occur when the Storage service synchronizes data from the Drainer. <code>sync_data_latency_us</code> The latency of the Storage service synchronizing data from the Drainer."},{"location":"nebula-dashboard/6.monitor-parameter/#graph_space","title":"Graph space","text":"<p>Note</p> <p>Space-level metrics are created dynamically, so that only when the behavior is triggered in the graph space, the corresponding metric is created and can be queried by the user.</p> Parameter Description <code>num_active_queries</code> The number of queries currently being executed. <code>num_queries</code> The number of queries. <code>num_sentences</code> The number of statements received by the Graphd service. <code>optimizer_latency_us</code> The latency of executing optimizer statements. <code>query_latency_us</code> The latency of queries. <code>num_slow_queries</code> The number of slow queries. <code>num_query_errors</code> The number of query errors. <code>num_query_errors_leader_changes</code> The number of raft leader changes due to query errors. <code>num_killed_queries</code> The number of killed queries. <code>num_aggregate_executors</code> The number of executions for the Aggregation operator. <code>num_sort_executors</code> The number of executions for the Sort operator. <code>num_indexscan_executors</code> The number of executions for index scan operators. <code>num_auth_failed_sessions_bad_username_password</code> The number of sessions where authentication failed due to incorrect username and password. <code>num_auth_failed_sessions</code> The number of sessions in which login authentication failed. <code>num_opened_sessions</code> The number of sessions connected to the server. <code>num_queries_hit_memory_watermark</code> The number of queries reached the memory watermark. <code>num_reclaimed_expired_sessions</code> The number of expired sessions actively reclaimed by the server. <code>num_rpc_sent_to_metad_failed</code> The number of failed RPC requests that the Graphd service sent to the Metad service. <code>num_rpc_sent_to_metad</code> The number of RPC requests that the Graphd service sent to the Metad service. <code>num_rpc_sent_to_storaged_failed</code> The number of failed RPC requests that the Graphd service sent to the Storaged service. <code>num_rpc_sent_to_storaged</code> The number of RPC requests that the Graphd service sent to the Storaged service. <code>slow_query_latency_us</code> The latency of slow queries."},{"location":"nebula-dashboard/6.monitor-parameter/#single_process_metrics","title":"Single process metrics","text":"<p>Graph, Meta, and Storage services all have their own single process metrics.</p> Parameter Description <code>context_switches_total</code> The number of context switches. <code>cpu_seconds_total</code> The CPU usage based on user and system time. <code>memory_bytes_gauge</code> The number of bytes of memory used. <code>open_filedesc_gauge</code> The number of file descriptors. <code>read_bytes_total</code> The number of bytes read. <code>write_bytes_total</code> The number of bytes written."},{"location":"nebula-dashboard-ent/1.what-is-dashboard-ent/","title":"What is NebulaGraph Dashboard Enterprise Edition","text":"<p>NebulaGraph Dashboard Enterprise Edition (Dashboard for short) is a visualization tool that monitors and manages the status of machines and services in NebulaGraph clusters. This topic introduces Dashboard Enterprise Edition. For more information, see What is NebulaGraph Dashboard Community Edition.</p> <p>Note</p> <p>You can also try some functions online in Dashboard.</p> <p></p>"},{"location":"nebula-dashboard-ent/1.what-is-dashboard-ent/#features","title":"Features","text":"<ul> <li>Create a NebulaGraph cluster of a specified version, import nodes in batches, scale out NebulaGraph services with one click</li> </ul> <ul> <li>Import clusters, balance data, scale out or in on the visualization interface.</li> </ul> <ul> <li>Manage clusters, and view the operation log of clusters.</li> </ul> <ul> <li>Start, stop, and restart services on the visualization interface.</li> </ul> <ul> <li>Update the configuration of Storage services and Graph services in clusters quickly.</li> </ul> <ul> <li>Set how often the metrics page refreshes.</li> </ul> <ul> <li>Monitor the information of all the services in clusters, including the IP address, version, and monitoring metrics (such as the number of queries, the latency of queries, and the latency of heartbeats).</li> </ul> <ul> <li>Monitor the status of all the machines in clusters, including CPU, memory, load, disk, and network.</li> </ul> <ul> <li>Monitor the information of clusters, including the information of services, partitions, configurations, and long-term tasks.</li> </ul> <ul> <li>Set notifications based on the monitoring information.</li> </ul>"},{"location":"nebula-dashboard-ent/1.what-is-dashboard-ent/#scenarios","title":"Scenarios","text":"<ul> <li>You want a visualized operation and maintenance monitoring platform for large-scale clusters.</li> </ul> <ul> <li>You want to monitor key metrics conveniently and quickly, and present multiple key information of the business to ensure that the business can be operated normally.</li> </ul> <ul> <li>You want to monitor clusters from multiple dimensions (such as the time, aggregate rules, and metrics).</li> </ul> <ul> <li>You want to review the failure after it occurs, confirm when it happened, and view its associated phenomena.</li> </ul>"},{"location":"nebula-dashboard-ent/1.what-is-dashboard-ent/#precautions","title":"Precautions","text":"<ul> <li>The monitoring data will be retained for 14 days by default, that is, only the monitoring data within the last 14 days can be queried.</li> </ul> <ul> <li>The version of NebulaGraph must be 2.5.0 or later.</li> </ul> <ul> <li>It is recommend to use the latest version of Chrome to access Dashboard.</li> </ul> <ul> <li>It is recommend to use the official installation package to create or import clusters.</li> </ul> <p>Note</p> <p>The monitoring feature is supported by Prometheus. The update frequency and retention intervals can be modified. For details, see Prometheus.</p>"},{"location":"nebula-dashboard-ent/1.what-is-dashboard-ent/#version_compatibility","title":"Version compatibility","text":"<p>The version correspondence between NebulaGraph and Dashboard Enterprise Edition is as follows.</p> NebulaGraph version Dashboard version 3.5.0 3.5.0 3.4.0 ~ 3.4.1 3.5.0\u30013.4.2\u30013.4.1\u30013.4.0\u30013.2.4\u30013.2.3\u30013.2.2\u30013.2.1\u30013.2.0 3.3.0 3.2.4, 3.2.3, 3.2.2, 3.2.1, 3.2.0 2.5.0 ~ 3.2.0 3.1.2, 3.1.1, 3.1.0 2.5.x ~ 3.1.0 3.0.4 2.5.1 ~ 3.0.0 1.1.0 2.0.1 ~ 2.6.1 1.0.2 2.0.1 ~ 2.6.1 1.0.1 2.0.1 ~ 2.6.1 1.0.0"},{"location":"nebula-dashboard-ent/1.what-is-dashboard-ent/#video","title":"Video","text":"<ul> <li>NebulaGraph Dashboard (Enterprise Edition) Intro Demo(5 minutes 25 seconds)</li> </ul>"},{"location":"nebula-dashboard-ent/10.tasks/","title":"Task Center","text":"<p>NebulaGraph Dashboard Enterprise Edition allows you to view the progress of running tasks and the information of ended tasks.</p>"},{"location":"nebula-dashboard-ent/10.tasks/#precautions","title":"Precautions","text":"<ul> <li>The running tasks can not be canceled.</li> <li>The historical tasks cannot be deleted.</li> </ul>"},{"location":"nebula-dashboard-ent/10.tasks/#task_types","title":"Task types","text":"<p>Currently the task center supports the following types of tasks:</p> <ul> <li>install: Create clusters.</li> <li>scale: Scale clusters.</li> <li>version update: Update NebulaGraph versions.</li> <li>package upload: Upload NebulaGraph installation packages.</li> <li>package download: Download NebulaGraph installation packages.</li> <li>package deploy: Deploy the installation package when adding a node.</li> </ul>"},{"location":"nebula-dashboard-ent/10.tasks/#entry","title":"Entry","text":"<p>At the top navigation bar of the Dashboard Enterprise Edition page, click Task Center to view task information.</p>"},{"location":"nebula-dashboard-ent/10.tasks/#running_tasks","title":"Running tasks","text":"<p>Click the tab Running Task to view the progress of the running tasks.</p> <ul> <li>Click a task name to view the ID, node name, type, create time, and operator of the running task. </li> <li>Clink Task information to view task details.</li> </ul>"},{"location":"nebula-dashboard-ent/10.tasks/#task_history","title":"Task history","text":"<p>Click Task History to view all ended tasks.</p> <ul> <li>You can filter historical tasks by status, type, date, and time.</li> <li>On the right side of the target historical task, click Task information to view task details, and click Logs to view task execution logs.</li> </ul>"},{"location":"nebula-dashboard-ent/11.license-manager/","title":"License Manager","text":"<p>After configuring the license manager (LM) in NebulaGraph and its affiliated software, you can use LM to verify the validity of the licenses and ensure the proper use of the graph database and affiliated software.</p> <p>Dashboard Enterprise Edition currently supports direct deployment of License Manager (LM) and provides pages to display License Info, License Usage, and Product List.</p>"},{"location":"nebula-dashboard-ent/11.license-manager/#prerequisites","title":"Prerequisites","text":"<p>Please ensure that Dashboard has been activated. For details, please refer to Activating Dashboard.</p>"},{"location":"nebula-dashboard-ent/11.license-manager/#entry","title":"Entry","text":"<p>At the top navigation bar of the Dashboard Enterprise Edition page, click License Manager to view the information.</p>"},{"location":"nebula-dashboard-ent/11.license-manager/#license_information","title":"License information","text":"<p>On the License Manager page, you can view:</p> <p></p> <ul> <li>The running status of LM. When LM is running normally, <code>Running</code> is displayed at the top of the page; when not, <code>Exited</code> is displayed.</li> <li>Relevant information about the license, including the status of the license, the validity period of the license, the ID of LM, and the access link of LM.</li> <li>The usage of the license, including the type of quota, the total number of quotas, and the remaining quotas.</li> <li>The list of products using the quota, including the product name, product status, type of quota used by the product, and the number of quotas.</li> </ul>"},{"location":"nebula-dashboard-ent/2.deploy-connect-dashboard-ent/","title":"Deploy Dashboard Enterprise Edition","text":"<p>This topic will introduce how to deploy Dashboard Enterprise Edition in detail.</p>"},{"location":"nebula-dashboard-ent/2.deploy-connect-dashboard-ent/#prerequisites","title":"Prerequisites","text":"<p>Before deploying Dashboard Enterprise Edition, you must do a check of these:</p> <ul> <li>Select and download Dashboard Enterprise Edition of the correct version. For information about the version correspondence between Dashboard Enterprise Edition and NebulaGraph, see Version compatibility.</li> </ul> <ul> <li> <p>MySQL and SQLite are supported to store Dashboard metadata. To use MySQL, make sure that the environment of MySQL is ready and a MySQL database named as <code>dashboard</code> is create. Make sure the default character set of the database is <code>utf8</code>.</p> <p>Note</p> <p>To store Dashboard metadata using SQLite, there is no need to configure the SQLite environment because SQLite is built-in to Dashboard.  </p> </li> </ul> <ul> <li> <p>Before the installation starts, the following ports are not occupied.</p> Port Description 7005 The port through which Dashboard Enterprise Edition provides the web service. 9091 The port of the prometheus service. 9200 The port of the nebula-stats-exporter service. 9093 The port of the Alertmanager service, used to receive Prometheus alerts and then send them to Dashboard. 9100 The port of the node-exporter service. The node-exporter is automatically deployed on the target machine after a cluster is created or imported. It is used to collect the source information of machines in the cluster, including the CPU, memory, load, disk, and network. </li> </ul> <p>Enterpriseonly</p> <p>You can apply online for Dashboard Enterprise Edition free trial. NebulaGraph Dashboard Enterprise Edition is available exclusively through our Enterprise Edition package and is not sold separately. Contact us for details.</p>"},{"location":"nebula-dashboard-ent/2.deploy-connect-dashboard-ent/#deploy_dashboard_enterprise_edition_with_tar","title":"Deploy Dashboard Enterprise Edition with TAR","text":""},{"location":"nebula-dashboard-ent/2.deploy-connect-dashboard-ent/#installation","title":"Installation","text":"<ol> <li> <p>Select and download the TAR package according to your needs. It is recommended to select the latest version.</p> </li> <li> <p>Use <code>tar -xzvf</code> to decompress the TAR package.</p> <pre><code>tar -xzvf nebula-dashboard-ent-&lt;version&gt;.linux-amd64.tar.gz -C &lt;install_path&gt;\n</code></pre> <p>For example:</p> <pre><code>tar -xzvf nebula-dashboard-ent-3.5.0.linux-amd64.tar.gz -C /usr/local/\n</code></pre> </li> <li> <p>Enter the extracted folder and modify the <code>config.yaml</code> file in the <code>etc</code> directory to set the <code>LicenseManagerURL</code> parameter to the host IP and port number <code>9119</code> where the License Manager (LM) is located and to set the relevant configuration.</p> <pre><code>Name: dashboard-api\nHost: 0.0.0.0     # Specifies the address segment that can access Dashboard.\nPort: 7005  # The default port used to access Dashboard Enterprise Edition.  \nMaxBytes: 1073741824 # The maximum content length of an Http request that can be accepted. The default value is 1048576. Value range: 0 ~ 8388608.\nTimeout: 60000  # Timeout duration of the access.\nLog:     # Dashboard run log settings.\n  KeepDays: 7    # The number of days for keeping log.\n  Mode: file   # The save mode of logs, including console and file. console means the service logs are logged in webserver.log. file means the service logs are logged in access.log, error.log, severe.log, slow.log, and stat.log respectively.\n  Encoding: plain # Log encoding method, plain and json are supported.\nDatabase:\n  Dialect: sqlite  # The database type used to store metadata. Only support SQLite and MySQL currently. The default value is SQLite.\n  AutoMigrate: true # Whether to automatically create a database table. Defaults to true.\n  Host: 127.0.0.1 # The IP address of the connected MySQL database.\n  Port: 3306  # The port of the connected MySQL database.\n  Username: root # The username to log in MySQL.\n  Password: nebula # The password to log in MySQL.\n  Name: dashboard # The name of the corresponding database.\n\n# Information about the exporter port\nExporter:\n  NodePort: 9100 # The port of the node-exporter service.\n  NebulaPort: 9200 # The port of the nebula-stats-exporter service.\n\n# Information of services \nProxy:\n  PrometheusAddr: 127.0.0.1:9091 # The IP address and port of the prometheus service.\n  AlertmanagerAddr: 127.0.0.1:9093 # The IP address and port of the Alertmanager service.\n\n# Information of the sender's Email used to invite LDAP accounts.\nMail:\n  Host: smtp.office365.com #  The SMTP server address.\n  Port: 587 # The port number of the SMTP server.\n  Username: \"\" # The SMTP server account name.\n  Password: \"\" # The SMTP server password.\n\n# SlowQuery\nSlowQuery:\n  Enable: true # The switch of slowquery data polling.\n  MessageStore: 14 # Slow queary data store time (day).\n  ScrapeInterval: 2m # The interval time for slow query data pulling, eg: 1s, 10s, 2m, 3h\n\n# System information\nSystem:\n  WebAddress: http://127.0.0.1:7005 # The external access for Dashboard. It can be set as a hostname, used for interface callbacks. For example, the invitee who is invited by mail can use this link to access Dashboard.\n  MessageStore: 90 # It sets the number of days to keep alert messages, the value of which is 90 by default.\n\nCloudProvider: \"\" # cloud provider name, used for aliyun now.\nLicenseManagerURL: \"\" # license manager url.\n</code></pre> </li> <li> <p>Start Dashboard.</p> <p>You can use the following command to start the Dashboard with one click.</p> <pre><code>cd /usr/local/nebula-dashboard-ent/scripts\nsudo ./dashboard.service start all\n</code></pre> <p>Or execute the following commands to start prometheus, webserver, nebula-stats-exporter and alertmanager services to start Dashboard.</p> <pre><code>cd scripts\nsudo ./dashboard.service start prometheus # Start prometheus service\nsudo ./dashboard.service start webserver # Start webserver service\nsudo ./dashboard.service start exporter # Start nebula-stats-exporter service\nsudo ./dashboard.service start alertmanager # Start alertmanager service\n</code></pre> </li> </ol> <p>Note</p> <p>If you change the configuration file after starting Dashboard, you can run <code>dashboard.service restart all</code> in the <code>scripts</code> directory to synchronize the changes to the Dashboard client page.</p>"},{"location":"nebula-dashboard-ent/2.deploy-connect-dashboard-ent/#deploy_dashboard_enterprise_edition_with_rpm","title":"Deploy Dashboard Enterprise Edition with RPM","text":""},{"location":"nebula-dashboard-ent/2.deploy-connect-dashboard-ent/#prerequisites_1","title":"Prerequisites","text":"<p>The Linux version used is CentOS and lsof has been installed.</p>"},{"location":"nebula-dashboard-ent/2.deploy-connect-dashboard-ent/#installation_1","title":"Installation","text":"<ol> <li> <p>Download an RPM package.</p> </li> <li> <p>Run <code>sudo rpm -i &lt;rpm&gt;</code> to install the RPM package.</p> <p>For example, run the following command to install Dashboard Enterprise Edition. Installation path is <code>/usr/local/nebula-dashboard-ent</code> by default.</p> <pre><code>sudo rpm -i nebula-dashboard-ent-&lt;version&gt;.x86_64.rpm\n</code></pre> <p>You can also run the following command to specify the installation path.</p> <pre><code>sudo rpm -i nebula-dashboard-ent-xxx.rpm --prefix=&lt;path&gt; \n</code></pre> </li> <li> <p>Enter the extracted folder and modify the <code>config.yaml</code> file in the <code>etc</code> directory to set the <code>LicenseManagerURL</code> parameter to the host IP and port number <code>9119</code> where the License Manager (LM) is located and to set the relevant configuration.</p> <pre><code>Name: dashboard-api\nHost: 0.0.0.0     # Specifies the address segment that can access Dashboard.\nPort: 7005  # The default port used to access Dashboard Enterprise Edition.  \nMaxBytes: 1073741824 # The maximum content length of an Http request that can be accepted. The default value is 1048576. Value range: 0 ~ 8388608.\nTimeout: 60000  # Timeout duration of the access.\nLog:     # Dashboard run log settings.\n  KeepDays: 7    # The number of days for keeping log.\n  Mode: file   # The save mode of logs, including console and file. console means the service logs are logged in webserver.log. file means the service logs are logged in access.log, error.log, severe.log, slow.log, and stat.log respectively.\n  Encoding: plain # Log encoding method, plain and json are supported.\nDatabase:\n  Dialect: sqlite  # The database type used to store metadata. Only support SQLite and MySQL currently. The default value is SQLite.\n  AutoMigrate: true # Whether to automatically create a database table. Defaults to true.\n  Host: 127.0.0.1 # The IP address of the connected MySQL database.\n  Port: 3306  # The port of the connected MySQL database.\n  Username: root # The username to log in MySQL.\n  Password: nebula # The password to log in MySQL.\n  Name: dashboard # The name of the corresponding database.\n\n# Information about the exporter port\nExporter:\n  NodePort: 9100 # The port of the node-exporter service.\n  NebulaPort: 9200 # The port of the nebula-stats-exporter service.\n\n# Information of services \nProxy:\n  PrometheusAddr: 127.0.0.1:9091 # The IP address and port of the prometheus service.\n  AlertmanagerAddr: 127.0.0.1:9093 # The IP address and port of the Alertmanager service.\n\n# Information of the sender's Email used to invite LDAP accounts.\nMail:\n  Host: smtp.office365.com #  The SMTP server address.\n  Port: 587 # The port number of the SMTP server.\n  Username: \"\" # The SMTP server account name.\n  Password: \"\" # The SMTP server password.\n\n# SlowQuery\nSlowQuery:\n  Enable: true # The switch of slowquery data polling.\n  MessageStore: 14 # Slow queary data store time (day).\n  ScrapeInterval: 2m # The interval time for slow query data pulling, eg: 1s, 10s, 2m, 3h\n\n# System information\nSystem:\n  WebAddress: http://127.0.0.1:7005 # The external access for Dashboard. It can be set as a hostname, used for interface callbacks. For example, the invitee who is invited by mail can use this link to access Dashboard.\n  MessageStore: 90 # It sets the number of days to keep alert messages, the value of which is 90 by default.\n\nCloudProvider: \"\" # cloud provider name, used for aliyun now.\nLicenseManagerURL: \"\" # license manager url.\n</code></pre> </li> <li> <p>Run the following commands to view the status of and start all the services.</p> <pre><code>sudo systemctl list-dependencies nebula-dashboard.target  # View the status of all the services.\nsudo systemctl\u00a0start\u00a0nebula-dashboard.target   # Start all the services.\n</code></pre> <p>You can also view, start, and stop a single service.</p> <pre><code>sudo systemctl\u00a0{status|stop|start}\u00a0{nbd-prometheus.service|nbd-alert-manager.service|nbd-stats-exporter.service|nbd-webserver.service}\n</code></pre> </li> </ol>"},{"location":"nebula-dashboard-ent/2.deploy-connect-dashboard-ent/#uninstallation","title":"Uninstallation","text":"<p>To uninstall Dashboard Enterprise Edition deployed with RPM, run the following command.</p> <pre><code>sudo rpm -e &lt;package_name&gt;\n</code></pre>"},{"location":"nebula-dashboard-ent/2.deploy-connect-dashboard-ent/#deploy_dashboard_enterprise_edition_with_deb","title":"Deploy Dashboard Enterprise Edition with DEB","text":""},{"location":"nebula-dashboard-ent/2.deploy-connect-dashboard-ent/#installation_2","title":"Installation","text":"<ol> <li> <p>Download a DEB package.</p> </li> <li> <p>Install the package.</p> <pre><code>sudo dpkg -i &lt;package_name&gt;\n</code></pre> <p>Note</p> <p>Custom installation paths are not supported when installing Dashboard Enterprise Edition with DEB. The default installation path is <code>/usr/local/nebula-dashboard-ent/</code>.</p> <p>For example, to install the DEB package of the 3.5.0 version:</p> <pre><code>sudo dpkg -i nebula-dashboard-ent-3.5.0.ubuntu1804.amd64.deb\n</code></pre> </li> <li> <p>Enter the extracted folder and modify the <code>config.yaml</code> file in the <code>etc</code> directory to set the <code>LicenseManagerURL</code> parameter to the host IP and port number <code>9119</code> where the License Manager (LM) is located and to set the relevant configuration.</p> <pre><code>Name: dashboard-api\nHost: 0.0.0.0     # Specifies the address segment that can access Dashboard.\nPort: 7005  # The default port used to access Dashboard Enterprise Edition.  \nMaxBytes: 1073741824 # The maximum content length of an Http request that can be accepted. The default value is 1048576. Value range: 0 ~ 8388608.\nTimeout: 60000  # Timeout duration of the access.\nLog:     # Dashboard run log settings.\n  KeepDays: 7    # The number of days for keeping log.\n  Mode: file   # The save mode of logs, including console and file. console means the service logs are logged in webserver.log. file means the service logs are logged in access.log, error.log, severe.log, slow.log, and stat.log respectively.\n  Encoding: plain # Log encoding method, plain and json are supported.\nDatabase:\n  Dialect: sqlite  # The database type used to store metadata. Only support SQLite and MySQL currently. The default value is SQLite.\n  AutoMigrate: true # Whether to automatically create a database table. Defaults to true.\n  Host: 127.0.0.1 # The IP address of the connected MySQL database.\n  Port: 3306  # The port of the connected MySQL database.\n  Username: root # The username to log in MySQL.\n  Password: nebula # The password to log in MySQL.\n  Name: dashboard # The name of the corresponding database.\n\n# Information about the exporter port\nExporter:\n  NodePort: 9100 # The port of the node-exporter service.\n  NebulaPort: 9200 # The port of the nebula-stats-exporter service.\n\n# Information of services \nProxy:\n  PrometheusAddr: 127.0.0.1:9091 # The IP address and port of the prometheus service.\n  AlertmanagerAddr: 127.0.0.1:9093 # The IP address and port of the Alertmanager service.\n\n# Information of the sender's Email used to invite LDAP accounts.\nMail:\n  Host: smtp.office365.com #  The SMTP server address.\n  Port: 587 # The port number of the SMTP server.\n  Username: \"\" # The SMTP server account name.\n  Password: \"\" # The SMTP server password.\n\n# SlowQuery\nSlowQuery:\n  Enable: true # The switch of slowquery data polling.\n  MessageStore: 14 # Slow queary data store time (day).\n  ScrapeInterval: 2m # The interval time for slow query data pulling, eg: 1s, 10s, 2m, 3h\n\n# System information\nSystem:\n  WebAddress: http://127.0.0.1:7005 # The external access for Dashboard. It can be set as a hostname, used for interface callbacks. For example, the invitee who is invited by mail can use this link to access Dashboard.\n  MessageStore: 90 # It sets the number of days to keep alert messages, the value of which is 90 by default.\n\nCloudProvider: \"\" # cloud provider name, used for aliyun now.\nLicenseManagerURL: \"\" # license manager url.\n</code></pre> </li> <li> <p>Run the following commands to view the status of and start all the services.</p> <pre><code>sudo systemctl list-dependencies nebula-dashboard.target  # View the status of all the services.\nsudo systemctl\u00a0start\u00a0nebula-dashboard.target   # Start all the services.\n</code></pre> <p>You can also view, start, and stop a single service.</p> <pre><code>sudo systemctl\u00a0{status|stop|start}\u00a0{nbd-prometheus.service|nbd-alert-manager.service|nbd-stats-exporter.service|nbd-webserver.service}\n</code></pre> </li> </ol>"},{"location":"nebula-dashboard-ent/2.deploy-connect-dashboard-ent/#uninstallation_1","title":"Uninstallation","text":"<p>To uninstall Dashboard Enterprise Edition, run the following command.</p> <pre><code>sudo dpkg -r &lt;package_name&gt;\n</code></pre>"},{"location":"nebula-dashboard-ent/2.deploy-connect-dashboard-ent/#manage_services_in_dashboard","title":"Manage services in Dashboard","text":"<p>The following section presents two methods for managing the Dashboard service. It's important to note that these methods are not interchangeable. For instance, you can't start the service using the <code>dashboard.service</code> script and then use the <code>systemctl</code> command to stop it.</p> <ul> <li> <p>You can use the <code>dashboard.service</code> script to start, restart, stop, and check the Dashboard services.</p> <pre><code>sudo &lt;dashboard_path&gt;/scripts/dashboard.service\n[-v] [-h] [-version]\n&lt;start|restart|stop|status&gt;  &lt;prometheus|webserver|exporter|gateway|all&gt;\n</code></pre> Parameter Description <code>dashboard_path</code> Dashboard installation path. <code>-v</code> Display detailed debugging information. <code>-h</code> Display help information. <code>-version</code> Display the Dashboard version. <code>start</code> Start the target services. <code>restart</code> Restart the target services. <code>stop</code> Stop the target services. <code>status</code> Check the status of the target services. <code>prometheus</code> Set the prometheus Service as the target service. <code>webserver</code> Set the webserver Service as the target service. <code>exporter</code> Set the exporter Service as the target service. <code>gateway</code> Set the gateway Service as the target service. <code>all</code> Set all the Dashboard services as the target services. <p>For example, if Dashboard is installed in the current directory, you can manage its services using the following commands:</p> <pre><code>sudo /dashboard/scripts/dashboard.service start all # Start Dashboard. \nsudo /dashboard/scripts/dashboard.service stop all # Stop Dashboard. \nsudo /dashboard/scripts/dashboard.service status all # Check Dashboard status.\nsudo /dashboard/scripts/dashboard.service restart all # Restart Dashboard.\n</code></pre> </li> </ul> <ul> <li> <p>If you installed Dashboard using RPM or DEB packages, you can manage the service using systemd. You can start, view, restart and stop the service using the <code>systemctl</code> command.</p> <pre><code>sudo systemctl start nebula-dashboard.target # Start all service.\nsudo systemctl status nebula-dashboard.target # Check Dashboard status.\nsudo systemctl restart &lt;nbd-prometheus.service|nbd-alert-manager.service|nbd-stats-exporter.service|nbd-webserver.service&gt; # Restart service respectively.\nsudo systemctl stop &lt;nbd-prometheus.service|nbd-alert-manager.service|nbd-stats-exporter.service|nbd-webserver.service&gt; # Stop service respectively.\n</code></pre> <p>For example, to stop the Prometheus service, run the following command:</p> <pre><code>sudo systemctl stop nbd-prometheus.service\n</code></pre> </li> </ul>"},{"location":"nebula-dashboard-ent/2.deploy-connect-dashboard-ent/#view_logs","title":"View logs","text":"<ul> <li> <p>Users who manage services using <code>dashboard.service</code> can view the Dashboard Enterprise Edition logs in the <code>logs</code> directory.</p> <p>For example:</p> <pre><code>cat logs/prometheus.log\n</code></pre> <p>The descriptions of the log files are as follows.</p> Log file Description <code>alertmanager.log</code> Alertmanager service log. <code>nebula-stats-exporter.log</code> nebula-stats-exporter service log. <code>prometheus.log</code> Prometheus service log. <code>br</code> Backup and restore service log. <code>webserver.log</code> Dashboard service log. It takes effect only when the <code>Log.Mode</code> in the Dashboard configuration is <code>console</code>. <code>access.log</code> Access log. It takes effect only when the <code>Log.Mode</code> in the Dashboard configuration is <code>file</code>. <code>error.log</code> Error log. It takes effect only when the <code>Log.Mode</code> in the Dashboard configuration is <code>file</code>. <code>severe.log</code> Severe log. It takes effect only when the <code>Log.Mode</code> in the Dashboard configuration is <code>file</code>. <code>slow.log</code> Slow log. It takes effect only when the <code>Log.Mode</code> in the Dashboard configuration is <code>file</code>. <code>stat.log</code> Statistic log. It takes effect only when the <code>Log.Mode</code> in the Dashboard configuration is <code>file</code>. </li> </ul> <ul> <li> <p>Users managing services with systemd can access the logs for each Dashboard Enterprise Edition service through <code>journalctl</code>.</p> <pre><code>journalctl -u {nbd-prometheus.service|nbd-alert-manager.service|nbd-stats-exporter.service|nbd-webserver.service} -b\n</code></pre> <p>For example, to view the logs of the Prometheus service:</p> <pre><code>journalctl -u nbd-prometheus.service -b\n</code></pre> </li> </ul>"},{"location":"nebula-dashboard-ent/2.deploy-connect-dashboard-ent/#next_to_do","title":"Next to do","text":"<p>Connect to Dashboard</p>"},{"location":"nebula-dashboard-ent/3.connect-dashboard/","title":"Connect to Dashboard","text":"<p>After Dashboard is deployed, you can log in and use Dashboard on the browser.</p>"},{"location":"nebula-dashboard-ent/3.connect-dashboard/#prerequisites","title":"Prerequisites","text":"<ul> <li>The Dashboard services are started. For more information, see Deploy Dashboard.</li> </ul> <ul> <li>We recommend you use the Chrome browser of the version above 89. Otherwise, there may be compatibility issues.</li> </ul>"},{"location":"nebula-dashboard-ent/3.connect-dashboard/#connect_to_dashboard_1","title":"Connect to Dashboard","text":"<ol> <li> <p>Confirm the IP address of the machine where the Dashboard is installed. Enter <code>http://&lt;ip_address&gt;:7005</code> in the browser to open the login page.</p> <p>If the following login interface is shown in the browser, then you have successfully deployed and started Dashboard.</p> <p></p> <p>Note</p> <p>When logging into the NebulaGraph Dashboard Enterprise Edition for the first time, the content of END USER LICENSE AGREEMENT is displayed on the login page. Please read it and then click I Agree. </p> </li> <li> <p>Log into Dashboard with the default account name <code>nebula</code> and password <code>nebula</code>.</p> <p>Note</p> <p>You can create LDAP, OAuth2.0, or general accounts after logging into Dashboard. For more information about the Dashboard account, see Authority Management.</p> </li> </ol>"},{"location":"nebula-dashboard-ent/3.connect-dashboard/#activate_dashboard","title":"Activate Dashboard","text":"<p>Before using Dashboard, it is necessary to configure License Manager (LM) to validate the effectiveness of the license and activate Dashboard.</p> <p>Users can configure LM in two ways to use Dashboard:</p> <ul> <li> <p>Configure LM directly through Dashboard:</p> <p>The dashboard currently supports visual one-click deployment of LM. After logging into Dashboard, follow the steps below to complete the deployment of LM and start using Dashboard.</p> <ol> <li> <p>Click Start on the LM Introduction page: This page introduces the relevant concepts and overall process of LM, including installation of LM, generation of License Key, and running LM. Click Start to continue.</p> </li> <li> <p>Fill in the configuration information and install LM: You need to fill in the host IP, SSH-related information, installation path, License Manager port, etc. After filling in the information, click Install LM to install it.</p> </li> <li> <p>View LMID and generate License Key: After successfully installing LM, LMID will be generated. Users can copy LMID and bind the LMID to generate a license key on LC. Users need to fill in the License Key in the Dashboard interface and click Confirm to complete the configuration of the License.</p> </li> </ol> </li> </ul> <p>Note</p> <p>If the user already has a License Key, then can click on the Configuration LM URL in the top right corner, enter the LM URL directly, and then click Confirm to complete the configuration of the License.</p> <ul> <li> <p>Configure LM through the installation package:</p> <ol> <li> <p>Install and start LM, and load the License Key in LM. For details, please refer to License Manager.</p> </li> <li> <p>In the installation path of Dashboard, go to the <code>etc</code> directory, and modify the <code>config.yaml</code> file. Set the value of <code>LicenseManagerURL</code> to the host IP and port number 9119 where LM is located, for example, <code>192.168.8.xxx:9119</code>. Users can also log into Dashboard and set the value of <code>LicenseManagerURL</code> in the License Manager Installation Guide page under Configuration LM URL.</p> </li> </ol> </li> </ul>"},{"location":"nebula-dashboard-ent/5.account-management/","title":"Authority management","text":"<p>You can log into NebulaGraph Dashboard Enterprise Edition with different types of accounts. Different accounts have different permissions. This article introduces account types, roles, and permissions.</p> <p>Note</p> <p>You need to configure the related protocols before using LDAP accounts or OAuth2.0 accounts. For details, see Single sign-on.</p>"},{"location":"nebula-dashboard-ent/5.account-management/#account_types","title":"Account types","text":"<p>Once you log into Dashboard Enterprise Edition using the initialized account name <code>nebula</code> and password <code>nebula</code>, you can create different types of accounts: LDAP accounts, OAuth2.0 accounts and general accounts.</p>"},{"location":"nebula-dashboard-ent/5.account-management/#ldap_accounts","title":"LDAP accounts","text":"<p>Dashboard Enterprise Edition enables you to log into it with your enterprise account by accessing LDAP (Lightweight Directory Access Protocol).</p>"},{"location":"nebula-dashboard-ent/5.account-management/#oauth20_accounts","title":"OAuth2.0 accounts","text":"<p>Caution</p> <p>The feature is still in beta. It will continue to be optimized.</p> <p>Dashboard Enterprise Edition enables you to use access_token to authorize the third-party applications to access the protected information based on OAuth2.0.</p>"},{"location":"nebula-dashboard-ent/5.account-management/#general_accounts","title":"General accounts","text":"<p>Dashboard Enterprise Edition enables you to create local accounts.</p>"},{"location":"nebula-dashboard-ent/5.account-management/#account_roles","title":"Account roles","text":"<p>You can set different roles for your accounts. Roles are different in permissions. There are two types of account roles in Dashboard Enterprise Edition: system roles (<code>admin</code> and <code>user</code>) and cluster roles (<code>owner</code> and <code>operator</code>).</p> <p>The relationship between system roles and cluster roles and their descriptions are as follows.</p> <p></p> <p>System roles:</p> Roles Permission Description admin 1. Create accounts.2. Modify the role of an existing account.3. Perform platform settings, system-level alert settings.4. Delete accounts. 1. There can be multiple <code>admin</code> roles, i.e. system administrators. 2. An <code>admin</code> is the <code>operator</code> of all clusters by default, i.e. an <code>admin</code> can manage all clusters. 3. An <code>admin</code> can assign a <code>user</code> to be the <code>operator</code> of a cluster.4. Displayed in the cluster member list by default. An <code>owner</code> cannot remove an <code>admin</code> unless the <code>admin</code> is converted to <code>user</code>, and the system will automatically remove the <code>admin</code> from the cluster member list. user 1. Has read-only permissions for the system dimension. 2. After an <code>admin</code> creates a new account with the <code>user</code> role, the <code>user</code> account cannot view any clusters if the corresponding cluster is not assigned to the account.  3. Can create clusters and become the <code>owner</code> of the clusters. 1. General role. 2. There can be multiple <code>user</code> roles. <p>Cluster roles: </p> Roles Permission Description <code>operator</code> 1. Scale clusters. 2. Set cluster alerts. 3. Manage cluster nodes.4. Manage cluster services. 1. The cluster operator. 2. There can be multiple <code>operator</code> roles in a cluster. <code>owner</code> 1. Have all the permissions of <code>operator</code>. 2. Unbind and delete clusters.3. Add and remove accounts with <code>operator</code> roles. 4. Transfer the <code>owner</code> role. 1. The cluster owner. 2. There can only be one <code>owner</code> in a cluster."},{"location":"nebula-dashboard-ent/5.account-management/#create_accounts","title":"Create accounts","text":"<p>Accounts with <code>admin</code> roles can create other accounts. The steps are as follows:</p> <ol> <li>At the top navigation bar of the Dashboard Enterprise Edition page, click Authority, and click Create.</li> <li> <p>Select one method and input information to create an account, and click OK.</p> <ul> <li>Invite (LDAP or OAuth2.0 accounts): Set the invitee's account type, enterprise email and role. After the invitee clicks the Accept button in the email to activate the account, the invitee needs to click Login to automatically jump to the Dashboard Enterprise Edition login page. The invitee can log into Dashboard with his/her enterprise email account and password.</li> </ul> <p>Note</p> <p>Automatic registration is also supported after LDAP is enabled. When you enter an unregistered account in LDAP mode on the login page, the Dashboard automatically registers the account, but the role permission is <code>user</code>.</p> <ul> <li>Create Account (general accounts): Set the login name, password, and role for the new account. For information about roles, see the above content.</li> </ul> </li> </ol>"},{"location":"nebula-dashboard-ent/5.account-management/#view_accounts","title":"View accounts","text":"<p>The created accounts are displayed on the Authority page.</p> <ul> <li> <p>You can view the username, account type, role, associated cluster, and create time of accounts.</p> <ul> <li>Account Type: Includes ldap, oauth2.0 and platform. platform is a general account.</li> <li>Role: Displays the role of an account, including admin and user. For more information about roles, see the above content.</li> <li>Associated Clusters: Displays all the clusters that can be operated by an account. If the cluster was created by the account, the associated cluster has the <code>owner</code> tag.</li> </ul> </li> </ul> <ul> <li>You can search for accounts in the search box, and filter accounts by selecting an associated cluster.</li> </ul>"},{"location":"nebula-dashboard-ent/5.account-management/#other_operations","title":"Other operations","text":"<ul> <li>In the Action column on the Authority page, click  to edit account information.</li> </ul> <ul> <li>In the Action column on the Authority page, click  to delete an account.</li> </ul>"},{"location":"nebula-dashboard-ent/7.monitor-parameter/","title":"Metrics","text":"<p>This topic will describe the monitoring metrics in NebulaGraph Dashboard.</p>"},{"location":"nebula-dashboard-ent/7.monitor-parameter/#machine","title":"Machine","text":"<p>Note</p> <ul> <li>All the machine metrics listed below are for the Linux operating system.</li> <li>The default unit for Disk and Network is byte. The unit changes with the data magnitude as the page displays. For example, when the flow is less than 1 KB/s, the unit is Bytes/s.</li> <li>For all versions of Dashboard Enterprise Edition, the memory occupied by Buff and Cache will not be counted in the memory usage.</li> </ul>"},{"location":"nebula-dashboard-ent/7.monitor-parameter/#cpu","title":"CPU","text":"Parameter Description <code>cpu_utilization</code> The percentage of used CPU. <code>cpu_idle</code> The percentage of idled CPU. <code>cpu_wait</code> The percentage of CPU waiting for IO operations. <code>cpu_user</code> The percentage of CPU used by users. <code>cpu_system</code> The percentage of CPU used by the system."},{"location":"nebula-dashboard-ent/7.monitor-parameter/#memory","title":"Memory","text":"Parameter Description <code>memory_utilization</code> The percentage of used memory. <code>memory_used</code> The memory space used (not including caches). <code>memory_free</code> The memory space available."},{"location":"nebula-dashboard-ent/7.monitor-parameter/#load","title":"Load","text":"Parameter Description <code>load_1m</code> The average load of the system in the last 1 minute. <code>load_5m</code> The average load of the system in the last 5 minutes. <code>load_15m</code> The average load of the system in the last 15 minutes."},{"location":"nebula-dashboard-ent/7.monitor-parameter/#disk","title":"Disk","text":"Parameter Description <code>disk_used_percentage</code> The disk utilization percentage. <code>disk_used</code> The disk space used. <code>disk_free</code> The disk space available. <code>disk_readbytes</code> The number of bytes that the system reads in the disk per second. <code>disk_writebytes</code> The number of bytes that the system writes in the disk per second. <code>disk_readiops</code> The number of read queries that the disk receives per second. <code>disk_writeiops</code> The number of write queries that the disk receives per second. <code>inode_utilization</code> The percentage of used inode."},{"location":"nebula-dashboard-ent/7.monitor-parameter/#network","title":"Network","text":"Parameter Description <code>network_in_rate</code> The number of bytes that the network card receives per second. <code>network_out_rate</code> The number of bytes that the network card sends out per second. <code>network_in_errs</code> The number of wrong bytes that the network card receives per second. <code>network_out_errs</code> The number of wrong bytes that the network card sends out per second. <code>network_in_packets</code> The number of data packages that the network card receives per second. <code>network_out_packets</code> The number of data packages that the network card sends out per second."},{"location":"nebula-dashboard-ent/7.monitor-parameter/#service","title":"Service","text":""},{"location":"nebula-dashboard-ent/7.monitor-parameter/#period","title":"Period","text":"<p>The period is the time range of counting metrics. It currently supports 5 seconds, 60 seconds, 600 seconds, and 3600 seconds, which respectively represent the last 5 seconds, the last 1 minute, the last 10 minutes, and the last 1 hour.</p>"},{"location":"nebula-dashboard-ent/7.monitor-parameter/#metric_methods","title":"Metric methods","text":"Parameter Description <code>rate</code> The average rate of operations per second in a period. <code>sum</code> The sum of operations in the period. <code>avg</code> The average latency in the cycle. <code>P75</code> The 75th percentile latency. <code>P95</code> The 95th percentile latency. <code>P99</code> The 99th percentile latency. <code>P999</code> The 99.9th percentile latency. <p>Note</p> <p>Dashboard collects the following metrics from the NebulaGraph core, but only shows the metrics that are important to it. </p>"},{"location":"nebula-dashboard-ent/7.monitor-parameter/#graph","title":"Graph","text":"Parameter Description <code>num_active_queries</code> The number of changes in the number of active queries. Formula: The number of started queries minus the number of finished queries within a specified time. <code>num_active_sessions</code> The number of changes in the number of active sessions. Formula: The number of logged in sessions minus the number of logged out sessions within a specified time.For example, when querying <code>num_active_sessions.sum.5</code>, if there were 10 sessions logged in and 30 sessions logged out in the last 5 seconds, the value of this metric is <code>-20</code> (10-30). <code>num_aggregate_executors</code> The number of executions for the Aggregation operator. <code>num_auth_failed_sessions_bad_username_password</code> The number of sessions where authentication failed due to incorrect username and password. <code>num_auth_failed_sessions_out_of_max_allowed</code> The number of sessions that failed to authenticate logins because the value of the parameter <code>FLAG_OUT_OF_MAX_ALLOWED_CONNECTIONS</code> was exceeded. <code>num_auth_failed_sessions</code> The number of sessions in which login authentication failed. <code>num_indexscan_executors</code> The number of executions for index scan operators. <code>num_killed_queries</code> The number of killed queries. <code>num_opened_sessions</code> The number of sessions connected to the server. <code>num_queries</code> The number of queries. <code>num_query_errors_leader_changes</code> The number of the raft leader changes due to query errors. <code>num_query_errors</code> The number of query errors. <code>num_reclaimed_expired_sessions</code> The number of expired sessions actively reclaimed by the server. <code>num_rpc_sent_to_metad_failed</code> The number of failed RPC requests that the Graphd service sent to the Metad service. <code>num_rpc_sent_to_metad</code> The number of RPC requests that the Graphd service sent to the Metad service. <code>num_rpc_sent_to_storaged_failed</code> The number of failed RPC requests that the Graphd service sent to the Storaged service. <code>num_rpc_sent_to_storaged</code> The number of RPC requests that the Graphd service sent to the Storaged service. <code>num_sentences</code> The number of statements received by the Graphd service. <code>num_slow_queries</code> The number of slow queries. <code>num_sort_executors</code> The number of executions for the Sort operator. <code>optimizer_latency_us</code> The latency of executing optimizer statements. <code>query_latency_us</code> The latency of queries. <code>slow_query_latency_us</code> The latency of slow queries. <code>num_queries_hit_memory_watermark</code> The number of queries reached the memory watermark."},{"location":"nebula-dashboard-ent/7.monitor-parameter/#meta","title":"Meta","text":"Parameter Description <code>commit_log_latency_us</code> The latency of committing logs in Raft. <code>commit_snapshot_latency_us</code> The latency of committing snapshots in Raft. <code>heartbeat_latency_us</code> The latency of heartbeats. <code>num_heartbeats</code> The number of heartbeats. <code>num_raft_votes</code> The number of votes in Raft. <code>transfer_leader_latency_us</code> The latency of transferring the raft leader. <code>num_agent_heartbeats</code> The number of heartbeats for the AgentHBProcessor. <code>agent_heartbeat_latency_us</code> The latency of the AgentHBProcessor. <code>replicate_log_latency_us</code> The latency of replicating the log record to most nodes by Raft. <code>num_send_snapshot</code> The number of times that Raft sends snapshots to other nodes. <code>append_log_latency_us</code> The latency of replicating the log record to a single node by Raft. <code>append_wal_latency_us</code> The Raft write latency for a single WAL. <code>num_grant_votes</code> The number of times that Raft votes for other nodes. <code>num_start_elect</code> The number of times that Raft starts an election."},{"location":"nebula-dashboard-ent/7.monitor-parameter/#storage","title":"Storage","text":"Parameter Description <code>add_edges_latency_us</code> The latency of adding edges. <code>add_vertices_latency_us</code> The latency of adding vertices. <code>commit_log_latency_us</code> The latency of committing logs in Raft. <code>commit_snapshot_latency_us</code> The latency of committing snapshots in Raft. <code>delete_edges_latency_us</code> The latency of deleting edges. <code>delete_vertices_latency_us</code> The latency of deleting vertices. <code>get_neighbors_latency_us</code> The latency of querying neighbor vertices. <code>get_dst_by_src_latency_us</code> The latency of querying the destination vertex by the source vertex. <code>num_get_prop</code> The number of executions for the GetPropProcessor. <code>num_get_neighbors_errors</code> The number of execution errors for the GetNeighborsProcessor. <code>num_get_dst_by_src_errors</code> The number of execution errors for the GetDstBySrcProcessor. <code>get_prop_latency_us</code> The latency of executions for the GetPropProcessor. <code>num_edges_deleted</code> The number of deleted edges. <code>num_edges_inserted</code> The number of inserted edges. <code>num_raft_votes</code> The number of votes in Raft. <code>num_rpc_sent_to_metad_failed</code> The number of failed RPC requests that the Storage service sent to the Meta service. <code>num_rpc_sent_to_metad</code> The number of RPC requests that the Storaged service sent to the Metad service. <code>num_tags_deleted</code> The number of deleted tags. <code>num_vertices_deleted</code> The number of deleted vertices. <code>num_vertices_inserted</code> The number of inserted vertices. <code>transfer_leader_latency_us</code> The latency of transferring the raft leader. <code>lookup_latency_us</code> The latency of executions for the LookupProcessor. <code>num_lookup_errors</code> The number of execution errors for the LookupProcessor. <code>num_scan_vertex</code> The number of executions for the ScanVertexProcessor. <code>num_scan_vertex_errors</code> The number of execution errors for the ScanVertexProcessor. <code>update_edge_latency_us</code> The latency of executions for the UpdateEdgeProcessor. <code>num_update_vertex</code> The number of executions for the UpdateVertexProcessor. <code>num_update_vertex_errors</code> The number of execution errors for the UpdateVertexProcessor. <code>kv_get_latency_us</code> The latency of executions for the Getprocessor. <code>kv_put_latency_us</code> The latency of executions for the PutProcessor. <code>kv_remove_latency_us</code> The latency of executions for the RemoveProcessor. <code>num_kv_get_errors</code> The number of execution errors for the GetProcessor. <code>num_kv_get</code> The number of executions for the GetProcessor. <code>num_kv_put_errors</code> The number of execution errors for the PutProcessor. <code>num_kv_put</code> The number of executions for the PutProcessor. <code>num_kv_remove_errors</code> The number of execution errors for the RemoveProcessor. <code>num_kv_remove</code> The number of executions for the RemoveProcessor. <code>forward_tranx_latency_us</code> The latency of transmission. <code>scan_edge_latency_us</code> The latency of executions for the ScanEdgeProcessor. <code>num_scan_edge_errors</code> The number of execution errors for the ScanEdgeProcessor. <code>num_scan_edge</code> The number of executions for the ScanEdgeProcessor. <code>scan_vertex_latency_us</code> The latency of executions for the ScanVertexProcessor. <code>num_add_edges</code> The number of times that edges are added. <code>num_add_edges_errors</code> The number of errors when adding edges. <code>num_add_vertices</code> The number of times that vertices are added. <code>num_start_elect</code> The number of times that Raft starts an election. <code>num_add_vertices_errors</code> The number of errors when adding vertices. <code>num_delete_vertices_errors</code> The number of errors when deleting vertices. <code>append_log_latency_us</code> The latency of replicating the log record to a single node by Raft. <code>num_grant_votes</code> The number of times that Raft votes for other nodes. <code>replicate_log_latency_us</code> The latency of replicating the log record to most nodes by Raft. <code>num_delete_tags</code> The number of times that tags are deleted. <code>num_delete_tags_errors</code> The number of errors when deleting tags. <code>num_delete_edges</code> The number of edge deletions. <code>num_delete_edges_errors</code> The number of errors when deleting edges <code>num_send_snapshot</code> The number of times that snapshots are sent. <code>update_vertex_latency_us</code> The latency of executions for the UpdateVertexProcessor. <code>append_wal_latency_us</code> The Raft write latency for a single WAL. <code>num_update_edge</code> The number of executions for the UpdateEdgeProcessor. <code>delete_tags_latency_us</code> The latency of deleting tags. <code>num_update_edge_errors</code> The number of execution errors for the UpdateEdgeProcessor. <code>num_get_neighbors</code> The number of executions for the GetNeighborsProcessor. <code>num_get_dst_by_src</code> The number of executions for the GetDstBySrcProcessor. <code>num_get_prop_errors</code> The number of execution errors for the GetPropProcessor. <code>num_delete_vertices</code> The number of times that vertices are deleted. <code>num_lookup</code> The number of executions for the LookupProcessor. <code>num_sync_data</code> The number of times the Storage service synchronizes data from the Drainer. <code>num_sync_data_errors</code> The number of errors that occur when the Storage service synchronizes data from the Drainer. <code>sync_data_latency_us</code> The latency of the Storage service synchronizing data from the Drainer."},{"location":"nebula-dashboard-ent/7.monitor-parameter/#graph_space","title":"Graph space","text":"<p>Note</p> <p>Space-level metrics are created dynamically, so that only when the behavior is triggered in the graph space, the corresponding metric is created and can be queried by the user.</p> Parameter Description <code>num_active_queries</code> The number of queries currently being executed. <code>num_queries</code> The number of queries. <code>num_sentences</code> The number of statements received by the Graphd service. <code>optimizer_latency_us</code> The latency of executing optimizer statements. <code>query_latency_us</code> The latency of queries. <code>num_slow_queries</code> The number of slow queries. <code>num_query_errors</code> The number of query errors. <code>num_query_errors_leader_changes</code> The number of raft leader changes due to query errors. <code>num_killed_queries</code> The number of killed queries. <code>num_aggregate_executors</code> The number of executions for the Aggregation operator. <code>num_sort_executors</code> The number of executions for the Sort operator. <code>num_indexscan_executors</code> The number of executions for index scan operators. <code>num_auth_failed_sessions_bad_username_password</code> The number of sessions where authentication failed due to incorrect username and password. <code>num_auth_failed_sessions</code> The number of sessions in which login authentication failed. <code>num_opened_sessions</code> The number of sessions connected to the server. <code>num_queries_hit_memory_watermark</code> The number of queries reached the memory watermark. <code>num_reclaimed_expired_sessions</code> The number of expired sessions actively reclaimed by the server. <code>num_rpc_sent_to_metad_failed</code> The number of failed RPC requests that the Graphd service sent to the Metad service. <code>num_rpc_sent_to_metad</code> The number of RPC requests that the Graphd service sent to the Metad service. <code>num_rpc_sent_to_storaged_failed</code> The number of failed RPC requests that the Graphd service sent to the Storaged service. <code>num_rpc_sent_to_storaged</code> The number of RPC requests that the Graphd service sent to the Storaged service. <code>slow_query_latency_us</code> The latency of slow queries."},{"location":"nebula-dashboard-ent/7.monitor-parameter/#single_process_metrics","title":"Single process metrics","text":"<p>Graph, Meta, and Storage services all have their own single process metrics.</p> Parameter Description <code>context_switches_total</code> The number of context switches. <code>cpu_seconds_total</code> The CPU usage based on user and system time. <code>memory_bytes_gauge</code> The number of bytes of memory used. <code>open_filedesc_gauge</code> The number of file descriptors. <code>read_bytes_total</code> The number of bytes read. <code>write_bytes_total</code> The number of bytes written."},{"location":"nebula-dashboard-ent/8.faq/","title":"FAQ","text":"<p>This topic lists the frequently asked questions for using NebulaGraph Dashboard. You can use the search box in the help center or the search function of the browser to match the questions you are looking for.</p>"},{"location":"nebula-dashboard-ent/8.faq/#what_are_cluster_node_and_service","title":"\"What are Cluster, Node, and Service?\"","text":"<ul> <li>Cluster: refers to a group of systems composed of nodes where multiple NebulaGraph services are located.</li> </ul> <ul> <li>Node: refers to the physical or virtual machine hosting NebulaGraph services.</li> </ul> <ul> <li>Service: refers to NebulaGraph services, including Metad, Storaged, and Graphd services.</li> </ul>"},{"location":"nebula-dashboard-ent/8.faq/#what_is_the_cluster_status","title":"\"What is the cluster status?\"","text":"<p>The status of a cluster is as follows:</p> <ul> <li>installing: The cluster is being created. The process will take about 3 to 10 minutes.</li> <li>healthy: All services in the cluster are healthy.</li> <li>unhealthy: There is an unhealthy service in the cluster service.</li> </ul>"},{"location":"nebula-dashboard-ent/8.faq/#why_authorizing_nodes","title":"\"Why authorizing nodes?\"","text":"<p>Managing clusters requires the SSH information of the corresponding node. Therefore, you need to have at least an SSH account and the corresponding password with executable permissions before performing operations on Dashboard.</p>"},{"location":"nebula-dashboard-ent/8.faq/#what_is_scaling","title":"\"What is scaling?\"","text":"<p>NebulaGraph is a distributed graph database that supports dynamic scaling services at runtime. Therefore, you can dynamically scale Storaged and Graphd services through Dashboard. The Metad service cannot be scaled.</p>"},{"location":"nebula-dashboard-ent/8.faq/#why_cannot_operate_on_the_metad_service","title":"\"Why cannot operate on the Metad service?\"","text":"<p>The Metad service stores the metadata of the NebulaGraph database. Once the Metad service fails to function, the entire cluster may break down. Besides, the amount of data processed by the Metad service is not much, so it is not recommended to scale the Metad service. And we directly disabled operating on the Metad service in Dashboard to prevent the cluster from being unavailable due to the misoperation of users.</p>"},{"location":"nebula-dashboard-ent/8.faq/#what_impact_will_the_scaling_have_on_the_data","title":"\"What impact will the scaling have on the data?\"","text":"<ul> <li>Scale out the Storaged service: Dashboard will create and start the Storaged service on the specified machine, which will not affect the existing data. You can choose to perform <code>Balance Leader</code> in the <code>Storage Leader Distribution</code> area and <code>Balance Data</code> in the <code>Partition Distribution</code> area on the Information-&gt;Overview Info page according to your own needs.</li> </ul> <ul> <li>Scale in the Storaged service: Dashboard will not scale in Storage services until you execute <code>Balance Data Remove</code> to migrate all the partitions from the specified Storage service to other Storage services in the <code>Partition Distribution</code> area on the Information-&gt;Overview Info page.  </li> </ul> <ul> <li>Scaling the Graphd service will not affect the data.</li> </ul>"},{"location":"nebula-dashboard-ent/8.faq/#why_dashboard_enterprise_edition_cannot_be_started","title":"\"Why Dashboard Enterprise Edition cannot be started?\"","text":"<ul> <li>Make sure that the license key is loaded.</li> </ul> <ul> <li>Make sure that the license is not expired.</li> </ul> <p>You can also execute <code>cat logs/webserver.log</code> in the Dashboard directory to view the startup information of each module. If the above conditions are met but Dashboard still cannot be started, go to NebulaGraph Official Forum for consultation.</p>"},{"location":"nebula-dashboard-ent/8.faq/#can_i_add_the_nebulagraph_installation_package_manually","title":"\"Can I add the NebulaGraph installation package manually?\"","text":"<p>You can add the installation package manually in Dashboard. To download the system and RPM/DEB package you need, see How to download NebulaGraph and add the package to <code>nebula-dashboard-ent/download/nebula-graph</code>. And you can select the added package for deployment when creating and scaling out a cluster.</p>"},{"location":"nebula-dashboard-ent/8.faq/#why_does_it_prompt_ssh_connection_error_when_importing_a_cluster","title":"Why does it prompt \u201cSSH connection error\u201d when importing a cluster\uff1f","text":"<p>If Service Host shows <code>127.0.0.1</code>, and your Dashboard and NebulaGraph are deployed on the same machine when authorizing service hosts, the system will prompt \"SSH connection error\u201d. You need to change the Host IP of each service to the real machine IP in the configuration files of all NebulaGraph services. For more information, see Configuration management.</p> <p>If you import a cluster deployed with Docker, it also prompts \"SSH connection error\". Dashboard does not support importing a cluster deployed with Docker.</p>"},{"location":"nebula-dashboard-ent/3.create-import-dashboard/1.create-cluster/","title":"Create clusters","text":"<p>This topic introduces how to create clusters using Dashboard.</p>"},{"location":"nebula-dashboard-ent/3.create-import-dashboard/1.create-cluster/#steps","title":"Steps","text":"<p>You can create a cluster following these steps:</p> <ol> <li>At the top of the Dashboard page, click the Cluster Management button.</li> <li>On the Cluster management page, click Create cluster.</li> <li> <p>On the Create cluster page, fill in the following:</p> <ul> <li>Enter a Cluster Name, up to 15 characters for each name.</li> <li> <p>Choose a NebulaGraph version to install.</p> <p>Note</p> <p>Only one Enterprise Edition of NebulaGraph is provided for you to choose from on the Create cluster page. To install other versions of NebulaGraph, you can download or upload the corresponding installer package on the Package Management page. For details, see Package management.</p> <p>Note</p> <p>When creating a cluster with versions below 3.5.0, you need to upload the license certificate manually.</p> </li> </ul> <ul> <li> <p>Add nodes. Enter the following information, the Host, SSH port, SSH user, authentication type, NebulaGraph package, etc.</p> <p>The authentication type is described as follows:</p> <ul> <li>SSH Password: Enter the password of the SSH user.</li> </ul> <ul> <li>SSH Key: Click Upload and select the private key file of the node. You need to generate the secret key files on the node to be added and send the private key file to the current computer (not the machine where Dashboard is deployed). If the passphrase is set, this parameter is also required.</li> </ul> <p></p> </li> </ul> <ul> <li>Import nodes in batches. The information of each node is required. To import nodes in batches, you need to choose the installation package and click download the CSV template. Fill in the template and upload it. Ensure that the node is correct, otherwise, upload failure may happen.</li> </ul> </li> <li> <p>Select the node and add the service you need in the upper right corner. To create a cluster, you need to add 3 types of services to the node. If not familiar with the NebulaGraph architecture, click Auto add service.</p> <p></p> </li> <li> <p>(Optional) Edit the port and HTTP port of the meta, graph, and storage services, and then click OK.</p> </li> <li> <p>Click Create Cluster. Make sure the configuration is correct and there is no conflict between nodes, and then click Confirm.</p> </li> <li> <p>If a cluster with the status of <code>installing</code> appears in the list on the cluster management page, you need to wait for 3 to 10 minutes until the status changes to <code>healthy</code>, that is, the cluster is created successfully. If the service status is <code>unhealthy</code>, it means that there is an abnormal service in the cluster, click Detail for more information.</p> </li> </ol>"},{"location":"nebula-dashboard-ent/3.create-import-dashboard/1.create-cluster/#next_to_do","title":"Next to do","text":"<p>After the cluster is successfully created, you can operate on the cluster. For details, see Cluster operations.</p>"},{"location":"nebula-dashboard-ent/3.create-import-dashboard/2.import-cluster/","title":"Import clusters","text":"<p>This topic introduces how to import clusters using Dashboard. The current version only supports importing clusters deployed by the official DEB or RPM packages and clusters created by Dashboard. Currently, importing clusters deployed by Docker and Kubernetes is not supported.</p>"},{"location":"nebula-dashboard-ent/3.create-import-dashboard/2.import-cluster/#steps","title":"Steps","text":"<p>Caution</p> <p>In the same cluster, the service versions need to be unified. Importing NebulaGraph examples from different versions in the same cluster is not supported.</p> <ol> <li> <p>In the configuration files of each service, change the IP in <code>&lt;meta|graph|storage&gt;_server_addrs</code> and <code>local_ip</code> to the server's IP, and then start NebulaGraph.</p> <p>For details, see Configurations and Manage NebulaGraph services.</p> </li> <li> <p>On the Cluster management page, click Import cluster.</p> </li> <li> <p>On the Import cluster page, enter the information of Connect to NebulaGraph.</p> <ul> <li>Graphd Host: :n. In this example, the IP is <code>192.168.8.157:9669</code>. <li>Username: The account to connect to NebulaGraph. In this example, the username is <code>vesoft</code>.</li> <li>Password: The password to connect to NebulaGraph. In this example, the password is <code>nebula</code>.</li> <p>Note</p> <p>By default, authentication is disabled in NebulaGraph. Therefore, you can use <code>root</code> as the username and any password to connect to NebulaGraph.   When authentication is enabled in NebulaGraph, you need to use the specified username and password to connect to NebulaGraph. For details of authentication, see NebulaGraph manual.</p> <li> <p>On the NebulaGraph connection panel, fill in the following:</p> <ul> <li> <p>Enter the cluster name, 15 characters at most. In this example, the cluster name is <code>create_1027</code>, and choose whether to use <code>sudo</code> to connect to the cluster.</p> <p>Notice</p> <p>If your SSH account does not have permission for the NebulaGraph cluster, you can use <code>sudo</code> to connect to it.</p> </li> </ul> <ul> <li> <p>Authorize the node. The SSH username and password of each node are required, and choose to run <code>sudo</code> or not.</p> <p>Notice</p> <p>If your SSH account has no permission to operate NebulaGraph, but can execute <code>sudo</code> commands without password, set use sudo to yes.</p> </li> </ul> <ul> <li>Batch authorization requires uploading the CSV file. Edit the authentication information of each node according to the downloaded CSV file. Ensure that the node information is correct, otherwise upload failure may happen.</li> </ul> <ul> <li> <p>If the node status on the page becomes authorized, the node authentication is successful.</p> <p></p> </li> </ul> </li> <li> <p>Ensure that all nodes are authorized successfully. Click Import cluster.</p> </li>"},{"location":"nebula-dashboard-ent/3.create-import-dashboard/2.import-cluster/#next_to_do","title":"Next to do","text":"<p>After the cluster is successfully imported, you can operate the cluster. For details, see Overview.</p>"},{"location":"nebula-dashboard-ent/4.cluster-operator/1.overview/","title":"Cluster Overview","text":"<p>This topic introduces the Cluster Overview page of Dashboard, which contains the following parts:</p> <ul> <li>Info Overview</li> <li>Cluster Topology</li> <li>Monitoring Screen</li> </ul>"},{"location":"nebula-dashboard-ent/4.cluster-operator/1.overview/#entry","title":"Entry","text":"<ol> <li>At the top navigation bar of the Dashboard Enterprise Edition page, click Cluster Management.</li> <li>Click Detail on the right of the cluster management page to check the overview of a specified cluster.</li> </ol>"},{"location":"nebula-dashboard-ent/4.cluster-operator/1.overview/#info_overview","title":"Info Overview","text":""},{"location":"nebula-dashboard-ent/4.cluster-operator/1.overview/#cluster_key_information","title":"Cluster key information","text":"<p>In the cluster key information, the following information is displayed.</p> <ul> <li>Cluster health score: Displays the health of the cluster on a percentage basis. It is refreshed every 5 minutes, or immediately if there is an emergency level alarm. The calculation formula can be configured on the Cluster Diagnosis page.</li> <li>Monitoring nodes and services: Display the number of online nodes/total nodes and the number of online services/total services. Different types of services are displayed separately. Click on  to access the Service page.</li> <li>Number of slow queries (5min): The number of slow queries in the cluster in the last 5 minutes.</li> <li>Newly created sessions (5min): The number of newly created sessions in the cluster in the last 5 minutes.</li> <li>Latest backup time: The latest backup time of the cluster. Click on  to access the Backup and Restore page.</li> <li>Total storage usage: The total storage usage of the cluster. Click on  to display more details.</li> </ul>"},{"location":"nebula-dashboard-ent/4.cluster-operator/1.overview/#cluster_monitoring_panel","title":"Cluster monitoring panel","text":"<p>In the cluster monitoring panel, users can select and view monitoring data from different periods. Users can either select a custom time range or choose from predefined time ranges such as the last 5 minutes, 1 hour, 6 hours, 12 hours, 1 day, and 3 days. The monitoring panel displays the following information:</p> <ul> <li>QPS: The time-series diagram of the QPS.</li> <li>Query latency(P95): The time-series diagram of the query latency.</li> <li>CPU usage: The CPU usage of the cluster.</li> <li>Memory Utilization: The memory utilization of the cluster.</li> <li>Opened sessions: The total number of sessions created in the cluster.</li> <li>Slow Query: The number of slow queries in the cluster.</li> </ul> <p>Users can click  to jump to the detailed service monitoring panel page.</p>"},{"location":"nebula-dashboard-ent/4.cluster-operator/1.overview/#cluster_topology","title":"Cluster topology","text":"<p>Shows the distribution and status of nodes and services in the cluster. Click Cluster Topology at the top of the page to enter the Cluster Topology page.</p>"},{"location":"nebula-dashboard-ent/4.cluster-operator/1.overview/#monitoring_screen","title":"Monitoring screen","text":"<p>The monitoring screen helps users understand the health status of the cluster and the information on services and nodes at a glance.</p> <p>Click Switch to Monitoring Screen to enter the monitoring screen page.</p> <p></p> Screen area Information displayed Upper middle area 1. The health degree of your cluster. The system scores the health of your cluster. For more information, see the following note. 2. The information and number of running nodes, the number of running services and abnormal services in the cluster. 3. CPU and memory usage of the node at the current time.4. Alert notifications. The system displays the 5 most recently triggered alert messages based on their severity level (emergency&gt;critical&gt;warning). For more information, Monitoring alerts. Lower middle area Monitoring information of 4 Graph service metrics at different periods. The 4 metrics are: 1. num_active_sessions2. num_slow_queries3. num_active_queries4. num_query_errors Left side of the area 1. QPS (Query Per Second) of your cluster.2. The monitoring information of 2 Storage service metrics at different periods. The two metrics are: add_edges_latency_us,add_vertices_latency_us. Right side of the area The node-related metrics information at different periods. Metrics include: 1. cpu_utilization2. memory_utilization3. load_1m4. disk_readbytes5. disk_writebytes <p>For more information about the monitoring metrics, see Metrics.</p> <p>Note</p> <p>Cluster scoring rules are as follows:</p> <ul> <li>The maximum score is 100; The minimum score is 13.</li> <li>When 100\u2265Health Degree\u226580, the score is blue; When 80\uff1eHealth Degree\u226560, the score is yellow; When Health Degree\uff1c60, the score is yellow.</li> <li>Algorithm: (1-number of abnormal services/total number of services)*100%.</li> <li>Except for the appearance of the first <code>emergency</code> level alert that deducts 40 points, 10 points are deducted for each of the other <code>emergency</code> level alerts and other levels of alerts.</li> </ul>"},{"location":"nebula-dashboard-ent/4.cluster-operator/2.monitor/","title":"Cluster monitoring","text":"<p>This section introduces the monitoring function of the Dashboard, including default monitoring and custom monitoring.</p> <p>At the top navigation bar of the Dashboard Enterprise Edition page, click Cluster Management, and click Detail at the right of the target cluster. Monitoring at the left navigation bar contains default monitoring and custom monitoring.</p>"},{"location":"nebula-dashboard-ent/4.cluster-operator/2.monitor/#default_monitoring","title":"Default monitoring","text":"<p>The default cluster monitoring page of Dashboard displays 8 monitoring metric panels, which cannot be modified or deleted. Users can add, modify, and delete monitoring metric panels in a new dashboard using the Custom Monitoring function described below.</p> <p>The 8 default monitoring metrics displayed are:</p> <ul> <li>CPU Utilization: Percentage of CPU usage.</li> <li>Memory Utilization: Percentage of memory usage.</li> <li>Disk Usage: Disk usage shows instances, disk name, mount point, and usage.</li> <li>Load 5: System average load over the last 5 minutes.</li> <li>QPS: Queries per second.</li> <li>Num Queries &amp; Slow Queries: Number of queries and slow queries.</li> <li>Num Opened Sessions: Number of active sessions.</li> <li>Add Edge Latency &amp; Add Vertex Latency: Delay time for adding edges and vertices.</li> </ul> <p>Clicking the  button on the upper right corner of each panel can filter the time and view each monitoring metric on a separate subpage.</p>"},{"location":"nebula-dashboard-ent/4.cluster-operator/2.monitor/#custom_monitoring","title":"Custom Monitoring","text":""},{"location":"nebula-dashboard-ent/4.cluster-operator/2.monitor/#creating_monitoring","title":"Creating Monitoring","text":"<ol> <li> <p>Click on New Dashboard and enter a custom Dashboard name in the pop-up dialog box. Click OK to create a new cluster monitoring.</p> </li> <li> <p>Click on Add Panel in the upper right corner or Create Panel in the middle of the page to add a monitoring panel. Set the parameters and click Confirm. The parameter descriptions are as follows:</p> <p></p> </li> </ol> <ul> <li>Panel Name: Custom panel name.</li> </ul> <ul> <li>Panel Type: Includes Node Linear Panel, Service Linear Panel, and Disk Usage Bar Panel. Different panel types determine the available metrics for selection.</li> </ul> <ul> <li>Metric: Select different metrics based on the panel type. For example, for Node Linear Panel, you can select CPU, Memory, Load, Disk, Network In/Out, etc. Multiple metrics can be selected. Please refer to the Metrics for more details.</li> </ul> <ul> <li>Metric Type: Choose from percentage, byte, byte/s, io/s, number, etc.</li> </ul> <ul> <li>Show Index: Panel display order, with numbers ranging from 1 to infinity, with smaller numbers being displayed first. The panels are sorted by the creation time by default.</li> </ul> <p>Note</p> <p>The Graph service supports a set of graph space-based monitoring metrics. When selecting Graph service-related metrics, you can select the corresponding graph space in the added spaces dropdown list.</p> <p>Caution</p> <p>Before using Graph Space Metrics, users need to set <code>enable_space_level_metrics</code> to <code>true</code> in the Graph service. For specific operations, see Update config.</p>"},{"location":"nebula-dashboard-ent/4.cluster-operator/2.monitor/#managing_monitoring","title":"Managing Monitoring","text":"<p>To manage monitoring metric panels, click the  button in the upper right corner of each panel to modify or delete the panel.</p> <p>You can also click the  button in the upper right corner of the panel to enter the subpage of the monitoring metrics panel and view the monitoring metrics.</p>"},{"location":"nebula-dashboard-ent/4.cluster-operator/2.monitor/#more_operations","title":"More operations","text":"<p>The section above the page is used for monitoring filtering, with the following features:</p> <ul> <li>Time selection: By default, 6 hours of monitoring data is selected for viewing. You can select a time based on a specific date, or choose a recent period, such as the past 5 minutes, 1 hour, 6 hours, 12 hours, 1 day, or 3 days.</li> </ul> <ul> <li>Instance selection: Monitoring data for all nodes is displayed by default, and you can adjust the selections within the Instance box.</li> </ul> <ul> <li>Service selection: Monitoring data for all services is displayed by default, and you can adjust the selections within the Service box.</li> </ul> <ul> <li>Data update: The monitoring data on the page is not automatically updated by default (i.e., it is turned <code>Off</code>). You can set the page update frequency in the drop-down box, such as every 5 seconds, 30 seconds, or 5 minutes. Click the  button to manually update the data.</li> </ul>"},{"location":"nebula-dashboard-ent/4.cluster-operator/5.operation-record/","title":"Operation record","text":"<p>This topic shows how to use the operation record feature in NebulaGraph Dashboard.</p> <p>At the top navigation bar of the Dashboard Enterprise Edition page, click Cluster Management, and click Detail at the right of the target cluster, and on the left-side navigation bar, click Operation Record to enter the operation history page.</p> <p>On the Operation record page, you can check the operation records of the latest 1 hour, 6 hours, 1 day, 3 days, 7days, or 14 days. You can also view who runs what operation on which cluster at what time.</p>"},{"location":"nebula-dashboard-ent/4.cluster-operator/6.settings/","title":"Other settings","text":"<p>The following shows other settings in NebulaGraph Dashboard.</p> <p>At the top navigation bar of the Dashboard Enterprise Edition page, click Cluster Management, click Details on the right side of the target cluster, and on the left-side navigation bar, click Other Settings to enter the other settings page.</p> <ul> <li>Information: shows the cluster name, the creation time, the creator, and the owner of the current cluster.</li> </ul> <ul> <li> <p>Unbind: Unbind a cluster and remove its information from the platform. The unbound cluster info will be removed and no operations will be done on cluster services or NebulaGraph data.</p> <p>Note</p> <p>To unbind a cluster, enter the cluster name first.</p> </li> </ul> <ul> <li> <p>Delete: Delete a cluster and remove its information from the platform. Deleting the cluster will stop its service and unbind the cluster info, but retain its NebulaGraph data. Be cautious when you delete a cluster.</p> <p>Note</p> <p>To delete a cluster, enter the cluster name first</p> </li> </ul>"},{"location":"nebula-dashboard-ent/4.cluster-operator/7.data-synchronization/","title":"Data Synchronization","text":"<p>The Data Synchronization function of Dashboard Enterprise Edition is used to realize data synchronization between clusters.</p> <p>Click Cluster Management on the top navigation bar of Dashboard Enterprise Edition, click Details on the right side of the target cluster, and then click Data Synchronization on the left navigation bar to enter.</p> <p>Note</p> <p>For more information about the data synchronization function, see Synchronize between two clusters.</p>"},{"location":"nebula-dashboard-ent/4.cluster-operator/7.data-synchronization/#prerequisites","title":"Prerequisites","text":"<p>Complete the deployment of the current cluster Listener in Service Configuration, and restart the Meta Listener and Storage Listener.</p>"},{"location":"nebula-dashboard-ent/4.cluster-operator/7.data-synchronization/#steps","title":"Steps","text":"<p>Click Sync Space on the page and follow the steps below:</p> <ol> <li>Select Space: Check the graph space that needs to be synchronized in the primary cluster, and modify or use the default graph space name. Click Next after making your selections.</li> <li>Select Secondary Cluster: Select the data synchronization secondary cluster in the drop-down box, and click Next.</li> <li>Start the synchronization.</li> </ol>"},{"location":"nebula-dashboard-ent/4.cluster-operator/9.notification/","title":"Notification","text":"<p>NebulaGraph Dashboard Enterprise Edition notifies on monitoring metrics. You can view alert messages, set alert rules, and set alert receivers.</p> <p>At the top navigation bar of the Dashboard Enterprise Edition page, click Cluster Management, and click Detail at the right of the target cluster. Notification at the left navigation bar contains Alert Messages, Alert Rules and Receivers.</p> <p></p>"},{"location":"nebula-dashboard-ent/4.cluster-operator/9.notification/#alert_messages","title":"Alert messages","text":"<p>Alert messages will pop up in the upper right corner of the page, you can do the following operations:</p> <ul> <li>Click the View button to go to the Notification-&gt;Alert Messages page to view detailed alert information.</li> </ul> <ul> <li>Click the Mute buttons, the alert rule will not be triggered again for 2 hours only for this user.</li> </ul> <p>You can perform the following operations on the Alert Messages page:</p> <ul> <li>You can search for alert messages by message name.</li> <li>You can filter alert messages by date and time, and period. Available periods are 1 hour, 6 hours, 12 hours, 1 day, 3 days, 7 days, and 14 days.</li> <li>You can filter alert messages by severity, type, and status. Click Reset to empty all filtering results.</li> <li>You can set the processing status of alert messages. The status is <code>unsolved</code> by default, and you can set the status to <code>Dealing</code> or <code>Solved</code>.</li> </ul> <p>Alert messages cannot be deleted. In the <code>nebula-dashboard-ent/config/config.yaml</code> file, <code>messageStore</code> sets the number of days to keep alert messages, the value of which is 90 by default. For more information about the configuration file, see Deploy Dashboard.</p>"},{"location":"nebula-dashboard-ent/4.cluster-operator/9.notification/#alert_rules","title":"Alert rules","text":"<p>Before receiving alert messages, you need to set alert rules. Alert rules include custom rules and build-in rules.</p>"},{"location":"nebula-dashboard-ent/4.cluster-operator/9.notification/#create_custom_rules","title":"Create custom rules","text":"<p>Follow the below steps to create a custom rule.</p> <ol> <li>At the top navigation bar of the Dashboard Enterprise Edition page, click Cluster Management, and then on the right side of the target cluster, click Detail.</li> <li>On the left side of the Cluster Management page, click Notification-&gt;Alert Rules.</li> <li>On the Alert Rules page, click Custom Rules, and then click Create Rule at the top right of the page.</li> <li> <p>Set alert rules.</p> <ol> <li> <p>On the Basic Information tab, set alert name, severity, and frequency.</p> Parameter Description Alert Name Set a name for an alert rule. The name can only contain lowercase letters, numbers, and hyphens (<code>-</code>), and must begin and end with a lowercase letter or number. The name contains up to 253 characters. Severity Set a severity level for an alert rule. The severity level includes <code>emergency</code>, <code>critical</code>, and <code>warning</code>. Alert Frequency Set how often an alert message is sent. Unit: Minute (Min). </li> <li> <p>On the Condition tab, set metric type, rule, and alert duration.</p> Parameter Description Metric Type Set a metric type. Metric type includes the node metric type and the service type (graphd,storaged,metad). Metric Rule Click + Add condition to set metric rules for a node or a service. It supports adding composite conditions (like the usage of AND). For more information, see Monitoring metrics. Alert duration Set how long an alert lasts before the alert message is triggered. Unit: Minute (Min). </li> <li> <p>On the Rules Overview tab, check the overall rules.</p> </li> <li> <p>On the Message Settings tab, you can see the rule summary and rule details, and then click Submit.</p> <p>Note</p> <p>DO NOT modify the rule details unless you are clear of the consequences. </p> </li> </ol> </li> </ol>"},{"location":"nebula-dashboard-ent/4.cluster-operator/9.notification/#view_custom_rules","title":"View custom rules","text":"<p>On the Custom Rules, you can do the following operations.</p> <ul> <li>Search for alert rules and filter alert rules by severity, type, metric, and status.</li> </ul> <ul> <li>Click Reset to empty all filtering results.</li> </ul> <ul> <li>Turn on or off the alert rule you set. The status of an alert rule that has been turned on is active. The status of an alert rule that has been turned off is disable.</li> </ul>"},{"location":"nebula-dashboard-ent/4.cluster-operator/9.notification/#edit_custom_rules","title":"Edit custom rules","text":"<p>In the Custom Rules list, select the target rule, and then click the edit icon  to edit the rule.</p>"},{"location":"nebula-dashboard-ent/4.cluster-operator/9.notification/#delete_custom_rules","title":"Delete custom rules","text":"<p>In the Custom Rules list, select the target rule, click the delete icon  to delete the rule.</p>"},{"location":"nebula-dashboard-ent/4.cluster-operator/9.notification/#built-in_rules","title":"Built-in Rules","text":"<p>The built-in rules are the default rules in Dashboard Enterprise Edition. You can enable or disable the built-in rules. The status of a built-in rule that has been turned on is active. The status of a built-in rule that has been turned off is disable.</p> <p>Note</p> <p>Built-in rules cannot be edited or deleted.</p>"},{"location":"nebula-dashboard-ent/4.cluster-operator/9.notification/#mute_alert_rules","title":"Mute alert rules","text":"<p>Click  to silence the alert rule for two hours. To cancel the silence midway, click .</p>"},{"location":"nebula-dashboard-ent/4.cluster-operator/9.notification/#receiver_configuration","title":"Receiver configuration","text":"<p>Alerts can be configured to send notifications to receivers. You can set the email address of the receiver who receives alert notifications. You can also view your Webhook URL and whether the webhook is enabled or not. For more information about the Webhook, see Notification Endpoint.</p> <ol> <li>At the top navigation bar of the Dashboard Enterprise Edition page, Click Cluster Management, and on the right side of the target cluster, click Detail.</li> <li>On the left-side navigation bar of the Cluster Management page, click Notification-&gt;Receivers.</li> <li> <p>On the Receivers page,</p> <ul> <li>Click Mail and input the email of the receiver who receives alert notifications and then click Add.</li> <li>Click Webhook and see your Webhook URL and whether the webhook is enabled or not.</li> </ul> </li> </ol>"},{"location":"nebula-dashboard-ent/4.cluster-operator/analysis-diagnosis/cluster-diagnosis/","title":"Cluster diagnostics","text":"<p>The cluster diagnostics feature in Dashboard Enterprise Edition is to locate and analyze the current cluster problems within a specified time range and summarize the diagnostic results and cluster monitoring information to web-based diagnostic reports.</p>"},{"location":"nebula-dashboard-ent/4.cluster-operator/analysis-diagnosis/cluster-diagnosis/#features","title":"Features","text":"<ul> <li>Diagnostic reports allow you to troubleshoot the current cluster problems within a specified time range.</li> <li>Quickly understand the basic information of the nodes, services, service configurations, and query sessions in the cluster.</li> <li>Based on the diagnostic reports, you can make operation and maintenance recommendations and cluster alerts.</li> </ul>"},{"location":"nebula-dashboard-ent/4.cluster-operator/analysis-diagnosis/cluster-diagnosis/#entry","title":"Entry","text":"<ol> <li>In the top navigation bar of the Dashboard Enterprise Edition page, click Cluster Management.</li> <li>On the right side of the target cluster, click Detail.</li> <li>In the left navigation bar, click Analysis-&gt;Cluster Diagnostics.</li> </ol>"},{"location":"nebula-dashboard-ent/4.cluster-operator/analysis-diagnosis/cluster-diagnosis/#create_diagnostic_reports","title":"Create diagnostic reports","text":"<ol> <li> <p>Select a time range for diagnostics. You can customize the time range or set the range by selecting time intervals, including <code>1 Hour</code>, <code>6 Hours</code>, <code>12 Hours</code>, <code>1 Day</code>, <code>3 Days</code>, <code>7 Days</code>, and <code>14 Days</code>.</p> <p>Caution</p> <p>Note that the end time of the diagnostic range you set cannot be longer than the current time. If the end time is longer than the current time, the end time will be set to the current time.</p> </li> <li> <p>Confirm the configuration of the Formula Config. Users can adjust the formula manually.</p> <p>The weight function in the formula is <code>weight(value&gt;conditionValue , weightValue)</code>, where the numbers in the blue font support modification. The method for calculating the value of the function is <code>(value - conditionValue * timeRange)/((maxValue - conditionValue) * timeRange) * weightValue</code>.</p> <ul> <li>value: Current metric value.</li> <li>conditionValue: The lowest value that matches the condition.</li> <li>timeRange: The number of days in the time range. For avg type metrics, the value of this parameter is 1.</li> <li>maxValue: The maximum metric value. The value for the percentage type is 100, and the value for the rest of the types is 2 times the conditionValue.</li> </ul> <p>For example, <code>weight(hit_memory_times &gt; 10 * days, 10)</code>, when the time range is selected as 2 days and <code>hit_memory_times = 40</code>, the formula is <code>(40 - 10 * 2)/((10*2 - 10) * 2) * 10 = 10</code>\u3002</p> </li> <li> <p>Click Start.</p> <p></p> </li> <li> <p>Wait for the diagnostic report to be generated. When the diagnostic status is changed to success, the diagnostic report is ready.</p> </li> </ol>"},{"location":"nebula-dashboard-ent/4.cluster-operator/analysis-diagnosis/cluster-diagnosis/#view_diagnostic_reports","title":"View diagnostic reports","text":"<p>In the diagnostic report list, you can view the diagnostic reports by clicking Detail on the right side of the target report.</p> <p>A diagnostic report contains the following information:</p> <ul> <li>Basic Info<ul> <li>Displays information that needs to be focused on, such as emergency alarms, warning alarms, tips, etc.</li> <li>Display cluster health score, maximum average CPU usage, maximum average memory usage, etc.</li> </ul> </li> <li>Node Info<ul> <li>Display the Host, instances, architecture, and system of each node.</li> <li>Display CPU, memory, disk, and network traffic of each node.</li> </ul> </li> <li>Service Info<ul> <li>Show the online status and session information of each service.</li> <li>Display graph, meta, and storage service stability information.</li> </ul> </li> <li>Configuration Info<ul> <li>Display configuration change information.</li> </ul> </li> </ul> <p>You can also download the diagnostic report in HTML or PDF format.</p>"},{"location":"nebula-dashboard-ent/4.cluster-operator/analysis-diagnosis/slow-query-analyst/","title":"Slow query analyst","text":"<p>DBAs need to analyze and manage the execution of query statements in a cluster as part of their daily work. NebulaGraph Dashboard provides a feature that allows users to view slow queries, including the statements, duration, categories, execution plans, and more.</p>"},{"location":"nebula-dashboard-ent/4.cluster-operator/analysis-diagnosis/slow-query-analyst/#features","title":"Features","text":"<ul> <li>Display the information about current slow queries. Users can filter slow queries by keywords and graph space.</li> <li>Display the history of slow queries. Users can filter the records according to keywords, graph space, category, and time range.</li> </ul>"},{"location":"nebula-dashboard-ent/4.cluster-operator/analysis-diagnosis/slow-query-analyst/#prerequisites","title":"Prerequisites","text":"<p><code>enable_record_slow_query</code>=<code>true</code> is set in the graph service configuration of NebulaGraph. For details, see Graph service configuration.</p>"},{"location":"nebula-dashboard-ent/4.cluster-operator/analysis-diagnosis/slow-query-analyst/#entry","title":"Entry","text":"<ol> <li>In the top navigation bar of the Dashboard Enterprise Edition page, click Cluster Management.</li> <li>On the right side of the target cluster, click Detail.</li> <li>In the left navigation bar, click Analysis-&gt;Slow Query Analyst.</li> </ol>"},{"location":"nebula-dashboard-ent/4.cluster-operator/analysis-diagnosis/slow-query-analyst/#view_current_slow_queries","title":"View current slow queries","text":"<p>Clicking the Running tab will display the current slow queries. The parameters are described as follows.</p> Parameter Description Query The statement of the slow query. Duration(\u03bcs) The duration that the slow query has been executed. Start Time The time that the slow query starts executing. Status The status of the slow query, including <code>running</code> and <code>killing</code>. User The user name to execute the query. Host The address and port of the server that the user connected to. Action Supports killing the slow query."},{"location":"nebula-dashboard-ent/4.cluster-operator/analysis-diagnosis/slow-query-analyst/#view_slow_query_history","title":"View slow query history","text":"<p>Clicking the History tab will display the slow query history. The parameters are described as follows.</p> Parameter Description nGQL The statement of the slow query. Duration(\u03bcs) The duration of the slow query. Category The type of the slow query statement, including <code>DDL</code>, <code>DQL</code>, <code>DML</code>, <code>DCL</code>, <code>UTIL</code> and <code>UNKNOWN</code>. Space The name of the graph space where the slow query was executed. Record Time The time to record the statement into memory as a slow query. Action Supports viewing the execution plan, making it easy for the DBA to optimize slow query statements based on the execution plan. <p>Note</p> <p>Turning off the slow query analyst function will not clear the slow query history.</p>"},{"location":"nebula-dashboard-ent/4.cluster-operator/cluster-information/audit-log/","title":"Audit log","text":"<p>The NebulaGraph audit logs store and categorize all operations performed on the Graph service. Dashboard Enterprise Edition allows you to quickly view audit logs.</p> <p>Enterpriseonly</p> <p>Only when the cluster you created or imported is the Enterprise Edition, this feature is available.</p>"},{"location":"nebula-dashboard-ent/4.cluster-operator/cluster-information/audit-log/#entry","title":"Entry","text":"<ol> <li>At the top navigation bar of the Dashboard Enterprise Edition page, click Cluster Management.</li> <li>On the right side of the target cluster, click Detail.</li> <li>On the left-side navigation bar of the page, click Information-&gt;Audit Log.</li> </ol> <p>Note</p> <ul> <li>To use the audit log for the first time, you need to jump to the Config Management page as prompts to enable the audit log and restart the graph service.</li> <li>For the description of audit log parameters, see Configure audit logs.</li> </ul>"},{"location":"nebula-dashboard-ent/4.cluster-operator/cluster-information/audit-log/#view_audit_log","title":"View audit log","text":"<p>In the upper corner of the page, you can filter services or search for the log name. Click View Log in the Operation column.</p> <ul> <li>Support copying all logs in the window with one click.</li> <li>Support copying the log file path.</li> <li>Support Tail Mode and Range Mode to view logs. You need to click Refresh after setting.</li> <li>Support searching logs by keywords (at least 3 characters).</li> </ul>"},{"location":"nebula-dashboard-ent/4.cluster-operator/cluster-information/job-management/","title":"Job management","text":"<p>Users can manage the jobs in a specified graph space through the Dashboard, including viewing, stopping, and recovering jobs, and supports viewing the details of a single job.</p> <p>Note</p> <p>How to run jobs, see Job manager and the JOB statements.</p>"},{"location":"nebula-dashboard-ent/4.cluster-operator/cluster-information/job-management/#prerequisites","title":"Prerequisites","text":"<ul> <li>The job management feature is available in NebulaGraph Enterprise Edition 3.4.0 and above versions.</li> <li>The job management feature is available in NebulaGraph Community Edition 3.3.0 and above versions.</li> </ul>"},{"location":"nebula-dashboard-ent/4.cluster-operator/cluster-information/job-management/#entry","title":"Entry","text":"<ol> <li>At the top navigation bar of the Dashboard Enterprise Edition page, click Cluster Management.</li> <li>On the right side of the target cluster, click Detail.</li> <li>On the left-side navigation bar of the page, click Information-&gt;Job Management.</li> <li>Select any online Graph service address, enter the account to log in to NebulaGraph (not the Dashboard login account), and the corresponding password.</li> <li>Select the target graph space at the upper left corner of the page.</li> </ol>"},{"location":"nebula-dashboard-ent/4.cluster-operator/cluster-information/job-management/#view_job","title":"View job","text":"<p>After you select the graph space, the page will display all the job information that has not expired by default. You can quickly find jobs through the filter box at the top of the page as follows:</p> <ul> <li>Select a job status for filtering. The status includes <code>QUEUE</code>, <code>RUNNING</code>, <code>FINISHED</code>, <code>FAILED</code>, <code>STOPPED</code>, and <code>SUCCEEDED</code>. For the status description, see Job manager and the JOB statements.</li> <li>Select a time range for filtering. You can view the job information of the maximum of 7 days by default. You can also select a time range or quickly select latest 12 hours, 1 day, 3 days, or 7 days.</li> <li>Select a <code>Job ID</code> or <code>Command</code> for filtering and enter what you want to search for.</li> <li>By default, the job information page will not be updated automatically. You can set the update frequency of the job information page globally or click the  button to update the page manually.</li> <li>Click <code>Detail</code> in the <code>Operation</code> column on the right side of the target job to view more information, including <code>Task ID</code>, <code>Host</code>, <code>Error Code</code>, etc.</li> </ul>"},{"location":"nebula-dashboard-ent/4.cluster-operator/cluster-information/job-management/#stop_job","title":"Stop job","text":"<p>Click <code>Stop Job</code> in the <code>Operation</code> column on the right side of the target job to stop an unfinished job. After clicking, the status of the job becomes <code>STOPPED</code>.</p>"},{"location":"nebula-dashboard-ent/4.cluster-operator/cluster-information/job-management/#recover_job","title":"Recover job","text":"<p>Click <code>Recover Job</code> in the <code>Operation</code> column on the right side of the target job to recover the job whose status is <code>FAILED</code> or <code>STOPPED</code>. After clicking, the status of the job becomes <code>RUNNING</code>.</p> <p>Note</p> <ul> <li>If there are multiple <code>BALANCE DATA</code> jobs in <code>STOPPED</code> status, only the latest one can be recovered.</li> <li>The completed job can not be recovered.</li> </ul>"},{"location":"nebula-dashboard-ent/4.cluster-operator/cluster-information/overview-info/","title":"Information overview","text":"<p>On the Overview Info page, you can see the information of the NebulaGraph cluster, including Storage leader distribution, Storage service details, versions and hosts information of each NebulaGraph service, and partition distribution and details.</p>"},{"location":"nebula-dashboard-ent/4.cluster-operator/cluster-information/overview-info/#entry","title":"Entry","text":"<ol> <li>At the top navigation bar of the Dashboard Enterprise Edition page, click Cluster Management.</li> <li>On the right side of the target cluster, click Detail.</li> <li>On the left-side navigation bar of the page, click Information-&gt;Overview Info.</li> </ol> <p>Note</p> <p>Before viewing the cluster information, you need to select any online Graph service address, enter the account to log in to NebulaGraph (not the Dashboard login account), and the corresponding password.</p> <p></p>"},{"location":"nebula-dashboard-ent/4.cluster-operator/cluster-information/overview-info/#storage_leader_distribution","title":"Storage Leader Distribution","text":"<p>In this section, the number of Leaders and the Leader distribution will be shown.</p> <ul> <li>Click the Balance Leader button in the upper right corner to distribute Leaders evenly and quickly in the NebulaGraph cluster. For details about the Leader, see Storage Service.</li> </ul> <ul> <li>Click Detail in the upper right corner to view the details of the Leader distribution.</li> </ul>"},{"location":"nebula-dashboard-ent/4.cluster-operator/cluster-information/overview-info/#version","title":"Version","text":"<p>In this section, the version and host information of each NebulaGraph service will be shown. Click Detail in the upper right corner to view the details of the version and host information.</p>"},{"location":"nebula-dashboard-ent/4.cluster-operator/cluster-information/overview-info/#service_information","title":"Service information","text":"<p>In this section, the information on Storage services will be shown. The parameter description is as follows:</p> Parameter Description <code>Host</code> The IP address of the host. <code>Port</code> The port of the host. <code>Status</code> The host status. <code>Git Info Sha</code> The commit ID of the current version. <code>Leader Count</code> The number of Leaders. <code>Partition Distribution</code> The distribution of partitions. <code>Leader Distribution</code> The distribution of Leaders. <p>Click Detail in the upper right corner to view the details of the Storage service information.</p>"},{"location":"nebula-dashboard-ent/4.cluster-operator/cluster-information/overview-info/#partition_distribution","title":"Partition Distribution","text":"<p>Select the specified graph space in the upper left corner, and then you can perform the following operations:</p> <ul> <li>View the distribution of partitions in the specified graph space. You can see the IP addresses and ports of all Storage services in the cluster, and the number of partitions in each Storage service.</li> <li>Click Balance Data to evenly distribute the partitions in the specified graph space.</li> <li>Click Balance Data Remove to migrate the partitions in the specified Storage service and distribute them evenly to the other Storage services in the cluster. The system will guide you to select the host IP where the specified Storage service is located.</li> </ul> <p>Click Detail in the upper right corner to view more details.</p>"},{"location":"nebula-dashboard-ent/4.cluster-operator/cluster-information/overview-info/#partition_information","title":"Partition information","text":"<p>In this section, the information on partitions will be shown. Before viewing the partition information, you need to select a graph space in the upper left corner. The parameter description is as follows:</p> Parameter Description <code>Partition ID</code> The ID of the partition. <code>Leader</code> The IP address and port of the leader. <code>Peers</code> The IP addresses and ports of all the replicas. <code>Losts</code> The IP addresses and ports of faulty replicas. <p>Click Detail in the upper right corner to view details. You can also enter the partition ID into the input box in the upper right corner of the details page to filter the shown data. </p>"},{"location":"nebula-dashboard-ent/4.cluster-operator/cluster-information/runtime-log/","title":"Runtime log","text":"<p>DBAs and developers can use runtime logs to investigate and identify issues when the system malfunctions. Dashboard Enterprise Edition allows you to quickly view runtime logs.</p>"},{"location":"nebula-dashboard-ent/4.cluster-operator/cluster-information/runtime-log/#entry","title":"Entry","text":"<ol> <li>At the top navigation bar of the Dashboard Enterprise Edition page, click Cluster Management.</li> <li>On the right side of the target cluster, click Detail.</li> <li>On the left-side navigation bar of the page, click Information-&gt;Runtime Log.</li> </ol> <p>Note</p> <p>For the description of runtime log parameters, see Runtime log.</p>"},{"location":"nebula-dashboard-ent/4.cluster-operator/cluster-information/runtime-log/#view_runtime_log","title":"View runtime log","text":"<p>In the upper corner of the page, you can filter services or search for the log name. Click View Log in the Operation column.</p> <ul> <li>Support copying all logs in the window with one click.</li> <li>Support copying the log file path.</li> <li>Support Tail Mode and Range Mode to view logs. You need to click Refresh after setting.</li> <li>Support searching logs by keywords (at least 3 characters).</li> </ul>"},{"location":"nebula-dashboard-ent/4.cluster-operator/operator/backup-and-restore/","title":"Back up and restore NebulaGraph data","text":"<p>To prevent data loss due to operational errors or system failures, NebulaGraph offers the Backup &amp; Restore (BR) tool to help users back up and restore graph data. Dashboard Enterprise Edition integrates BR capabilities and offers simple UIs that allow users to perform data backup and restore operations in just a few steps. This document describes how to use Dashboard Enterprise Edition to back up and restore NebulaGraph data.</p>"},{"location":"nebula-dashboard-ent/4.cluster-operator/operator/backup-and-restore/#limits","title":"Limits","text":"<ul> <li> <p>Currently, Dashboard only supports backup data to cloud storage services compatible with the S3 protocol (e.g. OSS, MinIO, Ceph RGW, etc.) and does not support local backups.</p> <p>Note</p> <p>To back up data to a local device, see What is Backup &amp; Restore.</p> </li> </ul> <ul> <li>Backup and restoration of space-level data are not supported.</li> <li>Backup data can only be restored to the original cluster, and cannot be restored across clusters.</li> <li>Breakpoint moving of backup and restore data is not supported.</li> <li>Currently, only the logs generated by backup and restore operations are supported.</li> </ul>"},{"location":"nebula-dashboard-ent/4.cluster-operator/operator/backup-and-restore/#prerequisites","title":"Prerequisites","text":"<ul> <li>A cluster is created with Dashboard.</li> <li>A cloud storage service that is compatible with the S3 protocol is activated and a storage bucket is created. For details, see the documentation for the corresponding cloud storage service.</li> </ul>"},{"location":"nebula-dashboard-ent/4.cluster-operator/operator/backup-and-restore/#steps","title":"Steps","text":""},{"location":"nebula-dashboard-ent/4.cluster-operator/operator/backup-and-restore/#entry","title":"Entry","text":"<ol> <li>In the top navigation bar, click Cluster Management.</li> <li>On the right side of the target cluster, click Detail. </li> <li>In the left navigation bar, click Operation-&gt;Backup&amp;Restore.</li> </ol>"},{"location":"nebula-dashboard-ent/4.cluster-operator/operator/backup-and-restore/#full_backup","title":"Full backup","text":""},{"location":"nebula-dashboard-ent/4.cluster-operator/operator/backup-and-restore/#backup_to_cloud_storage_compatible_with_s3","title":"Backup to cloud storage compatible with S3","text":"<p>Data is backed up to the cloud storage service by creating a backup file as follows.</p> <ol> <li>On the Backup&amp;Restore page, click the Backup List tab. </li> <li>In the upper right corner of the page, click S3 Service Settings. </li> <li> <p>Fill in the configuration information for the corresponding cloud storage service and click Submit.</p> Parameter Description s3.access_key The Access Key ID that is used to identify a user. For example, <code>AKIAI44QH8DHBxxxx</code>. s3.endpoint The domain URL of the entry point for the cloud storage service. For example, <code>https://s3.us-east-2.amazonaws.com</code>. The URL containing <code>bucket_name</code> is not supported, such as <code>https://{bucket_name}.s3.us-west-2.amazonaws.com</code>. s3.region The physical location of a data center. For example, <code>us-east-1</code>. s3.secret_key The Access Key Secret that is used to verify the identity of the user. For example, <code>je7MtGbClwBF/2Zp9Utk/h3yCoxxxx</code>. storage path The data storage path which must start with <code>s3</code>. For example, <code>s3://br-test/backup/</code>. <p>The following configurations are examples for Alibaba Cloud Object Storage Service and Amazon S3:</p> <ul> <li> <p>For Amazon S3:</p> <p></p> </li> </ul> <ul> <li> <p>For Alibaba Cloud Object Storage Service:</p> <p></p> </li> </ul> <p>Caution</p> <p>To back up data to OSS, you need to replace <code>oss</code> with <code>s3</code> for the OSS storage path. For example, change the original OSS path <code>oss://nebula-br-test/</code> to <code>s3://nebula-br-test/</code>.</p> </li> <li> <p>In the upper right corner of the page, click Create New Backup.</p> </li> <li>On the Create New Backup page, choose Full backup.</li> <li> <p>Click Environment check to check whether the relevant configurations are working properly, and then click Submit.</p> <p>Environment check includes:</p> <ul> <li>The NebulaGraph services are running.</li> <li>The cluster must have at least one space.</li> <li>The access key to log onto the storage service has not expired.</li> <li>The status of business traffic. It only checks if the QPS of your business is 0. When QPS is not 0, you are prompted to back up data during off-peak hours.</li> </ul> <p>Note</p> <p>You are unable to submit the backup when your cluster works abnormally or the access key to the storage service has expired.</p> </li> <li> <p>View the created backup file in the backup list.</p> <p></p> <p>Note</p> <p>You are unable to perform a new backup until the previous backup is completed. </p> </li> <li> <p>Check if the created backup file exists in the storage service.   Successfully created backup files are stored to the storage path set above, like <code>s3://nebula-br-test</code>.</p> <ul> <li>Amazon S3: </li> <li>Alibaba Cloud Object Storage Service: </li> </ul> <p>Danger</p> <p>Do not modify the file name and storage path of backup files, otherwise, the backup data cannot be restored to the cluster.</p> </li> </ol>"},{"location":"nebula-dashboard-ent/4.cluster-operator/operator/backup-and-restore/#backup_to_local","title":"Backup to local","text":"<p>Users can save the backup data locally by creating a backup file, the operation is as follows.</p> <ol> <li>In the upper right corner of the page, click Create New Backup.</li> <li>On the Create New Backup page, choose Full backup.</li> <li>In Backup type, choose Local.</li> <li>Enter the storage path of the backup file in Storage Path.</li> <li> <p>Click Environment check to check whether the relevant configurations are working properly, and then click Submit.</p> <p>Environment check includes:</p> <ul> <li>The NebulaGraph services are running.</li> <li>The cluster must have at least one space.</li> <li>The status of business traffic. It only checks if the QPS of your business is 0. When QPS is not 0, you are prompted to back up data during off-peak hours.</li> </ul> </li> <li> <p>View the created backup file in the backup list.</p> <p>Note</p> <p>You are unable to perform a new backup until the previous backup is completed. </p> </li> <li> <p>Check if the created backup file exists in the storage path. Successfully created backup files are stored in the storage path set above.</p> <p>Danger</p> <p>Do not modify the file name and storage path of backup files, otherwise, the backup data cannot be restored to the cluster.</p> </li> </ol>"},{"location":"nebula-dashboard-ent/4.cluster-operator/operator/backup-and-restore/#incremental_backup","title":"Incremental backup","text":"<p>Users can perform incremental backup based on existing backup files. Incremental backup only covers all files that have changed or been modified since the last backup was made.</p> <ol> <li>On the Backup&amp;Restore page, click the Backup List tab. </li> <li>In the upper right corner of the page, click Create New Backup.</li> <li>On the Create New Backup page, choose Incremental backup.</li> <li> <p>Choose Base Backup Name.</p> <p>Note</p> <p>New data will be backed up based on this base backup. Make sure that the data of the base backup is not changed. Otherwise, the incremental backup may fail.</p> </li> <li> <p>Click Environment check to check whether the relevant configurations are working properly, and then click Submit.</p> <p>Environment check includes:</p> <ul> <li>Your NebulaGraph cluster is running.</li> <li>The access key to log onto the storage service has not expired.</li> <li>The status of business traffic. It only checks if the QPS of your business is 0. When QPS is not 0, you are prompted to back up data during off-peak hours.</li> </ul> <p>Note</p> <p>You are unable to submit the backup when your cluster works abnormally or the access key to the storage service has expired.</p> </li> </ol>"},{"location":"nebula-dashboard-ent/4.cluster-operator/operator/backup-and-restore/#restore_data","title":"Restore data","text":"<p>You can restore the backed-up data stored in the cloud storage service to the original cluster.</p> <p>Caution</p> <ul> <li>Before restoring the data, please make sure that the name and storage path of the backup file stored in the cloud storage service are not changed, otherwise, the data restoration will fail.</li> <li>During the data restoration process, all data in the cluster is removed and replaced with the data in the backup file.</li> <li>The restoration process is executed offline, and you cannot perform other operations during the data restoration process.</li> </ul> <p>Follow the steps below to restore data.</p> <ol> <li>On the Backup&amp;Restore page, click the Backup list tab.</li> <li>To the right of the target backup file, click Restore.</li> <li> <p>Click Environment check, and when the environment check is passed, click Submit.</p> <p>Environment check includes:</p> <ul> <li>Your NebulaGraph cluster is running.</li> <li>The access key to log onto the storage service has not expired.</li> <li>No business website traffic. </li> </ul> </li> <li> <p>On the Restore record list page, view restoration records.   </p> <ul> <li>Restoration records cannot be deleted. </li> <li>The list page displays restoration records created within 30 days.</li> <li>The list page displays the restoration ID, backup file name, status, time, graph space, storage path, operator, and the log generated by the restoration operation.</li> <li> <p>The restoration status includes <code>running</code>, <code>success</code>, and <code>failed</code>.</p> <p>Note</p> <p>You're unable to restore the backup data until the previous restoration is complete.</p> </li> </ul> <ul> <li>You can filter restoration records by creation time and status, or search backup file names for restoration records.</li> </ul> </li> </ol>"},{"location":"nebula-dashboard-ent/4.cluster-operator/operator/backup-and-restore/#view_backuprestore_progress","title":"View backup/restore progress","text":"<p>After the backup or restoration has started, you can click View Log in the Operation column of the corresponding task to view the progress.</p>"},{"location":"nebula-dashboard-ent/4.cluster-operator/operator/config-management/","title":"Config Management","text":"<p>On Config Management page, you can view and update the service configuration files.</p>"},{"location":"nebula-dashboard-ent/4.cluster-operator/operator/config-management/#precautions","title":"Precautions","text":"<p>You need to restart the corresponding service in the Service page after the configuration modification. For details, see Service.</p>"},{"location":"nebula-dashboard-ent/4.cluster-operator/operator/config-management/#entry","title":"Entry","text":"<ol> <li>At the top navigation bar of the Dashboard Enterprise Edition page, click Cluster Management.</li> <li>On the right side of the target cluster, click Detail.</li> <li>On the left-side navigation bar of the page, click Operation-&gt;Config Management.</li> </ol>"},{"location":"nebula-dashboard-ent/4.cluster-operator/operator/config-management/#modify_configuration","title":"Modify configuration","text":"<ol> <li>Select the type of service whose configuration you want to modify.</li> <li>Locate the configuration to be modified and click Edit in the Operation column.</li> <li> <p>In the pop-up dialog box, you can modify the Value individually. They can also be modified uniformly at the top, and you need to click Apply To All Services after modification.</p> <p></p> </li> <li> <p>Click Confirm after the modification is complete.</p> </li> </ol>"},{"location":"nebula-dashboard-ent/4.cluster-operator/operator/config-management/#add_configuration","title":"Add configuration","text":"<p>If you need to adjust a parameter that is not included in the configuration file, you need to add the configuration first.</p> <ol> <li>Click Add Config at the top left.</li> <li>Fill in the parameter name in Config Key, then fill in the Config Value, and apply the config value to all services. You can also adjust the value for individual services below.</li> <li>Click Confirm.</li> </ol>"},{"location":"nebula-dashboard-ent/4.cluster-operator/operator/config-management/#delete_configuration","title":"Delete configuration","text":"<p>Note</p> <p>After deleting the configuration and restarting the service, the corresponding configuration will be restored to its default value.</p> <ol> <li>Select the type of service whose configuration you want to delete.</li> <li>Locate the configuration to be deleted and click Delete in the Operation column, and then Click OK.</li> </ol>"},{"location":"nebula-dashboard-ent/4.cluster-operator/operator/member-management/","title":"Member management","text":"<p>Member Management page shows only the cluster creator account (<code>owner</code> role) by default. The account with the <code>owner</code> role can add and delete the cluster administrator (<code>operator</code> role).</p>"},{"location":"nebula-dashboard-ent/4.cluster-operator/operator/member-management/#entry","title":"Entry","text":"<ol> <li>At the top navigation bar of the Dashboard Enterprise Edition page, click Cluster Management.</li> <li>On the right side of the target cluster, click Detail.</li> <li>On the left-side navigation bar of the page, click Operation-&gt;Member Management.</li> </ol>"},{"location":"nebula-dashboard-ent/4.cluster-operator/operator/member-management/#steps","title":"Steps","text":"<ul> <li> <p>Add the cluster administrator: Click the search box at the top left. Select the target account that you want to add to be the administrator of the cluster in the drop-down list, and then click Add.</p> <p>Note</p> <p>The accounts of cluster members must be included in Dashboard accounts. For information about how to create an account, see Authority management.</p> </li> </ul> <ul> <li>Delete the cluster administrator: Click  in the operation column on the right of the cluster administrator account, and then click Confirm.</li> </ul> <ul> <li>Transfer the <code>owner</code> role: Click Transfer in the operation column on the right of the <code>owner</code> role. Select the target account that you want to be transferred, and then click Confirm.</li> </ul>"},{"location":"nebula-dashboard-ent/4.cluster-operator/operator/node/","title":"Node","text":"<p>On this page, the information of all nodes will be shown, including the cluster name, Host(SSH_User), CPU (Core), etc. Users can add nodes, view node monitoring, and manage services on the node.</p>"},{"location":"nebula-dashboard-ent/4.cluster-operator/operator/node/#entry","title":"Entry","text":"<ol> <li>At the top navigation bar of the Dashboard Enterprise Edition page, click Cluster Management.</li> <li>On the right side of the target cluster, click Detail.</li> <li>On the left-side navigation bar of the page, click Operation-&gt;Node.</li> </ol>"},{"location":"nebula-dashboard-ent/4.cluster-operator/operator/node/#add_node","title":"Add node","text":"<p>Click Add Node and enter the following information, the Host, SSH port, SSH user, authentication type, NebulaGraph package, etc., and click OK.</p> <p>The authentication type is described as follows:</p> <ul> <li>SSH Password: Enter the password of the SSH user.</li> </ul> <ul> <li>SSH Key: Click Upload and select the private key file of the node. You need to generate the secret key files on the node to be added and send the private key file to the current computer (not the machine where Dashboard is deployed). If the passphrase is set, this parameter is also required.</li> </ul> <p>Note</p> <p>After a node is added, data is not automatically imbalanced. You need to select the target graph space on the Overview Info page and then perform the <code>Balance Data</code> and <code>Balance Leader</code> operations.</p>"},{"location":"nebula-dashboard-ent/4.cluster-operator/operator/node/#other_node_operations","title":"Other node operations","text":"<p>Click the  button to view the process name, service type, status, and runtime directory of the corresponding node.</p> <ul> <li>Click Node Monitoring to jump to the detailed node monitoring page. For more information, see Cluster monitoring.</li> </ul> <ul> <li>Click Service Management to jump to the service management page.</li> </ul> <ul> <li>Click Edit Node to modify the node settings.</li> </ul> <ul> <li>If a node has no service, you can Delete Node. For details about how to delete a service, see section Scale below.</li> </ul>"},{"location":"nebula-dashboard-ent/4.cluster-operator/operator/scale/","title":"Scale","text":"<p>On the Scale page, you can add node and import node in batches quickly, and add Graph services and Storage services to the existing nodes.</p> <p>Enterpriseonly</p> <p>Only when the cluster you created or imported is the Enterprise Edition, this feature is available.</p>"},{"location":"nebula-dashboard-ent/4.cluster-operator/operator/scale/#entry","title":"Entry","text":"<ol> <li>At the top navigation bar of the Dashboard Enterprise Edition page, click Cluster Management.</li> <li>On the right side of the target cluster, click Detail.</li> <li>On the left-side navigation bar of the page, click Operation-&gt;Scale.</li> </ol>"},{"location":"nebula-dashboard-ent/4.cluster-operator/operator/scale/#steps","title":"Steps","text":""},{"location":"nebula-dashboard-ent/4.cluster-operator/operator/scale/#add_node","title":"Add node","text":"<p>See Node.</p> <p>Note</p> <p>After a node is added, data is not automatically imbalanced. You need to select the target graph space on the Overview Info page and then perform the <code>Balance Data</code> and <code>Balance Leader</code> operations.</p>"},{"location":"nebula-dashboard-ent/4.cluster-operator/operator/scale/#batch_import_of_node","title":"Batch import of node","text":"<p>Download and fill in the CSV template file, then upload the file and select the installation package. Click OK to import nodes in batches.</p>"},{"location":"nebula-dashboard-ent/4.cluster-operator/operator/scale/#modify_services","title":"Modify services","text":"<ol> <li>Select the nodes in the node list. Click the service to be added in the upper right corner of the list, or click X on the label of the service to be deleted in the Service type column in the list.</li> <li> <p>Confirm the modified service in the Service display area below. You can modify the port, HTTP port, and HTTP2 port of the newly added service.</p> <p>Note</p> <p>Green indicates services that will be added soon, and red indicates services that will be removed.</p> </li> <li> <p>Click OK at the bottom of the page.</p> </li> </ol> <p>Caution</p> <ul> <li>Currently, you can dynamically scale Storaged and Graphd services through Dashboard. The Metad service cannot be scaled. When scaling a cluster, it is recommended to back up data in advance so that data can be rolled back when scaling fails. For more information, see FAQ.</li> </ul> <ul> <li>Make sure that services of the same type are not deployed on the same node, and that at least one of each type of service is deployed in the cluster.</li> </ul> <ul> <li>Before removing the storage service, you must migrate the data stored on the node. You need to perform the <code>Balance Data Remove</code> operation on the Overview Info page.</li> </ul> <p>In this example, storage services with nodes <code>192.168.8.143</code> and <code>192.168.8.167</code> are added, and Graph services with node <code>192.168.8.169</code> are deleted. If the box is dotted and the service name is greyed, it means the service is removed. If the box is solid, it means the service is newly added.</p> <p></p>"},{"location":"nebula-dashboard-ent/4.cluster-operator/operator/scale/#reset","title":"Reset","text":"<p>Click the Reset button to cancel all uncommitted operations and restore them to the initial state.</p>"},{"location":"nebula-dashboard-ent/4.cluster-operator/operator/service/","title":"Service","text":"<p>On Service page, you can view the host, path, and status of the services, and start, stop, kill, or restart the services. In addition, you can easily and quickly view the contents of the log file.</p>"},{"location":"nebula-dashboard-ent/4.cluster-operator/operator/service/#entry","title":"Entry","text":"<ol> <li>At the top navigation bar of the Dashboard Enterprise Edition page, click Cluster Management.</li> <li>On the right side of the target cluster, click Detail.</li> <li>On the left-side navigation bar of the page, click Operation-&gt;Service.</li> </ol>"},{"location":"nebula-dashboard-ent/4.cluster-operator/operator/service/#steps","title":"Steps","text":"<p>Danger</p> <p>If you click Stop/Restart, the running task will be stopped instantly, which may cause data inconsistency. It is recommended to perform this operation during the low peak period of the business.</p> <ul> <li>Locate the target service and perform the related operation in the Operation column.</li> </ul> <ul> <li>Select multiple services and perform batch operations at the upper corner of the page.</li> </ul> <ul> <li>Click the  icon to quickly view the service monitoring information.</li> </ul> <ul> <li>When synchronizing data, you can view and manage related services on the Dependency page. For details about data synchronization, see Synchronize between two clusters.</li> </ul>"},{"location":"nebula-dashboard-ent/4.cluster-operator/operator/version-upgrade/","title":"Version upgrade","text":"<p>NebulaGraph Dashboard Enterprise Edition supports upgrading the version of the existing NebulaGraph cluster.</p> <p>Caution</p> <ul> <li>During the upgrade, the cluster will replace binary files. The upgrade speed is fast, but the cluster will still be stopped and restarted.</li> <li>Automatic rollback is not supported. Users can manually upgrade the cluster again when the upgrade failed.</li> <li>The upgrade cannot be stopped or canceled.</li> </ul> <p>Note</p> <ul> <li>Only supports upgrading the NebulaGraph cluster that version greater than 3.0.0 to the version equal to or lower than 3.2.1. To upgrade to 3.3.0, see manual upgrade. </li> <li>Do not support upgrading clusters across the major version.</li> <li>The community edition can be upgraded to the enterprise edition, and the enterprise edition can be upgraded to the community edition.</li> <li>The cluster can be upgraded to a minor version in the current major version, including a smaller version than the current minor version.</li> </ul> <ol> <li>At the top navigation bar of the Dashboard Enterprise Edition page, click Cluster Management.</li> <li>On the right side of the target cluster, click Detail.</li> <li>On the left-side navigation bar of the page, click Operation-&gt;Version Upgrade.</li> <li> <p>On the Version Upgrade page, confirm Current NebulaGraph version and select the upgrade version.</p> <p>Note</p> <p>If you do not find the suitable version, click Package Management to download or upload the required version installation package. For details, see Package management.</p> <p>Note</p> <p>When upgrading Enterprise Edition clusters with version below 3.5.0, you need to click Upload Certificate to upload license.</p> </li> <li> <p>Click Next to perform the upgrade check, and then click Next.</p> <p>The cluster will be shut down during the upgrade and automatically restart the services after the upgrade. You can use the diagnostics report to help you judge whether the timing to upgrade is suitable.</p> </li> <li> <p>Confirm the upgrade information again, including Cluster Name, Current NebulaGraph Version, and Upgrade NebulaGraph Version, and then click Upgrade.    Users can view the upgrade task information in task center, the task type is <code>version update</code>.</p> </li> </ol>"},{"location":"nebula-dashboard-ent/system-settings/manage-package/","title":"Package management","text":"<p>NebulaGraph Dashboard Enterprise Edition supports managing NebulaGraph installation packages, such as downloading the community edition installation packages or manually uploading the installation packages.</p>"},{"location":"nebula-dashboard-ent/system-settings/manage-package/#precautions","title":"Precautions","text":"<ul> <li>Only the admin user can manage the installation package.</li> </ul> <ul> <li>Do not support downloading enterprise edition installation packages. For downloading Enterprise Edition packages, please contact us.</li> </ul>"},{"location":"nebula-dashboard-ent/system-settings/manage-package/#entry","title":"Entry","text":"<ol> <li>At the top navigation bar of the Dashboard Enterprise Edition page, click System Settings.</li> <li>On the left-side navigation bar of the page, click Package Management.</li> </ol>"},{"location":"nebula-dashboard-ent/system-settings/manage-package/#steps","title":"Steps","text":""},{"location":"nebula-dashboard-ent/system-settings/manage-package/#view_packages","title":"View packages","text":"<p>The list of existing installation packages are displayed on the right-side, showing the package name, version, size, and created time.</p> <p>Users can filter packages through the search box in the upper right corner.</p>"},{"location":"nebula-dashboard-ent/system-settings/manage-package/#download_packages","title":"Download packages","text":"<ol> <li> <p>Click Download Package, select the installation package you want to download. The description are as follows:</p> <p>Note</p> <p>If you download an existing installation package, the system will prompt you to overwrite the existing installation package.</p> <ul> <li>Version: Supports stable versions later than v2.5. It is recommended to use the latest version.</li> <li>Platform: Supports CentOS 7/8 and Ubuntu 1604/1804/2004.</li> <li>Package Type: Supports RPM, DEB and tar.gz.</li> </ul> </li> <li> <p>Click Download.</p> </li> </ol> <p>Users can view the download task information in task center, the task type is <code>package download</code>. If the task status is <code>success</code>, users can return to the Package Management page to view the newly downloaded installation package.</p>"},{"location":"nebula-dashboard-ent/system-settings/manage-package/#upload_packages","title":"Upload packages","text":"<p>If the required installation package is not listed in the downloaded list, users can manually upload installation packages, such as upload an enterprise edition installation package.</p> <p>Click Upload Package, select the local installation package you want to upload. The package type can be RPM, DEB, or tar.gz. View the upload progress on the upper of the page and wait until the upload is complete.</p> <p>Users can view the upload task information in task center, the task type is <code>package upload</code>. If the task status is <code>success</code>, users can return to the Package Management page to view the newly uploaded installation package.</p>"},{"location":"nebula-dashboard-ent/system-settings/manage-package/#delete_packages","title":"Delete packages","text":"<p>In the operation column of the target installation package, click Delete and confirm.</p>"},{"location":"nebula-dashboard-ent/system-settings/manage-package/#faq","title":"FAQ","text":""},{"location":"nebula-dashboard-ent/system-settings/manage-package/#how_to_resolve_the_error_request_entity_too_large","title":"How to resolve the error <code>Request Entity Too Large</code>?","text":"<p>If users use Nginx as the reverse proxy, the default limit for uploaded files is 1 MB. Add <code>client_max_body_size 200m;</code> to the <code>http{}</code> section of <code>nginx.conf</code>, that means the file of up to 200 MB is allowed to be uploaded.</p>"},{"location":"nebula-dashboard-ent/system-settings/notification-endpoint/","title":"Notification endpoint","text":"<p>On Notification Endpoint page, you can set the notification mail and webhook.</p>"},{"location":"nebula-dashboard-ent/system-settings/notification-endpoint/#entry","title":"Entry","text":"<ol> <li>At the top navigation bar of the Dashboard Enterprise Edition page, click System Settings.</li> <li>On the left-side navigation bar of the page, click Notification Endpoint.</li> </ol>"},{"location":"nebula-dashboard-ent/system-settings/notification-endpoint/#steps","title":"Steps","text":""},{"location":"nebula-dashboard-ent/system-settings/notification-endpoint/#mail","title":"Mail","text":"<p>Dashboard supports sending and receiving alert messages for all clusters via E-mail.</p> <ul> <li> <p>You need to set the following parameters to send alert messages.</p> Parameter Description SMTP Server Address The SMTP server address corresponding to yourmailbox. Port The port number of the SMTP server corresponding to yourmailbox. Use SSL Check the box to enable SSL for encrypted datatransmission. SMTP User Name The SMTP server account name. SMTP Password The SMTP server password. Sender Email The email address of the one who sent you the email. </li> </ul> <ul> <li> <p>You need to set a receiver to receive alert messages.</p> Parameter Description Receiver Set the email address to receive alert messages. This email address will receive alert messages for all clusters created on Dashboard. </li> </ul>"},{"location":"nebula-dashboard-ent/system-settings/notification-endpoint/#webhook","title":"Webhook","text":"<p>Dashboard supports configuring Webhook to bring all cluster alert messages into third-party projects.</p> <p>On the left-side navigation bar of the System Settings page, click Notification Endpoints-&gt;Webhook to input the Webhook URL and Webhook request body (optional) used to receive alert messages. You can turn on or off the Webhook feature at the top right of the page.</p>"},{"location":"nebula-dashboard-ent/system-settings/single-sign-on/","title":"Single sign-on","text":"<p>NebulaGraph Dashboard Enterprise Edition supports general accounts, LDAP accounts, and OAuth2.0 accounts. This article introduces how to configure the protocols of LDAP and OAuth2.0.</p> <p>Note</p> <ul> <li>After the configuration is complete, you can create the account and activate the invitation. For details\uff0csee Authority management.</li> <li>You can quickly switch on or off LDAP or OAuth2.0 in the left navigation bar.</li> </ul>"},{"location":"nebula-dashboard-ent/system-settings/single-sign-on/#ldap_configuration","title":"LDAP configuration","text":""},{"location":"nebula-dashboard-ent/system-settings/single-sign-on/#entry","title":"Entry","text":"<ol> <li>At the top navigation bar of the Dashboard Enterprise Edition page, click System Settings.</li> <li>On the left-side navigation bar of the page, click Single Sign-on-&gt;LDAP.</li> </ol>"},{"location":"nebula-dashboard-ent/system-settings/single-sign-on/#configuration_description","title":"Configuration description","text":"Parameter Example Description <code>LDAP Server Address</code> <code>ldap://192.168.10.100</code> The LDAP server address. <code>Bind DN</code> <code>cn=admin,dc=vesoft,dc=com</code> The LDAP login username. <code>Password</code> <code>123456</code> The LDAP login password. <code>Base DN</code> <code>dc=vesoft,dc=com</code> Set the path to query user data. <code>User Filter</code> <code>&amp;(objectClass=*)</code> Set a filter to LDAP search queries. <code>Email Key</code> <code>mail</code> Set the field name used to restore email in LDAP."},{"location":"nebula-dashboard-ent/system-settings/single-sign-on/#instruction","title":"Instruction","text":"<p>After LDAP is enabled, you can register an LDAP account in two ways:</p> <ul> <li>Email invitation: When creating an account on the Members page, you can invite others to register by email. The advantage is that you can set the role permissions of the account.</li> </ul> <ul> <li>Automatic registration: When you enter an unregistered account in LDAP mode on the login page, the Dashboard automatically registers the account, but the role permission is <code>user</code>.</li> </ul>"},{"location":"nebula-dashboard-ent/system-settings/single-sign-on/#oauth20_configuration","title":"OAuth2.0 configuration","text":"<p>Caution</p> <p>The feature is still in beta. It will continue to be optimized.</p>"},{"location":"nebula-dashboard-ent/system-settings/single-sign-on/#entry_1","title":"Entry","text":"<ol> <li>At the top navigation bar of the Dashboard Enterprise Edition page, click System Settings.</li> <li>On the left-side navigation bar of the page, click Single Sign-on-&gt;OAuth2.0.</li> </ol>"},{"location":"nebula-dashboard-ent/system-settings/single-sign-on/#configuration_description_1","title":"Configuration description","text":"Parameter Example Description <code>ClientID</code> <code>4953xxx-mmnoge13xx.apps.googleusercontent.com</code> The application's ClientId. <code>ClientSecret</code> <code>GOCxxx-xaytomFexxx</code> The application's ClientSecret. <code>RedirectURL</code> <code>http://dashboard.vesoft-inc.com/login</code> The URL that redirects to Dashboard. <code>AuthURL</code> <code>https://accounts.google.com/o/oauth2/auth</code> The URL used for authentication. <code>TokenURL</code> <code>https://oauth2.googleapis.com/token</code> The URL used to get the access_token. <code>UserInfoURL</code> <code>https://www.googleapis.com/oauth2/v1/userinfo</code> The URL used to get the user information. <code>Username Key</code> <code>email</code> The key of user name. <code>Organization</code> <code>vesoft company</code> The organization name. <code>Requested scopes for OAuth</code> <code>email</code> Scope of OAuth permissions. The scope of permissions needs to be a subset of the scope configured by the vendor's OAuth2.0 platform, otherwise, the request will fail. Make sure the <code>Username Key</code> is accessible within the requested scope."},{"location":"nebula-dashboard-ent/system-settings/single-sign-on/#instruction_1","title":"Instruction","text":"<p>After OAuth2.0 is enabled, you can invite others to register by email.</p>"},{"location":"nebula-dashboard-ent/system-settings/system-settings/","title":"System settings","text":"<p>On the System Settings page, you can modify the page title, logo image, and cover image, and also switch language or help prompts.</p>"},{"location":"nebula-dashboard-ent/system-settings/system-settings/#entry","title":"Entry","text":"<ol> <li>At the top navigation bar of the Dashboard Enterprise Edition page, click System Settings.</li> <li>On the left-side navigation bar of the page, click System Settings.</li> </ol>"},{"location":"nebula-dashboard-ent/system-settings/system-settings/#steps","title":"Steps","text":"<ul> <li>After modifying the page title, logo image, and cover image, click Save.</li> <li>Change the display language. Currently, only Chinese and English are supported.</li> <li> <p>Turn on or off help tips. An example of tips is as follows.</p> <p></p> </li> </ul>"},{"location":"nebula-exchange/ex-ug-FAQ/","title":"Exchange FAQ","text":""},{"location":"nebula-exchange/ex-ug-FAQ/#compilation","title":"Compilation","text":""},{"location":"nebula-exchange/ex-ug-FAQ/#q_some_packages_not_in_central_repository_failed_to_download_error_could_not_resolve_dependencies_for_project_xxx","title":"Q: Some packages not in central repository failed to download, error: <code>Could not resolve dependencies for project xxx</code>","text":"<p>Please check the <code>mirror</code> part of Maven installation directory <code>libexec/conf/settings.xml</code>:</p> <pre><code>&lt;mirror&gt;\n    &lt;id&gt;alimaven&lt;/id&gt;\n    &lt;mirrorOf&gt;central&lt;/mirrorOf&gt;\n    &lt;name&gt;aliyun maven&lt;/name&gt;\n    &lt;url&gt;http://maven.aliyun.com/nexus/content/repositories/central/&lt;/url&gt;\n&lt;/mirror&gt;\n</code></pre> <p>Check whether the value of <code>mirrorOf</code> is configured to <code>*</code>. If it is, change it to <code>central</code> or <code>*,!SparkPackagesRepo,!bintray-streamnative-maven</code>.</p> <p>Reason: There are two dependency packages in Exchange's <code>pom.xml</code> that are not in Maven's central repository. <code>pom.xml</code> configures the repository address for these two dependencies. If the <code>mirrorOf</code> value for the mirror address configured in Maven is <code>*</code>, all dependencies will be downloaded from the Central repository, causing the download to fail.</p>"},{"location":"nebula-exchange/ex-ug-FAQ/#q_unable_to_download_snapshot_packages_when_compiling_exchange","title":"Q: Unable to download SNAPSHOT packages when compiling Exchange","text":"<p>Problem description: The system reports <code>Could not find artifact com.vesoft:client:jar:xxx-SNAPSHOT</code> when compiling.</p> <p>Cause: There is no local Maven repository for storing or downloading SNAPSHOT packages. The default central repository in Maven only stores official releases, not development versions (SNAPSHOT).</p> <p>Solution: Add the following configuration in the <code>profiles</code> scope of Maven's <code>setting.xml</code> file:</p> <pre><code>  &lt;profile&gt;\n     &lt;activation&gt;\n        &lt;activeByDefault&gt;true&lt;/activeByDefault&gt;\n     &lt;/activation&gt;\n     &lt;repositories&gt;\n        &lt;repository&gt;\n            &lt;id&gt;snapshots&lt;/id&gt;\n            &lt;url&gt;https://oss.sonatype.org/content/repositories/snapshots/&lt;/url&gt;\n            &lt;snapshots&gt;\n               &lt;enabled&gt;true&lt;/enabled&gt;\n            &lt;/snapshots&gt;\n      &lt;/repository&gt;\n     &lt;/repositories&gt;\n  &lt;/profile&gt;\n</code></pre>"},{"location":"nebula-exchange/ex-ug-FAQ/#execution","title":"Execution","text":""},{"location":"nebula-exchange/ex-ug-FAQ/#q_error_javalangclassnotfoundexception_comvesoftnebulaexchangeexchange","title":"Q: Error: <code>java.lang.ClassNotFoundException: com.vesoft.nebula.exchange.Exchange</code>","text":"<p>To submit a task in Yarn-Cluster mode, run the following command, especially the two '--conf' commands in the example.</p> <pre><code>$SPARK_HOME/bin/spark-submit --class com.vesoft.nebula.exchange.Exchange \\\n--master yarn-cluster \\\n--files application.conf \\\n--conf spark.driver.extraClassPath=./ \\\n--conf spark.executor.extraClassPath=./ \\\nnebula-exchange-3.0.0.jar \\\n-c application.conf\n</code></pre>"},{"location":"nebula-exchange/ex-ug-FAQ/#q_error_method_name_xxx_not_found","title":"Q: Error: <code>method name xxx not found</code>","text":"<p>Generally, the port configuration is incorrect. Check the port configuration of the Meta service, Graph service, and Storage service.</p>"},{"location":"nebula-exchange/ex-ug-FAQ/#q_error_nosuchmethod_methodnotfound_exception_in_thread_main_javalangnosuchmethoderror_etc","title":"Q: Error: NoSuchMethod, MethodNotFound (<code>Exception in thread \"main\" java.lang.NoSuchMethodError</code>, etc)","text":"<p>Most errors are caused by JAR package conflicts or version conflicts. Check whether the version of the error reporting service is the same as that used in Exchange, especially Spark, Scala, and Hive.</p>"},{"location":"nebula-exchange/ex-ug-FAQ/#q_when_exchange_imports_hive_data_error_exception_in_thread_main_orgapachesparksqlanalysisexception_table_or_view_not_found","title":"Q: When Exchange imports Hive data, error: <code>Exception in thread \"main\" org.apache.spark.sql.AnalysisException: Table or view not found</code>","text":"<p>Check whether the <code>-h</code> parameter is omitted in the command for submitting the Exchange task and whether the table and database are correct, and run the user-configured exec statement in spark-SQL to verify the correctness of the exec statement.</p>"},{"location":"nebula-exchange/ex-ug-FAQ/#q_run_error_comfacebookthriftprotocoltprotocolexception_expected_protocol_id_xxx","title":"Q: Run error: <code>com.facebook.thrift.protocol.TProtocolException: Expected protocol id xxx</code>","text":"<p>Check that the NebulaGraph service port is configured correctly.</p> <ul> <li>For source, RPM, or DEB installations, configure the port number corresponding to <code>--port</code> in the configuration file for each service.</li> </ul> <ul> <li>For docker installation, configure the docker mapped port number as follows:<p>Execute <code>docker-compose ps</code> in the <code>nebula-docker-compose</code> directory, for example:</p> <pre><code>$ docker-compose ps\n              Name                             Command                  State                                                         Ports\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nnebula-docker-compose_graphd_1      /usr/local/nebula/bin/nebu ...   Up (healthy)   0.0.0.0:33205-&gt;19669/tcp, 0.0.0.0:33204-&gt;19670/tcp, 0.0.0.0:9669-&gt;9669/tcp\nnebula-docker-compose_metad0_1      ./bin/nebula-metad --flagf ...   Up (healthy)   0.0.0.0:33165-&gt;19559/tcp, 0.0.0.0:33162-&gt;19560/tcp, 0.0.0.0:33167-&gt;9559/tcp, 9560/tcp\nnebula-docker-compose_metad1_1      ./bin/nebula-metad --flagf ...   Up (healthy)   0.0.0.0:33166-&gt;19559/tcp, 0.0.0.0:33163-&gt;19560/tcp, 0.0.0.0:33168-&gt;9559/tcp, 9560/tcp\nnebula-docker-compose_metad2_1      ./bin/nebula-metad --flagf ...   Up (healthy)   0.0.0.0:33161-&gt;19559/tcp, 0.0.0.0:33160-&gt;19560/tcp, 0.0.0.0:33164-&gt;9559/tcp, 9560/tcp\nnebula-docker-compose_storaged0_1   ./bin/nebula-storaged --fl ...   Up (healthy)   0.0.0.0:33180-&gt;19779/tcp, 0.0.0.0:33178-&gt;19780/tcp, 9777/tcp, 9778/tcp, 0.0.0.0:33183-&gt;9779/tcp, 9780/tcp\nnebula-docker-compose_storaged1_1   ./bin/nebula-storaged --fl ...   Up (healthy)   0.0.0.0:33175-&gt;19779/tcp, 0.0.0.0:33172-&gt;19780/tcp, 9777/tcp, 9778/tcp, 0.0.0.0:33177-&gt;9779/tcp, 9780/tcp\nnebula-docker-compose_storaged2_1   ./bin/nebula-storaged --fl ...   Up (healthy)   0.0.0.0:33184-&gt;19779/tcp, 0.0.0.0:33181-&gt;19780/tcp, 9777/tcp, 9778/tcp, 0.0.0.0:33185-&gt;9779/tcp, 9780/tcp\n</code></pre> <p>Check the <code>Ports</code> column to find the docker mapped port number, for example:</p> <p>- The port number available for Graph service is 9669.</p> <p>- The port number for Meta service are 33167, 33168, 33164.</p> <p>- The port number for Storage service are 33183, 33177, 33185.</p> </li> </ul>"},{"location":"nebula-exchange/ex-ug-FAQ/#q_error_exception_in_thread_main_comfacebookthriftprotocoltprotocolexception_the_field_code_has_been_assigned_the_invalid_value_-4","title":"Q: Error: <code>Exception in thread \"main\" com.facebook.thrift.protocol.TProtocolException: The field 'code' has been assigned the invalid value -4</code>","text":"<p>Check whether the version of Exchange is the same as that of NebulaGraph. For more information, see Limitations.</p>"},{"location":"nebula-exchange/ex-ug-FAQ/#q_how_to_correct_the_messy_code_when_importing_hive_data_into_nebulagraph","title":"Q: How to correct the messy code when importing Hive data into NebulaGraph?","text":"<p>It may happen if the property value of the data in Hive contains Chinese characters. The solution is to add the following options before the JAR package path in the import command:</p> <pre><code>--conf spark.driver.extraJavaOptions=-Dfile.encoding=utf-8\n--conf spark.executor.extraJavaOptions=-Dfile.encoding=utf-8\n</code></pre> <p>Namely:</p> <pre><code>&lt;spark_install_path&gt;/bin/spark-submit --master \"local\" \\\n--conf spark.driver.extraJavaOptions=-Dfile.encoding=utf-8 \\\n--conf spark.executor.extraJavaOptions=-Dfile.encoding=utf-8 \\\n--class com.vesoft.nebula.exchange.Exchange \\\n&lt;nebula-exchange-3.x.y.jar_path&gt; -c &lt;application.conf_path&gt;\n</code></pre> <p>In YARN, use the following command:</p> <pre><code>&lt;spark_install_path&gt;/bin/spark-submit \\\n--class com.vesoft.nebula.exchange.Exchange \\\n--master yarn-cluster \\\n--files &lt;application.conf_path&gt; \\\n--conf spark.driver.extraClassPath=./ \\\n--conf spark.executor.extraClassPath=./ \\\n--conf spark.driver.extraJavaOptions=-Dfile.encoding=utf-8 \\\n--conf spark.executor.extraJavaOptions=-Dfile.encoding=utf-8 \\\n&lt;nebula-exchange-3.x.y.jar_path&gt; \\\n-c application.conf\n</code></pre>"},{"location":"nebula-exchange/ex-ug-FAQ/#q_orgrocksdbrocksdbexception_while_open_a_file_for_appending_pathsst1-xxxsst_no_such_file_or_directory","title":"Q: org.rocksdb.RocksDBException: While open a file for appending: /path/sst/1-xxx.sst: No such file or directory","text":"<p>Solution:</p> <ol> <li>Check if <code>/path</code> exists. If not, or if the path is set incorrectly, create or correct it.</li> <li>Check if Spark's current user on each machine has the operation permission on <code>/path</code>. If not, grant the permission.</li> </ol>"},{"location":"nebula-exchange/ex-ug-FAQ/#configuration","title":"Configuration","text":""},{"location":"nebula-exchange/ex-ug-FAQ/#q_which_configuration_fields_will_affect_import_performance","title":"Q: Which configuration fields will affect import performance?","text":"<ul> <li>batch: The number of data contained in each nGQL statement sent to the NebulaGraph service.</li> </ul> <ul> <li>partition: The number of Spark data partitions, indicating the number of concurrent data imports.</li> </ul> <ul> <li>nebula.rate: Get a token from the token bucket before sending a request to NebulaGraph.<p>- limit: Represents the size of the token bucket.</p> <p>- timeout: Represents the timeout period for obtaining the token.</p> </li> </ul> <p>The values of these four parameters can be adjusted appropriately according to the machine performance. If the leader of the Storage service changes during the import process, you can adjust the values of these four parameters to reduce the import speed.</p>"},{"location":"nebula-exchange/ex-ug-FAQ/#others","title":"Others","text":""},{"location":"nebula-exchange/ex-ug-FAQ/#q_which_versions_of_nebulagraph_are_supported_by_exchange","title":"Q: Which versions of NebulaGraph are supported by Exchange?","text":"<p>See Limitations.</p>"},{"location":"nebula-exchange/ex-ug-FAQ/#q_what_is_the_relationship_between_exchange_and_spark_writer","title":"Q: What is the relationship between Exchange and Spark Writer?","text":"<p>Exchange is the Spark application developed based on Spark Writer. Both are suitable for bulk migration of cluster data to NebulaGraph in a distributed environment, but later maintenance work will be focused on Exchange. Compared with Spark Writer, Exchange has the following improvements:</p> <ul> <li>It supports more abundant data sources, such as MySQL, Neo4j, Hive, HBase, Kafka, Pulsar, etc.</li> </ul> <ul> <li>It fixed some problems of Spark Writer. For example, when Spark reads data from HDFS, the default source data is String, which may be different from the NebulaGraph's Schema. So Exchange adds automatic data type matching and type conversion. When the data type in the NebulaGraph's Schema is non-String (e.g. double), Exchange converts the source data of String type to the corresponding type.</li> </ul>"},{"location":"nebula-exchange/ex-ug-compile/","title":"Get Exchange","text":"<p>This topic introduces how to get the JAR file of NebulaGraph Exchange.</p>"},{"location":"nebula-exchange/ex-ug-compile/#download_the_jar_file_directly","title":"Download the JAR file directly","text":"<p>The JAR file of Exchange Community Edition can be downloaded directly.</p> <p>To download Exchange Enterprise Edition, get NebulaGraph Enterprise Edition Package first.</p>"},{"location":"nebula-exchange/ex-ug-compile/#get_the_jar_file_by_compiling_the_source_code","title":"Get the JAR file by compiling the source code","text":"<p>You can get the JAR file of Exchange Community Edition by compiling the source code. The following introduces how to compile the source code of Exchange.</p> <p>Enterpriseonly</p> <p>You can get Exchange Enterprise Edition in NebulaGraph Enterprise Edition Package only.</p>"},{"location":"nebula-exchange/ex-ug-compile/#prerequisites","title":"Prerequisites","text":"<ul> <li>Install Maven.</li> </ul> <ul> <li>Install the correct version of Apache Spark. Exporting data from different sources requires different Spark versions. For more information, see Software dependencies.</li> </ul>"},{"location":"nebula-exchange/ex-ug-compile/#steps","title":"Steps","text":"<ol> <li> <p>Clone the repository <code>nebula-exchange</code> in the <code>/</code> directory.</p> <pre><code>git clone -b release-3.5 https://github.com/vesoft-inc/nebula-exchange.git\n</code></pre> </li> <li> <p>Switch to the directory <code>nebula-exchange</code>.</p> <pre><code>cd nebula-exchange\n</code></pre> </li> <li> <p>Package NebulaGraph Exchange. Run the following command based on the Spark version:</p> <ul> <li> <p>For Spark 2.2\uff1a</p> <pre><code>mvn clean package -Dmaven.test.skip=true -Dgpg.skip -Dmaven.javadoc.skip=true \\\n-pl nebula-exchange_spark_2.2 -am -Pscala-2.11 -Pspark-2.2\n</code></pre> </li> </ul> <ul> <li> <p>For Spark 2.4\uff1a</p> <pre><code>mvn clean package -Dmaven.test.skip=true -Dgpg.skip -Dmaven.javadoc.skip=true \\\n-pl nebula-exchange_spark_2.4 -am -Pscala-2.11 -Pspark-2.4\n</code></pre> </li> </ul> <ul> <li> <p>For Spark 3.0\uff1a</p> <pre><code>mvn clean package -Dmaven.test.skip=true -Dgpg.skip -Dmaven.javadoc.skip=true \\\n-pl nebula-exchange_spark_3.0 -am -Pscala-2.12 -Pspark-3.0\n</code></pre> </li> </ul> </li> </ol> <p>After the compilation is successful, you can find the <code>nebula-exchange_spark_x.x-release-3.5.jar</code> file in the <code>nebula-exchange_spark_x.x/target/</code> directory. <code>x.x</code> indicates the Spark version, for example, <code>2.4</code>.</p> <p>Note</p> <p>The JAR file version changes with the release of the NebulaGraph Java Client. Users can view the latest version on the Releases page.</p> <p>When migrating data, you can refer to configuration file <code>target/classes/application.conf</code>.</p>"},{"location":"nebula-exchange/ex-ug-compile/#failed_to_download_the_dependency_package","title":"Failed to download the dependency package","text":"<p>If downloading dependencies fails when compiling:</p> <ul> <li>Check the network settings and ensure that the network is normal.</li> </ul> <ul> <li> <p>Modify the <code>mirror</code> part of Maven installation directory <code>libexec/conf/settings.xml</code>:</p> <pre><code>&lt;mirror&gt;\n &lt;id&gt;alimaven&lt;/id&gt;\n &lt;mirrorOf&gt;central&lt;/mirrorOf&gt;\n &lt;name&gt;aliyun maven&lt;/name&gt;\n &lt;url&gt;http://maven.aliyun.com/nexus/content/repositories/central/&lt;/url&gt;\n&lt;/mirror&gt;\n</code></pre> </li> </ul>"},{"location":"nebula-exchange/about-exchange/ex-ug-limitations/","title":"Limitations","text":"<p>This topic describes some of the limitations of using Exchange 3.x.</p> <p>JAR packages are available in two ways: compile them yourself or download them from the Maven repository.</p> <p>If you are using NebulaGraph 1.x, use NebulaGraph Exchange 1.x.</p>"},{"location":"nebula-exchange/about-exchange/ex-ug-limitations/#environment","title":"Environment","text":"<p>Exchange 3.x supports the following operating systems:</p> <ul> <li>CentOS 7</li> <li>macOS</li> </ul>"},{"location":"nebula-exchange/about-exchange/ex-ug-limitations/#software_dependencies","title":"Software dependencies","text":"<p>To ensure the healthy operation of Exchange, ensure that the following software has been installed on the machine:</p> <ul> <li>Java version 1.8</li> </ul> <ul> <li>Scala version 2.10.7, 2.11.12, or 2.12.10</li> </ul> <ul> <li> <p>Apache Spark. The requirements for Spark versions when using Exchange to export data from data sources are as follows. In the following table, Y means that the corresponding Spark version is supported, and N means not supported.</p> <p>Note</p> <p>Use the correct Exchange JAR file based on the Spark version. For example, for Spark version 2.4, use nebula-exchange_spark_2.4-3.5.0.jar.</p> Data source Spark 2.2 Spark 2.4 Spark 3 CSV file Y N Y JSON file Y Y Y ORC file Y Y Y Parquet file Y Y Y HBase Y Y Y MySQL Y Y Y PostgreSQL Y Y Y Oracle Y Y Y ClickHouse Y Y Y Neo4j N Y N Hive Y Y Y MaxCompute N Y N Pulsar N Y Untested Kafka N Y Untested NebulaGraph N Y N </li> </ul> <p>Hadoop Distributed File System (HDFS) needs to be deployed in the following scenarios:</p> <ul> <li>Migrate HDFS data</li> <li>Generate SST files</li> </ul>"},{"location":"nebula-exchange/about-exchange/ex-ug-what-is-exchange/","title":"What is NebulaGraph Exchange","text":"<p>NebulaGraph Exchange (Exchange) is an Apache Spark\u2122 application for bulk migration of cluster data to NebulaGraph in a distributed environment, supporting batch and streaming data migration in a variety of formats.</p> <p>Exchange consists of Reader, Processor, and Writer. After Reader reads data from different sources and returns a DataFrame, the Processor iterates through each row of the DataFrame and obtains the corresponding value based on the mapping between <code>fields</code> in the configuration file. After iterating through the number of rows in the specified batch, Writer writes the captured data to the NebulaGraph at once. The following figure illustrates the process by which Exchange completes the data conversion and migration.</p> <p></p>"},{"location":"nebula-exchange/about-exchange/ex-ug-what-is-exchange/#editions","title":"Editions","text":"<p>Exchange has two editions, the Community Edition and the Enterprise Edition. The Community Edition is open source developed on GitHub. The Enterprise Edition supports not only the functions of the Community Edition but also adds additional features. For details, see Comparisons.</p>"},{"location":"nebula-exchange/about-exchange/ex-ug-what-is-exchange/#scenarios","title":"Scenarios","text":"<p>Exchange applies to the following scenarios:</p> <ul> <li>Streaming data from Kafka and Pulsar platforms, such as log files, online shopping data, activities of game players, information on social websites, financial transactions or geospatial services, and telemetry data from connected devices or instruments in the data center, are required to be converted into the vertex or edge data of the property graph and import them into the NebulaGraph database.</li> </ul> <ul> <li>Batch data, such as data from a time period, needs to be read from a relational database (such as MySQL) or a distributed file system (such as HDFS), converted into vertex or edge data for a property graph, and imported into the NebulaGraph database.</li> </ul> <ul> <li>A large volume of data needs to be generated into SST files that NebulaGraph can recognize and then imported into the NebulaGraph database.</li> </ul> <ul> <li> <p>The data saved in NebulaGraph needs to be exported.</p> <p>Enterpriseonly</p> <p>Exporting the data saved in NebulaGraph is supported by Exchange Enterprise Edition only.</p> </li> </ul>"},{"location":"nebula-exchange/about-exchange/ex-ug-what-is-exchange/#advantages","title":"Advantages","text":"<p>Exchange has the following advantages:</p> <ul> <li>High adaptability: It supports importing data into the NebulaGraph database in a variety of formats or from a variety of sources, making it easy to migrate data.</li> </ul> <ul> <li>SST import: It supports converting data from different sources into SST files for data import.</li> </ul> <ul> <li>SSL encryption: It supports establishing the SSL encryption between Exchange and NebulaGraph to ensure data security.</li> </ul> <ul> <li> <p>Resumable data import: It supports resumable data import to save time and improve data import efficiency.</p> <p>Note</p> <p>Resumable data import is currently supported when migrating Neo4j data only.</p> </li> </ul> <ul> <li>Asynchronous operation: An insert statement is generated in the source data and sent to the Graph service. Then the insert operation is performed.</li> </ul> <ul> <li>Great flexibility: It supports importing multiple Tags and Edge types at the same time. Different Tags and Edge types can be from different data sources or in different formats.</li> </ul> <ul> <li>Statistics: It uses the accumulator in Apache Spark\u2122 to count the number of successful and failed insert operations.</li> </ul> <ul> <li>Easy to use: It adopts the Human-Optimized Config Object Notation (HOCON) configuration file format and has an object-oriented style, which is easy to understand and operate.</li> </ul>"},{"location":"nebula-exchange/about-exchange/ex-ug-what-is-exchange/#version_compatibility","title":"Version compatibility","text":"<p>Exchange supports Spark versions 2.2.x, 2.4.x, and 3.x.x, which are named <code>nebula-exchange_spark_2.2</code>, <code>nebula-exchange_spark_2.4</code>, and <code>nebula-exchange_spark_3.0</code> for different Spark versions.</p> <p>The correspondence between the NebulaGraph Exchange version (the JAR version), the NebulaGraph core version and the Spark version is as follows.</p> Exchange version NebulaGraph version Spark version nebula-exchange_spark_3.0-3.0-SNAPSHOT.jar nightly 3.3.x\u30013.2.x\u30013.1.x\u30013.0.x nebula-exchange_spark_2.4-3.0-SNAPSHOT.jar nightly 2.4.x nebula-exchange_spark_2.2-3.0-SNAPSHOT.jar nightly 2.2.x nebula-exchange_spark_3.0-3.4.0.jar 3.x.x 3.3.x\u30013.2.x\u30013.1.x\u30013.0.x nebula-exchange_spark_2.4-3.4.0.jar 3.x.x 2.4.x nebula-exchange_spark_2.2-3.4.0.jar 3.x.x 2.2.x nebula-exchange_spark_3.0-3.3.0.jar 3.x.x 3.3.x\u30013.2.x\u30013.1.x\u30013.0.x nebula-exchange_spark_2.4-3.3.0.jar 3.x.x 2.4.x nebula-exchange_spark_2.2-3.3.0.jar 3.x.x 2.2.x nebula-exchange_spark_3.0-3.0.0.jar 3.x.x 3.3.x\u30013.2.x\u30013.1.x\u30013.0.x nebula-exchange_spark_2.4-3.0.0.jar 3.x.x 2.4.x nebula-exchange_spark_2.2-3.0.0.jar 3.x.x 2.2.x nebula-exchange-2.6.3.jar 2.6.1\u30012.6.0 2.4.x nebula-exchange-2.6.2.jar 2.6.1\u30012.6.0 2.4.x nebula-exchange-2.6.1.jar 2.6.1\u30012.6.0 2.4.x nebula-exchange-2.6.0.jar 2.6.1\u30012.6.0 2.4.x nebula-exchange-2.5.2.jar 2.5.1\u30012.5.0 2.4.x nebula-exchange-2.5.1.jar 2.5.1\u30012.5.0 2.4.x nebula-exchange-2.5.0.jar 2.5.1\u30012.5.0 2.4.x nebula-exchange-2.1.0.jar 2.0.1\u30012.0.0 2.4.x nebula-exchange-2.0.1.jar 2.0.1\u30012.0.0 2.4.x nebula-exchange-2.0.0.jar 2.0.1\u30012.0.0 2.4.x"},{"location":"nebula-exchange/about-exchange/ex-ug-what-is-exchange/#data_source","title":"Data source","text":"<p>Exchange 3.5.0 supports converting data from the following formats or sources into vertexes and edges that NebulaGraph can recognize, and then importing them into NebulaGraph in the form of nGQL statements:</p> <ul> <li>Data stored in HDFS or locally:<ul> <li>Apache Parquet</li> <li>Apache ORC</li> <li>JSON</li> <li>CSV</li> </ul> </li> </ul> <ul> <li>Apache HBase\u2122</li> </ul> <ul> <li> <p>Data repository:</p> <ul> <li>Hive</li> <li>MaxCompute</li> </ul> </li> </ul> <ul> <li>Graph database: Neo4j (Client version 2.4.5-M1)</li> </ul> <ul> <li>Relational database:<ul> <li>MySQL</li> <li>PostgreSQL</li> <li>Oracle</li> </ul> </li> </ul> <ul> <li>Column database: ClickHouse</li> </ul> <ul> <li>Stream processing software platform: Apache Kafka\u00ae</li> </ul> <ul> <li>Publish/Subscribe messaging platform: Apache Pulsar 2.4.5</li> </ul> <ul> <li>JDBC</li> </ul> <p>In addition to importing data as nGQL statements, Exchange supports generating SST files for data sources and then importing SST files via Console.</p> <p>In addition, Exchange Enterprise Edition also supports exporting data to a CSV file or another graph space using NebulaGraph as data sources.</p>"},{"location":"nebula-exchange/about-exchange/ex-ug-what-is-exchange/#release_note","title":"Release note","text":"<p>Release</p>"},{"location":"nebula-exchange/parameter-reference/ex-ug-para-import-command/","title":"Options for import","text":"<p>After editing the configuration file, run the following commands to import specified source data into the NebulaGraph database.</p> <ul> <li> <p>First import</p> <pre><code>&lt;spark_install_path&gt;/bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange &lt;nebula-exchange-2.x.y.jar_path&gt; -c &lt;application.conf_path&gt; \n</code></pre> </li> </ul> <ul> <li> <p>Import the reload file</p> <p>If some data fails to be imported during the first import, the failed data will be stored in the reload file. Use the parameter <code>-r</code> to import the reload file.</p> <pre><code>&lt;spark_install_path&gt;/bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange &lt;nebula-exchange-2.x.y.jar_path&gt; -c &lt;application.conf_path&gt; -r \"&lt;reload_file_path&gt;\" \n</code></pre> </li> </ul> <p>Note</p> <p>The version number of a JAR file is subject to the name of the JAR file that is actually compiled.</p> <p>Faq</p> <p>If users use the yarn-cluster mode to submit a job, see the following command, especially the two '--conf' commands in the example.</p> <pre><code>$SPARK_HOME/bin/spark-submit     --master yarn-cluster \\\n--class com.vesoft.nebula.exchange.Exchange \\\n--files application.conf \\\n--conf spark.driver.extraClassPath=./ \\\n--conf spark.executor.extraClassPath=./ \\\nnebula-exchange-3.5.0.jar \\\n-c application.conf\n</code></pre> <p>The following table lists command parameters.</p> Parameter Required Default value Description <code>--class</code> Yes - Specify the main class of the driver. <code>--master</code> Yes - Specify the URL of the master process in a Spark cluster. For more information, see master-urls. <code>-c</code>\u00a0 / <code>--config</code> Yes - Specify the path of the configuration file. <code>-h</code>\u00a0 / <code>--hive</code> No <code>false</code> Indicate support for importing Hive data. <code>-D</code>\u00a0 / <code>--dry</code> No <code>false</code> Check whether the format of the configuration file meets the requirements, but it does not check whether the configuration items of <code>tags</code> and <code>edges</code> are correct. This parameter cannot be added when users import data. <code>-r</code> / <code>--reload</code> No - Specify the path of the reload file that needs to be reloaded. <p>For more Spark parameter configurations, see Spark Configuration.</p>"},{"location":"nebula-exchange/parameter-reference/ex-ug-parameter/","title":"Parameters in the configuration file","text":"<p>This topic describes how to configure the file <code>application.conf</code> when users use NebulaGraph Exchange.</p> <p>Before configuring the <code>application.conf</code> file, it is recommended to copy the file name <code>application.conf</code> and then edit the file name according to the file type of a data source. For example, change the file name to <code>csv_application.conf</code> if the file type of the data source is CSV.</p> <p>The <code>application.conf</code> file contains the following content types:</p> <ul> <li>Spark configurations</li> </ul> <ul> <li>Hive configurations (optional)</li> </ul> <ul> <li>NebulaGraph configurations</li> </ul> <ul> <li>Vertex configurations</li> </ul> <ul> <li>Edge configurations</li> </ul>"},{"location":"nebula-exchange/parameter-reference/ex-ug-parameter/#spark_configurations","title":"Spark configurations","text":"<p>This topic lists only some Spark parameters. For more information, see Spark Configuration.</p> Parameter Type Default value Required Description <code>spark.app.name</code> string - No The drive name in Spark. <code>spark.driver.cores</code> int <code>1</code> No The number of CPU cores used by a driver, only applicable to a cluster mode. <code>spark.driver.maxResultSize</code> string <code>1G</code> No The total size limit (in bytes) of the serialized results of all partitions in a single Spark operation (such as collect). The minimum value is 1M, and 0 means unlimited. <code>spark.executor.memory</code> string <code>1G</code> No The amount of memory used by a Spark driver which can be specified in units, such as 512M or 1G. <code>spark.cores.max</code> int <code>16</code> No The maximum number of CPU cores of applications requested across clusters (rather than from each node) when a driver runs in a coarse-grained sharing mode on a standalone cluster or a Mesos cluster. The default value is <code>spark.deploy.defaultCores</code> on a Spark standalone cluster manager or the value of the <code>infinite</code> parameter (all available cores) on Mesos."},{"location":"nebula-exchange/parameter-reference/ex-ug-parameter/#hive_configurations_optional","title":"Hive configurations (optional)","text":"<p>Users only need to configure parameters for connecting to Hive if Spark and Hive are deployed in different clusters. Otherwise, please ignore the following configurations.</p> Parameter Type Default value Required Description <code>hive.warehouse</code> string - Yes The warehouse path in HDFS. Enclose the path in double quotes and start with <code>hdfs://</code>. <code>hive.connectionURL</code> string - Yes The URL of a JDBC connection. For example, <code>\"jdbc:mysql://127.0.0.1:3306/hive_spark?characterEncoding=UTF-8\"</code>. <code>hive.connectionDriverName</code> string <code>\"com.mysql.jdbc.Driver\"</code> Yes The driver name. <code>hive.connectionUserName</code> list[string] - Yes The username for connections. <code>hive.connectionPassword</code> list[string] - Yes The account password."},{"location":"nebula-exchange/parameter-reference/ex-ug-parameter/#nebulagraph_configurations","title":"NebulaGraph configurations","text":"Parameter Type Default value Required Description <code>nebula.address.graph</code> list[string] <code>[\"127.0.0.1:9669\"]</code> Yes The addresses of all Graph services, including IPs and ports, separated by commas (,). Example: <code>[\"ip1:port1\",\"ip2:port2\",\"ip3:port3\"]</code>. <code>nebula.address.meta</code> list[string] <code>[\"127.0.0.1:9559\"]</code> Yes The addresses of all Meta services, including IPs and ports, separated by commas (,). Example: <code>[\"ip1:port1\",\"ip2:port2\",\"ip3:port3\"]</code>. <code>nebula.user</code> string - Yes The username with write permissions for NebulaGraph. <code>nebula.pswd</code> string - Yes The account password. <code>nebula.space</code> string - Yes The name of the graph space where data needs to be imported. <code>nebula.ssl.enable.graph</code> bool <code>false</code> Yes Enables the SSL encryption between Exchange and Graph services. If the value is <code>true</code>, the SSL encryption is enabled and the following SSL parameters take effect. If Exchange is run on a multi-machine cluster, you need to store the corresponding files in the same path on each machine when setting the following SSL-related paths. <code>nebula.ssl.sign</code> string <code>ca</code> Yes Specifies the SSL sign. Optional values are <code>ca</code> and <code>self</code>. <code>nebula.ssl.ca.param.caCrtFilePath</code> string Specifies the storage path of the CA certificate. It takes effect when the value of <code>nebula.ssl.sign</code> is <code>ca</code>. <code>nebula.ssl.ca.param.crtFilePath</code> string <code>\"/path/crtFilePath\"</code> Yes Specifies the storage path of the CRT certificate. It takes effect when the value of <code>nebula.ssl.sign</code> is <code>ca</code>. <code>nebula.ssl.ca.param.keyFilePath</code> string <code>\"/path/keyFilePath\"</code> Yes Specifies the storage path of the key file. It takes effect when the value of <code>nebula.ssl.sign</code> is <code>ca</code>. <code>nebula.ssl.self.param.crtFilePath</code> string <code>\"/path/crtFilePath\"</code> Yes Specifies the storage path of the CRT certificate. It takes effect when the value of <code>nebula.ssl.sign</code> is <code>self</code>. <code>nebula.ssl.self.param.keyFilePath</code> string <code>\"/path/keyFilePath\"</code> Yes Specifies the storage path of the key file. It takes effect when the value of <code>nebula.ssl.sign</code> is <code>self</code>. <code>nebula.ssl.self.param.password</code> string <code>\"nebula\"</code> Yes Specifies the storage path of the password. It takes effect when the value of <code>nebula.ssl.sign</code> is <code>self</code>. <code>nebula.path.local</code> string <code>\"/tmp\"</code> No The local SST file path which needs to be set when users import SST files. <code>nebula.path.remote</code> string <code>\"/sst\"</code> No The remote SST file path which needs to be set when users import SST files. <code>nebula.path.hdfs.namenode</code> string <code>\"hdfs://name_node:9000\"</code> No The NameNode path which needs to be set when users import SST files. <code>nebula.connection.timeout</code> int <code>3000</code> No The timeout set for Thrift connections. Unit: ms. <code>nebula.connection.retry</code> int <code>3</code> No Retries set for Thrift connections. <code>nebula.execution.retry</code> int <code>3</code> No Retries set for executing nGQL statements. <code>nebula.error.max</code> int <code>32</code> No The maximum number of failures during the import process. When the number of failures reaches the maximum, the Spark job submitted will stop automatically . <code>nebula.error.output</code> string <code>/tmp/errors</code> No The path to output error logs. Failed nGQL statement executions are saved in the error log. <code>nebula.rate.limit</code> int <code>1024</code> No The limit on the number of tokens in the token bucket when importing data. <code>nebula.rate.timeout</code> int <code>1000</code> No The timeout period for getting tokens from a token bucket. Unit: milliseconds. <p>Note</p> <p>NebulaGraph doesn't support vertices without tags by default. To import vertices without tags, enable vertices without tags in the NebulaGraph cluster and then add parameter <code>nebula.enableTagless</code> to the Exchange configuration with the value <code>true</code>. For example:</p> <pre><code>nebula: {\n    address:{\n      graph:[\"127.0.0.1:9669\"]\n      meta:[\"127.0.0.1:9559\"]\n    }\n    user: root\n    pswd: nebula\n    space: test\n    enableTagless: true\n    ......\n\n }\n</code></pre>"},{"location":"nebula-exchange/parameter-reference/ex-ug-parameter/#vertex_configurations","title":"Vertex configurations","text":"<p>For different data sources, the vertex configurations are different. There are many general parameters and some specific parameters. General parameters and specific parameters of different data sources need to be configured when users configure vertices.</p>"},{"location":"nebula-exchange/parameter-reference/ex-ug-parameter/#general_parameters","title":"General parameters","text":"Parameter Type Default value Required Description <code>tags.name</code> string - Yes The tag name defined in NebulaGraph. <code>tags.type.source</code> string - Yes Specify a data source. For example, <code>csv</code>. <code>tags.type.sink</code> string <code>client</code> Yes Specify an import method. Optional values are <code>client</code> and <code>SST</code>. <code>tags.writeMode</code> string <code>INSERT</code> No Types of batch operations on data, including batch inserts, updates, and deletes. Optional values are <code>INSERT</code>, <code>UPDATE</code>, <code>DELETE</code>. <code>tags.deleteEdge</code> string <code>false</code> No Whether or not to delete the related incoming and outgoing edges of the vertices when performing a batch delete operation. This parameter takes effect when <code>tags.writeMode</code> is <code>DELETE</code>. <code>tags.fields</code> list[string] - Yes The header or column name of the column corresponding to properties. If there is a header or a column name, please use that name directly. If a CSV file does not have a header, use the form of <code>[_c0, _c1, _c2]</code> to represent the first column, the second column, the third column, and so on. <code>tags.nebula.fields</code> list[string] - Yes Property names defined in NebulaGraph, the order of which must correspond to <code>tags.fields</code>. For example, <code>[_c1, _c2]</code> corresponds to <code>[name, age]</code>, which means that values in the second column are the values of the property <code>name</code>, and values in the third column are the values of the property <code>age</code>. <code>tags.vertex.field</code> string - Yes The column of vertex IDs. For example, when a CSV file has no header, users can use <code>_c0</code> to indicate values in the first column are vertex IDs. <code>tags.vertex.udf.separator</code> string - No Support merging multiple columns by custom rules. This parameter specifies the join character. <code>tags.vertex.udf.oldColNames</code> list - No Support merging multiple columns by custom rules. This parameter specifies the names of the columns to be merged. Multiple columns are separated by commas. <code>tags.vertex.udf.newColName</code> string - No Support merging multiple columns by custom rules. This parameter specifies the new column name. <code>tags.vertex.prefix</code> string - No Add the specified prefix to the VID. For example, if the VID is <code>12345</code>, adding the prefix <code>tag1</code> will result in <code>tag1_12345</code>. The underscore cannot be modified. <code>tags.vertex.policy</code> string - No Supports only the value <code>hash</code>. Performs hashing operations on VIDs of type string. <code>tags.batch</code> int <code>256</code> Yes The maximum number of vertices written into NebulaGraph in a single batch. <code>tags.partition</code> int <code>32</code> Yes The number of Spark partitions."},{"location":"nebula-exchange/parameter-reference/ex-ug-parameter/#specific_parameters_of_parquetjsonorc_data_sources","title":"Specific parameters of Parquet/JSON/ORC data sources","text":"Parameter Type Default value Required Description <code>tags.path</code> string - Yes The path of vertex data files in HDFS. Enclose the path in double quotes and start with <code>hdfs://</code>."},{"location":"nebula-exchange/parameter-reference/ex-ug-parameter/#specific_parameters_of_csv_data_sources","title":"Specific parameters of CSV data sources","text":"Parameter Type Default value Required Description <code>tags.path</code> string - Yes The path of vertex data files in HDFS. Enclose the path in double quotes and start with <code>hdfs://</code>. <code>tags.separator</code> string <code>,</code> Yes The separator. The default value is a comma (,). For special characters, such as the control character <code>^A</code>, you can use ASCII octal <code>\\001</code> or UNICODE encoded hexadecimal <code>\\u0001</code>, for the control character <code>^B</code>, use ASCII octal <code>\\002</code> or UNICODE encoded hexadecimal <code>\\u0002</code>, for the control character <code>^C</code>, use ASCII octal <code>\\003</code> or UNICODE encoded hexadecimal <code>\\u0003</code>. <code>tags.header</code> bool <code>true</code> Yes Whether the file has a header."},{"location":"nebula-exchange/parameter-reference/ex-ug-parameter/#specific_parameters_of_hive_data_sources","title":"Specific parameters of Hive data sources","text":"Parameter Type Default value Required Description <code>tags.exec</code> string - Yes The statement to query data sources. For example, <code>select name,age from mooc.users</code>."},{"location":"nebula-exchange/parameter-reference/ex-ug-parameter/#specific_parameters_of_maxcompute_data_sources","title":"Specific parameters of MaxCompute data sources","text":"Parameter Type Default value Required Description <code>tags.table</code> string - Yes The table name of the MaxCompute. <code>tags.project</code> string - Yes The project name of the MaxCompute. <code>tags.odpsUrl</code> string - Yes The odpsUrl of the MaxCompute service. For more information about odpsUrl, see Endpoints. <code>tags.tunnelUrl</code> string - Yes The tunnelUrl of the MaxCompute service. For more information about tunnelUrl, see Endpoints. <code>tags.accessKeyId</code> string - Yes The accessKeyId of the MaxCompute service. <code>tags.accessKeySecret</code> string - Yes The accessKeySecret of the MaxCompute service. <code>tags.partitionSpec</code> string - No Partition descriptions of MaxCompute tables. <code>tags.sentence</code> string - No Statements to query data sources. The table name in the SQL statement is the same as the value of the table above."},{"location":"nebula-exchange/parameter-reference/ex-ug-parameter/#specific_parameters_of_neo4j_data_sources","title":"Specific parameters of Neo4j data sources","text":"Parameter Type Default value Required Description <code>tags.exec</code> string - Yes Statements to query data sources. For example: <code>match (n:label) return n.neo4j-field-0</code>. <code>tags.server</code> string <code>\"bolt://127.0.0.1:7687\"</code> Yes The server address of Neo4j. <code>tags.user</code> string - Yes The Neo4j username with read permissions. <code>tags.password</code> string - Yes The account password. <code>tags.database</code> string - Yes The name of the database where source data is saved in Neo4j. <code>tags.check_point_path</code> string <code>/tmp/test</code> No The directory set to import progress information, which is used for resuming transfers. If not set, the resuming transfer is disabled."},{"location":"nebula-exchange/parameter-reference/ex-ug-parameter/#specific_parameters_of_mysqlpostgresql_data_sources","title":"Specific parameters of MySQL/PostgreSQL data sources","text":"Parameter Type Default value Required Description <code>tags.host</code> string - Yes The MySQL/PostgreSQL server address. <code>tags.port</code> string - Yes The MySQL/PostgreSQL server port. <code>tags.database</code> string - Yes The database name. <code>tags.table</code> string - Yes The name of a table used as a data source. <code>tags.user</code> string - Yes The MySQL/PostgreSQL username with read permissions. <code>tags.password</code> string - Yes The account password. <code>tags.sentence</code> string - Yes Statements to query data sources. For example: <code>\"select teamid, name from team order by teamid\"</code>."},{"location":"nebula-exchange/parameter-reference/ex-ug-parameter/#specific_parameters_of_oracle_data_sources","title":"Specific parameters of Oracle data sources","text":"Parameter Type Default value Required Description <code>tags.url</code> string - Yes The Oracle server address. <code>tags.driver</code> string - Yes The Oracle driver address. <code>tags.user</code> string - Yes The Oracle username with read permissions. <code>tags.password</code> string - Yes The account password. <code>tags.table</code> string - Yes The name of a table used as a data source. <code>tags.sentence</code> string - Yes Statements to query data sources. For example: <code>\"select playerid, name, age from player\"</code>."},{"location":"nebula-exchange/parameter-reference/ex-ug-parameter/#specific_parameters_of_clickhouse_data_sources","title":"Specific parameters of ClickHouse data sources","text":"Parameter Type Default value Required Description <code>tags.url</code> string - Yes The JDBC URL of ClickHouse. <code>tags.user</code> string - Yes The ClickHouse username with read permissions. <code>tags.password</code> string - Yes The account password. <code>tags.numPartition</code> string - Yes The number of ClickHouse partitions. <code>tags.sentence</code> string - Yes Statements to query data sources."},{"location":"nebula-exchange/parameter-reference/ex-ug-parameter/#specific_parameters_of_hbase_data_sources","title":"Specific parameters of Hbase data sources","text":"Parameter Type Default value Required Description <code>tags.host</code> string <code>127.0.0.1</code> Yes The Hbase server address. <code>tags.port</code> string <code>2181</code> Yes The Hbase server port. <code>tags.table</code> string - Yes The name of a table used as a data source. <code>tags.columnFamily</code> string - Yes The column family to which a table belongs."},{"location":"nebula-exchange/parameter-reference/ex-ug-parameter/#specific_parameters_of_pulsar_data_sources","title":"Specific parameters of Pulsar data sources","text":"Parameter Type Default value Required Description <code>tags.service</code> string <code>\"pulsar://localhost:6650\"</code> Yes The Pulsar server address. <code>tags.admin</code> string <code>\"http://localhost:8081\"</code> Yes The admin URL used to connect pulsar. <code>tags.options.&lt;topic\\|topics\\| topicsPattern&gt;</code> string - Yes Options offered by Pulsar, which can be configured by choosing one from <code>topic</code>, <code>topics</code>, and <code>topicsPattern</code>. <code>tags.interval.seconds</code> int <code>10</code> Yes The interval for reading messages. Unit: seconds."},{"location":"nebula-exchange/parameter-reference/ex-ug-parameter/#specific_parameters_of_kafka_data_sources","title":"Specific parameters of Kafka data sources","text":"Parameter Type Default value Required Description <code>tags.service</code> string - Yes The Kafka server address. <code>tags.topic</code> string - Yes The message type. <code>tags.interval.seconds</code> int <code>10</code> Yes The interval for reading messages. Unit: seconds."},{"location":"nebula-exchange/parameter-reference/ex-ug-parameter/#specific_parameters_for_generating_sst_files","title":"Specific parameters for generating SST files","text":"Parameter Type Default value Required Description <code>tags.path</code> string - Yes The path of the source file specified to generate SST files. <code>tags.repartitionWithNebula</code> bool <code>true</code> No Whether to repartition data based on the number of partitions of graph spaces in NebulaGraph when generating the SST file. Enabling this function can reduce the time required to DOWNLOAD and INGEST SST files."},{"location":"nebula-exchange/parameter-reference/ex-ug-parameter/#specific_parameters_of_nebulagraph","title":"Specific parameters of NebulaGraph","text":"<p>Enterpriseonly</p> <p>Specific parameters of NebulaGraph are used for exporting NebulaGraph data, which is supported by Exchange Enterprise Edition only.</p> Parameter Data type Default value Required Description <code>tags.path</code> string <code>\"hdfs://namenode:9000/path/vertex\"</code> Yes Specifies the storage path of the CSV file. You need to set a new path and Exchange will automatically create the path you set. If you store the data to the HDFS server, the path format is the same as the default value, such as <code>\"hdfs://192.168.8.177:9000/vertex/player\"</code>. If you store the data to the local, the path format is <code>\"file:///path/vertex\"</code>, such as <code>\"file:///home/nebula/vertex/player\"</code>. If there are multiple Tags, different directories must be set for each Tag. <code>tags.noField</code> bool <code>false</code> Yes If the value is <code>true</code>, only VIDs will be exported, not the property data. If the value is <code>false</code>, VIDs and the property data will be exported. <code>tags.return.fields</code> list <code>[]</code> Yes Specifies the properties to be exported. For example, to export the <code>name</code> and <code>age</code>, you need to set the parameter value to <code>[\"name\",\"age\"]</code>. This parameter only takes effect when the value of <code>tags.noField</code> is <code>false</code>."},{"location":"nebula-exchange/parameter-reference/ex-ug-parameter/#edge_configurations","title":"Edge configurations","text":"<p>For different data sources, configurations of edges are also different. There are general parameters and some specific parameters. General parameters and specific parameters of different data sources need to be configured when users configure edges.</p> <p>For the specific parameters of different data sources for edge configurations, please refer to the introduction of specific parameters of different data sources above, and pay attention to distinguishing tags and edges.</p>"},{"location":"nebula-exchange/parameter-reference/ex-ug-parameter/#general_parameters_1","title":"General parameters","text":"Parameter Type Default value Required Description <code>edges.name</code> string - Yes The edge type name defined in NebulaGraph. <code>edges.type.source</code> string - Yes The data source of edges. For example, <code>csv</code>. <code>edges.type.sink</code> string <code>client</code> Yes The method specified to import data. Optional values are <code>client</code> and <code>SST</code>. <code>edges.writeMode</code> string <code>INSERT</code> No Types of batch operations on data, including batch inserts, updates, and deletes. Optional values are <code>INSERT</code>, <code>UPDATE</code>, <code>DELETE</code>. <code>edges.fields</code> list[string] - Yes The header or column name of the column corresponding to properties. If there is a header or column name, please use that name directly. If a CSV file does not have a header, use the form of <code>[_c0, _c1, _c2]</code> to represent the first column, the second column, the third column, and so on. <code>edges.nebula.fields</code> list[string] - Yes Edge names defined in NebulaGraph, the order of which must correspond to <code>edges.fields</code>. For example, <code>[_c2, _c3]</code> corresponds to <code>[start_year, end_year]</code>, which means that values in the third column are the values of the start year, and values in the fourth column are the values of the end year. <code>edges.source.field</code> string - Yes The column of source vertices of edges. For example, <code>_c0</code> indicates a value in the first column that is used as the source vertex of an edge. <code>edges.source.prefix</code> string - No Add the specified prefix to the VID. For example, if the VID is <code>12345</code>, adding the prefix <code>tag1</code> will result in <code>tag1_12345</code>. The underscore cannot be modified. <code>edges.source.policy</code> string - No Supports only the value <code>hash</code>. Performs hashing operations on VIDs of type string. <code>edges.target.field</code> string - Yes The column of destination vertices of edges. For example, <code>_c0</code> indicates a value in the first column that is used as the destination vertex of an edge. <code>edges.target.prefix</code> string - No Add the specified prefix to the VID. For example, if the VID is <code>12345</code>, adding the prefix <code>tag1</code> will result in <code>tag1_12345</code>. The underscore cannot be modified. <code>edges.target.policy</code> string - No Supports only the value <code>hash</code>. Performs hashing operations on VIDs of type string. <code>edges.ranking</code> int - No The column of rank values. If not specified, all rank values are <code>0</code> by default. <code>edges.batch</code> int <code>256</code> Yes The maximum number of edges written into NebulaGraph in a single batch. <code>edges.partition</code> int <code>32</code> Yes The number of Spark partitions."},{"location":"nebula-exchange/parameter-reference/ex-ug-parameter/#specific_parameters_for_generating_sst_files_1","title":"Specific parameters for generating SST files","text":"Parameter Type Default value Required Description <code>edges.path</code> string - Yes The path of the source file specified to generate SST files. <code>edges.repartitionWithNebula</code> bool <code>true</code> No Whether to repartition data based on the number of partitions of graph spaces in NebulaGraph when generating the SST file. Enabling this function can reduce the time required to DOWNLOAD and INGEST SST files."},{"location":"nebula-exchange/parameter-reference/ex-ug-parameter/#specific_parameters_of_nebulagraph_1","title":"Specific parameters of NebulaGraph","text":"Parameter Type Default value Required Description <code>edges.path</code> string <code>\"hdfs://namenode:9000/path/edge\"</code> Yes Specifies the storage path of the CSV file. You need to set a new path and Exchange will automatically create the path you set. If you store the data to the HDFS server, the path format is the same as the default value, such as <code>\"hdfs://192.168.8.177:9000/edge/follow\"</code>. If you store the data to the local, the path format is <code>\"file:///path/edge\"</code>, such as <code>\"file:///home/nebula/edge/follow\"</code>. If there are multiple Edges, different directories must be set for each Edge. <code>edges.noField</code> bool <code>false</code> Yes If the value is <code>true</code>, source vertex IDs, destination vertex IDs, and ranks will be exported, not the property data. If the vaue is <code>false</code>, ranks, source vertex IDs, destination vertex IDs, ranks, and the property data will be exported. <code>edges.return.fields</code> list <code>[]</code> Yes Specifies the properties to be exported. For example, to export <code>start_year</code> and <code>end_year</code>, you need to set the parameter value to <code>[\"start_year\",\"end_year\"]</code>. This parameter only takes effect when the value of <code>edges.noField</code> is <code>false</code>."},{"location":"nebula-exchange/use-exchange/ex-ug-export-from-nebula/","title":"Export data from NebulaGraph","text":"<p>The Exchange allows you to export data from NebulaGraph to a CSV file or another NebulaGraph space (supporting different NebulaGraph clusters). This topic describes the specific procedure.</p> <p>Enterpriseonly</p> <p>Only Exchange Enterprise Edition supports exporting data from NebulaGraph.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-export-from-nebula/#preparation","title":"Preparation","text":"<p>This example is completed on a virtual machine equipped with Linux. The hardware and software you need to prepare before exporting data are as follows.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-export-from-nebula/#hardware","title":"Hardware","text":"Type Information CPU 4 Intel(R) Xeon(R) Platinum 8260 CPU @ 2.30GHz Memory 16G Hard disk 50G"},{"location":"nebula-exchange/use-exchange/ex-ug-export-from-nebula/#system","title":"System","text":"<p>CentOS 7.9.2009</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-export-from-nebula/#software","title":"Software","text":"Name Version JDK 1.8.0 Scala 2.12.11 Spark 2.4.7 NebulaGraph 3.5.0"},{"location":"nebula-exchange/use-exchange/ex-ug-export-from-nebula/#dataset","title":"Dataset","text":"<p>As the data source, NebulaGraph stores the basketballplayer dataset in this example, the Schema elements of which are shown as follows.</p> Element Name Property Tag <code>player</code> <code>name string, age int</code> Tag <code>team</code> <code>name string</code> Edge type <code>follow</code> <code>degree int</code> Edge type <code>serve</code> <code>start_year int, end_year int</code>"},{"location":"nebula-exchange/use-exchange/ex-ug-export-from-nebula/#steps","title":"Steps","text":"<ol> <li> <p>Get the JAR file of Exchange Enterprise Edition from the NebulaGraph Enterprise Edition Package.</p> </li> <li> <p>Modify the configuration file.</p> <p>Exchange Enterprise Edition provides the configuration template <code>export_to_csv.conf</code> and <code>export_to_nebula.conf</code> for exporting NebulaGraph data. For details, see Exchange parameters. The core content of the configuration file used in this example is as follows:</p> <ul> <li>Export to a CSV file:</li> </ul> <pre><code># Use the command to submit the exchange job:\n\n# spark-submit \\\n# --master \"spark://master_ip:7077\" \\\n# --driver-memory=2G --executor-memory=30G  \\\n# --total-executor-cores=60 --executor-cores=20 \\\n# --class com.vesoft.nebula.exchange.Exchange \\\n# nebula-exchange-3.0-SNAPSHOT.jar -c export_to_csv.conf\n\n{\n  # Spark config\n  spark: {\n    app: {\n      name: NebulaGraph Exchange\n    }\n  }\n\n  # Nebula Graph config\n  # if you export nebula data to csv, please ignore these nebula config\n  nebula: {\n    address:{\n      graph:[\"127.0.0.1:9669\"]\n\n      # the address of any of the meta services.\n      # if your NebulaGraph server is in virtual network like k8s, please config the leader address of meta.\n      meta:[\"127.0.0.1:9559\"]\n    }\n    user: root\n    pswd: nebula\n    space: test\n\n    # nebula client connection parameters\n    connection {\n      # socket connect &amp; execute timeout, unit: millisecond\n      timeout: 30000\n    }\n\n    error: {\n      # max number of failures, if the number of failures is bigger than max, then exit the   application.\n      max: 32\n      # failed data will be recorded in output path, format with ngql\n      output: /tmp/errors\n    }\n\n    # use google's RateLimiter to limit the requests send to NebulaGraph\n    rate: {\n      # the stable throughput of RateLimiter\n      limit: 1024\n      # Acquires a permit from RateLimiter, unit: MILLISECONDS\n      # if it can't be obtained within the specified timeout, then give up the request.\n      timeout: 1000\n    }\n  }\n\n  # Processing tags\n  tags: [\n    {\n      # you can ignore the tag name when export nebula data to csv\n      name: tag-name-1\n      type: {\n        source: nebula\n        sink: csv\n      }\n      metaAddress:\"127.0.0.1:9559\"\n      space:\"test\"\n      label:\"person\"\n      # config the fields you want to export from nebula. If you want to export all properties, you can set this parameter to empty, that is `fields: []`.\n      fields: [nebula-field-0, nebula-field-1, nebula-field-2]\n      noFields:false  # default false, if true, just export id\n      partition: 60\n      # config the path to save your csv file. if your file in not in hdfs, config \"file:///path/  test.csv\"\n      path: \"hdfs://ip:port/path/person\"\n      separator: \",\"\n      header: true\n    }\n  ]\n\n  # process edges\n  edges: [\n    {\n      # you can ignore the edge name when export nebula data to csv\n      name: edge-name-1\n      type: {\n        source: nebula\n        sink: csv\n      }\n      metaAddress:\"127.0.0.1:9559\"\n      space:\"test\"\n      label:\"friend\"\n      # config the fields you want to export from nebula. If you want to export all properties, you can set this parameter to empty, that is `fields: []`.\n      fields: [nebula-field-0, nebula-field-1, nebula-field-2]\n      noFields:false  # default false, if true, just export id\n      partition: 60\n      # config the path to save your csv file. if your file in not in hdfs, config \"file:///path/  test.csv\"\n      path: \"hdfs://ip:port/path/friend\"\n      separator: \",\"\n      header: true\n    }\n  ]\n}\n</code></pre> <ul> <li>Export to another graph space:</li> </ul> <pre><code># Use the command to submit the exchange job:\n\n# spark-submit \\\n# --master \"spark://master_ip:7077\" \\\n# --driver-memory=2G --executor-memory=30G  \\\n# --total-executor-cores=60 --executor-cores=20 \\\n# --class com.vesoft.nebula.exchange.Exchange \\\n# nebula-exchange-3.0-SNAPSHOT.jar -c export_to_nebula.conf\n\n{\n  # Spark config\n  spark: {\n    app: {\n      name: NebulaGraph Exchange\n    }\n  }\n\n  # Nebula Graph config, just config the sink nebula information\n  nebula: {\n    address:{\n      graph:[\"127.0.0.1:9669\"]\n\n      # the address of any of the meta services\n      meta:[\"127.0.0.1:9559\"]\n    }\n    user: root\n    pswd: nebula\n    space: test\n\n    # nebula client connection parameters\n    connection {\n      # socket connect &amp; execute timeout, unit: millisecond\n      timeout: 30000\n    }\n\n    error: {\n      # max number of failures, if the number of failures is bigger than max, then exit the   application.\n      max: 32\n      # failed data will be recorded in output path, format with ngql\n      output: /tmp/errors\n    }\n\n    # use google's RateLimiter to limit the requests send to NebulaGraph\n    rate: {\n      # the stable throughput of RateLimiter\n      limit: 1024\n      # Acquires a permit from RateLimiter, unit: MILLISECONDS\n      # if it can't be obtained within the specified timeout, then give up the request.\n      timeout: 1000\n    }\n  }\n\n  # Processing tags\n  tags: [\n    {\n      name: tag-name-1\n      type: {\n        source: nebula\n        sink: client\n      }\n      # data source nebula config\n      metaAddress:\"127.0.0.1:9559\"\n      space:\"test\"\n      label:\"person\"\n      # mapping the fields of the original NebulaGraph to the fields of the target NebulaGraph.\n      fields: [source_nebula-field-0, source_nebula-field-1, source_nebula-field-2]\n      nebula.fields: [target_nebula-field-0, target_nebula-field-1, target_nebula-field-2]\n      limit:10000\n      vertex: _vertexId  # must be `_vertexId`\n    # udf:{\n    #            separator:\"_\"\n    #            oldColNames:[field-0,field-1,field-2]\n    #            newColName:new-field\n    #        }\n      batch: 2000\n      partition: 60\n    }\n  ]\n\n  # process edges\n  edges: [\n    {\n      name: edge-name-1\n      type: {\n        source: csv\n        sink: client\n      }\n      # data source nebula config\n      metaAddress:\"127.0.0.1:9559\"\n      space:\"test\"\n      label:\"friend\"\n      fields: [source_nebula-field-0, source_nebula-field-1, source_nebula-field-2]\n      nebula.fields: [target_nebula-field-0, target_nebula-field-1, target_nebula-field-2]\n      limit:1000\n      source: _srcId # must be `_srcId`\n    # udf:{\n    #            separator:\"_\"\n    #            oldColNames:[field-0,field-1,field-2]\n    #            newColName:new-field\n    #        }\n      target: _dstId # must be `_dstId`\n    # udf:{\n    #            separator:\"_\"\n    #            oldColNames:[field-0,field-1,field-2]\n    #            newColName:new-field\n    #        }\n      ranking: source_nebula-field-2\n      batch: 2000\n      partition: 60\n    }\n  ]   \n}\n</code></pre> </li> <li> <p>Export data from NebulaGraph with the following command.</p> <p>Note</p> <p>The parameters of the Driver and Executor process can be modified based on your own machine configuration.</p> <pre><code>&lt;spark_install_path&gt;/bin/spark-submit --master \"spark://&lt;master_ip&gt;:7077\" \\\n--driver-memory=2G --executor-memory=30G \\\n--total-executor-cores=60 --executor-cores=20 \\\n--class com.vesoft.nebula.exchange.Exchange nebula-exchange-x.y.z.jar_path&gt; \\\n-c &lt;conf_file_path&gt;\n</code></pre> <p>The following is an example command to export the data to a CSV file.</p> <pre><code>$ ./spark-submit --master \"spark://192.168.10.100:7077\" \\\n--driver-memory=2G --executor-memory=30G \\\n--total-executor-cores=60 --executor-cores=20 \\\n--class com.vesoft.nebula.exchange.Exchange ~/exchange-ent/nebula-exchange-ent-3.5.0.jar \\\n-c ~/exchange-ent/export_to_csv.conf\n</code></pre> </li> <li> <p>Check the exported data.</p> <ul> <li> <p>Export to a CSV file:</p> <p>Check whether the CSV file is successfully generated under the target path, and check the contents of the CSV file to ensure that the data export is successful.</p> <pre><code>$ hadoop fs -ls /vertex/player\nFound 11 items\n-rw-r--r--   3 nebula supergroup          0 2021-11-05 07:36 /vertex/player/_SUCCESS\n-rw-r--r--   3 nebula supergroup        160 2021-11-05 07:36 /vertex/player/    part-00000-17293020-ba2e-4243-b834-34495c0536b3-c000.csv\n-rw-r--r--   3 nebula supergroup        163 2021-11-05 07:36 /vertex/player/    part-00001-17293020-ba2e-4243-b834-34495c0536b3-c000.csv\n-rw-r--r--   3 nebula supergroup        172 2021-11-05 07:36 /vertex/player/    part-00002-17293020-ba2e-4243-b834-34495c0536b3-c000.csv\n-rw-r--r--   3 nebula supergroup        172 2021-11-05 07:36 /vertex/player/    part-00003-17293020-ba2e-4243-b834-34495c0536b3-c000.csv\n-rw-r--r--   3 nebula supergroup        144 2021-11-05 07:36 /vertex/player/    part-00004-17293020-ba2e-4243-b834-34495c0536b3-c000.csv\n-rw-r--r--   3 nebula supergroup        173 2021-11-05 07:36 /vertex/player/    part-00005-17293020-ba2e-4243-b834-34495c0536b3-c000.csv\n-rw-r--r--   3 nebula supergroup        160 2021-11-05 07:36 /vertex/player/    part-00006-17293020-ba2e-4243-b834-34495c0536b3-c000.csv\n-rw-r--r--   3 nebula supergroup        148 2021-11-05 07:36 /vertex/player/    part-00007-17293020-ba2e-4243-b834-34495c0536b3-c000.csv\n-rw-r--r--   3 nebula supergroup        125 2021-11-05 07:36 /vertex/player/    part-00008-17293020-ba2e-4243-b834-34495c0536b3-c000.csv\n-rw-r--r--   3 nebula supergroup        119 2021-11-05 07:36 /vertex/player/    part-00009-17293020-ba2e-4243-b834-34495c0536b3-c000.csv\n</code></pre> </li> </ul> <ul> <li> <p>Export to another graph space:</p> <p>Log in to the new graph space and check the statistics through <code>SUBMIT JOB STATS</code> and <code>SHOW STATS</code> commands to ensure the data export is successful.</p> </li> </ul> </li> </ol>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-clickhouse/","title":"Import data from ClickHouse","text":"<p>This topic provides an example of how to use Exchange to import data stored on ClickHouse into NebulaGraph.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-clickhouse/#data_set","title":"Data set","text":"<p>This topic takes the basketballplayer dataset as an example.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-clickhouse/#environment","title":"Environment","text":"<p>This example is done on MacOS. Here is the environment configuration information:</p> <ul> <li>Hardware specifications:<ul> <li>CPU: 1.7 GHz Quad-Core Intel Core i7</li> <li>Memory: 16 GB</li> </ul> </li> </ul> <ul> <li>Spark: 2.4.7, stand-alone</li> </ul> <ul> <li>ClickHouse: docker deployment yandex/clickhouse-server tag: latest(2021.07.01)</li> </ul> <ul> <li>NebulaGraph: 3.5.0. Deploy NebulaGraph with Docker Compose.</li> </ul>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-clickhouse/#prerequisites","title":"Prerequisites","text":"<p>Before importing data, you need to confirm the following information:</p> <ul> <li> <p>NebulaGraph has been installed and deployed with the following information:</p> <ul> <li>IP addresses and ports of Graph and Meta services.</li> </ul> <ul> <li>The user name and password with write permission to NebulaGraph.</li> </ul> </li> </ul> <ul> <li>Exchange has been compiled, or download the compiled <code>.jar</code> file directly.</li> </ul> <ul> <li>Spark has been installed.</li> </ul> <ul> <li>Learn about the Schema created in NebulaGraph, including names and properties of Tags and Edge types, and more.</li> </ul>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-clickhouse/#steps","title":"Steps","text":""},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-clickhouse/#step_1_create_the_schema_in_nebulagraph","title":"Step 1: Create the Schema in NebulaGraph","text":"<p>Analyze the data to create a Schema in NebulaGraph by following these steps:</p> <ol> <li> <p>Identify the Schema elements. The Schema elements in the NebulaGraph are shown in the following table.</p> Element Name Property Tag <code>player</code> <code>name string, age int</code> Tag <code>team</code> <code>name string</code> Edge Type <code>follow</code> <code>degree int</code> Edge Type <code>serve</code> <code>start_year int, end_year int</code> </li> <li> <p>Create a graph space basketballplayer in the NebulaGraph and create a Schema as shown below.</p> <pre><code>## Create a graph space.\nnebula&gt; CREATE SPACE basketballplayer \\\n        (partition_num = 10, \\\n        replica_factor = 1, \\\n        vid_type = FIXED_STRING(30));\n\n## Use the graph space basketballplayer.\nnebula&gt; USE basketballplayer;\n\n## Create the Tag player.\nnebula&gt; CREATE TAG player(name string, age int);\n\n## Create the Tag team.\nnebula&gt; CREATE TAG team(name string);\n\n## Create the Edge type follow.\nnebula&gt; CREATE EDGE follow(degree int);\n\n## Create the Edge type serve.\nnebula&gt; CREATE EDGE serve(start_year int, end_year int);\n</code></pre> </li> </ol> <p>For more information, see Quick start workflow.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-clickhouse/#step_2_modify_configuration_files","title":"Step 2: Modify configuration files","text":"<p>After Exchange is compiled, copy the conf file <code>target/classes/application.conf</code> to set ClickHouse data source configuration. In this example, the copied file is called <code>clickhouse_application.conf</code>. For details on each configuration item, see Parameters in the configuration file.</p> <pre><code>{\n  # Spark configuration\n  spark: {\n    app: {\n      name: NebulaGraph Exchange 3.5.0\n    }\n    driver: {\n      cores: 1\n      maxResultSize: 1G\n    }\n    cores: {\n      max: 16\n    }\n  }\n\n# NebulaGraph configuration\n  nebula: {\n    address:{\n      # Specify the IP addresses and ports for Graph and Meta services.\n      # If there are multiple addresses, the format is \"ip1:port\",\"ip2:port\",\"ip3:port\".\n      # Addresses are separated by commas.\n      graph:[\"127.0.0.1:9669\"]\n      # the address of any of the meta services.\n      # if your NebulaGraph server is in virtual network like k8s, please config the leader address of meta.\n      meta:[\"127.0.0.1:9559\"]\n    }\n    # The account entered must have write permission for the NebulaGraph space.\n    user: root\n    pswd: nebula\n    # Fill in the name of the graph space you want to write data to in the NebulaGraph.\n    space: basketballplayer\n    connection: {\n      timeout: 3000\n      retry: 3\n    }\n    execution: {\n      retry: 3\n    }\n    error: {\n      max: 32\n      output: /tmp/errors\n    }\n    rate: {\n      limit: 1024\n      timeout: 1000\n    }\n  }\n  # Processing vertexes\n  tags: [\n    # Set the information about the Tag player.\n    {\n      name: player\n      type: {\n        # Specify the data source file format to ClickHouse.\n        source: clickhouse\n        # Specify how to import the data of vertexes into NebulaGraph: Client or SST.\n        sink: client\n      }\n\n      # JDBC URL of ClickHouse\n      url:\"jdbc:clickhouse://192.168.*.*:8123/basketballplayer\"\n\n      user:\"user\"\n      password:\"123456\"\n\n      # The number of ClickHouse partitions\n      numPartition:\"5\"\n\n      sentence:\"select * from player\"\n\n      # Specify the column names in the player table in fields, and their corresponding values are specified as properties in the NebulaGraph.\n      # The sequence of fields and nebula.fields must correspond to each other.\n      # If multiple column names need to be specified, separate them by commas.\n      fields: [name,age]\n      nebula.fields: [name,age]\n\n      # Specify a column of data in the table as the source of vertex VID in the NebulaGraph.\n      vertex: {\n        field:playerid\n      # udf:{\n      #            separator:\"_\"\n      #            oldColNames:[field-0,field-1,field-2]\n      #            newColName:new-field\n      #        }\n      # Add the specified prefix to the VID. For example, if the VID is `12345`, adding the prefix `tag1` will result in `tag1_12345`. The underscore cannot be modified.\n      # prefix:\"tag1\"\n      # Performs hashing operations on VIDs of type string.\n      # policy:hash\n      }\n\n      # Batch operation types, including INSERT, UPDATE, and DELETE. defaults to INSERT.\n      #writeMode: INSERT\n\n      # Whether or not to delete the related incoming and outgoing edges of the vertices when performing a batch delete operation. This parameter takes effect when `writeMode` is `DELETE`.\n      #deleteEdge: false\n\n      # The number of data written to NebulaGraph in a single batch.\n      batch: 256\n\n      # The number of Spark partitions.\n      partition: 32\n    }\n\n    # Set the information about the Tag Team.\n    {\n      name: team\n      type: {\n        source: clickhouse\n        sink: client\n      }\n      url:\"jdbc:clickhouse://192.168.*.*:8123/basketballplayer\"\n      user:\"user\"\n      password:\"123456\"\n      numPartition:\"5\"\n      sentence:\"select * from team\"\n      fields: [name]\n      nebula.fields: [name]\n      vertex: {\n        field:teamid\n      }\n      batch: 256\n      partition: 32\n    }\n  ]\n\n  # Processing edges\n  edges: [\n    # Set the information about the Edge Type follow.\n    {\n      # The corresponding Edge Type name in NebulaGraph.\n      name: follow\n\n      type: {\n        # Specify the data source file format to ClickHouse.\n        source: clickhouse\n\n        # Specify how to import the data into NebulaGraph: Client or SST.\n        sink: client\n      }\n\n      # JDBC URL of ClickHouse\n      url:\"jdbc:clickhouse://192.168.*.*:8123/basketballplayer\"\n\n      user:\"user\"\n      password:\"123456\"\n\n      # The number of ClickHouse partitions.\n      numPartition:\"5\"\n\n      sentence:\"select * from follow\"\n\n      # Specify the column names in the follow table in fields, and their corresponding values are specified as properties in the NebulaGraph.\n      # The sequence of fields and nebula.fields must correspond to each other.\n      # If multiple column names need to be specified, separate them by commas.\n      fields: [degree]\n      nebula.fields: [degree]\n\n      # In source, use a column in the follow table as the source of the edge's source vertexes.\n      source: {\n        field:src_player\n      # udf:{\n      #            separator:\"_\"\n      #            oldColNames:[field-0,field-1,field-2]\n      #            newColName:new-field\n      #        }\n      # Add the specified prefix to the VID. For example, if the VID is `12345`, adding the prefix `tag1` will result in `tag1_12345`. The underscore cannot be modified.\n      # prefix:\"tag1\"\n      # Performs hashing operations on VIDs of type string.\n      # policy:hash\n      }\n\n      # In target, use a column in the follow table as the source of the edge's destination vertexes.\n      target: {\n        field:dst_player\n      # udf:{\n      #            separator:\"_\"\n      #            oldColNames:[field-0,field-1,field-2]\n      #            newColName:new-field\n      #        }\n      # Add the specified prefix to the VID. For example, if the VID is `12345`, adding the prefix `tag1` will result in `tag1_12345`. The underscore cannot be modified.\n      # prefix:\"tag1\"\n      # Performs hashing operations on VIDs of type string.\n      # policy:hash\n      }\n\n      # (Optional) Specify a column as the source of the rank.\n      #ranking: rank\n\n      # Batch operation types, including INSERT, UPDATE, and DELETE. defaults to INSERT.\n      #writeMode: INSERT\n\n      # The number of data written to NebulaGraph in a single batch.\n      batch: 256\n\n      # The number of Spark partitions.\n      partition: 32\n    }\n\n    # Set the information about the Edge Type serve.\n    {\n      name: serve\n      type: {\n        source: clickhouse\n        sink: client\n      }\n      url:\"jdbc:clickhouse://192.168.*.*:8123/basketballplayer\"\n      user:\"user\"\n      password:\"123456\"\n      numPartition:\"5\"\n      sentence:\"select * from serve\"\n      fields: [start_year,end_year]\n      nebula.fields: [start_year,end_year]\n      source: {\n        field:playerid\n      }\n      target: {\n        field:teamid\n      }\n\n      # (Optional) Specify a column as the source of the rank.\n      #ranking: rank\n\n      batch: 256\n      partition: 32\n    }\n  ]\n}\n</code></pre>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-clickhouse/#step_3_import_data_into_nebulagraph","title":"Step 3: Import data into NebulaGraph","text":"<p>Run the following command to import ClickHouse data into NebulaGraph. For descriptions of the parameters, see Options for import.</p> <pre><code>${SPARK_HOME}/bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange &lt;nebula-exchange-3.5.0.jar_path&gt; -c &lt;clickhouse_application.conf_path&gt;\n</code></pre> <p>Note</p> <p>JAR packages are available in two ways: compiled them yourself, or download the compiled <code>.jar</code> file directly.</p> <p>For example:</p> <pre><code>${SPARK_HOME}/bin/spark-submit  --master \"local\" --class com.vesoft.nebula.exchange.Exchange  /root/nebula-exchange/nebula-exchange/target/nebula-exchange-3.5.0.jar  -c /root/nebula-exchange/nebula-exchange/target/classes/clickhouse_application.conf\n</code></pre> <p>You can search for <code>batchSuccess.&lt;tag_name/edge_name&gt;</code> in the command output to check the number of successes. For example, <code>batchSuccess.follow: 300</code>.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-clickhouse/#step_4_optional_validate_data","title":"Step 4: (optional) Validate data","text":"<p>Users can verify that data has been imported by executing a query in the NebulaGraph client (for example, NebulaGraph Studio). For example:</p> <pre><code>LOOKUP ON player YIELD id(vertex);\n</code></pre> <p>Users can also run the SHOW STATS command to view statistics.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-clickhouse/#step_5_optional_rebuild_indexes_in_nebulagraph","title":"Step 5: (optional) Rebuild indexes in NebulaGraph","text":"<p>With the data imported, users can recreate and rebuild indexes in NebulaGraph. For details, see Index overview.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-csv/","title":"Import data from CSV files","text":"<p>This topic provides an example of how to use Exchange to import NebulaGraph data stored in HDFS or local CSV files.</p> <p>To import a local CSV file to NebulaGraph, see NebulaGraph Importer.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-csv/#data_set","title":"Data set","text":"<p>This topic takes the basketballplayer dataset as an example.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-csv/#environment","title":"Environment","text":"<p>This example is done on MacOS. Here is the environment configuration information:</p> <ul> <li>Hardware specifications:<ul> <li>CPU: 1.7 GHz Quad-Core Intel Core i7</li> <li>Memory: 16 GB</li> </ul> </li> </ul> <ul> <li>Spark: 2.4.7, stand-alone</li> </ul> <ul> <li>Hadoop: 2.9.2, pseudo-distributed deployment</li> </ul> <ul> <li>NebulaGraph: 3.5.0. Deploy NebulaGraph with Docker Compose.</li> </ul>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-csv/#prerequisites","title":"Prerequisites","text":"<p>Before importing data, you need to confirm the following information:</p> <ul> <li> <p>NebulaGraph has been installed and deployed with the following information:</p> <ul> <li>IP addresses and ports of Graph and Meta services.</li> </ul> <ul> <li>The user name and password with write permission to NebulaGraph.</li> </ul> </li> </ul> <ul> <li>Exchange has been compiled, or download the compiled <code>.jar</code> file directly.</li> </ul> <ul> <li>Spark has been installed.</li> </ul> <ul> <li>Learn about the Schema created in NebulaGraph, including names and properties of Tags and Edge types, and more.</li> </ul> <ul> <li>If files are stored in HDFS, ensure that the Hadoop service is running normally.</li> </ul> <ul> <li>If files are stored locally and NebulaGraph is a cluster architecture, you need to place the files in the same directory locally on each machine in the cluster.</li> </ul>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-csv/#steps","title":"Steps","text":""},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-csv/#step_1_create_the_schema_in_nebulagraph","title":"Step 1: Create the Schema in NebulaGraph","text":"<p>Analyze the data to create a Schema in NebulaGraph by following these steps:</p> <ol> <li> <p>Identify the Schema elements. The Schema elements in the NebulaGraph are shown in the following table.</p> Element Name Property Tag <code>player</code> <code>name string, age int</code> Tag <code>team</code> <code>name string</code> Edge Type <code>follow</code> <code>degree int</code> Edge Type <code>serve</code> <code>start_year int, end_year int</code> </li> <li> <p>Create a graph space basketballplayer in the NebulaGraph and create a Schema as shown below.</p> <pre><code>## Create a graph space.\nnebula&gt; CREATE SPACE basketballplayer \\\n        (partition_num = 10, \\\n        replica_factor = 1, \\\n        vid_type = FIXED_STRING(30));\n\n## Use the graph space basketballplayer.\nnebula&gt; USE basketballplayer;\n\n## Create the Tag player.\nnebula&gt; CREATE TAG player(name string, age int);\n\n## Create the Tag team.\nnebula&gt; CREATE TAG team(name string);\n\n## Create the Edge type follow.\nnebula&gt; CREATE EDGE follow(degree int);\n\n## Create the Edge type serve.\nnebula&gt; CREATE EDGE serve(start_year int, end_year int);\n</code></pre> </li> </ol> <p>For more information, see Quick start workflow.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-csv/#step_2_process_csv_files","title":"Step 2: Process CSV files","text":"<p>Confirm the following information:</p> <ol> <li> <p>Process CSV files to meet Schema requirements.</p> <p>Note</p> <p>Exchange supports uploading CSV files with or without headers.</p> </li> <li> <p>Obtain the CSV file storage path.</p> </li> </ol>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-csv/#step_3_modify_configuration_files","title":"Step 3: Modify configuration files","text":"<p>After Exchange is compiled, copy the conf file <code>target/classes/application.conf</code> to set CSV data source configuration. In this example, the copied file is called <code>csv_application.conf</code>. For details on each configuration item, see Parameters in the configuration file.</p> <pre><code>{\n  # Spark configuration\n  spark: {\n    app: {\n      name: NebulaGraph Exchange 3.5.0\n    }\n    driver: {\n      cores: 1\n      maxResultSize: 1G\n    }\n    executor: {\n        memory:1G\n    }\n\n    cores: {\n      max: 16\n    }\n  }\n\n  # NebulaGraph configuration\n  nebula: {\n    address:{\n      # Specify the IP addresses and ports for Graph and Meta services.\n      # If there are multiple addresses, the format is \"ip1:port\",\"ip2:port\",\"ip3:port\".\n      # Addresses are separated by commas.\n      graph:[\"127.0.0.1:9669\"]\n      # the address of any of the meta services.\n      # if your NebulaGraph server is in virtual network like k8s, please config the leader address of meta.\n      meta:[\"127.0.0.1:9559\"]\n    }\n\n    # The account entered must have write permission for the NebulaGraph space.\n    user: root\n    pswd: nebula\n\n    # Fill in the name of the graph space you want to write data to in the NebulaGraph.\n    space: basketballplayer\n    connection: {\n      timeout: 3000\n      retry: 3\n    }\n    execution: {\n      retry: 3\n    }\n    error: {\n      max: 32\n      output: /tmp/errors\n    }\n    rate: {\n      limit: 1024\n      timeout: 1000\n    }\n  }\n\n  # Processing vertexes\n  tags: [\n    # Set the information about the Tag player.\n    {\n      # Specify the Tag name defined in NebulaGraph.\n      name: player\n      type: {\n        # Specify the data source file format to CSV.\n        source: csv\n\n        # Specify how to import the data into NebulaGraph: Client or SST.\n        sink: client\n      }\n\n      # Specify the path to the CSV file.\n      # If the file is stored in HDFS, use double quotation marks to enclose the file path, starting with hdfs://. For example: \"hdfs://ip:port/xx/xx\".\n      # If the file is stored locally, use double quotation marks to enclose the file path, starting with file://. For example: \"file:///tmp/xx.csv\".\n      path: \"hdfs://192.168.*.*:9000/data/vertex_player.csv\"\n\n      # If the CSV file does not have a header, use [_c0, _c1, _c2, ..., _cn] to represent its header and indicate the columns as the source of the property values.\n      # If the CSV file has headers, use the actual column names.\n      fields: [_c1, _c2]\n\n      # Specify the column names in the player table in fields, and their corresponding values are specified as properties in the NebulaGraph.\n      # The sequence of fields and nebula.fields must correspond to each other.\n      nebula.fields: [age, name]\n\n      # Specify a column of data in the table as the source of vertex VID in the NebulaGraph.\n      # The value of vertex must be the same as the column names in the above fields or csv.fields.\n      # Currently, NebulaGraph 3.5.0 supports only strings or integers of VID.\n      vertex: {\n        field:_c0\n      # udf:{\n      #            separator:\"_\"\n      #            oldColNames:[field-0,field-1,field-2]\n      #            newColName:new-field\n      #        }\n      # Add the specified prefix to the VID. For example, if the VID is `12345`, adding the prefix `tag1` will result in `tag1_12345`. The underscore cannot be modified.\n      # prefix:\"tag1\"\n      # Performs hashing operations on VIDs of type string.\n      # policy:hash\n      }\n\n      # The delimiter specified. The default value is comma.\n      separator: \",\"\n\n      # If the CSV file has a header, set the header to true.\n      # If the CSV file does not have a header, set the header to false. The default value is false.\n      header: false\n\n      # Batch operation types, including INSERT, UPDATE, and DELETE. defaults to INSERT.\n      #writeMode: INSERT\n\n      # Whether or not to delete the related incoming and outgoing edges of the vertices when performing a batch delete operation. This parameter takes effect when `writeMode` is `DELETE`.\n      #deleteEdge: false\n\n      # The number of data written to NebulaGraph in a single batch.\n      batch: 256\n\n      # The number of Spark partitions.\n      partition: 32\n    }\n\n    # Set the information about the Tag Team.\n    {\n      name: team\n      type: {\n        source: csv\n        sink: client\n      }\n      path: \"hdfs://192.168.*.*:9000/data/vertex_team.csv\"\n      fields: [_c1]\n      nebula.fields: [name]\n      vertex: {\n        field:_c0\n      }\n      separator: \",\"\n      header: false\n      batch: 256\n      partition: 32\n    }\n\n\n    # If more vertexes need to be added, refer to the previous configuration to add them.\n  ]\n  # Processing edges\n  edges: [\n    # Set the information about the Edge Type follow.\n    {\n      # Specify the Edge Type name defined in NebulaGraph.\n      name: follow\n      type: {\n        # Specify the data source file format to CSV.\n        source: csv\n\n        # Specify how to import the data into NebulaGraph: Client or SST.\n        sink: client\n      }\n\n      # Specify the path to the CSV file.\n      # If the file is stored in HDFS, use double quotation marks to enclose the file path, starting with hdfs://. For example: \"hdfs://ip:port/xx/xx\".\n      # If the file is stored locally, use double quotation marks to enclose the file path, starting with file://. For example: \"file:///tmp/xx.csv\".\n      path: \"hdfs://192.168.*.*:9000/data/edge_follow.csv\"\n\n      # If the CSV file does not have a header, use [_c0, _c1, _c2, ..., _cn] to represent its header and indicate the columns as the source of the property values.\n      # If the CSV file has headers, use the actual column names.\n      fields: [_c2]\n\n      # Specify the column names in the edge table in fields, and their corresponding values are specified as properties in the NebulaGraph.\n      # The sequence of fields and nebula.fields must correspond to each other.\n      nebula.fields: [degree]\n\n      # Specify a column as the source for the source and destination vertexes.\n      # The value of vertex must be the same as the column names in the above fields or csv.fields.\n      # Currently, NebulaGraph 3.5.0 supports only strings or integers of VID.\n      source: {\n        field: _c0\n      # udf:{\n      #            separator:\"_\"\n      #            oldColNames:[field-0,field-1,field-2]\n      #            newColName:new-field\n      #        }\n      # Add the specified prefix to the VID. For example, if the VID is `12345`, adding the prefix `tag1` will result in `tag1_12345`. The underscore cannot be modified.\n      # prefix:\"tag1\"\n      # Performs hashing operations on VIDs of type string.\n      # policy:hash\n      }\n      target: {\n        field: _c1\n      # udf:{\n      #            separator:\"_\"\n      #            oldColNames:[field-0,field-1,field-2]\n      #            newColName:new-field\n      #        }\n      # Add the specified prefix to the VID. For example, if the VID is `12345`, adding the prefix `tag1` will result in `tag1_12345`. The underscore cannot be modified.\n      # prefix:\"tag1\"\n      # Performs hashing operations on VIDs of type string.\n      # policy:hash\n      }\n\n      # The delimiter specified. The default value is comma.\n      separator: \",\"\n\n      # Specify a column as the source of the rank (optional).\n\n      #ranking: rank\n\n      # If the CSV file has a header, set the header to true.\n      # If the CSV file does not have a header, set the header to false. The default value is false.\n      header: false\n\n      # Batch operation types, including INSERT, UPDATE, and DELETE. defaults to INSERT.\n      #writeMode: INSERT\n\n      # The number of data written to NebulaGraph in a single batch.\n      batch: 256\n\n      # The number of Spark partitions.\n      partition: 32\n    }\n\n    # Set the information about the Edge Type serve.\n    {\n      name: serve\n      type: {\n        source: csv\n        sink: client\n      }\n      path: \"hdfs://192.168.*.*:9000/data/edge_serve.csv\"\n      fields: [_c2,_c3]\n      nebula.fields: [start_year, end_year]\n      source: {\n        field: _c0\n      }\n      target: {\n        field: _c1\n      }\n      separator: \",\"\n      header: false\n      batch: 256\n      partition: 32\n    }\n\n  ]\n    # If more edges need to be added, refer to the previous configuration to add them.\n}\n</code></pre>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-csv/#step_4_import_data_into_nebulagraph","title":"Step 4: Import data into NebulaGraph","text":"<p>Run the following command to import CSV data into NebulaGraph. For descriptions of the parameters, see Options for import.</p> <pre><code>${SPARK_HOME}/bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange &lt;nebula-exchange-3.5.0.jar_path&gt; -c &lt;csv_application.conf_path&gt; \n</code></pre> <p>Note</p> <p>JAR packages are available in two ways: compiled them yourself, or download the compiled <code>.jar</code> file directly.</p> <p>For example:</p> <pre><code>${SPARK_HOME}/bin/spark-submit  --master \"local\" --class com.vesoft.nebula.exchange.Exchange  /root/nebula-exchange/nebula-exchange/target/nebula-exchange-3.5.0.jar  -c /root/nebula-exchange/nebula-exchange/target/classes/csv_application.conf\n</code></pre> <p>You can search for <code>batchSuccess.&lt;tag_name/edge_name&gt;</code> in the command output to check the number of successes. For example, <code>batchSuccess.follow: 300</code>.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-csv/#step_5_optional_validate_data","title":"Step 5: (optional) Validate data","text":"<p>Users can verify that data has been imported by executing a query in the NebulaGraph client (for example, NebulaGraph Studio). For example:</p> <pre><code>LOOKUP ON player YIELD id(vertex);\n</code></pre> <p>Users can also run the <code>SHOW STATS</code> command to view statistics.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-csv/#step_6_optional_rebuild_indexes_in_nebulagraph","title":"Step 6: (optional) Rebuild indexes in NebulaGraph","text":"<p>With the data imported, users can recreate and rebuild indexes in NebulaGraph. For details, see Index overview.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-hbase/","title":"Import data from HBase","text":"<p>This topic provides an example of how to use Exchange to import NebulaGraph data stored in HBase.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-hbase/#data_set","title":"Data set","text":"<p>This topic takes the basketballplayer dataset as an example.</p> <p>In this example, the data set has been stored in HBase. All vertexes and edges are stored in the <code>player</code>, <code>team</code>, <code>follow</code>, and <code>serve</code> tables. The following are some of the data for each table.</p> <pre><code>hbase(main):002:0&gt; scan \"player\"\nROW                                COLUMN+CELL\n player100                         column=cf:age, timestamp=1618881347530, value=42\n player100                         column=cf:name, timestamp=1618881354604, value=Tim Duncan\n player101                         column=cf:age, timestamp=1618881369124, value=36\n player101                         column=cf:name, timestamp=1618881379102, value=Tony Parker\n player102                         column=cf:age, timestamp=1618881386987, value=33\n player102                         column=cf:name, timestamp=1618881393370, value=LaMarcus Aldridge\n player103                         column=cf:age, timestamp=1618881402002, value=32\n player103                         column=cf:name, timestamp=1618881407882, value=Rudy Gay\n ...\n\nhbase(main):003:0&gt; scan \"team\"\nROW                                COLUMN+CELL\n team200                           column=cf:name, timestamp=1618881445563, value=Warriors\n team201                           column=cf:name, timestamp=1618881453636, value=Nuggets\n ...\n\nhbase(main):004:0&gt; scan \"follow\"\nROW                                COLUMN+CELL\n player100                         column=cf:degree, timestamp=1618881804853, value=95\n player100                         column=cf:dst_player, timestamp=1618881791522, value=player101\n player101                         column=cf:degree, timestamp=1618881824685, value=90\n player101                         column=cf:dst_player, timestamp=1618881816042, value=player102\n ...\n\nhbase(main):005:0&gt; scan \"serve\"\nROW                                COLUMN+CELL\n player100                         column=cf:end_year, timestamp=1618881899333, value=2016\n player100                         column=cf:start_year, timestamp=1618881890117, value=1997\n player100                         column=cf:teamid, timestamp=1618881875739, value=team204\n ...\n</code></pre>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-hbase/#environment","title":"Environment","text":"<p>This example is done on MacOS. Here is the environment configuration information:</p> <ul> <li>Hardware specifications:<ul> <li>CPU: 1.7 GHz Quad-Core Intel Core i7</li> <li>Memory: 16 GB</li> </ul> </li> </ul> <ul> <li>Spark: 2.4.7, stand-alone</li> </ul> <ul> <li>HBase: 2.2.7</li> </ul> <ul> <li>NebulaGraph: 3.5.0. Deploy NebulaGraph with Docker Compose.</li> </ul>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-hbase/#prerequisites","title":"Prerequisites","text":"<p>Before importing data, you need to confirm the following information:</p> <ul> <li> <p>NebulaGraph has been installed and deployed with the following information:</p> <ul> <li>IP addresses and ports of Graph and Meta services.</li> </ul> <ul> <li>The user name and password with write permission to NebulaGraph.</li> </ul> </li> </ul> <ul> <li>Exchange has been compiled, or download the compiled <code>.jar</code> file directly.</li> </ul> <ul> <li>Spark has been installed.</li> </ul> <ul> <li>Learn about the Schema created in NebulaGraph, including names and properties of Tags and Edge types, and more.</li> </ul>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-hbase/#steps","title":"Steps","text":""},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-hbase/#step_1_create_the_schema_in_nebulagraph","title":"Step 1: Create the Schema in NebulaGraph","text":"<p>Analyze the data to create a Schema in NebulaGraph by following these steps:</p> <ol> <li> <p>Identify the Schema elements. The Schema elements in the NebulaGraph are shown in the following table.</p> Element Name Property Tag <code>player</code> <code>name string, age int</code> Tag <code>team</code> <code>name string</code> Edge Type <code>follow</code> <code>degree int</code> Edge Type <code>serve</code> <code>start_year int, end_year int</code> </li> <li> <p>Create a graph space basketballplayer in the NebulaGraph and create a Schema as shown below.</p> <pre><code>## Create a graph space.\nnebula&gt; CREATE SPACE basketballplayer \\\n        (partition_num = 10, \\\n        replica_factor = 1, \\\n        vid_type = FIXED_STRING(30));\n\n## Use the graph space basketballplayer.\nnebula&gt; USE basketballplayer;\n\n## Create the Tag player.\nnebula&gt; CREATE TAG player(name string, age int);\n\n## Create the Tag team.\nnebula&gt; CREATE TAG team(name string);\n\n## Create the Edge type follow.\nnebula&gt; CREATE EDGE follow(degree int);\n\n## Create the Edge type serve.\nnebula&gt; CREATE EDGE serve(start_year int, end_year int);\n</code></pre> </li> </ol> <p>For more information, see Quick start workflow.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-hbase/#step_2_modify_configuration_files","title":"Step 2: Modify configuration files","text":"<p>After Exchange is compiled, copy the conf file <code>target/classes/application.conf</code> to set HBase data source configuration. In this example, the copied file is called <code>hbase_application.conf</code>. For details on each configuration item, see Parameters in the configuration file.</p> <pre><code>{\n  # Spark configuration\n  spark: {\n    app: {\n      name: NebulaGraph Exchange 3.5.0\n    }\n    driver: {\n      cores: 1\n      maxResultSize: 1G\n    }\n    cores: {\n      max: 16\n    }\n  }\n\n\n  # NebulaGraph configuration\n  nebula: {\n    address:{\n      # Specify the IP addresses and ports for Graph and all Meta services.\n      # If there are multiple addresses, the format is \"ip1:port\",\"ip2:port\",\"ip3:port\".\n      # Addresses are separated by commas.\n      graph:[\"127.0.0.1:9669\"]\n      # the address of any of the meta services.\n      # if your NebulaGraph server is in virtual network like k8s, please config the leader address of meta.\n      meta:[\"127.0.0.1:9559\"]\n    }\n    # The account entered must have write permission for the NebulaGraph space.\n    user: root\n    pswd: nebula\n    # Fill in the name of the graph space you want to write data to in the NebulaGraph.\n    space: basketballplayer\n    connection: {\n      timeout: 3000\n      retry: 3\n    }\n    execution: {\n      retry: 3\n    }\n    error: {\n      max: 32\n      output: /tmp/errors\n    }\n    rate: {\n      limit: 1024\n      timeout: 1000\n    }\n  }\n  # Processing vertexes\n  tags: [\n    # Set information about Tag player.\n    # If you want to set RowKey as the data source, enter rowkey and the actual column name of the column family.\n    {\n      # The Tag name in NebulaGraph.\n      name: player\n      type: {\n        # Specify the data source file format to HBase.\n        source: hbase\n        # Specify how to import the data into NebulaGraph: Client or SST.\n        sink: client\n      }\n      host:192.168.*.*\n      port:2181\n      table:\"player\"\n      columnFamily:\"cf\"\n\n      # Specify the column names in the player table in fields, and their corresponding values are specified as properties in the NebulaGraph.\n      # The sequence of fields and nebula.fields must correspond to each other.\n      # If multiple column names need to be specified, separate them by commas.\n      fields: [age,name]\n      nebula.fields: [age,name]\n\n      # Specify a column of data in the table as the source of vertex VID in the NebulaGraph.\n      # For example, if rowkey is the source of the VID, enter rowkey.\n      vertex:{\n          field:rowkey\n      # udf:{\n      #            separator:\"_\"\n      #            oldColNames:[field-0,field-1,field-2]\n      #            newColName:new-field\n      #        }\n      # Add the specified prefix to the VID. For example, if the VID is `12345`, adding the prefix `tag1` will result in `tag1_12345`. The underscore cannot be modified.\n      # prefix:\"tag1\"\n      # Performs hashing operations on VIDs of type string.\n      # policy:hash\n      }\n\n      # Batch operation types, including INSERT, UPDATE, and DELETE. defaults to INSERT.\n      #writeMode: INSERT\n\n      # Whether or not to delete the related incoming and outgoing edges of the vertices when performing a batch delete operation. This parameter takes effect when `writeMode` is `DELETE`.\n      #deleteEdge: false\n\n      # Number of pieces of data written to NebulaGraph in a single batch.\n      batch: 256\n\n      # Number of Spark partitions\n      partition: 32\n    }\n    # Set Tag Team information.\n    {\n      name: team\n      type: {\n        source: hbase\n        sink: client\n      }\n      host:192.168.*.*\n      port:2181\n      table:\"team\"\n      columnFamily:\"cf\"\n      fields: [name]\n      nebula.fields: [name]\n      vertex:{\n          field:rowkey\n      }\n      batch: 256\n      partition: 32\n    }\n\n  ]\n\n  # Processing edges\n  edges: [\n    # Set the information about the Edge Type follow.\n    {\n      # The corresponding Edge Type name in NebulaGraph.\n      name: follow\n\n      type: {\n        # Specify the data source file format to HBase.\n        source: hbase\n\n        # Specify how to import the Edge type data into NebulaGraph.\n        # Specify how to import the data into NebulaGraph: Client or SST.\n        sink: client\n      }\n\n      host:192.168.*.*\n      port:2181\n      table:\"follow\"\n      columnFamily:\"cf\"\n\n      # Specify the column names in the follow table in fields, and their corresponding values are specified as properties in the NebulaGraph.\n      # The sequence of fields and nebula.fields must correspond to each other.\n      # If multiple column names need to be specified, separate them by commas.\n      fields: [degree]\n      nebula.fields: [degree]\n\n      # In source, use a column in the follow table as the source of the edge's source vertex.\n      # In target, use a column in the follow table as the source of the edge's destination vertex.\n      source:{\n          field:rowkey\n      # udf:{\n      #            separator:\"_\"\n      #            oldColNames:[field-0,field-1,field-2]\n      #            newColName:new-field\n      #        }\n      # Add the specified prefix to the VID. For example, if the VID is `12345`, adding the prefix `tag1` will result in `tag1_12345`. The underscore cannot be modified.\n      # prefix:\"tag1\"\n      # Performs hashing operations on VIDs of type string.\n      # policy:hash\n      }\n\n\n      target:{\n          field:dst_player\n      # udf:{\n      #            separator:\"_\"\n      #            oldColNames:[field-0,field-1,field-2]\n      #            newColName:new-field\n      #        }\n      # Add the specified prefix to the VID. For example, if the VID is `12345`, adding the prefix `tag1` will result in `tag1_12345`. The underscore cannot be modified.\n      # prefix:\"tag1\"\n      # Performs hashing operations on VIDs of type string.\n      # policy:hash\n      }\n\n      # (Optional) Specify a column as the source of the rank.\n      #ranking: rank\n\n      # Batch operation types, including INSERT, UPDATE, and DELETE. defaults to INSERT.\n      #writeMode: INSERT\n\n      # The number of data written to NebulaGraph in a single batch.\n      batch: 256\n\n      # The number of Spark partitions.\n      partition: 32\n    }\n\n    # Set the information about the Edge Type serve.\n    {\n      name: serve\n      type: {\n        source: hbase\n        sink: client\n      }\n      host:192.168.*.*\n      port:2181\n      table:\"serve\"\n      columnFamily:\"cf\"\n\n      fields: [start_year,end_year]\n      nebula.fields: [start_year,end_year]\n      source:{\n          field:rowkey\n      }\n\n      target:{\n          field:teamid\n      }\n\n      # (Optional) Specify a column as the source of the rank.\n      #ranking: rank\n\n      batch: 256\n      partition: 32\n    }\n  ]\n}\n</code></pre>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-hbase/#step_3_import_data_into_nebulagraph","title":"Step 3: Import data into NebulaGraph","text":"<p>Run the following command to import HBase data into NebulaGraph. For descriptions of the parameters, see Options for import.</p> <pre><code>${SPARK_HOME}/bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange &lt;nebula-exchange-3.5.0.jar_path&gt; -c &lt;hbase_application.conf_path&gt;\n</code></pre> <p>Note</p> <p>JAR packages are available in two ways: compiled them yourself, or download the compiled <code>.jar</code> file directly.</p> <p>For example:</p> <pre><code>${SPARK_HOME}/bin/spark-submit  --master \"local\" --class com.vesoft.nebula.exchange.Exchange  /root/nebula-exchange/nebula-exchange/target/nebula-exchange-3.5.0.jar  -c /root/nebula-exchange/nebula-exchange/target/classes/hbase_application.conf\n</code></pre> <p>You can search for <code>batchSuccess.&lt;tag_name/edge_name&gt;</code> in the command output to check the number of successes. For example, <code>batchSuccess.follow: 300</code>.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-hbase/#step_4_optional_validate_data","title":"Step 4: (optional) Validate data","text":"<p>Users can verify that data has been imported by executing a query in the NebulaGraph client (for example, NebulaGraph Studio). For example:</p> <pre><code>LOOKUP ON player YIELD id(vertex);\n</code></pre> <p>Users can also run the SHOW STATS command to view statistics.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-hbase/#step_5_optional_rebuild_indexes_in_nebulagraph","title":"Step 5: (optional) Rebuild indexes in NebulaGraph","text":"<p>With the data imported, users can recreate and rebuild indexes in NebulaGraph. For details, see Index overview.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-hive/","title":"Import data from Hive","text":"<p>This topic provides an example of how to use Exchange to import NebulaGraph data stored in Hive.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-hive/#data_set","title":"Data set","text":"<p>This topic takes the basketballplayer dataset as an example.</p> <p>In this example, the data set has been stored in Hive. All vertexes and edges are stored in the <code>player</code>, <code>team</code>, <code>follow</code>, and <code>serve</code> tables. The following are some of the data for each table.</p> <pre><code>scala&gt; spark.sql(\"describe basketball.player\").show\n+--------+---------+-------+\n|col_name|data_type|comment|\n+--------+---------+-------+\n|playerid|   string|   null|\n|     age|   bigint|   null|\n|    name|   string|   null|\n+--------+---------+-------+\n\nscala&gt; spark.sql(\"describe basketball.team\").show\n+----------+---------+-------+\n|  col_name|data_type|comment|\n+----------+---------+-------+\n|    teamid|   string|   null|\n|      name|   string|   null|\n+----------+---------+-------+\n\nscala&gt; spark.sql(\"describe basketball.follow\").show\n+----------+---------+-------+\n|  col_name|data_type|comment|\n+----------+---------+-------+\n|src_player|   string|   null|\n|dst_player|   string|   null|\n|    degree|   bigint|   null|\n+----------+---------+-------+\n\nscala&gt; spark.sql(\"describe basketball.serve\").show\n+----------+---------+-------+\n|  col_name|data_type|comment|\n+----------+---------+-------+\n|  playerid|   string|   null|\n|    teamid|   string|   null|\n|start_year|   bigint|   null|\n|  end_year|   bigint|   null|\n+----------+---------+-------+\n</code></pre> <p>Note</p> <p>The Hive data type <code>bigint</code> corresponds to the NebulaGraph <code>int</code>.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-hive/#environment","title":"Environment","text":"<p>This example is done on MacOS. Here is the environment configuration information:</p> <ul> <li>Hardware specifications:<ul> <li>CPU: 1.7 GHz Quad-Core Intel Core i7</li> <li>Memory: 16 GB</li> </ul> </li> </ul> <ul> <li>Spark: 2.4.7, stand-alone</li> </ul> <ul> <li>Hadoop: 2.9.2, pseudo-distributed deployment</li> </ul> <ul> <li>Hive: 2.3.7, Hive Metastore database is MySQL 8.0.22</li> </ul> <ul> <li>NebulaGraph: 3.5.0. Deploy NebulaGraph with Docker Compose.</li> </ul>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-hive/#prerequisites","title":"Prerequisites","text":"<p>Before importing data, you need to confirm the following information:</p> <ul> <li> <p>NebulaGraph has been installed and deployed with the following information:</p> <ul> <li>IP addresses and ports of Graph and Meta services.</li> </ul> <ul> <li>The user name and password with write permission to NebulaGraph.</li> </ul> </li> </ul> <ul> <li>Exchange has been compiled, or download the compiled <code>.jar</code> file directly.</li> </ul> <ul> <li>Spark has been installed.</li> </ul> <ul> <li>Learn about the Schema created in NebulaGraph, including names and properties of Tags and Edge types, and more.</li> </ul> <ul> <li>The Hive Metastore database (MySQL in this example) has been started.</li> </ul>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-hive/#steps","title":"Steps","text":""},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-hive/#step_1_create_the_schema_in_nebulagraph","title":"Step 1: Create the Schema in NebulaGraph","text":"<p>Analyze the data to create a Schema in NebulaGraph by following these steps:</p> <ol> <li> <p>Identify the Schema elements. The Schema elements in the NebulaGraph are shown in the following table.</p> Element Name Property Tag <code>player</code> <code>name string, age int</code> Tag <code>team</code> <code>name string</code> Edge Type <code>follow</code> <code>degree int</code> Edge Type <code>serve</code> <code>start_year int, end_year int</code> </li> <li> <p>Create a graph space basketballplayer in the NebulaGraph and create a Schema as shown below.</p> <pre><code>## Create a graph space\nnebula&gt; CREATE SPACE basketballplayer \\\n        (partition_num = 10, \\\n        replica_factor = 1, \\\n        vid_type = FIXED_STRING(30));\n\n## Use the graph space basketballplayer\nnebula&gt; USE basketballplayer;\n\n## Create the Tag player\nnebula&gt; CREATE TAG player(name string, age int);\n\n## Create the Tag team\nnebula&gt; CREATE TAG team(name string);\n\n## Create the Edge type follow\nnebula&gt; CREATE EDGE follow(degree int);\n\n## Create the Edge type serve\nnebula&gt; CREATE EDGE serve(start_year int, end_year int);\n</code></pre> </li> </ol> <p>For more information, see Quick start workflow.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-hive/#step_2_use_spark_sql_to_confirm_hive_sql_statements","title":"Step 2: Use Spark SQL to confirm Hive SQL statements","text":"<p>After the Spark-shell environment is started, run the following statements to ensure that Spark can read data in Hive.</p> <pre><code>scala&gt; sql(\"select playerid, age, name from basketball.player\").show\nscala&gt; sql(\"select teamid, name from basketball.team\").show\nscala&gt; sql(\"select src_player, dst_player, degree from basketball.follow\").show\nscala&gt; sql(\"select playerid, teamid, start_year, end_year from basketball.serve\").show\n</code></pre> <p>The following is the result read from the table <code>basketball.player</code>.</p> <pre><code>+---------+----+-----------------+\n| playerid| age|             name|\n+---------+----+-----------------+\n|player100|  42|       Tim Duncan|\n|player101|  36|      Tony Parker|\n|player102|  33|LaMarcus Aldridge|\n|player103|  32|         Rudy Gay|\n|player104|  32|  Marco Belinelli|\n+---------+----+-----------------+\n...\n</code></pre>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-hive/#step_3_modify_configuration_file","title":"Step 3: Modify configuration file","text":"<p>After Exchange is compiled, copy the conf file <code>target/classes/application.conf</code> to set Hive data source configuration. In this example, the copied file is called <code>hive_application.conf</code>. For details on each configuration item, see Parameters in the configuration file.</p> <pre><code>{\n  # Spark configuration\n  spark: {\n    app: {\n      name: NebulaGraph Exchange 3.5.0\n    }\n    driver: {\n      cores: 1\n      maxResultSize: 1G\n    }\n    cores: {\n      max: 16\n    }\n  }\n\n  # If Spark and Hive are deployed in different clusters, you need to configure the parameters for connecting to Hive. Otherwise, skip these configurations.\n  #hive: {\n  #  waredir: \"hdfs://NAMENODE_IP:9000/apps/svr/hive-xxx/warehouse/\"\n  #  connectionURL: \"jdbc:mysql://your_ip:3306/hive_spark?characterEncoding=UTF-8\"\n  #  connectionDriverName: \"com.mysql.jdbc.Driver\"\n  #  connectionUserName: \"user\"\n  #  connectionPassword: \"password\"\n  #}\n\n  # NebulaGraph configuration\n  nebula: {\n    address:{\n      # Specify the IP addresses and ports for Graph and all Meta services.\n      # If there are multiple addresses, the format is \"ip1:port\",\"ip2:port\",\"ip3:port\".\n      # Addresses are separated by commas.\n      graph:[\"127.0.0.1:9669\"]\n      # the address of any of the meta services.\n      # if your NebulaGraph server is in virtual network like k8s, please config the leader address of meta.\n      meta:[\"127.0.0.1:9559\"]\n    }\n    # The account entered must have write permission for the NebulaGraph space.\n    user: root\n    pswd: nebula\n    # Fill in the name of the graph space you want to write data to in the NebulaGraph.\n    space: basketballplayer\n    connection: {\n      timeout: 3000\n      retry: 3\n    }\n    execution: {\n      retry: 3\n    }\n    error: {\n      max: 32\n      output: /tmp/errors\n    }\n    rate: {\n      limit: 1024\n      timeout: 1000\n    }\n  }\n  # Processing vertexes\n  tags: [\n    # Set the information about the Tag player.\n    {\n      # The Tag name in NebulaGraph.\n      name: player\n      type: {\n        # Specify the data source file format to Hive.\n        source: hive\n        # Specify how to import the data into NebulaGraph: Client or SST.\n        sink: client\n      }\n\n      # Set the SQL statement to read the data of player table in basketball database.\n      exec: \"select playerid, age, name from basketball.player\"\n\n      # Specify the column names in the player table in fields, and their corresponding values are specified as properties in the NebulaGraph.\n      # The sequence of fields and nebula.fields must correspond to each other.\n      # If multiple column names need to be specified, separate them by commas.\n      fields: [age,name]\n      nebula.fields: [age,name]\n\n      # Specify a column of data in the table as the source of vertex VID in the NebulaGraph.\n      vertex:{\n        field:playerid\n      # udf:{\n      #            separator:\"_\"\n      #            oldColNames:[field-0,field-1,field-2]\n      #            newColName:new-field\n      #        }\n      # Add the specified prefix to the VID. For example, if the VID is `12345`, adding the prefix `tag1` will result in `tag1_12345`. The underscore cannot be modified.\n      # prefix:\"tag1\"\n      # Performs hashing operations on VIDs of type string.\n      # policy:hash\n      }\n\n      # Batch operation types, including INSERT, UPDATE, and DELETE. defaults to INSERT.\n      #writeMode: INSERT\n\n      # Whether or not to delete the related incoming and outgoing edges of the vertices when performing a batch delete operation. This parameter takes effect when `writeMode` is `DELETE`.\n      #deleteEdge: false\n\n      # The number of data written to NebulaGraph in a single batch.\n      batch: 256\n\n      # The number of Spark partitions.\n      partition: 32\n    }\n    # Set the information about the Tag Team.\n    {\n      name: team\n      type: {\n        source: hive\n        sink: client\n      }\n      exec: \"select teamid, name from basketball.team\"\n      fields: [name]\n      nebula.fields: [name]\n      vertex: {\n        field: teamid\n      }\n      batch: 256\n      partition: 32\n    }\n\n  ]\n\n  # Processing edges\n  edges: [\n    # Set the information about the Edge Type follow.\n    {\n      # The corresponding Edge Type name in NebulaGraph.\n      name: follow\n\n      type: {\n        # Specify the data source file format to Hive.\n        source: hive\n\n        # Specify how to import the Edge type data into NebulaGraph.\n        # Specify how to import the data into NebulaGraph: Client or SST.\n        sink: client\n      }\n\n      # Set the SQL statement to read the data of follow table in the basketball database.\n      exec: \"select src_player, dst_player, degree from basketball.follow\"\n\n      # Specify the column names in the follow table in Fields, and their corresponding values are specified as properties in the NebulaGraph.\n      # The sequence of fields and nebula.fields must correspond to each other.\n      # If multiple column names need to be specified, separate them by commas.\n      fields: [degree]\n      nebula.fields: [degree]\n\n      # In source, use a column in the follow table as the source of the edge's starting vertex.\n      # In target, use a column in the follow table as the source of the edge's destination vertex.\n      source: {\n        field: src_player\n      # udf:{\n      #            separator:\"_\"\n      #            oldColNames:[field-0,field-1,field-2]\n      #            newColName:new-field\n      #        }\n      # Add the specified prefix to the VID. For example, if the VID is `12345`, adding the prefix `tag1` will result in `tag1_12345`. The underscore cannot be modified.\n      # prefix:\"tag1\"\n      # Performs hashing operations on VIDs of type string.\n      # policy:hash\n      }\n\n      target: {\n        field: dst_player\n      # udf:{\n      #            separator:\"_\"\n      #            oldColNames:[field-0,field-1,field-2]\n      #            newColName:new-field\n      #        }\n      # Add the specified prefix to the VID. For example, if the VID is `12345`, adding the prefix `tag1` will result in `tag1_12345`. The underscore cannot be modified.\n      # prefix:\"tag1\"\n      # Performs hashing operations on VIDs of type string.\n      # policy:hash\n      }\n\n      # (Optional) Specify a column as the source of the rank.\n      #ranking: rank\n\n      # Batch operation types, including INSERT, UPDATE, and DELETE. defaults to INSERT.\n      #writeMode: INSERT\n\n      # The number of data written to NebulaGraph in a single batch.\n      batch: 256\n\n      # The number of Spark partitions.\n      partition: 32\n    }\n\n    # Set the information about the Edge Type serve.\n    {\n      name: serve\n      type: {\n        source: hive\n        sink: client\n      }\n      exec: \"select playerid, teamid, start_year, end_year from basketball.serve\"\n      fields: [start_year,end_year]\n      nebula.fields: [start_year,end_year]\n      source: {\n        field: playerid\n      }\n      target: {\n        field: teamid\n      }\n\n      # (Optional) Specify a column as the source of the rank.\n      #ranking: rank\n\n      batch: 256\n      partition: 32\n    }\n  ]\n}\n</code></pre>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-hive/#step_4_import_data_into_nebulagraph","title":"Step 4: Import data into NebulaGraph","text":"<p>Run the following command to import Hive data into NebulaGraph. For a description of the parameters, see Options for import.</p> <pre><code>${SPARK_HOME}/bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange &lt;nebula-exchange-3.5.0.jar_path&gt; -c &lt;hive_application.conf_path&gt; -h\n</code></pre> <p>Note</p> <p>JAR packages are available in two ways: compiled them yourself, or download the compiled <code>.jar</code> file directly.</p> <p>For example:</p> <pre><code>${SPARK_HOME}/bin/spark-submit  --master \"local\" --class com.vesoft.nebula.exchange.Exchange  /root/nebula-exchange/nebula-exchange/target/nebula-exchange-3.5.0.jar  -c /root/nebula-exchange/nebula-exchange/target/classes/hive_application.conf -h\n</code></pre> <p>You can search for <code>batchSuccess.&lt;tag_name/edge_name&gt;</code> in the command output to check the number of successes. For example, <code>batchSuccess.follow: 300</code>.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-hive/#step_5_optional_validate_data","title":"Step 5: (optional) Validate data","text":"<p>Users can verify that data has been imported by executing a query in the NebulaGraph client (for example, NebulaGraph Studio). For example:</p> <pre><code>LOOKUP ON player YIELD id(vertex);\n</code></pre> <p>Users can also run the SHOW STATS command to view statistics.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-hive/#step_6_optional_rebuild_indexes_in_nebulagraph","title":"Step 6: (optional) Rebuild indexes in NebulaGraph","text":"<p>With the data imported, users can recreate and rebuild indexes in NebulaGraph. For details, see Index overview.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-jdbc/","title":"Import data from general JDBC","text":"<p>JDBC data refers to the data of various databases accessed through the JDBC interface. This topic provides an example of how to use Exchange to export MySQL data and import to NebulaGraph.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-jdbc/#data_set","title":"Data set","text":"<p>This topic takes the basketballplayer dataset as an example.</p> <p>In this example, the data set has been stored in MySQL. All vertexes and edges are stored in the <code>player</code>, <code>team</code>, <code>follow</code>, and <code>serve</code> tables. The following are some of the data for each table.</p> <pre><code>mysql&gt; desc player;\n+----------+-------------+------+-----+---------+-------+\n| Field    | Type        | Null | Key | Default | Extra |\n+----------+-------------+------+-----+---------+-------+\n| playerid | int         | YES  |     | NULL    |       |\n| age      | int         | YES  |     | NULL    |       |\n| name     | varchar(30) | YES  |     | NULL    |       |\n+----------+-------------+------+-----+---------+-------+\n\nmysql&gt; desc team;\n+--------+-------------+------+-----+---------+-------+\n| Field  | Type        | Null | Key | Default | Extra |\n+--------+-------------+------+-----+---------+-------+\n| teamid | int         | YES  |     | NULL    |       |\n| name   | varchar(30) | YES  |     | NULL    |       |\n+--------+-------------+------+-----+---------+-------+\n\nmysql&gt; desc follow;\n+------------+-------------+------+-----+---------+-------+\n| Field      | Type        | Null | Key | Default | Extra |\n+------------+-------------+------+-----+---------+-------+\n| src_player | int         | YES  |     | NULL    |       |\n| dst_player | int         | YES  |     | NULL    |       |\n| degree     | int         | YES  |     | NULL    |       |\n+------------+-------------+------+-----+---------+-------+\n\nmysql&gt; desc serve;\n+------------+-------------+------+-----+---------+-------+\n| Field      | Type        | Null | Key | Default | Extra |\n+------------+-------------+------+-----+---------+-------+\n| playerid   | int         | YES  |     | NULL    |       |\n| teamid     | int         | YES  |     | NULL    |       |\n| start_year | int         | YES  |     | NULL    |       |\n| end_year   | int         | YES  |     | NULL    |       |\n+------------+-------------+------+-----+---------+-------+\n</code></pre>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-jdbc/#environment","title":"Environment","text":"<p>This example is done on MacOS. Here is the environment configuration information:</p> <ul> <li>Hardware specifications:<ul> <li>CPU: 1.7 GHz Quad-Core Intel Core i7</li> <li>Memory: 16 GB</li> </ul> </li> </ul> <ul> <li>Spark: 2.4.7, stand-alone</li> </ul> <ul> <li>MySQL: 8.0.23</li> </ul> <ul> <li>NebulaGraph: 3.5.0. Deploy NebulaGraph with Docker Compose.</li> </ul>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-jdbc/#prerequisites","title":"Prerequisites","text":"<p>Before importing data, you need to confirm the following information:</p> <ul> <li> <p>NebulaGraph has been installed and deployed with the following information:</p> <ul> <li>IP addresses and ports of Graph and Meta services.</li> </ul> <ul> <li>The user name and password with write permission to NebulaGraph.</li> </ul> </li> </ul> <ul> <li>Exchange has been compiled, or download the compiled <code>.jar</code> file directly.</li> </ul> <ul> <li>Spark has been installed.</li> </ul> <ul> <li>Learn about the Schema created in NebulaGraph, including names and properties of Tags and Edge types, and more.</li> </ul> <ul> <li>The Hadoop service has been installed and started.</li> </ul>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-jdbc/#precautions","title":"Precautions","text":"<p>nebula-exchange_spark_2.2 supports only single table queries, not multi-table queries.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-jdbc/#steps","title":"Steps","text":""},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-jdbc/#step_1_create_the_schema_in_nebulagraph","title":"Step 1: Create the Schema in NebulaGraph","text":"<p>Analyze the data to create a Schema in NebulaGraph by following these steps:</p> <ol> <li> <p>Identify the Schema elements. The Schema elements in the NebulaGraph are shown in the following table.</p> Element Name Property Tag <code>player</code> <code>name string, age int</code> Tag <code>team</code> <code>name string</code> Edge Type <code>follow</code> <code>degree int</code> Edge Type <code>serve</code> <code>start_year int, end_year int</code> </li> <li> <p>Create a graph space basketballplayer in the NebulaGraph and create a Schema as shown below.</p> <pre><code>## Create a graph space.\nnebula&gt; CREATE SPACE basketballplayer \\\n        (partition_num = 10, \\\n        replica_factor = 1, \\\n        vid_type = FIXED_STRING(30));\n\n## Use the graph space basketballplayer.\nnebula&gt; USE basketballplayer;\n\n## Create the Tag player.\nnebula&gt; CREATE TAG player(name string, age int);\n\n## Create the Tag team.\nnebula&gt; CREATE TAG team(name string);\n\n## Create the Edge type follow.\nnebula&gt; CREATE EDGE follow(degree int);\n\n## Create the Edge type serve.\nnebula&gt; CREATE EDGE serve(start_year int, end_year int);\n</code></pre> </li> </ol> <p>For more information, see Quick start workflow.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-jdbc/#step_2_modify_configuration_files","title":"Step 2: Modify configuration files","text":"<p>After Exchange is compiled, copy the conf file <code>target/classes/application.conf</code> to set JDBC data source configuration. In this case, the copied file is called <code>jdbc_application.conf</code>. For details on each configuration item, see Parameters in the configuration file.</p> <pre><code>{\n  # Spark configuration\n  spark: {\n    app: {\n      name: NebulaGraph Exchange 3.5.0\n    }\n    driver: {\n      cores: 1\n      maxResultSize: 1G\n    }\n    cores: {\n      max: 16\n    }\n  }\n\n  # NebulaGraph configuration\n  nebula: {\n    address:{\n      # Specify the IP addresses and ports for Graph and Meta services.\n      # If there are multiple addresses, the format is \"ip1:port\",\"ip2:port\",\"ip3:port\".\n      # Addresses are separated by commas.\n      graph:[\"127.0.0.1:9669\"]\n      # the address of any of the meta services.\n      # if your NebulaGraph server is in virtual network like k8s, please config the leader address of meta.\n      meta:[\"127.0.0.1:9559\"]\n    }\n    # The account entered must have write permission for the NebulaGraph space.\n    user: root\n    pswd: nebula\n    # Fill in the name of the graph space you want to write data to in the NebulaGraph.\n    space: basketballplayer\n    connection: {\n      timeout: 3000\n      retry: 3\n    }\n    execution: {\n      retry: 3\n    }\n    error: {\n      max: 32\n      output: /tmp/errors\n    }\n    rate: {\n      limit: 1024\n      timeout: 1000\n    }\n  }\n  # Processing vertexes\n  tags: [\n    # Set the information about the Tag player.\n    {\n      # The Tag name in NebulaGraph.\n      name: player\n      type: {\n        # Specify the data source file format to JDBC.\n        source: jdbc\n        # Specify how to import the data into NebulaGraph: Client or SST.\n        sink: client\n      }\n\n      # URL of the JDBC data source. The example is MySql database.\n      url:\"jdbc:mysql://127.0.0.1:3306/basketball?useUnicode=true&amp;characterEncoding=utf-8\"\n\n      # JDBC driver \n      driver:\"com.mysql.cj.jdbc.Driver\"\n\n      # Database user name and password\n      user:\"root\"\n      password:\"12345\"\n\n      # Scanning a single table to read data.\n      # nebula-exchange_spark_2.2 must configure this parameter, and can additionally configure sentence.\n      # nebula-exchange_spark_2.4 and nebula-exchange_spark_3.0 can configure this parameter, but not at the same time as sentence.\n      table:\"basketball.player\"\n\n      # Use query statement to read data.\n      # nebula-exchange_spark_2.2 can configure this parameter. Multi-table queries are not supported. Only the table name needs to be written after from. The form `db.table` is not supported.\n      # nebula-exchange_spark_2.4 and nebula-exchange_spark_3.0 can configure this parameter, but not at the same time as table. Multi-table queries are supported.\n      # sentence:\"select playerid, age, name from player, team order by playerid\"\n\n      # (optional)Multiple connections read parameters. See https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html\n      partitionColumn:playerid    # optional. Must be a numeric, date, or timestamp column from the table in question.\n      lowerBound:1                # optional\n      upperBound:5                # optional\n      numPartitions:5             # optional\n\n\n      fetchSize:2           # The JDBC fetch size, which determines how many rows to fetch per round trip.\n\n      # Specify the column names in the player table in fields, and their corresponding values are specified as properties in the NebulaGraph.\n      # The sequence of fields and nebula.fields must correspond to each other.\n      # If multiple column names need to be specified, separate them by commas.\n      fields: [age,name]\n      nebula.fields: [age,name]\n\n      # Specify a column of data in the table as the source of VIDs in the NebulaGraph.\n      vertex: {\n        field:playerid\n      # udf:{\n      #            separator:\"_\"\n      #            oldColNames:[field-0,field-1,field-2]\n      #            newColName:new-field\n      #        }\n      # Add the specified prefix to the VID. For example, if the VID is `12345`, adding the prefix `tag1` will result in `tag1_12345`. The underscore cannot be modified.\n      # prefix:\"tag1\"\n      # Performs hashing operations on VIDs of type string.\n      # policy:hash\n      }\n\n      # Batch operation types, including INSERT, UPDATE, and DELETE. defaults to INSERT.\n      #writeMode: INSERT\n\n      # Whether or not to delete the related incoming and outgoing edges of the vertices when performing a batch delete operation. This parameter takes effect when `writeMode` is `DELETE`.\n      #deleteEdge: false\n\n      # The number of data written to NebulaGraph in a single batch.\n      batch: 256\n\n      # The number of Spark partitions.\n      partition: 32\n    }\n    # Set the information about the Tag Team.\n    {\n      name: team\n      type: {\n        source: jdbc\n        sink: client\n      }\n\n      url:\"jdbc:mysql://127.0.0.1:3306/basketball?useUnicode=true&amp;characterEncoding=utf-8\"\n      driver:\"com.mysql.cj.jdbc.Driver\"\n      user:root\n      password:\"12345\"\n      table:team\n      sentence:\"select teamid, name from team order by teamid\"\n      partitionColumn:teamid    \n      lowerBound:1                \n      upperBound:5                \n      numPartitions:5             \n      fetchSize:2  \n\n      fields: [name]\n      nebula.fields: [name]\n      vertex: {\n        field: teamid\n      }\n      batch: 256\n      partition: 32\n    }\n\n  ]\n\n  # Processing edges\n  edges: [\n    # Set the information about the Edge Type follow.\n    {\n      # The corresponding Edge Type name in NebulaGraph.\n      name: follow\n\n      type: {\n        # Specify the data source file format to JDBC.\n        source: jdbc\n\n        # Specify how to import the Edge type data into NebulaGraph.\n        # Specify how to import the data into NebulaGraph: Client or SST.\n        sink: client\n      }\n\n      url:\"jdbc:mysql://127.0.0.1:3306/basketball?useUnicode=true&amp;characterEncoding=utf-8\"\n      driver:\"com.mysql.cj.jdbc.Driver\"\n      user:root\n      password:\"12345\"\n\n      # Scanning a single table to read data.\n      # nebula-exchange_spark_2.2 must configure this parameter, and can additionally configure sentence.\n      # nebula-exchange_spark_2.4 and nebula-exchange_spark_3.0 can configure this parameter, but not at the same time as sentence.\n      table:\"basketball.follow\"\n\n      # Use query statement to read data.\n      # nebula-exchange_spark_2.2 can configure this parameter. Multi-table queries are not supported. Only the table name needs to be written after from. The form `db.table` is not supported.\n      # nebula-exchange_spark_2.4 and nebula-exchange_spark_3.0 can configure this parameter, but not at the same time as table. Multi-table queries are supported.\n      # sentence:\"select src_player,dst_player,degree from follow order by src_player\"\n\n      partitionColumn:src_player    \n      lowerBound:1                \n      upperBound:5                \n      numPartitions:5             \n      fetchSize:2  \n\n      # Specify the column names in the follow table in fields, and their corresponding values are specified as properties in the NebulaGraph.\n      # The sequence of fields and nebula.fields must correspond to each other.\n      # If multiple column names need to be specified, separate them by commas.\n      fields: [degree]\n      nebula.fields: [degree]\n\n      # In source, use a column in the follow table as the source of the edge's source vertex.\n      # In target, use a column in the follow table as the source of the edge's destination vertex.\n      source: {\n        field: src_player\n      # udf:{\n      #            separator:\"_\"\n      #            oldColNames:[field-0,field-1,field-2]\n      #            newColName:new-field\n      #        }\n      # Add the specified prefix to the VID. For example, if the VID is `12345`, adding the prefix `tag1` will result in `tag1_12345`. The underscore cannot be modified.\n      # prefix:\"tag1\"\n      # Performs hashing operations on VIDs of type string.\n      # policy:hash\n      }\n\n      target: {\n        field: dst_player\n      # udf:{\n      #            separator:\"_\"\n      #            oldColNames:[field-0,field-1,field-2]\n      #            newColName:new-field\n      #        }\n      # Add the specified prefix to the VID. For example, if the VID is `12345`, adding the prefix `tag1` will result in `tag1_12345`. The underscore cannot be modified.\n      # prefix:\"tag1\"\n      # Performs hashing operations on VIDs of type string.\n      # policy:hash\n      }\n\n      # (Optional) Specify a column as the source of the rank.\n      #ranking: rank\n\n      # Batch operation types, including INSERT, UPDATE, and DELETE. defaults to INSERT.\n      #writeMode: INSERT\n\n      # The number of data written to NebulaGraph in a single batch.\n      batch: 256\n\n      # The number of Spark partitions.\n      partition: 32\n    }\n\n    # Set the information about the Edge Type serve.\n    {\n      name: serve\n      type: {\n        source: jdbc\n        sink: client\n      }\n\n      url:\"jdbc:mysql://127.0.0.1:3306/basketball?useUnicode=true&amp;characterEncoding=utf-8\"\n      driver:\"com.mysql.cj.jdbc.Driver\"\n      user:root\n      password:\"12345\"\n      table:serve\n      sentence:\"select playerid,teamid,start_year,end_year from serve order by playerid\"\n      partitionColumn:playerid    \n      lowerBound:1                \n      upperBound:5                \n      numPartitions:5             \n      fetchSize:2\n\n      fields: [start_year,end_year]\n      nebula.fields: [start_year,end_year]\n      source: {\n        field: playerid\n      }\n      target: {\n        field: teamid\n      }\n      batch: 256\n      partition: 32\n    }\n  ]\n}\n</code></pre>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-jdbc/#step_3_import_data_into_nebulagraph","title":"Step 3: Import data into NebulaGraph","text":"<p>Run the following command to import general JDBC data into NebulaGraph. For a description of the parameters, see Options for import.</p> <pre><code>${SPARK_HOME}/bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange &lt;nebula-exchange-3.5.0.jar_path&gt; -c &lt;jdbc_application.conf_path&gt;\n</code></pre> <p>Note</p> <p>JAR packages are available in two ways: compiled them yourself, or download the compiled <code>.jar</code> file directly.</p> <p>For example:</p> <pre><code>${SPARK_HOME}/bin/spark-submit  --master \"local\" --class com.vesoft.nebula.exchange.Exchange  /root/nebula-exchange/nebula-exchange/target/nebula-exchange-3.5.0.jar  -c /root/nebula-exchange/nebula-exchange/target/classes/jdbc_application.conf\n</code></pre> <p>You can search for <code>batchSuccess.&lt;tag_name/edge_name&gt;</code> in the command output to check the number of successes. For example, <code>batchSuccess.follow: 300</code>.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-jdbc/#step_4_optional_validate_data","title":"Step 4: (optional) Validate data","text":"<p>Users can verify that data has been imported by executing a query in the NebulaGraph client (for example, NebulaGraph Studio). For example:</p> <pre><code>LOOKUP ON player YIELD id(vertex);\n</code></pre> <p>Users can also run the SHOW STATS command to view statistics.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-jdbc/#step_5_optional_rebuild_indexes_in_nebulagraph","title":"Step 5: (optional) Rebuild indexes in NebulaGraph","text":"<p>With the data imported, users can recreate and rebuild indexes in NebulaGraph. For details, see Index overview.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-json/","title":"Import data from JSON files","text":"<p>This topic provides an example of how to use Exchange to import NebulaGraph data stored in HDFS or local JSON files.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-json/#data_set","title":"Data set","text":"<p>This topic takes the basketballplayer dataset as an example. Some sample data are as follows:</p> <ul> <li> <p>player</p> <pre><code>{\"id\":\"player100\",\"age\":42,\"name\":\"Tim Duncan\"}\n{\"id\":\"player101\",\"age\":36,\"name\":\"Tony Parker\"}\n{\"id\":\"player102\",\"age\":33,\"name\":\"LaMarcus Aldridge\"}\n{\"id\":\"player103\",\"age\":32,\"name\":\"Rudy Gay\"}\n...\n</code></pre> </li> </ul> <ul> <li> <p>team</p> <pre><code>{\"id\":\"team200\",\"name\":\"Warriors\"}\n{\"id\":\"team201\",\"name\":\"Nuggets\"}\n...\n</code></pre> </li> </ul> <ul> <li> <p>follow</p> <pre><code>{\"src\":\"player100\",\"dst\":\"player101\",\"degree\":95}\n{\"src\":\"player101\",\"dst\":\"player102\",\"degree\":90}\n...\n</code></pre> </li> </ul> <ul> <li> <p>serve</p> <pre><code>{\"src\":\"player100\",\"dst\":\"team204\",\"start_year\":\"1997\",\"end_year\":\"2016\"}\n{\"src\":\"player101\",\"dst\":\"team204\",\"start_year\":\"1999\",\"end_year\":\"2018\"}\n...\n</code></pre> </li> </ul>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-json/#environment","title":"Environment","text":"<p>This example is done on MacOS. Here is the environment configuration information:</p> <ul> <li>Hardware specifications:<ul> <li>CPU: 1.7 GHz Quad-Core Intel Core i7</li> <li>Memory: 16 GB</li> </ul> </li> </ul> <ul> <li>Spark: 2.4.7, stand-alone</li> </ul> <ul> <li>Hadoop: 2.9.2, pseudo-distributed deployment</li> </ul> <ul> <li>NebulaGraph: 3.5.0. Deploy NebulaGraph with Docker Compose.</li> </ul>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-json/#prerequisites","title":"Prerequisites","text":"<p>Before importing data, you need to confirm the following information:</p> <ul> <li> <p>NebulaGraph has been installed and deployed with the following information:</p> <ul> <li>IP addresses and ports of Graph and Meta services.</li> </ul> <ul> <li>The user name and password with write permission to NebulaGraph.</li> </ul> </li> </ul> <ul> <li>Exchange has been compiled, or download the compiled <code>.jar</code> file directly.</li> </ul> <ul> <li>Spark has been installed.</li> </ul> <ul> <li>Learn about the Schema created in NebulaGraph, including names and properties of Tags and Edge types, and more.</li> </ul> <ul> <li>If files are stored in HDFS, ensure that the Hadoop service is running properly.</li> </ul> <ul> <li>If files are stored locally and NebulaGraph is a cluster architecture, you need to place the files in the same directory locally on each machine in the cluster.</li> </ul>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-json/#steps","title":"Steps","text":""},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-json/#step_1_create_the_schema_in_nebulagraph","title":"Step 1: Create the Schema in NebulaGraph","text":"<p>Analyze the data to create a Schema in NebulaGraph by following these steps:</p> <ol> <li> <p>Identify the Schema elements. The Schema elements in the NebulaGraph are shown in the following table.</p> Element Name Property Tag <code>player</code> <code>name string, age int</code> Tag <code>team</code> <code>name string</code> Edge Type <code>follow</code> <code>degree int</code> Edge Type <code>serve</code> <code>start_year int, end_year int</code> </li> <li> <p>Create a graph space basketballplayer in the NebulaGraph and create a Schema as shown below.</p> <pre><code>## Create a graph space.\nnebula&gt; CREATE SPACE basketballplayer \\\n        (partition_num = 10, \\\n        replica_factor = 1, \\\n        vid_type = FIXED_STRING(30));\n\n## Use the graph space basketballplayer.\nnebula&gt; USE basketballplayer;\n\n## Create the Tag player.\nnebula&gt; CREATE TAG player(name string, age int);\n\n## Create the Tag team.\nnebula&gt; CREATE TAG team(name string);\n\n## Create the Edge type follow.\nnebula&gt; CREATE EDGE follow(degree int);\n\n## Create the Edge type serve.\nnebula&gt; CREATE EDGE serve(start_year int, end_year int);\n</code></pre> </li> </ol> <p>For more information, see Quick start workflow.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-json/#step_2_process_json_files","title":"Step 2: Process JSON files","text":"<p>Confirm the following information:</p> <ol> <li> <p>Process JSON files to meet Schema requirements.</p> </li> <li> <p>Obtain the JSON file storage path.</p> </li> </ol>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-json/#step_3_modify_configuration_files","title":"Step 3: Modify configuration files","text":"<p>After Exchange is compiled, copy the conf file <code>target/classes/application.conf</code> to set JSON data source configuration. In this example, the copied file is called <code>json_application.conf</code>. For details on each configuration item, see Parameters in the configuration file.</p> <pre><code>{\n  # Spark configuration\n  spark: {\n    app: {\n      name: NebulaGraph Exchange 3.5.0\n    }\n    driver: {\n      cores: 1\n      maxResultSize: 1G\n    }\n    executor: {\n        memory:1G\n    }\n\n    cores: {\n      max: 16\n    }\n  }\n\n  # NebulaGraph configuration\n  nebula: {\n    address:{\n      # Specify the IP addresses and ports for Graph and all Meta services.\n      # If there are multiple addresses, the format is \"ip1:port\",\"ip2:port\",\"ip3:port\".\n      # Addresses are separated by commas.\n      graph:[\"127.0.0.1:9669\"]\n      # the address of any of the meta services.\n      # if your NebulaGraph server is in virtual network like k8s, please config the leader address of meta.\n      meta:[\"127.0.0.1:9559\"]\n    }\n\n    # The account entered must have write permission for the NebulaGraph space.\n    user: root\n    pswd: nebula\n\n    # Fill in the name of the graph space you want to write data to in the NebulaGraph.\n    space: basketballplayer\n    connection: {\n      timeout: 3000\n      retry: 3\n    }\n    execution: {\n      retry: 3\n    }\n    error: {\n      max: 32\n      output: /tmp/errors\n    }\n    rate: {\n      limit: 1024\n      timeout: 1000\n    }\n  }\n\n  # Processing vertexes\n  tags: [\n    # Set the information about the Tag player.\n    {\n      # Specify the Tag name defined in NebulaGraph.\n      name: player\n      type: {\n        # Specify the data source file format to JSON.\n        source: json\n\n        # Specify how to import the data into NebulaGraph: Client or SST.\n        sink: client\n      }\n\n      # Specify the path to the JSON file.\n      # If the file is stored in HDFS, use double quotation marks to enclose the file path, starting with hdfs://. For example, \"hdfs://ip:port/xx/xx\".\n      # If the file is stored locally, use double quotation marks to enclose the file path, starting with file://. For example, \"file:///tmp/xx.json\".\n      path: \"hdfs://192.168.*.*:9000/data/vertex_player.json\"\n\n      # Specify the key name in the JSON file in fields, and its corresponding value will serve as the data source for the properties specified in the NebulaGraph.\n      # If multiple column names need to be specified, separate them by commas.\n      fields: [age,name]\n\n      # Specify the column names in the player table in fields, and their corresponding values are specified as properties in the NebulaGraph.\n      # The sequence of fields and nebula.fields must correspond to each other.\n      nebula.fields: [age, name]\n\n      # Specify a column of data in the table as the source of vertex VID in the NebulaGraph.\n      # The value of vertex must be the same as that in the JSON file.\n      # Currently, NebulaGraph 3.5.0 supports only strings or integers of VID.\n      vertex: {\n        field:id\n      # udf:{\n      #            separator:\"_\"\n      #            oldColNames:[field-0,field-1,field-2]\n      #            newColName:new-field\n      #        }\n      # Add the specified prefix to the VID. For example, if the VID is `12345`, adding the prefix `tag1` will result in `tag1_12345`. The underscore cannot be modified.\n      # prefix:\"tag1\"\n      # Performs hashing operations on VIDs of type string.\n      # policy:hash\n      }\n\n      # Batch operation types, including INSERT, UPDATE, and DELETE. defaults to INSERT.\n      #writeMode: INSERT\n\n      # Whether or not to delete the related incoming and outgoing edges of the vertices when performing a batch delete operation. This parameter takes effect when `writeMode` is `DELETE`.\n      #deleteEdge: false\n\n      # The number of data written to NebulaGraph in a single batch.\n      batch: 256\n\n      # The number of Spark partitions.\n      partition: 32\n    }\n\n    # Set the information about the Tag Team.\n{\n      name: team\n      type: {\n        source: json\n        sink: client\n      }\n      path: \"hdfs://192.168.*.*:9000/data/vertex_team.json\"\n      fields: [name]\n      nebula.fields: [name]\n      vertex: {\n        field:id\n      }\n      batch: 256\n      partition: 32\n    }\n\n\n    # If more vertexes need to be added, refer to the previous configuration to add them.\n  ]\n  # Processing edges\n  edges: [\n    # Set the information about the Edge Type follow.\n    {\n      # Specify the Edge Type name defined in NebulaGraph.\n      name: follow\n      type: {\n        # Specify the data source file format to JSON.\n        source: json\n\n        # Specify how to import the data into NebulaGraph: Client or SST.\n        sink: client\n      }\n\n      # Specify the path to the JSON file.\n      # If the file is stored in HDFS, use double quotation marks to enclose the file path, starting with hdfs://. For example, \"hdfs://ip:port/xx/xx\".\n      # If the file is stored locally, use double quotation marks to enclose the file path, starting with file://. For example, \"file:///tmp/xx.json\".\n      path: \"hdfs://192.168.*.*:9000/data/edge_follow.json\"\n\n      # Specify the key name in the JSON file in fields, and its corresponding value will serve as the data source for the properties specified in the NebulaGraph.\n      # If multiple column names need to be specified, separate them by commas.\n      fields: [degree]\n\n      # Specify the column names in the edge table in fields, and their corresponding values are specified as properties in the NebulaGraph.\n      # The sequence of fields and nebula.fields must correspond to each other.\n      nebula.fields: [degree]\n\n      # Specify a column as the source for the source and destination vertexes.\n      # The value of vertex must be the same as that in the JSON file.\n      # Currently, NebulaGraph 3.5.0 supports only strings or integers of VID.\n      source: {\n        field: src\n      # udf:{\n      #            separator:\"_\"\n      #            oldColNames:[field-0,field-1,field-2]\n      #            newColName:new-field\n      #        }\n      # Add the specified prefix to the VID. For example, if the VID is `12345`, adding the prefix `tag1` will result in `tag1_12345`. The underscore cannot be modified.\n      # prefix:\"tag1\"\n      # Performs hashing operations on VIDs of type string.\n      # policy:hash\n      }\n      target: {\n        field: dst\n      # udf:{\n      #            separator:\"_\"\n      #            oldColNames:[field-0,field-1,field-2]\n      #            newColName:new-field\n      #        }\n      # Add the specified prefix to the VID. For example, if the VID is `12345`, adding the prefix `tag1` will result in `tag1_12345`. The underscore cannot be modified.\n      # prefix:\"tag1\"\n      # Performs hashing operations on VIDs of type string.\n      # policy:hash\n      }\n\n\n      # (Optional) Specify a column as the source of the rank.\n      #ranking: rank\n\n      # Batch operation types, including INSERT, UPDATE, and DELETE. defaults to INSERT.\n      #writeMode: INSERT\n\n      # The number of data written to NebulaGraph in a single batch.\n      batch: 256\n\n      # The number of Spark partitions.\n      partition: 32\n    }\n\n    # Set the information about the Edge Type serve.\n    {\n      name: serve\n      type: {\n        source: json\n        sink: client\n      }\n      path: \"hdfs://192.168.*.*:9000/data/edge_serve.json\"\n      fields: [start_year,end_year]\n      nebula.fields: [start_year, end_year]\n      source: {\n        field: src\n      }\n      target: {\n        field: dst\n      }\n      batch: 256\n      partition: 32\n    }\n\n  ]\n  # If more edges need to be added, refer to the previous configuration to add them.\n}\n</code></pre>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-json/#step_4_import_data_into_nebulagraph","title":"Step 4: Import data into NebulaGraph","text":"<p>Run the following command to import JSON data into NebulaGraph. For a description of the parameters, see Options for import.</p> <pre><code>${SPARK_HOME}/bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange &lt;nebula-exchange-3.5.0.jar_path&gt; -c &lt;json_application.conf_path&gt; \n</code></pre> <p>Note</p> <p>JAR packages are available in two ways: compiled them yourself, or download the compiled <code>.jar</code> file directly.</p> <p>For example:</p> <pre><code>${SPARK_HOME}/bin/spark-submit  --master \"local\" --class com.vesoft.nebula.exchange.Exchange  /root/nebula-echange/nebula-exchange/target/nebula-exchange-3.5.0.jar  -c /root/nebula-exchange/nebula-exchange/target/classes/json_application.conf\n</code></pre> <p>You can search for <code>batchSuccess.&lt;tag_name/edge_name&gt;</code> in the command output to check the number of successes. For example, <code>batchSuccess.follow: 300</code>.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-json/#step_5_optional_validate_data","title":"Step 5: (optional) Validate data","text":"<p>Users can verify that data has been imported by executing a query in the NebulaGraph client (for example, NebulaGraph Studio). For example:</p> <pre><code>LOOKUP ON player YIELD id(vertex);\n</code></pre> <p>Users can also run the <code>SHOW STATS</code> command to view statistics.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-json/#step_6_optional_rebuild_indexes_in_nebulagraph","title":"Step 6: (optional) Rebuild indexes in NebulaGraph","text":"<p>With the data imported, users can recreate and rebuild indexes in NebulaGraph. For details, see Index overview.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-kafka/","title":"Import data from Kafka","text":"<p>This topic provides a simple guide to importing Data stored on Kafka into NebulaGraph using Exchange.</p> <p>Compatibility</p> <p>Please use Exchange 3.5.0/3.3.0/3.0.0 when importing Kafka data. In version 3.4.0, caching of imported data was added, and streaming data import is not supported.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-kafka/#environment","title":"Environment","text":"<p>This example is done on MacOS. Here is the environment configuration information:</p> <ul> <li>Hardware specifications:<ul> <li>CPU: 1.7 GHz Quad-Core Intel Core i7</li> <li>Memory: 16 GB</li> </ul> </li> </ul> <ul> <li>Spark: 2.4.7, stand-alone</li> </ul> <ul> <li>NebulaGraph: 3.5.0. Deploy NebulaGraph with Docker Compose.</li> </ul>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-kafka/#prerequisites","title":"Prerequisites","text":"<p>Before importing data, you need to confirm the following information:</p> <ul> <li> <p>NebulaGraph has been installed and deployed with the following information:</p> <ul> <li>IP addresses and ports of Graph and Meta services.</li> </ul> <ul> <li>The user name and password with write permission to NebulaGraph.</li> </ul> </li> </ul> <ul> <li>Exchange has been compiled, or download the compiled <code>.jar</code> file directly.</li> </ul> <ul> <li>Spark has been installed.</li> </ul> <ul> <li> <p>The following JAR files have been downloaded and placed in the directory <code>SPARK_HOME/jars</code> of Spark:</p> <ul> <li>spark-streaming-kafka_xxx.jar</li> </ul> <ul> <li>spark-sql-kafka-0-10_xxx.jar</li> </ul> <ul> <li>kafka-clients-xxx.jar</li> </ul> </li> </ul> <ul> <li>Learn about the Schema created in NebulaGraph, including names and properties of Tags and Edge types, and more.</li> </ul> <ul> <li>The Kafka service has been installed and started.</li> </ul>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-kafka/#precautions","title":"Precautions","text":"<ul> <li>Only client mode is supported when importing Kafka data, i.e. the value of parameters <code>tags.type.sink</code> and <code>edges.type.sink</code> is <code>client</code>.</li> </ul> <ul> <li>When importing Kafka data, do not use Exchange version 3.4.0, which adds caching of imported data and does not support streaming data import. Use Exchange versions 3.0.0, 3.3.0, or 3.5.0.</li> </ul>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-kafka/#steps","title":"Steps","text":""},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-kafka/#step_1_create_the_schema_in_nebulagraph","title":"Step 1: Create the Schema in NebulaGraph","text":"<p>Analyze the data to create a Schema in NebulaGraph by following these steps:</p> <ol> <li> <p>Identify the Schema elements. The Schema elements in the NebulaGraph are shown in the following table.</p> Element Name Property Tag <code>player</code> <code>name string, age int</code> Tag <code>team</code> <code>name string</code> Edge Type <code>follow</code> <code>degree int</code> Edge Type <code>serve</code> <code>start_year int, end_year int</code> </li> <li> <p>Create a graph space basketballplayer in the NebulaGraph and create a Schema as shown below.</p> <pre><code>## Create a graph space.\nnebula&gt; CREATE SPACE basketballplayer \\\n        (partition_num = 10, \\\n        replica_factor = 1, \\\n        vid_type = FIXED_STRING(30));\n\n## Use the graph space basketballplayer.\nnebula&gt; USE basketballplayer;\n\n## Create the Tag player.\nnebula&gt; CREATE TAG player(name string, age int);\n\n## Create the Tag team.\nnebula&gt; CREATE TAG team(name string);\n\n## Create the Edge type follow.\nnebula&gt; CREATE EDGE follow(degree int);\n\n## Create the Edge type serve.\nnebula&gt; CREATE EDGE serve(start_year int, end_year int);\n</code></pre> </li> </ol> <p>For more information, see Quick start workflow.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-kafka/#step_2_modify_configuration_files","title":"Step 2: Modify configuration files","text":"<p>Note</p> <p>If some data is stored in Kafka's value field, you need to modify the source code, get the value from Kafka, parse the value through the from_JSON function, and return it as a Dataframe.</p> <p>After Exchange is compiled, copy the conf file <code>target/classes/application.conf</code> to set Kafka data source configuration. In this example, the copied file is called <code>kafka_application.conf</code>. For details on each configuration item, see Parameters in the configuration file.</p> <p>Note</p> <p>When importing Kafka data, a configuration file can only handle one tag or edge type. If there are multiple tag or edge types, you need to create multiple configuration files.</p> <pre><code>{\n  # Spark configuration\n  spark: {\n    app: {\n      name: NebulaGraph Exchange 3.5.0\n    }\n    driver: {\n      cores: 1\n      maxResultSize: 1G\n    }\n    cores: {\n      max: 16\n    }\n  }\n\n\n  # NebulaGraph configuration\n  nebula: {\n    address:{\n      # Specify the IP addresses and ports for Graph and all Meta services.\n      # If there are multiple addresses, the format is \"ip1:port\",\"ip2:port\",\"ip3:port\".\n      # Addresses are separated by commas.\n      graph:[\"127.0.0.1:9669\"]\n      # the address of any of the meta services.\n      # if your NebulaGraph server is in virtual network like k8s, please config the leader address of meta.\n      meta:[\"127.0.0.1:9559\"]\n    }\n    # The account entered must have write permission for the NebulaGraph space.\n    user: root\n    pswd: nebula\n    # Fill in the name of the graph space you want to write data to in the NebulaGraph.\n    space: basketballplayer\n    connection: {\n      timeout: 3000\n      retry: 3\n    }\n    execution: {\n      retry: 3\n    }\n    error: {\n      max: 32\n      output: /tmp/errors\n    }\n    rate: {\n      limit: 1024\n      timeout: 1000\n    }\n  }\n  # Processing vertexes\n  tags: [\n    # Set the information about the Tag player.\n    {\n\n      # The corresponding Tag name in NebulaGraph.\n      name: player\n      type: {\n        # Specify the data source file format to Kafka.\n        source: kafka\n        # Specify how to import the data into NebulaGraph. Only client is supported.\n        sink: client\n      }\n      # Kafka server address.\n      service: \"127.0.0.1:9092\"\n      # Message category.\n      topic: \"topic_name1\"\n\n      # Kafka data has a fixed domain name: key, value, topic, partition, offset, timestamp, timestampType.\n      # If multiple fields need to be specified after Spark reads as DataFrame, separate them with commas.\n      # Specify the field name in fields. For example, use key for name in NebulaGraph and value for age in Nebula, as shown in the following.\n      fields: [key,value]\n      nebula.fields: [name,age]\n\n      # Specify a column of data in the table as the source of vertex VID in the NebulaGraph.\n      # The key is the same as the value above, indicating that key is used as both VID and property name.\n      vertex:{\n          field:key\n      # udf:{\n      #            separator:\"_\"\n      #            oldColNames:[field-0,field-1,field-2]\n      #            newColName:new-field\n      #        }\n      # Add the specified prefix to the VID. For example, if the VID is `12345`, adding the prefix `tag1` will result in `tag1_12345`. The underscore cannot be modified.\n      # prefix:\"tag1\"\n      # Performs hashing operations on VIDs of type string.\n      # policy:hash\n      }\n\n      # Batch operation types, including INSERT, UPDATE, and DELETE. defaults to INSERT.\n      #writeMode: INSERT\n\n      # Whether or not to delete the related incoming and outgoing edges of the vertices when performing a batch delete operation. This parameter takes effect when `writeMode` is `DELETE`.\n      #deleteEdge: false\n\n      # The number of data written to NebulaGraph in a single batch.\n      batch: 10\n\n      # The number of Spark partitions.\n      partition: 10\n      # The interval for message reading. Unit: second.\n      interval.seconds: 10\n      # The consumer offsets. The default value is latest. Optional value are latest and earliest.\n      startingOffsets: latest\n      # Flow control, with a rate limit on the maximum offset processed per trigger interval, may not be configured.\n      # maxOffsetsPerTrigger:10000\n    }\n  ]\n\n  # Processing edges\n  #edges: [\n  #  # Set the information about the Edge Type follow.\n  #  {\n  #    # The corresponding Edge Type name in NebulaGraph.\n  #    name: follow\n\n  #    type: {\n  #      # Specify the data source file format to Kafka.\n  #      source: kafka\n\n  #      # Specify how to import the Edge type data into NebulaGraph.\n  #      # Specify how to import the data into NebulaGraph. Only client is supported.\n  #      sink: client\n  #    }\n\n  #    # Kafka server address.\n  #    service: \"127.0.0.1:9092\"\n  #    # Message category.\n  #    topic: \"topic_name3\"\n\n  #    # Kafka data has a fixed domain name: key, value, topic, partition, offset, timestamp, timestampType.\n  #    # If multiple fields need to be specified after Spark reads as DataFrame, separate them with commas.\n  #    # Specify the field name in fields. For example, use key for degree in Nebula, as shown in the following.\n  #    fields: [key]\n  #    nebula.fields: [degree]\n\n  #    # In source, use a column in the topic as the source of the edge's source vertex.\n  #    # In target, use a column in the topic as the source of the edge's destination vertex.\n  #    source:{\n  #        field:timestamp\n  #    # udf:{\n  #    #            separator:\"_\"\n  #    #            oldColNames:[field-0,field-1,field-2]\n  #    #            newColName:new-field\n  #    #        }\n  #    # Add the specified prefix to the VID. For example, if the VID is `12345`, adding the prefix `tag1` will result in `tag1_12345`. The underscore cannot be modified.\n  #    # prefix:\"tag1\"\n  #    # Performs hashing operations on VIDs of type string.\n  #    # policy:hash\n  #    }\n\n\n  #    target:{\n  #        field:offset\n  #    # udf:{\n  #    #            separator:\"_\"\n  #    #            oldColNames:[field-0,field-1,field-2]\n  #    #            newColName:new-field\n  #    #        }\n  #    # Add the specified prefix to the VID. For example, if the VID is `12345`, adding the prefix `tag1` will result in `tag1_12345`. The underscore cannot be modified.\n  #    # prefix:\"tag1\"\n  #    # Performs hashing operations on VIDs of type string.\n  #    # policy:hash\n  #    }\n\n  #    # (Optional) Specify a column as the source of the rank.\n  #    #ranking: rank\n\n  #    # Batch operation types, including INSERT, UPDATE, and DELETE. defaults to INSERT.\n  #    #writeMode: INSERT\n\n  #    # The number of data written to NebulaGraph in a single batch.\n  #    batch: 10\n\n  #    # The number of Spark partitions.\n  #    partition: 10\n\n  #    # The interval for message reading. Unit: second.\n  #    interval.seconds: 10\n  #    # The consumer offsets. The default value is latest. Optional value are latest and earliest.\n  #    startingOffsets: latest\n  #    # Flow control, with a rate limit on the maximum offset processed per trigger interval, may not be configured.\n  #    # maxOffsetsPerTrigger:10000\n  #  }\n  #]\n}\n</code></pre>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-kafka/#step_3_import_data_into_nebulagraph","title":"Step 3: Import data into NebulaGraph","text":"<p>Run the following command to import Kafka data into NebulaGraph. For a description of the parameters, see Options for import.</p> <pre><code>${SPARK_HOME}/bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange &lt;nebula-exchange-3.5.0.jar_path&gt; -c &lt;kafka_application.conf_path&gt;\n</code></pre> <p>Note</p> <p>JAR packages are available in two ways: compiled them yourself, or download the compiled <code>.jar</code> file directly.</p> <p>For example:</p> <pre><code>${SPARK_HOME}/bin/spark-submit  --master \"local\" --class com.vesoft.nebula.exchange.Exchange  /root/nebula-exchange/nebula-exchange/target/nebula-exchange-3.5.0.jar  -c /root/nebula-exchange/nebula-exchange/target/classes/kafka_application.conf\n</code></pre> <p>You can search for <code>batchSuccess.&lt;tag_name/edge_name&gt;</code> in the command output to check the number of successes. For example, <code>batchSuccess.follow: 300</code>.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-kafka/#step_4_optional_validate_data","title":"Step 4: (optional) Validate data","text":"<p>Users can verify that data has been imported by executing a query in the NebulaGraph client (for example, NebulaGraph Studio). For example:</p> <pre><code>LOOKUP ON player YIELD id(vertex);\n</code></pre> <p>Users can also run the SHOW STATS command to view statistics.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-kafka/#step_5_optional_rebuild_indexes_in_nebulagraph","title":"Step 5: (optional) Rebuild indexes in NebulaGraph","text":"<p>With the data imported, users can recreate and rebuild indexes in NebulaGraph. For details, see Index overview.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-maxcompute/","title":"Import data from MaxCompute","text":"<p>This topic provides an example of how to use Exchange to import NebulaGraph data stored in MaxCompute.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-maxcompute/#data_set","title":"Data set","text":"<p>This topic takes the basketballplayer dataset as an example.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-maxcompute/#environment","title":"Environment","text":"<p>This example is done on MacOS. Here is the environment configuration information:</p> <ul> <li>Hardware specifications:<ul> <li>CPU: 1.7 GHz Quad-Core Intel Core i7</li> <li>Memory: 16 GB</li> </ul> </li> </ul> <ul> <li>Spark: 2.4.7, stand-alone</li> </ul> <ul> <li>MaxCompute: Alibaba Cloud official version</li> </ul> <ul> <li>NebulaGraph: 3.5.0. Deploy NebulaGraph with Docker Compose.</li> </ul>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-maxcompute/#prerequisites","title":"Prerequisites","text":"<p>Before importing data, you need to confirm the following information:</p> <ul> <li> <p>NebulaGraph has been installed and deployed with the following information:</p> <ul> <li>IP addresses and ports of Graph and Meta services.</li> </ul> <ul> <li>The user name and password with write permission to NebulaGraph.</li> </ul> </li> </ul> <ul> <li>Exchange has been compiled, or download the compiled <code>.jar</code> file directly.</li> </ul> <ul> <li>Spark has been installed.</li> </ul> <ul> <li>Learn about the Schema created in NebulaGraph, including names and properties of Tags and Edge types, and more.</li> </ul>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-maxcompute/#steps","title":"Steps","text":""},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-maxcompute/#step_1_create_the_schema_in_nebulagraph","title":"Step 1: Create the Schema in NebulaGraph","text":"<p>Analyze the data to create a Schema in NebulaGraph by following these steps:</p> <ol> <li> <p>Identify the Schema elements. The Schema elements in the NebulaGraph are shown in the following table.</p> Element Name Property Tag <code>player</code> <code>name string, age int</code> Tag <code>team</code> <code>name string</code> Edge Type <code>follow</code> <code>degree int</code> Edge Type <code>serve</code> <code>start_year int, end_year int</code> </li> <li> <p>Create a graph space basketballplayer in the NebulaGraph and create a Schema as shown below.</p> <pre><code>## Create a graph space.\nnebula&gt; CREATE SPACE basketballplayer \\\n        (partition_num = 10, \\\n        replica_factor = 1, \\\n        vid_type = FIXED_STRING(30));\n\n## Use the graph space basketballplayer.\nnebula&gt; USE basketballplayer;\n\n## Create the Tag player.\nnebula&gt; CREATE TAG player(name string, age int);\n\n## Create the Tag team.\nnebula&gt; CREATE TAG team(name string);\n\n## Create the Edge type follow.\nnebula&gt; CREATE EDGE follow(degree int);\n\n## Create the Edge type serve.\nnebula&gt; CREATE EDGE serve(start_year int, end_year int);\n</code></pre> </li> </ol> <p>For more information, see Quick start workflow.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-maxcompute/#step_2_modify_configuration_files","title":"Step 2: Modify configuration files","text":"<p>After Exchange is compiled, copy the conf file <code>target/classes/application.conf</code> to set MaxCompute data source configuration. In this example, the copied file is called <code>maxcompute_application.conf</code>. For details on each configuration item, see Parameters in the configuration file.</p> <pre><code>{\n  # Spark configuration\n  spark: {\n    app: {\n      name: NebulaGraph Exchange 3.5.0\n    }\n    driver: {\n      cores: 1\n      maxResultSize: 1G\n    }\n    cores: {\n      max: 16\n    }\n  }\n\n  # NebulaGraph configuration\n  nebula: {\n    address:{\n      # Specify the IP addresses and ports for Graph and Meta services.\n      # If there are multiple addresses, the format is \"ip1:port\",\"ip2:port\",\"ip3:port\".\n      # Addresses are separated by commas.\n      graph:[\"127.0.0.1:9669\"]\n      # the address of any of the meta services.\n      # if your NebulaGraph server is in virtual network like k8s, please config the leader address of meta.\n      meta:[\"127.0.0.1:9559\"]\n    }\n    # The account entered must have write permission for the NebulaGraph space.\n    user: root\n    pswd: nebula\n    # Fill in the name of the graph space you want to write data to in the NebulaGraph.\n    space: basketballplayer\n    connection: {\n      timeout: 3000\n      retry: 3\n    }\n    execution: {\n      retry: 3\n    }\n    error: {\n      max: 32\n      output: /tmp/errors\n    }\n    rate: {\n      limit: 1024\n      timeout: 1000\n    }\n  }\n  # Processing vertexes\n  tags: [\n    # Set the information about the Tag player.\n    {\n      name: player\n      type: {\n        # Specify the data source file format to MaxCompute.\n        source: maxcompute\n        # Specify how to import the data into NebulaGraph: Client or SST.\n        sink: client\n      }\n\n      # Table name of MaxCompute.\n      table:player\n\n      # Project name of MaxCompute.\n      project:project\n\n      # OdpsUrl and tunnelUrl for the MaxCompute service.\n      # The address is https://help.aliyun.com/document_detail/34951.html.\n      odpsUrl:\"http://service.cn-hangzhou.maxcompute.aliyun.com/api\"\n      tunnelUrl:\"http://dt.cn-hangzhou.maxcompute.aliyun.com\"\n\n      # AccessKeyId and accessKeySecret of the MaxCompute service.\n      accessKeyId:xxx\n      accessKeySecret:xxx\n\n      # Partition description of the MaxCompute table. This configuration is optional.\n      partitionSpec:\"dt='partition1'\"\n\n      # Ensure that the table name in the SQL statement is the same as the value of the table above. This configuration is optional.\n      sentence:\"select id, name, age, playerid from player where id &lt; 10\"\n\n      # Specify the column names in the player table in fields, and their corresponding values are specified as properties in the NebulaGraph.\n      # The sequence of fields and nebula.fields must correspond to each other.\n      # If multiple column names need to be specified, separate them by commas.\n      fields:[name, age]\n      nebula.fields:[name, age]\n\n      # Specify a column of data in the table as the source of vertex VID in the NebulaGraph.\n      vertex:{\n        field: playerid\n      # udf:{\n      #            separator:\"_\"\n      #            oldColNames:[field-0,field-1,field-2]\n      #            newColName:new-field\n      #        }\n      # Add the specified prefix to the VID. For example, if the VID is `12345`, adding the prefix `tag1` will result in `tag1_12345`. The underscore cannot be modified.\n      # prefix:\"tag1\"\n      # Performs hashing operations on VIDs of type string.\n      # policy:hash\n      }\n\n      # Batch operation types, including INSERT, UPDATE, and DELETE. defaults to INSERT.\n      #writeMode: INSERT\n\n      # Whether or not to delete the related incoming and outgoing edges of the vertices when performing a batch delete operation. This parameter takes effect when `writeMode` is `DELETE`.\n      #deleteEdge: false\n\n      # The number of data written to NebulaGraph in a single batch.\n      batch: 256\n\n      # The number of Spark partitions.\n      partition: 32\n    }\n\n    # Set the information about the Tag Team.\n    {\n      name: team\n      type: {\n        source: maxcompute\n        sink: client\n      }\n      table:team\n      project:project\n      odpsUrl:\"http://service.cn-hangzhou.maxcompute.aliyun.com/api\"\n      tunnelUrl:\"http://dt.cn-hangzhou.maxcompute.aliyun.com\"\n      accessKeyId:xxx\n      accessKeySecret:xxx\n      partitionSpec:\"dt='partition1'\"\n      sentence:\"select id, name, teamid from team where id &lt; 10\"\n      fields:[name]\n      nebula.fields:[name]\n      vertex:{\n        field: teamid\n      }\n      batch: 256\n      partition: 32\n    }\n  ]\n\n  # Processing edges\n  edges: [\n    # Set the information about the Edge Type follow.\n    {\n      # The corresponding Edge Type name in NebulaGraph.\n      name: follow\n\n      type:{\n        # Specify the data source file format to MaxCompute.\n        source:maxcompute\n\n        # Specify how to import the Edge type data into NebulaGraph.\n        # Specify how to import the data into NebulaGraph: Client or SST.\n        sink:client\n      }\n\n      # Table name of MaxCompute.\n      table:follow\n\n      # Project name of MaxCompute.\n      project:project\n\n      # OdpsUrl and tunnelUrl for MaxCompute service.\n      # The address is https://help.aliyun.com/document_detail/34951.html.\n      odpsUrl:\"http://service.cn-hangzhou.maxcompute.aliyun.com/api\"\n      tunnelUrl:\"http://dt.cn-hangzhou.maxcompute.aliyun.com\"\n\n      # AccessKeyId and accessKeySecret of the MaxCompute service.\n      accessKeyId:xxx\n      accessKeySecret:xxx\n\n      # Partition description of the MaxCompute table. This configuration is optional.\n      partitionSpec:\"dt='partition1'\"\n\n      # Ensure that the table name in the SQL statement is the same as the value of the table above. This configuration is optional.\n      sentence:\"select * from follow\"\n\n      # Specify the column names in the follow table in Fields, and their corresponding values are specified as properties in the NebulaGraph.\n      # The sequence of fields and nebula.fields must correspond to each other.\n      # If multiple column names need to be specified, separate them by commas.\n      fields:[degree]\n      nebula.fields:[degree]\n\n      # In source, use a column in the follow table as the source of the edge's source vertex.\n      source:{\n        field: src_player\n      # udf:{\n      #            separator:\"_\"\n      #            oldColNames:[field-0,field-1,field-2]\n      #            newColName:new-field\n      #        }\n      # Add the specified prefix to the VID. For example, if the VID is `12345`, adding the prefix `tag1` will result in `tag1_12345`. The underscore cannot be modified.\n      # prefix:\"tag1\"\n      # Performs hashing operations on VIDs of type string.\n      # policy:hash\n      }\n\n      # In target, use a column in the follow table as the source of the edge's destination vertex.\n      target:{\n        field: dst_player\n      # udf:{\n      #            separator:\"_\"\n      #            oldColNames:[field-0,field-1,field-2]\n      #            newColName:new-field\n      #        }\n      # Add the specified prefix to the VID. For example, if the VID is `12345`, adding the prefix `tag1` will result in `tag1_12345`. The underscore cannot be modified.\n      # prefix:\"tag1\"\n      # Performs hashing operations on VIDs of type string.\n      # policy:hash\n      }\n\n      # (Optional) Specify a column as the source of the rank.\n      #ranking: rank\n\n      # Batch operation types, including INSERT, UPDATE, and DELETE. defaults to INSERT.\n      #writeMode: INSERT\n\n      # The number of Spark partitions.\n      partition:10\n\n      # The number of data written to NebulaGraph in a single batch.\n      batch:10\n    }\n\n    # Set the information about the Edge Type serve.\n    {\n      name: serve\n      type:{\n        source:maxcompute\n        sink:client\n      }\n      table:serve\n      project:project\n      odpsUrl:\"http://service.cn-hangzhou.maxcompute.aliyun.com/api\"\n      tunnelUrl:\"http://dt.cn-hangzhou.maxcompute.aliyun.com\"\n      accessKeyId:xxx\n      accessKeySecret:xxx\n      partitionSpec:\"dt='partition1'\"\n      sentence:\"select * from serve\"\n      fields:[start_year,end_year]\n      nebula.fields:[start_year,end_year]\n      source:{\n        field: playerid\n      }\n      target:{\n        field: teamid\n      }\n\n      # (Optional) Specify a column as the source of the rank.\n      #ranking: rank\n\n      partition:10\n      batch:10\n    }\n  ]\n}\n</code></pre>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-maxcompute/#step_3_import_data_into_nebulagraph","title":"Step 3: Import data into NebulaGraph","text":"<p>Run the following command to import MaxCompute data into NebulaGraph. For a description of the parameters, see Options for import.</p> <pre><code>${SPARK_HOME}/bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange &lt;nebula-exchange-3.5.0.jar_path&gt; -c &lt;maxcompute_application.conf_path&gt;\n</code></pre> <p>Note</p> <p>JAR packages are available in two ways: compiled them yourself, or download the compiled <code>.jar</code> file directly.</p> <p>For example:</p> <pre><code>${SPARK_HOME}/bin/spark-submit  --master \"local\" --class com.vesoft.nebula.exchange.Exchange  /root/nebula-exchange/nebula-exchange/target/nebula-exchange-3.5.0.jar  -c /root/nebula-exchange/nebula-exchange/target/classes/maxcompute_application.conf\n</code></pre> <p>You can search for <code>batchSuccess.&lt;tag_name/edge_name&gt;</code> in the command output to check the number of successes. For example, <code>batchSuccess.follow: 300</code>.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-maxcompute/#step_4_optional_validate_data","title":"Step 4: (optional) Validate data","text":"<p>Users can verify that data has been imported by executing a query in the NebulaGraph client (for example, NebulaGraph Studio). For example:</p> <pre><code>LOOKUP ON player YIELD id(vertex);\n</code></pre> <p>Users can also run the <code>SHOW STATS</code> command to view statistics.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-maxcompute/#step_5_optional_rebuild_indexes_in_nebulagraph","title":"Step 5: (optional) Rebuild indexes in NebulaGraph","text":"<p>With the data imported, users can recreate and rebuild indexes in NebulaGraph. For details, see Index overview.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-mysql/","title":"Import data from MySQL/PostgreSQL","text":"<p>This topic provides an example of how to use Exchange to export MySQL data and import to NebulaGraph. It also applies to exporting data from PostgreSQL into NebulaGraph.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-mysql/#data_set","title":"Data set","text":"<p>This topic takes the basketballplayer dataset as an example.</p> <p>In this example, the data set has been stored in MySQL. All vertexes and edges are stored in the <code>player</code>, <code>team</code>, <code>follow</code>, and <code>serve</code> tables. The following are some of the data for each table.</p> <pre><code>mysql&gt; desc player;\n+----------+-------------+------+-----+---------+-------+\n| Field    | Type        | Null | Key | Default | Extra |\n+----------+-------------+------+-----+---------+-------+\n| playerid | varchar(30) | YES  |     | NULL    |       |\n| age      | int         | YES  |     | NULL    |       |\n| name     | varchar(30) | YES  |     | NULL    |       |\n+----------+-------------+------+-----+---------+-------+\n\nmysql&gt; desc team;\n+--------+-------------+------+-----+---------+-------+\n| Field  | Type        | Null | Key | Default | Extra |\n+--------+-------------+------+-----+---------+-------+\n| teamid | varchar(30) | YES  |     | NULL    |       |\n| name   | varchar(30) | YES  |     | NULL    |       |\n+--------+-------------+------+-----+---------+-------+\n\nmysql&gt; desc follow;\n+------------+-------------+------+-----+---------+-------+\n| Field      | Type        | Null | Key | Default | Extra |\n+------------+-------------+------+-----+---------+-------+\n| src_player | varchar(30) | YES  |     | NULL    |       |\n| dst_player | varchar(30) | YES  |     | NULL    |       |\n| degree     | int         | YES  |     | NULL    |       |\n+------------+-------------+------+-----+---------+-------+\n\nmysql&gt; desc serve;\n+------------+-------------+------+-----+---------+-------+\n| Field      | Type        | Null | Key | Default | Extra |\n+------------+-------------+------+-----+---------+-------+\n| playerid   | varchar(30) | YES  |     | NULL    |       |\n| teamid     | varchar(30) | YES  |     | NULL    |       |\n| start_year | int         | YES  |     | NULL    |       |\n| end_year   | int         | YES  |     | NULL    |       |\n+------------+-------------+------+-----+---------+-------+\n</code></pre>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-mysql/#environment","title":"Environment","text":"<p>This example is done on MacOS. Here is the environment configuration information:</p> <ul> <li>Hardware specifications:<ul> <li>CPU: 1.7 GHz Quad-Core Intel Core i7</li> <li>Memory: 16 GB</li> </ul> </li> </ul> <ul> <li>Spark: 2.4.7, stand-alone</li> </ul> <ul> <li>MySQL: 8.0.23</li> </ul> <ul> <li>NebulaGraph: 3.5.0. Deploy NebulaGraph with Docker Compose.</li> </ul>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-mysql/#prerequisites","title":"Prerequisites","text":"<p>Before importing data, you need to confirm the following information:</p> <ul> <li> <p>NebulaGraph has been installed and deployed with the following information:</p> <ul> <li>IP addresses and ports of Graph and Meta services.</li> </ul> <ul> <li>The user name and password with write permission to NebulaGraph.</li> </ul> </li> </ul> <ul> <li>Exchange has been compiled, or download the compiled <code>.jar</code> file directly.</li> </ul> <ul> <li>Spark has been installed.</li> </ul> <ul> <li>mysql-connector-java-xxx.jar has been downloaded and placed in the directory <code>SPARK_HOME/jars</code> of Spark.</li> </ul> <ul> <li>Learn about the Schema created in NebulaGraph, including names and properties of Tags and Edge types, and more.</li> </ul> <ul> <li>The Hadoop service has been installed and started.</li> </ul>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-mysql/#precautions","title":"Precautions","text":"<p>nebula-exchange_spark_2.2 supports only single table queries, not multi-table queries.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-mysql/#steps","title":"Steps","text":""},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-mysql/#step_1_create_the_schema_in_nebulagraph","title":"Step 1: Create the Schema in NebulaGraph","text":"<p>Analyze the data to create a Schema in NebulaGraph by following these steps:</p> <ol> <li> <p>Identify the Schema elements. The Schema elements in the NebulaGraph are shown in the following table.</p> Element Name Property Tag <code>player</code> <code>name string, age int</code> Tag <code>team</code> <code>name string</code> Edge Type <code>follow</code> <code>degree int</code> Edge Type <code>serve</code> <code>start_year int, end_year int</code> </li> <li> <p>Create a graph space basketballplayer in the NebulaGraph and create a Schema as shown below.</p> <pre><code>## Create a graph space.\nnebula&gt; CREATE SPACE basketballplayer \\\n        (partition_num = 10, \\\n        replica_factor = 1, \\\n        vid_type = FIXED_STRING(30));\n\n## Use the graph space basketballplayer.\nnebula&gt; USE basketballplayer;\n\n## Create the Tag player.\nnebula&gt; CREATE TAG player(name string, age int);\n\n## Create the Tag team.\nnebula&gt; CREATE TAG team(name string);\n\n## Create the Edge type follow.\nnebula&gt; CREATE EDGE follow(degree int);\n\n## Create the Edge type serve.\nnebula&gt; CREATE EDGE serve(start_year int, end_year int);\n</code></pre> </li> </ol> <p>For more information, see Quick start workflow.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-mysql/#step_2_modify_configuration_files","title":"Step 2: Modify configuration files","text":"<p>After Exchange is compiled, copy the conf file <code>target/classes/application.conf</code> to set MySQL data source configuration. In this case, the copied file is called <code>mysql_application.conf</code>. For details on each configuration item, see Parameters in the configuration file.</p> <pre><code>{\n  # Spark configuration\n  spark: {\n    app: {\n      name: NebulaGraph Exchange 3.5.0\n    }\n    driver: {\n      cores: 1\n      maxResultSize: 1G\n    }\n    cores: {\n      max: 16\n    }\n  }\n\n  # NebulaGraph configuration\n  nebula: {\n    address:{\n      # Specify the IP addresses and ports for Graph and Meta services.\n      # If there are multiple addresses, the format is \"ip1:port\",\"ip2:port\",\"ip3:port\".\n      # Addresses are separated by commas.\n      graph:[\"127.0.0.1:9669\"]\n      # the address of any of the meta services.\n      # if your NebulaGraph server is in virtual network like k8s, please config the leader address of meta.\n      meta:[\"127.0.0.1:9559\"]\n    }\n    # The account entered must have write permission for the NebulaGraph space.\n    user: root\n    pswd: nebula\n    # Fill in the name of the graph space you want to write data to in the NebulaGraph.\n    space: basketballplayer\n    connection: {\n      timeout: 3000\n      retry: 3\n    }\n    execution: {\n      retry: 3\n    }\n    error: {\n      max: 32\n      output: /tmp/errors\n    }\n    rate: {\n      limit: 1024\n      timeout: 1000\n    }\n  }\n  # Processing vertexes\n  tags: [\n    # Set the information about the Tag player.\n    {\n      # The Tag name in NebulaGraph.\n      name: player\n      type: {\n        # Specify the data source file format to MySQL.\n        source: mysql\n        # Specify how to import the data into NebulaGraph: Client or SST.\n        sink: client\n      }\n\n      host:192.168.*.*\n      port:3306\n      user:\"test\"\n      password:\"123456\"\n      database:\"basketball\"\n\n      # Scanning a single table to read data.\n      # nebula-exchange_spark_2.2 must configure this parameter. Sentence is not supported.\n      # nebula-exchange_spark_2.4 and nebula-exchange_spark_3.0 can configure this parameter, but not at the same time as sentence.\n      table:\"basketball.player\"\n\n      # Use query statement to read data.\n      # This parameter is not supported by nebula-exchange_spark_2.2.\n      # nebula-exchange_spark_2.4 and nebula-exchange_spark_3.0 can configure this parameter, but not at the same time as table. Multi-table queries are supported.\n      # sentence: \"select * from people, player, team\"\n\n      # Specify the column names in the player table in fields, and their corresponding values are specified as properties in the NebulaGraph.\n      # The sequence of fields and nebula.fields must correspond to each other.\n      # If multiple column names need to be specified, separate them by commas.\n      fields: [age,name]\n      nebula.fields: [age,name]\n\n      # Specify a column of data in the table as the source of VIDs in the NebulaGraph.\n      vertex: {\n        field:playerid\n      # udf:{\n      #            separator:\"_\"\n      #            oldColNames:[field-0,field-1,field-2]\n      #            newColName:new-field\n      #        }\n      # Add the specified prefix to the VID. For example, if the VID is `12345`, adding the prefix `tag1` will result in `tag1_12345`. The underscore cannot be modified.\n      # prefix:\"tag1\"\n      # Performs hashing operations on VIDs of type string.\n      # policy:hash\n      }\n\n      # Batch operation types, including INSERT, UPDATE, and DELETE. defaults to INSERT.\n      #writeMode: INSERT\n\n      # Whether or not to delete the related incoming and outgoing edges of the vertices when performing a batch delete operation. This parameter takes effect when `writeMode` is `DELETE`.\n      #deleteEdge: false\n\n      # The number of data written to NebulaGraph in a single batch.\n      batch: 256\n\n      # The number of Spark partitions.\n      partition: 32\n    }\n    # Set the information about the Tag Team.\n    {\n      name: team\n      type: {\n        source: mysql\n        sink: client\n      }\n\n      host:192.168.*.*\n      port:3306\n      database:\"basketball\"\n      table:\"team\"\n      user:\"test\"\n      password:\"123456\"\n      sentence:\"select teamid, name from team order by teamid;\"\n\n      fields: [name]\n      nebula.fields: [name]\n      vertex: {\n        field: teamid\n      }\n      batch: 256\n      partition: 32\n    }\n\n  ]\n\n  # Processing edges\n  edges: [\n    # Set the information about the Edge Type follow.\n    {\n      # The corresponding Edge Type name in NebulaGraph.\n      name: follow\n\n      type: {\n        # Specify the data source file format to MySQL.\n        source: mysql\n\n        # Specify how to import the Edge type data into NebulaGraph.\n        # Specify how to import the data into NebulaGraph: Client or SST.\n        sink: client\n      }\n\n      host:192.168.*.*\n      port:3306\n      user:\"test\"\n      password:\"123456\"\n      database:\"basketball\"\n\n      # Scanning a single table to read data.\n      # nebula-exchange_spark_2.2 must configure this parameter. Sentence is not supported.\n      # nebula-exchange_spark_2.4 and nebula-exchange_spark_3.0 can configure this parameter, but not at the same time as sentence.\n      table:\"basketball.follow\"\n\n      # Use query statement to read data.\n      # This parameter is not supported by nebula-exchange_spark_2.2.\n      # nebula-exchange_spark_2.4 and nebula-exchange_spark_3.0 can configure this parameter, but not at the same time as table. Multi-table queries are supported.\n      # sentence: \"select * from follow, serve\"\n\n      # Specify the column names in the follow table in fields, and their corresponding values are specified as properties in the NebulaGraph.\n      # The sequence of fields and nebula.fields must correspond to each other.\n      # If multiple column names need to be specified, separate them by commas.\n      fields: [degree]\n      nebula.fields: [degree]\n\n      # In source, use a column in the follow table as the source of the edge's source vertex.\n      # In target, use a column in the follow table as the source of the edge's destination vertex.\n      source: {\n        field: src_player\n      # udf:{\n      #            separator:\"_\"\n      #            oldColNames:[field-0,field-1,field-2]\n      #            newColName:new-field\n      #        }\n      # Add the specified prefix to the VID. For example, if the VID is `12345`, adding the prefix `tag1` will result in `tag1_12345`. The underscore cannot be modified.\n      # prefix:\"tag1\"\n      # Performs hashing operations on VIDs of type string.\n      # policy:hash\n      }\n\n      target: {\n        field: dst_player\n      # udf:{\n      #            separator:\"_\"\n      #            oldColNames:[field-0,field-1,field-2]\n      #            newColName:new-field\n      #        }\n      # Add the specified prefix to the VID. For example, if the VID is `12345`, adding the prefix `tag1` will result in `tag1_12345`. The underscore cannot be modified.\n      # prefix:\"tag1\"\n      # Performs hashing operations on VIDs of type string.\n      # policy:hash\n      }\n\n      # (Optional) Specify a column as the source of the rank.\n      #ranking: rank\n\n      # Batch operation types, including INSERT, UPDATE, and DELETE. defaults to INSERT.\n      #writeMode: INSERT\n\n      # The number of data written to NebulaGraph in a single batch.\n      batch: 256\n\n      # The number of Spark partitions.\n      partition: 32\n    }\n\n    # Set the information about the Edge Type serve.\n    {\n      name: serve\n      type: {\n        source: mysql\n        sink: client\n      }\n\n      host:192.168.*.*\n      port:3306\n      database:\"basketball\"\n      table:\"serve\"\n      user:\"test\"\n      password:\"123456\"\n      sentence:\"select playerid,teamid,start_year,end_year from serve order by playerid;\"\n      fields: [start_year,end_year]\n      nebula.fields: [start_year,end_year]\n      source: {\n        field: playerid\n      }\n      target: {\n        field: teamid\n      }\n      batch: 256\n      partition: 32\n    }\n  ]\n}\n</code></pre>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-mysql/#step_3_import_data_into_nebulagraph","title":"Step 3: Import data into NebulaGraph","text":"<p>Run the following command to import MySQL data into NebulaGraph. For a description of the parameters, see Options for import.</p> <pre><code>${SPARK_HOME}/bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange &lt;nebula-exchange-3.5.0.jar_path&gt; -c &lt;mysql_application.conf_path&gt;\n</code></pre> <p>Note</p> <p>JAR packages are available in two ways: compiled them yourself, or download the compiled <code>.jar</code> file directly.</p> <p>For example:</p> <pre><code>${SPARK_HOME}/bin/spark-submit  --master \"local\" --class com.vesoft.nebula.exchange.Exchange  /root/nebula-exchange/nebula-exchange/target/nebula-exchange-3.5.0.jar  -c /root/nebula-exchange/nebula-exchange/target/classes/mysql_application.conf\n</code></pre> <p>You can search for <code>batchSuccess.&lt;tag_name/edge_name&gt;</code> in the command output to check the number of successes. For example, <code>batchSuccess.follow: 300</code>.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-mysql/#step_4_optional_validate_data","title":"Step 4: (optional) Validate data","text":"<p>Users can verify that data has been imported by executing a query in the NebulaGraph client (for example, NebulaGraph Studio). For example:</p> <pre><code>LOOKUP ON player YIELD id(vertex);\n</code></pre> <p>Users can also run the SHOW STATS command to view statistics.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-mysql/#step_5_optional_rebuild_indexes_in_nebulagraph","title":"Step 5: (optional) Rebuild indexes in NebulaGraph","text":"<p>With the data imported, users can recreate and rebuild indexes in NebulaGraph. For details, see Index overview.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-neo4j/","title":"Import data from Neo4j","text":"<p>This topic provides an example of how to use Exchange to import NebulaGraph data stored in Neo4j.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-neo4j/#implementation_method","title":"Implementation method","text":"<p>Exchange uses Neo4j Driver 4.0.1 to read Neo4j data. Before batch export, you need to write Cypher statements that are automatically executed based on labels and relationship types and the number of Spark partitions in the configuration file to improve data export performance.</p> <p>When Exchange reads Neo4j data, it needs to do the following:</p> <ol> <li> <p>The Reader in Exchange replaces the statement following the Cypher <code>RETURN</code> statement in the <code>exec</code> part of the configuration file with <code>COUNT(*)</code>, and executes this statement to get the total amount of data, then calculates the starting offset and size of each partition based on the number of Spark partitions.</p> </li> <li> <p>(Optional) If the user has configured the <code>check_point_path</code> directory, Reader reads the files in the directory. In the transferring state, Reader calculates the offset and size that each Spark partition should have.</p> </li> <li> <p>In each Spark partition, the Reader in Exchange adds different <code>SKIP</code> and <code>LIMIT</code> statements to the Cypher statement and calls the Neo4j Driver for parallel execution to distribute data to different Spark partitions.</p> </li> <li> <p>The Reader finally processes the returned data into a DataFrame.</p> </li> </ol> <p>At this point, Exchange has finished exporting the Neo4j data. The data is then written in parallel to the NebulaGraph database.</p> <p>The whole process is illustrated below.</p> <p></p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-neo4j/#data_set","title":"Data set","text":"<p>This topic takes the basketballplayer dataset as an example.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-neo4j/#environment","title":"Environment","text":"<p>This example is done on MacOS. Here is the environment configuration information:</p> <ul> <li> <p>Hardware specifications:</p> <ul> <li>CPU\uff1aIntel(R) Xeon(R) CPU E5-2697 v3 @ 2.60GHz</li> </ul> <ul> <li>CPU cores: 14</li> </ul> <ul> <li>Memory: 251 GB</li> </ul> </li> </ul> <ul> <li>Spark: Stand-alone, 2.4.6 pre-build for Hadoop 2.7</li> </ul> <ul> <li>Neo4j: 3.5.20 Community Edition</li> </ul> <ul> <li>NebulaGraph: 3.5.0. Deploy NebulaGraph with Docker Compose.</li> </ul>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-neo4j/#prerequisites","title":"Prerequisites","text":"<p>Before importing data, you need to confirm the following information:</p> <ul> <li> <p>NebulaGraph has been installed and deployed with the following information:</p> <ul> <li>IP addresses and ports of Graph and Meta services.</li> </ul> <ul> <li>The user name and password with NebulaGraph write permission.</li> </ul> </li> </ul> <ul> <li>Exchange has been compiled, or download the compiled <code>.jar</code> file directly.</li> </ul> <ul> <li>Spark has been installed.</li> </ul> <ul> <li>Learn about the Schema created in NebulaGraph, including names and properties of Tags and Edge types, and more.</li> </ul>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-neo4j/#steps","title":"Steps","text":""},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-neo4j/#step_1_create_the_schema_in_nebulagraph","title":"Step 1: Create the Schema in NebulaGraph","text":"<p>Analyze the data to create a Schema in NebulaGraph by following these steps:</p> <ol> <li> <p>Identify the Schema elements. The Schema elements in the NebulaGraph are shown in the following table.</p> Element Name Property Tag <code>player</code> <code>name string, age int</code> Tag <code>team</code> <code>name string</code> Edge Type <code>follow</code> <code>degree int</code> Edge Type <code>serve</code> <code>start_year int, end_year int</code> </li> <li> <p>Create a graph space basketballplayer in the NebulaGraph and create a Schema as shown below.</p> <pre><code>## Create a graph space\nnebula&gt; CREATE SPACE basketballplayer \\\n        (partition_num = 10, \\\n        replica_factor = 1, \\\n        vid_type = FIXED_STRING(30));\n\n## Use the graph space basketballplayer\nnebula&gt; USE basketballplayer;\n\n## Create the Tag player\nnebula&gt; CREATE TAG player(name string, age int);\n\n## Create the Tag team\nnebula&gt; CREATE TAG team(name string);\n\n## Create the Edge type follow\nnebula&gt; CREATE EDGE follow(degree int);\n\n## Create the Edge type serve\nnebula&gt; CREATE EDGE serve(start_year int, end_year int);\n</code></pre> </li> </ol> <p>For more information, see Quick start workflow.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-neo4j/#step_2_configuring_source_data","title":"Step 2: Configuring source data","text":"<p>To speed up the export of Neo4j data, create indexes for the corresponding properties in the Neo4j database. For more information, refer to the Neo4j manual.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-neo4j/#step_3_modify_configuration_files","title":"Step 3: Modify configuration files","text":"<p>After Exchange is compiled, copy the conf file <code>target/classes/application.conf</code> to set Neo4j data source configuration. In this example, the copied file is called <code>neo4j_application.conf</code>. For details on each configuration item, see Parameters in the configuration file.</p> <pre><code>{\n  # Spark configuration\n  spark: {\n    app: {\n      name: NebulaGraph Exchange 3.5.0\n    }\n\n    driver: {\n      cores: 1\n      maxResultSize: 1G\n    }\n\n    executor: {\n        memory:1G\n    }\n\n    cores: {\n      max: 16\n    }\n  }\n\n\n  # NebulaGraph configuration\n  nebula: {\n    address:{\n      graph:[\"127.0.0.1:9669\"]\n      # the address of any of the meta services.\n      # if your NebulaGraph server is in virtual network like k8s, please config the leader address of meta.\n      meta:[\"127.0.0.1:9559\"]\n    }\n    user: root\n    pswd: nebula\n    space: basketballplayer\n\n    connection: {\n      timeout: 3000\n      retry: 3\n    }\n\n    execution: {\n      retry: 3\n    }\n\n    error: {\n      max: 32\n      output: /tmp/errors\n    }\n\n    rate: {\n      limit: 1024\n      timeout: 1000\n    }\n  }\n\n  # Processing vertexes\n  tags: [\n\n\n    # Set the information about the Tag player\n    {\n      name: player\n      type: {\n        source: neo4j\n        sink: client\n      }\n      server: \"bolt://192.168.*.*:7687\"\n      user: neo4j\n      password:neo4j\n      # bolt 3 does not support multiple databases, do not configure database names. 4 and above can configure database names.\n      # database:neo4j\n      exec: \"match (n:player) return n.id as id, n.age as age, n.name as name\"\n      fields: [age,name]\n      nebula.fields: [age,name]\n      vertex: {\n        field:id\n      # udf:{\n      #            separator:\"_\"\n      #            oldColNames:[field-0,field-1,field-2]\n      #            newColName:new-field\n      #        }\n      # Add the specified prefix to the VID. For example, if the VID is `12345`, adding the prefix `tag1` will result in `tag1_12345`. The underscore cannot be modified.\n      # prefix:\"tag1\"\n      # Performs hashing operations on VIDs of type string.\n      # policy:hash\n      }\n\n      # Batch operation types, including INSERT, UPDATE, and DELETE. defaults to INSERT.\n      #writeMode: INSERT\n\n      # Whether or not to delete the related incoming and outgoing edges of the vertices when performing a batch delete operation. This parameter takes effect when `writeMode` is `DELETE`.\n      #deleteEdge: false\n\n      partition: 10\n      batch: 1000\n      check_point_path: /tmp/test\n   }\n  # Set the information about the Tag Team\n  {\n      name: team\n      type: {\n        source: neo4j\n        sink: client\n      }\n      server: \"bolt://192.168.*.*:7687\"\n      user: neo4j\n      password:neo4j\n      database:neo4j\n      exec: \"match (n:team) return n.id as id,n.name as name\"\n      fields: [name]\n      nebula.fields: [name]\n      vertex: {\n        field:id\n      }\n      partition: 10\n      batch: 1000\n      check_point_path: /tmp/test\n   }\n  ]\n\n  # Processing edges\n  edges: [\n    # Set the information about the Edge Type follow\n    {\n      name: follow\n      type: {\n        source: neo4j\n        sink: client\n      }\n      server: \"bolt://192.168.*.*:7687\"\n      user: neo4j\n      password:neo4j\n      # bolt 3 does not support multiple databases, do not configure database names. 4 and above can configure database names.\n      # database:neo4j\n      exec: \"match (a:player)-[r:follow]-&gt;(b:player) return a.id as src, b.id as dst, r.degree as degree  order by id(r)\"\n      fields: [degree]\n      nebula.fields: [degree]\n      source: {\n        field: src\n      # udf:{\n      #            separator:\"_\"\n      #            oldColNames:[field-0,field-1,field-2]\n      #            newColName:new-field\n      #        }\n      # Add the specified prefix to the VID. For example, if the VID is `12345`, adding the prefix `tag1` will result in `tag1_12345`. The underscore cannot be modified.\n      # prefix:\"tag1\"\n      # Performs hashing operations on VIDs of type string.\n      # policy:hash\n      }\n      target: {\n        field: dst\n      # udf:{\n      #            separator:\"_\"\n      #            oldColNames:[field-0,field-1,field-2]\n      #            newColName:new-field\n      #        }\n      # Add the specified prefix to the VID. For example, if the VID is `12345`, adding the prefix `tag1` will result in `tag1_12345`. The underscore cannot be modified.\n      # prefix:\"tag1\"\n      # Performs hashing operations on VIDs of type string.\n      # policy:hash\n      }\n      #ranking: rank\n\n      # Batch operation types, including INSERT, UPDATE, and DELETE. defaults to INSERT.\n      #writeMode: INSERT\n\n      partition: 10\n      batch: 1000\n      check_point_path: /tmp/test\n    }\n   # Set the information about the Edge Type serve\n   {\n      name: serve\n      type: {\n        source: neo4j\n        sink: client\n      }\n      server: \"bolt://192.168.*.*:7687\"\n      user: neo4j\n      password:neo4j\n      database:neo4j\n      exec: \"match (a:player)-[r:serve]-&gt;(b:team) return a.id as src, b.id as dst, r.start_year as start_year, r.end_year as end_year  order by id(r)\"\n      fields: [start_year,end_year]\n      nebula.fields: [start_year,end_year]\n      source: {\n        field: src\n      }\n      target: {\n        field: dst\n      }\n      #ranking: rank\n      partition: 10\n      batch: 1000\n      check_point_path: /tmp/test\n    }\n   ]\n}\n</code></pre>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-neo4j/#exec_configuration","title":"Exec configuration","text":"<p>When configuring either the <code>tags.exec</code> or <code>edges.exec</code> parameters, you need to fill in the Cypher query. To prevent loss of data during import, it is strongly recommended to include <code>ORDER BY</code> clause in Cypher queries. Meanwhile, in order to improve data import efficiency, it is better to select indexed properties for ordering. If there is no index, users can also observe the default order and select the appropriate properties for ordering to improve efficiency. If the pattern of the default order cannot be found, users can order them by the ID of the vertex or relationship and set the <code>partition</code> to a small value to reduce the ordering pressure of Neo4j.</p> <p>Note</p> <p>Using the <code>ORDER BY</code> clause lengthens the data import time.</p> <p>Exchange needs to execute different <code>SKIP</code> and <code>LIMIT</code> Cypher statements on different Spark partitions, so <code>SKIP</code> and <code>LIMIT</code> clauses cannot be included in the Cypher statements corresponding to <code>tags.exec</code> and <code>edges.exec</code>.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-neo4j/#tagsvertex_or_edgesvertex_configuration","title":"tags.vertex or edges.vertex configuration","text":"<p>NebulaGraph uses ID as the unique primary key when creating vertexes and edges, overwriting the data in that primary key if it already exists. So, if a Neo4j property value is given as the NebulaGraph'S ID and the value is duplicated in Neo4j, duplicate IDs will be generated. One and only one of their corresponding data will be stored in the NebulaGraph, and the others will be overwritten. Because the data import process is concurrently writing data to NebulaGraph, the final saved data is not guaranteed to be the latest data in Neo4j.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-neo4j/#check_point_path_configuration","title":"check_point_path configuration","text":"<p>If breakpoint transfers are enabled, to avoid data loss, the state of the database should not change between the breakpoint and the transfer. For example, data cannot be added or deleted, and the <code>partition</code> quantity configuration should not be changed.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-neo4j/#step_4_import_data_into_nebulagraph","title":"Step 4: Import data into NebulaGraph","text":"<p>Run the following command to import Neo4j data into NebulaGraph. For a description of the parameters, see Options for import.</p> <pre><code>${SPARK_HOME}/bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange &lt;nebula-exchange-3.5.0.jar_path&gt; -c &lt;neo4j_application.conf_path&gt; \n</code></pre> <p>Note</p> <p>JAR packages are available in two ways: compiled them yourself, or download the compiled <code>.jar</code> file directly.</p> <p>For example:</p> <pre><code>${SPARK_HOME}/bin/spark-submit  --master \"local\" --class com.vesoft.nebula.exchange.Exchange  /root/nebula-exchange/nebula-exchange/target/nebula-exchange-3.5.0.jar  -c /root/nebula-exchange/nebula-exchange/target/classes/neo4j_application.conf\n</code></pre> <p>You can search for <code>batchSuccess.&lt;tag_name/edge_name&gt;</code> in the command output to check the number of successes. For example, <code>batchSuccess.follow: 300</code>.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-neo4j/#step_5_optional_validate_data","title":"Step 5: (optional) Validate data","text":"<p>Users can verify that data has been imported by executing a query in the NebulaGraph client (for example, NebulaGraph Studio). For example:</p> <pre><code>LOOKUP ON player YIELD id(vertex);\n</code></pre> <p>Users can also run the <code>SHOW STATS</code> command to view statistics.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-neo4j/#step_6_optional_rebuild_indexes_in_nebulagraph","title":"Step 6: (optional) Rebuild indexes in NebulaGraph","text":"<p>With the data imported, users can recreate and rebuild indexes in NebulaGraph. For details, see Index overview.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-oracle/","title":"Import data from Oracle","text":"<p>This topic provides an example of how to use Exchange to export Oracle data and import to NebulaGraph.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-oracle/#data_set","title":"Data set","text":"<p>This topic takes the basketballplayer dataset as an example.</p> <p>In this example, the data set has been stored in Oracle. All vertexes and edges are stored in the <code>player</code>, <code>team</code>, <code>follow</code>, and <code>serve</code> tables. The following are some of the data for each table.</p> <pre><code>oracle&gt; desc player;\n+-----------+-------+---------------+ \n| Column    | Null  | Type          |\n+-----------+-------+---------------+ \n| PLAYERID  |  -    | VARCHAR2(30)  |\n| NAME      |  -    | VARCHAR2(30)  |\n| AGE       |  -    | NUMBER        |\n+-----------+-------+---------------+ \n\noracle&gt; desc team;\n+-----------+-------+---------------+ \n| Column    | Null  | Type          |\n+-----------+-------+---------------+ \n| TEAMID    |  -    | VARCHAR2(30)  |\n| NAME      |  -    | VARCHAR2(30)  |\n+-----------+-------+---------------+ \n\noracle&gt; desc follow;\n+-------------+-------+---------------+ \n| Column      | Null  | Type          |\n+-------------+-------+---------------+ \n| SRC_PLAYER  |  -    | VARCHAR2(30)  |\n| DST_PLAYER  |  -    | VARCHAR2(30)  |\n| DEGREE      |  -    | NUMBER        |\n+-------------+-------+---------------+ \n\noracle&gt; desc serve;\n+------------+-------+---------------+ \n| Column     | Null  | Type          |\n+------------+-------+---------------+ \n| PLAYERID   |  -    | VARCHAR2(30)  |\n| TEAMID     |  -    | VARCHAR2(30)  |\n| START_YEAR |  -    | NUMBER        |\n| END_YEAR   |  -    | NUMBER        |\n+------------+-------+---------------+ \n</code></pre>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-oracle/#environment","title":"Environment","text":"<p>This example is done on MacOS. Here is the environment configuration information:</p> <ul> <li>Hardware specifications:<ul> <li>CPU: 1.7 GHz Quad-Core Intel Core i7</li> <li>Memory: 16 GB</li> </ul> </li> </ul> <ul> <li>Spark: 2.4.7, stand-alone</li> </ul> <ul> <li>NebulaGraph: 3.5.0. Deploy NebulaGraph with Docker Compose.</li> </ul>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-oracle/#prerequisites","title":"Prerequisites","text":"<p>Before importing data, you need to confirm the following information:</p> <ul> <li> <p>NebulaGraph has been installed and deployed with the following information:</p> <ul> <li>IP addresses and ports of Graph and Meta services.</li> </ul> <ul> <li>The user name and password with write permission to NebulaGraph.</li> </ul> </li> </ul> <ul> <li>Exchange has been compiled, or download the compiled <code>.jar</code> file directly.</li> </ul> <ul> <li>Spark has been installed.</li> </ul> <ul> <li>Learn about the Schema created in NebulaGraph, including names and properties of Tags and Edge types, and more.</li> </ul> <ul> <li>The Hadoop service has been installed and started.</li> </ul>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-oracle/#precautions","title":"Precautions","text":"<p>nebula-exchange_spark_2.2 supports only single table queries, not multi-table queries.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-oracle/#steps","title":"Steps","text":""},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-oracle/#step_1_create_the_schema_in_nebulagraph","title":"Step 1: Create the Schema in NebulaGraph","text":"<p>Analyze the data to create a Schema in NebulaGraph by following these steps:</p> <ol> <li> <p>Identify the Schema elements. The Schema elements in the NebulaGraph are shown in the following table.</p> Element Name Property Tag <code>player</code> <code>name string, age int</code> Tag <code>team</code> <code>name string</code> Edge Type <code>follow</code> <code>degree int</code> Edge Type <code>serve</code> <code>start_year int, end_year int</code> </li> <li> <p>Create a graph space basketballplayer in the NebulaGraph and create a Schema as shown below.</p> <pre><code>## Create a graph space.\nnebula&gt; CREATE SPACE basketballplayer \\\n        (partition_num = 10, \\\n        replica_factor = 1, \\\n        vid_type = FIXED_STRING(30));\n\n## Use the graph space basketballplayer.\nnebula&gt; USE basketballplayer;\n\n## Create the Tag player.\nnebula&gt; CREATE TAG player(name string, age int);\n\n## Create the Tag team.\nnebula&gt; CREATE TAG team(name string);\n\n## Create the Edge type follow.\nnebula&gt; CREATE EDGE follow(degree int);\n\n## Create the Edge type serve.\nnebula&gt; CREATE EDGE serve(start_year int, end_year int);\n</code></pre> </li> </ol> <p>For more information, see Quick start workflow.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-oracle/#step_2_modify_configuration_files","title":"Step 2: Modify configuration files","text":"<p>After Exchange is compiled, copy the conf file <code>target/classes/application.conf</code> to set Oracle data source configuration. In this case, the copied file is called <code>oracle_application.conf</code>. For details on each configuration item, see Parameters in the configuration file.</p> <pre><code>{\n  # Spark configuration\n  spark: {\n    app: {\n      name: NebulaGraph Exchange 3.5.0\n    }\n    driver: {\n      cores: 1\n      maxResultSize: 1G\n    }\n    cores: {\n      max: 16\n    }\n  }\n\n  # NebulaGraph configuration\n  nebula: {\n    address:{\n      # Specify the IP addresses and ports for Graph and Meta services.\n      # If there are multiple addresses, the format is \"ip1:port\",\"ip2:port\",\"ip3:port\".\n      # Addresses are separated by commas.\n      graph:[\"127.0.0.1:9669\"]\n      # the address of any of the meta services.\n      # if your NebulaGraph server is in virtual network like k8s, please config the leader address of meta.\n      meta:[\"127.0.0.1:9559\"]\n    }\n    # The account entered must have write permission for the NebulaGraph space.\n    user: root\n    pswd: nebula\n    # Fill in the name of the graph space you want to write data to in the NebulaGraph.\n    space: basketballplayer\n    connection: {\n      timeout: 3000\n      retry: 3\n    }\n    execution: {\n      retry: 3\n    }\n    error: {\n      max: 32\n      output: /tmp/errors\n    }\n    rate: {\n      limit: 1024\n      timeout: 1000\n    }\n  }\n  # Processing vertexes\n  tags: [\n    # Set the information about the Tag player.\n    {\n      # The Tag name in NebulaGraph.\n      name: player\n      type: {\n        # Specify the data source file format to Oracle.\n        source: oracle\n        # Specify how to import the data into NebulaGraph: Client or SST.\n        sink: client\n      }\n\n      url:\"jdbc:oracle:thin:@host:1521:basketball\"\n      driver: \"oracle.jdbc.driver.OracleDriver\"\n      user: \"root\"\n      password: \"123456\"\n\n      # Scanning a single table to read data.\n      # nebula-exchange_spark_2.2 must configure this parameter. Sentence is not supported.\n      # nebula-exchange_spark_2.4 and nebula-exchange_spark_3.0 can configure this parameter, but not at the same time as sentence.\n      table:\"basketball.player\"\n\n      # Use query statement to read data.\n      # This parameter is not supported by nebula-exchange_spark_2.2.\n      # nebula-exchange_spark_2.4 and nebula-exchange_spark_3.0 can configure this parameter, but not at the same time as table. Multi-table queries are supported.\n      # sentence: \"select * from people, player, team\"\n\n      # Specify the column names in the player table in fields, and their corresponding values are specified as properties in the NebulaGraph.\n      # The sequence of fields and nebula.fields must correspond to each other.\n      # If multiple column names need to be specified, separate them by commas.\n      fields: [age,name]\n      nebula.fields: [age,name]\n\n      # Specify a column of data in the table as the source of VIDs in the NebulaGraph.\n      vertex: {\n        field:playerid\n      # udf:{\n      #            separator:\"_\"\n      #            oldColNames:[field-0,field-1,field-2]\n      #            newColName:new-field\n      #        }\n      # Add the specified prefix to the VID. For example, if the VID is `12345`, adding the prefix `tag1` will result in `tag1_12345`. The underscore cannot be modified.\n      # prefix:\"tag1\"\n      # Performs hashing operations on VIDs of type string.\n      # policy:hash\n      }\n\n      # Batch operation types, including INSERT, UPDATE, and DELETE. defaults to INSERT.\n      #writeMode: INSERT\n\n      # Whether or not to delete the related incoming and outgoing edges of the vertices when performing a batch delete operation. This parameter takes effect when `writeMode` is `DELETE`.\n      #deleteEdge: false\n\n      # The number of data written to NebulaGraph in a single batch.\n      batch: 256\n\n      # The number of Spark partitions.\n      partition: 32\n    }\n    # Set the information about the Tag Team.\n    {\n      name: team\n      type: {\n        source: oracle\n        sink: client\n      }\n\n      url:\"jdbc:oracle:thin:@host:1521:basketball\"\n      driver: \"oracle.jdbc.driver.OracleDriver\"\n      user: \"root\"\n      password: \"123456\"\n      table: \"basketball.team\"\n      sentence: \"select teamid, name from team\"\n\n      fields: [name]\n      nebula.fields: [name]\n      vertex: {\n        field: teamid\n      }\n      batch: 256\n      partition: 32\n    }\n\n  ]\n\n  # Processing edges\n  edges: [\n    # Set the information about the Edge Type follow.\n    {\n      # The corresponding Edge Type name in NebulaGraph.\n      name: follow\n\n      type: {\n        # Specify the data source file format to Oracle.\n        source: oracle\n\n        # Specify how to import the Edge type data into NebulaGraph.\n        # Specify how to import the data into NebulaGraph: Client or SST.\n        sink: client\n      }\n\n      url:\"jdbc:oracle:thin:@host:1521:basketball\"\n      driver: \"oracle.jdbc.driver.OracleDriver\"\n      user: \"root\"\n      password: \"123456\"\n\n      # Scanning a single table to read data.\n      # nebula-exchange_spark_2.2 must configure this parameter. Sentence is not supported.\n      # nebula-exchange_spark_2.4 and nebula-exchange_spark_3.0 can configure this parameter, but not at the same time as sentence.\n      table:\"basketball.follow\"\n\n      # Use query statement to read data.\n      # This parameter is not supported by nebula-exchange_spark_2.2.\n      # nebula-exchange_spark_2.4 and nebula-exchange_spark_3.0 can configure this parameter, but not at the same time as table. Multi-table queries are supported.\n      # sentence: \"select * from follow, serve\"\n\n      # Specify the column names in the follow table in fields, and their corresponding values are specified as properties in the NebulaGraph.\n      # The sequence of fields and nebula.fields must correspond to each other.\n      # If multiple column names need to be specified, separate them by commas.\n      fields: [degree]\n      nebula.fields: [degree]\n\n      # In source, use a column in the follow table as the source of the edge's source vertex.\n      # In target, use a column in the follow table as the source of the edge's destination vertex.\n      source: {\n        field: src_player\n      # udf:{\n      #            separator:\"_\"\n      #            oldColNames:[field-0,field-1,field-2]\n      #            newColName:new-field\n      #        }\n      # Add the specified prefix to the VID. For example, if the VID is `12345`, adding the prefix `tag1` will result in `tag1_12345`. The underscore cannot be modified.\n      # prefix:\"tag1\"\n      # Performs hashing operations on VIDs of type string.\n      # policy:hash\n      }\n\n      target: {\n        field: dst_player\n      # udf:{\n      #            separator:\"_\"\n      #            oldColNames:[field-0,field-1,field-2]\n      #            newColName:new-field\n      #        }\n      # Add the specified prefix to the VID. For example, if the VID is `12345`, adding the prefix `tag1` will result in `tag1_12345`. The underscore cannot be modified.\n      # prefix:\"tag1\"\n      # Performs hashing operations on VIDs of type string.\n      # policy:hash\n      }\n\n      # (Optional) Specify a column as the source of the rank.\n      #ranking: rank\n\n      # Batch operation types, including INSERT, UPDATE, and DELETE. defaults to INSERT.\n      #writeMode: INSERT\n\n      # The number of data written to NebulaGraph in a single batch.\n      batch: 256\n\n      # The number of Spark partitions.\n      partition: 32\n    }\n\n    # Set the information about the Edge Type serve.\n    {\n      name: serve\n      type: {\n        source: oracle\n        sink: client\n      }\n\n      url:\"jdbc:oracle:thin:@host:1521:basketball\"\n      driver: \"oracle.jdbc.driver.OracleDriver\"\n      user: \"root\"\n      password: \"123456\"\n      table: \"basketball.serve\"\n      sentence: \"select playerid, teamid, start_year, end_year from serve\"\n\n      fields: [start_year,end_year]\n      nebula.fields: [start_year,end_year]\n      source: {\n        field: playerid\n      }\n      target: {\n        field: teamid\n      }\n      batch: 256\n      partition: 32\n    }\n  ]\n}\n</code></pre>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-oracle/#step_3_import_data_into_nebulagraph","title":"Step 3: Import data into NebulaGraph","text":"<p>Run the following command to import Oracle data into NebulaGraph. For a description of the parameters, see Options for import.</p> <pre><code>${SPARK_HOME}/bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange &lt;nebula-exchange-3.5.0.jar_path&gt; -c &lt;oracle_application.conf_path&gt;\n</code></pre> <p>Note</p> <p>JAR packages are available in two ways: compiled them yourself, or download the compiled <code>.jar</code> file directly.</p> <p>For example:</p> <pre><code>${SPARK_HOME}/bin/spark-submit  --master \"local\" --class com.vesoft.nebula.exchange.Exchange  /root/nebula-exchange/nebula-exchange/target/nebula-exchange-3.5.0.jar  -c /root/nebula-exchange/nebula-exchange/target/classes/oracle_application.conf\n</code></pre> <p>You can search for <code>batchSuccess.&lt;tag_name/edge_name&gt;</code> in the command output to check the number of successes. For example, <code>batchSuccess.follow: 300</code>.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-oracle/#step_4_optional_validate_data","title":"Step 4: (optional) Validate data","text":"<p>Users can verify that data has been imported by executing a query in the NebulaGraph client (for example, NebulaGraph Studio). For example:</p> <pre><code>LOOKUP ON player YIELD id(vertex);\n</code></pre> <p>Users can also run the SHOW STATS command to view statistics.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-oracle/#step_5_optional_rebuild_indexes_in_nebulagraph","title":"Step 5: (optional) Rebuild indexes in NebulaGraph","text":"<p>With the data imported, users can recreate and rebuild indexes in NebulaGraph. For details, see Index overview.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-orc/","title":"Import data from ORC files","text":"<p>This topic provides an example of how to use Exchange to import NebulaGraph data stored in HDFS or local ORC files.</p> <p>To import a local ORC file to NebulaGraph, see NebulaGraph Importer.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-orc/#data_set","title":"Data set","text":"<p>This topic takes the basketballplayer dataset as an example.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-orc/#environment","title":"Environment","text":"<p>This example is done on MacOS. Here is the environment configuration information:</p> <ul> <li>Hardware specifications:<ul> <li>CPU: 1.7 GHz Quad-Core Intel Core i7</li> <li>Memory: 16 GB</li> </ul> </li> </ul> <ul> <li>Spark: 2.4.7, stand-alone</li> </ul> <ul> <li>Hadoop: 2.9.2, pseudo-distributed deployment</li> </ul> <ul> <li>NebulaGraph: 3.5.0. Deploy NebulaGraph with Docker Compose.</li> </ul>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-orc/#prerequisites","title":"Prerequisites","text":"<p>Before importing data, you need to confirm the following information:</p> <ul> <li> <p>NebulaGraph has been installed and deployed with the following information:</p> <ul> <li>IP addresses and ports of Graph and Meta services.</li> </ul> <ul> <li>The user name and password with write permission to NebulaGraph.</li> </ul> </li> </ul> <ul> <li>Exchange has been compiled, or download the compiled <code>.jar</code> file directly.</li> </ul> <ul> <li>Spark has been installed.</li> </ul> <ul> <li>Learn about the Schema created in NebulaGraph, including names and properties of Tags and Edge types, and more.</li> </ul> <ul> <li>If files are stored in HDFS, ensure that the Hadoop service is running properly.</li> </ul> <ul> <li>If files are stored locally and NebulaGraph is a cluster architecture, you need to place the files in the same directory locally on each machine in the cluster.</li> </ul>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-orc/#steps","title":"Steps","text":""},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-orc/#step_1_create_the_schema_in_nebulagraph","title":"Step 1: Create the Schema in NebulaGraph","text":"<p>Analyze the data to create a Schema in NebulaGraph by following these steps:</p> <ol> <li> <p>Identify the Schema elements. The Schema elements in the NebulaGraph are shown in the following table.</p> Element Name Property Tag <code>player</code> <code>name string, age int</code> Tag <code>team</code> <code>name string</code> Edge Type <code>follow</code> <code>degree int</code> Edge Type <code>serve</code> <code>start_year int, end_year int</code> </li> <li> <p>Create a graph space basketballplayer in the NebulaGraph and create a Schema as shown below.</p> <pre><code>## Create a graph space.\nnebula&gt; CREATE SPACE basketballplayer \\\n        (partition_num = 10, \\\n        replica_factor = 1, \\\n        vid_type = FIXED_STRING(30));\n\n## Use the graph space basketballplayer.\nnebula&gt; USE basketballplayer;\n\n## Create the Tag player.\nnebula&gt; CREATE TAG player(name string, age int);\n\n## Create the Tag team.\nnebula&gt; CREATE TAG team(name string);\n\n## Create the Edge type follow.\nnebula&gt; CREATE EDGE follow(degree int);\n\n## Create the Edge type serve.\nnebula&gt; CREATE EDGE serve(start_year int, end_year int);\n</code></pre> </li> </ol> <p>For more information, see Quick start workflow.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-orc/#step_2_process_orc_files","title":"Step 2: Process ORC files","text":"<p>Confirm the following information:</p> <ol> <li> <p>Process ORC files to meet Schema requirements.</p> </li> <li> <p>Obtain the ORC file storage path.</p> </li> </ol>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-orc/#step_3_modify_configuration_files","title":"Step 3: Modify configuration files","text":"<p>After Exchange is compiled, copy the conf file <code>target/classes/application.conf</code> to set ORC data source configuration. In this example, the copied file is called <code>orc_application.conf</code>. For details on each configuration item, see Parameters in the configuration file.</p> <pre><code>{\n  # Spark configuration\n  spark: {\n    app: {\n      name: NebulaGraph Exchange 3.5.0\n    }\n    driver: {\n      cores: 1\n      maxResultSize: 1G\n    }\n    executor: {\n        memory:1G\n    }\n\n    cores: {\n      max: 16\n    }\n  }\n\n  # NebulaGraph configuration\n  nebula: {\n    address:{\n      # Specify the IP addresses and ports for Graph and all Meta services.\n      # If there are multiple addresses, the format is \"ip1:port\",\"ip2:port\",\"ip3:port\".\n      # Addresses are separated by commas.\n      graph:[\"127.0.0.1:9669\"]\n      # the address of any of the meta services.\n      # if your NebulaGraph server is in virtual network like k8s, please config the leader address of meta.\n      meta:[\"127.0.0.1:9559\"]\n    }\n\n    # The account entered must have write permission for the NebulaGraph space.\n    user: root\n    pswd: nebula\n\n    # Fill in the name of the graph space you want to write data to in the NebulaGraph.\n    space: basketballplayer\n    connection: {\n      timeout: 3000\n      retry: 3\n    }\n    execution: {\n      retry: 3\n    }\n    error: {\n      max: 32\n      output: /tmp/errors\n    }\n    rate: {\n      limit: 1024\n      timeout: 1000\n    }\n  }\n\n  # Processing vertexes\n  tags: [\n    # Set the information about the Tag player.\n    {\n      name: player\n      type: {\n        # Specify the data source file format to ORC.\n        source: orc\n\n        # Specify how to import the data into NebulaGraph: Client or SST.\n        sink: client\n      }\n\n      # Specify the path to the ORC file.\n      # If the file is stored in HDFS, use double quotation marks to enclose the file path, starting with hdfs://. For example, \"hdfs://ip:port/xx/xx\".\n      # If the file is stored locally, use double quotation marks to enclose the file path, starting with file://. For example, \"file:///tmp/xx.orc\".\n      path: \"hdfs://192.168.*.*:9000/data/vertex_player.orc\"\n\n      # Specify the key name in the ORC file in fields, and its corresponding value will serve as the data source for the properties specified in the NebulaGraph.\n      # If multiple values need to be specified, separate them with commas.\n      fields: [age,name]\n\n      # Specify the property names defined in NebulaGraph.\n      # The sequence of fields and nebula.fields must correspond to each other.\n      nebula.fields: [age, name]\n\n      # Specify a column of data in the table as the source of VIDs in the NebulaGraph.\n      # The value of vertex must be consistent with the field in the ORC file.\n      # Currently, NebulaGraph 3.5.0 supports only strings or integers of VID.\n      vertex: {\n        field:id\n      # udf:{\n      #            separator:\"_\"\n      #            oldColNames:[field-0,field-1,field-2]\n      #            newColName:new-field\n      #        }\n      # Add the specified prefix to the VID. For example, if the VID is `12345`, adding the prefix `tag1` will result in `tag1_12345`. The underscore cannot be modified.\n      # prefix:\"tag1\"\n      # Performs hashing operations on VIDs of type string.\n      # policy:hash\n      }\n\n      # Batch operation types, including INSERT, UPDATE, and DELETE. defaults to INSERT.\n      #writeMode: INSERT\n\n      # Whether or not to delete the related incoming and outgoing edges of the vertices when performing a batch delete operation. This parameter takes effect when `writeMode` is `DELETE`.\n      #deleteEdge: false\n\n      # The number of data written to NebulaGraph in a single batch.\n      batch: 256\n\n      # The number of Spark partitions.\n      partition: 32\n    }\n\n    # Set the information about the Tag team.\n    {\n      name: team\n      type: {\n        source: orc\n        sink: client\n      }\n      path: \"hdfs://192.168.*.*:9000/data/vertex_team.orc\"\n      fields: [name]\n      nebula.fields: [name]\n      vertex: {\n        field:id\n      }\n      batch: 256\n      partition: 32\n    }\n\n\n\n    # If more vertexes need to be added, refer to the previous configuration to add them.\n  ]\n  # Processing edges\n  edges: [\n    # Set the information about the Edge Type follow.\n    {\n      # Specify the Edge Type name defined in NebulaGraph.\n      name: follow\n      type: {\n        # Specify the data source file format to ORC.\n        source: orc\n\n        # Specify how to import the data into NebulaGraph: Client or SST.\n        sink: client\n      }\n\n      # Specify the path to the ORC file.\n      # If the file is stored in HDFS, use double quotation marks to enclose the file path, starting with hdfs://. For example, \"hdfs://ip:port/xx/xx\".\n      # If the file is stored locally, use double quotation marks to enclose the file path, starting with file://. For example, \"file:///tmp/xx.orc\".\n      path: \"hdfs://192.168.*.*:9000/data/edge_follow.orc\"\n\n      # Specify the key name in the ORC file in fields, and its corresponding value will serve as the data source for the properties specified in the NebulaGraph.\n      # If multiple values need to be specified, separate them with commas.\n      fields: [degree]\n\n      # Specify the property names defined in NebulaGraph.\n      # The sequence of fields and nebula.fields must correspond to each other.\n      nebula.fields: [degree]\n\n      # Specify a column as the source for the source and destination vertexes.\n      # The value of vertex must be consistent with the field in the ORC file.\n      # Currently, NebulaGraph 3.5.0 supports only strings or integers of VID.\n      source: {\n        field: src\n      # udf:{\n      #            separator:\"_\"\n      #            oldColNames:[field-0,field-1,field-2]\n      #            newColName:new-field\n      #        }\n      # Add the specified prefix to the VID. For example, if the VID is `12345`, adding the prefix `tag1` will result in `tag1_12345`. The underscore cannot be modified.\n      # prefix:\"tag1\"\n      # Performs hashing operations on VIDs of type string.\n      # policy:hash\n      }\n      target: {\n        field: dst\n      # udf:{\n      #            separator:\"_\"\n      #            oldColNames:[field-0,field-1,field-2]\n      #            newColName:new-field\n      #        }\n      # Add the specified prefix to the VID. For example, if the VID is `12345`, adding the prefix `tag1` will result in `tag1_12345`. The underscore cannot be modified.\n      # prefix:\"tag1\"\n      # Performs hashing operations on VIDs of type string.\n      # policy:hash\n      }\n\n      # (Optional) Specify a column as the source of the rank.\n      #ranking: rank\n\n      # Batch operation types, including INSERT, UPDATE, and DELETE. defaults to INSERT.\n      #writeMode: INSERT\n\n      # The number of data written to NebulaGraph in a single batch.\n      batch: 256\n\n      # The number of Spark partitions.\n      partition: 32\n    }\n\n    # Set the information about the Edge type serve.\n    {\n      name: serve\n      type: {\n        source: orc\n        sink: client\n      }\n      path: \"hdfs://192.168.*.*:9000/data/edge_serve.orc\"\n      fields: [start_year,end_year]\n      nebula.fields: [start_year, end_year]\n      source: {\n        field: src\n      }\n      target: {\n        field: dst\n      }\n      batch: 256\n      partition: 32\n    }\n\n  # If more edges need to be added, refer to the previous configuration to add them.\n}\n</code></pre>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-orc/#step_4_import_data_into_nebulagraph","title":"Step 4: Import data into NebulaGraph","text":"<p>Run the following command to import ORC data into NebulaGraph. For a description of the parameters, see Options for import.</p> <pre><code>${SPARK_HOME}/bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange &lt;nebula-exchange-3.5.0.jar_path&gt; -c &lt;orc_application.conf_path&gt; \n</code></pre> <p>Note</p> <p>JAR packages are available in two ways: compiled them yourself, or download the compiled <code>.jar</code> file directly.</p> <p>For example:</p> <pre><code>${SPARK_HOME}/bin/spark-submit  --master \"local\" --class com.vesoft.nebula.exchange.Exchange  /root/nebula-exchange/nebula-exchange/target/nebula-exchange-3.5.0.jar  -c /root/nebula-exchange/nebula-exchange/target/classes/orc_application.conf\n</code></pre> <p>You can search for <code>batchSuccess.&lt;tag_name/edge_name&gt;</code> in the command output to check the number of successes. For example, <code>batchSuccess.follow: 300</code>.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-orc/#step_5_optional_validate_data","title":"Step 5: (optional) Validate data","text":"<p>Users can verify that data has been imported by executing a query in the NebulaGraph client (for example, NebulaGraph Studio). For example:</p> <pre><code>LOOKUP ON player YIELD id(vertex);\n</code></pre> <p>Users can also run the <code>SHOW STATS</code> command to view statistics.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-orc/#step_6_optional_rebuild_indexes_in_nebulagraph","title":"Step 6: (optional) Rebuild indexes in NebulaGraph","text":"<p>With the data imported, users can recreate and rebuild indexes in NebulaGraph. For details, see Index overview.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-parquet/","title":"Import data from Parquet files","text":"<p>This topic provides an example of how to use Exchange to import NebulaGraph data stored in HDFS or local Parquet files.</p> <p>To import a local Parquet file to NebulaGraph, see NebulaGraph Importer.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-parquet/#data_set","title":"Data set","text":"<p>This topic takes the basketballplayer dataset as an example.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-parquet/#environment","title":"Environment","text":"<p>This example is done on MacOS. Here is the environment configuration information:</p> <ul> <li>Hardware specifications:<ul> <li>CPU: 1.7 GHz Quad-Core Intel Core i7</li> <li>Memory: 16 GB</li> </ul> </li> </ul> <ul> <li>Spark: 2.4.7, stand-alone</li> </ul> <ul> <li>Hadoop: 2.9.2, pseudo-distributed deployment</li> </ul> <ul> <li>NebulaGraph: 3.5.0. Deploy NebulaGraph with Docker Compose.</li> </ul>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-parquet/#prerequisites","title":"Prerequisites","text":"<p>Before importing data, you need to confirm the following information:</p> <ul> <li> <p>NebulaGraph has been installed and deployed with the following information:</p> <ul> <li>IP addresses and ports of Graph and Meta services.</li> </ul> <ul> <li>The user name and password with write permission to NebulaGraph.</li> </ul> </li> </ul> <ul> <li>Exchange has been compiled, or download the compiled <code>.jar</code> file directly.</li> </ul> <ul> <li>Spark has been installed.</li> </ul> <ul> <li>Learn about the Schema created in NebulaGraph, including names and properties of Tags and Edge types, and more.</li> </ul> <ul> <li>If files are stored in HDFS, ensure that the Hadoop service is running properly.</li> </ul> <ul> <li>If files are stored locally and NebulaGraph is a cluster architecture, you need to place the files in the same directory locally on each machine in the cluster.</li> </ul>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-parquet/#steps","title":"Steps","text":""},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-parquet/#step_1_create_the_schema_in_nebulagraph","title":"Step 1: Create the Schema in NebulaGraph","text":"<p>Analyze the data to create a Schema in NebulaGraph by following these steps:</p> <ol> <li> <p>Identify the Schema elements. The Schema elements in the NebulaGraph are shown in the following table.</p> Element Name Property Tag <code>player</code> <code>name string, age int</code> Tag <code>team</code> <code>name string</code> Edge Type <code>follow</code> <code>degree int</code> Edge Type <code>serve</code> <code>start_year int, end_year int</code> </li> <li> <p>Create a graph space basketballplayer in the NebulaGraph and create a Schema as shown below.</p> <pre><code>## Create a graph space.\nnebula&gt; CREATE SPACE basketballplayer \\\n        (partition_num = 10, \\\n        replica_factor = 1, \\\n        vid_type = FIXED_STRING(30));\n\n## Use the graph space basketballplayer.\nnebula&gt; USE basketballplayer;\n\n## Create the Tag player.\nnebula&gt; CREATE TAG player(name string, age int);\n\n## Create the Tag team.\nnebula&gt; CREATE TAG team(name string);\n\n## Create the Edge type follow.\nnebula&gt; CREATE EDGE follow(degree int);\n\n## Create the Edge type serve.\nnebula&gt; CREATE EDGE serve(start_year int, end_year int);\n</code></pre> </li> </ol> <p>For more information, see Quick start workflow.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-parquet/#step_2_process_parquet_files","title":"Step 2: Process Parquet files","text":"<p>Confirm the following information:</p> <ol> <li> <p>Process Parquet files to meet Schema requirements.</p> </li> <li> <p>Obtain the Parquet file storage path.</p> </li> </ol>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-parquet/#step_3_modify_configuration_files","title":"Step 3: Modify configuration files","text":"<p>After Exchange is compiled, copy the conf file <code>target/classes/application.conf</code> to set Parquet data source configuration. In this example, the copied file is called <code>parquet_application.conf</code>. For details on each configuration item, see Parameters in the configuration file.</p> <pre><code>{\n  # Spark configuration\n  spark: {\n    app: {\n      name: NebulaGraph Exchange 3.5.0\n    }\n    driver: {\n      cores: 1\n      maxResultSize: 1G\n    }\n    executor: {\n        memory:1G\n    }\n\n    cores: {\n      max: 16\n    }\n  }\n\n  # NebulaGraph configuration\n  nebula: {\n    address:{\n      # Specify the IP addresses and ports for Graph and all Meta services.\n      # If there are multiple addresses, the format is \"ip1:port\",\"ip2:port\",\"ip3:port\".\n      # Addresses are separated by commas.\n      graph:[\"127.0.0.1:9669\"]\n      # the address of any of the meta services.\n      # if your NebulaGraph server is in virtual network like k8s, please config the leader address of meta.\n      meta:[\"127.0.0.1:9559\"]\n    }\n\n    # The account entered must have write permission for the NebulaGraph space.\n    user: root\n    pswd: nebula\n\n    # Fill in the name of the graph space you want to write data to in the NebulaGraph.\n    space: basketballplayer\n    connection: {\n      timeout: 3000\n      retry: 3\n    }\n    execution: {\n      retry: 3\n    }\n    error: {\n      max: 32\n      output: /tmp/errors\n    }\n    rate: {\n      limit: 1024\n      timeout: 1000\n    }\n  }\n\n  # Processing vertexes\n  tags: [\n    # Set the information about the Tag player.\n    {\n      # Specify the Tag name defined in NebulaGraph.\n      name: player\n      type: {\n        # Specify the data source file format to Parquet.\n        source: parquet\n\n        # Specifies how to import the data into NebulaGraph: Client or SST.\n        sink: client\n      }\n\n      # Specify the path to the Parquet file.\n      # If the file is stored in HDFS, use double quotation marks to enclose the file path, starting with hdfs://. For example, \"hdfs://ip:port/xx/xx\".\n      # If the file is stored locally, use double quotation marks to enclose the file path, starting with file://. For example, \"file:///tmp/xx.parquet\".\n      path: \"hdfs://192.168.*.13:9000/data/vertex_player.parquet\"\n\n      # Specify the key name in the Parquet file in fields, and its corresponding value will serve as the data source for the properties specified in the NebulaGraph.\n      # If multiple values need to be specified, separate them with commas.\n      fields: [age,name]\n\n      # Specify the property name defined in NebulaGraph.\n      # The sequence of fields and nebula.fields must correspond to each other.\n      nebula.fields: [age, name]\n\n      # Specify a column of data in the table as the source of VIDs in the NebulaGraph.\n      # The value of vertex must be consistent with the field in the Parquet file.\n      # Currently, NebulaGraph 3.5.0 supports only strings or integers of VID.\n      vertex: {\n        field:id\n      # udf:{\n      #            separator:\"_\"\n      #            oldColNames:[field-0,field-1,field-2]\n      #            newColName:new-field\n      #        }\n      # Add the specified prefix to the VID. For example, if the VID is `12345`, adding the prefix `tag1` will result in `tag1_12345`. The underscore cannot be modified.\n      # prefix:\"tag1\"\n      # Performs hashing operations on VIDs of type string.\n      # policy:hash\n      }\n\n      # Batch operation types, including INSERT, UPDATE, and DELETE. defaults to INSERT.\n      #writeMode: INSERT\n\n      # Whether or not to delete the related incoming and outgoing edges of the vertices when performing a batch delete operation. This parameter takes effect when `writeMode` is `DELETE`.\n      #deleteEdge: false\n\n      # The number of data written to NebulaGraph in a single batch.\n      batch: 256\n\n      # The number of Spark partitions.\n      partition: 32\n    }\n\n    # Set the information about the Tag team.\n    {\n      name: team\n      type: {\n        source: parquet\n        sink: client\n      }\n      path: \"hdfs://192.168.11.13:9000/data/vertex_team.parquet\"\n      fields: [name]\n      nebula.fields: [name]\n      vertex: {\n        field:id\n      }\n      batch: 256\n      partition: 32\n    }\n\n\n    # If more vertexes need to be added, refer to the previous configuration to add them.\n  ]\n  # Processing edges\n  edges: [\n    # Set the information about the Edge Type follow.\n    {\n      # Specify the Edge Type name defined in NebulaGraph.\n      name: follow\n      type: {\n        # Specify the data source file format to Parquet.\n        source: parquet\n\n        # Specifies how to import the data into NebulaGraph: Client or SST.\n        sink: client\n      }\n\n      # Specify the path to the Parquet file.\n      # If the file is stored in HDFS, use double quotation marks to enclose the file path, starting with hdfs://. For example, \"hdfs://ip:port/xx/xx\".\n      # If the file is stored locally, use double quotation marks to enclose the file path, starting with file://. For example, \"file:///tmp/xx.parquet\".\n      path: \"hdfs://192.168.11.13:9000/data/edge_follow.parquet\"\n\n      # Specify the key name in the Parquet file in fields, and its corresponding value will serve as the data source for the properties specified in the NebulaGraph.\n      # If multiple values need to be specified, separate them with commas.\n      fields: [degree]\n\n      # Specify the property name defined in NebulaGraph.\n      # The sequence of fields and nebula.fields must correspond to each other.\n      nebula.fields: [degree]\n\n      # Specify a column as the source for the source and destination vertexes.\n      # The values of vertex must be consistent with the fields in the Parquet file.\n      # Currently, NebulaGraph 3.5.0 supports only strings or integers of VID.\n      source: {\n        field: src\n      # udf:{\n      #            separator:\"_\"\n      #            oldColNames:[field-0,field-1,field-2]\n      #            newColName:new-field\n      #        }\n      # Add the specified prefix to the VID. For example, if the VID is `12345`, adding the prefix `tag1` will result in `tag1_12345`. The underscore cannot be modified.\n      # prefix:\"tag1\"\n      # Performs hashing operations on VIDs of type string.\n      # policy:hash\n      }\n      target: {\n        field: dst\n      # udf:{\n      #            separator:\"_\"\n      #            oldColNames:[field-0,field-1,field-2]\n      #            newColName:new-field\n      #        }\n      # Add the specified prefix to the VID. For example, if the VID is `12345`, adding the prefix `tag1` will result in `tag1_12345`. The underscore cannot be modified.\n      # prefix:\"tag1\"\n      # Performs hashing operations on VIDs of type string.\n      # policy:hash\n      }\n\n      # (Optional) Specify a column as the source of the rank.\n      #ranking: rank\n\n      # Batch operation types, including INSERT, UPDATE, and DELETE. defaults to INSERT.\n      #writeMode: INSERT\n\n      # The number of data written to NebulaGraph in a single batch.\n      batch: 256\n\n      # The number of Spark partitions.\n      partition: 32\n    }\n\n    # Set the information about the Edge type serve.\n    {\n      name: serve\n      type: {\n        source: parquet\n        sink: client\n      }\n      path: \"hdfs://192.168.11.13:9000/data/edge_serve.parquet\"\n      fields: [start_year,end_year]\n      nebula.fields: [start_year, end_year]\n      source: {\n        field: src\n      }\n      target: {\n        field: dst\n      }\n      batch: 256\n      partition: 32\n    }\n\n  ]\n  # If more edges need to be added, refer to the previous configuration to add them.\n}\n</code></pre>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-parquet/#step_4_import_data_into_nebulagraph","title":"Step 4: Import data into NebulaGraph","text":"<p>Run the following command to import Parquet data into NebulaGraph. For a description of the parameters, see Options for import.</p> <pre><code>${SPARK_HOME}/bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange &lt;nebula-exchange-3.5.0.jar_path&gt; -c &lt;parquet_application.conf_path&gt; \n</code></pre> <p>Note</p> <p>JAR packages are available in two ways: compiled them yourself, or download the compiled <code>.jar</code> file directly.</p> <p>For example:</p> <pre><code>${SPARK_HOME}/bin/spark-submit  --master \"local\" --class com.vesoft.nebula.exchange.Exchange  /root/nebula-exchange/nebula-exchange/target/nebula-exchange-3.5.0.jar  -c /root/nebula-exchange/nebula-exchange/target/classes/parquet_application.conf\n</code></pre> <p>You can search for <code>batchSuccess.&lt;tag_name/edge_name&gt;</code> in the command output to check the number of successes. For example, <code>batchSuccess.follow: 300</code>.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-parquet/#step_5_optional_validate_data","title":"Step 5: (optional) Validate data","text":"<p>Users can verify that data has been imported by executing a query in the NebulaGraph client (for example, NebulaGraph Studio). For example:</p> <pre><code>LOOKUP ON player YIELD id(vertex);\n</code></pre> <p>Users can also run the <code>SHOW STATS</code> command to view statistics.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-parquet/#step_6_optional_rebuild_indexes_in_nebulagraph","title":"Step 6: (optional) Rebuild indexes in NebulaGraph","text":"<p>With the data imported, users can recreate and rebuild indexes in NebulaGraph. For details, see Index overview.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-pulsar/","title":"Import data from Pulsar","text":"<p>This topic provides an example of how to use Exchange to import NebulaGraph data stored in Pulsar.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-pulsar/#environment","title":"Environment","text":"<p>This example is done on MacOS. Here is the environment configuration information:</p> <ul> <li>Hardware specifications:<ul> <li>CPU: 1.7 GHz Quad-Core Intel Core i7</li> <li>Memory: 16 GB</li> </ul> </li> </ul> <ul> <li>Spark: 2.4.7, stand-alone</li> </ul> <ul> <li>NebulaGraph: 3.5.0. Deploy NebulaGraph with Docker Compose.</li> </ul>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-pulsar/#prerequisites","title":"Prerequisites","text":"<p>Before importing data, you need to confirm the following information:</p> <ul> <li> <p>NebulaGraph has been installed and deployed with the following information:</p> <ul> <li>IP addresses and ports of Graph and Meta services.</li> </ul> <ul> <li>The user name and password with write permission to NebulaGraph.</li> </ul> </li> </ul> <ul> <li>Exchange has been compiled, or download the compiled <code>.jar</code> file directly.</li> </ul> <ul> <li>Spark has been installed.</li> </ul> <ul> <li>Learn about the Schema created in NebulaGraph, including names and properties of Tags and Edge types, and more.</li> </ul> <ul> <li>The Pulsar service has been installed and started.</li> </ul>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-pulsar/#precautions","title":"Precautions","text":"<ul> <li>Only client mode is supported when importing Pulsar data, i.e. the value of parameters <code>tags.type.sink</code> and <code>edges.type.sink</code> is <code>client</code>.</li> </ul> <ul> <li>When importing Pulsar data, do not use Exchange version 3.4.0, which adds caching of imported data and does not support streaming data import. Use Exchange versions 3.0.0, 3.3.0, or 3.5.0.</li> </ul>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-pulsar/#steps","title":"Steps","text":""},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-pulsar/#step_1_create_the_schema_in_nebulagraph","title":"Step 1: Create the Schema in NebulaGraph","text":"<p>Analyze the data to create a Schema in NebulaGraph by following these steps:</p> <ol> <li> <p>Identify the Schema elements. The Schema elements in the NebulaGraph are shown in the following table.</p> Element Name Property Tag <code>player</code> <code>name string, age int</code> Tag <code>team</code> <code>name string</code> Edge Type <code>follow</code> <code>degree int</code> Edge Type <code>serve</code> <code>start_year int, end_year int</code> </li> <li> <p>Create a graph space basketballplayer in the NebulaGraph and create a Schema as shown below.</p> <pre><code>## Create a graph space\nnebula&gt; CREATE SPACE basketballplayer \\\n        (partition_num = 10, \\\n        replica_factor = 1, \\\n        vid_type = FIXED_STRING(30));\n\n## Use the graph space basketballplayer\nnebula&gt; USE basketballplayer;\n\n## Create the Tag player\nnebula&gt; CREATE TAG player(name string, age int);\n\n## Create the Tag team\nnebula&gt; CREATE TAG team(name string);\n\n## Create the Edge type follow\nnebula&gt; CREATE EDGE follow(degree int);\n\n## Create the Edge type serve\nnebula&gt; CREATE EDGE serve(start_year int, end_year int);\n</code></pre> </li> </ol> <p>For more information, see Quick start workflow.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-pulsar/#step_2_modify_configuration_files","title":"Step 2: Modify configuration files","text":"<p>After Exchange is compiled, copy the conf file <code>target/classes/application.conf</code> to set Pulsar data source configuration. In this example, the copied file is called <code>pulsar_application.conf</code>. For details on each configuration item, see Parameters in the configuration file.</p> <pre><code>{\n  # Spark configuration\n  spark: {\n    app: {\n      name: NebulaGraph Exchange 3.5.0\n    }\n    driver: {\n      cores: 1\n      maxResultSize: 1G\n    }\n    cores: {\n      max: 16\n    }\n  }\n\n\n  # NebulaGraph configuration\n  nebula: {\n    address:{\n      # Specify the IP addresses and ports for Graph and all Meta services.\n      # If there are multiple addresses, the format is \"ip1:port\",\"ip2:port\",\"ip3:port\".\n      # Addresses are separated by commas.\n      graph:[\"127.0.0.1:9669\"]\n      # the address of any of the meta services.\n      # if your NebulaGraph server is in virtual network like k8s, please config the leader address of meta.\n      meta:[\"127.0.0.1:9559\"]\n    }\n\n    # The account entered must have write permission for the NebulaGraph space.\n    user: root\n    pswd: nebula\n\n    # Fill in the name of the graph space you want to write data to in the NebulaGraph.\n    space: basketballplayer\n    connection: {\n      timeout: 3000\n      retry: 3\n    }\n    execution: {\n      retry: 3\n    }\n    error: {\n      max: 32\n      output: /tmp/errors\n    }\n    rate: {\n      limit: 1024\n      timeout: 1000\n    }\n  }\n  # Processing vertices\n  tags: [\n    # Set the information about the Tag player.\n    {\n      # The corresponding Tag name in NebulaGraph.\n      name: player\n      type: {\n        # Specify the data source file format to Pulsar.\n        source: pulsar\n        # Specify how to import the data into NebulaGraph. Only client is supported.\n        sink: client\n      }\n      # The address of the Pulsar server.\n      service: \"pulsar://127.0.0.1:6650\"\n      # admin.url of pulsar.\n      admin: \"http://127.0.0.1:8081\"\n      # The Pulsar option can be configured from topic, topics or topicsPattern.\n      options: {\n        topics: \"topic1,topic2\"\n      }\n\n      # Specify the column names in the player table in fields, and their corresponding values are specified as properties in the NebulaGraph.\n      # The sequence of fields and nebula.fields must correspond to each other.\n      # If multiple column names need to be specified, separate them by commas.\n      fields: [age,name]\n      nebula.fields: [age,name]\n\n      # Specify a column of data in the table as the source of VIDs in the NebulaGraph.\n      vertex:{\n          field:playerid\n      # udf:{\n      #            separator:\"_\"\n      #            oldColNames:[field-0,field-1,field-2]\n      #            newColName:new-field\n      #        }\n      # Add the specified prefix to the VID. For example, if the VID is `12345`, adding the prefix `tag1` will result in `tag1_12345`. The underscore cannot be modified.\n      # prefix:\"tag1\"\n      # Performs hashing operations on VIDs of type string.\n      # policy:hash\n      }\n\n      # Batch operation types, including INSERT, UPDATE, and DELETE. defaults to INSERT.\n      #writeMode: INSERT\n\n      # Whether or not to delete the related incoming and outgoing edges of the vertices when performing a batch delete operation. This parameter takes effect when `writeMode` is `DELETE`.\n      #deleteEdge: false\n\n      # The number of data written to NebulaGraph in a single batch.\n      batch: 10\n\n      # The number of Spark partitions.\n      partition: 10\n      # The interval for message reading. Unit: second.\n      interval.seconds: 10\n    }\n    # Set the information about the Tag Team.\n    {\n      name: team\n      type: {\n        source: pulsar\n        sink: client\n      }\n      service: \"pulsar://127.0.0.1:6650\"\n      admin: \"http://127.0.0.1:8081\"\n      options: {\n        topics: \"topic1,topic2\"\n      }\n      fields: [name]\n      nebula.fields: [name]\n      vertex:{\n          field:teamid\n      }\n      batch: 10\n      partition: 10\n      interval.seconds: 10\n    }\n\n  ]\n\n  # Processing edges\n  edges: [\n    # Set the information about Edge Type follow\n    {\n      # The corresponding Edge Type name in NebulaGraph.\n      name: follow\n\n      type: {\n        # Specify the data source file format to Pulsar.\n        source: pulsar\n\n        # Specify how to import the Edge type data into NebulaGraph.\n        # Specify how to import the data into NebulaGraph. Only client is supported.\n        sink: client\n      }\n\n      # The address of the Pulsar server.\n      service: \"pulsar://127.0.0.1:6650\"\n      # admin.url of pulsar.\n      admin: \"http://127.0.0.1:8081\"\n      # The Pulsar option can be configured from topic, topics or topicsPattern.\n      options: {\n        topics: \"topic1,topic2\"\n      }\n\n      # Specify the column names in the follow table in fields, and their corresponding values are specified as properties in the NebulaGraph.\n      # The sequence of fields and nebula.fields must correspond to each other.\n      # If multiple column names need to be specified, separate them by commas.\n      fields: [degree]\n      nebula.fields: [degree]\n\n      # In source, use a column in the follow table as the source of the edge's source vertex.\n      # In target, use a column in the follow table as the source of the edge's destination vertex.\n      source:{\n          field:src_player\n      # udf:{\n      #            separator:\"_\"\n      #            oldColNames:[field-0,field-1,field-2]\n      #            newColName:new-field\n      #        }\n      # Add the specified prefix to the VID. For example, if the VID is `12345`, adding the prefix `tag1` will result in `tag1_12345`. The underscore cannot be modified.\n      # prefix:\"tag1\"\n      # Performs hashing operations on VIDs of type string.\n      # policy:hash\n      }\n\n      target:{\n          field:dst_player\n      # udf:{\n      #            separator:\"_\"\n      #            oldColNames:[field-0,field-1,field-2]\n      #            newColName:new-field\n      #        }\n      # Add the specified prefix to the VID. For example, if the VID is `12345`, adding the prefix `tag1` will result in `tag1_12345`. The underscore cannot be modified.\n      # prefix:\"tag1\"\n      # Performs hashing operations on VIDs of type string.\n      # policy:hash\n      }\n\n      # (Optional) Specify a column as the source of the rank.\n      #ranking: rank\n\n      # Batch operation types, including INSERT, UPDATE, and DELETE. defaults to INSERT.\n      #writeMode: INSERT\n\n      # The number of data written to NebulaGraph in a single batch.\n      batch: 10\n\n      # The number of Spark partitions.\n      partition: 10\n\n      # The interval for message reading. Unit: second.\n      interval.seconds: 10\n    }\n\n    # Set the information about the Edge Type serve\n    {\n      name: serve\n      type: {\n        source: Pulsar\n        sink: client\n      }\n      service: \"pulsar://127.0.0.1:6650\"\n      admin: \"http://127.0.0.1:8081\"\n      options: {\n        topics: \"topic1,topic2\"\n      }\n\n      fields: [start_year,end_year]\n      nebula.fields: [start_year,end_year]\n      source:{\n          field:playerid\n      }\n\n      target:{\n          field:teamid\n      }\n\n      # (Optional) Specify a column as the source of the rank.\n      #ranking: rank\n\n      batch: 10\n      partition: 10\n      interval.seconds: 10\n    }\n  ]\n}\n</code></pre>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-pulsar/#step_3_import_data_into_nebulagraph","title":"Step 3: Import data into NebulaGraph","text":"<p>Run the following command to import Pulsar data into NebulaGraph. For a description of the parameters, see Options for import.</p> <pre><code>${SPARK_HOME}/bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange &lt;nebula-exchange-3.5.0.jar_path&gt; -c &lt;pulsar_application.conf_path&gt;\n</code></pre> <p>Note</p> <p>JAR packages are available in two ways: compiled them yourself, or download the compiled <code>.jar</code> file directly.</p> <p>For example:</p> <pre><code>${SPARK_HOME}/bin/spark-submit  --master \"local\" --class com.vesoft.nebula.exchange.Exchange  /root/nebula-exchange/nebula-exchange/target/nebula-exchange-3.5.0.jar  -c /root/nebula-exchange/nebula-exchange/target/classes/pulsar_application.conf\n</code></pre> <p>You can search for <code>batchSuccess.&lt;tag_name/edge_name&gt;</code> in the command output to check the number of successes. For example, <code>batchSuccess.follow: 300</code>.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-pulsar/#step_4_optional_validate_data","title":"Step 4: (optional) Validate data","text":"<p>Users can verify that data has been imported by executing a query in the NebulaGraph client (for example, NebulaGraph Studio). For example:</p> <pre><code>LOOKUP ON player YIELD id(vertex);\n</code></pre> <p>Users can also run the <code>SHOW STATS</code> command to view statistics.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-pulsar/#step_5_optional_rebuild_indexes_in_nebulagraph","title":"Step 5: (optional) Rebuild indexes in NebulaGraph","text":"<p>With the data imported, users can recreate and rebuild indexes in NebulaGraph. For details, see Index overview.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-sst/","title":"Import data from SST files","text":"<p>This topic provides an example of how to generate the data from the data source into an SST (Sorted String Table) file and save it on HDFS, and then import it into NebulaGraph. The sample data source is a CSV file.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-sst/#precautions","title":"Precautions","text":"<ul> <li>The SST file can be imported only in Linux.</li> </ul> <ul> <li>The default value of the property is not supported.</li> </ul>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-sst/#background_information","title":"Background information","text":"<p>Exchange supports two data import modes:</p> <ul> <li>Import the data from the data source directly into NebulaGraph as nGQL statements.</li> </ul> <ul> <li>Generate the SST file from the data source, and use Console to import the SST file into NebulaGraph.</li> </ul> <p>The following describes the scenarios, implementation methods, prerequisites, and steps for generating an SST file and importing data.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-sst/#scenarios","title":"Scenarios","text":"<ul> <li> <p>Suitable for online services, because the generation almost does not affect services (just reads the Schema), and the import speed is fast.</p> <p>Caution</p> <p>Although the import speed is fast, write operations in the corresponding space are blocked during the import period (about 10 seconds). Therefore, you are advised to import data in off-peak hours.</p> </li> </ul> <ul> <li>Suitable for scenarios with a large amount of data from data sources for its fast import speed.</li> </ul>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-sst/#implementation_methods","title":"Implementation methods","text":"<p>The underlying code in NebulaGraph uses RocksDB as the key-value storage engine. RocksDB is a storage engine based on the hard disk, providing a series of APIs for creating and importing SST files to help quickly import massive data.</p> <p>The SST file is an internal file containing an arbitrarily long set of ordered key-value pairs for efficient storage of large amounts of key-value data. The entire process of generating SST files is mainly done by Exchange Reader, sstProcessor, and sstWriter. The whole data processing steps are as follows:</p> <ol> <li> <p>Reader reads data from the data source.</p> </li> <li> <p>sstProcessor generates the SST file from the NebulaGraph's Schema information and uploads it to the HDFS. For details about the format of the SST file, see Data Storage Format.</p> </li> <li> <p>sstWriter opens a file and inserts data. When generating SST files, keys must be written in sequence.</p> </li> <li> <p>After the SST file is generated, RocksDB imports the SST file into NebulaGraph using the <code>IngestExternalFile()</code> method. For example:</p> <pre><code>IngestExternalFileOptions ifo;\n# Import two SST files\nStatus s = db_-&gt;IngestExternalFile({\"/home/usr/file1.sst\", \"/home/usr/file2.sst\"}, ifo);\nif (!s.ok()) {\n  printf(\"Error while adding file %s and %s, Error %s\\n\",\n         file_path1.c_str(), file_path2.c_str(), s.ToString().c_str());\n  return 1;\n}\n</code></pre> <p>When the <code>IngestExternalFile()</code> method is called, RocksDB copies the file to the data directory by default and blocks the RocksDB write operation. If the key range in the SST file overwrites the Memtable key range, flush the Memtable to the hard disk. After placing the SST file in an optimal location in the LSM tree, assign a global serial number to the file and turn on the write operation.</p> </li> </ol>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-sst/#data_set","title":"Data set","text":"<p>This topic takes the basketballplayer dataset as an example.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-sst/#environment","title":"Environment","text":"<p>This example is done on MacOS. Here is the environment configuration information:</p> <ul> <li>Hardware specifications:<ul> <li>CPU: 1.7 GHz Quad-Core Intel Core i7</li> <li>Memory: 16 GB</li> </ul> </li> </ul> <ul> <li>Spark: 2.4.7, stand-alone</li> </ul> <ul> <li>Hadoop: 2.9.2, pseudo-distributed deployment</li> </ul> <ul> <li>NebulaGraph: 3.5.0.</li> </ul>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-sst/#prerequisites","title":"Prerequisites","text":"<p>Before importing data, you need to confirm the following information:</p> <ul> <li> <p>NebulaGraph has been installed and deployed with the following information:</p> <ul> <li>IP addresses and ports of Graph and Meta services.</li> </ul> <ul> <li>The user name and password with write permission to NebulaGraph.</li> </ul> <ul> <li><code>--ws_storage_http_port</code> in the Meta service configuration file is the same as <code>--ws_http_port</code> in the Storage service configuration file. For example, <code>19779</code>.</li> </ul> <ul> <li><code>--ws_meta_http_port</code> in the Graph service configuration file is the same as <code>--ws_http_port</code> in the Meta service configuration file. For example, <code>19559</code>.</li> </ul> <ul> <li>The information about the Schema, including names and properties of Tags and Edge types, and more.</li> </ul> </li> </ul> <ul> <li>Exchange has been compiled, or download the compiled <code>.jar</code> file directly.</li> </ul> <ul> <li>Spark has been installed.</li> </ul> <ul> <li>JDK 1.8 or the later version has been installed and the environment variable <code>JAVA_HOME</code> has been configured.</li> </ul> <ul> <li> <p>The Hadoop service has been installed and started.</p> <p>Note</p> <ul> <li>To generate SST files of other data sources, see documents of the corresponding data source and check the prerequisites.</li> </ul> <ul> <li>To generate SST files only, users do not need to install the Hadoop service on the machine where the Storage service is deployed.</li> </ul> <ul> <li>To delete the SST file after the ingest (data import) operation, add the configuration <code>-- move_Files =true</code> to the Storage Service configuration file.</li> </ul> </li> </ul>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-sst/#steps","title":"Steps","text":""},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-sst/#step_1_create_the_schema_in_nebulagraph","title":"Step 1: Create the Schema in NebulaGraph","text":"<p>Analyze the data to create a Schema in NebulaGraph by following these steps:</p> <ol> <li> <p>Identify the Schema elements. The Schema elements in the NebulaGraph are shown in the following table.</p> Element Name Property Tag <code>player</code> <code>name string, age int</code> Tag <code>team</code> <code>name string</code> Edge Type <code>follow</code> <code>degree int</code> Edge Type <code>serve</code> <code>start_year int, end_year int</code> </li> <li> <p>Create a graph space basketballplayer in the NebulaGraph and create a Schema as shown below.</p> <pre><code>## Create a graph space\nnebula&gt; CREATE SPACE basketballplayer \\\n        (partition_num = 10, \\\n        replica_factor = 1, \\\n        vid_type = FIXED_STRING(30));\n\n## Use the graph space basketballplayer\nnebula&gt; USE basketballplayer;\n\n## Create the Tag player\nnebula&gt; CREATE TAG player(name string, age int);\n\n## Create the Tag team\nnebula&gt; CREATE TAG team(name string);\n\n## Create the Edge type follow\nnebula&gt; CREATE EDGE follow(degree int);\n\n## Create the Edge type serve\nnebula&gt; CREATE EDGE serve(start_year int, end_year int);\n</code></pre> </li> </ol> <p>For more information, see Quick start workflow.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-sst/#step_2_process_csv_files","title":"Step 2: Process CSV files","text":"<p>Confirm the following information:</p> <ol> <li> <p>Process CSV files to meet Schema requirements.</p> <p>Note</p> <p>Exchange supports uploading CSV files with or without headers.</p> </li> <li> <p>Obtain the CSV file storage path.</p> </li> </ol>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-sst/#step_3_modify_configuration_files","title":"Step 3: Modify configuration files","text":"<p>After Exchange is compiled, copy the conf file <code>target/classes/application.conf</code> to set SST data source configuration. In this example, the copied file is called <code>sst_application.conf</code>. For details on each configuration item, see Parameters in the configuration file.</p> <pre><code>{\n  # Spark configuration\n  spark: {\n    app: {\n      name: NebulaGraph Exchange 3.5.0\n    }\n\n    master:local\n\n    driver: {\n      cores: 1\n      maxResultSize: 1G\n    }\n\n    executor: {\n        memory:1G\n    }\n\n    cores:{\n      max: 16\n    }\n  }\n\n  # NebulaGraph configuration\n  nebula: {\n    address:{\n      graph:[\"192.8.168.XXX:9669\"]\n      # the address of any of the meta services.\n      # if your NebulaGraph server is in virtual network like k8s, please config the leader address of meta.\n      meta:[\"192.8.168.XXX:9559\"]\n    }\n    user: root\n    pswd: nebula\n    space: basketballplayer\n\n    # SST file configuration\n    path:{\n        # The local directory that temporarily stores generated SST files\n        local:\"/tmp\"\n\n        # The path for storing the SST file in the HDFS\n        remote:\"/sst\"\n\n        # The NameNode address of HDFS, for example, \"hdfs://&lt;ip/hostname&gt;:&lt;port&gt;\"\n        hdfs.namenode: \"hdfs://*.*.*.*:9000\"\n    }\n\n    # The connection parameters of clients\n    connection: {\n      # The timeout duration of socket connection and execution. Unit: milliseconds.\n      timeout: 30000\n    }\n\n    error: {\n      # The maximum number of failures that will exit the application.\n      max: 32\n      # Failed import jobs are logged in the output path.\n      output: /tmp/errors\n    }\n\n    # Use Google's RateLimiter to limit requests to NebulaGraph.\n    rate: {\n      # Steady throughput of RateLimiter.\n      limit: 1024\n\n      # Get the allowed timeout duration from RateLimiter. Unit: milliseconds.\n      timeout: 1000\n    }\n  }\n\n\n  # Processing vertices\n  tags: [\n    # Set the information about the Tag player.\n    {\n      # Specify the Tag name defined in NebulaGraph.\n      name: player\n      type: {\n        # Specify the data source file format to CSV.\n        source: csv\n\n        # Specify how to import the data into NebulaGraph: Client or SST.\n        sink: sst\n      }\n\n      # Specify the path to the CSV file.\n      # If the file is stored in HDFS, use double quotation marks to enclose the file path, starting with hdfs://. For example, \"hdfs://&lt;ip/hostname&gt;:port/xx/xx.csv\".\n      path: \"hdfs://*.*.*.*:9000/dataset/vertex_player.csv\"\n\n      # If the CSV file does not have a header, use [_c0, _c1, _c2, ..., _cn] to represent its header and indicate the columns as the source of the property values.\n      # If the CSV file has a header, use the actual column name.\n      fields: [_c1, _c2]\n\n      # Specify the property name defined in NebulaGraph.\n      # The sequence of fields and nebula.fields must correspond to each other.\n      nebula.fields: [age, name]\n\n      # Specify a column of data in the table as the source of VIDs in NebulaGraph.\n      # The value of vertex must be consistent with the column name in the above fields or csv.fields.\n      # Currently, NebulaGraph 3.5.0 supports only strings or integers of VID.\n      vertex: {\n        field:_c0\n        # Add the specified prefix to the VID. For example, if the VID is `12345`, adding the prefix `tag1` will result in `tag1_12345`. The underscore cannot be modified.\n      # prefix:\"tag1\"\n      # Performs hashing operations on VIDs of type string.\n      # policy:hash\n      }\n\n      # The delimiter specified. The default value is comma.\n      separator: \",\"\n\n      # If the CSV file has a header, set the header to true.\n      # If the CSV file does not have a header, set the header to false. The default value is false.\n      header: false\n\n      # Batch operation types, including INSERT, UPDATE, and DELETE. defaults to INSERT.\n      #writeMode: INSERT\n\n      # The number of data written to NebulaGraph in a single batch.\n      batch: 256\n\n      # The number of Spark partitions.\n      partition: 32\n\n      # Whether to repartition data based on the number of partitions of graph spaces in NebulaGraph when generating the SST file.\n      repartitionWithNebula: false\n    }\n\n    # Set the information about the Tag Team.\n    {\n      name: team\n      type: {\n        source: csv\n        sink: sst\n      }\n      path: \"hdfs://*.*.*.*:9000/dataset/vertex_team.csv\"\n      fields: [_c1]\n      nebula.fields: [name]\n      vertex: {\n        field:_c0\n      }\n      separator: \",\"\n      header: false\n      batch: 256\n      partition: 32\n      repartitionWithNebula: false\n    }\n    # If more vertices need to be added, refer to the previous configuration to add them.\n  ]\n  # Processing edges\n  edges: [\n    # Set the information about the Edge Type follow.\n    {\n      # The Edge Type name defined in NebulaGraph.\n      name: follow\n      type: {\n        # Specify the data source file format to CSV.\n        source: csv\n\n        # Specify how to import the data into NebulaGraph: Client or SST.\n        sink: sst\n      }\n\n      # Specify the path to the CSV file.\n      # If the file is stored in HDFS, use double quotation marks to enclose the file path, starting with hdfs://. For example, \"hdfs://&lt;ip/hostname&gt;:port/xx/xx.csv\".\n      path: \"hdfs://*.*.*.*:9000/dataset/edge_follow.csv\"\n\n      # If the CSV file does not have a header, use [_c0, _c1, _c2, ..., _cn] to represent its header and indicate the columns as the source of the property values.\n      # If the CSV file has a header, use the actual column name.\n      fields: [_c2]\n\n      # Specify the property name defined in NebulaGraph.\n      # The sequence of fields and nebula.fields must correspond to each other.\n      nebula.fields: [degree]\n\n      # Specify a column as the source for the source and destination vertices.\n      # The value of vertex must be consistent with the column name in the above fields or csv.fields.\n      # Currently, NebulaGraph 3.5.0 supports only strings or integers of VID.\n      source: {\n        field: _c0\n        # Add the specified prefix to the VID. For example, if the VID is `12345`, adding the prefix `tag1` will result in `tag1_12345`. The underscore cannot be modified.\n      # prefix:\"tag1\"\n      # Performs hashing operations on VIDs of type string.\n      # policy:hash\n      }\n      target: {\n        field: _c1\n        # Add the specified prefix to the VID. For example, if the VID is `12345`, adding the prefix `tag1` will result in `tag1_12345`. The underscore cannot be modified.\n      # prefix:\"tag1\"\n      # Performs hashing operations on VIDs of type string.\n      # policy:hash\n      }\n\n      # The delimiter specified. The default value is comma.\n      separator: \",\"\n\n      # (Optional) Specify a column as the source of the rank.\n\n      #ranking: rank\n\n      # If the CSV file has a header, set the header to true.\n      # If the CSV file does not have a header, set the header to false. The default value is false.\n      header: false\n\n      # Batch operation types, including INSERT, UPDATE, and DELETE. defaults to INSERT.\n      #writeMode: INSERT\n\n      # The number of data written to NebulaGraph in a single batch.\n      batch: 256\n\n      # The number of Spark partitions.\n      partition: 32\n\n      # Whether to repartition data based on the number of partitions of graph spaces in NebulaGraph when generating the SST file.\n      repartitionWithNebula: false\n    }\n\n    # Set the information about the Edge Type serve.\n    {\n      name: serve\n      type: {\n        source: csv\n        sink: sst\n      }\n      path: \"hdfs://*.*.*.*:9000/dataset/edge_serve.csv\"\n      fields: [_c2,_c3]\n      nebula.fields: [start_year, end_year]\n      source: {\n        field: _c0\n      }\n      target: {\n        field: _c1\n      }\n      separator: \",\"\n      header: false\n      batch: 256\n      partition: 32\n      repartitionWithNebula: false\n    }\n\n  ]\n  # If more edges need to be added, refer to the previous configuration to add them.\n}\n</code></pre>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-sst/#step_4_generate_the_sst_file","title":"Step 4: Generate the SST file","text":"<p>Run the following command to generate the SST file from the CSV source file. For a description of the parameters, see Options for import.</p> <pre><code>${SPARK_HOME}/bin/spark-submit --master \"local\" --conf spark.sql.shuffle.partition=&lt;shuffle_concurrency&gt; --class com.vesoft.nebula.exchange.Exchange &lt;nebula-exchange-3.5.0.jar_path&gt; -c &lt;sst_application.conf_path&gt; \n</code></pre> <p>Note</p> <p>When generating SST files, the shuffle operation of Spark will be involved. Note that the configuration of <code>spark.sql.shuffle.partition</code> should be added when you submit the command.</p> <p>Note</p> <p>JAR packages are available in two ways: compiled them yourself, or download the compiled <code>.jar</code> file directly.</p> <p>For example:</p> <pre><code>${SPARK_HOME}/bin/spark-submit  --master \"local\" --conf spark.sql.shuffle.partition=200 --class com.vesoft.nebula.exchange.Exchange  /root/nebula-exchange/nebula-exchange/target/nebula-exchange-3.5.0.jar  -c /root/nebula-exchange/nebula-exchange/target/classes/sst_application.conf\n</code></pre> <p>After the task is complete, you can view the generated SST file in the <code>/sst</code> directory (specified by the <code>nebula.path.remote</code> parameter) on HDFS.</p> <p>Note</p> <p>If you modify the Schema, such as rebuilding the graph space, modifying the Tag, or modifying the Edge type, you need to regenerate the SST file because the SST file verifies the space ID, Tag ID, and Edge ID.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-sst/#step_5_import_the_sst_file","title":"Step 5: Import the SST file","text":"<p>Note</p> <p>Confirm the following information before importing:</p> <ul> <li>Confirm that the Hadoop service has been deployed on all the machines where the Storage service is deployed, and configure <code>HADOOP_HOME</code> and <code>JAVA_HOME</code>.</li> </ul> <ul> <li>The <code>--ws_storage_http_port</code> in the Meta service configuration file (add it manually if it does not exist) is the same as the <code>--ws_http_port</code> in the Storage service configuration file. For example, both are <code>19779</code>.</li> </ul> <ul> <li>The <code>--ws_meta_http_port</code> in the Graph service configuration file (add it manually if it does not exist) is the same as the <code>--ws_http_port</code> in the Meta service configuration file. For example, both are <code>19559</code>.</li> </ul> <p>Connect to the NebulaGraph database using the client tool and import the SST file as follows:</p> <ol> <li> <p>Run the following command to select the graph space you created earlier.</p> <pre><code>nebula&gt; USE basketballplayer;\n</code></pre> </li> <li> <p>Run the following command to download the SST file:</p> <pre><code>nebula&gt; SUBMIT JOB DOWNLOAD HDFS \"hdfs://&lt;hadoop_address&gt;:&lt;hadoop_port&gt;/&lt;sst_file_path&gt;\";\n</code></pre> <p>For example:</p> <pre><code>nebula&gt; SUBMIT JOB DOWNLOAD HDFS \"hdfs://*.*.*.*:9000/sst\";\n</code></pre> </li> <li> <p>Run the following command to import the SST file:</p> <pre><code>nebula&gt; SUBMIT JOB INGEST;\n</code></pre> </li> </ol> <p>Note</p> <ul> <li>To download the SST file again, delete the <code>download</code> folder in the space ID in the <code>data/storage/nebula</code> directory in the NebulaGraph installation path, and then download the SST file again. If the space has multiple copies, the <code>download</code> folder needs to be deleted on all machines where the copies are saved.</li> </ul> <ul> <li>If there is a problem with the import and re-importing is required, re-execute <code>SUBMIT JOB INGEST;</code>.</li> </ul>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-sst/#step_6_optional_validate_data","title":"Step 6: (optional) Validate data","text":"<p>Users can verify that data has been imported by executing a query in the NebulaGraph client (for example, NebulaGraph Studio). For example:</p> <pre><code>LOOKUP ON player YIELD id(vertex);\n</code></pre> <p>Users can also run the <code>SHOW STATS</code> command to view statistics.</p>"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-sst/#step_7_optional_rebuild_indexes_in_nebulagraph","title":"Step 7: (optional) Rebuild indexes in NebulaGraph","text":"<p>With the data imported, users can recreate and rebuild indexes in NebulaGraph. For details, see Index overview.</p>"},{"location":"nebula-explorer/12.query-visually/","title":"Visual Query","text":"<p>The Visual Query feature uses a visual representation to express related requests. It allows you to create query scenarios to look up the desired data and view the corresponding statements. You can construct visual query statements by simply dragging and dropping, and then the system displays the query results on the query panel.</p> <p>Compatibility</p> <p>The Visual Query feature is not compatible with NebulaGraph versions below 3.0.0. </p> <p>Note</p> <p>Currently, the Visual Query feature is still in beta.</p>"},{"location":"nebula-explorer/12.query-visually/#prerequisite","title":"Prerequisite","text":"<ul> <li>You have choosen a graph space. For details, see Choose graph spaces.</li> <li>You have created indexes for particular queries. For details, see MATCH precautions and CREATE INDEX.</li> </ul>"},{"location":"nebula-explorer/12.query-visually/#page_elements","title":"Page elements","text":"<p>At the top of the Explorer page, click Visual Query to enter the visual query page. On the left side of the Visual Query page, all the Tag(s) corresponding to the graph space (e.g.player and team) and the Tag named Any Tag are displayed. You can query vertices without tags by the Tag named Any Tag.</p> <p>Note</p> <p>Any Tag can also be used to query the vertex without tags.</p> <p>On the page, the descriptions of other icons are as follows.</p> Icon/Element Description The selected vertices and edges are the results to be queried. Double-click on the query pattern frame to limit the number of queries (with a priority higher than the value of the maximum number of returns in the global settings). Only querying edges is not supported. Zoom in on the query page. Zoom out on the query page. Save the current query graph. The saved graph is cached in the browser. View all stored query graphs. Up to 10 recently saved visual graphs are displayed. Click any of the stored graphs to display them on the visual query page. nGQL Click nGQL to view the statement corresponding to the query pattern. Run Query Click Run Query to display the query results visually on the canvas."},{"location":"nebula-explorer/12.query-visually/#steps","title":"Steps","text":"<ol> <li> <p>Drag several target tags from the left side of the Visual Query page to the canvas to create the corresponding vertices.</p> </li> <li> <p>Click a vertex, hold down the left mouse button on the anchor point at the edge of the vertex, and drag it to another vertex to create the corresponding edge.</p> </li> <li> <p>Set a vertex by clicking it. The descriptions of configuration options are as follows.</p> <p></p> <ul> <li>Tag Name: Set zero, one, or multiple tags.</li> </ul> <p>Note</p> <p>One vertex can have zero or multiple tags:</p> <ul> <li>When 0 tag is set, query the vertex without tags.</li> <li>When 1 tag is set, query the vertex with that tag.</li> <li>When multiple tags are set, query the vertex that has all the tags you set. </li> </ul> <ul> <li>Filter: Add one or more sets of filter conditions, including vertex properties, operators, and property values. </li> </ul> <p>Note</p> <p>When setting multiple tags in the Tag Name dialog box, it is not supported to set filter conditions to query data.</p> </li> <li> <p>Set an edge by clicking. The descriptions configuration options are as follows.</p> <p></p> <ul> <li>Edge Type: Set one or multiple edge types.</li> </ul> <p>Note</p> <p>One edge have one and only one edge type:</p> <ul> <li>When one edge type is set, query the edge with that edge type.</li> </ul> <ul> <li>When multiple edge types are set, query the edge that has any of the edge types you set.</li> </ul> <ul> <li>Direction: Set the edge direction between two vertices, including Outgoing, Incoming, and Bidirect.</li> </ul> <ul> <li>Single: Set a fixed-length path.</li> </ul> <ul> <li>Range: Set a variable-length.</li> </ul> <ul> <li>Filter: Add one or more sets of filter conditions, including edge properties, operators, and property values.</li> </ul> <p>Note</p> <p>When setting multiple edge types in the Edge Type dialog box, it is not supported to set filter conditions to query data.</p> </li> <li> <p>After the query scenarios (pattern) is created, click  and select the result you want to return.</p> </li> <li> <p>Click Run Query on the upper right corner of the Visual Query page to display the query results on the canvas.</p> </li> </ol>"},{"location":"nebula-explorer/12.query-visually/#examples","title":"Examples","text":""},{"location":"nebula-explorer/12.query-visually/#example_1","title":"Example 1","text":"<p>Find out players who follow each other with Yao Ming and older than 35, and which teams these players are loyal to, and limit the number of the query patterns of the players and teams to 6.</p> <p></p> <ol> <li> <p>Create a query pattern by dragging and dropping Tags to the panel (2 players and 1 team). </p> </li> <li> <p>Configure filter conditions.</p> <ol> <li>Set the filter condition for the first player to <code>player.name == Yao Ming</code>.</li> <li>Set the edge type of the edge between the first and second vertices of the tag player to <code>follow</code>, set the direction to <code>Bidirect</code>, and the steps to <code>1</code>.</li> <li>Set the filter condition for the second player to <code>player.age &gt; 35</code>.</li> <li>Set the edge type of the edge between the second player and the team to <code>serve</code>, the direction to <code>Outgoing</code>, and the steps to <code>1</code>.</li> <li>Click  to select the second player, the team, and the serve edge between them.</li> <li>Click the Query Pattern frame, and set the Limit Number to <code>6</code>.</li> </ol> </li> <li> <p>Click Run Query, and the system displays 6 query patterns on the canvas.</p> </li> </ol>"},{"location":"nebula-explorer/12.query-visually/#example_2","title":"Example 2","text":"<p>Find out what teams two mutually-following players are loyal to and query for all players on that team who are older than 30.</p> <p></p> <ol> <li>Create a query pattern by dragging and dropping Tags to the panel (3 players and 1 team).</li> <li> <p>Configure filter conditions.</p> <ol> <li>Set the edge type of the edge between the first and second players to <code>follow</code>, set the direction to <code>Bidirect</code>, and the steps to <code>1</code>.</li> <li>Set the edge type of the edge between the first player and the team to <code>serve</code>, the direction to <code>Outgoing</code>, and the steps to <code>1</code>.</li> <li>Set the edge type of the edge between the second player and the team to <code>serve</code>, the direction to <code>Outgoing</code>, and the steps to <code>1</code>.</li> <li>Set the filter conditions for the third player to <code>player.age &gt; 30</code>.</li> <li>Set the edge type of the edge between the third player and the team to <code>serve</code>, the direction to <code>Outgoing</code>, and the steps to <code>1</code>.</li> <li>Click  to select the third player, the team, and the serve edge between them.</li> </ol> </li> <li> <p>Click Run Query.</p> </li> </ol>"},{"location":"nebula-explorer/ex-ug-page-overview/","title":"Page overview","text":"<p>This topic introduces the NebulaGraph Explorer page to help you learn more about NebulaGraph Explorer's functions.</p> <p>The NebulaGraph Explorer page consists of three modules top navigation bar, left-side navigation bar, and canvas. </p> <p></p>"},{"location":"nebula-explorer/ex-ug-page-overview/#top_navigation_bar","title":"Top navigation bar","text":"Icon/Element Description Explorer Visually explore and analyze data. For more information, see Start querying, Vertex Filter, Graph exploration and Graph algorithm. Visual Query Visually construct scenarios for data queries. For more information, see Visual Query. Workflow Visually construct custom workflows for complex graph computing. The Workflow page can be displayed only when Workflow is enabled in . For more information, see Workflow overview. Users can design their schemas on the canvas to visually display the relationships between vertices and edges. For more information, see Schema drafting. Manage NebulaGraph database graph spaces. For more information, see Create a schema. Bulk import of data into NebulaGraph. For more information, see Import data. Query the NebulaGraph data with nGQL statements. For more information, see Console. The template of the nGQL. For details, see nGQL template. Manage the users in NebulaGraph database. For more information, see Database user Management\u3002 Global Settings. You can set the language of the Explorer page, enable Beta functions, and the maximum number of canvas query results. Guide and help you in using NebulaGraph. Feedback page. You can report troubles, submit suggestions, participate in research, or contact the NebulaGraph team. Show the connection information and version information. You can change passwords and log out."},{"location":"nebula-explorer/ex-ug-page-overview/#left-side_navigation_bar","title":"Left-side navigation bar","text":"<p>Note</p> <p>After logging into Explorer, select a graph space and click on it to unlock query and exploration functions in the left-side navigation bar. For more information, see Choose graph spaces.</p> <p>Click the icons in the left-side navigation bar to import, analyze, and explore graph data. The descriptions of the icons are as follows:</p> Icon Description Enter VIDs or tags to query data. For more information, see Ways to query data. Search for target vertexes displayed on the canvas. For more information, see Filter vertices. Perform explorations on the vertices on the canvas by setting edge directions, steps, and filtering conditions. For more information, see Graph exploration. Select at least two vertices on the canvas to search for their common neighbors. For more information, see Graph exploration. Find all paths, the shortest path, and the non-loop paths from the source to the destination vertex. For more information, see Graph exploration. Choose whether to display the properties of vertices or edges on the canvas. For more information, see Graph exploration. Perform graph computing based on the vertexes and edges on the canvas. For more Information see Graph computing. Perform property calculation based on the aggregated edges on the canvas. For more Information see Property calculation\u3002 View historical snapshots. For more information, see Canvas snapshots. View all graph spaces. Click a graph space to create a canvas corresponding to it. For more information, see Choose graph spaces."},{"location":"nebula-explorer/ex-ug-page-overview/#canvas","title":"Canvas","text":"<p>Note</p> <p>After logging into Explorer, select a graph space and click on it to enter the canvas page. For more information, see Choose graph spaces.</p> <p>Graph data can be displayed visually on a canvas. The canvas consists of the following parts:</p> <ul> <li>Tabs on the Top</li> <li>Visualization modes</li> <li>Data storage</li> <li>Search box</li> <li>Layouts</li> <li>Minimap</li> <li>Data overview</li> </ul> <p>For more information, see Canvas overview.</p>"},{"location":"nebula-explorer/ex-ug-shortcuts/","title":"Basic operations and shortcuts","text":"<p>This topic lists the basic operations and shortcuts supported in Explorer.</p>"},{"location":"nebula-explorer/ex-ug-shortcuts/#basic_operations","title":"Basic operations","text":"Operation Description Move a canvas Hold down left click and drag the canvas. Zoom in or out the canvas Use the mouse wheel to zoom in or out. Select one single vertex or edge Left-click a vertex or an edge. Select multiple vertices and edges Hold Shift and left-click vertices and edges. Batch selection Hold down right click and frame vertices and edges; Or Hold Shift and hold down left click, and then frame vertices and edges. Move selected vertices Left-click the selected vertices and then move them."},{"location":"nebula-explorer/ex-ug-shortcuts/#shortcuts","title":"Shortcuts","text":"Operation Description Enter Expand Shift + '-' Zoom out Shift + '+' Zoom in Shift + 'l' Display Ctrl/Cmd + 'z' Undo Ctrl/Cmd + Shift + 'z' Redo Ctrl/Cmd + 'a' Select all vertices. Selected + 'Backspace' Hide the selected elements. Selected + Shift + 'Backspace' Hide the unselected elements."},{"location":"nebula-explorer/faq/","title":"FAQ","text":"<p>This topic lists the frequently asked questions for using NebulaGraph Explorer. You can use the search box in the help center or the search function of the browser to match the questions you are looking for.</p>"},{"location":"nebula-explorer/faq/#will_the_dag_controller_service_crash_if_the_graph_service_returns_too_much_result_data","title":"Will the Dag Controller service crash if the Graph service returns too much result data?","text":"<p>The Dag Controller service only provides scheduling capabilities and will not crash, but the NebulaGraph Analytics service may crash due to insufficient memory when writing too much data to HDFS or NebulaGraph, or reading too much data from HDFS or NebulaGraph.</p>"},{"location":"nebula-explorer/faq/#can_i_continue_a_job_from_a_failed_task","title":"Can I continue a job from a failed task?","text":"<p>Not supported. You can only re-execute the entire job.</p>"},{"location":"nebula-explorer/faq/#how_can_i_speed_it_up_if_a_task_result_is_saved_slowly_or_data_is_transferred_slowly_between_tasks","title":"How can I speed it up if a task result is saved slowly or data is transferred slowly between tasks?","text":"<p>The Dag Controller contains graph query components and graph computing components. Graph queries send requests to a graph service for queries, so the graph queries can only be accelerated by increasing the memory of the graph service. Graph computing is performed on distributed nodes provided by NebulaGraph Analytics, so graph computing can be accelerated by increasing the size of the NebulaGraph Analytics cluster.</p>"},{"location":"nebula-explorer/faq/#the_hdfs_server_cannot_be_connected_and_the_task_status_is_running","title":"The HDFS server cannot be connected and the task status is running.","text":"<p>Set the timeout period for HDFS connections as follows:</p> <pre><code>&lt;configuration&gt;\n&lt;property&gt;\n    &lt;name&gt;ipc.client.connect.timeout&lt;/name&gt;\n    &lt;value&gt;3000&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n    &lt;name&gt;ipc.client.connect.max.retries.on.timeouts&lt;/name&gt;\n    &lt;value&gt;3&lt;/value&gt;\n&lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>"},{"location":"nebula-explorer/faq/#how_to_resolve_the_error_errdial_unix_missing_address","title":"How to resolve the error <code>Err:dial unix: missing address</code>?","text":"<p>Modify the configuration file <code>dag-ctrl/etc/dag-ctrl-api.yaml</code> to configure the <code>UserName</code> of the SSH.</p>"},{"location":"nebula-explorer/faq/#how_to_resolve_the_error_bash_homexxxnebula-analyticsscriptsrun_algosh_no_such_file_or_directory","title":"How to resolve the error <code>bash: /home/xxx/nebula-analytics/scripts/run_algo.sh: No such file or directory</code>?","text":"<p>Modify the configuration file <code>dag-ctrl/etc/tasks.yaml</code>to configure the algorithm execution path parameter <code>exec_file</code>.</p>"},{"location":"nebula-explorer/faq/#how_to_resolve_the_error_lib64libmso6_version_glibc_229_not_found_required_by_homevesoftjdk-1801jrelibamd64serverlibjvmso","title":"How to resolve the error <code>/lib64/libm.so.6: version 'GLIBC_2.29' not found (required by /home/vesoft/jdk-18.0.1/jre/lib/amd64/server/libjvm.so)</code>?","text":"<p>Because the operating system version does not support JDK18, the command <code>YUM</code> cannot download <code>GLIBC_2.29</code>, you can install JDK1.8. Does not forget to change the JDK address in <code>nebula-analytics/scripts/set_env.sh</code>.</p>"},{"location":"nebula-explorer/faq/#how_to_resolve_the_error_handshake_failed_ssh_unable_to_authenticate_attempted_methods_none_publickey_no_supported_methods_remain","title":"How to resolve the error <code>handshake failed: ssh: unable to authenticate, attempted methods [none publickey], no supported methods remain</code>?","text":"<p>Reconfigure the permissions to <code>744</code> on the folder <code>.ssh</code> and <code>600</code> on the file <code>.ssh/authorized_keys</code>.</p>"},{"location":"nebula-explorer/faq/#how_to_resolve_the_error_there_are_0_nebulagraph_analytics_available_clustersize_should_be_less_than_or_equal_to_it","title":"How to resolve the error <code>There are 0 NebulaGraph Analytics available. clusterSize should be less than or equal to it</code>?","text":"<p>Check according to the following procedure:</p> <ol> <li> <p>Check whether the configuration of SSH password-free login between nodes is successful. You can run the <code>ssh &lt;user_name&gt;@&lt;node_ip&gt;</code> command on the Dag Controller machine to check whether the login succeeds.</p> <p>Note</p> <p>If the Dag Controller and Analytics are on the same machine, you also need to configure SSH password-free login.</p> </li> <li> <p>Check the configuration file of the Dag Controller.</p> <ul> <li>Check whether the SSH user in <code>etc/dag-ctrl-api.yaml</code> is the same as the user who starts the Dag Controller service and the user who configs SSH password-free login.</li> </ul> <ul> <li>Check whether the algorithm path in <code>etc/tasks.yaml</code> is correct.</li> </ul> <ul> <li>Check whether Hadoop and Java paths in <code>scripts/set_env.sh</code> are correct.</li> </ul> </li> <li> <p>Restart the Dag Controller for the settings to take effect.</p> </li> </ol>"},{"location":"nebula-explorer/faq/#how_to_resolve_the_error_no_available_namenodes_dial_tcp_xxxxxxxx8020_connect_connection_timed_out","title":"How to resolve the error  <code>no available namenodes: dial tcp xx.xx.xx.xx:8020: connect: connection timed out</code>?","text":"<p>Check whether the HDFS namenode port 8020 is open.</p>"},{"location":"nebula-explorer/faq/#how_to_resolve_the_error_orgapachehadoopnetconnecttimeoutexception_60000_millis_timeout","title":"How to resolve the error  <code>org.apache.hadoop.net.ConnectTimeoutException: 60000 millis timeout</code>?","text":"<p>Check whether the HDFS datanode port 50010 is open.</p> <p>If the port is not opened, an error similar to the following may be reported:</p> <ul> <li><code>Check failed: false close hdfs-file failed</code></li> <li><code>org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /analytics/xx/tasks/analytics_xxx/xxx.csv could only be replicated to 0 nodes instead of minReplication</code></li> </ul>"},{"location":"nebula-explorer/faq/#how_to_resolve_the_error_broadcasthpp193_check_failed_size_trecv_bytes_sizeofchunk_tail_t_recv_message_too_small_0","title":"How to resolve the error <code>broadcast.hpp:193] Check failed: (size_t)recv_bytes &gt;= sizeof(chunk_tail_t) recv message too small: 0</code>?","text":"<p>The amount of data to be processed is too small, but the number of compute nodes and processes is too large. Smaller <code>clusterSize</code> and <code>processes</code> need to be set when submitting jobs.</p>"},{"location":"nebula-explorer/iframe/","title":"Inline frame","text":"<p>NebulaGraph Explorer supports inline frame (iframe), which can be used to embed canvases into third-party pages. This topic describes how to embed a canvas.</p>"},{"location":"nebula-explorer/iframe/#prerequisites","title":"Prerequisites","text":"<p>The Explorer has been installed.</p>"},{"location":"nebula-explorer/iframe/#precautions","title":"Precautions","text":"<ul> <li>Embedded Explorer pages only access the corresponding graph space by default, so some pages and features are not displayed. For example, the upper navigation bar and some left-navigation-bar features are hidden. If you need to access multiple graph spaces, you can embed them separately on multiple pages.</li> <li>Language switching is not supported. The default language is Chinese.</li> </ul>"},{"location":"nebula-explorer/iframe/#steps","title":"Steps","text":"<ol> <li> <p>Modify the configuration file <code>config/app-config.yaml</code> in the installation directory of Explorer. The following parameters need to be modified.</p> <pre><code># Uncomment the CertFile and KeyFile parameters.\nCertFile: \"./config/NebulaGraphExplorer.crt\"\nKeyFile: \"./config/NebulaGraphExplorer.key\"\n\n# Modify the value of IframeMode.Enable to true.\nIframeMode:\n  Enable: true\n# You can set the URI whitelist of the window. By default, no URI is restricted.\n  # Origins:\n  #  - \"http://192.168.8.8\"\n</code></pre> </li> <li> <p>Use the command <code>openssl</code> in the directory <code>config</code> to generate a self-signed certificate. The following is an example.</p> <pre><code>openssl req -newkey rsa:4096 -x509 -sha512 -days 365 -nodes -subj \"/CN=NebulaGraphExplorer.com\" -out NebulaGraphExplorer.crt -keyout NebulaGraphExplorer.key\n</code></pre> <ul> <li><code>-newkey</code>: The secret key is automatically generated when a certificate request or self-signed certificate is generated.</li> <li><code>-x509</code>: Generates a self-signed certificate.</li> <li><code>-sha512</code>: Specifies the algorithm of the message digest.</li> <li><code>-days</code>: The number of days that the certificate generated with parameter <code>-x509</code> is valid.</li> <li><code>-nodes</code>: Outputs the secret key without encryption.</li> <li><code>-subj</code>: Sets the subject of the request.</li> <li><code>-out</code>: Specifies the name of the generated certificate request or self-signed certificate.</li> <li><code>-keyout</code>: Specifies the name of the automatically generated secret key.</li> </ul> </li> <li> <p>Embed the Explorer page by using iframe on a third-party page. The work needs to be developed by yourself.</p> </li> <li> <p>On the parent page, pass the login message through the postMessage method in the following format:</p> <pre><code>{ type: 'NebulaGraphExploreLogin', \n  data: { \n    authorization: 'WyJyb290IiwibmVidWxhIl0=', \n    host: '192.168.8.240:9669', \n    space: 'basketballplayer' \n    } }\n</code></pre> <ul> <li>type: The method type must be <code>NebulaGraphExploreLogin</code>.</li> <li>data\uff1a<ul> <li><code>authorization</code>: NebulaGraph accounts and passwords were formed into an array and serialized, then Base64 encoded. The array format is <code>['account', 'password']</code>. The example is['root', 'nebula']. The encoded result is <code>WyJyb290IiwibmVidWxhIl0=</code>.</li> <li><code>host</code>: The graph service address of NebulaGraph.</li> <li><code>space</code>: The name of the target graph space.</li> </ul> </li> </ul> </li> <li> <p>Start the Explorer service.</p> <p>Note</p> <p>If the Explorer is installed by RPM/DEB package, run the command <code>sudo ./nebula-explorer-server &amp;</code>\u3002</p> <pre><code>./scripts/start.sh\n</code></pre> </li> <li> <p>Check whether the embedded Explorer page is displayed on the third-party page. For example, the first page displays the graph space <code>basketballplayer</code>, and the second and third pages display other graph spaces.</p> <p></p> </li> </ol>"},{"location":"nebula-explorer/system-settings/","title":"System settings","text":"<p>This topic introduces the system settings of NebulaGraph Explorer, including global settings and custom settings.</p>"},{"location":"nebula-explorer/system-settings/#global_settings","title":"Global settings","text":"<p>Global settings include the language settings, beta functions, and canvas query limit.</p> <p>Language settings: switch the interface language, supporting Chinese and English.</p> <p>Beta functions: switch on/off for beta features. Currently, beta features include workflow and view schema.</p> <p>Canvas query limit: the maximum number of query results for vertices and edges displayed on the canvas.</p> <p>!!! note</p> <pre><code>    It is recommended to limit the number of vertices and edges displayed on the canvas to no more than 5000 in 2D mode, otherwise, it may stuck when rendering.\n</code></pre>"},{"location":"nebula-explorer/system-settings/#custom_settings","title":"Custom settings","text":"<p>Custom settings include the product logo, login page logo, and product name settings.</p> <p>Product logo: supports uploading an image as the logo on the top left corner of the main page. Login page logo: supports uploading an image as the logo on the login page. Product name: supports modifying the product name displayed on the login page.</p>"},{"location":"nebula-explorer/about-explorer/ex-ug-what-is-explorer/","title":"What is NebulaGraph Explorer","text":"<p>NebulaGraph Explorer (Explorer in short) is a browser-based visualization tool. It is used with the NebulaGraph core to visualize interaction with graph data. Even if there is no experience in graph database, you can quickly become a graph exploration expert.</p> <p>Enterpriseonly</p> <ul> <li>To purchase the NebulaGraph Explorer, contact us. </li> <li>New users can apply for a 30-day trial. You can also try some functions online in Explorer.</li> </ul> <p>Note</p>"},{"location":"nebula-explorer/about-explorer/ex-ug-what-is-explorer/#scenarios","title":"Scenarios","text":"<p>You can use Explorer in one of these scenarios:</p> <ul> <li>You need to quickly find neighbor relationships from complex relationships, analyze suspicious targets, and display graph data in a visual manner.</li> <li>For large-scale data sets, the data needs to be filtered, analyzed, and explored in a visual manner.</li> </ul>"},{"location":"nebula-explorer/about-explorer/ex-ug-what-is-explorer/#features","title":"Features","text":"<p>Explorer has these features:</p> <ul> <li>Easy to use: Explorer can be deployed in simple steps.</li> </ul> <ul> <li>User-friendly: Explorer uses simple visual interaction, no need to conceive nGQL sentences, easy to realize graph exploration.</li> </ul> <ul> <li>Flexible: Explorer supports querying data through VID, Tag, and Subgraph.</li> </ul> <ul> <li>Exploration operations: Explorer supports exploration operations on multiple vertices, querying the common neighbors of multiple vertices, and querying the path between the source vertex and the destination vertex.</li> </ul> <ul> <li>Various display: Explorer supports modifying the color and icon of the vertex in the canvas to highlight key nodes. Data can also be displayed in different modes.</li> </ul> <ul> <li>Data storage: Data on a canvas can be stored and exported.</li> </ul> <ul> <li>Inline frame: Explorer supports embedding the canvas on third-party pages.</li> </ul>"},{"location":"nebula-explorer/about-explorer/ex-ug-what-is-explorer/#authentication","title":"Authentication","text":"<p>Authentication is not enabled in NebulaGraph by default. Users can log into Studio with the <code>root</code> account and any password.</p> <p>When NebulaGraph enables authentication, users can only sign into Studio with the specified account. For more information, see Authentication.</p>"},{"location":"nebula-explorer/about-explorer/ex-ug-what-is-explorer/#version_compatibility","title":"Version compatibility","text":"<p>Note</p> <p>Explorer is released separately, not synchronized with NebulaGraph. And the version naming of Explorer is different from that of NebulaGraph. The version correspondence between NebulaGraph and Explorer is as follows.</p> NebulaGraph version Explorer version 3.5.0 3.5.1\u30013.5.0\u30013.4.0 3.4.0 ~ 3.4.1 3.5.1\u30013.5.0\u30013.4.0\u30013.2.1\u30013.2.0 3.3.0 3.2.1, 3.2.0 3.1.0 ~ 3.2.x 3.1.0 3.0.0 ~ 3.1.0 3.0.0 2.5.x ~ 3.0.0 2.2.0 2.6.x 2.1.0 2.5.x 2.0.0"},{"location":"nebula-explorer/about-explorer/ex-ug-what-is-explorer/#video","title":"Video","text":"<ul> <li>NebulaGraph Explorer Intro Demo(5 minutes 22 seconds)</li> </ul>"},{"location":"nebula-explorer/canvas-operations/canvas-overview/","title":"Canvas overview","text":"<p>You can visually explore data on a canvas. This topic introduces the composition of a canvas and its related functions.</p> <p>Canvas overview diagram:</p> <p></p>"},{"location":"nebula-explorer/canvas-operations/canvas-overview/#tabs_on_the_top","title":"Tabs on the Top","text":"<p>Click the plus sign  to add a new canvas. You can have operations on multiple canvases simultaneously.</p> <p></p> <ul> <li>Canvas data on different canvases can come from the same graph space or from different graph spaces.</li> <li>You can customize the name of a canvas except for the canvas in the left-most tab.</li> </ul>"},{"location":"nebula-explorer/canvas-operations/canvas-overview/#visualization_modes","title":"Visualization modes","text":"<p>Graph data can be visually explored in 2D mode and 3D mode. For more information, Visualization modes.</p>"},{"location":"nebula-explorer/canvas-operations/canvas-overview/#data_storage","title":"Data storage","text":"<p>Graph data on the current canvas can be stored by creating snapshots or exporting canvas data as images or CSV files.</p> <p>At the top right of the page, you can:</p> <ul> <li>Click  to create a snapshot. For more information, see Canvas snapshots.</li> </ul> <ul> <li>Click  and then click Export CSV File to store canvas data as CSV files.</li> </ul> <ul> <li>Click  and then click Export PNG File to store canvas data as images.</li> </ul>"},{"location":"nebula-explorer/canvas-operations/canvas-overview/#search_box","title":"Search box","text":"<p>In the search box at the top left of the page, click  and enter a VID  or the property values of tags to locate target vertices.</p>"},{"location":"nebula-explorer/canvas-operations/canvas-overview/#layouts","title":"Layouts","text":"<p>Explorer provides 6 layouts to show the relationship between the data on a canvas.</p> Force Dagre Circular Grid Neural Network Radial <p></p>"},{"location":"nebula-explorer/canvas-operations/canvas-overview/#minimap","title":"Minimap","text":"<p>You can display the vertices on a canvas on full screen. You can also collapse the minimap, zoom in or zoom out the canvass, etc. The percentage of a canvas graph to the total is displayed in the lower-left corner of the minimap.</p> <p></p>"},{"location":"nebula-explorer/canvas-operations/canvas-overview/#data_overview","title":"Data overview","text":"<p>On the right side of the page, click  to expand the data overview panel.</p> <p></p> <p>On the data overview panel, you are enabled to:</p> <ul> <li>See the number of tags and edge types, and the number of the corresponding vertices and edges on a canvas.  </li> <li> <p>Click the color icon of the tags or edge types to customize the color and size. You can also customize the icons and images of the tags.</p> <p>Note</p> <p>You can only change colors in batches in the data overview panel. Right-click a single vertex on a canvas to manually modify the style of the vertex.</p> </li> </ul> <ul> <li> <p>Upload images to personalize the style of the vertices in the canvas, and the uploaded images are stored in the browser. To store uploaded images permanently, save the canvas data as a snapshot. For details, see Manage snapshots.</p> <p></p> </li> </ul> <p>Select vertices and edges on the canvas, and then click Selected Vertices {number} Selected Edges {number} in the lower left corner to view the detailed information of the vertices and edges. You can export the data as a CSV file. </p> <p></p>"},{"location":"nebula-explorer/canvas-operations/canvas-snapshot/","title":"Canvas snapshots","text":"<p>Explorer provides a snapshot feature that lets you store the visualized canvas data so that the data can be restored when your browser is opened again.</p>"},{"location":"nebula-explorer/canvas-operations/canvas-snapshot/#create_snapshots","title":"Create snapshots","text":"<ol> <li>In the upper right corner of a canvas page, click the camera icon .</li> <li>Fill in the snapshot name and notes (optional).</li> <li>Click submit.</li> </ol> <p>Note</p> <p>Created snapshots are stored on the snapshot list page. For more information, see below.</p>"},{"location":"nebula-explorer/canvas-operations/canvas-snapshot/#historical_snapshots","title":"Historical snapshots","text":"<p>Note</p> <p>Up to 50 snapshots can be stored in the snapshot list currently.</p> <p>In the left navigation bar of the Explorer page, click  to enter the Snapshot page. You can switch graph spaces and view the historical snapshots of the corresponding graph space. You can also import snapshots to a canvas, download canvas snapshots to your local drive, and delete snapshots.</p> <p>Under the Operation column to the right of the target snapshot, you are enabled to: </p> <ul> <li>Click  to import a historical snapshot to a new canvas.</li> <li>Click  to download a snapshot in JSON format locally.</li> <li>Click  to delete a snapshot.</li> </ul> <p>At the top left of the Snapshot page, click Import Snapshot to import previously downloaded files in JSON format to the Snapshot page for sharing the snapshot data offline. The system automatically places the imported snapshots in the corresponding graph space based on the graph space information recorded in the JSON file.</p>"},{"location":"nebula-explorer/canvas-operations/visualization-mode/","title":"Visualization modes","text":"<p>Explorer provides 2D and 3D visualization modes for you to explore data. 2D enables you to operate on graph data and view data information. 3D lets you explore graph data from a different perspective. The 3D is suitable for cases with a large amount of data or situations requiring presentations.</p> <p>Note</p> <p>In 3D mode, operations on graph data are unavailable. </p> <p></p>"},{"location":"nebula-explorer/canvas-operations/visualization-mode/#2d_mode","title":"2D mode","text":"<p>Exploration of the data on a canvas is possible in 2D mode.</p> <p></p> Parameter Description Auto Weight Degree\uff1aAutomatically resizes vertices according to the number of outgoing and incoming edges of all the vertices on the canvas.Reset Degree\uff1aResets the vertices on the canvas to their original size. Edge Aggregation: Automatically aggregate all edges on the canvas that match the aggregation rules. Edge Disaggregate: Resets the aggregated edges on the canvas. Detection Outlier: Detects the vertices that connect no edges on a canvas.Hang Edge: Detects edges associated with vertices of one degree in the canvas (associated vertices are included).Loop Detection: Detects the paths that connect a vertex to itself. N-Step Vertex Detection: Starting from the selected vertex, the vertices in the outbound direction are displayed on the canvas hop by hop. Aggregation Aggregate the edges between the vertices: Aggregate the edges between the selected vertices on the canvas.Cancels aggregation of edges between vertices: Resets the aggregated edges between the selected vertices on the canvas. Edit Dismiss: Hide the selected vertices and edges on the canvas.Dismiss Others: Hide the unselected vertices and edges on the canvas.Undo: Undo the action in the previous step.Redo: Restore the action that was previously undone. <p>For more information about the operations available in 2D mode, see Canvas.</p>"},{"location":"nebula-explorer/canvas-operations/visualization-mode/#edge_aggregation_description","title":"Edge aggregation description","text":"<p>When there are a large number of vertices in the canvas, to enhance the readability and analyzability of the graph, edges with the same start vertex, end vertex and edge type can be aggregated to make the relationship between vertices clearer.</p> <ul> <li>Edge aggregation automatically displays the number of aggregated edges.</li> <li>Edge aggregation supports the calculation of properties in it. For details, see Property calculation.</li> <li>Hovering over the aggregated edge displays the edge type, the number of aggregated edges, edge properties, and property values. If the property calculation was performed, the result is also displayed.</li> <li>In addition to canceling edge aggregation in the upper bar, you can also double-click the aggregated edge or right-click the aggregated edge and select disaggregate.</li> </ul>"},{"location":"nebula-explorer/canvas-operations/visualization-mode/#3d_mode","title":"3D mode","text":"<p>At the top left of the page, toggle the view button to switch to 3D mode. 3D mode allows you to switch back to 2D mode and does not influence operations in 2D.</p> Parameter Description Bird View Shows a bird view of all the data in the current graph space. By default, displays data for up to 20,000 vertices and 2,000 edges in the current graph space. Click  to adjust the settings, but setting them too large may crash the browser. Image Quality High: Vertices are displayed in the form of balls with better light and shadow effects.Normal: Vertices are represented in a circle format and support a large amount of data. Reheat Disperses the distance between vertices when the vertices overlap. <p>Legacy version compatibility</p> <p>For versions of NebulaGraph below 3.0.0, you need to create an index before using the Bird View feature. For more information, see Create an index.</p>"},{"location":"nebula-explorer/db-management/10.create-schema/","title":"Create a schema","text":"<p>Explorer allows you to create a schema by using GUI.</p> <p>Note</p> <ul> <li>Users can use the Schema drafting function to design schema visually. For more information, see Schema drafting.</li> <li>Users can directly execute nGQL commands on the console to manage the schema.</li> </ul>"},{"location":"nebula-explorer/db-management/10.create-schema/#prerequisites","title":"Prerequisites","text":"<ul> <li>Your account has the privilege of GOD, ADMIN, or DBA. For more information, see Roles and privileges .</li> </ul> <ul> <li>The schema is designed.</li> </ul> <p>Note</p> <p>If no graph space exists and your account has the GOD privilege, you can create a graph space on the Console page.</p>"},{"location":"nebula-explorer/db-management/10.create-schema/#entry","title":"Entry","text":"<p>At the top navigation bar, click .</p>"},{"location":"nebula-explorer/db-management/10.create-schema/#create_graph_space","title":"Create graph space","text":"<ol> <li>Click Create Space.</li> <li>Set parameters. For descriptions of the parameters, see CREATE SPACE.</li> <li>Click Create.</li> </ol>"},{"location":"nebula-explorer/db-management/10.create-schema/#create_tag_or_edge_type","title":"Create Tag or Edge type","text":"<ol> <li>In the Graph Space List page, find a graph space and then click its name or click Schema in the Operations column.</li> <li>Click the Tag or Edge Type tab and click the Create button.</li> <li> <p>Set parameters. For descriptions of the parameters, see CREATE TAG or CREATE EDGE.</p> <p>Note</p> <p>If no index is set for the tag, you can set the TTL configuration. For more information, see TTL configuration.</p> </li> <li> <p>Click Create.</p> </li> </ol> <p>In the Tag and Edge Type lists, you can perform modification and deletion operations.</p>"},{"location":"nebula-explorer/db-management/10.create-schema/#create_index","title":"Create index","text":"<p>Note</p> <ul> <li>Before creating an index, ensure that the associated Tag or Edge type has been created.</li> <li>The index can decrease the write speed during data import. We recommend that you import data first and then create and rebuild an index. For more information, see Index overview.</li> </ul> <ol> <li>In the Graph Space List page, find a graph space and then click its name or click Schema in the Operations column.</li> <li>Click the Index tab and click the Create button.</li> <li> <p>Set parameters. For descriptions of the parameters, see CREATE INDEX.</p> <p>Note</p> <p>The order of the indexed properties has an effect on the result of the <code>LOOKUP</code> statement. For more information, see LOOKUP.</p> </li> <li> <p>Click Create.</p> </li> </ol> <p>In the Index list, you can rebuild or delete the index.</p>"},{"location":"nebula-explorer/db-management/10.create-schema/#view_statistics","title":"View statistics","text":"<ol> <li>In the Graph Space List page, find a graph space and then click its name or click Schema in the Operations column.</li> <li>Click Statistics tab and click the Refresh button.</li> </ol>"},{"location":"nebula-explorer/db-management/10.create-schema/#view_schema","title":"View schema","text":"<p>Note</p> <p>To display this function, you need to enable View Schema in .</p> <ol> <li>In the Graph Space List page, find a graph space and then click its name or click Schema in the Operations column.</li> <li>Click View Schema tab and click the Refresh button.</li> </ol>"},{"location":"nebula-explorer/db-management/10.create-schema/#other_operations","title":"Other operations","text":"<p>In the Graph Space List page, find a graph space and then perform the following operations in the Operations column:</p> <ul> <li>View Schema DDL: Displays schema creation statements for the graph space, including graph spaces, tags, edge types, and indexes.</li> <li>Clone Graph Space: Clone the schema of the graph space to a new graph space.</li> <li>Delete Graph pace: Delete the graph space, including the schema and all vertices and edges.</li> </ul>"},{"location":"nebula-explorer/db-management/11.import-data/","title":"Import data","text":"<p>Explorer allows you to import data in CSV format into NebulaGraph using GUI.</p>"},{"location":"nebula-explorer/db-management/11.import-data/#prerequisites","title":"Prerequisites","text":"<ul> <li>CSV files meet the demands of the Schema.</li> </ul> <ul> <li>Your account has the privilege of GOD, ADMIN, DBA, or USER. For more information, see Roles and privileges .</li> </ul>"},{"location":"nebula-explorer/db-management/11.import-data/#entry","title":"Entry","text":"<p>At the top navigation bar, click .</p>"},{"location":"nebula-explorer/db-management/11.import-data/#steps","title":"Steps","text":""},{"location":"nebula-explorer/db-management/11.import-data/#upload_files","title":"Upload files","text":"<ol> <li> <p>In the Upload Files tab, click the Upload Files button and then choose CSV files.</p> <p>Note</p> <p>You can choose multiple CSV files at the same time.</p> </li> <li> <p>After uploading, you can click the  button in the Operations column to preview the file content.</p> </li> </ol>"},{"location":"nebula-explorer/db-management/11.import-data/#import_data_1","title":"Import Data","text":"<ol> <li> <p>In the Import Data tab, click + New Import button to complete these operations:</p> <p>Caution</p> <p>users can click Import Template to download the example configuration file <code>example.yaml</code>, and upload the configuration file after configuration. The configuration mode is similar to that of NebulaGraph Importer, but all file paths for configuration files in the template retain the filename only. And make sure all CSV data files are uploaded before importing the YAML file.</p> <ul> <li>Space: The name of the graph space that you want to import data from.</li> <li>Task Name: Automatically generated by default and can be modified.</li> <li>Batch Size (Optional): The number of rows of imported data per batch.</li> <li> <p>Map Vertices: Click the Bind Datasource button, then select bind source file in the dialog box, and click the Confirm button.</p> <ul> <li>In the vertexID item of the vertices 1 drop-down list, click Select CSV Index, and select the data source for VID in the pop-up dialog box.</li> <li>Click the Add Tag button, then click Select Tag in the newly created tab and select the Tag you want to associate. In the property list, select the data source for the property.</li> </ul> </li> </ul> <ul> <li> <p>Map Edges: Similar to the operation of the Map Vertices.</p> <p></p> </li> </ul> </li> <li> <p>After completing the settings, click the Import button and enter the password of your NebulaGraph account.</p> </li> </ol> <p>On the Import Data tab, you can view logs, download logs, download configuration files, and delete tasks.</p>"},{"location":"nebula-explorer/db-management/dbuser_management/","title":"Database user management","text":"<p>NebulaGraph Explorer supports managing the users in the NebulaGraph database, including creating users, deleting users, changing passwords, etc.</p>"},{"location":"nebula-explorer/db-management/dbuser_management/#prerequisites","title":"Prerequisites","text":"<p>The user who logs in to Explorer must have permissions for related operations. For example, users with <code>God</code> permission can perform all operations, and users with <code>Admin</code> permission can authorize the permission of a graph space within their permission to other users. For details about role privileges, see Roles and privileges.</p>"},{"location":"nebula-explorer/db-management/dbuser_management/#entry","title":"Entry","text":"<p>At the top navigation bar, click  .</p> <p></p>"},{"location":"nebula-explorer/db-management/dbuser_management/#create_user","title":"Create user","text":"<p>Note</p> <p>Only the <code>root</code> user can create users.</p> <ol> <li> <p>In the tab User list, click Create User and set the following parameters.</p> Parameters Description Account The user name. Password The password corresponding to the user name. IP Whitelist The user can connect to NebulaGraph only from IP addresses in the list. Use commas to separate multiple IP addresses. Only NebulaGraph Enterprise Edition supports the parameter. <p>Note</p> <p>Click Add in the upper left corner to create users in batches.</p> </li> <li> <p>Click Confirm.</p> </li> </ol>"},{"location":"nebula-explorer/db-management/dbuser_management/#authorize_user","title":"Authorize user","text":"<ol> <li> <p>Switch the tab to Authorization, and select the name of the graph space that you want to authorize to a user in the upper left corner. The page shows all users (except <code>root</code> user) who have permission on the graph space.</p> </li> <li> <p>Click Grant Role and set the following parameters.</p> Parameters Description Username Set the user name to be authorized. If you log in as the <code>root</code> user, select the user from the drop-down menu. If you log in with the <code>Admin</code> permission, fill in the user name manually. Role Select the role to be authorized from the drop-down menu. For details about role privileges, see Roles and privileges. </li> <li> <p>Click Confirm.</p> </li> </ol>"},{"location":"nebula-explorer/db-management/dbuser_management/#other_operations_in_the_user_list","title":"Other operations in the user list","text":"<p>Note</p> <p>Only the <code>root</code> user can view the User List.</p> <ul> <li>View: View the user permissions in each space.</li> <li>Edit: Change the password and IP whitelist of the user. You do not need to provide the old password when changing the password. If the user is not <code>root</code>, you can change the password in  on the upper right corner of the page.</li> <li>Delete User: Only the <code>root</code> user can delete other users.</li> <li>Search user: Search for the account by keyword.</li> </ul>"},{"location":"nebula-explorer/db-management/dbuser_management/#other_operations_in_the_authorization","title":"Other operations in the authorization","text":"<ul> <li>Edit: Change the role of the user.</li> <li>Revoke Role: Revoke the role of the user.</li> <li>Search user: Search for the account by keyword.</li> </ul> <p>Note</p> <p>After a user is modified or revoked, the modification takes effect only after the user logs in next time.</p>"},{"location":"nebula-explorer/db-management/draft/","title":"Schema drafting","text":"<p>Explorer supports the schema drafting function. Users can design their schemas on the canvas to visually display the relationships between vertices and edges, and apply the schema to a specified graph space after the design is completed.</p>"},{"location":"nebula-explorer/db-management/draft/#features","title":"Features","text":"<ul> <li>Design schema visually.</li> <li>Applies schema to a specified graph space.</li> <li>Export the schema as a PNG image.</li> </ul>"},{"location":"nebula-explorer/db-management/draft/#entry","title":"Entry","text":"<p>At the top navigation bar, click  .</p>"},{"location":"nebula-explorer/db-management/draft/#design_schema","title":"Design schema","text":"<p>The following steps take designing the schema of the <code>basketballplayer</code> dataset as an example to demonstrate how to use the schema drafting function.</p> <ol> <li>At the upper left corner of the page, click New.</li> <li>Create a tag by selecting the appropriate color tag under the canvas. You can hold down the left button and drag the tag into the canvas.</li> <li>Click the tag. On the right side of the page, you need to fill in the name of the tag as <code>player</code>, and add two properties <code>name</code> and <code>age</code>.</li> <li>Create a tag again. The name of the tag is <code>team</code>, and the property is <code>name</code>.</li> <li>Connect from the anchor point of the tag <code>player</code> to the anchor point of the tag <code>team</code>. Click the generated edge, fill in the name of the edge type as <code>serve</code>, and add two properties <code>start_year</code> and <code>end_year</code>.</li> <li>Connect from an anchor point of the tag <code>player</code> to another one of its own. Click the generated edge, fill in the name of the edge type as <code>follow</code>, and add a property <code>degree</code>.</li> <li>After the design is complete, click  at the top of the page to change the name of the draft, and then click  at the top right corner to save the draft.</li> </ol> <p></p>"},{"location":"nebula-explorer/db-management/draft/#apply_schema","title":"Apply schema","text":"<ol> <li>Select the draft that you want to import from the Draft list on the left side of the page, and then click Apply to Space at the upper right corner.</li> <li> <p>Import the schema to a new or existing space, and click Confirm.</p> <p>Note</p> <ul> <li>For more information about the parameters for creating a graph space, see CREATE SPACE.</li> <li>If the same schema exists in the graph space, the import operation fails, and the system prompts you to modify the name or change the graph space.</li> </ul> </li> </ol>"},{"location":"nebula-explorer/db-management/draft/#modify_schema","title":"Modify schema","text":"<p>Select the schema draft that you want to modify from the Draft list on the left side of the page. Click  at the upper right corner after the modification.</p> <p>Note</p> <p>The graph space to which the schema has been applied will not be modified synchronously.</p>"},{"location":"nebula-explorer/db-management/draft/#delete_schema","title":"Delete schema","text":"<p>Select the schema draft that you want to delete from the Draft list on the left side of the page, click X at the upper right corner of the thumbnail, and confirm to delete it.</p>"},{"location":"nebula-explorer/db-management/draft/#export_schema","title":"Export Schema","text":"<p>Click  at the upper right corner to export the schema as a PNG image.</p>"},{"location":"nebula-explorer/db-management/explorer-console/","title":"Explorer console","text":"<p>Explorer console allows you to enter nGQL statements and visualize the query results. This topic describes the console page.</p>"},{"location":"nebula-explorer/db-management/explorer-console/#entry","title":"Entry","text":"<p>At the top navigation bar, click  .</p>"},{"location":"nebula-explorer/db-management/explorer-console/#overview","title":"Overview","text":"<p>The following table lists the functions on the console page.</p> number function descriptions 1 select a space Select a space in the Current Graph Space list. The <code>USE &lt;space_name&gt;</code> statement in the console is not supported to switch graph spaces. 2 favorites Click the  button to expand the favorites, click one of the statements, and the input box will automatically enter the statement. 3 history list Click  button representing the statement record. In the statement running record list, click one of the statements, and the statement will be automatically entered in the input box. The list provides the record of the last 15 statements. 4 clean input box Click  button to clear the content entered in the input box. 5 run After inputting the nGQL statement in the input box, click  button to indicate the operation to start running the statement. 6 save as template Save the nGQL statement entered in the input box as a template. For details, see nGQL template. 7 input box After inputting the nGQL statements, click the  button to run the statement. You can input multiple statements and run them at the same time by using the separator <code>;</code>, and also use the symbol <code>//</code> to add comments. 8 custom parameters display Click the  button to expand the custom parameters for parameterized query. For details, see Manage parameters. 9 statement running status After running the nGQL statement, the statement running status is displayed. If the statement runs successfully, the statement is displayed in green. If the statement fails, the statement is displayed in red. 10 add to favorites Click the  button to save the statement as a favorite, the button for the favorite statement is colored in yellow exhibit. 11 export CSV file or PNG file After running the nGQL statement to return the result, when the result is in Table window, click the  button to export as a CSV file. Switch to the Graph window and click the  button to save the results as a CSV file or PNG image export. 12 expand/hide execution results Click the  button to hide the result or click  button to expand the result. 13 close execution results Click the  button to close the result returned by this nGQL statement. 14 Table window Display the result from running nGQL statement. If the statement returns results, the window displays the results in a table. 15 Graph window Display the result from running nGQL statement. If the statement returns the complete vertex-edge result, the window displays the result as a graph . Click the  button on the right to view the overview panel."},{"location":"nebula-explorer/db-management/ngql-template/","title":"nGQL template","text":"<p>NebulaGraph Explorer supports saving the commonly nGQL statement as a template for yourself or others. The text in the nGQL statement supports parameterization, and parameter values can be filled in as needed.</p>"},{"location":"nebula-explorer/db-management/ngql-template/#prerequisites","title":"Prerequisites","text":"<p>The schema has been created in the NebulaGraph database.</p>"},{"location":"nebula-explorer/db-management/ngql-template/#entry","title":"Entry","text":"<p>At the top navigation bar, click  .</p>"},{"location":"nebula-explorer/db-management/ngql-template/#create_new_template","title":"Create new template","text":"<ol> <li> <p>Click + New Template, and set the parameters as follows.</p> <p></p> Parameter Example Description Template name <code>test</code> The name of the template. Space <code>basketballplayer</code> The graph space to which the template applies. Description <code>Returns the neighbor name of the specified player</code> Describes the function of the template. Query template <code>MATCH (v:player{name:\"${name}\"})--(v2:player) RETURN v2.player.name AS Name;</code> nGQL template. You can select the text you want to parameterize, click + parameterize selected content on the right, and set the parameter name and description. In the example, <code>${name}</code> is parameterized text. In actual use, you can fill in a name such as <code>Tim Duncan</code>.You can add comments in a single line using <code>//</code>. Input - Displays parameterized text content. You can edit or delete it. <p>Note</p> <p>Click + Save as template on the upper left corner of the console page to use the entered query statement as a template statement automatically.</p> </li> <li> <p>Click Save as template.</p> </li> </ol>"},{"location":"nebula-explorer/db-management/ngql-template/#other_operations","title":"Other Operations","text":"<ul> <li>Click  on the right of the target template to modify the template context.</li> <li>Click  on the right of the target template to automatically jump to the console and enter the template.</li> <li>Click  on the right of the target template to delete the template.</li> <li>The filter box in the upper right corner allows you to filter templates for a specified graph space.</li> <li>The search box in the upper right corner allows you to search the template name.</li> </ul>"},{"location":"nebula-explorer/db-management/ngql-template/#use_template","title":"Use template","text":"<ul> <li>(Recommended) Use templates on the graph exploration page. For details, see Start querying.</li> </ul> <ul> <li>Click  on the template list page to automatically jump to the console and enter the template. You need to modify the parameterized text.</li> </ul>"},{"location":"nebula-explorer/deploy-connect/ex-ug-connect/","title":"Connect to NebulaGraph","text":"<p>After successfully launching Explorer, you need to configure to connect to NebulaGraph. You can connect directly to NebulaGraph by default. To ensure data security, OAuth2.0 authentication is also supported. You can connect to NebulaGraph only after the authentication is passed.</p>"},{"location":"nebula-explorer/deploy-connect/ex-ug-connect/#prerequisites","title":"Prerequisites","text":"<p>Before connecting to the NebulaGraph database, you need to confirm the following information:</p> <ul> <li>The NebulaGraph services and Explorer are started. For more information, see Deploy Explorer.</li> </ul> <ul> <li>You have the local IP address and the port used by the Graph service of NebulaGraph. The default port is <code>9669</code>.</li> </ul> <ul> <li>You have a NebulaGraph account and its password.</li> </ul> <ul> <li>We recommend you to use the Chrome browser of the version above 89. Otherwise, there may be compatibility issues.</li> </ul>"},{"location":"nebula-explorer/deploy-connect/ex-ug-connect/#oauth20_configuration","title":"OAuth2.0 Configuration","text":"<p>Caution</p> <p>The feature is still in beta. It will continue to be optimized.</p> <p>Note</p> <p>If you want to connect directly to NebulaGraph, see Procedure below.</p> <p>To enable OAuth2.0 authentication, modify the configuration file in the Explorer installation directory. The path is <code>config/app-config.yaml</code>.</p> <p>The descriptions of the OAuth configuration are as follows.</p> Parameter Example Description <code>Enable</code> <code>false</code> Enable or disable OAuth2.0 authentication. <code>ClientID</code> <code>4953xxx-mmnoge13xx.apps.googleusercontent.com</code> The application's ClientId. <code>ClientSecret</code> <code>GOCxxx-xaytomFexxx</code> The application's ClientSecret. <code>RedirectURL</code> <code>http://dashboard.vesoft-inc.com/login</code> The URL that redirects to Dashboard. <code>AuthURL</code> <code>https://accounts.google.com/o/oauth2/auth</code> The URL used for authentication. <code>TokenURL</code> <code>https://oauth2.googleapis.com/token</code> The URL used to get the access_token. <code>UserInfoURL</code> <code>https://www.googleapis.com/oauth2/v1/userinfo</code> The URL used to get the user information. <code>UsernameKey</code> <code>email</code> The key of the user name. <code>Organization</code> <code>vesoft company</code> The organization name. <code>TokenName</code> <code>oauth_token</code> The name of the token in the cookie. <code>Scope</code> <code>email</code> Scope of OAuth permissions. The scope of permissions needs to be a subset of the scope configured by the vendor's OAuth2.0 platform, otherwise, the request will fail. Make sure the <code>UsernameKey</code> is accessible within the requested scope. <code>AvatarKey</code> <code>picture</code> The key of the avatar in the user information. <p>After the configuration is complete, restart the Explorer service. The OAuth authentication is displayed on the login page. You can continue to connect to NebulaGraph only after the authentication is passed.</p>"},{"location":"nebula-explorer/deploy-connect/ex-ug-connect/#procedure","title":"Procedure","text":"<p>To connect Explorer to NebulaGraph, follow these steps:</p> <ol> <li> <p>Type <code>http://&lt;ip_address&gt;:7002</code> in the address bar of your browser.</p> <p>The following login page shows that Explorer is successfully connected to NebulaGraph.</p> <p></p> <p>Note</p> <p>When logging into NebulaGraph Explorer for the first time, the content of END USER LICENSE AGREEMENT is displayed on the login page. Please read it and then click I agree.</p> </li> <li> <p>On the Config Server page of Explorer, configure these fields:</p> <ul> <li> <p>Graphd IP address: Enter the IP address of the Graph service of NebulaGraph. For example, <code>192.168.10.100</code>.</p> <p>Note</p> <ul> <li>When NebulaGraph and Explorer are deployed on the same machine, you must enter the IP address of the machine, instead of <code>127.0.0.1</code> or <code>localhost</code>.</li> <li>When connecting a NebulaGraph database on a new tab, The new session will overwrite the sessions of the old TAB. If you need to log in to multiple NebulaGraph databases at the same time, you can use different browsers or non-trace mode.</li> </ul> </li> </ul> <ul> <li>Port: The port of the Graph service. The default port is <code>9669</code>.</li> </ul> <ul> <li> <p>Username and Password: Fill in the log in account according to the authentication settings of NebulaGraph.</p> <ul> <li>If authentication is not enabled, you can use <code>root</code> and any password as the username and its password.</li> </ul> <ul> <li>If authentication is enabled and no account information has been created, you can only log in as GOD role and use <code>root</code> and <code>nebula</code> as the username and its password.</li> </ul> <ul> <li>If authentication is enabled and different users are created and assigned roles, users in different roles log in with their accounts and passwords.</li> </ul> </li> </ul> </li> <li> <p>After the configuration, click the Login button.</p> <p>Note</p> <p>One session continues for up to 30 minutes. If you do not operate Explorer within 30 minutes, the active session will time out and you must connect to NebulaGraph again.</p> </li> </ol> <p>A welcome page is displayed on the first login, showing the relevant functions according to the usage process, and the test datasets can be automatically downloaded and imported.</p> <p>To visit the welcome page, click  -&gt; Beginner's Guide.</p>"},{"location":"nebula-explorer/deploy-connect/ex-ug-connect/#clear_connection","title":"Clear connection","text":"<p>When Explorer is still connected to a NebulaGraph database, on the upper right corner of the page, select  &gt; Clear Connect.</p> <p>After that, if the configuration database page is displayed on the browser, it means that Explorer has successfully disconnected from the NebulaGraph.</p>"},{"location":"nebula-explorer/deploy-connect/ex-ug-deploy/","title":"Deploy Explorer","text":"<p>This topic describes how to deploy Explorer locally by RPM, DEB, and tar packages.</p>"},{"location":"nebula-explorer/deploy-connect/ex-ug-deploy/#prerequisites","title":"Prerequisites","text":"<p>Before deploying Explorer, you must check the following information:</p> <ul> <li>The license key is loaded.</li> </ul> <ul> <li>The NebulaGraph services are deployed and started. For more information, see NebulaGraph Database Manual.</li> </ul> <ul> <li> <p>Before the installation starts, the following ports are not occupied.</p> Port Description 7002 Web service provided by Explorer <p>Caution</p> <p>By default, Explorer uses the port <code>7002</code>. You can modify the <code>httpport</code> in the <code>conf/app.conf</code> file in the installation directory and restart the service.</p> </li> </ul> <ul> <li>The Linux distribution is CentOS.</li> </ul>"},{"location":"nebula-explorer/deploy-connect/ex-ug-deploy/#precautions","title":"Precautions","text":"<p>The Dag Controller installation package is built in Explorer starting from version 3.2.0, which provides graph computing services. The user can decide whether or not to start the Dag Controller service. If the Dag Controller service is not started, the Workflow menu in Explorer will appear gray and cannot be clicked.</p> <p>In addition, if you need to use Workflow for complex graph computing, you need to configure NFS or HDFS after installing Explorer. Namenode uses port 8020 by default, and datanode uses port 50010 by default. For details, see Prepare resources of Workflow.</p> <p>Enterpriseonly</p> <p>You can apply online for Explorer free trial. NebulaGraph Explorer Enterprise Edition is available exclusively through our Enterprise Edition package and is not sold separately. Contact us for details.</p>"},{"location":"nebula-explorer/deploy-connect/ex-ug-deploy/#rpm-based_deployment","title":"RPM-based deployment","text":""},{"location":"nebula-explorer/deploy-connect/ex-ug-deploy/#installation","title":"Installation","text":"<ol> <li> <p>Select and download the RPM package according to your needs. It is recommended to select the latest version.</p> </li> <li> <p>Use <code>sudo rpm -i &lt;rpm&gt;</code> to install RPM package.</p> <p>For example, use the following command to install Explorer. The default installation path is <code>/usr/local/nebula-explorer</code>.</p> <pre><code>sudo rpm -i nebula-explorer-&lt;version&gt;.x86_64.rpm\n</code></pre> <p>You can also install it to the specified path using the following command:  <pre><code>sudo rpm -i nebula-explorer-&lt;version&gt;.x86_64.rpm --prefix=&lt;path&gt;\n</code></pre></p> </li> <li> <p>Enter the extracted folder, and modify the <code>app-config.yaml</code> file in the <code>config</code> directory, set the value of <code>LicenseManagerURL</code> to the host IP of LM and the port number <code>9119</code>, for example, <code>192.168.8.100:9119</code>.</p> </li> <li> <p>(Optional) Configure the Dag Controller. See the Configure Dag Controller section below.</p> </li> <li> <p>Enter the folder <code>nebula-explorer</code>, and start the service using the following command.</p> <pre><code>cd nebula-explorer\n# Start Explorer.\nsudo ./scripts/start.sh\n# (Optional) Start Dag Controller.\nsudo ./dag-ctrl/scripts/start.sh\n</code></pre> </li> </ol>"},{"location":"nebula-explorer/deploy-connect/ex-ug-deploy/#start_and_stop","title":"Start and stop","text":"<p>You can use SystemCTL to start and stop the service.</p> <pre><code>systemctl status nebula-explorer #Check the status\nsystemctl stop nebula-explorer #Stop the service\nsystemctl start nebula-explorer #Start the service\n</code></pre> <p>You can also start or stop the service manually using the following command in the installation directory.</p> <pre><code>sudo ./scripts/start.sh #Start Explorer\nsudo ./scripts/stop.sh #Stop Explorer\nsudo ./dag-ctrl/scripts/start.sh #Start Dag Controller\nsudo ./dag-ctrl/scripts/stop.sh #Stop Dag Controller\n</code></pre>"},{"location":"nebula-explorer/deploy-connect/ex-ug-deploy/#uninstallation","title":"Uninstallation","text":"<p>You can uninstall Explorer using the following command:</p> <pre><code>sudo rpm -e nebula-graph-explorer-&lt;version&gt;.x86_64\n</code></pre>"},{"location":"nebula-explorer/deploy-connect/ex-ug-deploy/#deb-based_deployment","title":"DEB-based deployment","text":""},{"location":"nebula-explorer/deploy-connect/ex-ug-deploy/#installation_1","title":"Installation","text":"<ol> <li> <p>Select and download the RPM package according to your needs. It is recommended to select the latest version. Common links are as follows:</p> </li> <li> <p>Run <code>sudo dpkg -i &lt;package_name&gt;</code> to unpack the DEB package.</p> <p>For example, run the following command to install Explorer (The default installation path is <code>/usr/local/nebula-explorer</code>).</p> <pre><code>sudo dpkg -i nebula-explorer-3.5.1.x86_64.deb\n</code></pre> <p>Note</p> <p>You cannot customize the installation path of Explorer when installing a DEB package.</p> </li> <li> <p>Enter the extracted folder, and modify the <code>app-config.yaml</code> file in the <code>config</code> directory, set the value of <code>LicenseManagerURL</code> to the host IP of LM and the port number <code>9119</code>, for example, <code>192.168.8.100:9119</code>.</p> </li> <li> <p>(Optional) Configure the Dag Controller. See the Configure Dag Controller section below.</p> </li> <li> <p>Enter the folder <code>nebula-explorer</code>, and start the service using the following command.</p> <p><code>bash  cd nebula-explorer  # Start Explorer.  sudo ./lib/start.sh  # (Optional) Start Dag Controller.  sudo ./dag-ctrl/scripts/start.sh</code></p> </li> </ol>"},{"location":"nebula-explorer/deploy-connect/ex-ug-deploy/#view_the_status","title":"View the status","text":"<pre><code>sudo systemctl status nebula-explorer.service\n</code></pre>"},{"location":"nebula-explorer/deploy-connect/ex-ug-deploy/#stop_the_service","title":"Stop the service","text":"<pre><code>sudo systemctl stop nebula-explorer.service\n</code></pre>"},{"location":"nebula-explorer/deploy-connect/ex-ug-deploy/#uninstallation_1","title":"Uninstallation","text":"<p>Run the following command to uninstall Explorer:</p> <pre><code>sudo dpkg -r nebula-explorer\n</code></pre>"},{"location":"nebula-explorer/deploy-connect/ex-ug-deploy/#tar-based_deployment","title":"TAR-based deployment","text":""},{"location":"nebula-explorer/deploy-connect/ex-ug-deploy/#installation_2","title":"Installation","text":"<ol> <li> <p>Select and download the TAR package according to your needs. It is recommended to select the latest version. Common links are as follows:</p> </li> <li> <p>Use <code>tar -xvf</code> to decompress the TAR package.</p> <pre><code>tar -xvf nebula-explorer-&lt;version&gt;.tar.gz\n</code></pre> </li> <li> <p>Enter the extracted folder, and modify the <code>app-config.yaml</code> file in the <code>config</code> directory, set the value of <code>LicenseManagerURL</code> to the host IP of LM and the port number <code>9119</code>, for example, <code>192.168.8.100:9119</code>.</p> </li> <li> <p>(Optional) Configure the Dag Controller. See the Configure Dag Controller section below.</p> </li> <li> <p>Enter the folder <code>nebula-explorer</code>, and start the service using the following command.</p> <pre><code>cd nebula-explorer\n# Start Explorer and Dag Controller.\nsudo ./scripts/start.sh\n# Start Explorer separately.\nsudo nohup ./nebula-explorer-server &gt; explorer.log 2&gt;&amp;1 &amp;\n</code></pre> </li> </ol>"},{"location":"nebula-explorer/deploy-connect/ex-ug-deploy/#stop_service","title":"Stop Service","text":"<p>You can use <code>kill pid</code> to stop the service.</p> <pre><code>kill $(lsof -t -i :7002)\n</code></pre>"},{"location":"nebula-explorer/deploy-connect/ex-ug-deploy/#configure_dag_controller","title":"Configure Dag Controller","text":"<p>Dag Controller is a task scheduling tool that can schedule the jobs whose type is DAG (directed acyclic graph). The job consists of multiple tasks to form a directed acyclic graph, and there is a dependency between the tasks.</p> <p>The Dag Controller can perform complex graph computing with NebulaGraph Analytics. For example, the Dag Controller sends an algorithm request to NebulaGraph Analytics, which saves the result to NebulaGraph or HDFS. The Dag Controller then takes the result as input to the next algorithmic task to create a new task.</p>"},{"location":"nebula-explorer/deploy-connect/ex-ug-deploy/#steps","title":"Steps","text":"<ol> <li> <p>Complete the SSH password-free configurations so that the Dag Controller machine can log directly into the NebulaGraph Analytics machines and all machines within the NebulaGraph Analytics cluster can connect directly to each other without passwords.</p> <p>For example, the user in machine A (Dag Controller) logs directly into machine B-1 in the NebulaGraph Analytics cluster over SSH  without passwords. Run the following commands on machine A:</p> <pre><code>//Press Enter to execute the default option to generate the key.\nssh-keygen -t rsa\n\n//After the public key file of machine A is installed to the user of the machine B-1, you can log into the machine B-1 from the machine A without passwords.\nssh-copy-id -i ~/.ssh/id_rsa.pub &lt;B_user&gt;@&lt;B_IP&gt;\n</code></pre> <p>In the same way, complete the SSH password-free configurations so that the user in machine A can log directly into machines B-2, B-3, etc. and all machines within the NebulaGraph Analytics cluster can connect directly to each other without passwords.</p> </li> <li> <p>Run <code>eval $(ssh-agent)</code> on the Dag Controller machine to start the ssh-agent, then run <code>ssh-add ~/.ssh/id_rsa</code> to give the private key to the ssh-agent to manage.</p> <p>Note</p> <p>ssh-agent is a key manager that manages multiple keys and provides proxies for other programs that need to use SSH key pairs.</p> </li> <li> <p>Configure the username and port of the NebulaGraph Analytics in the file <code>dag-ctrl-api.yaml</code>, the file path is <code>dag-ctrl/etc/dag-ctrl-api.yaml</code>. If there are multiple machines, ensure that the usernames and ports are the same.</p> <pre><code># configuration name.\nName: task-api\n\nHost: 0.0.0.0     # The IP address of Dag Controller.\nPort: 9002        # The port of Dag Controller.\nTimeout: 60000    # he timeout duration of HTTP interface requests.\n\nLog:              # The parameters related to log printing. For more Information, see https://go-zero.dev/cn/docs/blog/tool/logx/\n  Mode: file      # The log printing method\n  KeepDays: 7     # The maximum number of days to keep logs\n  Path: logs      # The output path of the log file\n  Level: info     # The log printing level\n  Compress: false  #  Whether the log needs to be compressed\n\n# The user name and SSH port of the NebulaGraph Analytics machine.\nSSH:\n UserName: vesoft\n Port: 22  \n\n# The parallel thread pool sizes of the tasks and jobs.\nJobPool:\n Sleep: 3    # Check every 3 seconds for any outstanding jobs.\n Size: 3    # Up to 3 jobs can be executed in parallel.\nTaskPool:\n CheckStatusSleep: 1    # Check the task status every second.\n Size: 10    # Up to 10 tasks can be executed in parallel.\nDag:\n VarDataListMaxSize: 100    # If HDFS columns are read, the number is limited to 100 at a time.\n\n# Other\nDebug:\n  Enable: false  #  Whether to enable Debugging.\n\n# The key for the Explorer to communicate with the Dag Controller. No modification is required.\nRsaPriKey: |\n  -----BEGIN RSA PRIVATE KEY-----\n  MIICXAIBAAKBgQDcR0keIMmmV...\n  -----END RSA PRIVATE KEY-----  \nRsaPubKey: |\n  -----BEGIN RSA PUBLIC KEY-----\n  MIGJAoGBANxHSR4gyaZX7uet7...\n  -----END RSA PUBLIC KEY-----\n</code></pre> </li> <li> <p>Configure the algorithm file path (<code>exec_file</code>) only in the file <code>tasks.yaml</code>, the file path of which is <code>dag-ctrl/etc/tasks.yaml</code>. Currently, all <code>exec_file</code> parameters are set to the path of the <code>run_algo.sh</code> file.</p> <p>Note</p> <ul> <li>The algorithm files are provided by NebulaGraph Analytics. Please find the <code>scripts</code> directory under the installation path of NebulaGraph Analytics above. All algorithm files are in this directory.  </li> <li>If there are multiple machines, ensure that the algorithm file paths are the same.</li> <li>The other parameters are the execution parameters of the algorithms and are configured later on the visual workflow page.</li> </ul> <pre><code>exec_file: /home/xxx/nebula-analytics/scripts/run_algo.sh\n</code></pre> </li> </ol>"},{"location":"nebula-explorer/deploy-connect/ex-ug-deploy/#next_to_do","title":"Next to do","text":"<p>Connect to Explorer</p>"},{"location":"nebula-explorer/graph-explorer/13.choose-graphspace/","title":"Choose graph spaces","text":"<p>You must first choose a graph space and then query and analyze data with Explorer. This topic introduces how to choose a graph space.</p>"},{"location":"nebula-explorer/graph-explorer/13.choose-graphspace/#prerequisite","title":"Prerequisite","text":"<p>You have connected to Explorer. For details, see Connect to Explorer.</p>"},{"location":"nebula-explorer/graph-explorer/13.choose-graphspace/#steps","title":"Steps","text":"<p>After connecting to Explorer, the system automatically displays the graph space selection page. You only need to select the target graph space.</p> <p></p> <p>If you want to select a graph space again, follow the below steps to choose one.</p> <ol> <li> <p>In the navigation bar on the left side of the Explorer page, click the graph space icon .</p> </li> <li> <p>Choose the target graph space.</p> <p>Note</p> <p>You can select the same or different graph spaces multiple times, and each selection creates a new canvas.</p> </li> </ol>"},{"location":"nebula-explorer/graph-explorer/ex-ug-graph-exploration/","title":"Graph exploration","text":"<p>The graph exploration can be performed from the following four aspects:</p> <ul> <li>Expand</li> <li>Common Neighbor</li> <li>Search for Path</li> <li>Inspect Property</li> </ul> <p></p>"},{"location":"nebula-explorer/graph-explorer/ex-ug-graph-exploration/#prerequisite","title":"Prerequisite","text":"<p>Make sure that there are vertices on the canvas. For more information, see Start querying.</p>"},{"location":"nebula-explorer/graph-explorer/ex-ug-graph-exploration/#expand","title":"Expand","text":"<ol> <li> <p>In the navigation bar on the left side of the page, click  to open the Expand panel. You can set expansion conditions on the panel, including edge type, direction, vertex style, steps or filter, as described below.</p> Parameter Description Edge type All edges in the graph space are displayed and selected by default. Direction Define the edge direction for the selected vertices, including <code>Outgoing</code>, <code>Incoming</code>, and <code>Bidirect</code>. Vertex Style <code>Group by vertex tag</code>: The target vertices are displayed in the same color as the corresponding tag.<code>Custom Style</code>: You can customize the color of the target vertices. Steps <code>Single</code>: Customize the number of steps from the selected vertex to the target vertex.<code>Range</code>: Customize the step range from the selected vertex to the target vertex. Filter Query target vertices by filtering conditions. </li> <li> <p>Select the vertex you want to expand, either by holding down the right mouse to select or by holding down the <code>Shift</code> key and clicking on multiple vertexes on the canvas, and then click the <code>Expand</code> button in the Expand panel. For a single vertex, you can double-click the left mouse on the vertex to expand.</p> </li> </ol> <p>Note</p> <p>The system saves the current configurations on the panel. When you double-click or right-click on a vertex for exploration, the exploration will be performed based on the saved configurations.</p>"},{"location":"nebula-explorer/graph-explorer/ex-ug-graph-exploration/#common_neighbor","title":"Common Neighbor","text":"<p>In the navigation bar on the left side of the page, click  to open the Common Neighbor panel. You can select two or more vertices either by holding down the right mouse to select or by holding down the <code>Shift</code> key and clicking on multiple vertexes on the canvas and query their common neighbors. When the selected vertices have no common neighbor, the default returns **There is no data.</p>"},{"location":"nebula-explorer/graph-explorer/ex-ug-graph-exploration/#search_for_path","title":"Search for Path","text":"<ol> <li> <p>In the navigation bar on the left side of the page, click  to open the Search Path panel. You can set the edge type, direction, query type or filter, as described below.</p> Parameter Description Edge Type All edges in the graph space are displayed and selected by default. Direction Define the edge direction for the selected vertices, including <code>Outgoing</code>, <code>Incoming</code>, and <code>Bidirect</code>. Query Type <code>All path</code>: Request for vertices and edges in all paths from the source vertex to the destination vertex.<code>Shortest Path</code>: Request for vertices and edges in the shortest path from the source vertex to the destination vertex.<code>NoLoop Path</code>: Request for vertices and edges in non-loop paths from the source vertex to the destination vertex. Steps Customize the number of steps from the source vertex to the destination vertex. Filter Query target vertices by filtering conditions. </li> <li> <p>Hold down the <code>Shift</code> key and left-click to select two vertexes on the canvas. The first selected vertex is the source and the second is the destination vertex by default. Then click Find Path in the Search Path window.</p> </li> </ol>"},{"location":"nebula-explorer/graph-explorer/ex-ug-graph-exploration/#inspect_property","title":"Inspect Property","text":"<p>In the navigation bar on the left side of the page, click  to open the Inspect Property panel. Properties of vertices or edges can be hidden or displayed on the canvas.</p> <p>Note</p> <ul> <li>Vertex properties are displayed on the canvas only when the zoom ratio is greater than 90%, and properties are automatically hidden when the zoom ratio is less than 90%.</li> <li>Edge properties are displayed on the canvas only when the zoom ratio is greater than 100%, and properties are automatically hidden when the zoom ratio is less than 100%.</li> </ul>"},{"location":"nebula-explorer/graph-explorer/ex-ug-query-exploration/","title":"Start querying","text":"<p>To explore graph data, users need to query some initial data, and based on these initial data, can further analysis and filtering. This topic describes how to query initial data.</p>"},{"location":"nebula-explorer/graph-explorer/ex-ug-query-exploration/#prerequisites","title":"Prerequisites","text":"<p>Select a target graph space before querying data. For more information, see Choose graph spaces.</p> <p>Legacy version compatibility</p> <p>For versions of NebulaGraph below 3.0.0, you need to create an index before querying data. For more information, see Create an index.</p>"},{"location":"nebula-explorer/graph-explorer/ex-ug-query-exploration/#steps","title":"Steps","text":"<p>Click the Start  icon to query target data on the <code>Explorer</code> page. The queried data will be displayed on the canvas. You have the following ways to query data:</p> <ul> <li>Query by VID</li> <li>Query by Tag</li> <li>Query Subgraph</li> <li>Query by template</li> </ul>"},{"location":"nebula-explorer/graph-explorer/ex-ug-query-exploration/#query_by_vid","title":"Query by VID","text":"<p>You can enter VIDs to query the target vertices.</p> <p>There are three ways to generate VIDs: Manual input, Random import, and File import.</p> <p>Note</p> <p>Only one VID per row is supported in the querying area. Press <code>Enter</code> to separate the VIDs.</p> <p>The following GIF shows how to query data using the <code>basketballplayer</code> graph space and related data.</p> <p></p>"},{"location":"nebula-explorer/graph-explorer/ex-ug-query-exploration/#query_by_tag","title":"Query by Tag","text":"<p>You can select the tag and corresponding index to query the target vertices, and set the number of results limit or filter conditions.</p> <p>Note</p> <p>Make sure that the corresponding tags and indexes exist in the graph space when querying by tag. For more information, Create tags and Create indexes.</p> <p>The following example queries 10 players whose age is greater than 30 years old and not equal to 40 years old.</p> <p></p>"},{"location":"nebula-explorer/graph-explorer/ex-ug-query-exploration/#query_subgraph","title":"Query Subgraph","text":"<p>When querying subgraphs, you can specify the number of steps, edge types, and the direction of inflow and outflow of the subgraph. VID is mandatory. The default value of optional steps is 1, and the default value of optional edge type is all.</p> <p>Note</p> <p>When multiple VIDs are entered, the VIDs are separated by the Enter key.</p> <p>The following is an example of VIDs <code>Kings</code> and <code>Suns</code>, step number <code>2</code>, and incoming edge types with a VID value of 101, the number of steps of 4, and edge types of <code>server</code> and <code>like</code>.</p> <p></p>"},{"location":"nebula-explorer/graph-explorer/ex-ug-query-exploration/#query_by_template","title":"Query by template","text":"<p>You can select the created nGQL template, and set the parameter value.</p> <p></p> <ul> <li>When the returned result is vertices, they will be displayed on the canvas.</li> <li>When the returned result is not vertices, they will be displayed in table format. For example, return player name, age, etc.</li> </ul> <p>For more information, see nGQL template.</p>"},{"location":"nebula-explorer/graph-explorer/graph-algorithm/","title":"Graph computing","text":"<p>To better mine and analyze the graph data, users can perform graph computing based on the vertexes and edges in the canvas and view the graph computing results directly.</p> <p>Note</p> <p>This function only performs graph computing for existing vertexes in the canvas. If you need to perform complex graph computing, it is recommended to use Workflow to perform complex visual graph computing.</p>"},{"location":"nebula-explorer/graph-explorer/graph-algorithm/#prerequisites","title":"Prerequisites","text":"<p>Ensure the canvas has the vertex and edge data needed for the graph calculation. For details, see Start querying.</p>"},{"location":"nebula-explorer/graph-explorer/graph-algorithm/#steps","title":"Steps","text":"<ol> <li> <p>In the navigation bar on the left side of the page, click  button to open Graph algorithm panel.</p> </li> <li> <p>Select the algorithm and set related parameters. For more Information about algorithm and parameter, see Algorithm overview\u3002</p> </li> <li> <p>Click Run and the result pops up from below the canvas.</p> </li> <li> <p>On the result page, you can do the following operations:</p> <ul> <li>Click Auto complete 1-degree edges to completes the one-degree path relationship between all vertexes in the canvas.</li> <li>Click Export CSV File to download the graph computing result file in CSV format.</li> </ul> </li> </ol> <p></p>"},{"location":"nebula-explorer/graph-explorer/node-filtering/","title":"Vertex Filter","text":"<p>The Vertex Filter helps you filter the vertices and edges displayed on the canvas. You can filter data by tag only or by one or more sets of filter conditions. </p>"},{"location":"nebula-explorer/graph-explorer/node-filtering/#prerequisite","title":"Prerequisite","text":"<p>Make sure that there are vertices on the canvas. For more information, see Start query.</p>"},{"location":"nebula-explorer/graph-explorer/node-filtering/#notes","title":"Notes","text":"<ul> <li> <p>When filtering vertices and associated edges by Tag:</p> <ul> <li>All the tags in the graph space are displayed on the Filters panel.</li> </ul> <ul> <li>The selected tag turns gray, and the vertices and associated edges of the corresponding tag are hidden. </li> </ul> <ul> <li>For multi-tag vertices, if any of its tags is selected, the vertices are hidden.</li> </ul> <ul> <li>You can enter a tag name in the search box to search for tags.</li> </ul> </li> </ul> <ul> <li> <p>When filtering vertices and associated edges by filter conditions.</p> <ul> <li>Each set of filter conditions is only for the data with the target tag. The filtering conditions include Tag, Property, Operator, and Value. If the conditions are met, and the corresponding vertices will be automatically selected. If the conditions are not met, the corresponding vertices can be set to be hidden or turning gray. The vertices with other tags are not affected.</li> </ul> <ul> <li>The filtering priority by Tag is the highest. If the filter conditions include a selected tag (in gray), the corresponding data will not be displayed on the canvas.</li> </ul> <ul> <li>Each time you perform Vertex Filter, only one tag can be selected. If you want to filter data based on more tags, conduct Add New Filter multiple times.</li> </ul> <ul> <li>The same tag cannot be filtered multiple times. Only the result of the first filtering is displayed.</li> </ul> </li> </ul>"},{"location":"nebula-explorer/graph-explorer/node-filtering/#example","title":"Example","text":""},{"location":"nebula-explorer/graph-explorer/node-filtering/#example_1_filter_vertices_on_the_canvas_with_the_tag_player","title":"Example 1 Filter vertices on the canvas with the tag player","text":"<ol> <li> <p>In the left navigation bar, click Vertex Filter .</p> </li> <li> <p>On the Filters panel, click player.</p> </li> <li> <p>Only vertices with the tag team are displayed on the canvas.</p> <p></p> <p>The orange vertices filtered out in the above figure are the vertices with the tag team.</p> </li> </ol>"},{"location":"nebula-explorer/graph-explorer/node-filtering/#example_2_filter_players_older_than_33_years_old","title":"Example 2 Filter players older than 33 years old","text":"<ol> <li> <p>In the left navigation bar, click Vertex Filter .</p> </li> <li> <p>Click Add New Filter, and set filter conditions (The values in the example are <code>player</code>, <code>age</code>, <code>&gt;</code>, and <code>33</code>).</p> </li> <li> <p>Click Grayscale to gray the vertices that do not meet the filter conditions.</p> </li> <li> <p>Turn on the Apply Filter button.</p> <p></p> </li> </ol>"},{"location":"nebula-explorer/graph-explorer/property-calculation/","title":"Property calculation","text":"<p>When there are a large number of vertices in the canvas, to enhance the readability and analyzability of the graph, edges with the same start vertex, end vertex and edge type can be aggregated. The aggregated edges can be computed and displayed based on their properties.</p>"},{"location":"nebula-explorer/graph-explorer/property-calculation/#prerequisites","title":"Prerequisites","text":"<p>There were aggregated edges on the canvas.</p>"},{"location":"nebula-explorer/graph-explorer/property-calculation/#precautions","title":"Precautions","text":"<ul> <li>Currently, only summation is supported.</li> <li>Only properties of type INT can be aggregated.</li> <li>Users can select multiple Edge types for aggregation separately.</li> <li>Users can select multiple properties for aggregation separately.</li> <li>An edge can display only one aggregation result. You can hover over the aggregated edge to see all the results.</li> </ul>"},{"location":"nebula-explorer/graph-explorer/property-calculation/#steps","title":"Steps","text":""},{"location":"nebula-explorer/graph-explorer/property-calculation/#method_1","title":"Method 1","text":"<ol> <li> <p>In the left navigation bar, click  to open the Property Calculation panel.</p> </li> <li> <p>Click + and set the edge type, properties and calculation. You can select multiple attributes to be aggregated separately.</p> </li> <li> <p>Click Confirm\u3002</p> </li> </ol> <p>Click + to add more edge types for property calculation.</p> <p></p>"},{"location":"nebula-explorer/graph-explorer/property-calculation/#method_2","title":"Method 2","text":"<ol> <li> <p>Right-click the aggregated edge on the canvas and select Property Calculation.</p> </li> <li> <p>Set the properties and calculation.</p> </li> <li> <p>Click Confirm.</p> </li> </ol>"},{"location":"nebula-explorer/workflow/1.prepare-resources/","title":"Prepare resources","text":"<p>You must prepare your environment for running a workflow, including NebulaGraph configurations, DAG configurations, NebulaGraph Analytics configurations and HDFS configurations.</p>"},{"location":"nebula-explorer/workflow/1.prepare-resources/#prerequisites","title":"Prerequisites","text":"<ul> <li>NebulaGraph Analytics 3.5.0 or later have been deployed. For details, see NebulaGraph Analytics.</li> </ul> <ul> <li>Dag Controller have been deployed and started. For details, see Deploy Explorer.</li> </ul>"},{"location":"nebula-explorer/workflow/1.prepare-resources/#steps","title":"Steps","text":"<ol> <li> <p>At the top of the Explorer page, click Workflow.</p> </li> <li> <p>Click Workflow Configuration in the upper right corner of the page.</p> </li> <li> <p>Configure the following resources.</p> <p></p> <ul> <li> <p>NebulaGraph configuration</p> <p>The access address of the graph service that executes a graph query or to which the graph computing result is written. The default address is the address that you use to log into Explorer and can not be changed. You can set timeout periods for three services.</p> </li> </ul> <ul> <li> <p>DAG configuration</p> <p>The configuration of the Dag Controller for the graph computing.</p> <ul> <li>Username: The username to run the service, and does not need to be changed.</li> <li>Local data directory: The Analytics data directory, the shared directory of NFS service. By default, the workflow uses NFS to store the graph computing results, but the user needs to install NFS and mount the directory manually.</li> <li>SSH key path for passwordless auth: The path to the private key file of the machine where Dag Controller is located. It is used for SSH-free login between machines.</li> </ul> </li> </ul> <ul> <li> <p>NebulaGraph Analytics configuration</p> <p>Add the address of the NebulaGraph Analytics where the graph computing will be performed.</p> <ul> <li>Nebula Analytics Node IP address: fill in the new Analytics node IP address.</li> <li>Username: The username used for password-free access to this Analytics node. The username must be consistent across all Analytics nodes.</li> <li>SSH port number: Default is <code>22</code>.</li> <li>SSH key path for passwordless auth: Used for SSH-free login between machines. Default is <code>~/.ssh/id_rsa</code>.</li> <li>Local data directory: Default is <code>~/analytics-data</code>.</li> <li>ALGORITHM script path: Default is <code>~/nebula-analytics/scripts/run_algo.sh</code>.</li> </ul> </li> </ul> <ul> <li> <p>NFS configuration</p> <p>Note</p> <p>Users need to deploy the NFS Server on the Dag Controller machine and configure the shared directory by themselves, and then deploy the NFS client on all Analytics machines and mount the shared directory.</p> <p>Enabled by default, the results of the task will be saved locally. Each Analytics node needs to be configured with a local data directory for NFS.</p> </li> </ul> <ul> <li> <p>HDFS configuration (Optional)</p> <p>By default, NFS is used to save the graph computing results. If you need to use HDFS, please install the HDFS client on the machine where Analytics is located first.</p> <ul> <li>HDFS name: Fill in the name of HDFS configuration, it is convenient to distinguish different HDFS configurations.</li> <li>HDFS path: The <code>fs.defaultFS</code> configuration in HDFS. Support configure the save path, such as <code>hdfs://192.168.8.100:9000/test</code>.</li> <li>HDFS username: The name of the user using HDFS.</li> </ul> </li> </ul> </li> <li> <p>Click Save.</p> </li> <li> <p>Click Configuration Check in the upper right corner and click Start Check to check if the configuration is working.</p> </li> </ol>"},{"location":"nebula-explorer/workflow/2.create-workflow/","title":"Workflow example","text":"<p>This topic describes how to create a simple workflow.</p>"},{"location":"nebula-explorer/workflow/2.create-workflow/#prerequisites","title":"Prerequisites","text":"<ul> <li>The data source is ready. The data source can be data in NebulaGraph or CSV files on NFS/HDFS.</li> </ul> <ul> <li>The resource has been configured.</li> </ul>"},{"location":"nebula-explorer/workflow/2.create-workflow/#add_workflow","title":"Add workflow","text":"<p>With the result of the MATCH statement <code>MATCH (v1:player)--(v2)  RETURN id(v1), id(v2);</code> as the input of the PageRank algorithm, the following will introduce how to create a simple workflow.</p> <ol> <li> <p>At the top of the Explorer page, click Workflow.</p> </li> <li> <p>In the Workflows tab, click New workflow to enter the process canvas page.</p> </li> <li> <p>In the component library list on the left side of the process canvas page, select Query-&gt;Query and drag it onto the canvas. Click the graph query component and set the following parameters in the configuration panel on the right side.</p> <p></p> Parameters Description Query Click  to modify the component name to identify the component. Input Set custom parameters that can be used for parameterized query. Click Add parameter to add more custom parameters. Query language Select the graph space to execute the nGQL statement and fill in the nGQL statement. Click Parse Parameter to display the returned column name in the Output. Output The column name returned by parsing the query language. You can change the name, which is equivalent to aliasing the column with <code>AS</code>. Results Set the saving project of the result. To call the results expediently for other algorithms, the results of the graph query component can be saved in the NFS or HDFS. <p>Note</p> <p>The connection anchors are shown in yellow, indicating that it is optional and can be set by user or provided by any other component.</p> </li> <li> <p>In the component library list on the left side of the process canvas page, select Node importance-&gt;PageRank and drag it onto the canvas. Connect the anchor <code>output0</code> to the anchor <code>input0</code> and the anchor <code>output1</code> to the anchor <code>input1</code>.</p> <p></p> <p>If you use multiple graph query components in series, you need to add parameterized text. For example, if you fill in the statement <code>GO FROM ${id} OVER follow YIELD dst(vertex)</code> and click <code>Parse parameter</code>, a yellow anchor for <code>${id}</code> will appear in the graph query component. The output anchor of the previous graph query component can be connected to the yellow anchor as input.</p> <p></p> </li> <li> <p>Click the graph computing component and set the following parameters in the configuration panel on the right side.</p> <p></p> Parameters Description PageRank Click  to modify the component name to identify the component. Input Multiple data sources are supported as input. NebulaGraph: Users must select one graph space and corresponding edge types.Dependence: The system will automatically recognize the data source according to the connection of the anchor.HDFS: Users must select HDFS and fill in the relative path of the data source file.Local: Users must fill in the relative path of the data source file. Parameter settings Set the parameters of the graph algorithm. The parameters of different algorithms are different. Some parameters can be obtained from any upstream component where the anchor are shown in yellow. Output Display the column name of the graph computing results. The name can not be modified. Execution settings Machine num: The number of machines executing the algorithm.Processes: The total number of processes executing the algorithm. Allocate these processes equally to each machine based on the number of machines.Threads: How many threads are started per process. Results Set the restoration path of the results in NFS, HDFS or NebulaGraph.Local: The save path is automatically generated based on the job and task ID.HDFS: The save path is automatically generated based on the job and task ID.NebulaGraph: Tags need to be created beforehand in the corresponding graph space to store the results. For more information about the properties of the tag, see Algorithm overview.Some algorithms can only be saved in the HDFS. </li> <li> <p>Click  next to the automatically generated workflow name at the upper left corner of the canvas page to modify the workflow name, and click Run at the upper right corner of the canvas page. The job page is automatically displayed to show the job progress. You can view the result after the job is completed. For details, see Job management.</p> <p>Note</p> <p>When you click Run, the workflow will be automatically saved. If you do not perform graph computing and only make modifications, click  to save the modification, or click  to save the workflow as a new workflow.</p> </li> </ol>"},{"location":"nebula-explorer/workflow/3.workflow-management/","title":"Workflow management","text":"<p>This topic describes how to manage workflows, including view, modify, rename, clone, delete, and compare workflows.</p>"},{"location":"nebula-explorer/workflow/3.workflow-management/#steps","title":"Steps","text":"<ol> <li> <p>At the top of the Explorer page, click Workflow.</p> </li> <li> <p>In the Workflows tab, users can view all saved workflows. The list displays <code>Workflow name</code>, <code>Created time</code>, <code>Update time</code>, and <code>Algorithm</code>.</p> <ul> <li>At the top of the list page, click Comparison and select two workflows or different historical versions of the same workflow for code comparison.</li> </ul> <ul> <li>At the top of the list page, users can search the workflow by keywords in the search box.</li> </ul> <ul> <li> <p>In the Operation column of the list page, users an perform the following operations:</p> <ul> <li>Run: Instantiate the workflow directly as a job and execute the job.</li> <li>Open: Open a workflow to view and modify the workflow. After modifying the workflow, click  to save the modification or click  to save the workflow as a new workflow.</li> <li>View jobs: Jump to the job list to view all the jobs instantiated by this workflow.</li> <li>: Users can view the workflow code, rename the workflow, clone the workflow, and delete the workflow.</li> </ul> </li> </ul> </li> </ol>"},{"location":"nebula-explorer/workflow/4.jobs-management/","title":"Job management","text":"<p>This topic describes how to view the lists, progresses, results, logs of the jobs and rerun jobs.</p>"},{"location":"nebula-explorer/workflow/4.jobs-management/#steps","title":"Steps","text":"<ol> <li> <p>At the top of the Explorer page, click Workflow.</p> </li> <li> <p>In the Jobs tab, users can view all the jobs. The page displays <code>Job ID</code>, <code>Job name</code>, <code>Status</code>, <code>CREATE time</code>, <code>End time</code> and <code>Workflow version</code>.</p> <ul> <li>At the top of the list page, click Comparison and select two workflows or different jobs of the same workflow for code comparison.</li> </ul> <ul> <li>At the top of the list page, users can filter the workflow and version in the filter box.</li> </ul> <ul> <li>At the top of the list page, users can search the job by keywords in the search box.</li> </ul> <ul> <li> <p>In the Operation column of the list page, users an perform the following operations:</p> <ul> <li>View in Explorer: For successfully executed jobs, users can select the graph space and the component to view the output of the component. Users can export the results to a CSV file.</li> </ul> <ul> <li>Rerun: For failed executed jobs, users can rerun the job.</li> </ul> <ul> <li>Open: Users can rerun the job and view the results and logs of the job. Users can also jump to the corresponding workflow for editing (the workflow is the latest version).</li> </ul> </li> </ul> </li> </ol>"},{"location":"nebula-explorer/workflow/workflows/","title":"Workflow overview","text":"<p>NebulaGraph Explorer supports visual and complex graph computing with custom workflows.</p>"},{"location":"nebula-explorer/workflow/workflows/#background","title":"Background","text":"<p>NebulaGraph Explorer provides multiple components, including graph query and graph computing components. Users can combine these components based on the scheduling tool Dag Controller for free. For example, using the output of a graph query component as an input to a graph computing component. The whole process is a directed acyclic workflow.</p> <p></p> <p>Instantiate the workflow when performing graph computing. The instantiated component is called task, and the instantiated workflow is called job. A job can consist of multiple tasks. The NebulaGraph Explorer sends the job to NebulaGraph Analytics for graph computing, and you can view the result in the job list.</p>"},{"location":"nebula-explorer/workflow/workflows/#features","title":"Features","text":"<ul> <li>Add, view, modify, delete, compare, clone and rename workflows.</li> <li>A workflow supports one query component and multiple graph computing components. You can search for, add, configure, and rename component.</li> <li>View the lists, progresses, results and logs of the jobs, and rerun jobs.</li> <li>Search for workflows or jobs.</li> </ul>"},{"location":"nebula-explorer/workflow/workflows/#precautions","title":"Precautions","text":"<ul> <li>The Workflow page can be displayed only when Workflow is enabled in .</li> </ul> <ul> <li>Additional deployment of the Dag Controller and the NebulaGraph Analytics is required to use a workflow. For details, see NebulaGraph Analytics and Deploy Explorer.</li> </ul> <ul> <li>The input to the graph query component can only be the nGQL.</li> </ul> <ul> <li>The results of a graph query component can be stored in the NFS by default and also in HDFS, which is convenient to be called by multiple algorithms.</li> </ul> <ul> <li>The input to the graph computing component can be the specified data in the NebulaGraph, NFS or HDFS, or can depend on the results of the graph query component.   If an input depends on the results of the previous graph query component, the graph computing component must be fully connected to the graph query component, that is, the white output anchors of the previous graph query component are all connected to the white input anchors of the graph compute component.</li> </ul> <ul> <li>The parameters of some algorithms can also depend on the upstream components.</li> </ul> <ul> <li>The result of the graph computing components can be stored in the NebulaGraph, NFS or HDFS, but not all algorithm results are suitable to be stored in NebulaGraph. Some algorithms can only be saved in NFS or HDFS when configuring the save results page.</li> </ul>"},{"location":"nebula-explorer/workflow/workflows/#algorithm_description","title":"Algorithm description","text":"<p>See Algorithm description.</p>"},{"location":"nebula-explorer/workflow/workflow-api/api-cancel-job/","title":"Cancel a running job","text":"<p>This topic describes how to use an API to cancel a running job.</p>"},{"location":"nebula-explorer/workflow/workflow-api/api-cancel-job/#api_path","title":"API path","text":"<p><code>api-open/v1/jobs/&lt;job_id&gt;/cancel</code></p> <p><code>&lt;job_id&gt;</code>: The job ID. See request parameters below.</p>"},{"location":"nebula-explorer/workflow/workflow-api/api-cancel-job/#request_parameters","title":"Request parameters","text":""},{"location":"nebula-explorer/workflow/workflow-api/api-cancel-job/#path_parameters","title":"Path parameters","text":"Parameters Type If required Default value Example Description <code>job_id</code> number yes - <code>1964</code> The job ID. It can be queried through the API Get a list of all jobs or viewed on the job list page."},{"location":"nebula-explorer/workflow/workflow-api/api-cancel-job/#headers_parameters","title":"Headers parameters","text":"Parameters Type If required Default value Example Description <code>Content-Type</code> string yes - <code>application/x-www-form-urlencoded</code> The content type. <code>explorer_token</code> string yes - <code>eyJhbxxx</code> The authorization token that is used to verify account information. For details, see Workflow API overview."},{"location":"nebula-explorer/workflow/workflow-api/api-cancel-job/#body_parameters","title":"Body parameters","text":"<p>None.</p>"},{"location":"nebula-explorer/workflow/workflow-api/api-cancel-job/#request_example","title":"Request example","text":"<pre><code>curl -i -X PUT -H \"Content-Type: application/x-www-form-urlencoded\" -H \"Cookie: \"explorer_token=eyJhbxxx\"\" http://192.168.8.145:7002/api-open/v1/jobs/30600/cancel\n</code></pre>"},{"location":"nebula-explorer/workflow/workflow-api/api-cancel-job/#response_parameters","title":"Response parameters","text":"Parameters Type Example Description <code>code</code> number <code>0</code> The result code of the request. Return <code>0</code> if the request is successful, and return an error code if the request is unsuccessful. For details, see Workflow API overview. <code>message</code> string <code>Success</code> The result information of the execution. <code>data</code> object - The list of returned data. \u00a0\u00a0\u00a0 - <code>success</code> bool <code>true</code> Whether the job was canceled successfully."},{"location":"nebula-explorer/workflow/workflow-api/api-cancel-job/#response_example","title":"Response example","text":"<pre><code>{\n  \"cookie\": [],\n  \"Content-Type\": \"application/json\",\n  \"Traceparent\": \"00-8b4b47413a211d9b5e0839aadc712052-4a98bae37fe5948a-00\",\n  \"Date\": \"Mon, 18 Jul 2022 01:45:08 GMT\",\n  \"Content-Length\": \"54\"\n}\n{\n  \"code\": 0,\n  \"data\": {\n    \"success\": true\n  },\n  \"message\": \"Success\"\n}\n</code></pre>"},{"location":"nebula-explorer/workflow/workflow-api/api-desc-job/","title":"Query details for a specified job","text":"<p>This topic describes how to use an API to query details for a specified job.</p>"},{"location":"nebula-explorer/workflow/workflow-api/api-desc-job/#api_path","title":"API path","text":"<p><code>api-open/v1/jobs/&lt;job_id&gt;</code></p> <p><code>&lt;job_id&gt;</code>: The job ID. See request parameters below.</p>"},{"location":"nebula-explorer/workflow/workflow-api/api-desc-job/#request_parameters","title":"Request parameters","text":""},{"location":"nebula-explorer/workflow/workflow-api/api-desc-job/#path_parameters","title":"Path parameters","text":"Parameters Type If required Default value Example Description <code>job_id</code> number yes - <code>1964</code> The job ID. It can be queried through the API Get a list of all jobs or viewed on the job list page."},{"location":"nebula-explorer/workflow/workflow-api/api-desc-job/#headers_parameters","title":"Headers parameters","text":"Parameters Type If required Default value Example Description <code>Content-Type</code> string yes - <code>application/json</code> The content type. <code>explorer_token</code> string yes - <code>eyJhbxxx</code> The authorization token that is used to verify account information. For details, see Workflow API overview."},{"location":"nebula-explorer/workflow/workflow-api/api-desc-job/#body_parameters","title":"Body parameters","text":"<p>None.</p>"},{"location":"nebula-explorer/workflow/workflow-api/api-desc-job/#request_example","title":"Request example","text":"<pre><code>curl -i -X GET -H \"Content-Type: application/json\" -H \"Cookie: \"explorer_token=eyJhbxxx\"\" http://192.168.8.145:7002/api-open/v1/jobs/1964\n</code></pre>"},{"location":"nebula-explorer/workflow/workflow-api/api-desc-job/#response_parameters","title":"Response parameters","text":"Parameters Type Example Description <code>code</code> number <code>0</code> The result code of the request. Return <code>0</code> if the request is successful, and return an error code if the request is unsuccessful. For details, see Workflow API overview. <code>message</code> string <code>Success</code> The result information of the execution. <code>data</code> object - The list of returned data. \u00a0\u00a0\u00a0 - <code>id</code> number <code>1964</code> The job ID. \u00a0\u00a0\u00a0 - <code>name</code> string <code>workflow_xkkjf_20220712103332</code> The job name. \u00a0\u00a0\u00a0 - <code>workflowId</code> string <code>3992429968</code> The workflow ID. \u00a0\u00a0\u00a0 - <code>workflowName</code> string <code>workflow_xkkjf</code> The workflow name. \u00a0\u00a0\u00a0 - <code>status</code> number <code>2</code> The job status code. For details, see Workflow API overview. \u00a0\u00a0\u00a0 - <code>tasks</code> object - The task details. \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 - <code>id</code> string <code>f93dea90fc3a11ecac7e6da0662c195b</code> The task ID. \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 - <code>name</code> string <code>BFS</code> The task name. \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 - <code>runBeginTime</code> datetime <code>2022-07-12T10:33:35+08:00</code> The start time of the task execution. \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 - <code>runEndTime</code> datetime <code>2022-07-12T10:33:38+08:00</code> The end time of the task execution. \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 - <code>status</code> number <code>2</code> The task status code. For details, see Workflow API overview."},{"location":"nebula-explorer/workflow/workflow-api/api-desc-job/#response_example","title":"Response example","text":"<pre><code>{\n  \"cookie\": [],\n  \"Content-Type\": \"application/json\",\n  \"Traceparent\": \"00-3db17c9fd9e0a4c3824973471523d214-4384705e523dce83-00\",\n  \"Date\": \"Fri, 15 Jul 2022 09:08:20 GMT\",\n  \"Content-Length\": \"400\"\n}\n{\n  \"code\": 0,\n  \"data\": {\n    \"id\": 1964,\n    \"name\": \"workflow_xkkjf_20220712103332\",\n    \"workflowId\": \"3992429968\",\n    \"workflowName\": \"workflow_xkkjf\",\n    \"status\": 2,\n    \"tasks\": [\n      {\n        \"id\": \"f93dea90fc3a11ecac7e6da0662c195b\",\n        \"name\": \"BFS\",\n        \"runBeginTime\": \"2022-07-12T10:33:35+08:00\",\n        \"runEndTime\": \"2022-07-12T10:33:38+08:00\",\n        \"status\": 2\n      }\n    ],\n    \"runBeginTime\": 1657593215000,\n    \"runEndTime\": 1657593218000,\n    \"createTime\": 1657593212505\n  },\n  \"message\": \"Success\"\n}\n</code></pre>"},{"location":"nebula-explorer/workflow/workflow-api/api-desc-task/","title":"Get the result data of a specified task","text":"<p>This topic describes how to use an API to get the result data of a specified task.</p>"},{"location":"nebula-explorer/workflow/workflow-api/api-desc-task/#api_path","title":"API path","text":"<p><code>api-open/v1/jobs/&lt;job_id&gt;/tasks/&lt;task_id&gt;/sample_result</code></p> <ul> <li><code>&lt;job_id&gt;</code>: The job ID. See request parameters below.</li> </ul> <ul> <li><code>&lt;task_id&gt;</code>: The task ID. See request parameters below.</li> </ul>"},{"location":"nebula-explorer/workflow/workflow-api/api-desc-task/#request_parameters","title":"Request parameters","text":""},{"location":"nebula-explorer/workflow/workflow-api/api-desc-task/#path_parameters","title":"Path parameters","text":"Parameters Type If required Default value Example Description <code>job_id</code> number yes - <code>29987</code> The job ID. It can be queried through the API Get a list of all jobs or viewed on the job list page. <code>task_id</code> number yes - <code>8c171f70fb6f11ecac7e6da0662c195b</code> The task ID. It can be queried through the API Query details for a specified job or viewed in the upper right corner of the specified job page by clicking the component."},{"location":"nebula-explorer/workflow/workflow-api/api-desc-task/#headers_parameters","title":"Headers parameters","text":"Parameters Type If required Default value Example Description <code>Content-Type</code> string yes - <code>application/x-www-form-urlencoded</code> The content type. <code>explorer_token</code> string yes - <code>eyJhbxxx</code> The authorization token that is used to verify account information. For details, see Workflow API overview."},{"location":"nebula-explorer/workflow/workflow-api/api-desc-task/#body_parameters","title":"Body parameters","text":"Parameters Type If required Default value Example Description <code>limit</code> number yes <code>10</code> - Limit the number of rows to return results."},{"location":"nebula-explorer/workflow/workflow-api/api-desc-task/#request_example","title":"Request example","text":"<pre><code>curl -i -X GET -H \"Content-Type: application/x-www-form-urlencoded\" -H \"Cookie: \"explorer_token=eyJhbxxx\"\" http://192.168.8.145:7002/api-open/v1/jobs/29987/tasks/8c171f70fb6f11ecac7e6da0662c195b/sample_result?limit=1000\n</code></pre>"},{"location":"nebula-explorer/workflow/workflow-api/api-desc-task/#response_parameters","title":"Response parameters","text":"Parameters Type Example Description <code>code</code> number <code>0</code> The result code of the request. Return <code>0</code> if the request is successful, and return an error code if the request is unsuccessful. For details, see Workflow API overview. <code>message</code> string <code>Success</code> The result information of the execution. <code>data</code> object - The list of returned data. \u00a0\u00a0\u00a0 - <code>items</code> list - The list of detailed results. \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 - <code>result</code> string <code>\"player110\",\"0.150000\"</code> Depending on the algorithm, the result could be 2 or 3 columns."},{"location":"nebula-explorer/workflow/workflow-api/api-desc-task/#response_example","title":"Response example","text":"<pre><code>{\n  \"cookie\": [],\n  \"Content-Type\": \"application/json\",\n  \"Traceparent\": \"00-14047b04b6810be06be22e010f500506-4c310a844b824a7f-00\",\n  \"Date\": \"Fri, 15 Jul 2022 09:36:56 GMT\",\n  \"Content-Length\": \"2014\"\n}\n{\n  \"code\": 0,\n  \"data\": {\n    \"items\": [\n      [\n        \"player110\",\n        \"0.150000\"\n      ],\n      [\n        \"team219\",\n        \"0.452126\"\n      ],\n      ......\n      [\n        \"player121\",\n        \"0.262148\"\n      ]\n    ]\n  },\n  \"message\": \"Success\"\n}\n</code></pre>"},{"location":"nebula-explorer/workflow/workflow-api/api-get-jobs/","title":"Get a list of all jobs","text":"<p>This topic describes how to use an API to get a list of all jobs.</p>"},{"location":"nebula-explorer/workflow/workflow-api/api-get-jobs/#api_path","title":"API path","text":"<p><code>api-open/v1/jobs</code></p>"},{"location":"nebula-explorer/workflow/workflow-api/api-get-jobs/#request_parameters","title":"Request parameters","text":""},{"location":"nebula-explorer/workflow/workflow-api/api-get-jobs/#path_parameters","title":"Path parameters","text":"<p>None.</p>"},{"location":"nebula-explorer/workflow/workflow-api/api-get-jobs/#headers_parameters","title":"Headers parameters","text":"Parameters Type If required Default value Example Description <code>Content-Type</code> string yes - <code>application/json</code> The content type. <code>explorer_token</code> string yes - <code>eyJhbxxx</code> The authorization token that is used to verify account information. For details, see Workflow API overview."},{"location":"nebula-explorer/workflow/workflow-api/api-get-jobs/#body_parameters","title":"Body parameters","text":"Parameters Type If required Default value Example Description <code>filter</code> object no - - The filter settings. \u00a0\u00a0\u00a0- <code>name</code> string no - <code>workflow_q745a_20220715092236</code> The job name. \u00a0\u00a0\u00a0- <code>status</code> number no - <code>2</code> The job status code. For details, see Workflow API overview. \u00a0\u00a0\u00a0- <code>fromCreateTime</code> number no - <code>1657848036000</code> Start time stamp. Filtering based on the job creation time. \u00a0\u00a0\u00a0- <code>toCreateTime</code> number no - <code>1657848157000</code> End time stamp. Filtering based on the job creation time. \u00a0\u00a0\u00a0- <code>orderByCreateTime</code> string no <code>desc</code> - Sorting mode. The available value are <code>desc</code> and <code>asc</code>. <code>pageSize</code> number no <code>10</code> - The number of entries to return on each page. <code>page</code> number no <code>1</code> - The number of the page to return."},{"location":"nebula-explorer/workflow/workflow-api/api-get-jobs/#request_example","title":"Request example","text":"<p>Note</p> <p>The content after <code>jobs?</code> is the body parameter, and the content of <code>filter</code> is the result of URL encoding. The original content of <code>filter</code> was <code>{ \"status\": 2,  \"orderByCreateTime\": \"asc\"}</code>.</p> <pre><code>curl -i -X GET -H \"Content-Type: application/json\" -H \"Cookie: \"explorer_token=eyJhbxxx\"\" http://192.168.8.145:7002/api-open/v1/jobs?filter=%7B%20%22status%22%3A%202%2C%20%20%22orderByCreateTime%22%3A%20%22asc%22%7D&amp;pageSize=10&amp;page=1\n</code></pre>"},{"location":"nebula-explorer/workflow/workflow-api/api-get-jobs/#response_parameters","title":"Response parameters","text":"Parameters Type Example Description <code>code</code> number <code>0</code> The result code of the request. Return <code>0</code> if the request is successful, and return an error code if the request is unsuccessful. For details, see Workflow API overview. <code>message</code> string <code>Success</code> The result information of the execution. <code>data</code> object - The list of returned data. \u00a0\u00a0\u00a0- <code>total</code> number <code>2</code> The total number of records. \u00a0\u00a0\u00a0- <code>Page</code> number <code>1</code> The number of the page to return. \u00a0\u00a0\u00a0- <code>PageSize</code> number <code>10</code> The number of entries to return on each page. \u00a0\u00a0\u00a0- <code>items</code> object - The list of record details. \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 - <code>id</code> number <code>105</code> The job ID. \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 - <code>name</code> string <code>workflow_q745a_20220715090915</code> The job name. \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 - <code>workflowId</code> string <code>4216617528</code> The workflow ID. \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 - <code>workflowName</code> string <code>workflow_q745a</code> The workflow name. \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 - <code>status</code> number <code>2</code> The job status code. For details, see Workflow API overview. \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 - <code>runBeginTime</code> number <code>1657847358000</code> The start time of the job execution. \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 - <code>runEndTime</code> number <code>1657847364000</code> The end time of the job execution. \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 - <code>createTime</code> number <code>1657847355906</code> The creation time of the job."},{"location":"nebula-explorer/workflow/workflow-api/api-get-jobs/#response_example","title":"Response example","text":"<pre><code>{\n  \"cookie\": [],\n  \"Content-Type\": \"application/json\",\n  \"Traceparent\": \"00-d3a1943f5baf46771e9afc629e0b5d40-920db2f06142f5ff-00\",\n  \"Date\": \"Fri, 15 Jul 2022 06:17:21 GMT\",\n  \"Content-Length\": \"512\"\n}\n\n{\n  \"code\": 0,\n  \"data\": {\n    \"items\": [\n      {\n        \"id\": 105,\n        \"name\": \"workflow_q745a_20220715090915\",\n        \"workflowId\": \"4216617528\",\n        \"workflowName\": \"workflow_q745a\",\n        \"status\": 2,\n        \"runBeginTime\": 1657847358000,\n        \"runEndTime\": 1657847364000,\n        \"createTime\": 1657847355906\n      },\n      {\n        \"id\": 106,\n        \"name\": \"workflow_q745a_20220715092236\",\n        \"workflowId\": \"4216617528\",\n        \"workflowName\": \"workflow_q745a\",\n        \"status\": 2,\n        \"runBeginTime\": 1657848157000,\n        \"runEndTime\": 1657848163000,\n        \"createTime\": 1657848156290\n      }\n    ],\n    \"total\": 2,\n    \"Page\": 1,\n    \"PageSize\": 10\n  },\n  \"message\": \"Success\"\n}\n</code></pre>"},{"location":"nebula-explorer/workflow/workflow-api/api-get-workflow-jobs/","title":"Get a list of jobs for a specified workflow","text":"<p>This topic describes how to use an API to get the list of jobs for a specified workflow.</p>"},{"location":"nebula-explorer/workflow/workflow-api/api-get-workflow-jobs/#api_path","title":"API path","text":"<p><code>api-open/v1/workflows/&lt;workflow_id&gt;/jobs</code></p> <p><code>&lt;workflow_id&gt;</code>: The workflow ID. See request parameters below.</p>"},{"location":"nebula-explorer/workflow/workflow-api/api-get-workflow-jobs/#request_parameters","title":"Request parameters","text":""},{"location":"nebula-explorer/workflow/workflow-api/api-get-workflow-jobs/#path_parameters","title":"Path parameters","text":"Parameters Type If required Default value Example Description <code>workflow_id</code> number yes - <code>4216617528</code> The workflow ID. The system instantiates a specified workflow as a job. The ID can be viewed in the upper left corner of the specified workflow page."},{"location":"nebula-explorer/workflow/workflow-api/api-get-workflow-jobs/#headers_parameters","title":"Headers parameters","text":"Parameters Type If required Default value Example Description <code>Content-Type</code> string yes - <code>application/json</code> The content type. <code>explorer_token</code> string yes - <code>eyJhbxxx</code> The authorization token that is used to verify account information. For details, see Workflow API overview."},{"location":"nebula-explorer/workflow/workflow-api/api-get-workflow-jobs/#body_parameters","title":"Body parameters","text":"Parameters Type If required Default value Example Description <code>filter</code> object no - - The filter settings. \u00a0\u00a0\u00a0- <code>name</code> string no - <code>workflow_q745a_20220715092236</code> The job name. \u00a0\u00a0\u00a0- <code>status</code> number no - <code>2</code> The job status code. For details, see Workflow API overview. \u00a0\u00a0\u00a0- <code>fromCreateTime</code> number no - <code>1657848036000</code> Start time stamp. Filtering based on the job creation time. \u00a0\u00a0\u00a0- <code>toCreateTime</code> number no - <code>1657848157000</code> End time stamp. Filtering based on the job creation time. \u00a0\u00a0\u00a0- <code>orderByCreateTime</code> string no <code>desc</code> - Sorting mode. The available value are <code>desc</code> and <code>asc</code>. <code>pageSize</code> number no <code>10</code> - The number of entries to return on each page. <code>page</code> number no <code>1</code> - The number of the page to return."},{"location":"nebula-explorer/workflow/workflow-api/api-get-workflow-jobs/#request_example","title":"Request example","text":"<p>Note</p> <p>The content after <code>jobs?</code> is the body parameter, and the content of <code>filter</code> is the result of URL encoding. The original content of <code>filter</code> was <code>{\"status\": 2, \"fromCreateTime\": 1657874100000}</code>.</p> <pre><code>curl -i -X GET -H \"Content-Type: application/json\" -H \"Cookie: \"explorer_token=eyJhbxxx\"\" http://192.168.8.145:7002/api-open/v1/workflows/4216617528/jobs?filter=%7B%22status%22%3A%202%2C%20%20%22fromCreateTime%22%3A%201657874100000%7D&amp;pageSize=10&amp;page=1\n</code></pre>"},{"location":"nebula-explorer/workflow/workflow-api/api-get-workflow-jobs/#response_parameters","title":"Response parameters","text":"Parameters Type Example Description <code>code</code> number <code>0</code> The result code of the request. Return <code>0</code> if the request is successful, and return an error code if the request is unsuccessful. For details, see Workflow API overview. <code>message</code> string <code>Success</code> The result information of the execution. <code>data</code> object - The list of returned data. \u00a0\u00a0\u00a0- <code>total</code> number <code>2</code> The total number of records. \u00a0\u00a0\u00a0- <code>Page</code> number <code>1</code> The number of the page to return. \u00a0\u00a0\u00a0- <code>PageSize</code> number <code>10</code> The number of entries to return on each page. \u00a0\u00a0\u00a0- <code>items</code> object - The list of record details. \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 - <code>id</code> number <code>105</code> The job ID. \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 - <code>name</code> string <code>workflow_q745a_20220715090915</code> The job name. \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 - <code>workflowId</code> string <code>4216617528</code> The workflow ID. \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 - <code>workflowName</code> string <code>workflow_q745a</code> The workflow name. \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 - <code>status</code> number <code>2</code> The job status code. For details, see Workflow API overview. \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 - <code>runBeginTime</code> number <code>1657847358000</code> The start time of the job execution. \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 - <code>runEndTime</code> number <code>1657847364000</code> The end time of the job execution. \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 - <code>createTime</code> number <code>1657847355906</code> The creation time of the job."},{"location":"nebula-explorer/workflow/workflow-api/api-get-workflow-jobs/#response_example","title":"Response example","text":"<pre><code>{\n  \"cookie\": [],\n  \"Content-Type\": \"application/json\",\n  \"Traceparent\": \"00-008c3056686dd3f3be38b8eda42a917e-b5616e30434cb803-00\",\n  \"Date\": \"Fri, 15 Jul 2022 08:44:06 GMT\",\n  \"Content-Length\": \"297\"\n}\n{\n  \"code\": 0,\n  \"data\": {\n    \"items\": [\n      {\n        \"id\": 115,\n        \"name\": \"workflow_q745a_20220715163650\",\n        \"workflowId\": \"4216617528\",\n        \"workflowName\": \"workflow_q745a\",\n        \"status\": 2,\n        \"runBeginTime\": 1657874212000,\n        \"runEndTime\": 1657874218000,\n        \"createTime\": 1657874210088\n      }\n    ],\n    \"total\": 1,\n    \"Page\": 1,\n    \"PageSize\": 10\n  },\n  \"message\": \"Success\"\n}\n</code></pre>"},{"location":"nebula-explorer/workflow/workflow-api/api-post-jobs/","title":"Add a new job","text":"<p>This topic describes how to use an API to add a new job.</p>"},{"location":"nebula-explorer/workflow/workflow-api/api-post-jobs/#api_path","title":"API path","text":"<p><code>api-open/v1/workflows/&lt;workflow_id&gt;/jobs</code></p> <p><code>&lt;workflow_id&gt;</code>: The workflow ID. See request parameters below.</p>"},{"location":"nebula-explorer/workflow/workflow-api/api-post-jobs/#request_parameters","title":"Request parameters","text":""},{"location":"nebula-explorer/workflow/workflow-api/api-post-jobs/#path_parameters","title":"Path parameters","text":"Parameters Type If required Default value Example Description <code>workflow_id</code> number yes - <code>4216617528</code> The workflow ID. The system instantiates a specified workflow as a job. The ID can be viewed in the upper left corner of the specified workflow page."},{"location":"nebula-explorer/workflow/workflow-api/api-post-jobs/#headers_parameters","title":"Headers parameters","text":"Parameters Type If required Default value Example Description <code>Content-Type</code> string yes - <code>application/json</code> The content type. <code>explorer_token</code> string yes - <code>eyJhbxxx</code> The authorization token that is used to verify account information. For details, see Workflow API overview."},{"location":"nebula-explorer/workflow/workflow-api/api-post-jobs/#body_parameters","title":"Body parameters","text":"<p>Note</p> <p>Users must ensure the rationality and correctness of the user-defined input parameters. Otherwise, the operation will fail.</p> Parameters Type If required Default value Example Description <code>input</code> object no - - The user-defined input parameters. \u00a0\u00a0\u00a0- <code>task_id</code> object no - <code>query_1</code> The task ID. Users can view the ID in the upper right corner of the component settings page. A task can set multiple parameters represented by key-value pairs. \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0- <code>param_name: param_value</code> string: {string or number} no - <code>param0: player100</code> <code>param_name</code> is the parameter key, that is, the parameter name. <code>param_value</code> is the parameter value."},{"location":"nebula-explorer/workflow/workflow-api/api-post-jobs/#request_example","title":"Request example","text":"<p>The following is an example of using the user-defined input parameter <code>name</code> in an nGQL statement. Pass in the parameter value <code>Tim Duncan</code> when creating a job.</p> <p></p> <pre><code>curl -i -X POST -H \"Content-Type: application/json\" -H \"Cookie: \"explorer_token=eyJhbxxx\"\" -d '{\"input\":{\"query_1\":{\"name\":\"Tim Duncan\"}}}' http://192.168.8.145:7002/api-open/v1/workflows/4216617528/jobs\n</code></pre>"},{"location":"nebula-explorer/workflow/workflow-api/api-post-jobs/#response_parameters","title":"Response parameters","text":"Parameters Type Example Description <code>code</code> number <code>0</code> The result code of the request. Return <code>0</code> if the request is successful, and return an error code if the request is unsuccessful. For details, see Workflow API overview. <code>message</code> string <code>Success</code> The result information of the execution. <code>data</code> object - The list of returned data. \u00a0\u00a0\u00a0- <code>id</code> string <code>107</code> The ID of the new job."},{"location":"nebula-explorer/workflow/workflow-api/api-post-jobs/#response_example","title":"Response example","text":"<pre><code>{\n  \"cookie\": [],\n  \"Content-Type\": \"application/json\",\n  \"Traceparent\": \"00-1ba128615cdc2226c921973a689e9f1b-7630b12963494672-00\",\n  \"Date\": \"Fri, 15 Jul 2022 07:19:25 GMT\",\n  \"Content-Length\": \"48\"\n}\n\n{\n  \"code\": 0,\n  \"data\": {\n    \"id\": 107\n  },\n  \"message\": \"Success\"\n}\n</code></pre>"},{"location":"nebula-explorer/workflow/workflow-api/workflow-api-overview/","title":"Workflow API overview","text":"<p>NebulaGraph Explorer provides some APIs for using workflow.</p> <p>The supported APIs are as follows:</p> <ul> <li>Add a new job</li> <li>Get a list of all jobs</li> <li>Get a list of jobs for a specified workflow</li> <li>Query details for a specified job</li> <li>Cancel a running job</li> <li>Get the result data of a specified task</li> </ul>"},{"location":"nebula-explorer/workflow/workflow-api/workflow-api-overview/#request_method","title":"Request method","text":"<p>Users can use curl to call APIs to achieve corresponding functions.</p> <p>The format is as follows:</p> <pre><code>curl &lt;options&gt; http://&lt;explorer_address&gt;:&lt;explorer_port&gt;/&lt;api_path&gt;?{&lt;body&gt;}\n</code></pre> <ul> <li><code>&lt;options&gt;</code>: Curl supports a large number of options. The most commonly used options for workflow are <code>-X</code>, <code>-H</code> and <code>-d</code>. For more information about options, see curl official documentation.</li> </ul> <ul> <li><code>&lt;explorer_address&gt;</code>: The access address of the NebulaGraph Explorer.</li> </ul> <ul> <li><code>&lt;explorer_port&gt;</code>: The access port of the NebulaGraph Explorer.</li> </ul> <ul> <li><code>&lt;api_path&gt;</code>: The call path of APIs. For example: <code>api-open/v1/jobs</code>.</li> </ul> <ul> <li><code>&lt;body&gt;</code>: The body parameters that needs to be supplied when calling APIs.</li> </ul>"},{"location":"nebula-explorer/workflow/workflow-api/workflow-api-overview/#get_authorization_token","title":"Get authorization token","text":"<p>Token information verification is required when calling an API. Run the following command to get the authorization token.</p> <pre><code>curl -i -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer &lt;account_base64_encode&gt;\" -d '{\"address\":\"&lt;nebula_address&gt;\",\"port\":&lt;nebula_port&gt;}' http://&lt;explorer_address&gt;:&lt;explorer_port&gt;/api-open/v1/connect\n</code></pre> <ul> <li><code>&lt;account_base64_encode&gt;</code>: The character string of the base64 encoded NebulaGraph account and password. Take the username <code>root</code> and password <code>123</code> as an example, the serialized string is <code>[\"root\", \"123\"]</code>. After the encoding, the result is <code>WyJyb290IiwiMTIzIl0=</code>.</li> <li><code>&lt;nebula_address&gt;</code>: The access address of the NebulaGraph.</li> <li><code>&lt;nebula_port&gt;</code>: The access port of the NebulaGraph.</li> <li><code>&lt;explorer_address&gt;</code>: The access address of the NebulaGraph Explorer.</li> <li><code>&lt;explorer_port&gt;</code>: The access port of the NebulaGraph Explorer.</li> </ul> <p>Example: </p> <pre><code>curl -i -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer WyJyb290IiwiMTIzIl0=\" -d '{\"address\":\"192.168.8.111\",\"port\":9669}' http://192.168.8.145:7002/api-open/v1/connect\n</code></pre> <p>Response:</p> <pre><code>HTTP/1.1 200 OK\nContent-Type: application/json\nSet-Cookie: explorer_token=eyJhbxxx; Path=/; # Max-Age=259200; HttpOnly\nTraceparent: 00-1c3f55cdbf81e13a2331ed88155ce0bf-2b97474943563f20-# 00\nDate: Thu, 14 Jul 2022 06:47:01 GMT\nContent-Length: 54\n\n{\n  \"code\": 0,\n  \"data\": {\n    \"success\": true\n  },\n  \"message\": \"Success\"\n}\n</code></pre> <p>Note the following parameters:</p> <ul> <li><code>explorer_token</code>: The authorization token.</li> </ul> <ul> <li><code>Max-Age</code>: Token validity time. Unit: second. The default value is 259,200 seconds, that is 3 days. You can change the default validity time in the <code>config/app-config.yaml</code> file in the installation directory.</li> </ul>"},{"location":"nebula-explorer/workflow/workflow-api/workflow-api-overview/#response","title":"Response","text":"<ul> <li> <p>If an API is called successfully, the system returns the following information:</p> <pre><code>{\n  code: 0,\n  message: 'Success',\n  data: &lt;ResponseData&gt;   //Return the results based on the API.\n}\n</code></pre> </li> </ul> <ul> <li> <p>If an API is called failed, the system returns the corresponding common error code. For example:</p> <pre><code>{\n  code: 40004000,\n  message: '&lt;ErrBadRequest&gt;',  //Display the error information.\n}\n</code></pre> <p>For descriptions of common error codes, see the following sections.</p> </li> </ul>"},{"location":"nebula-explorer/workflow/workflow-api/workflow-api-overview/#common_error_codes","title":"Common error codes","text":"Error code Information Description 40004000 <code>ErrBadRequest</code> Request error. 40004001 <code>ErrParam</code> Request parameter error. 40104000 <code>ErrUnauthorized</code> Request authorization error. 40104001 <code>ErrSession</code> Login session error. 40304000 <code>ErrForbidden</code> Request denied. 40404000 <code>ErrNotFound</code> Requested resource does not exist. 50004000 <code>ErrInternalServer</code> Internal service error. 50004001 <code>ErrInternalDatabase</code> Database error. 50004002 <code>ErrInternalController</code> Controller error. 50004003 <code>ErrInternalLicense</code> Certificate verification error. 90004000 <code>ErrUnknown</code> Unknown error."},{"location":"nebula-explorer/workflow/workflow-api/workflow-api-overview/#jobtask_status_code","title":"Job/Task status code","text":"Status code Description 0 Preparing 1 Running 2 Success 3 Failed 4 Interrupted 5 Stopping"},{"location":"nebula-importer/use-importer/","title":"NebulaGraph Importer","text":"<p>NebulaGraph Importer (Importer) is a standalone tool for importing data from CSV files into NebulaGraph. Importer can read and import CSV file data from multiple data sources.</p>"},{"location":"nebula-importer/use-importer/#features","title":"Features","text":"<ul> <li>Support multiple data sources, including local, S3, OSS, HDFS, FTP, and SFTP.</li> <li>Support importing data from CSV format files. A single file can contain multiple tags, multiple edge types or a mix of both.</li> <li>Support connecting to multiple Graph services simultaneously for importing and dynamic load balancing.</li> <li>Support reconnect or retry after failure.</li> <li>Support displaying statistics in multiple dimensions, including import time, import percentage, etc. Support for printing statistics in Console or logs.</li> </ul>"},{"location":"nebula-importer/use-importer/#advantage","title":"Advantage","text":"<ul> <li>Lightweight and fast: no complex environment can be used, fast data import.</li> </ul> <ul> <li>Flexible filtering: You can flexibly filter CSV data through configuration files.</li> </ul>"},{"location":"nebula-importer/use-importer/#version_compatibility","title":"Version compatibility","text":"<p>The version correspondence between NebulaGraph and NebulaGraph Importer is as follows.</p> NebulaGraph version NebulaGraph Importer version 3.x.x 3.x.x, 4.x.x 2.x.x 2.x.x, 3.x.x <p>Note</p> <p>Importer 4.0.0 has redone the Importer for improved performance, but the configuration file is not compatible with older versions. It is recommended to use the new version of Importer.</p>"},{"location":"nebula-importer/use-importer/#release_note","title":"Release note","text":"<p>Release</p>"},{"location":"nebula-importer/use-importer/#prerequisites","title":"Prerequisites","text":"<p>Before using NebulaGraph Importer, make sure:</p> <ul> <li> <p>NebulaGraph service has been deployed. There are currently three deployment modes:</p> <ul> <li>Deploy NebulaGraph with Docker Compose</li> </ul> <ul> <li>Install NebulaGraph with RPM or DEB package</li> </ul> <ul> <li>Install NebulaGraph by compiling the source code</li> </ul> </li> </ul> <ul> <li>Schema is created in NebulaGraph, including space, Tag and Edge type, or set by parameter <code>manager.hooks.before.statements</code>.</li> </ul>"},{"location":"nebula-importer/use-importer/#steps","title":"Steps","text":"<p>Prepare the CSV file to be imported and configure the YAML file to use the tool to batch write data into NebulaGraph.</p> <p>Note</p> <p>For details about the YAML configuration file, see Configuration File Description at the end of topic.</p>"},{"location":"nebula-importer/use-importer/#download_binary_package_and_run","title":"Download binary package and run","text":"<ol> <li> <p>Download the executable binary package.</p> <p>Note</p> <p>The file installation path based on the RPM/DEB package is <code>/usr/bin/nebula-importer</code>. </p> </li> <li> <p>Under the directory where the binary file is located, run the following command to start importing data.</p> <pre><code>./&lt;binary_file_name&gt; --config &lt;yaml_config_file_path&gt;\n</code></pre> </li> </ol>"},{"location":"nebula-importer/use-importer/#source_code_compile_and_run","title":"Source code compile and run","text":"<p>Compiling the source code requires deploying a Golang environment. For details, see Build Go environment.</p> <ol> <li> <p>Clone repository.</p> <pre><code>git clone -b release-4.0 https://github.com/vesoft-inc/nebula-importer.git\n</code></pre> <p>Note</p> <p>Use the correct branch. Different branches have different RPC protocols.</p> </li> <li> <p>Access the directory <code>nebula-importer</code>.</p> <pre><code>cd nebula-importer\n</code></pre> </li> <li> <p>Compile the source code.</p> <pre><code>make build\n</code></pre> </li> <li> <p>Start the service.</p> <pre><code>./bin/nebula-importer --config &lt;yaml_config_file_path&gt;\n</code></pre> </li> </ol>"},{"location":"nebula-importer/use-importer/#run_in_docker_mode","title":"Run in Docker mode","text":"<p>Instead of installing the Go locale locally, you can use Docker to pull the image of the NebulaGraph Importer and mount the local configuration file and CSV data file into the container. The command is as follows:</p> <pre><code>docker pull vesoft/nebula-importer:&lt;version&gt;\ndocker run --rm -ti \\\n      --network=host \\\n      -v &lt;config_file&gt;:&lt;config_file&gt; \\\n      -v &lt;data_dir&gt;:&lt;data_dir&gt; \\\n      vesoft/nebula-importer:&lt;version&gt; \\\n      --config &lt;config_file&gt;\n</code></pre> <ul> <li><code>&lt;config_file&gt;</code>: The absolute path to the YAML configuration file.</li> <li><code>&lt;data_dir&gt;</code>: The absolute path to the CSV data file. If the file is not local, ignore this parameter.</li> <li><code>&lt;version&gt;</code>: NebulaGraph 3.x Please fill in 'v3'.</li> </ul> <p>Note</p> <p>A relative path is recommended. If you use a local absolute path, check that the path maps to the path in the Docker.</p> <p>Example:</p> <pre><code>docker pull vesoft/nebula-importer:v4\ndocker run --rm -ti \\\n      --network=host \\\n      -v /home/user/config.yaml:/home/user/config.yaml \\\n      -v /home/user/data:/home/user/data \\\n      vesoft/nebula-importer:v4 \\\n      --config /home/user/config.yaml\n</code></pre>"},{"location":"nebula-importer/use-importer/#configuration_file_description","title":"Configuration File Description","text":"<p>Various example configuration files are available within the Github of the NebulaGraph Importer. The configuration files are used to describe information about the files to be imported, NebulaGraph server information, etc. The following section describes the fields within the configuration file in categories.</p> <p>Note</p> <p>If users download a binary package, create the configuration file manually.</p>"},{"location":"nebula-importer/use-importer/#client_configuration","title":"Client configuration","text":"<p>Client configuration stores the configuration associated with the client's connection to the NebulaGraph.</p> <p>The example configuration is as follows:</p> <pre><code>client:\n  version: v3\n  address: \"192.168.1.100:9669,192.168.1.101:9669\"\n  user: root\n  password: nebula\n  concurrencyPerAddress: 10\n  reconnectInitialInterval: 1s\n  retry: 3\n  retryInitialInterval: 1s\n</code></pre> Parameter Default value Required Description <code>client.version</code> <code>v3</code> Yes Specifies the major version of the NebulaGraph. Currently only <code>v3</code> is supported. <code>client.address</code> <code>\"127.0.0.1:9669\"</code> Yes Specifies the address of the NebulaGraph. Multiple addresses are separated by commas. <code>client.user</code> <code>root</code> No NebulaGraph user name. <code>client.password</code> <code>nebula</code> No The password for the NebulaGraph user name. <code>client.concurrencyPerAddress</code> <code>10</code> No The number of concurrent client connections for a single graph service. <code>client.retryInitialInterval</code> <code>1s</code> No Reconnect interval time. <code>client.retry</code> <code>3</code> No The number of retries for failed execution of the nGQL statement. <code>client.retryInitialInterval</code> <code>1s</code> No Retry interval time."},{"location":"nebula-importer/use-importer/#manager_configuration","title":"Manager configuration","text":"<p>Manager configuration is a human-controlled configuration after connecting to the database.</p> <p>The example configuration is as follows:</p> <pre><code>manager:\n  spaceName: basic_string_examples\n  batch: 128\n  readerConcurrency: 50\n  importerConcurrency: 512\n  statsInterval: 10s\n  hooks:\n    before:\n      - statements:\n        - UPDATE CONFIGS storage:wal_ttl=3600;\n        - UPDATE CONFIGS storage:rocksdb_column_family_options = { disable_auto_compactions = true };\n      - statements:\n        - |\n            DROP SPACE IF EXISTS basic_string_examples;\n            CREATE SPACE IF NOT EXISTS basic_string_examples(partition_num=5, replica_factor=1, vid_type=int);\n            USE basic_string_examples;\n        wait: 10s\n    after:\n      - statements:\n          - |\n            UPDATE CONFIGS storage:wal_ttl=86400;\n            UPDATE CONFIGS storage:rocksdb_column_family_options = { disable_auto_compactions = false };\n</code></pre> Parameter Default value Required Description <code>manager.spaceName</code> - Yes Specifies the NebulaGraph space to import the data into. Do not support importing multiple map spaces at the same time. <code>manager.batch</code> <code>128</code> No The batch size for executing statements (global configuration). Setting the batch size individually for a data source can using the parameter <code>sources.batch</code> below. <code>manager.readerConcurrency</code> <code>50</code> No The number of concurrent reads of the data source by the reader. <code>manager.importerConcurrency</code> <code>512</code> No The number of concurrent nGQL statements generated to be executed, and then will call the client to execute these nGQL statements. <code>manager.statsInterval</code> <code>10s</code> No The time interval for printing statistical information <code>manager.hooks.before.[].statements</code> - No The command to execute in the graph space before importing. <code>manager.hooks.before.[].wait</code> - No The wait time after <code>statements</code> are executed. <code>manager.hooks.after.[].statements</code> - No The commands  to execute in the graph space after importing. <code>manager.hooks.after.[].wait</code> - No The wait time after <code>statements</code> are executed."},{"location":"nebula-importer/use-importer/#log_configuration","title":"Log configuration","text":"<p>Log configuration is the logging-related configuration.</p> <p>The example configuration is as follows:</p> <pre><code>log:\n  level: INFO\n  console: true\n  files:\n   - logs/nebula-importer.log\n</code></pre> Parameter Default value Required Description <code>log.level</code> <code>INFO</code> No Specifies the log level. Optional values are <code>DEBUG</code>, <code>INFO</code>, <code>WARN</code>, <code>ERROR</code>, <code>PANIC</code>, <code>FATAL</code>. <code>log.console</code> <code>true</code> No Whether to print the logs to console synchronously when storing logs. <code>log.files</code> - No The log file path. The log directory must exist."},{"location":"nebula-importer/use-importer/#source_configuration","title":"Source configuration","text":"<p>The Source configuration requires the configuration of data source information, data processing methods, and Schema mapping.</p> <p>The example configuration is as follows:</p> <pre><code>sources:\n  - path: ./person.csv  # Required. Specifies the path where the data files are stored. If a relative path is used, the path and current configuration file directory are spliced. Wildcard filename is also supported, for example: ./follower-*.csv, please make sure that all matching files with the same schema.\n#  - s3: # AWS S3\n#      endpoint: endpoint    # Optional. The endpoint of S3 service, can be omitted if using AWS S3.\n#      region: us-east-1     # Required. The region of S3 service.\n#      bucket: gdelt-open-data    # Required. The bucket of file in S3 service.\n#      key: events/20190918.export.csv     # Required. The object key of file in S3 service.\n#      accessKeyID: \"\"    # Optional. The access key of S3 service. If it is public data, no need to configure.\n#      accessKeySecret: \"\"     # Optional. The secret key of S3 service. If it is public data, no need to configure.\n#  - oss:\n#      endpoint: https://oss-cn-hangzhou.aliyuncs.com    # Required. The endpoint of OSS service.\n#      bucket: bucketName    # Required. The bucket of file in OSS service.\n#      key: objectKey    # Required. The object key of file in OSS service.\n#      accessKeyID: accessKey    # Required. The access key of OSS service.\n#      accessKeySecret: secretKey    # Required. The secret key of OSS service.\n#  - ftp:\n#      host: 192.168.0.10    # Required. The host of FTP service.\n#      port: 21    # Required. The port of FTP service.\n#      user: user    # Required. The user of FTP service.\n#      password: password    # Required. The password of FTP service.\n#      path: \"/events/20190918.export.csv\"    # Required. The path of file in the FTP service.\n#  - sftp:\n#      host: 192.168.0.10    # Required. The host of SFTP service.\n#      port: 22    # Required. The port of SFTP service.\n#      user: user    # Required. The user of SFTP service.\n#      password: password    # Optional. The password of SFTP service.\n#      keyFile: keyFile    # Optional. The ssh key file path of SFTP service.\n#      keyData: keyData    $ Optional. The ssh key file content of SFTP service.\n#      passphrase: passphrase    # Optional. The ssh key passphrase of SFTP service.\n#      path: \"/events/20190918.export.csv\"    # Required. The path of file in the SFTP service.\n#  - hdfs:\n#      address: \"127.0.0.1:8020\"    # Required. The address of HDFS service.\n#      user: \"hdfs\"    # Optional. The user of HDFS service.\n#      path: \"/events/20190918.export.csv\"    # Required. The path of file in the HDFS service.\n    batch: 256\n    csv:\n      delimiter: \"|\"\n      withHeader: false\n      lazyQuotes: false\n    tags:\n    - name: Person\n      id:\n        type: \"STRING\"\n        function: \"hash\"\n#       index: 0        \n        concatItems:\n          - person_\n          - 0\n          - _id\n      props:\n        - name: \"firstName\"\n          type: \"STRING\"\n          index: 1\n        - name: \"lastName\"\n          type: \"STRING\"\n          index: 2\n        - name: \"gender\"\n          type: \"STRING\"\n          index: 3\n          nullable: true\n          defaultValue: female\n        - name: \"birthday\"\n          type: \"DATE\"\n          index: 4\n          nullable: true\n          nullValue: _NULL_\n        - name: \"creationDate\"\n          type: \"DATETIME\"\n          index: 5\n        - name: \"locationIP\"\n          type: \"STRING\"\n          index: 6\n        - name: \"browserUsed\"\n          type: \"STRING\"\n          index: 7\n  - path: ./knows.csv\n    batch: 256\n    edges:\n    - name: KNOWS # person_knows_person\n      src:\n        id:\n          type: \"STRING\"\n          concatItems:\n            - person_\n            - 0\n            - _id\n      dst:\n        id:\n          type: \"STRING\"\n          concatItems:\n            - person_\n            - 1\n            - _id\n      props:\n        - name: \"creationDate\"\n          type: \"DATETIME\"\n          index: 2\n          nullable: true\n          nullValue: _NULL_\n          defaultValue: 0000-00-00T00:00:00\n</code></pre> <p>The configuration mainly includes the following parts:</p> <ul> <li>Specify the data source information.</li> <li>Specifies the batch size for executing statements.</li> <li>Specifies the CSV file format information.</li> <li>Specifies the schema mapping for Tag.</li> <li>Specifies the schema mapping for Edge type.</li> </ul> Parameter Default value Required Description <code>sources.path</code><code>sources.s3</code><code>sources.oss</code><code>sources.ftp</code><code>sources.sftp</code><code>sources.hdfs</code> - No Specify data source information, such as local file, HDFS, and S3. Only one source can be configured for the <code>source</code>. Configure multiple sources in multiple <code>source</code>.See the comments in the example for configuration items for different data sources. <code>sources.batch</code> <code>256</code> No The batch size for executing statements when importing this data source. The priority is higher than <code>manager.batch</code>. <code>sources.csv.delimiter</code> <code>,</code> No Specifies the delimiter for the CSV file. Only 1-character string separators are supported. When using special characters as separators, they need to be escaped. For example, when the delimiter is <code>0x03</code> in hexadecimal, i.e. <code>Ctrl+C</code>, the escape is written as <code>\"\\x03\"</code> or <code>\"\\u0003\"</code>. For details on escaping special characters in yaml format, see Escaped Characters. <code>sources.csv.withHeader</code> <code>false</code> No Whether to ignore the first record in the CSV file. <code>sources.csv.lazyQuotes</code> <code>false</code> No Whether to allow lazy quotes. If <code>lazyQuotes</code> is true, a quote may appear in an unquoted field and a non-doubled quote may appear in a quoted field. <code>sources.tags.name</code> - Yes The tag name. <code>sources.tags.id.type</code> <code>STRING</code> No The type of the VID. <code>sources.tags.id.function</code> - No Functions to generate the VID. Currently, only function <code>hash</code> are supported. <code>sources.tags.id.index</code> - No The column number corresponding to the VID in the data file. If <code>sources.tags.id.concatItems</code> is not configured, this parameter must be configured. <code>sources.tags.id.concatItems</code> - No Used to concatenate two or more arrays, the concatenated items can be <code>string</code>, <code>int</code> or mixed. <code>string</code> stands for a constant, <code>int</code> for an index column. If this parameter is set, the <code>sources.tags.id.index</code> parameter will not take effect. <code>sources.tags.ignoreExistedIndex</code> <code>true</code> No Whether to enable <code>IGNORE_EXISTED_INDEX</code>, that is, do not update index after insertion vertex. <code>sources.tags.props.name</code> - Yes The tag property name, which must match the Tag property in the database. <code>sources.tags.props.type</code> <code>STRING</code> No Property data type, supporting <code>BOOL</code>, <code>INT</code>, <code>FLOAT</code>, <code>DOUBLE</code>, <code>STRING</code>, <code>TIME</code>, <code>TIMESTAMP</code>, <code>DATE</code>, <code>DATETIME</code>, <code>GEOGRAPHY</code>, <code>GEOGRAPHY(POINT)</code>, <code>GEOGRAPHY(LINESTRING)</code> and <code>GEOGRAPHY(POLYGON)</code>. <code>sources.tags.props.index</code> - Yes The property corresponds to the column number in the data file. <code>sources.tags.props.nullable</code> <code>false</code> No Whether this prop property can be <code>NULL</code>, optional values is <code>true</code> or <code>false</code>. <code>sources.tags.props.nullValue</code> - No Ignored when <code>nullable</code> is <code>false</code>. The value used to determine whether it is a <code>NULL</code>. The property is set to <code>NULL</code> when the value is equal to <code>nullValue</code>. <code>sources.tags.props.alternativeIndices</code> - No Ignored when <code>nullable</code> is <code>false</code>. The property is fetched from records according to the indices in order until not equal to <code>nullValue</code>. <code>sources.tags.props.defaultValue</code> - No Ignored when <code>nullable</code> is <code>false</code>. The property default value, when all the values obtained by <code>index</code> and <code>alternativeIndices</code> are <code>nullValue</code>. <code>sources.edges.name</code> - Yes The edge type name. <code>sources.edges.src.id.type</code> <code>STRING</code> No The data type of the VID at the starting vertex on the edge. <code>sources.edges.src.id.index</code> - Yes The column number in the data file corresponding to the VID at the starting vertex on the edge. <code>sources.edges.dst.id.type</code> <code>STRING</code> No The data type of the VID at the destination vertex on the edge. <code>sources.edges.dst.id.index</code> - Yes The column number in the data file corresponding to the VID at the destination vertex on the edge. <code>sources.edges.rank.index</code> - No The column number in the data file corresponding to the rank on the edge. <code>sources.edges.ignoreExistedIndex</code> <code>true</code> No Whether to enable <code>IGNORE_EXISTED_INDEX</code>, that is, do not update index after insertion vertex. <code>sources.edges.props.name</code> - Yes The edge type property name, which must match the Tag property in the database. <code>sources.edges.props.type</code> <code>STRING</code> No Property data type, supporting <code>BOOL</code>, <code>INT</code>, <code>FLOAT</code>, <code>DOUBLE</code>, <code>STRING</code>, <code>TIME</code>, <code>TIMESTAMP</code>, <code>DATE</code>, <code>DATETIME</code>, <code>GEOGRAPHY</code>, <code>GEOGRAPHY(POINT)</code>, <code>GEOGRAPHY(LINESTRING)</code> and <code>GEOGRAPHY(POLYGON)</code>. <code>sources.edges.props.index</code> - Yes The property corresponds to the column number in the data file. <code>sources.edges.props.nullable</code> - No Whether this prop property can be <code>NULL</code>, optional values is <code>true</code> or <code>false</code>. <code>sources.edges.props.nullValue</code> - No Ignored when <code>nullable</code> is <code>false</code>. The value used to determine whether it is a <code>NULL</code>. The property is set to <code>NULL</code> when the value is equal to <code>nullValue</code>. <code>sources.edges.props.defaultValue</code> - No Ignored when <code>nullable</code> is <code>false</code>. The property default value, when all the values obtained by <code>index</code> and <code>alternativeIndices</code> are <code>nullValue</code>. <p>Note</p> <p>The sequence numbers of the columns in the CSV file start from 0, that is, the sequence numbers of the first column are 0, and the sequence numbers of the second column are 1.</p>"},{"location":"nebula-operator/1.introduction-to-nebula-operator/","title":"What is NebulaGraph Operator","text":""},{"location":"nebula-operator/1.introduction-to-nebula-operator/#concept","title":"Concept","text":"<p>NebulaGraph Operator is a tool to automate the deployment, operation, and maintenance of NebulaGraph clusters on Kubernetes. Building upon the excellent scalability mechanism of Kubernetes, NebulaGraph introduced its operation and maintenance knowledge into the Kubernetes system, which makes NebulaGraph a real cloud-native graph database.</p> <p></p>"},{"location":"nebula-operator/1.introduction-to-nebula-operator/#how_it_works","title":"How it works","text":"<p>For resource types that do not exist within Kubernetes, you can register them by adding custom API objects. The common way is to use the CustomResourceDefinition.</p> <p>NebulaGraph Operator abstracts the deployment management of NebulaGraph clusters as a CRD. By combining multiple built-in API objects including StatefulSet, Service, and ConfigMap, the routine management and maintenance of a NebulaGraph cluster are coded as a control loop in the Kubernetes system. When a CR instance is submitted, NebulaGraph Operator drives database clusters to the final state according to the control process.</p>"},{"location":"nebula-operator/1.introduction-to-nebula-operator/#features","title":"Features","text":"<p>The following features are already available in NebulaGraph Operator:</p> <ul> <li>Deploy and uninstall clusters: NebulaGraph Operator simplifies the process of deploying and uninstalling clusters for users. NebulaGraph Operator allows you to quickly create, update, or delete a NebulaGraph cluster by simply providing the corresponding CR file. For more information, see Deploy NebulaGraph Clusters with Kubectl or Deploy NebulaGraph Clusters with Helm.</li> </ul> <ul> <li>Scale clusters: NebulaGraph Operator calls NebulaGraph's native scaling interfaces in a control loop to implement the scaling logic. You can simply perform scaling operations with YAML configurations and ensure the stability of data. For more information, see Scale clusters with Kubectl or Scale clusters with Helm.</li> </ul> <ul> <li>Backup and Recovery\uff1aNebulaGraph supports data backup and recovery. Users can use NebulaGraph Operator to backup the data of the NebulaGraph cluster to storage services that are compatible with the S3 protocol, and can also restore data to the cluster from the storage service. For details, see Backup and restore using NebulaGraph Operator.</li> </ul> <ul> <li>Cluster Upgrade: NebulaGraph Operator supports cluster upgrading from version 3.5.0 to version 3.5.x.</li> </ul> <ul> <li>Self-Healing: NebulaGraph Operator calls interfaces provided by NebulaGraph clusters to dynamically sense cluster service status. Once an exception is detected, NebulaGraph Operator performs fault tolerance. For more information, see Self-Healing.</li> </ul> <ul> <li>Balance Scheduling: Based on the scheduler extension interface, the scheduler provided by NebulaGraph Operator evenly distributes Pods in a NebulaGraph cluster across all nodes.</li> </ul>"},{"location":"nebula-operator/1.introduction-to-nebula-operator/#limitations","title":"Limitations","text":""},{"location":"nebula-operator/1.introduction-to-nebula-operator/#version_limitations","title":"Version limitations","text":"<p>NebulaGraph Operator does not support the v1.x version of NebulaGraph. NebulaGraph Operator version and the corresponding NebulaGraph version are as follows:</p> NebulaGraph NebulaGraph Operator 3.5.x 1.5.0, 1.6.x 3.0.0 ~ 3.4.1 1.3.0, 1.4.0 ~ 1.4.2 3.0.0 ~ 3.3.x 1.0.0, 1.1.0, 1.2.0 2.5.x ~ 2.6.x 0.9.0 2.5.x 0.8.0 <p>Legacy version compatibility</p> <ul> <li>The 1.x version NebulaGraph Operator is not compatible with NebulaGraph of version below v3.x.</li> <li>Starting from NebulaGraph Operator 0.9.0, logs and data are stored separately. Using NebulaGraph Operator 0.9.0 or later versions to manage a NebulaGraph 2.5.x cluster created with Operator 0.8.0 can cause compatibility issues. You can backup the data of the NebulaGraph 2.5.x cluster and then create a 2.6.x cluster with Operator 0.9.0.</li> </ul>"},{"location":"nebula-operator/1.introduction-to-nebula-operator/#feature_limitations","title":"Feature limitations","text":"<p>The NebulaGraph Operator scaling feature is only available for the Enterprise Edition of NebulaGraph clusters and does not support scaling the Community Edition version of NebulaGraph clusters.</p>"},{"location":"nebula-operator/1.introduction-to-nebula-operator/#release_note","title":"Release note","text":"<p>Release</p>"},{"location":"nebula-operator/10.backup-restore-using-operator/","title":"Backup and restore data using NebulaGraph Operator","text":"<p>This article introduces how to back up and restore data of the NebulaGraph cluster on Kubernetes.</p> <p>Enterpriseonly</p> <p>This feature is only for the enterprise edition NebulaGraph clusters on Kubernetes.</p>"},{"location":"nebula-operator/10.backup-restore-using-operator/#overview","title":"Overview","text":"<p>NebulaGraph BR (Enterprise Edition) is a command line tool for data backup and recovery of NebulaGraph enterprise edition. NebulaGraph Operator is based on the BR tool to achieve data backup and recovery for NebulaGraph clusters on Kubernetes.</p> <p>When backing up data, NebulaGraph Operator creates a Job to back up the data in the NebulaGraph cluster to the specified storage service.</p> <p>When restoring data, NebulaGraph Operator checks the specified backup NebulaGraph cluster for existence, and whether the access to remote storage is normally based on the information defined in the NebulaRestore resource object. It then creates a new cluster and restores the backup data to the new NebulaGraph cluster. For more information, see restore flowchart.</p>"},{"location":"nebula-operator/10.backup-restore-using-operator/#prerequisites","title":"Prerequisites","text":"<p>To backup and restore data using NebulaGraph Operator, the following conditions must be met:</p> <ul> <li>Nebula Operator version &gt;= 1.4.0.</li> <li>The enterprise edition NebulaGraph cluster deployed on Kubernetes is running.</li> <li> <p>In the YAML file used to create the cluster, <code>spec.enableBR</code> is set to true.</p> <pre><code>// Partial content of a sample cluster YAML file.\napiVersion: apps.nebula-graph.io/v1alpha1\nkind: NebulaCluster\nmetadata:\n  name: nebula\nspec:  \nenableBR: true // Set to true to enable the backup and restore function.\n...\n</code></pre> </li> </ul> <ul> <li>Only storage services that use the S3 protocol (such as AWS S3, Minio, etc.) can be used to back up and restore data.</li> <li>Sufficient computing resources are available in the cluster to restore data.</li> </ul>"},{"location":"nebula-operator/10.backup-restore-using-operator/#backup","title":"Backup","text":""},{"location":"nebula-operator/10.backup-restore-using-operator/#notes","title":"Notes","text":"<ul> <li>NebulaGraph Operator supports full and incremental backups.</li> <li>During data backup, DDL and DML statements in the specified graph space will be blocked. We recommend performing the operation during off-peak hours, such as from 2:00 am to 5:00 am.</li> <li>The cluster executing incremental backups and the cluster specified for the last backup must be the same, and the (storage bucket) path for the last backup must be the same.</li> <li>Ensure that the time between each incremental backup and the last backup is less than a <code>wal_ttl</code>.</li> <li>Specifying the backup data of a specified graph space is not supported.</li> <li> <p>Before backing up data, you need to create a Secret to restore the credential for pulling the image of the BR-ent tool.</p> <pre><code>kubectl - &lt;nebula&gt; create secret docker-registry &lt;br-ent-secret&gt; \\\n--docker-server=REGISTRY_SERVER \\\n--docker-username=REGISTRY_USERNAME \\\n--docker-password=REGISTRY_PASSWORD \\\n</code></pre> <ul> <li><code>&lt;nebula&gt;</code>: The namespace where the Secret is located.</li> <li><code>&lt;br-ent-secret&gt;</code>: The name of the Secret.</li> <li><code>REGISTRY_SERVER</code>: The address of the private image repository server from which the image is pulled, for example, <code>reg.example-inc.com</code>.</li> <li><code>REGISTRY_USERNAME</code>: The username for logging in to the private image repository server.</li> <li><code>REGISTRY_PASSWORD</code>: The password for logging in to the private image repository server.</li> </ul> </li> </ul>"},{"location":"nebula-operator/10.backup-restore-using-operator/#full_backup","title":"Full backup","text":"<p>When backing up data to a storage service compatible with the S3 protocol, you need to create a backup Job, which will back up the full NebulaGraph data to the specified storage location.</p> <p>Here is an example of the YAML file for a full backup Job:</p> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: nebula-full-backup\nspec:\n  parallelism: 1\n  ttlSecondsAfterFinished: 600\n  template:\n    spec:\n      restartPolicy: OnFailure\n      imagePullSecrets:\n        - name: br-ent-secret  # The name of the Secret for pulling the image of the BR-ent tool.\n      containers:\n        - image: reg.vesoft-inc.com/cloud-dev/br-ent:v3.5.1\n          imagePullPolicy: Always\n          name: backup\n          command:\n            - /bin/sh\n            - -ecx\n            - 'exec /usr/local/bin/br-ent backup full \n            --meta nebula-metad-0.nebula-metad-headless.nebula.svc.cluster.local:9559   # The address of the Metad service. \n            --storage s3://BUCKET                           # The storage location of the backup file.\n            --s3.access_key ACCESS_KEY                      # The AccessKey for accessing the S3 protocol-compatible storage service.\n            --s3.secret_key SECRET_KEY                      # The SecretKey for accessing the S3 protocol-compatible storage service.\n            --s3.region REGION                              # The region of the S3 protocol-compatible storage service.\n            --s3.endpoint https://s3.REGION.amazonaws.com'  # The endpoint of the S3 protocol-compatible storage service.\n</code></pre>"},{"location":"nebula-operator/10.backup-restore-using-operator/#incremental_backup","title":"Incremental backup","text":"<p>Except for the name of the Job and the command specified in <code>spec.template.spec.containers[0].command</code>, the YAML file for incremental backup is the same as that for a full backup. Here is an example of the YAML file for incremental backup:</p> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: nebula-incr-backup\nspec:\n  parallelism: 1\n  ttlSecondsAfterFinished: 60\n  template:\n    spec:\n      restartPolicy: OnFailure\n      imagePullSecrets:\n        - name: br-ent-secret\n      containers:\n        - image: reg.vesoft-inc.com/cloud-dev/br-ent:v3.5.1\n          imagePullPolicy: Always\n          name: backup\n          command:\n            - /bin/sh\n            - -ecx\n            - 'exec /usr/local/bin/br-ent backup incr\n            --meta nebula-metad-0.nebula-metad-headless.nebula.svc.cluster.local:9559 # The address of the Metad service. \n            --storage s3://BUCKET                           # The storage location of the backup file.\n            --s3.access_key ACCESS_KEY                      # The AccessKey for accessing the S3 protocol-compatible storage service.\n            --s3.secret_key SECRET_KEY                      # The SecretKey for accessing the S3 protocol-compatible storage service.\n            --s3.region REGION                              # The region of the S3 protocol-compatible storage service.\n            --s3.endpoint https://s3.REGION.amazonaws.com'  # The endpoint of the S3 protocol-compatible storage service.\n</code></pre>"},{"location":"nebula-operator/10.backup-restore-using-operator/#parameter_description","title":"Parameter description","text":"<p>The main parameters are described as follows:</p> Parameter Default value Description <code>spec.parallelism</code> 1 The number of tasks executed in parallel. <code>spec.ttlSecondsAfterFinished</code> 60 The time to keep task information after the task is completed. <code>spec.template.spec.containers[0].image</code> <code>vesoft/br-ent:3.5.1</code> The image address of the NebulaGraph BR Enterprise Edition tool. <code>spec.template.spec.containers[0].command</code> - The command for backing up data to the storage service compatible with the S3 protocol.For descriptions of the options in the command, see Parametr description. <p>For more settings of the Job, see Kubernetes Jobs.</p> <p>After the YAML file for the backup Job is set, run the following command to start the backup Job:</p> <pre><code>kubectl apply -f &lt;backup_file_name&gt;.yaml\n</code></pre> <p>When the data backup succeeds, a backup file is generated in the specified storage location. For example, the backup file name is <code>BACKUP_2023_02_12_10_04_16</code>.</p>"},{"location":"nebula-operator/10.backup-restore-using-operator/#restore","title":"Restore","text":""},{"location":"nebula-operator/10.backup-restore-using-operator/#notes_1","title":"Notes","text":"<ul> <li>After the data recovery is successful, a new cluster will be created and the old cluster will not be deleted by default. You can decide whether to delete the old cluster themselves. The name of the new cluster is automatically generated by the Operator.</li> <li>There will be a period of service unavailability during the data recovery process, so it is recommended to perform the operation during a low period of business activity.</li> </ul>"},{"location":"nebula-operator/10.backup-restore-using-operator/#process","title":"Process","text":"<p>When restoring data from a compatible S3 protocol service, you need to create a Secret to store the credentials for accessing the compatible S3 protocol service. Then create a resource object (NebulaRestore) for restoring the data, which will instruct the Operator to create a new NebulaGraph cluster based on the information defined in this resource object and restore the backup data to the newly created cluster.</p> <p>Here is an example YAML for restoring data based on the backup file <code>BACKUP_2023_02_12_10_04_16</code>:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: aws-s3-secret\ntype: Opaque\ndata:\n  access-key: QVNJQVE0WFlxxx\n  secret-key: ZFJ6OEdNcDdxenMwVGxxx\n---\napiVersion: apps.nebula-graph.io/v1alpha1\nkind: NebulaRestore\nmetadata:\n  name: restore1\nspec:\n  br:\n    clusterName: nebula\n    backupName: \"BACKUP_2023_02_12_10_04_16\"\n    concurrency: 5\n    s3:\n      region: \"us-west-2\"\n      bucket: \"nebula-br-test\"\n      endpoint: \"https://s3.us-west-2.amazonaws.com\"\n      secretName: \"aws-s3-secret\"\n</code></pre>"},{"location":"nebula-operator/10.backup-restore-using-operator/#parameter_description_1","title":"Parameter Description","text":"<ul> <li> <p>Secret</p> Parameter Default Value Description <code>metadata.name</code> - The name of the Secret. <code>type</code> <code>Opaque</code> The type of the Secret. See Types of Secret for more information. <code>data.access-key</code> - The AccessKey for accessing the S3 protocol-compatible storage service. <code>data.secret-key</code> - The SecretKey for accessing the S3 protocol-compatible storage service. </li> </ul> <ul> <li> <p>NebulaRestore</p> Parameter Default Value Description <code>metadata.name</code> - The name of the resource object NebulaRestore. <code>spec.br.clusterName</code> - The name of the backup cluster. Backup based on this cluster, then create a new cluster with a name automatically generated. <code>spec.br.backupName</code> - The name of the backup file. Restore data based on this backup file. <code>spec.br.concurrency</code> <code>5</code> The number of concurrent downloads when restoring data. The default value is <code>5</code>. <code>spec.br.s3.region</code> - The geographical region where the S3 storage bucket is located. <code>spec.br.s3.bucket</code> - The path of the S3 storage bucket where backup data is stored. <code>spec.br.s3.endpoint</code> - The access address of the S3 storage bucket. <code>spec.br.s3.secretName</code> - The name of the Secret that is used to access the S3 storage bucket. </li> </ul> <p>After setting up the YAML file for restoring the data, run the following command to start the restore job:</p> <pre><code>kubectl apply -f &lt;restore_file_name&gt;.yaml\n</code></pre> <p>Run the following command to check the status of the NebulaRestore object.</p> <pre><code>kubectl get rt &lt;NebulaRestore_name&gt; -n &lt;namespace&gt;\n\n# Output example:\nNAME       STATUS     STARTED   COMPLETED   AGE\nrestore1   Complete   67m       59m         67m\n</code></pre> <p>After the restore job is completed, a new NebulaGraph cluster is created with the name automatically generated by the Operator. To check the status of the new cluster:</p> <pre><code>kubectl get nc -n &lt;namespace&gt;\n\n# Output example:\n\nNAME     GRAPHD-DESIRED   GRAPHD-READY   METAD-DESIRED   METAD-READY   STORAGED-DESIRED   STORAGED-READY   AGE\nnebula   1                1              1               1             3                  3                2d3h\nngxvsm   1                1              1               1             3                  3                92m  # The newly created cluster.\n</code></pre>"},{"location":"nebula-operator/11.rolling-update-strategy/","title":"NebulaGraph cluster rolling update strategy","text":"<p>NebulaGraph clusters use a distributed architecture to divide data into multiple logical partitions, which are typically evenly distributed across different nodes. In distributed systems, there are usually multiple replicas of the same data. To ensure the consistency of data across multiple replicas, NebulaGraph clusters use the Raft protocol to synchronize multiple partition replicas. In the Raft protocol, each partition elects a leader replica, which is responsible for handling write requests, while follower replicas handle read requests.</p> <p>When a NebulaGraph cluster created by NebulaGraph Operator performs a rolling update, a storage node temporarily stops providing services for the update. For an overview of rolling updates, see Performing a Rolling Update. If the node hosting the leader replica stops providing services, it will result in the unavailability of read and write operations for that partition. To avoid this situation, by default, Operator migrates the leader replicas to other unaffected nodes during the rolling update process of a NebulaGraph cluster. This way, when a storage node is being updated, the leader replicas on other nodes can continue processing client requests, ensuring the read and write availability of the cluster.</p> <p>The process of migrating all leader replicas from one storage node to the other nodes may take a long time. To better control the rolling update duration, Operator provides a field called <code>enableForceUpdate</code>. When it is confirmed that there is no external access traffic, you can set this field to <code>true</code>. This way, the leader replicas will not be migrated to other nodes, thereby speeding up the rolling update process.</p>"},{"location":"nebula-operator/11.rolling-update-strategy/#rolling_update_trigger_conditions","title":"Rolling update trigger conditions","text":"<p>Operator triggers a rolling update of the NebulaGraph cluster under the following circumstances:</p> <ul> <li>The version of the NebulaGraph cluster changes.</li> <li>The configuration of the NebulaGraph cluster changes.</li> </ul>"},{"location":"nebula-operator/11.rolling-update-strategy/#specify_a_rolling_update_strategy","title":"Specify a rolling update strategy","text":"<p>In the YAML file for creating a cluster instance, add the <code>spec.storaged.enableForceUpdate</code> field and set it to <code>true</code> or <code>false</code> to control the rolling update speed.</p> <p>When <code>enableForceUpdate</code> is set to <code>true</code>, it means that the partition leader replicas will not be migrated, thus speeding up the rolling update process. Conversely, when set to <code>false</code>, it means that the leader replicas will be migrated to other nodes to ensure the read and write availability of the cluster. The default value is <code>false</code>.</p> <p>Caution</p> <p>When setting <code>enableForceUpdate</code> to <code>true</code>, make sure there is no traffic entering the cluster for read and write operations. This is because this setting will force the cluster pods to be rebuilt, and during this process, data loss or client request failures may occur.</p> <p>Configuration example:</p> <pre><code>...\nspec:\n...\n  storaged:\n    enableForceUpdate: true // When set to true, it speeds up the rolling update process.\n    ...\n</code></pre>"},{"location":"nebula-operator/2.deploy-nebula-operator/","title":"Deploy NebulaGraph Operator","text":"<p>You can deploy NebulaGraph Operator with Helm.</p>"},{"location":"nebula-operator/2.deploy-nebula-operator/#background","title":"Background","text":"<p>NebulaGraph Operator automates the management of NebulaGraph clusters, and eliminates the need for you to install, scale, upgrade, and uninstall NebulaGraph clusters, which lightens the burden on managing different application versions.</p>"},{"location":"nebula-operator/2.deploy-nebula-operator/#prerequisites","title":"Prerequisites","text":"<p>Before installing NebulaGraph Operator, you need to install the following software and ensure the correct version of the software :</p> Software Requirement Kubernetes &gt;= 1.16 Helm &gt;= 3.2.0 CoreDNS &gt;= 1.6.0 <p>Note</p> <ul> <li>If using a role-based access control policy, you need to enable RBAC (optional).</li> </ul> <ul> <li>CoreDNS is a flexible and scalable DNS server that is installed for Pods in NebulaGraph clusters.</li> </ul>"},{"location":"nebula-operator/2.deploy-nebula-operator/#steps","title":"Steps","text":""},{"location":"nebula-operator/2.deploy-nebula-operator/#install_nebulagraph_operator","title":"Install NebulaGraph Operator","text":"<ol> <li> <p>Add the NebulaGraph Operator Helm repository.</p> <pre><code>helm repo add nebula-operator https://vesoft-inc.github.io/nebula-operator/charts\n</code></pre> </li> <li> <p>Update information of available charts locally from repositories.</p> <pre><code>helm repo update\n</code></pre> <p>For more information about <code>helm repo</code>, see Helm Repo.</p> </li> <li> <p>Create a namespace for NebulaGraph Operator.</p> <pre><code>kubectl create namespace &lt;namespace_name&gt;\n</code></pre> <p>For example, run the following command to create a namespace named <code>nebula-operator-system</code>.</p> <pre><code>kubectl create namespace nebula-operator-system\n</code></pre> <ul> <li>All the resources of NebulaGraph Operator are deployed in this namespace.</li> <li>You can also use a different name.</li> </ul> </li> <li> <p>Install NebulaGraph Operator.</p> <pre><code>helm install nebula-operator nebula-operator/nebula-operator --namespace=&lt;namespace_name&gt; --version=${chart_version}\n</code></pre> <p>For example, the command to install NebulaGraph Operator of version 1.6.2 is as follows.</p> <pre><code>helm install nebula-operator nebula-operator/nebula-operator --namespace=nebula-operator-system --version=1.6.2\n</code></pre> <ul> <li><code>nebula-operator-system</code> is a user-created namespace name. If you have not created this namespace, run <code>kubectl create namespace nebula-operator-system</code> to create one. You can also use a different name.</li> </ul> <ul> <li><code>1.6.2</code> is the version of the nebula-operator chart. When not specifying <code>--version</code>, the latest version of the nebula-operator chart is used by default. Run <code>helm search repo -l nebula-operator</code> to see chart versions.</li> </ul> <p>You can customize the configuration items of the NebulaGraph Operator chart before running the installation command. For more information, see Customize Helm charts below.</p> </li> </ol>"},{"location":"nebula-operator/2.deploy-nebula-operator/#customize_helm_charts","title":"Customize Helm charts","text":"<p>When executing the <code>helm install [NAME] [CHART] [flags]</code> command to install a chart, you can specify the chart configuration. For more information, see Customizing the Chart Before Installing.</p> <p>View the related configuration options in the nebula-operator chart configuration file.</p> <p>Alternatively, you can view the configurable options through the command <code>helm show values nebula-operator/nebula-operator</code>, as shown below.</p> <p>For example:</p> <pre><code>[k8s@master ~]$ helm show values nebula-operator/nebula-operator\nimage:\n  nebulaOperator:\n    image: vesoft/nebula-operator:v1.6.2\n    imagePullPolicy: Always\n  kubeRBACProxy:\n    image: bitnami/kube-rbac-proxy:0.14.2\n    imagePullPolicy: Always\n  kubeScheduler:\n    image: registry.k8s.io/kube-scheduler:v1.24.11\n    imagePullPolicy: Always\n\nimagePullSecrets: []\nkubernetesClusterDomain: \"\"\n\ncontrollerManager:\n  create: true\n  replicas: 2\n  env: []\n  resources:\n    limits:\n      cpu: 200m\n      memory: 200Mi\n    requests:\n      cpu: 100m\n      memory: 100Mi\n\nadmissionWebhook:\n  create: false\n\nscheduler:\n  create: true\n  schedulerName: nebula-scheduler\n  replicas: 2\n  env: []\n  resources:\n    limits:\n      cpu: 200m\n      memory: 20Mi\n    requests:\n      cpu: 100m\n      memory: 100Mi\n</code></pre> <p>Part of the above parameters are described as follows:</p> Parameter Default value Description <code>image.nebulaOperator.image</code> <code>vesoft/nebula-operator:v1.6.2</code> The image of NebulaGraph Operator, version of which is 1.6.2. <code>image.nebulaOperator.imagePullPolicy</code> <code>IfNotPresent</code> The image pull policy in Kubernetes. <code>imagePullSecrets</code> - The image pull secret in Kubernetes. <code>kubernetesClusterDomain</code> <code>cluster.local</code> The cluster domain. <code>controllerManager.create</code> <code>true</code> Whether to enable the controller-manager component. <code>controllerManager.replicas</code> <code>2</code> The numeric value of controller-manager replicas. <code>admissionWebhook.create</code> <code>false</code> Whether to enable Admission Webhook. This option is disabled. To enable it, set the value to <code>true</code> and you will need to install cert-manager. <code>shceduler.create</code> <code>true</code> Whether to enable Scheduler. <code>shceduler.schedulerName</code> <code>nebula-scheduler</code> The Scheduler name. <code>shceduler.replicas</code> <code>2</code> The numeric value of nebula-scheduler replicas. <p>You can run <code>helm install [NAME] [CHART] [flags]</code> to specify chart configurations when installing a chart. For more information, see Customizing the Chart Before Installing.</p> <p>The following example shows how to specify the NebulaGraph Operator's AdmissionWebhook mechanism to be turned on when you install NebulaGraph Operator (AdmissionWebhook is disabled by default):</p> <pre><code>helm install nebula-operator nebula-operator/nebula-operator --namespace=&lt;nebula-operator-system&gt; --set admissionWebhook.create=true\n</code></pre> <p>For more information about <code>helm install</code>, see Helm Install.</p>"},{"location":"nebula-operator/2.deploy-nebula-operator/#update_nebulagraph_operator","title":"Update NebulaGraph Operator","text":"<ol> <li> <p>Update the information of available charts locally from chart repositories.</p> <pre><code>helm repo update\n</code></pre> </li> <li> <p>Update NebulaGraph Operator by passing configuration parameters via <code>--set</code>.</p> <ul> <li><code>--set</code>\uff1aOverrides values using the command line. For configurable items, see the above-mentioned section Customize Helm charts.</li> </ul> <p>For example, to enable the AdmissionWebhook, run the following command:</p> <pre><code>helm upgrade nebula-operator nebula-operator/nebula-operator --namespace=nebula-operator-system --version=1.6.2 --set admissionWebhook.create=true\n</code></pre> <p>For more information, see Helm upgrade.</p> </li> </ol>"},{"location":"nebula-operator/2.deploy-nebula-operator/#upgrade_nebulagraph_operator","title":"Upgrade NebulaGraph Operator","text":"<p>Legacy version compatibility</p> <ul> <li>Does not support upgrading 0.9.0 and below version NebulaGraph Operator to 1.x.</li> <li>The 1.x version NebulaGraph Operator is not compatible with NebulaGraph of version below v3.x.</li> </ul> <ol> <li> <p>Update the information of available charts locally from chart repositories.</p> <pre><code>helm repo update\n</code></pre> </li> <li> <p>Upgrade Operator to v1.6.2.</p> <pre><code>helm upgrade nebula-operator nebula-operator/nebula-operator --namespace=&lt;namespace_name&gt; --version=1.6.2\n</code></pre> <p>For example:</p> <pre><code>helm upgrade nebula-operator nebula-operator/nebula-operator --namespace=nebula-operator-system --version=1.6.2\n</code></pre> <p>Output:</p> <pre><code>Release \"nebula-operator\" has been upgraded. Happy Helming!\nNAME: nebula-operator\nLAST DEPLOYED: Tue Apr 16 02:21:08 2022\nNAMESPACE: nebula-operator-system\nSTATUS: deployed\nREVISION: 3\nTEST SUITE: None\nNOTES:\nNebulaGraph Operator installed!\n</code></pre> </li> <li> <p>Pull the latest CRD configuration file.</p> <p>Note</p> <p>You need to upgrade the corresponding CRD configurations after NebulaGraph Operator is upgraded. Otherwise, the creation of NebulaGraph clusters will fail. For information about the CRD configurations, see apps.nebula-graph.io_nebulaclusters.yaml.</p> <ol> <li> <p>Pull the NebulaGraph Operator chart package.</p> <pre><code>helm pull nebula-operator/nebula-operator --version=1.6.2\n</code></pre> <ul> <li><code>--version</code>: The NebulaGraph Operator version you want to upgrade to. If not specified, the latest version will be pulled.</li> </ul> </li> <li> <p>Run <code>tar -zxvf</code> to unpack the charts.</p> <p>For example: To unpack v1.6.2 chart to the <code>/tmp</code> path, run the following command:</p> <pre><code>tar -zxvf nebula-operator-1.6.2.tgz -C /tmp\n</code></pre> <ul> <li><code>-C /tmp</code>: If not specified, the chart files will be unpacked to the current directory.</li> </ul> </li> </ol> </li> <li> <p>Upgrade the CRD configuration file in the <code>nebula-operator</code> directory.</p> <pre><code>kubectl apply -f crds/nebulacluster.yaml\n</code></pre> <p>Output:</p> <pre><code>customresourcedefinition.apiextensions.k8s.io/nebulaclusters.apps.nebula-graph.io configured\n</code></pre> </li> </ol>"},{"location":"nebula-operator/2.deploy-nebula-operator/#uninstall_nebulagraph_operator","title":"Uninstall NebulaGraph Operator","text":"<ol> <li> <p>Uninstall the NebulaGraph Operator chart.</p> <pre><code>helm uninstall nebula-operator --namespace=&lt;nebula-operator-system&gt;\n</code></pre> </li> <li> <p>Delete CRD.</p> <pre><code>kubectl delete crd nebulaclusters.apps.nebula-graph.io\n</code></pre> </li> </ol>"},{"location":"nebula-operator/2.deploy-nebula-operator/#whats_next","title":"What's next","text":"<p>Automate the deployment of NebulaGraph clusters with NebulaGraph Operator. For more information, see Deploy NebulaGraph Clusters with Kubectl or Deploy NebulaGraph Clusters with Helm.</p> <p>For the NebulaGraph Enterprise Edition deployment, you need first to deploy the License Manager and have the license key loaded. For more information, see Deploy LM.</p>"},{"location":"nebula-operator/4.connect-to-nebula-graph-service/","title":"Connect to NebulaGraph databases with Nebular Operator","text":"<p>After creating a NebulaGraph cluster with NebulaGraph Operator on Kubernetes, you can connect to NebulaGraph databases from within the cluster and outside the cluster.</p>"},{"location":"nebula-operator/4.connect-to-nebula-graph-service/#prerequisites","title":"Prerequisites","text":"<p>Create a NebulaGraph cluster with NebulaGraph Operator on Kubernetes. For more information, see Deploy NebulaGraph clusters with Kubectl or Deploy NebulaGraph clusters with Helm.</p>"},{"location":"nebula-operator/4.connect-to-nebula-graph-service/#connect_to_nebulagraph_databases_from_outside_a_nebulagraph_cluster_via_nodeport","title":"Connect to NebulaGraph databases from outside a NebulaGraph cluster via <code>NodePort</code>","text":"<p>You can create a <code>NodePort</code> type Service to access internal cluster services from outside the cluster using any node IP and the exposed node port. You can also utilize load balancing services provided by cloud vendors (such as Azure, AWS, etc.) by setting the Service type to <code>LoadBalancer</code>. This allows external access to internal cluster services through the public IP and port of the load balancer provided by the cloud vendor.</p> <p>The Service of type <code>NodePort</code> forwards the front-end requests via the label selector <code>spec.selector</code> to Graphd pods with labels <code>app.kubernetes.io/cluster: &lt;cluster-name&gt;</code> and <code>app.kubernetes.io/component: graphd</code>.</p> <p>After creating a NebulaGraph cluster based on the example template, where <code>spec.graphd.service.type=NodePort</code>, the NebulaGraph Operator will automatically create a NodePort type Service named <code>&lt;cluster-name&gt;-graphd-svc</code> in the same namespace. You can directly connect to the NebulaGraph database through any node IP and the exposed node port (see step 4 below). You can also create a custom Service according to your needs.</p> <p>Steps:</p> <ol> <li> <p>Create a YAML file named <code>graphd-nodeport-service.yaml</code>. The file contents are as follows:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/cluster: nebula\n    app.kubernetes.io/component: graphd\n    app.kubernetes.io/managed-by: nebula-operator\n    app.kubernetes.io/name: nebula-graph\n  name: nebula-graphd-svc-nodeport\n  namespace: default\nspec:\n  externalTrafficPolicy: Local\n  ports:\n  - name: thrift\n    port: 9669\n    protocol: TCP\n    targetPort: 9669\n  - name: http\n    port: 19669\n    protocol: TCP\n    targetPort: 19669\n  selector:\n    app.kubernetes.io/cluster: nebula\n    app.kubernetes.io/component: graphd\n    app.kubernetes.io/managed-by: nebula-operator\n    app.kubernetes.io/name: nebula-graph\n  type: NodePort   # Set the type to NodePort.\n</code></pre> <ul> <li>NebulaGraph uses port <code>9669</code> by default. <code>19669</code> is the HTTP port of the Graph service in a NebulaGraph cluster.</li> <li>The value of <code>targetPort</code> is the port mapped to the database Pods, which can be customized.</li> </ul> </li> <li> <p>Run the following command to create a NodePort Service.</p> <pre><code>kubectl create -f graphd-nodeport-service.yaml\n</code></pre> </li> <li> <p>Check the port mapped on all of your cluster nodes.</p> <pre><code>kubectl get services -l app.kubernetes.io/cluster=&lt;nebula&gt; # &lt;nebula&gt; is the name of your NebulaGraph cluster.\n</code></pre> <p>Output:</p> <pre><code>NAME                           TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                                          AGE\nnebula-graphd-svc-nodeport     NodePort    10.107.153.129 &lt;none&gt;        9669:32236/TCP,19669:31674/TCP,19670:31057/TCP   24h\n...\n</code></pre> <p>As you see, the mapped port of NebulaGraph databases on all cluster nodes is <code>32236</code>.</p> </li> <li> <p>Connect to NebulaGraph databases with your node IP and the node port above.</p> <pre><code>kubectl run -ti --image vesoft/nebula-console:v3.5.0 --restart=Never -- &lt;nebula_console_name&gt; -addr &lt;node_ip&gt; -port &lt;node_port&gt; -u &lt;username&gt; -p &lt;password&gt;\n</code></pre> <p>For example:</p> <pre><code>kubectl run -ti --image vesoft/nebula-console:v3.5.0 --restart=Never -- nebula-console -addr 192.168.8.24 -port 32236 -u root -p vesoft\nIf you don't see a command prompt, try pressing enter.\n\n(root@nebula) [(none)]&gt;\n</code></pre> <ul> <li><code>--image</code>: The image for the tool NebulaGraph Console used to connect to NebulaGraph databases.</li> <li><code>&lt;nebula-console&gt;</code>: The custom Pod name. The above example uses <code>nebula-console</code>.</li> <li><code>-addr</code>: The IP of any node in a NebulaGraph cluster. The above example uses <code>192.168.8.24</code>.</li> <li><code>-port</code>: The mapped port of NebulaGraph databases on all cluster nodes. The above example uses <code>32236</code>.</li> <li><code>-u</code>: The username of your NebulaGraph account. Before enabling authentication, you can use any existing username. The default username is root.</li> <li><code>-p</code>: The password of your NebulaGraph account. Before enabling authentication, you can use any characters as the password.</li> </ul> </li> </ol>"},{"location":"nebula-operator/4.connect-to-nebula-graph-service/#connect_to_nebulagraph_databases_from_within_a_nebulagraph_cluster","title":"Connect to NebulaGraph databases from within a NebulaGraph cluster","text":"<p>You can also create a <code>ClusterIP</code> type Service to provide an access point to the NebulaGraph database for other Pods within the cluster. By using the Service's IP and the Graph service's port number (9669), you can connect to the NebulaGraph database. For more information, see ClusterIP.</p> <ol> <li> <p>Create a file named <code>graphd-clusterip-service.yaml</code>. The file contents are as follows:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/cluster: nebula\n    app.kubernetes.io/component: graphd\n    app.kubernetes.io/managed-by: nebula-operator\n    app.kubernetes.io/name: nebula-graph\n  name: nebula-graphd-svc\n  namespace: default\nspec:\n  externalTrafficPolicy: Local\n  ports:\n  - name: thrift\n    port: 9669\n    protocol: TCP\n    targetPort: 9669\n  - name: http\n    port: 19669\n    protocol: TCP\n    targetPort: 19669\n  selector:\n    app.kubernetes.io/cluster: nebula\n    app.kubernetes.io/component: graphd\n    app.kubernetes.io/managed-by: nebula-operator\n    app.kubernetes.io/name: nebula-graph\n  type: ClusterIP  # Set the type to ClusterIP.\n</code></pre> <ul> <li>NebulaGraph uses port <code>9669</code> by default. <code>19669</code> is the HTTP port of the Graph service in a NebulaGraph cluster.</li> <li><code>targetPort</code> is the port mapped to the database Pods, which can be customized.</li> </ul> </li> <li> <p>Create a ClusterIP Service.</p> <pre><code>kubectl create -f graphd-clusterip-service.yaml     \n</code></pre> </li> <li> <p>Check the IP of the Service:</p> <pre><code>$ kubectl get service -l app.kubernetes.io/cluster=&lt;nebula&gt;  # &lt;nebula&gt; is the name of your NebulaGraph cluster.\nNAME                       TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                                          AGE\nnebula-graphd-svc          ClusterIP   10.98.213.34   &lt;none&gt;        9669/TCP,19669/TCP,19670/TCP                     23h\n...\n</code></pre> </li> <li> <p>Run the following command to connect to the NebulaGraph database using the IP of the <code>&lt;cluster-name&gt;-graphd-svc</code> Service above:</p> <pre><code>kubectl run -ti --image vesoft/nebula-console:v3.5.0 --restart=Never -- &lt;nebula_console_name&gt; -addr &lt;cluster_ip&gt;  -port &lt;service_port&gt; -u &lt;username&gt; -p &lt;password&gt;\n</code></pre> <p>For example:</p> <pre><code>kubectl run -ti --image vesoft/nebula-console:v3.5.0 --restart=Never -- nebula-console -addr 10.98.213.34  -port 9669 -u root -p vesoft\n\n- `--image`: The image for the tool NebulaGraph Console used to connect to NebulaGraph databases.\n- `&lt;nebula-console&gt;`: The custom Pod name.\n- `-addr`: The IP of the `ClusterIP` Service, used to connect to Graphd services.\n- `-port`: The port to connect to Graphd services, the default port of which is `9669`.\n- `-u`: The username of your NebulaGraph account. Before enabling authentication, you can use any existing username. The default username is root.\n- `-p`: The password of your NebulaGraph account. Before enabling authentication, you can use any characters as the password.\n\nA successful connection to the database is indicated if the following is returned:\n\n```bash\nIf you don't see a command prompt, try pressing enter.\n\n(root@nebula) [(none)]&gt;\n</code></pre> </li> </ol> <p>You can also connect to NebulaGraph databases with Fully Qualified Domain Name (FQDN). The domain format is <code>&lt;cluster-name&gt;-graphd.&lt;cluster-namespace&gt;.svc.&lt;CLUSTER_DOMAIN&gt;</code>. The default value of <code>CLUSTER_DOMAIN</code> is <code>cluster.local</code>.</p> <pre><code>kubectl run -ti --image vesoft/nebula-console:v3.5.0 --restart=Never -- &lt;nebula_console_name&gt; -addr &lt;cluster_name&gt;-graphd-svc.default.svc.cluster.local -port &lt;service_port&gt; -u &lt;username&gt; -p &lt;password&gt;\n</code></pre> <p><code>service_port</code> is the port to connect to Graphd services, the default port of which is <code>9669</code>.</p>"},{"location":"nebula-operator/4.connect-to-nebula-graph-service/#connect_to_nebulagraph_databases_from_outside_a_nebulagraph_cluster_via_ingress","title":"Connect to NebulaGraph databases from outside a NebulaGraph cluster via Ingress","text":"<p>When dealing with multiple pods in a cluster, managing services for each pod separately is not a good practice. Ingress is a Kubernetes resource that provides a unified entry point for accessing multiple services. Ingress can be used to expose multiple services under a single IP address.</p> <p>Nginx Ingress is an implementation of Kubernetes Ingress. Nginx Ingress watches the Ingress resource of a Kubernetes cluster and generates the Ingress rules into Nginx configurations that enable Nginx to forward 7 layers of traffic.</p> <p>You can use Nginx Ingress to connect to a NebulaGraph cluster from outside the cluster using a combination of the host network and DaemonSet pattern.</p> <p>Due to the use of <code>HostNetwork</code>, Nginx Ingress pods may be scheduled on the same node (port conflicts will occur when multiple pods try to listen on the same port on the same node). To avoid this situation, Nginx Ingress is deployed on these nodes in DaemonSet mode (ensuring that a pod replica runs on each node in the cluster). You first need to select some nodes and label them for the specific deployment of Nginx Ingress.</p> <p>Ingress does not support TCP or UDP services. For this reason, the nginx-ingress-controller pod uses the flags <code>--tcp-services-configmap</code> and <code>--udp-services-configmap</code> to point to an existing ConfigMap where the key refers to the external port to be used and the value refers to the format of the service to be exposed. The format of the value is <code>&lt;namespace/service_name&gt;:&lt;service_port&gt;</code>.</p> <p>For example, the configurations of the ConfigMap named as <code>tcp-services</code> is as follows:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: tcp-services\n  namespace: nginx-ingress\ndata:\n  # update \n  9769: \"default/nebula-graphd-svc:9669\"\n</code></pre> <p>Steps are as follows.</p> <ol> <li> <p>Create a file named <code>nginx-ingress-daemonset-hostnetwork.yaml</code>. </p> <p>Click on nginx-ingress-daemonset-hostnetwork.yaml to view the complete content of the example YAML file.</p> <p>Note</p> <p>The resource objects in the YAML file above use the namespace <code>nginx-ingress</code>. You can run <code>kubectl create namespace nginx-ingress</code> to create this namespace, or you can customize the namespace.</p> </li> <li> <p>Label a node where the DaemonSet named <code>nginx-ingress-controller</code> in the above YAML file (The node used in this example is named <code>worker2</code> with an IP of <code>192.168.8.160</code>) runs.</p> <pre><code>kubectl label node worker2 nginx-ingress=true\n</code></pre> </li> <li> <p>Run the following command to enable Nginx Ingress in the cluster you created. </p> <pre><code>kubectl create -f nginx-ingress-daemonset-hostnetwork.yaml\n</code></pre> <p>Output:</p> <pre><code>configmap/nginx-ingress-controller created\nconfigmap/tcp-services created\nserviceaccount/nginx-ingress created\nserviceaccount/nginx-ingress-backend created\nclusterrole.rbac.authorization.k8s.io/nginx-ingress created\nclusterrolebinding.rbac.authorization.k8s.io/nginx-ingress created\nrole.rbac.authorization.k8s.io/nginx-ingress created\nrolebinding.rbac.authorization.k8s.io/nginx-ingress created\nservice/nginx-ingress-controller-metrics created\nservice/nginx-ingress-default-backend created\nservice/nginx-ingress-proxy-tcp created\ndaemonset.apps/nginx-ingress-controller created\n</code></pre> <p>Since the network type that is configured in Nginx Ingress is <code>hostNetwork</code>, after successfully deploying Nginx Ingress, with the IP (<code>192.168.8.160</code>) of the node where Nginx Ingress is deployed and with the external port (<code>9769</code>) you define, you can access NebulaGraph. </p> </li> <li> <p>Use the IP address and the port configured in the preceding steps. You can connect to NebulaGraph with NebulaGraph Console. </p> <pre><code>kubectl run -ti --image vesoft/nebula-console:v3.5.0 --restart=Never -- &lt;nebula_console_name&gt; -addr &lt;host_ip&gt; -port &lt;external_port&gt; -u &lt;username&gt; -p &lt;password&gt;\n</code></pre> <p>Output:</p> <pre><code>kubectl run -ti --image vesoft/nebula-console:v3.5.0 --restart=Never -- nebula-console -addr 192.168.8.160 -port 9769 -u root -p vesoft\n</code></pre> <ul> <li><code>--image</code>: The image for the tool NebulaGraph Console used to connect to NebulaGraph databases.</li> <li><code>&lt;nebula-console&gt;</code> The custom Pod name. The above example uses <code>nebula-console</code>.</li> <li><code>-addr</code>: The IP of the node where Nginx Ingress is deployed. The above example uses <code>192.168.8.160</code>.</li> <li><code>-port</code>: The port used for external network access. The above example uses <code>9769</code>.</li> <li><code>-u</code>: The username of your NebulaGraph account. Before enabling authentication, you can use any existing username. The default username is root.</li> <li><code>-p</code>: The password of your NebulaGraph account. Before enabling authentication, you can use any characters as the password.</li> </ul> <p>A successful connection to the database is indicated if the following is returned:</p> <pre><code>If you don't see a command prompt, try pressing enter.\n(root@nebula) [(none)]&gt;\n</code></pre> </li> </ol>"},{"location":"nebula-operator/5.operator-failover/","title":"Self-healing","text":"<p>NebulaGraph Operator calls the interface provided by NebulaGraph clusters to dynamically sense cluster service status. Once an exception is detected (for example, a component in a NebulaGraph cluster stops running), NebulaGraph Operator automatically performs fault tolerance. This topic shows how Nebular Operator performs self-healing by simulating cluster failure of deleting one Storage service Pod in a NebulaGraph cluster.</p>"},{"location":"nebula-operator/5.operator-failover/#prerequisites","title":"Prerequisites","text":"<p>Install NebulaGraph Operator</p>"},{"location":"nebula-operator/5.operator-failover/#steps","title":"Steps","text":"<ol> <li> <p>Create a NebulaGraph cluster. For more information, see Deploy NebulaGraph clusters with Kubectl or Deploy NebulaGraph clusters with Helm.</p> </li> <li> <p>Delete the Pod named <code>&lt;cluster_name&gt;-storaged-2</code> after all pods are in the <code>Running</code> status.</p> <p><pre><code>kubectl delete pod &lt;cluster-name&gt;-storaged-2 --now\n</code></pre> <code>&lt;cluster_name&gt;</code> is the name of your NebulaGraph cluster.</p> </li> <li> <p>NebulaGraph Operator automates the creation of the Pod named <code>&lt;cluster-name&gt;-storaged-2</code> to perform self-healing.</p> <p>Run the <code>kubectl get pods</code> command to check the status of the Pod <code>&lt;cluster-name&gt;-storaged-2</code>.</p> <pre><code>...\nnebula-cluster-storaged-1        1/1     Running             0          5d23h\nnebula-cluster-storaged-2        0/1     ContainerCreating   0          1s\n...\n</code></pre> <p><pre><code>...\nnebula-cluster-storaged-1        1/1     Running     0          5d23h\nnebula-cluster-storaged-2        1/1     Running     0          4m2s\n...\n</code></pre> When the status of <code>&lt;cluster-name&gt;-storaged-2</code> is changed from <code>ContainerCreating</code> to <code>Running</code>, the self-healing is performed successfully.</p> </li> </ol>"},{"location":"nebula-operator/6.get-started-with-operator/","title":"Overview of using NebulaGraph Operator","text":"<p>To use NebulaGraph Operator to connect to NebulaGraph databases, see steps as follows:</p> <ol> <li>Install NebulaGraph Operator.</li> <li> <p>Create a NebulaGraph cluster.</p> <p>For more information, see Deploy NebulaGraph clusters with Kubectl or Deploy NebulaGraph clusters with Helm.</p> </li> <li> <p>Connect to a NebulaGraph database.</p> </li> </ol>"},{"location":"nebula-operator/7.operator-faq/","title":"FAQ","text":""},{"location":"nebula-operator/7.operator-faq/#does_nebulagraph_operator_support_the_v1x_version_of_nebulagraph","title":"Does NebulaGraph Operator support the v1.x version of NebulaGraph?","text":"<p>No, because the v1.x version of NebulaGraph does not support DNS, and NebulaGraph Operator requires the use of DNS.</p>"},{"location":"nebula-operator/7.operator-faq/#is_cluster_stability_guaranteed_if_using_local_storage","title":"Is cluster stability guaranteed if using local storage?","text":"<p>There is no guarantee. Using local storage means that the Pod is bound to a specific node, and NebulaGraph Operator does not currently support failover in the event of a failure of the bound node.</p>"},{"location":"nebula-operator/7.operator-faq/#how_to_ensure_the_stability_of_a_cluster_when_scaling_the_cluster","title":"How to ensure the stability of a cluster when scaling the cluster?","text":"<p>It is suggested to back up data in advance so that you can roll back data in case of failure.</p>"},{"location":"nebula-operator/7.operator-faq/#is_the_replica_in_the_operator_docs_the_same_as_the_replica_in_the_nebulagraph_core_docs","title":"Is the replica in the Operator docs the same as the replica in the NebulaGraph core docs?","text":"<p>They are different concepts. A replica in the Operator docs indicates a pod replica in K8s, while a replica in the core docs is a replica of a NebulaGraph storage partition.</p>"},{"location":"nebula-operator/7.operator-faq/#how_to_view_the_logs_of_each_service_in_the_nebulagraph_cluster","title":"How to view the logs of each service in the NebulaGraph cluster?","text":"<p>The logs for the NebulaGraph cluster are not gathered in the K8s cluster, which also means that they cannot be retrieved through the <code>kubectl logs</code> command. To obtain the logs of each cluster service, you need to access the container and view the log files that are stored inside. This is the only option available for users to get the service logs individually in the NebulaGraph cluster.</p> <p>Steps to view the logs of each service in the NebulaGraph cluster:</p> <pre><code># To view the name of the pod where the container you want to access is located. \n# Replace &lt;cluster-name&gt; with the name of the cluster.\nkubectl get pods -l app.kubernetes.io/cluster=&lt;cluster-name&gt;\n\n# To access the container within the pod, such as the nebula-graphd-0 container.\nkubectl exec -it nebula-graphd-0 -- /bin/bash\n\n# To go to /usr/local/nebula/logs directory to view the logs.\ncd /usr/local/nebula/logs\n</code></pre>"},{"location":"nebula-operator/7.operator-faq/#how_to_resolve_the_host_not_foundnebula-metadstoragedgraphd-0nebulametadstoragedgraphd-headlessdefaultsvcclusterlocal_error","title":"How to resolve the <code>host not found:nebula-&lt;metad|storaged|graphd&gt;-0.nebula.&lt;metad|storaged|graphd&gt;-headless.default.svc.cluster.local</code> error?","text":"<p>This error is generally caused by a DNS resolution failure, and you need to check whether the cluster domain has been modified. If the cluster domain has been modified, you need to modify the <code>kubernetesClusterDomain</code> field in the NebulaGraph Operator configuration file accordingly. The steps for modifying the Operator configuration file are as follows:</p> <ol> <li> <p>View the Operator configuration file.</p> <pre><code>[abby@master ~]$ helm show values nebula-operator/nebula-operator   \nimage:\n  nebulaOperator:\n    image: vesoft/nebula-operator:v1.6.2\n    imagePullPolicy: Always\n  kubeRBACProxy:\n    image: bitnami/kube-rbac-proxy:0.14.2\n    imagePullPolicy: Always\n  kubeScheduler:\n    image: registry.k8s.io/kube-scheduler:v1.24.11\n    imagePullPolicy: Always\n\nimagePullSecrets: []\nkubernetesClusterDomain: \"\"  # The cluster domain name, and the default is cluster.local.\n</code></pre> </li> <li> <p>Modify the value of the <code>kubernetesClusterDomain</code> field to the updated cluster domain name.</p> <p><pre><code>helm upgrade nebula-operator nebula-operator/nebula-operator --namespace=&lt;nebula-operator-system&gt; --version=1.6.2 --set kubernetesClusterDomain=&lt;cluster-domain&gt;\n</code></pre>  is the namespace where Operator is located and  is the updated domain name."},{"location":"nebula-operator/9.upgrade-nebula-cluster/","title":"Upgrade NebulaGraph clusters created with NebulaGraph Operator","text":"<p>This topic introduces how to upgrade a NebulaGraph cluster created with NebulaGraph Operator.</p> <p>Legacy version compatibility</p> <p>The 1.x version NebulaGraph Operator is not compatible with NebulaGraph of version below v3.x.</p>"},{"location":"nebula-operator/9.upgrade-nebula-cluster/#limits","title":"Limits","text":"<ul> <li>Only for upgrading the NebulaGraph clusters created with NebulaGraph Operator.</li> </ul> <ul> <li>Only support upgrading the NebulaGraph version from 3.5.0 to 3.5.x.</li> </ul> <ul> <li>For upgrading NebulaGraph Enterprise Edition clusters, contact us.</li> </ul>"},{"location":"nebula-operator/9.upgrade-nebula-cluster/#upgrade_a_nebulagraph_cluster_with_kubectl","title":"Upgrade a NebulaGraph cluster with Kubectl","text":""},{"location":"nebula-operator/9.upgrade-nebula-cluster/#prerequisites","title":"Prerequisites","text":"<p>You have created a NebulaGraph cluster with Kubectl. For details, see Create a NebulaGraph cluster with Kubectl.</p> <p>The version of the NebulaGraph cluster to be upgraded in this topic is <code>3.5.0</code>, and its YAML file name is <code>apps_v1alpha1_nebulacluster.yaml</code>.</p>"},{"location":"nebula-operator/9.upgrade-nebula-cluster/#steps","title":"Steps","text":"<ol> <li> <p>Check the image version of the services in the cluster.</p> <pre><code>kubectl get pods -l app.kubernetes.io/cluster=nebula  -o jsonpath=\"{.items[*].spec.containers[*].image}\" |tr -s '[[:space:]]' '\\n' |sort |uniq -c\n</code></pre> <p>Output:</p> <pre><code>      1 vesoft/nebula-graphd:3.5.0\n      1 vesoft/nebula-metad:3.5.0\n      3 vesoft/nebula-storaged:3.5.0  \n</code></pre> </li> <li> <p>Edit the <code>apps_v1alpha1_nebulacluster.yaml</code> file by changing the values of all the <code>version</code> parameters from 3.5.0 to v3.5.0.</p> <p>The modified YAML file reads as follows:</p> <pre><code>apiVersion: apps.nebula-graph.io/v1alpha1\nkind: NebulaCluster\nmetadata:\n  name: nebula\nspec:\n  graphd:\n    resources:\n      requests:\n        cpu: \"500m\"\n        memory: \"500Mi\"\n      limits:\n        cpu: \"1\"\n        memory: \"1Gi\"\n    replicas: 1\n    image: vesoft/nebula-graphd\n    version: v3.5.0 //Change the value from 3.5.0 to v3.5.0.\n    service:\n      type: NodePort\n      externalTrafficPolicy: Local\n    logVolumeClaim:\n      resources:\n        requests:\n          storage: 2Gi\n      storageClassName: fast-disks\n  metad:\n    resources:\n      requests:\n        cpu: \"500m\"\n        memory: \"500Mi\"\n      limits:\n        cpu: \"1\"\n        memory: \"1Gi\"\n    replicas: 1\n    image: vesoft/nebula-metad\n    version: v3.5.0 //Change the value from 3.5.0 to v3.5.0.\n    dataVolumeClaim:\n      resources:\n        requests:\n          storage: 2Gi\n      storageClassName: fast-disks\n    logVolumeClaim:\n      resources:\n        requests:\n          storage: 2Gi\n      storageClassName: fast-disks\n  storaged:\n    resources:\n      requests:\n        cpu: \"500m\"\n        memory: \"500Mi\"\n      limits:\n        cpu: \"1\"\n        memory: \"1Gi\"\n    replicas: 3\n    image: vesoft/nebula-storaged\n    version: v3.5.0 //Change the value from 3.5.0 to v3.5.0.\n    dataVolumeClaims:\n    - resources:\n        requests:\n          storage: 2Gi\n      storageClassName: fast-disks\n    - resources:\n        requests:\n          storage: 2Gi\n      storageClassName: fast-disks\n    logVolumeClaim:\n      resources:\n        requests:\n          storage: 2Gi\n      storageClassName: fast-disks\n  reference:\n    name: statefulsets.apps\n    version: v1\n  schedulerName: default-scheduler\n  imagePullPolicy: Always\n</code></pre> </li> <li> <p>Run the following command to apply the version update to the cluster CR.</p> <pre><code>kubectl apply -f apps_v1alpha1_nebulacluster.yaml\n</code></pre> </li> <li> <p>After waiting for about 2 minutes, run the following command to see if the image versions of the services in the cluster have been changed to v3.5.0.</p> <pre><code>kubectl get pods -l app.kubernetes.io/cluster=nebula  -o jsonpath=\"{.items[*].spec.containers[*].image}\" |tr -s '[[:space:]]' '\\n' |sort |uniq -c\n</code></pre> <p>Output:</p> <pre><code>      1 vesoft/nebula-graphd:v3.5.0\n      1 vesoft/nebula-metad:v3.5.0\n      3 vesoft/nebula-storaged:v3.5.0 \n</code></pre> </li> </ol>"},{"location":"nebula-operator/9.upgrade-nebula-cluster/#upgrade_a_nebulagraph_cluster_with_helm","title":"Upgrade a NebulaGraph cluster with Helm","text":""},{"location":"nebula-operator/9.upgrade-nebula-cluster/#prerequisites_1","title":"Prerequisites","text":"<p>You have created a NebulaGraph cluster with Helm. For details, see Create a NebulaGraph cluster with Helm.</p>"},{"location":"nebula-operator/9.upgrade-nebula-cluster/#steps_1","title":"Steps","text":"<ol> <li> <p>Update the information of available charts locally from chart repositories.</p> <pre><code>helm repo update\n</code></pre> </li> <li> <p>Set environment variables to your desired values.</p> <pre><code>export NEBULA_CLUSTER_NAME=nebula         # The desired NebulaGraph cluster name.\nexport NEBULA_CLUSTER_NAMESPACE=nebula    # The desired namespace where your NebulaGraph cluster locates.\n</code></pre> </li> <li> <p>Upgrade a NebulaGraph cluster.</p> <p>For example, upgrade a cluster to v3.5.0.</p> <pre><code>helm upgrade \"${NEBULA_CLUSTER_NAME}\" nebula-operator/nebula-cluster \\\n    --namespace=\"${NEBULA_CLUSTER_NAMESPACE}\" \\\n    --set nameOverride=${NEBULA_CLUSTER_NAME} \\\n    --set nebula.version=v3.5.0\n</code></pre> <p>The value of <code>--set nebula.version</code> specifies the version of the cluster you want to upgrade to.</p> </li> <li> <p>Run the following command to check the status and version of the upgraded cluster.</p> <p>Check cluster status:</p> <pre><code>$ kubectl -n \"${NEBULA_CLUSTER_NAMESPACE}\" get pod -l \"app.kubernetes.io/cluster=${NEBULA_CLUSTER_NAME}\"\nNAME                READY   STATUS    RESTARTS   AGE\nnebula-graphd-0     1/1     Running   0          2m\nnebula-graphd-1     1/1     Running   0          2m\nnebula-metad-0      1/1     Running   0          2m\nnebula-metad-1      1/1     Running   0          2m\nnebula-metad-2      1/1     Running   0          2m\nnebula-storaged-0   1/1     Running   0          2m\nnebula-storaged-1   1/1     Running   0          2m\nnebula-storaged-2   1/1     Running   0          2m\n</code></pre> <p>Check cluster version:</p> <pre><code>$ kubectl get pods -l app.kubernetes.io/cluster=nebula  -o jsonpath=\"{.items[*].spec.containers[*].image}\" |tr -s '[[:space:]]' '\\n' |sort |uniq -c\n      1 vesoft/nebula-graphd:v3.5.0\n      1 vesoft/nebula-metad:v3.5.0\n      3 vesoft/nebula-storaged:v3.5.0\n</code></pre> </li> </ol>"},{"location":"nebula-operator/9.upgrade-nebula-cluster/#accelerate_the_upgrade_process","title":"Accelerate the upgrade process","text":"<p>The upgrade process of a cluster is a rolling update process and can be time-consuming due to the state transition of the leader partition replicas in the Storage service. You can configure the <code>enableForceUpdate</code> field in the cluster instance's YAML file to skip the leader partition replica transfer operation, thereby accelerating the upgrade process. For more information, see Specify a rolling update strategy.</p>"},{"location":"nebula-operator/3.deploy-nebula-graph-cluster/3.0.deploy-lm/","title":"Deploy LM","text":"<p>Enterpriseonly</p> <p>The LM service is only used to manage the NebulaGraph Enterprise license. If you are using the Community Edition of NebulaGraph, you do not need to deploy LM.</p> <p>Before deploying NebulaGraph Enterprise 3.5.0 or later using Operator, you first need to deploy License Manager (LM) and configure the NebulaGraph Enterprise License in LM. LM is a standalone service used to manage the NebulaGraph license. LM checks the validity of the license when NebulaGraph Enterprise database starts. If the License is invalid, the database will not be able to start.</p>"},{"location":"nebula-operator/3.deploy-nebula-graph-cluster/3.0.deploy-lm/#deployment_instructions","title":"Deployment instructions","text":"<p>Operator does not currently support the deployment of LM. You need to deploy LM themselves.</p> <p>As LM needs to store data and is a stateful service. You can deploy LM through the StatefulSet resource type or outside the Kubernetes cluster.</p>"},{"location":"nebula-operator/3.deploy-nebula-graph-cluster/3.0.deploy-lm/#deploy_lm_outside_k8s_cluster","title":"Deploy LM outside K8s cluster","text":"<p>For information on how to deploy LM on a machine outside the K8s cluster, see License Manager.</p> <p>Caution</p> <p>If LM is deployed outside the K8s cluster, make sure that the port of the LM service (default is <code>9119</code>) can be accessed by all nodes in the K8s cluster.</p>"},{"location":"nebula-operator/3.deploy-nebula-graph-cluster/3.0.deploy-lm/#deploy_lm_in_k8s_using_statefulset","title":"Deploy LM in K8s using StatefulSet","text":""},{"location":"nebula-operator/3.deploy-nebula-graph-cluster/3.0.deploy-lm/#prerequisites","title":"Prerequisites","text":"<ul> <li>Prepare the LM image.</li> <li>A StorageClass has been created to store LM data. For more information, see Storage Class.</li> </ul>"},{"location":"nebula-operator/3.deploy-nebula-graph-cluster/3.0.deploy-lm/#steps","title":"Steps","text":"<ol> <li> <p>Create a namespace.</p> <pre><code># Create the nebula-license-manager namespace.\nkubectl create namespace nebula-license-manager\n</code></pre> </li> <li> <p>Create a Secret for pulling the LM image from a private repository.</p> <pre><code>kubectl -n nebula-license-manager create secret docker-registry &lt;image-pull-secret&gt; \\\n--docker-server=DOCKER_REGISTRY_SERVER \\\n--docker-username=DOCKER_USER \\\n--docker-password=DOCKER_PASSWORD\n</code></pre> <ul> <li><code>&lt;image-pull-secret&gt;</code>: Specify the name of the Secret.</li> <li>DOCKER_REGISTRY_SERVER: Specify the address of the private repository server from which the image is pulled, for example, <code>reg.example-inc.com</code>.</li> <li>DOCKER_USER: Image repository username.</li> <li>DOCKER_PASSWORD: Image repository password.</li> </ul> </li> <li> <p>Create a StatefulSet resource configuration file for LM. Here is an example:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nebula-license-manager\n  namespace: nebula-license-manager\n  labels:\n    app: nebula-license-manager\nspec:\n  ports:\n  - port: 9119\n  selector:\n    app: nebula-license-manager\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nebula-license-manager\n  namespace: nebula-license-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nebula-license-manager\n  serviceName: nebula-license-manager\n  template:\n    metadata:\n      labels:\n        app: nebula-license-manager\n    spec:\n      containers:\n      - name: nebula-license-manager    \n        image: # Fill in the corresponding LM image address.\n        ports:\n        - containerPort: 9119\n        volumeMounts:\n        - name: data\n          mountPath: /usr/local/nebula-license-manager/data\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 9119\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 9119\n      # Used to specify one or more Secret names for pulling private images.      \n      imagePullSecrets:\n        - name: image-pull-secret # The name of the Secret.\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      storageClassName: \"local-path\" # Storage Class name.\n      resources:\n        requests:\n          storage: 2Gi  \n</code></pre> </li> <li> <p>Create LM.</p> <pre><code>kubectl apply -f nebula-license-manager.yaml\n</code></pre> </li> <li> <p>Verify that LM has been successfully deployed.</p> <pre><code>kubectl -n nebula-license-manager get pods\n</code></pre> </li> </ol>"},{"location":"nebula-operator/3.deploy-nebula-graph-cluster/3.0.deploy-lm/#monitor_lm","title":"Monitor LM","text":"<p>You can use monitoring tools, such as Dashboard Enterprise or Prometheus, to monitor the running status and metrics of LM. For more information, see Monitor LM.</p>"},{"location":"nebula-operator/3.deploy-nebula-graph-cluster/3.0.deploy-lm/#use_lm_to_manage_the_license","title":"Use LM to manage the license","text":"<ul> <li>For commands related to using LM deployed outside the K8s cluster to manage License, see License Manager.</li> </ul> <ul> <li> <p>Commands for managing the license using LM deployed within the K8s cluster are as follows:</p> <pre><code># View license information.\nkubectl -n nebula-license-manager exec -it nebula-license-manager-0 -- \\\n/usr/local/nebula-license-manager/nebula-license-manager-cli info\n\n# Load the License Key. \n# You must load the License Key before starting the NebulaGraph database.\nkubectl -n nebula-license-manager exec -it nebula-license-manager-0 -- \\\n/usr/local/nebula-license-manager/nebula-license-manager-cli load --key XXXXX-XXXXX-...\n\n# View license quota usage.\nkubectl -n nebula-license-manager exec -it nebula-license-manager-0 -- \\\n  /usr/local/nebula-license-manager/nebula-license-manager-cli usage\n</code></pre> </li> </ul>"},{"location":"nebula-operator/3.deploy-nebula-graph-cluster/3.0.deploy-lm/#next_to_do","title":"Next to do","text":"<p>After deploying LM and loading the License Key, you need to configure the address and port of LM in the NebulaGraph Enterprise cluster through the <code>licenseManagerURL</code> parameter.</p> <p>For more information, see Deploying Using Kubectl or Deploying Using Helm.</p>"},{"location":"nebula-operator/3.deploy-nebula-graph-cluster/3.1create-cluster-with-kubectl/","title":"Deploy NebulaGraph clusters with Kubectl","text":"<p>Legacy version compatibility</p> <p>The 1.x version NebulaGraph Operator is not compatible with NebulaGraph of version below v3.x.</p>"},{"location":"nebula-operator/3.deploy-nebula-graph-cluster/3.1create-cluster-with-kubectl/#prerequisites","title":"Prerequisites","text":"<ul> <li>You have installed NebulaGraph Operator</li> </ul> <ul> <li>You have created StorageClass</li> </ul> <ul> <li>LM has been installed and the License Key has been successfully loaded (Enterprise only)</li> </ul>"},{"location":"nebula-operator/3.deploy-nebula-graph-cluster/3.1create-cluster-with-kubectl/#create_clusters","title":"Create clusters","text":"<p>The following example shows how to create a NebulaGraph cluster by creating a cluster named <code>nebula</code>.</p> <ol> <li> <p>Create a namespace, for example, <code>nebula</code>. If not specified, the <code>default</code> namespace is used.</p> <pre><code>kubectl create namespace nebula\n</code></pre> </li> <li> <p>Create a Secret for pulling the NebulaGraph Enterprise image from a private repository.</p> <p>Note</p> <p>Skip this step if you are using NebulaGraph Community Edition.</p> <pre><code>kubectl -n &lt;nebula&gt; create secret docker-registry &lt;image-pull-secret&gt; \\\n--docker-server=DOCKER_REGISTRY_SERVER \\\n--docker-username=DOCKER_USER \\\n--docker-password=DOCKER_PASSWORD\n</code></pre> <ul> <li><code>&lt;nebula&gt;</code>: The namespace where this Secret will be stored.</li> <li><code>&lt;image-pull-secret&gt;</code>: Specify the name of the Secret.</li> <li><code>DOCKER_REGISTRY_SERVER</code>: Specify the server address of the private repository from which the image will be pulled, such as <code>reg.example-inc.com</code>.</li> <li><code>DOCKER_USER</code>: The username for the image repository.</li> <li><code>DOCKER_PASSWORD</code>: The password for the image repository.</li> </ul> </li> <li> <p>Create a file named <code>apps_v1alpha1_nebulacluster.yaml</code>.</p> <ul> <li> <p>For a NebulaGraph Community cluster</p> <p>Create a file named <code>apps_v1alpha1_nebulacluster.yaml</code>. For the file content, see the sample configuration.</p> <p>The parameters in the file are described as follows:</p> Parameter Default value Description <code>metadata.name</code> - The name of the created NebulaGraph cluster. <code>spec.graphd.replicas</code> <code>1</code> The numeric value of replicas of the Graphd service. <code>spec.graphd.image</code> <code>vesoft/nebula-graphd</code> The container image of the Graphd service. <code>spec.graphd.version</code> <code>v3.5.0</code> The version of the Graphd service. <code>spec.graphd.service</code> - The Service configurations for the Graphd service. <code>spec.graphd.logVolumeClaim.storageClassName</code> - The log disk storage configurations for the Graphd service. <code>spec.metad.replicas</code> <code>1</code> The numeric value of replicas of the Metad service. <code>spec.metad.image</code> <code>vesoft/nebula-metad</code> The container image of the Metad service. <code>spec.metad.version</code> <code>v3.5.0</code> The version of the Metad service. <code>spec.metad.dataVolumeClaim.storageClassName</code> - The data disk storage configurations for the Metad service. <code>spec.metad.logVolumeClaim.storageClassName</code> - The log disk storage configurations for the Metad service. <code>spec.storaged.replicas</code> <code>3</code> The numeric value of replicas of the Storaged service. <code>spec.storaged.image</code> <code>vesoft/nebula-storaged</code> The container image of the Storaged service. <code>spec.storaged.version</code> <code>v3.5.0</code> The version of the Storaged service. <code>spec.storaged.dataVolumeClaims.resources.requests.storage</code> - Data disk storage size for the Storaged service. You can specify multiple data disks to store data. When multiple disks are specified, the storage path is <code>/usr/local/nebula/data1</code>, <code>/usr/local/nebula/data2</code>, etc. <code>spec.storaged.dataVolumeClaims.resources.storageClassName</code> - The data disk storage configurations for Storaged. If not specified, the global storage parameter is applied. <code>spec.storaged.logVolumeClaim.storageClassName</code> - The log disk storage configurations for the Storaged service. <code>spec.storaged.enableAutoBalance</code> <code>true</code> Whether to balance data automatically. <code>spec.agent</code> <code>{}</code> Configuration of the Agent service. This is used for backup and recovery as well as log cleanup functions. If you do not customize this configuration, the default configuration will be used. <code>spec.reference.name</code> - The name of the dependent controller. <code>spec.schedulerName</code> - The scheduler name. <code>spec.imagePullPolicy</code> The image policy to pull the NebulaGraph image. For details, see Image pull policy. The image pull policy in Kubernetes. <code>spec.logRotate</code> - Log rotation configuration. For more information, see Manage cluster logs. <code>spec.enablePVReclaim</code> <code>false</code> Define whether to automatically delete PVCs and release data after deleting the cluster. For more information, see Reclaim PVs. </li> </ul> <ul> <li> <p>For a NebulaGraph Enterprise cluster</p> <p>Enterpriseonly</p> <p>Make sure that you have access to NebulaGraph Enterprise Edition images before pulling the image. </p> <p>Create a file named <code>apps_v1alpha1_nebulacluster.yaml</code>. Contact our sales team to get a complete NebulaGraph Enterprise Edition cluster YAML example. You must customize and modify the following parameters, and other parameters can be changed as needed.</p> <ul> <li><code>spec.metad.licenseManagerURL</code></li> <li><code>spec.&lt;graphd|metad|storaged&gt;.image</code></li> <li><code>spec.imagePullSecrets</code></li> </ul> <p>The parameters only for NebulaGraph Enterprise Edition are described as follows:</p> Parameter Default value Description <code>spec.metad.licenseManagerURL</code> - Configure the URL that points to the LM, which consists of the access address and port number (default port <code>9119</code>) of the LM. For example, <code>192.168.8.100:9119</code>. You must configure this parameter in order to obtain the license information; otherwise, the enterprise edition cluster cannot be used. <code>spec.storaged.enableAutoBalance</code> <code>false</code> Specifies whether to enable automatic data balancing. For more information, see Balance storage data after scaling out. <code>spec.enableBR</code> <code>false</code> Specifies whether to enable the BR tool. For more information, see Backup and restore. <code>spec.imagePullSecrets</code> - Specifies the Secret for pulling the NebulaGraph Enterprise service images from a private repository. </li> </ul> </li> <li> <p>Create a NebulaGraph cluster.</p> <pre><code>kubectl create -f apps_v1alpha1_nebulacluster.yaml\n</code></pre> <p>Output:</p> <pre><code>nebulacluster.apps.nebula-graph.io/nebula created\n</code></pre> </li> <li> <p>Check the status of the NebulaGraph cluster.</p> <pre><code>kubectl get nebulaclusters.apps.nebula-graph.io nebula\n</code></pre> <p>Output:</p> <pre><code>NAME     GRAPHD-DESIRED   GRAPHD-READY   METAD-DESIRED   METAD-READY   STORAGED-DESIRED   STORAGED-READY   AGE\nnebula   1                1              1               1             3                  3                86s\n</code></pre> </li> </ol>"},{"location":"nebula-operator/3.deploy-nebula-graph-cluster/3.1create-cluster-with-kubectl/#scaling_clusters","title":"Scaling clusters","text":"<ul> <li>The cluster scaling feature is for NebulaGraph Enterprise Edition only. </li> </ul> <ul> <li>Scaling a NebulaGraph cluster for Enterprise Edition is supported only with NebulaGraph Operator version 1.1.0 or later. </li> </ul> <p>You can modify the value of <code>replicas</code> in <code>apps_v1alpha1_nebulacluster.yaml</code> to scale a NebulaGraph cluster.</p>"},{"location":"nebula-operator/3.deploy-nebula-graph-cluster/3.1create-cluster-with-kubectl/#scale_out_clusters","title":"Scale out clusters","text":"<p>The following shows how to scale out a NebulaGraph cluster by changing the number of Storage services to 5:</p> <ol> <li> <p>Change the value of the <code>storaged.replicas</code> from <code>3</code> to <code>5</code> in <code>apps_v1alpha1_nebulacluster.yaml</code>.</p> <pre><code>  storaged:\n    resources:\n      requests:\n        cpu: \"500m\"\n        memory: \"500Mi\"\n      limits:\n        cpu: \"1\"\n        memory: \"1Gi\"\n    replicas: 5\n    image: vesoft/nebula-storaged\n    version: v3.5.0\n    dataVolumeClaims:\n    - resources:\n        requests:\n          storage: 2Gi\n      storageClassName: fast-disks\n    - resources:\n        requests:\n          storage: 2Gi\n      storageClassName: fast-disks\n    logVolumeClaim:\n      resources:\n        requests:\n          storage: 2Gi\n      storageClassName: fast-disks\n  reference:\n    name: statefulsets.apps\n    version: v1\n  schedulerName: default-scheduler\n</code></pre> </li> <li> <p>Run the following command to update the NebulaGraph cluster CR.</p> <pre><code>kubectl apply -f apps_v1alpha1_nebulacluster.yaml\n</code></pre> </li> <li> <p>Check the number of Storage services.</p> <pre><code>kubectl get pods -l app.kubernetes.io/cluster=nebula\n</code></pre> <p>Output:</p> <pre><code>NAME                READY   STATUS    RESTARTS   AGE\nnebula-graphd-0     1/1     Running   0          2m\nnebula-metad-0      1/1     Running   0          2m\nnebula-storaged-0   1/1     Running   0          2m\nnebula-storaged-1   1/1     Running   0          2m\nnebula-storaged-2   1/1     Running   0          2m\nnebula-storaged-3   1/1     Running   0          5m\nnebula-storaged-4   1/1     Running   0          5m\n</code></pre> <p>As you can see above, the number of Storage services is scaled up to 5.</p> </li> </ol>"},{"location":"nebula-operator/3.deploy-nebula-graph-cluster/3.1create-cluster-with-kubectl/#scale_in_clusters","title":"Scale in clusters","text":"<p>The principle of scaling in a cluster is the same as scaling out a cluster. You scale in a cluster if the numeric value of the <code>replicas</code> in <code>apps_v1alpha1_nebulacluster.yaml</code> is changed smaller than the current number. For more information, see the Scale out clusters section above.</p> <p>Caution</p> <p>NebulaGraph Operator currently only supports scaling Graph and Storage services and does not support scale Meta services.</p>"},{"location":"nebula-operator/3.deploy-nebula-graph-cluster/3.1create-cluster-with-kubectl/#delete_clusters","title":"Delete clusters","text":"<p>Run the following command to delete a NebulaGraph cluster with Kubectl:</p> <pre><code>kubectl delete -f apps_v1alpha1_nebulacluster.yaml\n</code></pre>"},{"location":"nebula-operator/3.deploy-nebula-graph-cluster/3.1create-cluster-with-kubectl/#whats_next","title":"What's next","text":"<p>Connect to NebulaGraph databases</p>"},{"location":"nebula-operator/3.deploy-nebula-graph-cluster/3.2create-cluster-with-helm/","title":"Deploy NebulaGraph clusters with Helm","text":"<p>Legacy version compatibility</p> <p>The 1.x version NebulaGraph Operator is not compatible with NebulaGraph of version below v3.x.</p>"},{"location":"nebula-operator/3.deploy-nebula-graph-cluster/3.2create-cluster-with-helm/#prerequisite","title":"Prerequisite","text":"<ul> <li>You have installed NebulaGraph Operator</li> </ul> <ul> <li>You have created StorageClass</li> </ul> <ul> <li>LM has been installed and the License Key has been successfully loaded (Enterprise only)</li> </ul>"},{"location":"nebula-operator/3.deploy-nebula-graph-cluster/3.2create-cluster-with-helm/#create_clusters","title":"Create clusters","text":"<ol> <li> <p>Add the NebulaGraph Operator Helm repository.</p> <pre><code>helm repo add nebula-operator https://vesoft-inc.github.io/nebula-operator/charts\n</code></pre> </li> <li> <p>Update information of available charts locally from chart repositories.</p> <pre><code>helm repo update\n</code></pre> </li> <li> <p>Set environment variables to your desired values.</p> <pre><code>export NEBULA_CLUSTER_NAME=nebula         # The desired NebulaGraph cluster name.\nexport NEBULA_CLUSTER_NAMESPACE=nebula    # The desired namespace where your NebulaGraph cluster locates.\nexport STORAGE_CLASS_NAME=fast-disks             # The name of the StorageClass that has been created.\n</code></pre> </li> <li> <p>Create a namespace for your NebulaGraph cluster (If you have created one, skip this step).</p> <pre><code>kubectl create namespace \"${NEBULA_CLUSTER_NAMESPACE}\"\n</code></pre> </li> <li> <p>Create a Secret for pulling the NebulaGraph cluster image from a private repository (Enterprise only).</p> <pre><code>kubectl -n \"${NEBULA_CLUSTER_NAMESPACE}\" create secret docker-registry &lt;image-pull-secret&gt; \\\n--docker-server=DOCKER_REGISTRY_SERVER \\\n--docker-username=DOCKER_USER \\\n--docker-password=DOCKER_PASSWORD\n</code></pre> <ul> <li><code>&lt;image-pull-secret&gt;</code>: Specify the name of the Secret.</li> <li><code>DOCKER_REGISTRY_SERVER</code>: Specify the server address of the private repository from which the image will be pulled, such as <code>reg.example-inc.com</code>.</li> <li><code>DOCKER_USER</code>: The username for the image repository.</li> <li><code>DOCKER_PASSWORD</code>: The password for the image repository.</li> </ul> </li> <li> <p>Apply the variables to the Helm chart to create a NebulaGraph cluster.</p> <pre><code>helm install \"${NEBULA_CLUSTER_NAME}\" nebula-operator/nebula-cluster \\\n    --set nameOverride=${NEBULA_CLUSTER_NAME} \\\n    --set nebula.storageClassName=\"${STORAGE_CLASS_NAME}\" \\\n    # Specify the version of the NebulaGraph cluster. \n    --set nebula.version=v3.5.0 \\  \n    # Specify the version of the nebula-cluster chart. If not specified, the latest version of the chart is installed by default.  \n    # Run 'helm search repo nebula-operator/nebula-cluster' to view the available versions of the chart.   \n    --version=1.6.2 \n    --namespace=\"${NEBULA_CLUSTER_NAMESPACE}\" \\\n</code></pre> <p>Enterpriseonly</p> <p>For NebulaGraph Enterprise, run the following command to create a NebulaGraph cluster:</p> <pre><code>helm install \"${NEBULA_CLUSTER_NAME}\" nebula-operator/nebula-cluster \\\n    # Configure the access address and port (default port is '9119') that points to the LM. You must configure this parameter in order to obtain the license information. Only for NebulaGraph Enterprise Edition clusters.\n    --set nebula.metad.licenseManagerURL=`192.168.8.XXX:9119` \\\n    # Configure the image addresses for each service in the cluster.\n    --set nebula.graphd.image=&lt;reg.example-inc.com/test/graphd-ent&gt; \\\n    --set nebula.metad.image=&lt;reg.example-inc.com/test/metad-ent&gt; \\\n    --set nebula.storaged.image=&lt;reg.example-inc.com/test/storaged-ent&gt; \\\n    # Configure the Secret for pulling images from a private repository.\n    --set nebula.imagePullSecrets=&lt;image-pull-secret&gt; \\\n    --set nameOverride=${NEBULA_CLUSTER_NAME} \\\n    --set nebula.storageClassName=\"${STORAGE_CLASS_NAME}\" \\\n    # Specify the version of the NebulaGraph cluster. \n    --set nebula.version=v3.5.0 \\  \n    # Specify the version of the nebula-cluster chart. If not specified, the latest version of the chart is installed by default.\n    # Run 'helm search repo nebula-operator/nebula-cluster' to view the available versions of the chart.     \n    --version=1.6.2 \n    --namespace=\"${NEBULA_CLUSTER_NAMESPACE}\" \\\n</code></pre> <p>To view all configuration parameters of the NebulaGraph cluster, run the <code>helm show values nebula-operator/nebula-cluster</code> command or click nebula-cluster/values.yaml.</p> <p>Click Chart parameters to see descriptions and default values of the configurable cluster parameters.</p> <p>Use the <code>--set</code> argument to set configuration parameters for the cluster. For example, <code>--set nebula.storaged.replicas=3</code> will set the number of replicas for the Storage service in the cluster to 3.</p> </li> <li> <p>Check the status of the NebulaGraph cluster you created.</p> <pre><code>kubectl -n \"${NEBULA_CLUSTER_NAMESPACE}\" get pod -l \"app.kubernetes.io/cluster=${NEBULA_CLUSTER_NAME}\"\n</code></pre> <p>Output:</p> <pre><code>NAME                READY   STATUS    RESTARTS   AGE\nnebula-graphd-0     1/1     Running   0          5m34s\nnebula-graphd-1     1/1     Running   0          5m34s\nnebula-metad-0      1/1     Running   0          5m34s\nnebula-metad-1      1/1     Running   0          5m34s\nnebula-metad-2      1/1     Running   0          5m34s\nnebula-storaged-0   1/1     Running   0          5m34s\nnebula-storaged-1   1/1     Running   0          5m34s\nnebula-storaged-2   1/1     Running   0          5m34s\n</code></pre> </li> </ol>"},{"location":"nebula-operator/3.deploy-nebula-graph-cluster/3.2create-cluster-with-helm/#scaling_clusters","title":"Scaling clusters","text":"<ul> <li>The cluster scaling feature is for NebulaGraph Enterprise Edition only. </li> </ul> <ul> <li>Scaling a NebulaGraph cluster for Enterprise Edition is supported only with NebulaGraph Operator version 1.1.0 or later. </li> </ul> <p>You can scale a NebulaGraph cluster by defining the value of the <code>replicas</code> corresponding to the different services in the cluster.</p> <p>For example, run the following command to scale out a NebulaGraph cluster by changing the number of Storage services from 2 (the original value) to 5:  </p> <pre><code>helm upgrade \"${NEBULA_CLUSTER_NAME}\" nebula-operator/nebula-cluster \\\n    --namespace=\"${NEBULA_CLUSTER_NAMESPACE}\" \\\n    --set nameOverride=${NEBULA_CLUSTER_NAME} \\\n    --set nebula.storageClassName=\"${STORAGE_CLASS_NAME}\" \\\n    --set nebula.storaged.replicas=5\n</code></pre> <p>Similarly, you can scale in a NebulaGraph cluster by setting the value of the <code>replicas</code> corresponding to the different services in the cluster smaller than the original value.</p> <p>Caution</p> <p>NebulaGraph Operator currently only supports scaling Graph and Storage services and does not support scale Meta services.</p> <p>You can click on nebula-cluster/values.yaml to see more configurable parameters of the nebula-cluster chart. For more information about the descriptions of configurable parameters, see Configuration parameters of the nebula-cluster Helm chart below.</p>"},{"location":"nebula-operator/3.deploy-nebula-graph-cluster/3.2create-cluster-with-helm/#delete_clusters","title":"Delete clusters","text":"<p>Run the following command to delete a NebulaGraph cluster with Helm:</p> <pre><code>helm uninstall \"${NEBULA_CLUSTER_NAME}\" --namespace=\"${NEBULA_CLUSTER_NAMESPACE}\"\n</code></pre> <p>Or use variable values to delete a NebulaGraph cluster with Helm:</p> <pre><code>helm uninstall nebula --namespace=nebula\n</code></pre>"},{"location":"nebula-operator/3.deploy-nebula-graph-cluster/3.2create-cluster-with-helm/#whats_next","title":"What's next","text":"<p>Connect to NebulaGraph Databases</p>"},{"location":"nebula-operator/8.custom-cluster-configurations/8.1.custom-conf-parameter/","title":"Customize configuration parameters for a NebulaGraph cluster","text":"<p>Meta, Storage, and Graph services in a NebulaGraph Cluster have their own configuration settings, which are defined in the YAML file of the NebulaGraph cluster instance as <code>config</code>. These settings are mapped and loaded into the corresponding service's ConfigMap in Kubernetes. At the time of startup, the configuration present in the ConfigMap is mounted onto the directory <code>/usr/local/nebula/etc/</code> for every service.</p> <p>Note</p> <p>It is not available to customize configuration parameters for NebulaGraph Clusters deployed with Helm.</p> <p>The structure of <code>config</code> is as follows.</p> <pre><code>Config map[string]string `json:\"config,omitempty\"`\n</code></pre>"},{"location":"nebula-operator/8.custom-cluster-configurations/8.1.custom-conf-parameter/#prerequisites","title":"Prerequisites","text":"<p>You have created a NebulaGraph cluster. For how to create a cluster with Kubectl, see Create a cluster with Kubectl. </p>"},{"location":"nebula-operator/8.custom-cluster-configurations/8.1.custom-conf-parameter/#steps","title":"Steps","text":"<p>The following example uses a cluster named <code>nebula</code> and the cluster's configuration file named <code>nebula_cluster.yaml</code> to show how to set <code>config</code> for the Graph service in a NebulaGraph cluster.</p> <ol> <li> <p>Run the following command to access the edit page of the <code>nebula</code> cluster.</p> <pre><code>kubectl edit nebulaclusters.apps.nebula-graph.io nebula\n</code></pre> </li> <li> <p>Add <code>enable_authorize</code> and <code>auth_type</code> under <code>spec.graphd.config</code>.</p> <pre><code>apiVersion: apps.nebula-graph.io/v1alpha1\nkind: NebulaCluster\nmetadata:\n  name: nebula\n  namespace: default\nspec:\n  graphd:\n    resources:\n      requests:\n        cpu: \"500m\"\n        memory: \"500Mi\"\n      limits:\n        cpu: \"1\"\n        memory: \"1Gi\"\n    replicas: 1\n    image: vesoft/nebula-graphd\n    version: v3.5.0\n    storageClaim:\n      resources:\n        requests:\n          storage: 2Gi\n      storageClassName: fast-disks\n    config: // Custom configuration parameters for the Graph service in a cluster.\n      \"enable_authorize\": \"true\"\n      \"auth_type\": \"password\"\n...\n</code></pre> <p>To add the <code>config</code> for the Meta and Storage services, add <code>spec.metad.config</code> and <code>spec.storaged.config</code> respectively.</p> </li> <li> <p>Run <code>kubectl apply -f nebula_cluster.yaml</code> to push your configuration changes to the cluster.</p> <p>After customizing the parameters <code>enable_authorize</code> and <code>auth_type</code>, the configurations in the corresponding ConfigMap (<code>nebula-graphd</code>) of the Graph service will be overwritten.</p> </li> </ol>"},{"location":"nebula-operator/8.custom-cluster-configurations/8.1.custom-conf-parameter/#modify_cluster_configurations_online","title":"Modify cluster configurations online","text":"<p>Cluster configurations are modified online by calling the HTTP interface, without the need to restart the cluster Pod.</p> <p>It should be noted that only when all configuration items in <code>config</code> are the parameters that can be dynamically modified at runtime, can the operation of online modifications be triggered. If the configuration items in <code>config</code> contain parameters that cannot be dynamically modified, then the cluster configuration will be updated by restarting the Pod.</p> <p>For information about the parameters that can be dynamically modified for each service, see the parameter table column of Whether supports runtime dynamic modifications in Meta service configuration parameters, Storage service configuration parameters, and Graph service configuration parameters, respectively.</p>"},{"location":"nebula-operator/8.custom-cluster-configurations/8.1.custom-conf-parameter/#learn_more","title":"Learn more","text":"<p>For more information about the configuration parameters of Meta, Storage, and Graph services, see Meta service configuration parameters, Storage service configuration parameters, and Graph service configuration parameters.</p>"},{"location":"nebula-operator/8.custom-cluster-configurations/8.2.pv-reclaim/","title":"Reclaim PVs","text":"<p>NebulaGraph Operator uses PVs (Persistent Volumes) and PVCs (Persistent Volume Claims) to store persistent data. If you accidentally deletes a NebulaGraph cluster, by default, PV and PVC objects and the relevant data will be retained to ensure data security.</p> <p>You can also define the automatic deletion of PVCs to release data by setting the parameter <code>spec.enablePVReclaim</code> to <code>true</code> in the configuration file of the cluster instance. As for whether PV will be deleted automatically after PVC is deleted, you need to customize the PV reclaim policy. See reclaimPolicy in StorageClass and PV Reclaiming for details.</p>"},{"location":"nebula-operator/8.custom-cluster-configurations/8.2.pv-reclaim/#prerequisites","title":"Prerequisites","text":"<p>You have created a cluster. For how to create a cluster with Kubectl, see Create a cluster with Kubectl. </p>"},{"location":"nebula-operator/8.custom-cluster-configurations/8.2.pv-reclaim/#steps","title":"Steps","text":"<p>The following example uses a cluster named <code>nebula</code> and the cluster's configuration file named <code>nebula_cluster.yaml</code> to show how to set <code>enablePVReclaim</code>:</p> <ol> <li> <p>Run the following command to access the edit page of the <code>nebula</code> cluster.</p> <pre><code>kubectl edit nebulaclusters.apps.nebula-graph.io nebula\n</code></pre> </li> <li> <p>Add <code>enablePVReclaim</code> and set its value to <code>true</code> under <code>spec</code>.</p> <pre><code>apiVersion: apps.nebula-graph.io/v1alpha1\nkind: NebulaCluster\nmetadata:\n  name: nebula\nspec:\n  enablePVReclaim: true  //Set its value to true.\n  graphd:\n    image: vesoft/nebula-graphd\n    logVolumeClaim:\n      resources:\n        requests:\n          storage: 2Gi\n      storageClassName: fast-disks\n    replicas: 1\n    resources:\n      limits:\n        cpu: \"1\"\n        memory: 1Gi\n      requests:\n        cpu: 500m\n        memory: 500Mi\n    version: v3.5.0\n  imagePullPolicy: IfNotPresent\n  metad:\n    dataVolumeClaim:\n      resources:\n        requests:\n          storage: 2Gi\n      storageClassName: fast-disks\n    image: vesoft/nebula-metad\n    logVolumeClaim:\n      resources:\n        requests:\n          storage: 2Gi\n      storageClassName: fast-disks\n    replicas: 1\n    resources:\n      limits:\n        cpu: \"1\"\n        memory: 1Gi\n      requests:\n        cpu: 500m\n        memory: 500Mi\n    version: v3.5.0\n  nodeSelector:\n    nebula: cloud\n  reference:\n    name: statefulsets.apps\n    version: v1\n  schedulerName: default-scheduler\n  storaged:\n    dataVolumeClaims:\n    - resources:\n        requests:\n          storage: 2Gi\n      storageClassName: fast-disks\n    - resources:\n        requests:\n          storage: 2Gi\n      storageClassName: fast-disks\n    image: vesoft/nebula-storaged\n    logVolumeClaim:\n      resources:\n        requests:\n          storage: 2Gi\n      storageClassName: fast-disks\n    replicas: 3\n    resources:\n      limits:\n        cpu: \"1\"\n        memory: 1Gi\n      requests:\n        cpu: 500m\n        memory: 500Mi\n    version: v3.5.0\n...    \n</code></pre> </li> <li> <p>Run <code>kubectl apply -f nebula_cluster.yaml</code> to push your configuration changes to the cluster.</p> </li> </ol>"},{"location":"nebula-operator/8.custom-cluster-configurations/8.3.balance-data-when-scaling-storage/","title":"Balance storage data after scaling out","text":"<p>Enterpriseonly</p> <p>This feature is for NebulaGraph Enterprise Edition only.</p> <p>After the Storage service is scaled out, you can decide whether to balance the data in the Storage service. </p> <p>The scaling out of the NebulaGraph's Storage service is divided into two stages. In the first stage, the status of all pods is changed to <code>Ready</code>. In the second stage, the commands of <code>BALANCE DATA</code> and <code>BALANCE LEADER</code> are executed to balance data. These two stages decouple the scaling out process of the controller replica from the balancing data process, so that you can choose to perform the data balancing operation during low traffic period. The decoupling of the scaling out process from the balancing process can effectively reduce the impact on online services during data migration.</p> <p>You can define whether to balance data automatically or not with the parameter <code>enableAutoBalance</code> in the configuration file of the CR instance of the cluster you created.</p>"},{"location":"nebula-operator/8.custom-cluster-configurations/8.3.balance-data-when-scaling-storage/#prerequisites","title":"Prerequisites","text":"<p>You have created a NebulaGraph cluster. For how to create a cluster with Kubectl, see Create a cluster with Kubectl. </p>"},{"location":"nebula-operator/8.custom-cluster-configurations/8.3.balance-data-when-scaling-storage/#steps","title":"Steps","text":"<p>The following example uses a cluster named <code>nebula</code> and the cluster's configuration file named <code>nebula_cluster.yaml</code> to show how to set <code>enableAutoBalance</code>.</p> <ol> <li> <p>Run the following command to access the edit page of the <code>nebula</code> cluster.</p> <pre><code>kubectl edit nebulaclusters.apps.nebula-graph.io nebula\n</code></pre> </li> <li> <p>Add <code>enableAutoBalance</code> and set its value to <code>true</code> under <code>spec.storaged</code>.</p> <pre><code>apiVersion: apps.nebula-graph.io/v1alpha1\nkind: NebulaCluster\nmetadata:\n  name: nebula\nspec:\n  graphd:\n    image: vesoft/nebula-graphd\n    logVolumeClaim:\n      resources:\n        requests:\n          storage: 2Gi\n      storageClassName: fast-disks\n    replicas: 1\n    resources:\n      limits:\n        cpu: \"1\"\n        memory: 1Gi\n      requests:\n        cpu: 500m\n        memory: 500Mi\n    version: v3.5.0\n  imagePullPolicy: IfNotPresent\n  metad:\n    dataVolumeClaim:\n      resources:\n        requests:\n          storage: 2Gi\n      storageClassName: fast-disks\n    image: vesoft/nebula-metad\n    logVolumeClaim:\n      resources:\n        requests:\n          storage: 2Gi\n      storageClassName: fast-disks\n    replicas: 1\n    resources:\n      limits:\n        cpu: \"1\"\n        memory: 1Gi\n      requests:\n        cpu: 500m\n        memory: 500Mi\n    version: v3.5.0\n  nodeSelector:\n    nebula: cloud\n  reference:\n    name: statefulsets.apps\n    version: v1\n  schedulerName: default-scheduler\n  storaged:\n    enableAutoBalance: true   //Set its value to true which means storage data will be balanced after the Storage service is scaled out.\n    dataVolumeClaims:\n    - resources:\n        requests:\n          storage: 2Gi\n      storageClassName: fast-disks\n    - resources:\n        requests:\n          storage: 2Gi\n      storageClassName: fast-disks\n    image: vesoft/nebula-storaged\n    logVolumeClaim:\n      resources:\n        requests:\n          storage: 2Gi\n      storageClassName: fast-disks\n    replicas: 3\n    resources:\n      limits:\n        cpu: \"1\"\n        memory: 1Gi\n      requests:\n        cpu: 500m\n        memory: 500Mi\n    version: v3.5.0\n...    \n</code></pre> <ul> <li>When the value of <code>enableAutoBalance</code> is set to <code>true</code>, the Storage data will be automatically balanced after the Storage service is scaled out.</li> </ul> <ul> <li>When the value of <code>enableAutoBalance</code> is set to <code>false</code>, the Storage data will not be automatically balanced after the Storage service is scaled out.</li> </ul> <ul> <li>When the <code>enableAutoBalance</code> parameter is not set, the system will not automatically balance Storage data by default after the Storage service is scaled out. </li> </ul> </li> <li> <p>Run <code>kubectl apply -f nebula_cluster.yaml</code> to push your configuration changes to the cluster.</p> </li> </ol>"},{"location":"nebula-operator/8.custom-cluster-configurations/8.4.manage-running-logs/","title":"Manage cluster logs","text":"<p>Running logs of NebulaGraph cluster services (graphd, metad, storaged) are generated and stored in the <code>/usr/local/nebula/logs</code> directory of each service container by default.</p>"},{"location":"nebula-operator/8.custom-cluster-configurations/8.4.manage-running-logs/#view_logs","title":"View logs","text":"<p>To view the running logs of a NebulaGraph cluster, you can use the <code>kubectl logs</code> command. </p> <p>For example, to view the running logs of the Storage service:</p> <pre><code>// View the name of the Storage service Pod, nebula-storaged-0.\n$ kubectl get pods -l app.kubernetes.io/component=storaged\nNAME                               READY   STATUS    RESTARTS      AGE\nnebula-storaged-0                  1/1     Running   0             45h\n...\n\n// Enter the container storaged of the Storage service.\n$ kubectl exec -it nebula-storaged-0 -c storaged -- /bin/bash\n\n// View the running logs of the Storage service.\n$ cd /usr/local/nebula/logs\n</code></pre>"},{"location":"nebula-operator/8.custom-cluster-configurations/8.4.manage-running-logs/#clean_logs","title":"Clean logs","text":"<p>Running logs generated by cluster services during runtime will occupy disk space. To avoid occupying too much disk space, the Operator uses a sidecar container to periodically clean and archive logs.</p> <p>To facilitate log collection and management, each NebulaGraph service deploys a sidecar container responsible for collecting logs generated by the service container and sending them to the specified log disk. The sidecar container automatically cleans and archives logs using the logrotate tool.</p> <p>In the YAML configuration file of the cluster instance, set <code>spec.logRotate</code> to enable log rotation and set <code>timestamp_in_logfile_name</code> to <code>false</code> to disable the timestamp in the log file name to implement log rotation for the target service. The <code>timestamp_in_logfile_name</code> parameter is configured under the <code>spec.&lt;graphd|metad|storaged&gt;.config</code> field. By default, the log rotation feature is turned off. Here is an example of enabling log rotation for all services:</p> <pre><code>...\nspec:\n  graphd:\n    config:\n      # Whether to include a timestamp in the log file name. You must set this parameter to false to enable log rotation. It is set to true by default.\n      \"timestamp_in_logfile_name\": \"false\"\n  metad:\n    config:\n      \"timestamp_in_logfile_name\": \"false\"\n  storaged:\n    config:\n      \"timestamp_in_logfile_name\": \"false\"\n  logRotate: # Log rotation configuration\n    # The number of times a log file is rotated before being deleted. The default value is 5, and 0 means the log file will not be rotated before being deleted.\n    rotate: 5\n    # The log file is rotated only if it grows larger than the specified size. The default value is 200M.\n    size: \"200M\"\n</code></pre>"},{"location":"nebula-operator/8.custom-cluster-configurations/8.4.manage-running-logs/#collect_logs","title":"Collect logs","text":"<p>If you don't want to mount additional log disks to back up log files, or if you want to collect logs and send them to a log center using services like fluent-bit, you can configure logs to be output to standard error. The Operator uses the glog tool to log to standard error output.</p> <p>Caution</p> <p>Currently, NebulaGraph Operator only collects standard error logs.\u3002</p> <p>In the YAML configuration file of the cluster instance, you can configure logging to standard error output in the <code>config</code> and <code>env</code> fields of each service.</p> <pre><code>...\nspec:\n  graphd:\n    config:\n      # Whether to redirect standard error to a separate output file. The default value is false, which means it is not redirected.\n      redirect_stdout: \"false\"\n      # The severity level of log content: INFO, WARNING, ERROR, and FATAL. The corresponding values are 0, 1, 2, and 3.\n      stderrthreshold: \"0\"\n    env: \n    - name: GLOG_logtostderr # Write log to standard error output instead of a separate file.\n      value: \"1\" # 1 represents writing to standard error output, and 0 represents writing to a file.\n    image: vesoft/nebula-graphd\n    replicas: 1\n    resources:\n      requests:\n        cpu: 500m\n        memory: 500Mi\n    service:\n      externalTrafficPolicy: Local\n      type: NodePort\n    version: v3.5.0\n  metad:\n    config:\n      redirect_stdout: \"false\"\n      stderrthreshold: \"0\"\n    dataVolumeClaim:\n      resources:\n        requests:\n          storage: 1Gi\n      storageClassName: ebs-sc\n    env:\n    - name: GLOG_logtostderr\n      value: \"1\"\n    image: vesoft/nebula-metad\n  ...\n</code></pre>"},{"location":"nebula-operator/8.custom-cluster-configurations/8.5.enable-ssl/","title":"Enable mTLS in NebulaGraph","text":"<p>Transport Layer Security (TLS) is an encryption protocol in wide use on the Internet. TLS, which was formerly called SSL, authenticates the server in a client-server connection and encrypts communications between client and server so that external parties cannot spy on the communications. Its working principle is mainly to protect data transmitted over the network by using encryption algorithms to prevent data interception or tampering during transmission. During the TLS connection establishment process, the server sends a digital certificate containing a public key and some identity information to the client. This certificate is issued by a trusted third-party Certification Authority (CA). The client verifies this digital certificate to confirm the identity of the server. </p> <p>In the NebulaGraph environment running in Kubernetes, mutual TLS (mTLS) is used to encrypt the communication between the client and server by default, which means both the client and server need to verify their identities. This article explains how to enable mTLS encryption in NebulaGraph running in K8s.</p>"},{"location":"nebula-operator/8.custom-cluster-configurations/8.5.enable-ssl/#prerequisites","title":"Prerequisites","text":"<ul> <li>NebulaGraph Operator has been installed.</li> <li>A NebulaGraph cluster has been created. For details, see Create a NebulaGraph cluster with kubectl or Create a NebulaGraph cluster with Helm.</li> <li> <p>Certificates and their corresponding private keys have been generated for the client and server, and the CA certificate has been generated. For details, see Generate Certificates Manually.</p> <p>Note</p> <p>In the cluster created using Operator, the client and server use the same CA root certificate by default. </p> </li> </ul>"},{"location":"nebula-operator/8.custom-cluster-configurations/8.5.enable-ssl/#create_a_tls-type_secret","title":"Create a TLS-type Secret","text":"<p>In a K8s cluster, you can create Secrets to store sensitive information, such as passwords, OAuth tokens, and TLS certificates. In NebulaGraph, you can create a Secret to store TLS certificates and private keys. When creating a Secret, the type <code>tls</code> should be specified. A <code>tls</code> Secret is used to store TLS certificates.</p> <p>For example, to create a Secret for storing server certificates and private keys:</p> <pre><code>kubectl create secret tls &lt;server-cert&gt; --key=&lt;path/to/tls.key&gt; --cert=&lt;path/to/tls.cert&gt; --namespace=&lt;namespace&gt;\n</code></pre> <ul> <li><code>&lt;server-cert&gt;</code>: The name of the Secret storing the server certificate and private key.</li> <li><code>&lt;path/to/tls.key&gt;</code>: The path to the server private key file.</li> <li><code>&lt;path/to/tls.cert&gt;</code>: The path to the server certificate file.</li> <li><code>&lt;namespace&gt;</code>: The namespace where the Secret is located. If <code>--namespace</code> is not specified, it defaults to the <code>default</code> namespace.</li> </ul> <p>You can follow the above steps to create Secrets for the client certificate and private key, and the CA certificate.</p> <p>To view the created Secrets:</p> <pre><code>kubectl get secret --namespace=&lt;namespace&gt; \n</code></pre>"},{"location":"nebula-operator/8.custom-cluster-configurations/8.5.enable-ssl/#configure_certifications","title":"Configure certifications","text":"<p>Operator provides the <code>sslCerts</code> field to specify the encrypted certificates. The <code>sslCerts</code> field contains three subfields: <code>serverSecret</code>, <code>clientSecret</code>, and <code>caSecret</code>. These three fields are used to specify the Secret names of the NebulaGraph server certificate, client certificate, and CA certificate, respectively. When you specify these three fields, Operator reads the certificate content from the corresponding Secret and mounts it into the cluster's Pod.</p> <pre><code>sslCerts:\n  serverSecret: \"server-cert\"     # The name of the server certificate Secret.\n  serverCert: \"\"                  # The key name of the certificate in the server certificate Secret, default is tls.crt.\n  serverKey: \"\"                   # The key name of the private key in the server certificate Secret, default is tls.key.\n  clientSecret: \"client-cert\"     # The name of the client certificate Secret.\n  clientCert: \"\"                  # The key name of the certificate in the client certificate Secret, default is tls.crt.\n  clientKey: \"\"                   # The key name of the private key in the client certificate Secret, default is tls.key.\n  caSecret: \"ca-cert\"             # The name of the CA certificate Secret.\n  caCert: \"\"                      # The key name of the certificate in the CA certificate Secret, default is ca.crt.\n</code></pre> <p>The <code>serverCert</code> and <code>serverKey</code>, <code>clientCert</code> and <code>clientKey</code>, and <code>caCert</code> are used to specify the key names of the certificate and private key of the server Secret, the key names of the certificate and private key of the client Secret, and the key name of the CA Secret certificate. If you do not customize these field values, Operator defaults <code>serverCert</code> and <code>clientCert</code> to <code>tls.crt</code>, <code>serverKey</code> and <code>clientKey</code> to <code>tls.key</code>, and <code>caCert</code> to <code>ca.crt</code>. However, in the K8s cluster, the TLS type Secret uses <code>tls.crt</code> and <code>tls.key</code> as the default key names for the certificate and private key. Therefore, after creating the NebulaGraph cluster, you need to manually change the <code>caCert</code> field from <code>ca.crt</code> to <code>tls.crt</code> in the cluster configuration, so that the Operator can correctly read the content of the CA certificate. Before you customize these field values, you need to specify the key names of the certificate and private key in the Secret when creating it. For how to create a Secret with the key name specified, run the <code>kubectl create secret generic -h</code> command for help.</p> <p>You can use the <code>insecureSkipVerify</code> field to decide whether the client will verify the server's certificate chain and hostname. In production environments, it is recommended to set this to <code>false</code> to ensure the security of communication. If set to <code>true</code>, the client will not verify the server's certificate chain and hostname.</p> <pre><code>sslCerts:\n  # Determines whether the client needs to verify the server's certificate chain and hostname when establishing an SSL connection. It defaults to true.\n  insecureSkipVerify: false \n</code></pre> <p>When the certificates are approaching expiration, they can be automatically updated by installing cert-manager. NebulaGraph will monitor changes to the certificate directory files, and once a change is detected, it will load the new certificate content into memory. </p>"},{"location":"nebula-operator/8.custom-cluster-configurations/8.5.enable-ssl/#encryption_strategies","title":"Encryption strategies","text":"<p>NebulaGraph offers three encryption strategies that you can choose and configure according to your needs.</p> <ul> <li> <p>Encryption of client-graph and all inter-service communications</p> <p>If you want to encrypt all data transmission between the client, Graph service, Meta service, and Storage service, you need to add the <code>enable_ssl = true</code> field to each service.</p> <p>Here is an example configuration:</p> <pre><code>apiVersion: apps.nebula-graph.io/v1alpha1\nkind: NebulaCluster\nmetadata:\n  name: nebula\n  namespace: default\nspec:\n  sslCerts:\n    serverSecret: \"server-cert\"   # The Secret name of the server certificate and private key.\n    clientSecret: \"client-cert\"   # The Secret name of the client certificate and private key.\n    caSecret: \"ca-cert\"           # The Secret name of the CA certificate.\n  graphd:\n    config:\n      enable_ssl: \"true\"\n  metad:\n    config:\n      enable_ssl: \"true\"\n  storaged:\n    config:\n      enable_ssl: \"true\"\n</code></pre> </li> </ul> <ul> <li> <p>Encryption of only Graph service communication</p> <p>If the K8s cluster is deployed in the same data center and only the port of the Graph service is exposed externally, you can choose to encrypt only data transmission between the client and the Graph service. In this case, other services can communicate internally without encryption. Just add the <code>enable_graph_ssl = true</code> field to the Graph service.</p> <p>Here is an example configuration:</p> <pre><code>apiVersion: apps.nebula-graph.io/v1alpha1\nkind: NebulaCluster\nmetadata:\n  name: nebula\n  namespace: default\nspec:\n  sslCerts:\n    serverSecret: \"server-cert\"\n    caSecret: \"ca-cert\"\n  graphd:\n    config:\n      enable_graph_ssl: \"true\"\n</code></pre> <p>Note</p> <p>Because Operator doesn't need to call the Graph service through an interface, it's not necessary to set <code>clientSecret</code> in <code>sslCerts</code>.</p> </li> </ul> <ul> <li> <p>Encryption of only Meta service communication</p> <p>If you need to transmit confidential information to the Meta service, you can choose to encrypt data transmission related to the Meta service. In this case, you need to add the <code>enable_meta_ssl = true</code> configuration to each component.</p> <p>Here is an example configuration:</p> <pre><code>apiVersion: apps.nebula-graph.io/v1alpha1\nkind: NebulaCluster\nmetadata:\n  name: nebula\n  namespace: default\nspec:\n  sslCerts:\n    serverSecret: \"server-cert\"\n    clientSecret: \"client-cert\"\n    caSecret: \"ca-cert\"\n  graphd:\n    config:\n      enable_meta_ssl: \"true\"\n  metad:\n    config:\n      enable_meta_ssl: \"true\"\n  storaged:\n    config:\n      enable_meta_ssl: \"true\"\n</code></pre> </li> </ul> <p>After setting up the encryption policy, when an external client needs to connect to the Graph service with mutual TLS, you still need to set the relevant TLS parameters according to the different clients. See the Use NebulaGraph Console to connect to Graph service section below for examples.</p>"},{"location":"nebula-operator/8.custom-cluster-configurations/8.5.enable-ssl/#steps","title":"Steps","text":"<ol> <li> <p>Use the pre-generated server and client certificates and private keys, and the CA certificate to create a Secret for each.</p> <pre><code>kubectl create secret tls &lt;client/server/ca-cert-secret&gt; --key=&lt;client/server/ca.key&gt; --cert=&lt;client/server/ca.crt&gt;\n</code></pre> <ul> <li><code>tls</code>: Indicates that the type of secret being created is TLS, which is used to store TLS certificates.</li> <li><code>&lt;client/server/ca-cert-secret&gt;</code>: Specifies the name of the new secret being created, which can be customized.</li> <li><code>--key=&lt;client/server/ca.key&gt;</code>: Specifies the path to the private key file of the TLS certificate to be stored in the secret.</li> <li><code>--cert=&lt;client/server/ca.crt&gt;</code>: Specifies the path to the public key certificate file of the TLS certificate to be stored in the secret.</li> </ul> </li> <li> <p>Add server certificate, client certificate, CA certificate configuration, and encryption policy configuration in the corresponding cluster instance YAML file. For details, see Encryption strategies.</p> <p>For example, add encryption configuration for transmission data between client, Graph service, Meta service, and Storage service.</p> <pre><code>apiVersion: apps.nebula-graph.io/v1alpha1\nkind: NebulaCluster\nmetadata:\n  name: nebula\n  namespace: default\nspec:\n  sslCerts:\n    serverSecret: \"server-cert\"  // The name of the server Certificate Secret.\n    clientSecret: \"client-cert\"  // The name of the client Certificate Secret.\n    caSecret: \"ca-cert\"          // The name of the CA Certificate Secret.\n  graphd:\n    config:\n      enable_ssl: \"true\"\n  metad:\n    config:\n      enable_ssl: \"true\"\n  storaged:\n    config:\n      enable_ssl: \"true\"\n</code></pre> </li> <li> <p>Use <code>kubectl apply -f</code> to apply the file to the Kubernetes cluster.</p> </li> <li> <p>Verify that the values of <code>serverCert</code>, <code>serverKey</code>, <code>clientCert</code>, <code>clientKey</code>, <code>caCert</code> under the <code>sslCerts</code> field in the cluster configuration match the key names of the certificates and private keys stored in the created Secret.</p> <pre><code># Check the key names of the certificate and private key stored in the Secret. For example, check the key name of the CA certificate stored in the Secret.\nkubectl get secret ca-cert -o yaml\n</code></pre> <pre><code># Check the cluster configuration file.\nkubectl get nebulacluster nebula -o yaml\n</code></pre> <p>Example output:</p> <pre><code>...\nspec:\n  sslCerts:\n    serverSecret: server-cert\n    serverCert: tls.crt\n    serverKey: tls.key\n    clientSecret: client-cert\n    clientCert: tls.crt\n    clientKey: tls.key\n    caSecret: ca-cert\n    caCert: ca.crt    \n...\n</code></pre> <p>If the key names of the certificates and private keys stored in the Secret are different from the values of <code>serverCert</code>, <code>serverKey</code>, <code>clientCert</code>, <code>clientKey</code>, <code>caCert</code> under the <code>sslCerts</code> field in the cluster configuration, you need to execute <code>kubectl edit nebulacluster &lt;cluster_name&gt;</code> to manually modify the cluster configuration file.</p> <p>In the example output, the key name of the CA certificate in the TLS-type Secret is <code>tls.crt</code>, so you need to change the value of caCert from <code>ca.crt</code> to <code>tls.crt</code>.</p> </li> <li> <p>Use NebulaGraph Console to connect to the Graph service and establish a secure TLS connection.</p> <p>Example:</p> <pre><code>kubectl run -ti --image vesoft/nebula-console:v3.5.0 --restart=Never -- nebula-console -addr 10.98.xxx.xx  -port 9669 -u root -p nebula -enable_ssl -ssl_root_ca_path /path/to/cert/root.crt -ssl_cert_path /path/to/cert/client.crt -ssl_private_key_path /path/to/cert/client.key\n</code></pre> <ul> <li><code>-enable_ssl</code>: Use mTLS when connecting to NebulaGraph.</li> <li><code>-ssl_root_ca_path</code>: Specify the storage path of the CA root certificate.</li> <li><code>-ssl_cert_path</code>: Specify the storage path of the TLS public key certificate.</li> <li><code>-ssl_private_key_path</code>: Specify the storage path of the TLS private key.</li> <li>For details on using NebulaGraph Console to connect to the Graph service, see Connect to NebulaGraph.</li> </ul> </li> </ol> <p>At this point, you can enable mTLS in NebulaGraph.</p>"},{"location":"nebula-studio/about-studio/st-ug-limitations/","title":"Limitations","text":"<p>This topic introduces the limitations of Studio.</p>"},{"location":"nebula-studio/about-studio/st-ug-limitations/#architecture","title":"Architecture","text":"<p>For now, Studio v3.x supports x86_64 architecture only.</p>"},{"location":"nebula-studio/about-studio/st-ug-limitations/#upload_data","title":"Upload data","text":"<p>Only CSV files without headers can be uploaded, but no limitations are applied to the size and store period for a single file. The maximum data volume depends on the storage capacity of your machine.</p>"},{"location":"nebula-studio/about-studio/st-ug-limitations/#ngql_statements","title":"nGQL statements","text":"<p>On the Console page of Docker-based and RPM-based Studio v3.x, all the nGQL syntaxes except these are supported:</p> <ul> <li><code>USE &lt;space_name&gt;</code>: You cannot run such a statement on the Console page to choose a graph space. As an alternative, you can click a graph space name in the drop-down list of Current Graph Space.</li> <li>You cannot use line breaks (\\). As an alternative, you can use the Enter key to split a line.</li> </ul>"},{"location":"nebula-studio/about-studio/st-ug-limitations/#browser","title":"Browser","text":"<p>We recommend that you use the latest version of Chrome to get access to Studio.</p>"},{"location":"nebula-studio/about-studio/st-ug-what-is-graph-studio/","title":"What is NebulaGraph Studio","text":"<p>NebulaGraph Studio (Studio in short) is a browser-based visualization tool to manage NebulaGraph. It provides you with a graphical user interface to manipulate graph schemas, import data, and run nGQL statements to retrieve data. With Studio, you can quickly become a graph exploration expert from scratch. You can view the latest source code in the NebulaGraph GitHub repository, see nebula-studio for details.</p> <p>Note</p> <p>You can also try some functions online in Studio.</p>"},{"location":"nebula-studio/about-studio/st-ug-what-is-graph-studio/#released_versions","title":"Released versions","text":"<p>In addition to deploying Studio with RPM-based, DEB-based, or Tar-based package, or with Docker. You can also deploy Studio with Helm in the Kubernetes cluster. For more information, see Deploy Studio.</p> <p>The functions of the above four deployment methods are the same and may be restricted when using Studio. For more information, see Limitations.</p>"},{"location":"nebula-studio/about-studio/st-ug-what-is-graph-studio/#features","title":"Features","text":"<p>Studio can easily manage NebulaGraph data, with the following functions:</p> <ul> <li>On the Schema page, you can use the graphical user interface to create the space, Tag, Edge Type, Index, and view the statistics on the graph. It helps you quickly get started with NebulaGraph.</li> </ul> <ul> <li>On the Import page, you can operate batch import of vertex and edge data with clicks, and view a real-time import log.</li> </ul> <ul> <li>On the Console page, you can run nGQL statements and read the results in a human-friendly way.</li> </ul>"},{"location":"nebula-studio/about-studio/st-ug-what-is-graph-studio/#scenarios","title":"Scenarios","text":"<p>You can use Studio in one of these scenarios:</p> <ul> <li>You have a dataset, and you want to explore and analyze data in a visualized way. You can use Docker Compose to deploy NebulaGraph and then use Studio to explore or analyze data in a visualized way. </li> </ul> <ul> <li>You are a beginner of nGQL (NebulaGraph Query Language) and you prefer to use a GUI rather than a command-line interface (CLI) to learn the language.  </li> </ul>"},{"location":"nebula-studio/about-studio/st-ug-what-is-graph-studio/#authentication","title":"Authentication","text":"<p>Authentication is not enabled in NebulaGraph by default. Users can log into Studio with the <code>root</code> account and any password.</p> <p>When NebulaGraph enables authentication, users can only sign into Studio with the specified account. For more information, see Authentication.</p>"},{"location":"nebula-studio/about-studio/st-ug-what-is-graph-studio/#version_compatibility","title":"Version compatibility","text":"<p>Note</p> <p>The Studio version is released independently of the NebulaGraph core. The correspondence between the versions of Studio and the NebulaGraph core, as shown in the table below.</p> NebulaGraph version Studio version 3.5.0 3.7.0 3.4.0 ~ 3.4.1 3.7.0\u30013.6.0\u30013.5.1\u30013.5.0 3.3.0 3.5.1\u30013.5.0 3.0.0 \uff5e 3.2.0 3.4.1\u30013.4.0 3.1.0 3.3.2 3.0.0 3.2.x 2.6.x 3.1.x 2.6.x 3.1.x 2.0 &amp; 2.0.1 2.x 1.x 1.x"},{"location":"nebula-studio/about-studio/st-ug-what-is-graph-studio/#check_updates","title":"Check updates","text":"<p>Studio is in development. Users can view the latest releases features through Changelog.</p> <p>To view the Changelog, on the upper-right corner of the page, click the version and then New version.</p> <p></p>"},{"location":"nebula-studio/deploy-connect/st-ug-connect/","title":"Connect to NebulaGraph","text":"<p>After successfully launching Studio, you need to configure to connect to NebulaGraph. This topic describes how Studio connects to the NebulaGraph database.</p>"},{"location":"nebula-studio/deploy-connect/st-ug-connect/#prerequisites","title":"Prerequisites","text":"<p>Before connecting to the NebulaGraph database, you need to confirm the following information:</p> <ul> <li>The NebulaGraph services and Studio are started. For more information, see Deploy Studio.</li> </ul> <ul> <li>You have the local IP address and the port used by the Graph service of NebulaGraph. The default port is <code>9669</code>.  </li> </ul> <ul> <li>You have a NebulaGraph account and its password.</li> </ul>"},{"location":"nebula-studio/deploy-connect/st-ug-connect/#procedure","title":"Procedure","text":"<p>To connect Studio to NebulaGraph, follow these steps:</p> <ol> <li> <p>Type <code>http://&lt;ip_address&gt;:7001</code> in the address bar of your browser.</p> <p>The following login page shows that Studio is successfully connected to NebulaGraph.</p> <p></p> </li> <li> <p>On the Config Server page of Studio, configure these fields:</p> <ul> <li> <p>Graphd IP address: Enter the IP address of the Graph service of NebulaGraph. For example, <code>192.168.10.100</code>.</p> <p>Note</p> <ul> <li>When NebulaGraph and Studio are deployed on the same machine, you must enter the IP address of the machine, instead of <code>127.0.0.1</code> or <code>localhost</code>.</li> <li>When connecting to a NebulaGraph database on a new tab, a new session will overwrite the sessions of the old TAB. If you need to log in to multiple NebulaGraph databases simultaneously, you can use a different browser or non-trace mode.</li> </ul> </li> </ul> <ul> <li>Port: The port of the Graph service. The default port is <code>9669</code>.</li> </ul> <ul> <li> <p>Username and Password: Fill in the log in account according to the authentication settings of NebulaGraph.</p> <ul> <li>If authentication is not enabled, you can use <code>root</code> and any password as the username and its password.</li> </ul> <ul> <li>If authentication is enabled and no account information has been created, you can only log in as GOD role and use <code>root</code> and <code>nebula</code> as the username and its password.</li> </ul> <ul> <li>If authentication is enabled and different users are created and assigned roles, users in different roles log in with their accounts and passwords.</li> </ul> </li> </ul> </li> <li> <p>After the configuration, click the Connect button.</p> <p>Note</p> <p>One session continues for up to 30 minutes. If you do not operate Studio within 30 minutes, the active session will time out and you must connect to NebulaGraph again.</p> </li> </ol> <p>A welcome page is displayed on the first login, showing the relevant functions according to the usage process, and the test datasets can be automatically downloaded and imported.</p> <p>To visit the welcome page, click .</p>"},{"location":"nebula-studio/deploy-connect/st-ug-connect/#next_to_do","title":"Next to do","text":"<p>When Studio is successfully connected to NebulaGraph, you can do these operations:</p> <ul> <li>Create a schema on the Console page or on the Schema page.</li> <li>Batch import data on the Import page.</li> <li>Execute nGQL statements on the Console page.</li> <li>Design the schema visually on the Schema drafting page.</li> </ul> <p>Note</p> <p>The permissions of an account determine the operations that can be performed. For details, see Roles and privileges.</p>"},{"location":"nebula-studio/deploy-connect/st-ug-connect/#log_out","title":"Log out","text":"<p>If you want to reset NebulaGraph, you can log out and reconfigure the database.</p> <p>Click the user profile picture in the upper right corner, and choose Log out.</p>"},{"location":"nebula-studio/deploy-connect/st-ug-deploy/","title":"Deploy Studio","text":"<p>This topic describes how to deploy Studio locally by RPM, DEB, tar package and Docker.</p>"},{"location":"nebula-studio/deploy-connect/st-ug-deploy/#rpm-based_studio","title":"RPM-based Studio","text":""},{"location":"nebula-studio/deploy-connect/st-ug-deploy/#prerequisites","title":"Prerequisites","text":"<p>Before you deploy RPM-based Studio, you must confirm that:</p> <ul> <li>The NebulaGraph services are deployed and started. For more information, see NebulaGraph Database Manual.</li> </ul> <ul> <li>The Linux distribution is CentOS, install <code>lsof</code>.</li> </ul> <ul> <li> <p>Before the installation starts, the following ports are not occupied.</p> Port Description 7001 Web service provided by Studio. </li> </ul>"},{"location":"nebula-studio/deploy-connect/st-ug-deploy/#install","title":"Install","text":"<ol> <li> <p>Select and download the RPM package according to your needs. It is recommended to select the latest version. Common links are as follows:</p> Installation package Checksum NebulaGraph version nebula-graph-studio-3.7.0.x86_64.rpm nebula-graph-studio-3.7.0.x86_64.rpm.sha256 3.5.0 </li> <li> <p>Use <code>sudo rpm -i &lt;rpm_name&gt;</code> to install RPM package.</p> <p>For example, install Studio 3.7.0, use the following command. The default installation path is <code>/usr/local/nebula-graph-studio</code>.  <pre><code>$ sudo rpm -i nebula-graph-studio-3.7.0.x86_64.rpm\n</code></pre></p> <p>You can also install it to the specified path using the following command:  <pre><code>$ sudo rpm -i nebula-graph-studio-3.7.0.x86_64.rpm --prefix=&lt;path&gt; \n</code></pre></p> <p>When the screen returns the following message, it means that the PRM-based Studio has been successfully started.</p> <pre><code>Start installing NebulaGraph Studio now...\nNebulaGraph Studio has been installed.\nNebulaGraph Studio started automatically.\n</code></pre> </li> <li> <p>When Studio is started, use <code>http://&lt;ip address&gt;:7001</code> to get access to Studio.</p> <p>If you can see the Config Server page on the browser, Studio is started successfully.</p> <p></p> </li> </ol>"},{"location":"nebula-studio/deploy-connect/st-ug-deploy/#uninstall","title":"Uninstall","text":"<p>You can uninstall Studio using the following command:</p> <pre><code>$ sudo rpm -e nebula-graph-studio-3.7.0.x86_64\n</code></pre> <p>If these lines are returned, PRM-based Studio has been uninstalled.</p> <pre><code>NebulaGraph Studio removed, bye~\n</code></pre>"},{"location":"nebula-studio/deploy-connect/st-ug-deploy/#exception_handling","title":"Exception handling","text":"<p>If the automatic start fails during the installation process or you want to manually start or stop the service, use the following command:</p> <ul> <li>Start the service manually <pre><code>$ bash /usr/local/nebula-graph-studio/scripts/rpm/start.sh\n</code></pre></li> </ul> <ul> <li>Stop the service manually <pre><code>$ bash /usr/local/nebula-graph-studio/scripts/rpm/stop.sh\n</code></pre></li> </ul> <p>If you encounter an error <code>bind EADDRINUSE 0.0.0.0:7001</code> when starting the service, you can use the following command to check port 7001 usage.</p> <pre><code>$ lsof -i:7001\n</code></pre> <p>If the port is occupied and the process on that port cannot be terminated, you can modify the startup port within the studio configuration and restart the service.</p> <pre><code>//Modify the studio service configuration. The default path to the configuration file is `/usr/local/nebula-graph-studio`.\n$ vi etc/studio-api.yam\n\n//Modify this port number and change it to any \nPort: 7001\n\n//Restart service\n$ systemctl restart nebula-graph-studio.service\n</code></pre>"},{"location":"nebula-studio/deploy-connect/st-ug-deploy/#deb-based_studio","title":"DEB-based Studio","text":""},{"location":"nebula-studio/deploy-connect/st-ug-deploy/#prerequisites_1","title":"Prerequisites","text":"<p>Before you deploy DEB-based Studio, you must do a check of these:</p> <ul> <li>The NebulaGraph services are deployed and started. For more information, see NebulaGraph Database Manual.</li> </ul> <ul> <li>The Linux distribution is Ubuntu.</li> </ul> <ul> <li> <p>Before the installation starts, the following ports are not occupied.</p> Port Description 7001 Web service provided by Studio </li> </ul> <ul> <li>The path <code>/usr/lib/systemd/system</code> exists in the system. If not, create it manually.</li> </ul>"},{"location":"nebula-studio/deploy-connect/st-ug-deploy/#install_1","title":"Install","text":"<ol> <li> <p>Select and download the DEB package according to your needs. It is recommended to select the latest version. Common links are as follows:</p> Installation package Checksum NebulaGraph version nebula-graph-studio-3.7.0.x86_64.deb nebula-graph-studio-3.7.0.x86_64.deb.sha256 3.5.0 </li> <li> <p>Use <code>sudo dpkg -i &lt;deb_name&gt;</code> to install DEB package.</p> <p>For example, install Studio 3.7.0, use the following command:</p> <pre><code>$ sudo dpkg -i nebula-graph-studio-3.7.0.x86_64.deb\n</code></pre> </li> <li> <p>When Studio is started, use <code>http://&lt;ip address&gt;:7001</code> to get access to Studio.</p> <p>If you can see the Config Server page on the browser, Studio is started successfully.</p> <p></p> </li> </ol>"},{"location":"nebula-studio/deploy-connect/st-ug-deploy/#uninstall_1","title":"Uninstall","text":"<p>You can uninstall Studio using the following command:</p> <pre><code>$ sudo dpkg -r nebula-graph-studio\n</code></pre>"},{"location":"nebula-studio/deploy-connect/st-ug-deploy/#tar-based_studio","title":"tar-based Studio","text":""},{"location":"nebula-studio/deploy-connect/st-ug-deploy/#prerequisites_2","title":"Prerequisites","text":"<p>Before you deploy tar-based Studio, you must do a check of these:</p> <ul> <li>The NebulaGraph services are deployed and started. For more information, see NebulaGraph Database Manual.</li> </ul> <ul> <li> <p>Before the installation starts, the following ports are not occupied.</p> Port Description 7001 Web service provided by Studio </li> </ul>"},{"location":"nebula-studio/deploy-connect/st-ug-deploy/#install_and_deploy","title":"Install and deploy","text":"<ol> <li> <p>Select and download the tar package according to your needs. It is recommended to select the latest version. Common links are as follows:</p> Installation package Studio version nebula-graph-studio-3.7.0.x86_64.tar.gz 3.7.0 </li> <li> <p>Use <code>tar -xvf</code> to decompress the tar package.</p> <pre><code>$ tar -xvf nebula-graph-studio-3.7.0.x86_64.tar.gz\n</code></pre> </li> <li> <p>Deploy and start nebula-graph-studio.</p> <pre><code>$ cd nebula-graph-studio\n$ ./server\n</code></pre> </li> <li> <p>When Studio is started, use <code>http://&lt;ip address&gt;:7001</code> to get access to Studio.</p> <p>If you can see the Config Server page on the browser, Studio is started successfully.</p> <p></p> </li> </ol>"},{"location":"nebula-studio/deploy-connect/st-ug-deploy/#stop_service","title":"Stop Service","text":"<p>You can use <code>kill pid</code> to stop the service: <pre><code>$ kill $(lsof -t -i :7001) #stop nebula-graph-studio\n</code></pre></p>"},{"location":"nebula-studio/deploy-connect/st-ug-deploy/#docker-based_studio","title":"Docker-based Studio","text":""},{"location":"nebula-studio/deploy-connect/st-ug-deploy/#prerequisites_3","title":"Prerequisites","text":"<p>Before you deploy Docker-based Studio, you must do a check of these:</p> <ul> <li>The NebulaGraph services are deployed and started. For more information, see NebulaGraph Database Manual.</li> </ul> <ul> <li>On the machine where Studio will run, Docker Compose is installed and started. For more information, see Docker Compose Documentation.</li> </ul> <ul> <li> <p>Before the installation starts, the following ports are not occupied.</p> Port Description 7001 Web service provided by Studio </li> </ul>"},{"location":"nebula-studio/deploy-connect/st-ug-deploy/#procedure","title":"Procedure","text":"<p>To deploy and start Docker-based Studio, run the following commands. Here we use NebulaGraph v3.5.0 for demonstration:</p> <ol> <li> <p>Download the configuration files for the deployment.</p> Installation package NebulaGraph version nebula-graph-studio-3.7.0.tar.gz 3.5.0 </li> <li> <p>Create the <code>nebula-graph-studio-3.7.0</code> directory and decompress the installation package to the directory.</p> <pre><code>$ mkdir nebula-graph-studio-3.7.0 -zxvf nebula-graph-studio-3.7.0.gz -C nebula-graph-studio-3.7.0\n</code></pre> </li> <li> <p>Change to the <code>nebula-graph-studio-3.7.0</code> directory.    <pre><code>$ cd nebula-graph-studio-3.7.0\n</code></pre></p> </li> <li> <p>Pull the Docker image of Studio.</p> <pre><code>$ docker-compose pull\n</code></pre> </li> <li> <p>Build and start Docker-based Studio. In this command, <code>-d</code> is to run the containers in the background.</p> <pre><code>$ docker-compose up -d\n</code></pre> <p>If these lines are returned, Docker-based Studio v3.x is deployed and started.</p> <pre><code>Creating docker_web_1      ... done\n</code></pre> </li> <li> <p>When Docker-based Studio is started, use <code>http://&lt;ip address&gt;:7001</code> to get access to Studio.</p> <p>Note</p> <p>Run <code>ifconfig</code> or <code>ipconfig</code> to get the IP address of the machine where Docker-based Studio is running. On the machine running Docker-based Studio, you can use <code>http://localhost:7001</code> to get access to Studio.</p> <p>If you can see the Config Server page on the browser, Docker-based Studio is started successfully.</p> <p></p> </li> </ol>"},{"location":"nebula-studio/deploy-connect/st-ug-deploy/#helm-based_studio","title":"Helm-based Studio","text":"<p>This section describes how to deploy Studio with Helm.</p>"},{"location":"nebula-studio/deploy-connect/st-ug-deploy/#prerequisites_4","title":"Prerequisites","text":"<p>Before installing Studio, you need to install the following software and ensure the correct version of the software:</p> Software Requirement Kubernetes &gt;= 1.14 Helm &gt;= 3.2.0"},{"location":"nebula-studio/deploy-connect/st-ug-deploy/#install_2","title":"Install","text":"<ol> <li> <p>Use Git to clone the source code of Studio to the host.</p> <pre><code>$ git clone https://github.com/vesoft-inc/nebula-studio.git\n</code></pre> </li> <li> <p>Make the <code>nebula-studio</code> directory the current working directory.</p> <p><code>bash   $ cd nebula-studio</code></p> </li> <li> <p>Assume using release name:<code>my-studio</code>, installed Studio in Helm Chart.</p> <pre><code>$ helm upgrade --install my-studio --set service.type=NodePort --set service.port=30070deployment/helm\n</code></pre> <p>The configuration parameters of the Helm Chart are described below.</p> Parameter Default value Description replicaCount 0 The number of replicas for Deployment. image.nebulaStudio.name vesoft/nebula-graph-studio The image name of   nebula-graph-studio. image.nebulaStudio.version v3.7.0 The image version of nebula-graph-studio. service.type ClusterIP The service type, which should be one of <code>NodePort</code>, <code>ClusterIP</code>,   and <code>LoadBalancer</code>. service.port 7001 The expose port for nebula-graph-studio's web. service.nodePort 32701 The proxy port for accessing nebula-studio outside kubernetes   cluster. resources.nebulaStudio {} The resource limits/requests for nebula-studio. persistent.storageClassName \"\" The name of storageClass. The default value will be used   if not specified. persistent.size 5Gi The persistent volume size. </li> <li> <p>When Studio is started, use <code>http://&lt;node_address&gt;:30070/</code> to get access to Studio.</p> <p>If you can see the Config Server page on the browser, Studio is started successfully.</p> <p></p> </li> </ol>"},{"location":"nebula-studio/deploy-connect/st-ug-deploy/#uninstall_2","title":"Uninstall","text":"<pre><code> $ helm uninstall my-studio\n</code></pre>"},{"location":"nebula-studio/deploy-connect/st-ug-deploy/#next_to_do","title":"Next to do","text":"<p>On the Config Server page, connect Docker-based Studio to NebulaGraph. For more information, see Connect to NebulaGraph.</p>"},{"location":"nebula-studio/manage-schema/st-ug-crud-edge-type/","title":"Operate edge types","text":"<p>After a graph space is created in NebulaGraph, you can create edge types. With Studio, you can choose to use the Console page or the Schema page to create, retrieve, update, or delete edge types. This topic introduces how to use the Schema page to operate edge types in a graph space only.</p>"},{"location":"nebula-studio/manage-schema/st-ug-crud-edge-type/#prerequisites","title":"Prerequisites","text":"<p>To operate an edge type on the Schema page of Studio, you must do a check of these:</p> <ul> <li>Studio is connected to NebulaGraph.</li> <li>A graph space is created.</li> <li>Your account has the authority of GOD, ADMIN, or DBA.</li> </ul>"},{"location":"nebula-studio/manage-schema/st-ug-crud-edge-type/#create_an_edge_type","title":"Create an edge type","text":"<ol> <li> <p>In the toolbar, click the Schema tab.</p> </li> <li> <p>In the Graph Space List page, find a graph space and then click its name or click Schema in the Operations column.</p> </li> <li> <p>In the Current Graph Space field, confirm the name of the graph space. If necessary, you can choose another name to change the graph space.</p> </li> <li> <p>Click the Edge Type tab and click the + Create button.</p> </li> <li> <p>On the Create Edge Type page, do these settings:</p> <ul> <li>Name: Specify an appropriate name for the edge type. In this example, <code>serve</code> is used.</li> </ul> <ul> <li>Comment (Optional): Enter the description for edge type.</li> </ul> <ul> <li> <p>Define Properties (Optional): If necessary, click + Add Property to do these settings:</p> <ul> <li>Enter a property name.</li> </ul> <ul> <li>Select a data type.</li> </ul> <ul> <li>Select whether to allow null values..</li> </ul> <ul> <li>(Optional) Enter the default value.</li> </ul> <ul> <li>(Optional) Enter the description.</li> </ul> </li> </ul> <ul> <li>Set TTL (Time To Live) (Optional): If no index is set for the edge type, you can set the TTL configuration: In the upper left corner of the Set TTL panel, click the check box to expand the panel, and configure <code>TTL_COL</code> and <code>TTL_ DURATION</code> (in seconds). For more information about both parameters, see TTL configuration.</li> </ul> </li> <li> <p>When the preceding settings are completed, in the Equivalent to the following nGQL statement panel, you can see the nGQL statement equivalent to these settings.</p> <p></p> </li> <li> <p>Confirm the settings and then click the + Create button.</p> </li> </ol> <p>When the edge type is created successfully, the Define Properties panel shows all its properties on the list.</p>"},{"location":"nebula-studio/manage-schema/st-ug-crud-edge-type/#edit_an_edge_type","title":"Edit an edge type","text":"<ol> <li> <p>In the toolbar, click the Schema tab.</p> </li> <li> <p>In the Graph Space List page, find a graph space and then click its name or click Schema in the Operations column.</p> </li> <li> <p>In the Current Graph Space field, confirm the name of the graph space. If necessary, you can choose another name to change the graph space.</p> </li> <li> <p>Click the Edge Type tab, find an edge type and then click the button  in the Operations column.</p> </li> <li> <p>On the Edit page, do these operations:</p> <ul> <li>To edit a comment: Click Edit on the right of <code>Comment</code>.</li> <li>To edit a property: On the Define Properties panel, find a property, click Edit, and then change the data type or the default value.</li> </ul> <ul> <li>To delete a property: On the Define Properties panel, find a property, click Delete.</li> </ul> <ul> <li>To add more properties: On the Define Properties panel, click the Add Property button to add a new property.</li> </ul> <ul> <li>To set the TTL configuration: In the upper left corner of the Set TTL panel, click the check box and then set TTL.</li> </ul> <ul> <li>To delete the TTL configuration: When the Set TTL panel is expanded, in the upper left corner of the panel, click the check box to delete the configuration.</li> </ul> <ul> <li> <p>To edit the TTL configuration: On the Set TTL panel, click Edit and then change the configuration of <code>TTL_COL</code> and <code>TTL_DURATION</code> (in seconds).</p> <p>Note</p> <p>For information about the coexistence problem of TTL and index, see [TTL]((../../3.ngql-guide/8.clauses-and-options/ttl-options.md).</p> </li> </ul> </li> </ol>"},{"location":"nebula-studio/manage-schema/st-ug-crud-edge-type/#delete_an_edge_type","title":"Delete an Edge type","text":"<p>Danger</p> <p>Confirm the impact before deleting the Edge type. The deleted data cannot be restored if it is not backup.</p> <ol> <li> <p>In the toolbar, click the Schema tab.</p> </li> <li> <p>In the Graph Space List page, find a graph space and then click its name or click Schema in the Operations column.</p> </li> <li> <p>In the Current Graph Space field, confirm the name of the graph space. If necessary, you can choose another name to change the graph space.</p> </li> <li> <p>Click the Edge Type tab, find an edge type and then click the button  in the Operations column.</p> </li> <li> <p>Click OK to confirm in the pop-up dialog box.</p> </li> </ol>"},{"location":"nebula-studio/manage-schema/st-ug-crud-edge-type/#next_to_do","title":"Next to do","text":"<p>After the edge type is created, you can use the Console page to insert edge data one by one manually or use the Import page to bulk import edge data.</p>"},{"location":"nebula-studio/manage-schema/st-ug-crud-index/","title":"Operate Indexes","text":"<p>You can create an index for a Tag and/or an Edge type. An index lets traversal start from vertices or edges with the same property and it can make a query more efficient. With Studio, you can use the Console page or the Schema page to create, retrieve, and delete indexes. This topic introduces how to use the Schema page to operate an index only.</p> <p>Note</p> <p>You can create an index when a Tag or an Edge Type is created. But an index can decrease the write speed during data import. We recommend that you import data firstly and then create and rebuild an index. For more information, see Index overview.</p>"},{"location":"nebula-studio/manage-schema/st-ug-crud-index/#prerequisites","title":"Prerequisites","text":"<p>To operate an index on the Schema page of Studio, you must do a check of these:</p> <ul> <li>Studio is connected to NebulaGraph.</li> <li>A graph Space, Tags, and Edge Types are created.</li> <li>Your account has the authority of GOD, ADMIN, or DBA.</li> </ul>"},{"location":"nebula-studio/manage-schema/st-ug-crud-index/#create_an_index","title":"Create an index","text":"<ol> <li> <p>In the toolbar, click the Schema tab.</p> </li> <li> <p>In the Graph Space List page, find a graph space and then click its name or click Schema in the Operations column.</p> </li> <li> <p>In the Current Graph Space field, confirm the name of the graph space. If necessary, you can choose another name to change the graph space.</p> </li> <li> <p>Click the Index tab and then click the + Create button.</p> </li> <li> <p>On the Create page, do these settings:</p> <ul> <li>Index Type: Choose to create an index for a tag or for an edge type. In this example, Edge Type is chosen.</li> </ul> <ul> <li>Associated tag name: Choose a tag name or an edge type name. In this example, follow is chosen.</li> </ul> <ul> <li>Index Name: Specify a name for the new index. In this example, follow_index is used.</li> </ul> <ul> <li>Comment (Optional): Enter the description for index.</li> </ul> <ul> <li> <p>Indexed Properties (Optional): Click Add property, and then, in the dialog box, choose a property. If necessary, repeat this step to choose more properties. You can drag the properties to sort them. In this example, <code>degree</code> is chosen.</p> <p>Note</p> <p>The order of the indexed properties has an effect on the result of the <code>LOOKUP</code> statement. For more information, see nGQL Manual.</p> </li> </ul> </li> <li> <p>When the settings are done, the Equivalent to the following nGQL statement panel shows the statement equivalent to the settings.  </p> <p></p> </li> <li> <p>Confirm the settings and then click the + Create button. When an index is created, the index list shows the new index.</p> </li> </ol>"},{"location":"nebula-studio/manage-schema/st-ug-crud-index/#view_indexes","title":"View indexes","text":"<ol> <li> <p>In the toolbar, click the Schema tab.</p> </li> <li> <p>In the Graph Space List page, find a graph space and then click its name or click Schema in the Operations column.</p> </li> <li> <p>In the Current Graph Space field, confirm the name of the graph space. If necessary, you can choose another name to change the graph space.</p> </li> <li> <p>Click the Index tab, in the upper left corner, choose an index type, Tag or Edge Type.</p> </li> <li> <p>In the list, find an index and click its row. All its details are shown in the expanded row.</p> </li> </ol>"},{"location":"nebula-studio/manage-schema/st-ug-crud-index/#rebuild_indexes","title":"Rebuild indexes","text":"<ol> <li> <p>In the toolbar, click the Schema tab.</p> </li> <li> <p>In the Graph Space List page, find a graph space and then click its name or click Schema in the Operations column.</p> </li> <li> <p>In the Current Graph Space field, confirm the name of the graph space. If necessary, you can choose another name to change the graph space.</p> </li> <li> <p>Click the Index tab, in the upper left corner, choose an index type, Tag or Edge Type.</p> </li> <li> <p>Click the Index tab, find an index and then click the button Rebuild in the Operations column.</p> </li> </ol> <p>Note</p> <p>For more Information, see REBUILD INDEX.</p>"},{"location":"nebula-studio/manage-schema/st-ug-crud-index/#delete_an_index","title":"Delete an index","text":"<p>To delete an index on Schema, follow these steps:</p> <ol> <li> <p>In the toolbar, click the Schema tab.</p> </li> <li> <p>In the Graph Space List page, find a graph space and then click its name or click Schema in the Operations column.</p> </li> <li> <p>In the Current Graph Space field, confirm the name of the graph space. If necessary, you can choose another name to change the graph space.</p> </li> <li> <p>Click the Index tab, find an index and then click the button  in the Operations column.</p> </li> <li> <p>Click OK to confirm in the pop-up dialog box.</p> </li> </ol>"},{"location":"nebula-studio/manage-schema/st-ug-crud-space/","title":"Operate graph spaces","text":"<p>When Studio is connected to NebulaGraph, you can create or delete a graph space. You can use the Console page or the Schema page to do these operations. This article only introduces how to use the Schema page to operate graph spaces in NebulaGraph.</p>"},{"location":"nebula-studio/manage-schema/st-ug-crud-space/#prerequisites","title":"Prerequisites","text":"<p>To operate a graph space on the Schema page of Studio, you must do a check of these:</p> <ul> <li>Studio is connected to NebulaGraph.</li> <li>Your account has the authority of GOD. It means that:<ul> <li>If the authentication is enabled in NebulaGraph, you can use <code>root</code> and any password to sign in to Studio.</li> <li>If the authentication is disabled in NebulaGraph, you must use <code>root</code> and its password to sign in to Studio.</li> </ul> </li> </ul>"},{"location":"nebula-studio/manage-schema/st-ug-crud-space/#create_a_graph_space","title":"Create a graph space","text":"<ol> <li> <p>In the toolbar, click the Schema tab.</p> </li> <li> <p>In the Graph Space List page, click Create Space, do these settings:</p> <ul> <li>Name: Specify a name to the new graph space. In this example, <code>basketballplayer</code> is used. The name must be distinct in the database.</li> </ul> <ul> <li>Vid Type: The data types of VIDs are restricted to <code>FIXED_STRING(&lt;N&gt;)</code> or <code>INT64</code>. A graph space can only select one VID type. In this example, <code>FIXED_STRING(32)</code> is used. For more information, see VID.</li> </ul> <ul> <li>Comment: Enter the description for graph space. The maximum length is 256 bytes. By default, there will be no comments on a space. But in this example, <code>Statistics of basketball players</code> is used.</li> </ul> <ul> <li>Optional Parameters: Set the values of <code>partition_num</code> and <code>replica_factor</code> respectively. In this example, these parameters are set to <code>100</code> and <code>1</code> respectively. For more information, see <code>CREATE SPACE</code> syntax.</li> </ul> <p>In the Equivalent to the following nGQL statement panel, you can see the statement equivalent to the preceding settings.</p> <pre><code>CREATE SPACE basketballplayer (partition_num = 100, replica_factor = 1, vid_type = FIXED_STRING(32)) COMMENT = \"Statistics of basketball players\"\n</code></pre> </li> <li> <p>Confirm the settings and then click the + Create button. If the graph space is created successfully, you can see it on the graph space list.</p> </li> </ol> <p></p>"},{"location":"nebula-studio/manage-schema/st-ug-crud-space/#delete_a_graph_space","title":"Delete a graph space","text":"<p>Danger</p> <p>Deleting the space will delete all the data in it, and the deleted data cannot be restored if it is not backed up.</p> <ol> <li> <p>In the toolbar, click the Schema tab.</p> </li> <li> <p>In the Graph Space List, find the space you want to be deleted, and click Delete Graph Space in the Operation column.</p> <p></p> </li> <li> <p>On the dialog box, confirm the information and then click OK. </p> </li> </ol>"},{"location":"nebula-studio/manage-schema/st-ug-crud-space/#next_to_do","title":"Next to do","text":"<p>After a graph space is created, you can create or edit a schema, including:</p> <ul> <li>Operate tags</li> <li>Operate edge types</li> <li>Operate indexes</li> </ul>"},{"location":"nebula-studio/manage-schema/st-ug-crud-tag/","title":"Operate tags","text":"<p>After a graph space is created in NebulaGraph, you can create tags. With Studio, you can use the Console page or the Schema page to create, retrieve, update, or delete tags. This topic introduces how to use the Schema page to operate tags in a graph space only.</p>"},{"location":"nebula-studio/manage-schema/st-ug-crud-tag/#prerequisites","title":"Prerequisites","text":"<p>To operate a tag on the Schema page of Studio, you must do a check of these:</p> <ul> <li>Studio is connected to NebulaGraph.</li> <li>A graph space is created.</li> <li>Your account has the authority of GOD, ADMIN, or DBA.</li> </ul>"},{"location":"nebula-studio/manage-schema/st-ug-crud-tag/#create_a_tag","title":"Create a tag","text":"<ol> <li> <p>In the toolbar, click the Schema tab.</p> </li> <li> <p>In the Graph Space List page, find a graph space and then click its name or click Schema in the Operations column.</p> </li> <li> <p>In the Current Graph Space field, confirm the name of the graph space. If necessary, you can choose another name to change the graph space.</p> </li> <li> <p>Click the Tag tab and click the + Create button.</p> </li> <li> <p>On the Create page, do these settings:</p> <ul> <li>Name: Specify an appropriate name for the tag. In this example, <code>course</code> is specified.</li> </ul> <ul> <li>Comment (Optional): Enter the description for tag.</li> </ul> <ul> <li> <p>Define Properties (Optional): If necessary, click + Add Property to do these settings:</p> <ul> <li>Enter a property name.</li> </ul> <ul> <li>Select a data type.</li> </ul> <ul> <li>Select whether to allow null values..</li> </ul> <ul> <li>(Optional) Enter the default value.</li> </ul> <ul> <li>(Optional) Enter the description.</li> </ul> <ul> <li>Set TTL (Time To Live) (Optional): If no index is set for the tag, you can set the TTL configuration: In the upper left corner of the Set TTL panel, click the check box to expand the panel, and configure <code>TTL_COL</code> and <code>TTL_ DURATION</code> (in seconds). For more information about both parameters, see TTL configuration.</li> </ul> </li> </ul> </li> <li> <p>When the preceding settings are completed, in the Equivalent to the following nGQL statement panel, you can see the nGQL statement equivalent to these settings.</p> <p></p> </li> <li> <p>Confirm the settings and then click the + Create button. </p> </li> </ol> <p>When the tag is created successfully, the Define Properties panel shows all its properties on the list.</p>"},{"location":"nebula-studio/manage-schema/st-ug-crud-tag/#edit_a_tag","title":"Edit a tag","text":"<ol> <li> <p>In the toolbar, click the Schema tab.</p> </li> <li> <p>In the Graph Space List page, find a graph space and then click its name or click Schema in the Operations column.</p> </li> <li> <p>In the Current Graph Space field, confirm the name of the graph space. If necessary, you can choose another name to change the graph space.</p> </li> <li> <p>Click the Tag tab, find a tag and then click the button  in the Operations column.</p> </li> <li> <p>On the Edit page, do these operations:</p> <ul> <li>To edit a Comment: Click Edit on the right of <code>Comment</code>.</li> </ul> <ul> <li>To edit a property: On the Define Properties panel, find a property, click Edit, and then change the data type or the default value.</li> </ul> <ul> <li>To delete a property: On the Define Properties panel, find a property, click Delete.</li> </ul> <ul> <li>To add more properties: On the Define Properties panel, click the Add Property button to add a new property.</li> </ul> <ul> <li>To set the TTL configuration: In the upper left corner of the Set TTL panel, click the check box and then set TTL.</li> </ul> <ul> <li>To delete the TTL configuration: When the Set TTL panel is expanded, in the upper left corner of the panel, click the check box to delete the configuration.</li> </ul> <ul> <li> <p>To edit the TTL configuration: On the Set TTL panel, click Edit and then change the configuration of <code>TTL_COL</code> and <code>TTL_DURATION</code> (in seconds).</p> <p>Note</p> <p>The problem of coexistence of TTL and index, see [TTL]((../../3.ngql-guide/8.clauses-and-options/ttl-options.md).</p> </li> </ul> </li> </ol>"},{"location":"nebula-studio/manage-schema/st-ug-crud-tag/#delete_a_tag","title":"Delete a tag","text":"<p>Danger</p> <p>Confirm the impact before deleting the tag. The deleted data cannot be restored if it is not backup.</p> <ol> <li> <p>In the toolbar, click the Schema tab.</p> </li> <li> <p>In the Graph Space List page, find a graph space and then click its name or click Schema in the Operations column.</p> </li> <li> <p>In the Current Graph Space field, confirm the name of the graph space. If necessary, you can choose another name to change the graph space.</p> </li> <li> <p>Click the Tag tab, find an tag and then click the button  in the Operations column.</p> </li> <li> <p>Click OK to confirm delete a tag in the pop-up dialog box.</p> </li> </ol>"},{"location":"nebula-studio/manage-schema/st-ug-crud-tag/#next_to_do","title":"Next to do","text":"<p>After the tag is created, you can use the Console page to insert vertex data one by one manually or use the Import page to bulk import vertex data.</p>"},{"location":"nebula-studio/manage-schema/st-ug-view-schema/","title":"View Schema","text":"<p>Users can visually view schemas in NebulaGraph Studio.</p>"},{"location":"nebula-studio/manage-schema/st-ug-view-schema/#steps","title":"Steps","text":"<ol> <li> <p>In the toolbar, click the Schema tab.</p> </li> <li> <p>In the Graph Space List page, find a graph space and then click its name or click Schema in the Operations column.</p> </li> <li> <p>Click View Schema tab and click the Get Schema button.</p> </li> </ol>"},{"location":"nebula-studio/manage-schema/st-ug-view-schema/#other_operations","title":"Other operations","text":"<p>In the Graph Space List page, find a graph space and then perform the following operations in the Operations column:</p> <ul> <li>View Schema DDL: Displays schema creation statements for the graph space, including graph spaces, tags, edge types, and indexes.</li> <li>Clone Graph Space: Clones the schema of the graph space to a new graph space.</li> <li>Delete Graph pace: Deletes the graph space, including the schema and all vertices and edges.</li> </ul>"},{"location":"nebula-studio/quick-start/draft/","title":"Schema drafting","text":"<p>Studio supports the schema drafting function. Users can design their schemas on the canvas to visually display the relationships between vertices and edges, and apply the schema to a specified graph space after the design is completed.</p>"},{"location":"nebula-studio/quick-start/draft/#features","title":"Features","text":"<ul> <li>Design schema visually.</li> <li>Applies schema to a specified graph space.</li> <li>Export the schema as a PNG image.</li> </ul>"},{"location":"nebula-studio/quick-start/draft/#entry","title":"Entry","text":"<p>At the top navigation bar, click  .</p>"},{"location":"nebula-studio/quick-start/draft/#design_schema","title":"Design schema","text":"<p>The following steps take designing the schema of the <code>basketballplayer</code> dataset as an example to demonstrate how to use the schema drafting function.</p> <ol> <li>At the upper left corner of the page, click New.</li> <li>Create a tag by selecting the appropriate color tag under the canvas. You can hold down the left button and drag the tag into the canvas.</li> <li>Click the tag. On the right side of the page, you need to fill in the name of the tag as <code>player</code>, and add two properties <code>name</code> and <code>age</code>.</li> <li>Create a tag again. The name of the tag is <code>team</code>, and the property is <code>name</code>.</li> <li>Connect from the anchor point of the tag <code>player</code> to the anchor point of the tag <code>team</code>. Click the generated edge, fill in the name of the edge type as <code>serve</code>, and add two properties <code>start_year</code> and <code>end_year</code>.</li> <li>Connect from an anchor point of the tag <code>player</code> to another one of its own. Click the generated edge, fill in the name of the edge type as <code>follow</code>, and add a property <code>degree</code>.</li> <li>After the design is complete, click  at the top of the page to change the name of the draft, and then click  at the top right corner to save the draft.</li> </ol> <p></p>"},{"location":"nebula-studio/quick-start/draft/#apply_schema","title":"Apply schema","text":"<ol> <li>Select the draft that you want to import from the Draft list on the left side of the page, and then click Apply to Space at the upper right corner.</li> <li> <p>Import the schema to a new or existing space, and click Confirm.</p> <p>Note</p> <ul> <li>For more information about the parameters for creating a graph space, see CREATE SPACE.</li> <li>If the same schema exists in the graph space, the import operation fails, and the system prompts you to modify the name or change the graph space.</li> </ul> </li> </ol>"},{"location":"nebula-studio/quick-start/draft/#modify_schema","title":"Modify schema","text":"<p>Select the schema draft that you want to modify from the Draft list on the left side of the page. Click  at the upper right corner after the modification.</p> <p>Note</p> <p>The graph space to which the schema has been applied will not be modified synchronously.</p>"},{"location":"nebula-studio/quick-start/draft/#delete_schema","title":"Delete schema","text":"<p>Select the schema draft that you want to delete from the Draft list on the left side of the page, click X at the upper right corner of the thumbnail, and confirm to delete it.</p>"},{"location":"nebula-studio/quick-start/draft/#export_schema","title":"Export Schema","text":"<p>Click  at the upper right corner to export the schema as a PNG image.</p>"},{"location":"nebula-studio/quick-start/st-ug-console/","title":"Console","text":"<p>Studio console interface is shown as follows.</p> <p></p> <p>The following table lists various functions on the console interface.</p> number function descriptions 1 toolbar Click the Console tab to enter the console page. 2 select a space Select a space in the Current Graph Space list.  descriptions: Studio does not support running the <code>USE &lt;space_name&gt;</code> statements directly in the input box. 3 favorites Click the  button to expand the favorites, click one of the statements, and the input box will automatically enter the statement. 4 history list Click  button representing the statement record. In the statement running record list, click one of the statements, and the statement will be automatically entered in the input box. The list provides the record of the last 15 statements. 5 clean input box Click  button to clear the content entered in the input box. 6 run After inputting the nGQL statement in the input box, click  button to indicate the operation to start running the statement. 7 custom parameters display Click the  button to expand the custom parameters for parameterized query. For details, see Manage parameters. 8 input box After inputting the nGQL statements, click the  button to run the statement. You can input multiple statements and run them at the same time by using the separator <code>;</code>, and also use the symbol <code>//</code> to add comments. 9 statement running status After running the nGQL statement, the statement running status is displayed. If the statement runs successfully, the statement is displayed in green. If the statement fails, the statement is displayed in red. 10 add to favorites Click the  button to save the statement as a favorite, the button for the favorite statement is colored in yellow exhibit. 11 export CSV file or PNG file After running the nGQL statement to return the result, when the result is in Table window, click the  button to export as a CSV file. Switch to the Graph window and click the  button to save the results as a CSV file or PNG image export. 12 expand/hide execution results Click the  button to hide the result or click  button to expand the result. 13 close execution results Click the  button to close the result returned by this nGQL statement. 14 Table window Display the result from running nGQL statement. If the statement returns results, the window displays the results in a table. 15 Graph window Display the result from running nGQL statement. If the statement returns the complete vertex-edge result, the window displays the result as a graph . Click the  button on the right to view the overview panel."},{"location":"nebula-studio/quick-start/st-ug-create-schema/","title":"Create a schema","text":"<p>To batch import data into NebulaGraph, you must have a graph schema. You can create a schema on the Console page or on the Schema page of Studio.</p> <p>Note</p> <ul> <li>Users can use nebula-console to create a schema. For more information, see NebulaGraph Manual and Get started with NebulaGraph.</li> <li>Users can use the Schema drafting function to design schema visually. For more information, see Schema drafting.</li> </ul>"},{"location":"nebula-studio/quick-start/st-ug-create-schema/#prerequisites","title":"Prerequisites","text":"<p>To create a graph schema on Studio, you must do a check of these:</p> <ul> <li>Studio is connected to NebulaGraph.</li> </ul> <ul> <li>Your account has the privilege of GOD, ADMIN, or DBA.</li> </ul> <ul> <li>The schema is designed.</li> </ul> <ul> <li>A graph space is created.</li> </ul> <p>Note</p> <p>If no graph space exists and your account has the GOD privilege, you can create a graph space on the Console page. For more information, see CREATE SPACE.</p>"},{"location":"nebula-studio/quick-start/st-ug-create-schema/#create_a_schema_with_schema","title":"Create a schema with Schema","text":"<ol> <li> <p>Create tags. For more information, see Operate tags.</p> </li> <li> <p>Create edge types. For more information, see Operate edge types.</p> </li> </ol>"},{"location":"nebula-studio/quick-start/st-ug-create-schema/#create_a_schema_with_console","title":"Create a schema with Console","text":"<ol> <li> <p>In the toolbar, click the Console tab.</p> </li> <li> <p>In the Current Graph Space field, choose a graph space name. In this example, basketballplayer is used.</p> <p></p> </li> <li> <p>In the input box, enter these statements one by one and click the button Run.</p> <pre><code>// To create a tag named \"player\", with two property\nnebula&gt; CREATE TAG player(name string, age int);\n\n// To create a tag named \"team\", with one property\nnebula&gt; CREATE TAG team(name string);\n\n// To create an edge type named \"follow\", with one properties\nnebula&gt; CREATE EDGE follow(degree int);\n\n// To create an edge type named \"serve\", with two properties\nnebula&gt; CREATE EDGE serve(start_year int, end_year int);\n</code></pre> </li> </ol> <p>If the preceding statements are executed successfully, the schema is created. You can run the statements as follows to view the schema.</p> <pre><code>// To list all the tags in the current graph space\nnebula&gt; SHOW TAGS;\n\n// To list all the edge types in the current graph space\nnebula&gt; SHOW EDGES;\n\n// To view the definition of the tags and edge types\nDESCRIBE TAG player;\nDESCRIBE TAG team;\nDESCRIBE EDGE follow;\nDESCRIBE EDGE serve;\n</code></pre> <p>If the schema is created successfully, in the result window, you can see the definition of the tags and edge types.</p>"},{"location":"nebula-studio/quick-start/st-ug-create-schema/#next_to_do","title":"Next to do","text":"<p>When a schema is created, you can import data.</p>"},{"location":"nebula-studio/quick-start/st-ug-import-data/","title":"Import data","text":"<p>After CSV files of data and a schema are created, you can use the Import page to batch import vertex and edge data into NebulaGraph for graph exploration and data analysis.</p>"},{"location":"nebula-studio/quick-start/st-ug-import-data/#prerequisites","title":"Prerequisites","text":"<p>To batch import data, do a check of these:</p> <ul> <li>Studio is connected to NebulaGraph.</li> </ul> <ul> <li>A schema is created.</li> </ul> <ul> <li>CSV files meet the demands of the Schema.</li> </ul> <ul> <li>Your account has privilege of GOD, ADMIN, DBA, or USER.</li> </ul>"},{"location":"nebula-studio/quick-start/st-ug-import-data/#procedure","title":"Procedure","text":"<p>Before importing data, you need to upload the file first and then create the import task.</p>"},{"location":"nebula-studio/quick-start/st-ug-import-data/#upload_files","title":"Upload files","text":"<p>To upload files, follow these steps:</p> <ol> <li> <p>In the toolbar, click the Import tab.</p> </li> <li> <p>On the Upload Files page, click the Upload Files button and then choose CSV files. In this example, <code>edge_serve.csv</code>, <code>edge_follow.csv</code>, <code>vertex_player.csv</code>, and <code>vertex_team.csv</code> are chosen.</p> <p>Note</p> <p>You can choose multiple CSV files at the same time. The CSV file used in this article can be downloaded in the Design a schema.</p> </li> <li> <p>After uploading, you can click the  button in the Operations column to preview the file content, or click the  button to delete the uploaded file.</p> <p></p> </li> </ol>"},{"location":"nebula-studio/quick-start/st-ug-import-data/#import_data_1","title":"Import Data","text":"<p>To batch import data, follow these steps:</p> <ol> <li> <p>In the toolbar, click the Import tab.</p> </li> <li> <p>In Import tab, click the Import Data.</p> </li> <li> <p>On the Import Data page, click + New Import button to complete these operations:</p> <p>Caution</p> <p>users can click Import Template to download the example configuration file <code>example.yaml</code>, and upload the configuration file after configuration. The configuration mode is similar to that of NebulaGraph Importer, but all file paths for configuration files in the template retain the filename only. And make sure all CSV data files are uploaded before importing the YAML file.</p> <ul> <li>Select a graph space.</li> <li>Fill in the task name.</li> <li>(Optional) Fill in the batch size.</li> <li>In the Map Vertices section, click the + Bind Datasource button, select bind source file in the dialog box, and click the Confirm button, the <code>vertex_player.csv</code> file is chosen.<ul> <li>In the vertices 1 drop-down list, click Select CSV Index, and select the column where vertexID is located in the pop-up dialog box.</li> <li>Click the + Add Tag button and click the  icon on the right. In the displayed property list, bind the source data for the tag property. In this example, player is used for the <code>vertex_player.csv</code> file. For the player tag, choose Column 1 for the age property, and choose Column 2 for the name property.</li> </ul> </li> <li> <p>In the Map Edges section, click the + Bind Datasource button, select bind source file in the dialog box, and click the Confirm button, the <code>edge_follow.csv</code> file is chosen.</p> <ul> <li>In the vertices 1 drop-down list, click Select Edge Type. In this example, follow is chosen.</li> <li>Based on the edge type property, select the corresponding data column from the <code>edge_follow.csv</code> file. srcId and dstId are the VIDs of the source vertex and destination vertex of an edge. In this example, srcId must be set to the VIDs of the player and dstId must be set to the VIDs of another player. Rank is optional.</li> </ul> <p></p> </li> </ul> </li> <li> <p>After completing the settings, click the Import button.</p> </li> <li> <p>You need to enter the password of your NebulaGraph account before importing data.   </p> </li> <li> <p>After importing data, you can view logs, download logs, download configuration files, and delete tasks on the Import Data tab.   </p> </li> </ol>"},{"location":"nebula-studio/quick-start/st-ug-plan-schema/","title":"Design a schema","text":"<p>To manipulate graph data in NebulaGraph with Studio, you must have a graph schema. This article introduces how to design a graph schema for NebulaGraph.</p> <p>A graph schema for NebulaGraph must have these essential elements:</p> <ul> <li>Tags (namely vertex types) and their properties.</li> </ul> <ul> <li>Edge types and their properties.</li> </ul> <p>In this article, you can install the sample data set basketballplayer and use it to explore a pre-designed schema.</p> <p>This table gives all the essential elements of the schema.</p> Element Name Property name (Data type) Description Tag player - <code>name</code> (<code>string</code>) - <code>age</code> (<code>int</code>) Represents the player. Tag team - <code>name</code> (<code>string</code>) Represents the team. Edge type serve - <code>start_year</code> (<code>int</code>)  - <code>end_year</code> (<code>int</code>) Represent the players behavior.This behavior connects the player to the team, and the direction is from player to team. Edge type follow - <code>degree</code> (<code>int</code>) Represent the players behavior.This behavior connects the player to the player, and the direction is from a player to a player. <p>This figure shows the relationship (serve/follow) between a player and a team.</p> <p></p>"},{"location":"nebula-studio/troubleshooting/st-ug-config-server-errors/","title":"Connecting to the database error","text":""},{"location":"nebula-studio/troubleshooting/st-ug-config-server-errors/#problem_description","title":"Problem description","text":"<p>According to the connect Studio operation, it prompts failed.</p>"},{"location":"nebula-studio/troubleshooting/st-ug-config-server-errors/#possible_causes_and_solutions","title":"Possible causes and solutions","text":"<p>You can troubleshoot the problem by following the steps below.</p>"},{"location":"nebula-studio/troubleshooting/st-ug-config-server-errors/#step1_confirm_that_the_format_of_the_host_field_is_correct","title":"Step1: Confirm that the format of the Host field is correct","text":"<p>You must fill in the IP address (<code>graph_server_ip</code>) and port of the NebulaGraph database Graph service. If no changes are made, the port defaults to <code>9669</code>. Even if NebulaGraph and Studio are deployed on the current machine, you must use the local IP address instead of <code>127.0.0.1</code>, <code>localhost</code> or <code>0.0.0.0</code>.</p>"},{"location":"nebula-studio/troubleshooting/st-ug-config-server-errors/#step2_confirm_that_the_username_and_password_are_correct","title":"Step2: Confirm that the username and password are correct","text":"<p>If authentication is not enabled, you can use root and any password as the username and its password.</p> <p>If authentication is enabled and different users are created and assigned roles, users in different roles log in with their accounts and passwords.</p>"},{"location":"nebula-studio/troubleshooting/st-ug-config-server-errors/#step3_confirm_that_nebulagraph_service_is_normal","title":"Step3: Confirm that NebulaGraph service is normal","text":"<p>Check NebulaGraph service status. Regarding the operation of viewing services:</p> <ul> <li>If you compile and deploy NebulaGraph on a Linux server, refer to the NebulaGraph service.</li> </ul> <ul> <li>If you use NebulaGraph deployed by Docker Compose and RPM, refer to the NebulaGraph service status and ports.</li> </ul> <p>If the NebulaGraph service is normal, proceed to Step 4 to continue troubleshooting. Otherwise, please restart NebulaGraph service.</p> <p>Note</p> <p>If you used <code>docker-compose up -d</code> to satrt NebulaGraph before, you must run the <code>docker-compose down</code> to stop NebulaGraph.</p>"},{"location":"nebula-studio/troubleshooting/st-ug-config-server-errors/#step4_confirm_the_network_connection_of_the_graph_service_is_normal","title":"Step4: Confirm the network connection of the Graph service is normal","text":"<p>Run a command (for example, telnet  9669) on the Studio machine to confirm whether NebulaGraph's Graph service network connection is normal. <p>If the connection fails, check according to the following steps:</p> <ul> <li>If Studio and NebulaGraph are on the same machine, check if the port is exposed.</li> </ul> <ul> <li>If Studio and NebulaGraph are not on the same machine, check the network configuration of the NebulaGraph server, such as firewall, gateway, and port.</li> </ul> <p>If you cannot connect to the NebulaGraph service after troubleshooting with the above steps, please go to the NebulaGraph forum for consultation.</p>"},{"location":"nebula-studio/troubleshooting/st-ug-connection-errors/","title":"Cannot access to Studio","text":""},{"location":"nebula-studio/troubleshooting/st-ug-connection-errors/#problem_description","title":"Problem description","text":"<p>I follow the document description and visit <code>127.0.0.1:7001</code> or <code>0.0.0.0:7001</code> after starting Studio, why can\u2019t I open the page?</p>"},{"location":"nebula-studio/troubleshooting/st-ug-connection-errors/#possible_causes_and_solutions","title":"Possible causes and solutions","text":"<p>You can troubleshoot the problem by following the steps below.</p>"},{"location":"nebula-studio/troubleshooting/st-ug-connection-errors/#step1_confirm_system_architecture","title":"Step1: Confirm system architecture","text":"<p>It is necessary to confirm whether the machine where the Studio service is deployed is of x86_64 architecture. Currently, Studio only supports x86_64 architecture.</p>"},{"location":"nebula-studio/troubleshooting/st-ug-connection-errors/#step2_check_if_the_studio_service_starts_normally","title":"Step2: Check if the Studio service starts normally","text":"<ul> <li>For Studio deployed with RPM or DEB packages, use <code>systemctl status nebula-graph-studio</code> to see the running status.</li> </ul> <ul> <li>For Studio deployed with tar package, use <code>sudo lsof -i:7001</code> to check port status.</li> </ul> <ul> <li> <p>For Studio deployed with docker, use <code>docker-compose ps</code> to see the running status. Run <code>docker-compose ps</code> to check if the service has started normally.</p> <p>If the service is normal, the return result is as follows. Among them, the <code>State</code> column should all be displayed as <code>Up</code>.</p> <pre><code>    Name                          Command               State               Ports\n------------------------------------------------------------------------------------------------------\nnebula-web-docker_client_1     ./nebula-go-api                  Up      0.0.0.0:32782-&gt;8080/tcp\nnebula-web-docker_importer_1   nebula-importer --port=569 ...   Up      0.0.0.0:32783-&gt;5699/tcp\nnebula-web-docker_nginx_1      /docker-entrypoint.sh ngin ...   Up      0.0.0.0:7001-&gt;7001/tcp, 80/tcp\nnebula-web-docker_web_1        docker-entrypoint.sh npm r ...   Up      0.0.0.0:32784-&gt;7001/tcp\n</code></pre> </li> </ul> <p>If the above result is not returned, stop Studio and restart it first. For details, refer to Deploy Studio.</p> <p>!!! note</p> <pre><code>    If you used `docker-compose up -d` to satrt NebulaGraph before, you must run the `docker-compose down` to stop NebulaGraph.\n</code></pre>"},{"location":"nebula-studio/troubleshooting/st-ug-connection-errors/#step3_confirm_address","title":"Step3: Confirm address","text":"<p>If Studio and the browser are on the same machine, users can use <code>localhost:7001</code>, <code>127.0.0.1:7001</code> or <code>0.0.0.0:7001</code> in the browser to access Studio.</p> <p>If Studio and the browser are not on the same machine, you must enter <code>&lt;studio_server_ip&gt;:7001</code> in the browser. Among them, <code>studio_server_ip</code> refers to the IP address of the machine where the Studio service is deployed.</p>"},{"location":"nebula-studio/troubleshooting/st-ug-connection-errors/#step4_confirm_network_connection","title":"Step4: Confirm network connection","text":"<p>Run <code>curl &lt;studio_server_ip&gt;:7001</code> -I to confirm if it is normal. If it returns <code>HTTP/1.1 200 OK</code>, it means that the network is connected normally.</p> <p>If the connection is refused, check according to the following steps:</p> <p>If the connection fails, check according to the following steps:</p> <ul> <li>If Studio and NebulaGraph are on the same machine, check if the port is exposed.</li> </ul> <ul> <li>If Studio and NebulaGraph are not on the same machine, check the network configuration of the NebulaGraph server, such as firewall, gateway, and port.</li> </ul> <p>If you cannot connect to the NebulaGraph service after troubleshooting with the above steps, please go to the NebulaGraph forum for consultation.</p>"},{"location":"nebula-studio/troubleshooting/st-ug-faq/","title":"FAQ","text":"<p>Why can't I use a function?</p> <p>If you find that a function cannot be used, it is recommended to troubleshoot the problem according to the following steps:</p> <ol> <li> <p>Confirm that NebulaGraph is the latest version. If you use Docker Compose to deploy the NebulaGraph database, it is recommended to run <code>docker-compose pull &amp;&amp; docker-compose up -d</code> to pull the latest Docker image and start the container.</p> </li> <li> <p>Confirm that Studio is the latest version. For more information, refer to check updates.</p> </li> <li> <p>Search the nebula forum, nebula and nebula-studio projects on the GitHub to confirm if there are already similar problems.</p> </li> <li> <p>If none of the above steps solve the problem, you can submit a problem on the forum.</p> </li> </ol>"},{"location":"reuse/source-monitoring-metrics/","title":"Source monitoring metrics","text":""},{"location":"reuse/source-monitoring-metrics/#graph","title":"Graph","text":"Parameter Description <code>num_active_queries</code> The number of changes in the number of active queries. Formula: The number of started queries minus the number of finished queries within a specified time. <code>num_active_sessions</code> The number of changes in the number of active sessions. Formula: The number of logged in sessions minus the number of logged out sessions within a specified time.For example, when querying <code>num_active_sessions.sum.5</code>, if there were 10 sessions logged in and 30 sessions logged out in the last 5 seconds, the value of this metric is <code>-20</code> (10-30). <code>num_aggregate_executors</code> The number of executions for the Aggregation operator. <code>num_auth_failed_sessions_bad_username_password</code> The number of sessions where authentication failed due to incorrect username and password. <code>num_auth_failed_sessions_out_of_max_allowed</code> The number of sessions that failed to authenticate logins because the value of the parameter <code>FLAG_OUT_OF_MAX_ALLOWED_CONNECTIONS</code> was exceeded. <code>num_auth_failed_sessions</code> The number of sessions in which login authentication failed. <code>num_indexscan_executors</code> The number of executions for index scan operators. <code>num_killed_queries</code> The number of killed queries. <code>num_opened_sessions</code> The number of sessions connected to the server. <code>num_queries</code> The number of queries. <code>num_query_errors_leader_changes</code> The number of the raft leader changes due to query errors. <code>num_query_errors</code> The number of query errors. <code>num_reclaimed_expired_sessions</code> The number of expired sessions actively reclaimed by the server. <code>num_rpc_sent_to_metad_failed</code> The number of failed RPC requests that the Graphd service sent to the Metad service. <code>num_rpc_sent_to_metad</code> The number of RPC requests that the Graphd service sent to the Metad service. <code>num_rpc_sent_to_storaged_failed</code> The number of failed RPC requests that the Graphd service sent to the Storaged service. <code>num_rpc_sent_to_storaged</code> The number of RPC requests that the Graphd service sent to the Storaged service. <code>num_sentences</code> The number of statements received by the Graphd service. <code>num_slow_queries</code> The number of slow queries. <code>num_sort_executors</code> The number of executions for the Sort operator. <code>optimizer_latency_us</code> The latency of executing optimizer statements. <code>query_latency_us</code> The latency of queries. <code>slow_query_latency_us</code> The latency of slow queries. <code>num_queries_hit_memory_watermark</code> The number of queries reached the memory watermark."},{"location":"reuse/source-monitoring-metrics/#meta","title":"Meta","text":"Parameter Description <code>commit_log_latency_us</code> The latency of committing logs in Raft. <code>commit_snapshot_latency_us</code> The latency of committing snapshots in Raft. <code>heartbeat_latency_us</code> The latency of heartbeats. <code>num_heartbeats</code> The number of heartbeats. <code>num_raft_votes</code> The number of votes in Raft. <code>transfer_leader_latency_us</code> The latency of transferring the raft leader. <code>num_agent_heartbeats</code> The number of heartbeats for the AgentHBProcessor. <code>agent_heartbeat_latency_us</code> The latency of the AgentHBProcessor. <code>replicate_log_latency_us</code> The latency of replicating the log record to most nodes by Raft. <code>num_send_snapshot</code> The number of times that Raft sends snapshots to other nodes. <code>append_log_latency_us</code> The latency of replicating the log record to a single node by Raft. <code>append_wal_latency_us</code> The Raft write latency for a single WAL. <code>num_grant_votes</code> The number of times that Raft votes for other nodes. <code>num_start_elect</code> The number of times that Raft starts an election."},{"location":"reuse/source-monitoring-metrics/#storage","title":"Storage","text":"Parameter Description <code>add_edges_latency_us</code> The latency of adding edges. <code>add_vertices_latency_us</code> The latency of adding vertices. <code>commit_log_latency_us</code> The latency of committing logs in Raft. <code>commit_snapshot_latency_us</code> The latency of committing snapshots in Raft. <code>delete_edges_latency_us</code> The latency of deleting edges. <code>delete_vertices_latency_us</code> The latency of deleting vertices. <code>get_neighbors_latency_us</code> The latency of querying neighbor vertices. <code>get_dst_by_src_latency_us</code> The latency of querying the destination vertex by the source vertex. <code>num_get_prop</code> The number of executions for the GetPropProcessor. <code>num_get_neighbors_errors</code> The number of execution errors for the GetNeighborsProcessor. <code>num_get_dst_by_src_errors</code> The number of execution errors for the GetDstBySrcProcessor. <code>get_prop_latency_us</code> The latency of executions for the GetPropProcessor. <code>num_edges_deleted</code> The number of deleted edges. <code>num_edges_inserted</code> The number of inserted edges. <code>num_raft_votes</code> The number of votes in Raft. <code>num_rpc_sent_to_metad_failed</code> The number of failed RPC requests that the Storage service sent to the Meta service. <code>num_rpc_sent_to_metad</code> The number of RPC requests that the Storaged service sent to the Metad service. <code>num_tags_deleted</code> The number of deleted tags. <code>num_vertices_deleted</code> The number of deleted vertices. <code>num_vertices_inserted</code> The number of inserted vertices. <code>transfer_leader_latency_us</code> The latency of transferring the raft leader. <code>lookup_latency_us</code> The latency of executions for the LookupProcessor. <code>num_lookup_errors</code> The number of execution errors for the LookupProcessor. <code>num_scan_vertex</code> The number of executions for the ScanVertexProcessor. <code>num_scan_vertex_errors</code> The number of execution errors for the ScanVertexProcessor. <code>update_edge_latency_us</code> The latency of executions for the UpdateEdgeProcessor. <code>num_update_vertex</code> The number of executions for the UpdateVertexProcessor. <code>num_update_vertex_errors</code> The number of execution errors for the UpdateVertexProcessor. <code>kv_get_latency_us</code> The latency of executions for the Getprocessor. <code>kv_put_latency_us</code> The latency of executions for the PutProcessor. <code>kv_remove_latency_us</code> The latency of executions for the RemoveProcessor. <code>num_kv_get_errors</code> The number of execution errors for the GetProcessor. <code>num_kv_get</code> The number of executions for the GetProcessor. <code>num_kv_put_errors</code> The number of execution errors for the PutProcessor. <code>num_kv_put</code> The number of executions for the PutProcessor. <code>num_kv_remove_errors</code> The number of execution errors for the RemoveProcessor. <code>num_kv_remove</code> The number of executions for the RemoveProcessor. <code>forward_tranx_latency_us</code> The latency of transmission. <code>scan_edge_latency_us</code> The latency of executions for the ScanEdgeProcessor. <code>num_scan_edge_errors</code> The number of execution errors for the ScanEdgeProcessor. <code>num_scan_edge</code> The number of executions for the ScanEdgeProcessor. <code>scan_vertex_latency_us</code> The latency of executions for the ScanVertexProcessor. <code>num_add_edges</code> The number of times that edges are added. <code>num_add_edges_errors</code> The number of errors when adding edges. <code>num_add_vertices</code> The number of times that vertices are added. <code>num_start_elect</code> The number of times that Raft starts an election. <code>num_add_vertices_errors</code> The number of errors when adding vertices. <code>num_delete_vertices_errors</code> The number of errors when deleting vertices. <code>append_log_latency_us</code> The latency of replicating the log record to a single node by Raft. <code>num_grant_votes</code> The number of times that Raft votes for other nodes. <code>replicate_log_latency_us</code> The latency of replicating the log record to most nodes by Raft. <code>num_delete_tags</code> The number of times that tags are deleted. <code>num_delete_tags_errors</code> The number of errors when deleting tags. <code>num_delete_edges</code> The number of edge deletions. <code>num_delete_edges_errors</code> The number of errors when deleting edges <code>num_send_snapshot</code> The number of times that snapshots are sent. <code>update_vertex_latency_us</code> The latency of executions for the UpdateVertexProcessor. <code>append_wal_latency_us</code> The Raft write latency for a single WAL. <code>num_update_edge</code> The number of executions for the UpdateEdgeProcessor. <code>delete_tags_latency_us</code> The latency of deleting tags. <code>num_update_edge_errors</code> The number of execution errors for the UpdateEdgeProcessor. <code>num_get_neighbors</code> The number of executions for the GetNeighborsProcessor. <code>num_get_dst_by_src</code> The number of executions for the GetDstBySrcProcessor. <code>num_get_prop_errors</code> The number of execution errors for the GetPropProcessor. <code>num_delete_vertices</code> The number of times that vertices are deleted. <code>num_lookup</code> The number of executions for the LookupProcessor. <code>num_sync_data</code> The number of times the Storage service synchronizes data from the Drainer. <code>num_sync_data_errors</code> The number of errors that occur when the Storage service synchronizes data from the Drainer. <code>sync_data_latency_us</code> The latency of the Storage service synchronizing data from the Drainer."},{"location":"reuse/source-monitoring-metrics/#graph_space","title":"Graph space","text":"<p>Note</p> <p>Space-level metrics are created dynamically, so that only when the behavior is triggered in the graph space, the corresponding metric is created and can be queried by the user.</p> Parameter Description <code>num_active_queries</code> The number of queries currently being executed. <code>num_queries</code> The number of queries. <code>num_sentences</code> The number of statements received by the Graphd service. <code>optimizer_latency_us</code> The latency of executing optimizer statements. <code>query_latency_us</code> The latency of queries. <code>num_slow_queries</code> The number of slow queries. <code>num_query_errors</code> The number of query errors. <code>num_query_errors_leader_changes</code> The number of raft leader changes due to query errors. <code>num_killed_queries</code> The number of killed queries. <code>num_aggregate_executors</code> The number of executions for the Aggregation operator. <code>num_sort_executors</code> The number of executions for the Sort operator. <code>num_indexscan_executors</code> The number of executions for index scan operators. <code>num_auth_failed_sessions_bad_username_password</code> The number of sessions where authentication failed due to incorrect username and password. <code>num_auth_failed_sessions</code> The number of sessions in which login authentication failed. <code>num_opened_sessions</code> The number of sessions connected to the server. <code>num_queries_hit_memory_watermark</code> The number of queries reached the memory watermark. <code>num_reclaimed_expired_sessions</code> The number of expired sessions actively reclaimed by the server. <code>num_rpc_sent_to_metad_failed</code> The number of failed RPC requests that the Graphd service sent to the Metad service. <code>num_rpc_sent_to_metad</code> The number of RPC requests that the Graphd service sent to the Metad service. <code>num_rpc_sent_to_storaged_failed</code> The number of failed RPC requests that the Graphd service sent to the Storaged service. <code>num_rpc_sent_to_storaged</code> The number of RPC requests that the Graphd service sent to the Storaged service. <code>slow_query_latency_us</code> The latency of slow queries."},{"location":"reuse/source-monitoring-metrics/#single_process_metrics","title":"Single process metrics","text":"<p>Graph, Meta, and Storage services all have their own single process metrics.</p> Parameter Description <code>context_switches_total</code> The number of context switches. <code>cpu_seconds_total</code> The CPU usage based on user and system time. <code>memory_bytes_gauge</code> The number of bytes of memory used. <code>open_filedesc_gauge</code> The number of file descriptors. <code>read_bytes_total</code> The number of bytes read. <code>write_bytes_total</code> The number of bytes written."},{"location":"reuse/source_connect-to-nebula-graph/","title":"Source connect to nebula graph","text":"<p>This topic provides basic instruction on how to use the native CLI client NebulaGraph Console to connect to NebulaGraph.</p> <p>Caution</p> <p>When connecting to NebulaGraph for the first time, you must register the Storage Service before querying data.</p> <p>NebulaGraph supports multiple types of clients, including a CLI client, a GUI client, and clients developed in popular programming languages. For more information, see the client list.</p>"},{"location":"reuse/source_connect-to-nebula-graph/#prerequisites","title":"Prerequisites","text":"<ul> <li>You have started NebulaGraph services.</li> </ul> <ul> <li>The machine on which you plan to run NebulaGraph Console has network access to the Graph Service of NebulaGraph.</li> </ul> <ul> <li> <p>The NebulaGraph Console version is compatible with the NebulaGraph version.</p> <p>Note</p> <p>NebulaGraph Console and NebulaGraph of the same version number are the most compatible. There may be compatibility issues when connecting to NebulaGraph with a different version of NebulaGraph Console. The error message <code>incompatible version between client and server</code> is displayed when there is such an issue.</p> </li> </ul>"},{"location":"reuse/source_connect-to-nebula-graph/#steps","title":"Steps","text":"<ol> <li> <p>On the NebulaGraph Console releases page, select a NebulaGraph Console version and click Assets.</p> <p>Note</p> <p>It is recommended to select the latest version.</p> </li> <li> <p>In the Assets area, find the correct binary file for the machine where you want to run NebulaGraph Console and download the file to the machine.</p> </li> <li> <p>(Optional) Rename the binary file to <code>nebula-console</code> for convenience.</p> <p>Note</p> <p>For Windows, rename the file to <code>nebula-console.exe</code>.</p> </li> <li> <p>On the machine to run NebulaGraph Console, grant the execute permission of the nebula-console binary file to the user.</p> <p>Note</p> <p>For Windows, skip this step.</p> <pre><code>$ chmod 111 nebula-console\n</code></pre> </li> <li> <p>In the command line interface, change the working directory to the one where the nebula-console binary file is stored.</p> </li> <li> <p>Run the following command to connect to NebulaGraph.</p> <ul> <li>For Linux or macOS:</li> </ul> <pre><code>$ ./nebula-console -addr &lt;ip&gt; -port &lt;port&gt; -u &lt;username&gt; -p &lt;password&gt;\n[-t 120] [-e \"nGQL_statement\" | -f filename.nGQL]\n</code></pre> <ul> <li>For Windows:</li> </ul> <pre><code>&gt; nebula-console.exe -addr &lt;ip&gt; -port &lt;port&gt; -u &lt;username&gt; -p &lt;password&gt;\n[-t 120] [-e \"nGQL_statement\" | -f filename.nGQL]\n</code></pre> <p>Parameter descriptions are as follows:</p> Parameter Description <code>-h/-help</code> Shows the help menu. <code>-addr/-address</code> Sets the IP address of the Graph service. The default address is 127.0.0.1.  <code>-P/-port</code> Sets the port number of the graphd service. The default port number is 9669. <code>-u/-user</code> Sets the username of your NebulaGraph account. Before enabling authentication, you can use any existing username. The default username is <code>root</code>. <code>-p/-password</code> Sets the password of your NebulaGraph account. Before enabling authentication, you can use any characters as the password. <code>-t/-timeout</code> Sets an integer-type timeout threshold of the connection. The unit is millisecond. The default value is 120. <code>-e/-eval</code> Sets a string-type nGQL statement. The nGQL statement is executed once the connection succeeds. The connection stops after the result is returned. <code>-f/-file</code> Sets the path of an nGQL file. The nGQL statements in the file are executed once the connection succeeds. The result will be returned and the connection stops then. <code>-enable_ssl</code> Enables SSL encryption when connecting to NebulaGraph. <code>-ssl_root_ca_path</code> Sets the storage path of the certification authority file. <code>-ssl_cert_path</code> Sets the storage path of the certificate file. <code>-ssl_private_key_path</code> Sets the storage path of the private key file. <p>For information on more parameters, see the project repository.</p> </li> </ol>"},{"location":"reuse/source_install-nebula-graph-by-rpm-or-deb/","title":"Source install nebula graph by rpm or deb","text":"<p>RPM and DEB are common package formats on Linux systems. This topic shows how to quickly install NebulaGraph with the RPM or DEB package.</p> <p>Note</p> <p>The console is not complied or packaged with NebulaGraph server binaries. You can install nebula-console by yourself.</p> <p>Enterpriseonly</p> <p>For NebulaGraph Enterprise, please contact us.</p>"},{"location":"reuse/source_install-nebula-graph-by-rpm-or-deb/#prerequisites","title":"Prerequisites","text":"<ul> <li>The tool <code>wget</code> is installed.</li> </ul> <ul> <li>For NebulaGraph Enterprise, you must have the license key loaded in LM.</li> </ul>"},{"location":"reuse/source_install-nebula-graph-by-rpm-or-deb/#step_1_download_the_package_from_cloud_service","title":"Step 1: Download the package from cloud service","text":"<p>Note</p> <p>NebulaGraph is currently only supported for installation on Linux systems, and only CentOS 7.x, CentOS 8.x, Ubuntu 16.04, Ubuntu 18.04, and Ubuntu 20.04 operating systems are supported. </p> <ul> <li>Download the released version.<p>URL:</p> <pre><code>//Centos 7\nhttps://oss-cdn.nebula-graph.io/package/&lt;release_version&gt;/nebula-graph-&lt;release_version&gt;.el7.x86_64.rpm\n\n//Centos 8\nhttps://oss-cdn.nebula-graph.io/package/&lt;release_version&gt;/nebula-graph-&lt;release_version&gt;.el8.x86_64.rpm\n\n//Ubuntu 1604\nhttps://oss-cdn.nebula-graph.io/package/&lt;release_version&gt;/nebula-graph-&lt;release_version&gt;.ubuntu1604.amd64.deb\n\n//Ubuntu 1804\nhttps://oss-cdn.nebula-graph.io/package/&lt;release_version&gt;/nebula-graph-&lt;release_version&gt;.ubuntu1804.amd64.deb\n\n//Ubuntu 2004\nhttps://oss-cdn.nebula-graph.io/package/&lt;release_version&gt;/nebula-graph-&lt;release_version&gt;.ubuntu2004.amd64.deb\n</code></pre> <p>For example, download the release package <code>3.5.0</code> for <code>Centos 7.5</code>:</p> <pre><code>wget https://oss-cdn.nebula-graph.io/package/3.5.0/nebula-graph-3.5.0.el7.x86_64.rpm\nwget https://oss-cdn.nebula-graph.io/package/3.5.0/nebula-graph-3.5.0.el7.x86_64.rpm.sha256sum.txt\n</code></pre> <p>Download the release package <code>3.5.0</code> for <code>Ubuntu 1804</code>:</p> <pre><code>wget https://oss-cdn.nebula-graph.io/package/3.5.0/nebula-graph-3.5.0.ubuntu1804.amd64.deb\nwget https://oss-cdn.nebula-graph.io/package/3.5.0/nebula-graph-3.5.0.ubuntu1804.amd64.deb.sha256sum.txt\n</code></pre> </li> </ul> <ul> <li> <p>Download the nightly version.</p> <p>Danger</p> <ul> <li>Nightly versions are usually used to test new features. Do not use it in a production environment.</li> <li>Nightly versions may not be built successfully every night. And the names may change from day to day.</li> </ul> <p>URL:</p> <pre><code>//Centos 7\nhttps://oss-cdn.nebula-graph.io/package/nightly/&lt;yyyy.mm.dd&gt;/nebula-graph-&lt;yyyy.mm.dd&gt;-nightly.el7.x86_64.rpm\n\n//Centos 8\nhttps://oss-cdn.nebula-graph.io/package/nightly/&lt;yyyy.mm.dd&gt;/nebula-graph-&lt;yyyy.mm.dd&gt;-nightly.el8.x86_64.rpm\n\n//Ubuntu 1604\nhttps://oss-cdn.nebula-graph.io/package/nightly/&lt;yyyy.mm.dd&gt;/nebula-graph-&lt;yyyy.mm.dd&gt;-nightly.ubuntu1604.amd64.deb\n\n//Ubuntu 1804\nhttps://oss-cdn.nebula-graph.io/package/nightly/&lt;yyyy.mm.dd&gt;/nebula-graph-&lt;yyyy.mm.dd&gt;-nightly.ubuntu1804.amd64.deb\n\n//Ubuntu 2004\nhttps://oss-cdn.nebula-graph.io/package/nightly/&lt;yyyy.mm.dd&gt;/nebula-graph-&lt;yyyy.mm.dd&gt;-nightly.ubuntu2004.amd64.deb\n</code></pre> <p>For example, download the <code>Centos 7.5</code> package developed and built in <code>2021.11.28</code>:</p> <pre><code>wget https://oss-cdn.nebula-graph.io/package/nightly/2021.11.28/nebula-graph-2021.11.28-nightly.el7.x86_64.rpm\nwget https://oss-cdn.nebula-graph.io/package/nightly/2021.11.28/nebula-graph-2021.11.28-nightly.el7.x86_64.rpm.sha256sum.txt\n</code></pre> <p>For example, download the <code>Ubuntu 1804</code> package developed and built in <code>2021.11.28</code>:</p> <pre><code>wget https://oss-cdn.nebula-graph.io/package/nightly/2021.11.28/nebula-graph-2021.11.28-nightly.ubuntu1804.amd64.deb\nwget https://oss-cdn.nebula-graph.io/package/nightly/2021.11.28/nebula-graph-2021.11.28-nightly.ubuntu1804.amd64.deb.sha256sum.txt\n</code></pre> </li> </ul>"},{"location":"reuse/source_install-nebula-graph-by-rpm-or-deb/#step_2_install_nebulagraph","title":"Step 2: Install NebulaGraph","text":"<ul> <li> <p>Use the following syntax to install with an RPM package.</p> <pre><code>$ sudo rpm -ivh --prefix=&lt;installation_path&gt; &lt;package_name&gt;\n</code></pre> <p>The option <code>--prefix</code> indicates the installation path. The default path is <code>/usr/local/nebula/</code>.</p> <p>For example, to install an RPM package in the default path for the 3.5.0 version, run the following command.</p> <pre><code>sudo rpm -ivh nebula-graph-3.5.0.el7.x86_64.rpm\n</code></pre> </li> </ul> <ul> <li> <p>Use the following syntax to install with a DEB package.</p> <pre><code>$ sudo dpkg -i &lt;package_name&gt;\n</code></pre> <p>Note</p> <p>Customizing the installation path is not supported when installing NebulaGraph with a DEB package. The default installation path is <code>/usr/local/nebula/</code>.</p> <p>For example, to install a DEB package for the 3.5.0 version, run the following command.</p> <pre><code>sudo dpkg -i nebula-graph-3.5.0.ubuntu1804.amd64.deb\n</code></pre> <p>Note</p> <p>The default installation path is <code>/usr/local/nebula/</code>.</p> </li> </ul>"},{"location":"reuse/source_install-nebula-graph-by-rpm-or-deb/#step_3_configure_the_address_of_the_license_manager","title":"Step 3: Configure the address of the License Manager","text":"<p>Enterpriseonly</p> <p>This step is required only for NebulaGraph Enterprise.</p> <p>In the Meta service configuration file (<code>nebula-metad.conf</code>) of NebulaGraph, set the value of <code>license_manager_url</code> to the host IP and port number <code>9119</code> where the License Manager (LM) is located, e.g. <code>192.168.8.100:9119</code>.</p>"},{"location":"reuse/source_install-nebula-graph-by-rpm-or-deb/#next_to_do","title":"Next to do","text":"<ul> <li>Start NebulaGraph </li> </ul> <ul> <li>Connect to NebulaGraph</li> </ul>"},{"location":"reuse/source_manage-service/","title":"Source manage service","text":"<p>NebulaGraph supports managing services with scripts. </p> <p>Enterpriseonly</p> <p>You can also manage NebulaGraph with systemd in the NebulaGraph Enterprise Edition.</p> <p>Danger</p> <p>The two methods are incompatible. It is recommended to use only one method in a cluster.</p>"},{"location":"reuse/source_manage-service/#manage_services_with_script","title":"Manage services with script","text":"<p>You can use the <code>nebula.service</code> script to start, stop, restart, terminate, and check the NebulaGraph services.</p> <p>Note</p> <p><code>nebula.service</code> is stored in the <code>/usr/local/nebula/scripts</code> directory by default. If you have customized the path, use the actual path in your environment.</p>"},{"location":"reuse/source_manage-service/#syntax","title":"Syntax","text":"<pre><code>$ sudo /usr/local/nebula/scripts/nebula.service\n[-v] [-c &lt;config_file_path&gt;]\n&lt;start | stop | restart | kill | status&gt;\n&lt;metad | graphd | storaged | all&gt;\n</code></pre> Parameter Description <code>-v</code> Display detailed debugging information. <code>-c</code> Specify the configuration file path. The default path is <code>/usr/local/nebula/etc/</code>. <code>start</code> Start the target services. <code>stop</code> Stop the target services. <code>restart</code> Restart the target services. <code>kill</code> Terminate the target services. <code>status</code> Check the status of the target services. <code>metad</code> Set the Meta Service as the target service. <code>graphd</code> Set the Graph Service as the target service. <code>storaged</code> Set the Storage Service as the target service. <code>all</code> Set all the NebulaGraph services as the target services."},{"location":"reuse/source_manage-service/#manage_services_with_systemd","title":"Manage services with systemd","text":"<p>For easy maintenance, NebulaGraph Enterprise Edition supports managing services with systemd. You can start, stop, restart, and check services with <code>systemctl</code> commands.</p> <p>Note</p> <ul> <li>After installing NebulaGraph Enterprise Edition, the <code>.service</code> files required by systemd are located in the <code>etc/unit</code> path in the installation directory. NebulaGraph installed with the RPM/DEB package automatically places the <code>.service</code> files into the path <code>/usr/lib/systemd/system</code> and the parameter <code>ExecStart</code> is generated based on the specified NebulaGraph installation path, so you can use <code>systemctl</code> commands directly.</li> </ul> <ul> <li>The <code>systemctl</code> commands cannot be used to manage the Enterprise Edition cluster that is created with Dashboard of the Enterprise Edition.</li> </ul> <ul> <li>Otherwise, users need to move the <code>.service</code> files manually into the directory <code>/usr/lib/systemd/system</code>, and modify the file path of the parameter <code>ExecStart</code> in the <code>.service</code> files.</li> </ul>"},{"location":"reuse/source_manage-service/#syntax_1","title":"Syntax","text":"<pre><code>$ systemctl &lt;start | stop | restart | status &gt; &lt;nebula | nebula-metad | nebula-graphd | nebula-storaged&gt;\n</code></pre> Parameter Description <code>start</code> Start the target services. <code>stop</code> Stop the target services. <code>restart</code> Restart the target services. <code>status</code> Check the status of the target services. <code>nebula</code> Set all the NebulaGraph services as the target services. <code>nebula-metad</code> Set the Meta Service as the target service. <code>nebula-graphd</code> Set the Graph Service as the target service. <code>nebula-storaged</code> Set the Storage Service as the target service."},{"location":"reuse/source_manage-service/#start_nebulagraph","title":"Start NebulaGraph","text":"<p>Run the following command to start NebulaGraph.</p> <pre><code>$ sudo /usr/local/nebula/scripts/nebula.service start all\n[INFO] Starting nebula-metad...\n[INFO] Done\n[INFO] Starting nebula-graphd...\n[INFO] Done\n[INFO] Starting nebula-storaged...\n[INFO] Done\n</code></pre> <p>Users can also run the following command:</p> <pre><code>$ systemctl start nebula\n</code></pre> <p>If users want to automatically start NebulaGraph when the machine starts, run the following command:</p> <pre><code>$ systemctl enable nebula\n</code></pre>"},{"location":"reuse/source_manage-service/#stop_nebulagraph","title":"Stop NebulaGraph","text":"<p>Danger</p> <p>Do not run <code>kill -9</code> to forcibly terminate the processes. Otherwise, there is a low probability of data loss.</p> <p>Run the following command to stop NebulaGraph.</p> <pre><code>$ sudo /usr/local/nebula/scripts/nebula.service stop all\n[INFO] Stopping nebula-metad...\n[INFO] Done\n[INFO] Stopping nebula-graphd...\n[INFO] Done\n[INFO] Stopping nebula-storaged...\n[INFO] Done\n</code></pre> <p>Users can also run the following command:</p> <pre><code>$ systemctl stop nebula\n</code></pre>"},{"location":"reuse/source_manage-service/#check_the_service_status","title":"Check the service status","text":"<p>Run the following command to check the service status of NebulaGraph.</p> <pre><code>$ sudo /usr/local/nebula/scripts/nebula.service status all\n</code></pre> <ul> <li> <p>NebulaGraph is running normally if the following information is returned.</p> <pre><code>INFO] nebula-metad(33fd35e): Running as 29020, Listening on 9559\n[INFO] nebula-graphd(33fd35e): Running as 29095, Listening on 9669\n[WARN] nebula-storaged after v3.0.0 will not start service until it is added to cluster.\n[WARN] See Manage Storage hosts:ADD HOSTS in https://docs.nebula-graph.io/\n[INFO] nebula-storaged(33fd35e): Running as 29147, Listening on 9779\n</code></pre> <p>Note</p> <p>After starting NebulaGraph, the port of the <code>nebula-storaged</code> process is shown in red. Because the <code>nebula-storaged</code> process waits for the <code>nebula-metad</code> to add the current Storage service during the startup process. The Storage works after it receives the ready signal. Starting from NebulaGraph 3.0.0, the Meta service cannot directly read or write data in the Storage service that you add in the configuration file. The configuration file only registers the Storage service to the Meta service. You must run the <code>ADD HOSTS</code> command to enable the Meta to read and write data in the Storage service. For more information, see Manage Storage hosts.</p> </li> </ul> <ul> <li>If the returned result is similar to the following one, there is a problem. You may also go to the NebulaGraph community for help.<pre><code>[INFO] nebula-metad: Running as 25600, Listening on 9559\n[INFO] nebula-graphd: Exited\n[INFO] nebula-storaged: Running as 25646, Listening on 9779\n</code></pre> </li> </ul> <p>Users can also run the following command:</p> <pre><code>$ systemctl status nebula\n\u25cf nebula.service\n   Loaded: loaded (/usr/lib/systemd/system/nebula.service; disabled; vendor preset: disabled)\n   Active: active (exited) since \u4e00 2022-03-28 04:13:24 UTC; 1h 47min ago\n  Process: 21772 ExecStart=/usr/local/ent-nightly/scripts/nebula.service start all (code=exited, status=0/SUCCESS)\n Main PID: 21772 (code=exited, status=0/SUCCESS)\n    Tasks: 325\n   Memory: 424.5M\n   CGroup: /system.slice/nebula.service\n           \u251c\u250021789 /usr/local/ent-nightly/bin/nebula-metad --flagfile /usr/local/ent-nightly/etc/nebula-metad.conf\n           \u251c\u250021827 /usr/local/ent-nightly/bin/nebula-graphd --flagfile /usr/local/ent-nightly/etc/nebula-graphd.conf\n           \u2514\u250021900 /usr/local/ent-nightly/bin/nebula-storaged --flagfile /usr/local/ent-nightly/etc/nebula-storaged.conf\n3\u6708 28 04:13:24 xxxxxx systemd[1]: Started nebula.service.\n...\n</code></pre> <p>The NebulaGraph services consist of the Meta Service, Graph Service, and Storage Service. The configuration files for all three services are stored in the <code>/usr/local/nebula/etc/</code> directory by default. You can check the configuration files according to the returned result to troubleshoot problems.</p>"},{"location":"reuse/source_manage-service/#next_to_do","title":"Next to do","text":"<p>Connect to NebulaGraph</p>"},{"location":"synchronization-and-migration/2.balance-syntax/","title":"BALANCE syntax","text":"<p>We can submit tasks to load balance Storage services in NebulaGraph. For more information about storage load balancing and examples, see Storage load balance.</p> <p>Note</p> <p>For other job management commands, see Job manager and the JOB statements.</p> <p>The syntax for load balance is described as follows.</p> Syntax Description <code>SUBMIT JOB BALANCE LEADER</code> Starts a job to balance the distribution of all the storage leaders in all graph spaces. It returns the job ID. <code>SUBMIT JOB BALANCE DATA</code> Starts a job to balance the distribution of storage partitions in the current graph space. It returns the job ID. <code>SUBMIT JOB BALANCE DATA REMOVE &lt;ip:port&gt; [,&lt;ip&gt;:&lt;port&gt; ...]</code> Migrate the partitions in the specified storage host to other storage hosts in the current graph space. <p>Note</p> <ul> <li><code>SUBMIT JOB BALANCE DATA REMOVE</code> can only clear the partitions of the current graph space. If a Storage service has a large number of graph spaces, you need to switch to all different graph spaces to perform the <code>REMOVE</code> operation.</li> <li>Only NebulaGraph Enterprise Edition supports the commands <code>SUBMIT JOB BALANCE DATA</code> and <code>SUBMIT JOB BALANCE DATA REMOVE</code>.</li> </ul> <p>For details about how to view, stop, and restart a job, see Job manager and the JOB statements.</p>"},{"location":"synchronization-and-migration/replication-between-clusters/","title":"Synchronize between two clusters","text":"<p>NebulaGraph supports data synchronization from a primary cluster to a secondary cluster in almost real-time. It applies to scenarios such as disaster recovery and load balancing, and helps reduce the risk of data loss and enhance data security.</p> <p>Enterpriseonly</p> <p>This feature applies to the Enterprise Edition only.</p>"},{"location":"synchronization-and-migration/replication-between-clusters/#synchronization_workflow","title":"Synchronization workflow","text":"<p>The synchronization works as follows:</p> <p></p> <ol> <li>The primary cluster sends any data written into it to the Meta listener or the Storage listener in the form of WALs or snapshots.</li> <li>The listener sends the data to the drainer in the form of WALs.</li> <li>The drainer sends the data to the partitions of the secondary cluster through the Meta client or the Storage client.</li> </ol>"},{"location":"synchronization-and-migration/replication-between-clusters/#applicable_scenarios","title":"Applicable Scenarios","text":"<ul> <li>Remote disaster recovery: Data synchronization enables cross-data-center or cross-city disaster recovery.</li> </ul> <ul> <li>Data migration: The migration can be implemented by synchronizing data and then switching cluster roles, without stopping the service.</li> </ul> <ul> <li>Read/Write splitting: Enable only writing on the primary cluster and only reading on the secondary cluster to lower the system load, and improve stability and usability.</li> </ul>"},{"location":"synchronization-and-migration/replication-between-clusters/#precautions","title":"Precautions","text":"<ul> <li>Make sure that the primary and secondary clusters are deployed in the same NebulaGraph version. Otherwise, the synchronization will fail.</li> </ul> <ul> <li>The synchronization is based on graph spaces, i.e., from one graph space in the primary cluster to another in the secondary cluster.</li> </ul> <ul> <li> <p>About the synchronization topology, NebulaGraph:</p> <ul> <li>Supports synchronizing from one primary cluster to one secondary cluster, but not multiple primary clusters to one secondary cluster.</li> </ul> <ul> <li>Supports chained synchronization but not synchronization from one primary cluster to multiple secondary clusters directly. An example of chained synchronization is from cluster A to cluster B, and then cluster B to cluster C.</li> </ul> </li> </ul> <ul> <li>The synchronization is implemented asynchronously, but with low latency.</li> </ul> <ul> <li>The Meta listener listens to the Meta Service and the Storage listener listens to the Storage Service. Do not mix them up.</li> </ul> <ul> <li>One graph space can have one Meta listener and one to multiple Storage listeners. These listeners can work with one to multiple drainers:<ul> <li>One listener with one drainer.</li> <li>Multiple listeners with one drainer.</li> <li>Multiple listeners with multiple drainers.</li> </ul> </li> </ul> <ul> <li>The machines where the listeners and drainers run must have enough disk space to store the WAL or snapshot files.</li> </ul> <ul> <li>If the target graph space in the secondary cluster has data before the synchronization starts, data conflicts or inconsistencies may happen during the synchronization. It is recommended to keep the target graph space empty.</li> </ul> <ul> <li>It is recommended to use the NebulaGraph <code>root</code> user with the God privileges to perform the cluster data synchronization. The required user roles for each synchronization command are different. For details, see the Role permission requirements section at the end of this topic.</li> </ul> <ul> <li>During the synchronization, do not perform data recovery (backup recovery and snapshot recovery) operations on the primary cluster at the same time. Otherwise, the synchronization will fail.</li> </ul>"},{"location":"synchronization-and-migration/replication-between-clusters/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Prepare at least two machines to deploy the primary and secondary clusters, the listeners, and the drainer.</p> <p>The listener and drainer can be deployed in a standalone way, or on the machines hosting the primary and secondary clusters. The latter way can increase the machine load and decrease the service performance.</p> </li> </ul>"},{"location":"synchronization-and-migration/replication-between-clusters/#test_environment","title":"Test environment","text":"<p>The test environment for the operation example in this topic is as follows:</p> <ul> <li>The primary cluster runs on the machine with the IP address 192.168.10.101. The cluster has one nebula-graphd process, one nebula-metad process, and one nebula-storaged process.</li> </ul> <ul> <li> <p>The secondary cluster runs on the machine with the IP address 192.168.10.102. The cluster has one nebula-graphd process, one nebula-metad process, and one nebula-storaged process.</p> <p>Note</p> <p>The primary and secondary clusters can have different cluster specifications, such as different numbers of machines, service processes, and data partitions.</p> </li> </ul> <ul> <li>The processes for the Meta and Storage listeners run on the machine with the IP address 192.168.10.103.</li> </ul> <ul> <li>The process for the drainer runs on the machine with the IP address 192.168.10.104.</li> </ul>"},{"location":"synchronization-and-migration/replication-between-clusters/#steps","title":"Steps","text":""},{"location":"synchronization-and-migration/replication-between-clusters/#step_1_set_up_the_clusters_listeners_and_drainer","title":"Step 1: Set up the clusters, listeners, and drainer","text":"<ol> <li> <p>Install NebulaGraph on all the machines.</p> <p>For installation instructions, see Install NebulaGraph.</p> </li> <li> <p>Modify the configuration files on all the machines.</p> <p>Note</p> <p>For newly installed services, remove the suffix <code>.default</code> or <code>.production</code> of a configuration template file in the <code>conf</code> directory to make it take effect.</p> <ul> <li>In the Meta service configuration file (<code>nebula-metad.conf</code>) of NebulaGraph, set the value of <code>license_manager_url</code> to the host IP and port number <code>9119</code> where the license management tool is located, e.g. <code>192.168.8.100:9119</code>.</li> </ul> <ul> <li>On the primary and secondary cluster machines, modify <code>nebula-graphd.conf</code>, <code>nebula-metad.conf</code>, and <code>nebula-storaged.conf</code>. In all three files, set real IP addresses for <code>local_ip</code> instead of <code>127.0.0.1</code>, and set the IP addresses and ports for their own nebula-metad processes as the <code>meta_server_addrs</code> values. In <code>nebula-graphd.conf</code>, set <code>enable_authorize=true</code>.</li> </ul> <ul> <li>On the primary cluster, set <code>--snapshot_send_files=false</code> in both the <code>nebula-storaged.conf</code> file and the <code>nebula-metad.conf</code> file. </li> </ul> <ul> <li>On the Meta listener machine, modify <code>nebula-metad-listener.conf</code>. Set the IP addresses and ports of the primary cluster's nebula-metad processes for <code>meta_server_addrs</code>, and those of the listener process for <code>meta_sync_listener</code>.</li> </ul> <ul> <li>On the Storage listener machine, modify <code>nebula-storaged-listener.conf</code>. Set the IP addresses and ports of the primary cluster's nebula-metad processes for <code>meta_server_addrs</code>.</li> </ul> <ul> <li>On the drainer machine, modify <code>nebula-drainerd.conf</code>. Set the IP addresses and ports of the secondary cluster's nebula-metad processes for <code>meta_server_addrs</code>.</li> </ul> <p>For more information about the configurations, see Configurations.</p> </li> <li> <p>Go to the NebulaGraph installation directories on the machines and start the needed services.</p> <ul> <li>On the primary and secondary machines, run <code>sudo scripts/nebula.service start all</code>.</li> </ul> <ul> <li>On the Meta listener machine, run <code>sudo bin/nebula-metad --flagfile etc/nebula-metad-listener.conf</code>.</li> </ul> <ul> <li>On the Storage listener machine, run <code>sudo bin/nebula-storaged --flagfile etc/nebula-storaged-listener.conf</code>.</li> </ul> <ul> <li>On the drainer machine, run <code>sudo scripts/nebula-drainerd.service start</code>.</li> </ul> </li> <li> <p>Log into the primary cluster, add the Storage hosts, and check the status of the listeners.</p> <pre><code># Add the Storage hosts first.\nnebula&gt; ADD HOSTS 192.168.10.101:9779;\nnebula&gt; SHOW HOSTS STORAGE;\n+------------------+------+----------+-----------+--------------+----------------------+\n| Host             | Port | Status   | Role      | Git Info Sha | Version              |\n+------------------+------+----------+-----------+--------------+----------------------+\n| \"192.168.10.101\" | 9779 | \"ONLINE\" | \"STORAGE\" | \"xxxxxxx\"    | \"ent-3.1.0\"          |\n+------------------+------+----------+-----------+--------------+----------------------+\n\n# Check the status of the Storage listener.\nnebula&gt; SHOW HOSTS STORAGE LISTENER;\n+------------------+------+----------+--------------------+--------------+----------------------+\n| Host             | Port | Status   | Role               | Git Info Sha | Version              |\n+------------------+------+----------+--------------------+--------------+----------------------+\n| \"192.168.10.103\" | 9789 | \"ONLINE\" | \"STORAGE_LISTENER\" | \"xxxxxxx\"    | \"ent-3.1.0\"          |\n+------------------+------+----------+--------------------+--------------+----------------------+\n\n# Check the status of the Meta listener.\nnebula&gt; SHOW HOSTS META LISTENER;\n+------------------+------+----------+-----------------+--------------+----------------------+\n| Host             | Port | Status   | Role            | Git Info Sha | Version              |\n+------------------+------+----------+-----------------+--------------+----------------------+\n| \"192.168.10.103\" | 9569 | \"ONLINE\" | \"META_LISTENER\" | \"xxxxxxx\"    |  \"ent-3.1.0\"         |\n+------------------+------+----------+-----------------+--------------+----------------------+\n</code></pre> </li> <li> <p>Log into the secondary cluster, add the Storage hosts, and check the status of the drainer.</p> <pre><code>nebula&gt; ADD HOSTS 192.168.10.102:9779;\nnebula&gt; SHOW HOSTS STORAGE;\n+------------------+------+----------+-----------+--------------+----------------------+\n| Host             | Port | Status   | Role      | Git Info Sha | Version              |\n+------------------+------+----------+-----------+--------------+----------------------+\n| \"192.168.10.102\" | 9779 | \"ONLINE\" | \"STORAGE\" | \"xxxxxxx\"    | \"ent-3.1.0\"          |\n+------------------+------+----------+-----------+--------------+----------------------+\n\nnebula&gt; SHOW HOSTS DRAINER;\n+------------------+------+----------+-----------+--------------+----------------------+\n| Host             | Port | Status   | Role      | Git Info Sha | Version              |\n+------------------+------+----------+-----------+--------------+----------------------+\n| \"192.168.10.104\" | 9889 | \"ONLINE\" | \"DRAINER\" | \"xxxxxxx\"    | \"ent-3.1.0\"          |\n+------------------+------+----------+-----------+--------------+----------------------+\n</code></pre> </li> </ol>"},{"location":"synchronization-and-migration/replication-between-clusters/#step_2_set_up_the_synchronization","title":"Step 2: Set up the synchronization","text":"<ol> <li> <p>Log into the primary cluster and create a graph space <code>basketballplayer</code>.</p> <pre><code>nebula&gt; CREATE SPACE basketballplayer(partition_num=15, \\\n        replica_factor=1, \\\n        vid_type=fixed_string(30));\n</code></pre> </li> <li> <p>Use the graph space <code>basketballplayer</code> and register the drainer service.</p> <pre><code>nebula&gt; USE basketballplayer;\n\n# Register the drainer service.\nnebula&gt; SIGN IN DRAINER SERVICE(192.168.10.104:9889);\n\n# Check if the drainer service is successfully signed in.\nnebula&gt; SHOW DRAINER CLIENTS;\n+-----------+------------------+------+\n| Type      | Host             | Port |\n+-----------+------------------+------+\n| \"DRAINER\" | \"192.168.10.104\" | 9889 |\n+-----------+------------------+------+\n</code></pre> <p>Note</p> <p>To register multiple drainer services, run the command such as <code>SIGN IN DRAINER SERVICE(192.168.8.x:9889),(192.168.8.x:9889)</code>.</p> </li> <li> <p>Configure the listener service.</p> <pre><code># replication_basketballplayer is the synchronization target. It will be created in the following steps.\nnebula&gt; ADD LISTENER SYNC \\\n        META 192.168.10.103:9569 \\\n        STORAGE 192.168.10.103:9789 \\\n        TO SPACE replication_basketballplayer;\n\n# Check the listener status.\nnebula&gt; SHOW LISTENER SYNC;\n+--------+--------+------------------------+--------------------------------+----------+\n| PartId | Type   | Host                   | SpaceName                      | Status   |\n+--------+--------+------------------------+--------------------------------+----------+\n| 0      | \"SYNC\" | \"\"192.168.10.103\":9569\" | \"replication_basketballplayer\" | \"ONLINE\" |\n| 1      | \"SYNC\" | \"\"192.168.10.103\":9789\" | \"replication_basketballplayer\" | \"ONLINE\" |\n| 2      | \"SYNC\" | \"\"192.168.10.103\":9789\" | \"replication_basketballplayer\" | \"ONLINE\" |\n| 3      | \"SYNC\" | \"\"192.168.10.103\":9789\" | \"replication_basketballplayer\" | \"ONLINE\" |\n| 4      | \"SYNC\" | \"\"192.168.10.103\":9789\" | \"replication_basketballplayer\" | \"ONLINE\" |\n| 5      | \"SYNC\" | \"\"192.168.10.103\":9789\" | \"replication_basketballplayer\" | \"ONLINE\" |\n| 6      | \"SYNC\" | \"\"192.168.10.103\":9789\" | \"replication_basketballplayer\" | \"ONLINE\" |\n| 7      | \"SYNC\" | \"\"192.168.10.103\":9789\" | \"replication_basketballplayer\" | \"ONLINE\" |\n| 8      | \"SYNC\" | \"\"192.168.10.103\":9789\" | \"replication_basketballplayer\" | \"ONLINE\" |\n| 9      | \"SYNC\" | \"\"192.168.10.103\":9789\" | \"replication_basketballplayer\" | \"ONLINE\" |\n| 10     | \"SYNC\" | \"\"192.168.10.103\":9789\" | \"replication_basketballplayer\" | \"ONLINE\" |\n| 11     | \"SYNC\" | \"\"192.168.10.103\":9789\" | \"replication_basketballplayer\" | \"ONLINE\" |\n| 12     | \"SYNC\" | \"\"192.168.10.103\":9789\" | \"replication_basketballplayer\" | \"ONLINE\" |\n| 13     | \"SYNC\" | \"\"192.168.10.103\":9789\" | \"replication_basketballplayer\" | \"ONLINE\" |\n| 14     | \"SYNC\" | \"\"192.168.10.103\":9789\" | \"replication_basketballplayer\" | \"ONLINE\" |\n| 15     | \"SYNC\" | \"\"192.168.10.103\":9789\" | \"replication_basketballplayer\" | \"ONLINE\" |\n+--------+--------+------------------------+--------------------------------+----------+\n</code></pre> <p>Note</p> <p>To configure multiple storage listener services, run the command such as <code>ADD LISTENER SYNC META 192.168.10.xxx:9569 STORAGE 192.168.10.xxx:9789,192.168.10.xxx:9789 TO SPACE replication_basketballplayer</code>.</p> </li> <li> <p>Log into the secondary cluster and create graph space <code>replication_basketballplayer</code>.</p> <pre><code>nebula&gt; CREATE SPACE replication_basketballplayer(partition_num=15, \\\n        replica_factor=1, \\\n        vid_type=fixed_string(30));\n</code></pre> </li> <li> <p>Use <code>replication_basketballplayer</code> and configure the drainer service.</p> <pre><code>nebula&gt; USE replication_basketballplayer;\n\n# Configure the drainer service.\nnebula&gt; ADD DRAINER 192.168.10.104:9889;\n\n# Check the drainer status.\nnebula&gt; SHOW DRAINERS;\n+-------------------------+----------+\n| Host                    | Status   |\n+-------------------------+----------+\n| \"\"192.168.10.104\":9889\" | \"ONLINE\" |\n+-------------------------+----------+\n</code></pre> <p>Note</p> <p>To configure multiple drainer services, run the command such as <code>ADD DRAINER 192.168.8.x:9889,192.168.8.x:9889</code>.</p> </li> <li> <p>Set the target graph space <code>replication_basketballplayer</code> as read-only to avoid data inconsistency.</p> <p>Note</p> <p>This step only sets the target graph space, not other graph spaces.</p> <pre><code># Set the working graph space as read-only.\nnebula&gt; SET VARIABLES read_only=true;\n\n# Check the read_only status of the working graph space.\nnebula&gt; GET VARIABLES read_only;\n+-------------+--------+-------+\n| name        | type   | value |\n+-------------+--------+-------+\n| \"read_only\" | \"bool\" | true  |\n+-------------+--------+-------+\n</code></pre> </li> </ol>"},{"location":"synchronization-and-migration/replication-between-clusters/#step_3_validate_the_data","title":"Step 3: Validate the data","text":"<ol> <li> <p>Log into the primary cluster, create the schema, and insert data.</p> <pre><code>nebula&gt; USE basketballplayer;\nnebula&gt; CREATE TAG player(name string, age int);\nnebula&gt; CREATE EDGE follow(degree int);\nnebula&gt; INSERT VERTEX player(name, age) VALUES \"player100\":(\"Tim Duncan\", 42);\nnebula&gt; INSERT VERTEX player(name, age) VALUES \"player101\":(\"Tony Parker\", 36);\nnebula&gt; INSERT EDGE follow(degree) VALUES \"player101\" -&gt; \"player100\":(95);\n</code></pre> </li> <li> <p>Log into the secondary cluster and validate the data.</p> <pre><code>nebula&gt; USE replication_basketballplayer;\nnebula&gt; SUBMIT JOB STATS;\nnebula&gt; SHOW STATS;\n+---------+------------+-------+\n| Type    | Name       | Count |\n+---------+------------+-------+\n| \"Tag\"   | \"player\"   | 2     |\n| \"Edge\"  | \"follow\"   | 1     |\n| \"Space\" | \"vertices\" | 2     |\n| \"Space\" | \"edges\"    | 1     |\n+---------+------------+-------+\n\nnebula&gt; FETCH PROP ON player \"player100\" \\\n        YIELD properties(vertex);\n+-------------------------------+\n| properties(VERTEX)            |\n+-------------------------------+\n| {age: 42, name: \"Tim Duncan\"} |\n+-------------------------------+\n\nnebula&gt; GO FROM \"player101\" OVER follow \\\n        YIELD dst(edge);\n+-------------+\n| dst(EDGE)   |\n+-------------+\n| \"player100\" |\n+-------------+\n</code></pre> </li> </ol>"},{"location":"synchronization-and-migration/replication-between-clusters/#stoprestart_data_synchronization","title":"Stop/Restart data synchronization","text":"<p>The listener continuously sends the WALs to the drainer during data synchronization.</p> <p>To stop data synchronization, run the <code>STOP SYNC</code> command. The listener stops sending data to the drainer.</p> <p>To restart data synchronization, run the <code>RESTART SYNC</code> command. The listener sends the data accumulated during the period when the synchronization is stopped to the drainer. If the WALs are lost, the listener pulls the snapshot from the primary cluster and synchronizes data again.</p>"},{"location":"synchronization-and-migration/replication-between-clusters/#view_the_status_of_inter-cluster_data_synchronization","title":"View the status of inter-cluster data synchronization","text":"<p>When data is written to the primary cluster, you can check the status of inter-cluster data synchronization and tell whether data synchronization is normal.</p>"},{"location":"synchronization-and-migration/replication-between-clusters/#check_the_status_of_synchronized_data_in_the_primary_cluster","title":"Check the status of synchronized data in the primary cluster","text":"<p>You can execute the <code>SHOW SYNC STATUS</code> command in the primary cluster to view the status of the data sent from the primary cluster to the secondary cluster. <code>SHOW SYNC STATUS</code> gets the information of data synchronization status between clusters in real-time, and sends synchronized data to the secondary cluster only when the primary cluster has written successfully.</p> <p>Examples are as follows.</p> <pre><code>// Write data to the primary cluster.\nnebula&gt; INSERT VERTEX player(name,age) VALUES \"player102\":(\"LaMarcus Aldridge\", 33);\nnebula&gt; INSERT VERTEX player(name,age) VALUES \"player102\":(\"LaMarcus Aldridge\", 33);\nnebula&gt; INSERT VERTEX player(name,age) VALUES \"player103\":(\"Rudy Gay\", 32);\nnebula&gt; INSERT VERTEX player(name,age) VALUES \"player104\":(\"Marco Belinelli\", 32);\n\n// Check the status of data synchronization in the current cluster (the returned result indicates that data is being sent to the secondary cluster).\nnebula&gt; SHOW SYNC STATUS;\n+--------+-------------+-----------+--------------+\n| PartId | Sync Status | LogId Lag | Time Latency |\n+--------+-------------+-----------+--------------+\n| 0      | \"ONLINE\"    | 0         | 0            |\n| 1      | \"ONLINE\"    | 0         | 0            |\n| 2      | \"ONLINE\"    | 0         | 0            |\n| 3      | \"ONLINE\"    | 0         | 0            |\n| 4      | \"ONLINE\"    | 0         | 0            |\n| 5      | \"ONLINE\"    | 1         | 46242122     |\n| 6      | \"ONLINE\"    | 0         | 0            |\n| 7      | \"ONLINE\"    | 0         | 0            |\n| 8      | \"ONLINE\"    | 0         | 0            |\n| 9      | \"ONLINE\"    | 0         | 0            |\n| 10     | \"ONLINE\"    | 0         | 0            |\n| 11     | \"ONLINE\"    | 0         | 0            |\n| 12     | \"ONLINE\"    | 0         | 0            |\n| 13     | \"ONLINE\"    | 0         | 0            |\n| 14     | \"ONLINE\"    | 0         | 0            |\n| 15     | \"ONLINE\"    | 0         | 0            |\n+--------+-------------+-----------+--------------+\n\n// Check the status of data synchronization in the current cluster again (the returned result indicates that the data is fully synchronized to the secondary cluster and there is no data to be synchronized).\nnebula&gt; SHOW SYNC STATUS;\n+--------+-------------+-----------+--------------+\n| PartId | Sync Status | LogId Lag | Time Latency |\n+--------+-------------+-----------+--------------+\n| 0      | \"ONLINE\"    | 0         | 0            |\n| 1      | \"ONLINE\"    | 0         | 0            |\n| 2      | \"ONLINE\"    | 0         | 0            |\n| 3      | \"ONLINE\"    | 0         | 0            |\n| 4      | \"ONLINE\"    | 0         | 0            |\n| 5      | \"ONLINE\"    | 0         | 0            |\n| 6      | \"ONLINE\"    | 0         | 0            |\n| 7      | \"ONLINE\"    | 0         | 0            |\n| 8      | \"ONLINE\"    | 0         | 0            |\n| 9      | \"ONLINE\"    | 0         | 0            |\n| 10     | \"ONLINE\"    | 0         | 0            |\n| 11     | \"ONLINE\"    | 0         | 0            |\n| 12     | \"ONLINE\"    | 0         | 0            |\n| 13     | \"ONLINE\"    | 0         | 0            |\n| 14     | \"ONLINE\"    | 0         | 0            |\n| 15     | \"ONLINE\"    | 0         | 0            |\n+--------+-------------+-----------+--------------+\n</code></pre> <p>After executing the <code>SHOW SYNC STATUS</code> command, the parameters in the returned result are described as follows.</p> Parameter Description PartId The partition ID in the specified graph space in the primary cluster. The Meta data to be synchronized by Meta listener is located in the partition <code>0</code>. The Storage data to be synchronized by Storage listener is located in other partitions. Sync Status Indicates the status of the listener service.When the listener is <code>ONLINE</code>, it continuously sends data to the drainer service.When the listener is <code>OFFLINE</code>, it stops sending data to the drainer. LogId Lag Indicates the difference between Log IDs, that is how many logs are still sent to the secondary cluster from the corresponding partition of the primary cluster. The value <code>0</code> indicates that there are no logs to be sent in the corresponding partition of the primary cluster. Time Latency The difference between the timestamp in the WAL of the last log to be sent and the timestamp in the WAL of the last log that has been sent in the corresponding partition of the primary cluster. The value <code>0</code> indicates that data has been sent to the secondary cluster.  Unit: Millisecond."},{"location":"synchronization-and-migration/replication-between-clusters/#check_the_status_of_synchronized_data_in_the_secondary_cluster","title":"Check the status of synchronized data in the secondary cluster","text":"<p>In the secondary cluster, run <code>SHOW DRAINER SYNC STATUS</code> to view the status of synchronizing data to the Meta and Storage services in the secondary cluster. </p> <p><pre><code>nebula&gt; SHOW DRAINER SYNC STATUS;\n+--------+-------------+-----------+--------------+\n| PartId | Sync Status | LogId Lag | Time Latency |\n+--------+-------------+-----------+--------------+\n| 0      | \"ONLINE\"    | 0         | 0            |\n| 1      | \"ONLINE\"    | 0         | 0            |\n| 2      | \"ONLINE\"    | 0         | 0            |\n| 3      | \"ONLINE\"    | 0         | 0            |\n| 4      | \"ONLINE\"    | 0         | 0            |\n| 5      | \"ONLINE\"    | 0         | 0            |\n| 6      | \"ONLINE\"    | 0         | 0            |\n| 7      | \"ONLINE\"    | 0         | 0            |\n| 8      | \"ONLINE\"    | 0         | 0            |\n| 9      | \"ONLINE\"    | 0         | 0            |\n| 10     | \"ONLINE\"    | 0         | 0            |\n| 11     | \"ONLINE\"    | 0         | 0            |\n| 12     | \"ONLINE\"    | 0         | 0            |\n| 13     | \"ONLINE\"    | 0         | 0            |\n| 14     | \"ONLINE\"    | 0         | 0            |\n| 15     | \"ONLINE\"    | 0         | 0            |\n+--------+-------------+-----------+--------------+\n</code></pre> After executing <code>SHOW DRAINER SYNC STATUS</code>, the parameters in the returned result are described as follows.</p> Parameter Description PartId The partition ID in the specified graph space in the primary cluster. The partition <code>0</code> is where the Meta data to be synchronized is located. The Storage data is located in other partitions. Sync Status Indicates the status of the drainer service.When drainer is <code>ONLINE</code>, it continuously sends WAL to <code>metaClient</code>/<code>storageClient</code> in the secondary cluster for data synchronization.When drainer is <code>OFFLINE</code>, it stops sending WAL to <code>metaClient</code>/<code>storageClient</code> in the secondary cluster for data synchronization. LogId Lag Indicates the difference between Log IDs, that is how many logs are still sent to <code>metaClient</code>/<code>storageClient</code> from the corresponding drainer partition in the secondary cluster.The value <code>0</code> indicates that there are no logs to be synchronized in the corresponding drainer partition. Time Latency The difference between the timestamp in the WAL of the newest log received by the corresponding drainer partition between the timestamp in the WAL of the last log that has been synchronized to <code>metaClient</code>/<code>storageClient</code> in the secondary cluster. The value <code>0</code> indicates that drainer partition data has been sent to <code>metaClient</code>/<code>storageClient</code>.  Unit: Millisecond."},{"location":"synchronization-and-migration/replication-between-clusters/#switch_between_primary_and_secondary_clusters","title":"Switch between primary and secondary clusters","text":"<p>To migrate data or implement disaster recovery, manually switch between the primary and secondary clusters.</p> <p>Note</p> <p>Before the switching, set up a listener for the new primary cluster, and a drainer for the new secondary cluster. In the following example, the listener has IP address 192.168.10.105 and drainer 192.168.10.106.</p> <ol> <li> <p>Log into the old primary cluster and set the working graph space as read-only to avoid data inconsistency.</p> <pre><code>nebula&gt; USE basketballplayer;\nnebula&gt; SET VARIABLES read_only=true;\n</code></pre> </li> <li> <p>Check whether the data in the old primary cluster has been synchronized to the old secondary cluster. Make sure the data in the old primary cluster has been synchronized to the old secondary cluster.</p> <ol> <li>In the old primary cluster, view the status of the data sent from the old primary cluster to the old secondary cluster.</li> </ol> <pre><code>nebula&gt; SHOW SYNC STATUS;\n</code></pre> <ol> <li>Log into the old secondary cluster and then view the status of synchronizing data to the Meta and Storage services.</li> </ol> <pre><code>nebula&gt; USE replication_basketballplayer;\nnebula&gt; SHOW DRAINER SYNC STATUS;\n</code></pre> <p>When the values of <code>LogId Lag</code> and <code>Time Latency</code> in the returned results in both old primary and secondary clusters are <code>0</code>, the data synchronization is complete.</p> </li> <li> <p>In the old secondary cluster, disable read-only for the working graph space.</p> <pre><code>nebula&gt; SET VARIABLES read_only=false;\n</code></pre> <p>Note</p> <p>If there is business data to be written, you can now write the business data to the old secondary cluster (the new primary cluster).</p> </li> <li> <p>In the old secondary cluster, remove the old drainer service.</p> <pre><code>nebula&gt; REMOVE DRAINER;\n</code></pre> </li> <li> <p>Log into the old primary cluster, disable read-only, sign out the drainer, and remove the listener.</p> <pre><code>nebula&gt; USE basketballplayer;\n// Disable read-only for the working graph space, otherwise adding drainer fails.\nnebula&gt; SET VARIABLES read_only=false;\nnebula&gt; SIGN OUT DRAINER SERVICE;\nnebula&gt; REMOVE LISTENER SYNC;\n</code></pre> </li> <li> <p>In the old primary cluster, change the old primary cluster to the new secondary cluster by adding the new drainer service and setting the working graph space as read-only. </p> <p>Note</p> <p>Ensure that the new drainer service is deployed and started for the new secondary cluster.</p> <pre><code>nebula&gt; ADD DRAINER 192.168.10.106:9889;\nnebula&gt; SET VARIABLES read_only=true;\n</code></pre> </li> <li> <p>Log into the old secondary cluster and change the old secondary cluster to the new primary cluster.</p> <p>Note</p> <p>Ensure that the new meta listener and storage listener services are deployed and started for the new primary cluster.</p> <pre><code>nebula&gt; SIGN IN DRAINER SERVICE(192.168.10.106:9889);\nnebula&gt; ADD LISTENER SYNC META 192.168.10.105:9569 STORAGE 192.168.10.105:9789 TO SPACE basketballplayer;\n</code></pre> <p>The primary-secondary cluster switch is now complete.</p> </li> </ol>"},{"location":"synchronization-and-migration/replication-between-clusters/#role_permission_requirements","title":"Role permission requirements","text":"<p>The required user roles for each synchronization command are different. The required roles for each command are as follows (A check mark indicates that the role has the permission).</p> Command God Admin DBA User Guest <code>SIGN IN / SIGN OUT DRAINER SERVICE</code> \u221a <code>ADD / REMOVE LISTENER SYNC</code> \u221a \u221a \u221a <code>SHOW DRAINER CLIENTS</code> \u221a \u221a \u221a \u221a \u221a <code>SHOW LISTENER SYNC</code> \u221a \u221a \u221a \u221a \u221a <code>ADD / REMOVE DRAINER</code> \u221a \u221a \u221a <code>SET VARIABLES read_only</code> \u221a <code>SHOW DRAINERS</code> \u221a \u221a \u221a \u221a \u221a"},{"location":"synchronization-and-migration/replication-between-clusters/#faq","title":"FAQ","text":""},{"location":"synchronization-and-migration/replication-between-clusters/#can_the_pre-existent_data_in_the_primary_cluster_be_synchronized_to_the_secondary_cluster","title":"Can the pre-existent data in the primary cluster be synchronized to the secondary cluster?","text":"<p>Yes. After receiving the WAL from the listener, if the drainer finds that the data to be updated does not exist in the secondary cluster, it starts the synchronization of the complete data set.</p>"},{"location":"synchronization-and-migration/replication-between-clusters/#will_the_pre-existent_data_in_the_secondary_cluster_affect_the_synchronization","title":"Will the pre-existent data in the secondary cluster affect the synchronization?","text":"<p>If the pre-existent data in the secondary cluster is a subset of the data in the primary cluster, the data in the primary and secondary clusters will eventually become consistent through synchronization. If there is any pre-existent data (not a subset of the data in the primary cluster) in the secondary cluster before the synchronization, the data may be lost after the synchronization. It is recommended to use a secondary cluster without data for synchronization.</p>"},{"location":"synchronization-and-migration/replication-between-clusters/#will_the_pre-existent_schema_information_in_the_secondary_cluster_affect_the_synchronization","title":"Will the pre-existent schema information in the secondary cluster affect the synchronization?","text":"<p>The pre-existent schema information must not conflict with the schema of the primary cluster. Otherwise, it will be overwritten, and related data in the secondary cluster might become invalid.</p>"},{"location":"synchronization-and-migration/replication-between-clusters/#should_the_number_of_machines_replicas_and_partitions_in_the_primary_and_secondary_clusters_be_the_same","title":"Should the number of machines, replicas, and partitions in the primary and secondary clusters be the same?","text":"<p>No. The synchronization is based on graph spaces, not other elements such as partitions and replicas. The primary and secondary clusters do not need to have the exact specifications.</p>"},{"location":"synchronization-and-migration/replication-between-clusters/#does_altering_the_schema_in_the_primary_cluster_affect_the_synchronization","title":"Does altering the schema in the primary cluster affect the synchronization?","text":"<p>Altering the schema may increase the synchronization latency.</p> <p>The schema data is synchronized through the Meta listener, while the vertex/edge data is through the Storage listener. When synchronizing the vertex/edge data, the system checks the schema version of the data. If the system finds that the version number of the schema is greater than that in the secondary cluster, it pauses the vertex/edge data update, and updates the schema data first.</p>"},{"location":"synchronization-and-migration/replication-between-clusters/#how_to_deal_with_synchronization_failures","title":"How to deal with synchronization failures?","text":"<p>Fix the problems in the cluster, and then the synchronization will be automatically restored.</p> <ul> <li>If problems have happened in the primary cluster, the synchronization continues when the problems are fixed and the primary cluster restarts.</li> </ul> <ul> <li>If problems have happened in the secondary cluster, listeners, or drainers, when the problems are fixed, the services that had the problems will receive the WALs accumulated from its upstream and the synchronization will continue working. If the faulty machine is replaced with a new one, all the data of the synchronization services on the faulty machine must be copied to the new machine. Otherwise, the synchronization of the complete data set will start automatically.</li> </ul>"},{"location":"synchronization-and-migration/replication-between-clusters/#how_to_check_the_data_synchronization_status_and_progress","title":"How to check the data synchronization status and progress?","text":"<p>You can run <code>SHOW SYNC STATUS</code> to check the status of the data sent by the primary cluster and run <code>SHOW DRAINER SYNC STATUS</code> to check the status of the data received by the secondary cluster. If all the data is sent successfully from the primary cluster and all the data is received successfully by the secondary cluster, the data synchronization is completed.</p>"},{"location":"synchronization-and-migration/replication-between-clusters/#my_wal_log_files_has_expired_and_will_it_affect_the_cluster_synchronization","title":"My WAL log files has expired and will it affect the cluster synchronization?","text":"<p>Expired WAL files (beyond the time set by <code>--wal-ttl</code>) will cause unsynchronization of cluster data. You can manually add <code>--snapshot_send_files=false</code> to the configuration files of the Meta and Storage services to synchronize data. After updating the configuration files, you need to restart the services. For more information about the configuration files, see Configuration Files.</p>"}]}