# 自定义{{nebula.name}}集群的配置参数

{{nebula.name}}集群中 Meta、Storage、Graph 服务都有各自的配置，其在用户创建的{{nebula.name}}集群实例的 YAML 文件中被定义为`config`。`config`中的设置会被映射并加载到对应服务的 ConfigMap 中。各个服务在启动时会挂载 ConfigMap 中的配置到`/usr/local/nebula/etc/`目录下。

!!! note

    暂不支持通过 Helm 自定义{{nebula.name}}集群的配置参数。

`config`结构如下：

```go
Config map[string]string `json:"config,omitempty"`
```

## 前提条件

已使用 K8s 创建一个集群。具体步骤，参见[使用 Kubectl 创建{{nebula.name}}集群](../3.deploy-nebula-graph-cluster/3.1create-cluster-with-kubectl.md)。

## 操作步骤

以下示例使用名为`nebula`的集群、名为`nebula_cluster.yaml`的 YAML 配置文件，说明如何为集群的 Graph 服务配置`config`：

1. 执行以下命令进入`nebula`集群的编辑页面。
   
  ```bash
  kubectl edit nebulaclusters.apps.nebula-graph.io nebula
  ```

2. 在 YAML 文件的`spec.graphd.config`配置项中，添加需要修改的参数。下文以 `enable_authorize`和`auth_type`为例。

  ```yaml
  apiVersion: apps.nebula-graph.io/v1alpha1
  kind: NebulaCluster
  metadata:
    name: nebula
    namespace: default
  spec:
    graphd:
      resources:
        requests:
          cpu: "500m"
          memory: "500Mi"
        limits:
          cpu: "1"
          memory: "1Gi"
      replicas: 1
      image: vesoft/nebula-graphd
      version: {{nebula.tag}}
      storageClaim:
        resources:
          requests:
            storage: 2Gi
        storageClassName: fast-disks
      config: //为 Graph 服务自定义参数。
        "enable_authorize": "true"
        "auth_type": "password"
  ...
  ```
  在 `config` 字段下可以配置的参数有以下几种：

  - 支持运行时动态修改的参数。关于这些参数的更多信息，分别参见 [Meta 服务配置参数](../../5.configurations-and-logs/1.configurations/2.meta-config.md)、[Storage 服务配置参数](../../5.configurations-and-logs/1.configurations/4.storage-config.md)、[Graph 服务配置参数](../../5.configurations-and-logs/1.configurations/3.graph-config.md)的参数表格中**是否支持运行时动态修改**一列。

  - 不支持运行时动态修改但 flag 标记支持的参数。您可以运行下面任意命令查看完整的参数列表：
    - `./bin/nebula-graphd --help`
    - `curl -s  http://10.244.1.104:19669/flags`

  !!! note

      * 若要在集群运行时动态修改参数配置并且不触发 Pod 重启，请确保当前修改的参数全部支持运行时动态修改。
      * 若本次修改的参数包含一个或多个不支持运行时动态修改的参数，则会触发 Pod 重启。

  如果需要为 Meta 服务和 Storage 服务配置`config`，则在`spec.metad.config`和`spec.storaged.config`中添加对应的配置项。

3. 执行`kubectl apply -f nebula_cluster.yaml`使上述更新生效。

  在修改参数值后，Graph 服务对应的 ConfigMap（`nebula-graphd`）中的配置将被覆盖。

### 配置自定义端口

您可以在`config`字段中添加`port`和`ws_http_port`参数，从而配置自定义的端口。这两个参数的详细信息，请参见[Meta 服务配置参数](../../5.configurations-and-logs/1.configurations/2.meta-config.md)、[Storage 服务配置参数](../../5.configurations-and-logs/1.configurations/4.storage-config.md)、[Graph 服务配置参数](../../5.configurations-and-logs/1.configurations/3.graph-config.md)的 networking 配置一节。

!!! note

    * 自定义`port`和`ws_http_port`参数配置后，会触发 Pod 重启，并在重启后生效。 
    * 在集群启动后，不建议修改`port`参数。

1. 修改集群配置文件。

  ```yaml
  apiVersion: apps.nebula-graph.io/v1alpha1
  kind: NebulaCluster
  metadata:
    name: nebula
    namespace: default
  spec:
    graphd:
      config:
        port: "3669"
        ws_http_port: "8080"
      resources:
        requests:
          cpu: "200m"
          memory: "500Mi"
        limits:
          cpu: "1"
          memory: "1Gi"
      replicas: 1
      image: vesoft/nebula-graphd
      version: {{nebula.tag}}
    metad:
      config:
        ws_http_port: 8081
      resources:
        requests:
          cpu: "300m"
          memory: "500Mi"
        limits:
          cpu: "1"
          memory: "1Gi"
      replicas: 1
      image: vesoft/nebula-metad
      version: {{nebula.tag}}
      dataVolumeClaim:
        resources:
          requests:
            storage: 2Gi
        storageClassName: local-path
    storaged:
      config:
        ws_http_port: 8082
      resources:
        requests:
          cpu: "300m"
          memory: "500Mi"
        limits:
          cpu: "1"
          memory: "1Gi"
      replicas: 1
      image: vesoft/nebula-storaged
      version: {{nebula.tag}}
      dataVolumeClaims:
      - resources:
          requests:
            storage: 2Gi
        storageClassName: local-path
      enableAutoBalance: true
    reference:
      name: statefulsets.apps
      version: v1
    schedulerName: default-scheduler
    imagePullPolicy: IfNotPresent
    imagePullSecrets:
    - name: nebula-image
    enablePVReclaim: true
    topologySpreadConstraints:
    - topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: "ScheduleAnyway"
  ```

2. 执行`kubectl apply -f nebula_cluster.yaml`使上述更新生效。

3. 验证配置已经生效。

  ```bash
  kubectl get svc
  ```

  返回示例：
  
  ```
  NAME                        TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
  nebula-graphd-headless      ClusterIP   None             <none>        3669/TCP,8080/TCP            10m
  nebula-graphd-svc           ClusterIP   10.102.13.115    <none>        3669/TCP,8080/TCP            10m
  nebula-metad-headless       ClusterIP   None             <none>        9559/TCP,8081/TCP            11m
  nebula-storaged-headless    ClusterIP   None             <none>        9779/TCP,8082/TCP,9778/TCP   11m
  ```