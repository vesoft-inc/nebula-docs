# 管理集群日志

{{nebula.name}}集群各服务（graphd、metad、storaged）在运行期间会生成运行日志，日志默认存放在各个服务容器的`/usr/local/nebula/logs`目录下。

## 查看运行日志

如果您需要查看{{nebula.name}}集群的运行日志，可以通过`kubectl logs`命令查看。

例如，查看 Storage 服务的运行日志：

```bash
// 查看 Storage 服务 Pod 的名称（nebula-storaged-0）。
$ kubectl get pods -l app.kubernetes.io/component=storaged
NAME                               READY   STATUS    RESTARTS      AGE
nebula-storaged-0                  1/1     Running   0             45h
...

// 进入 Storage 服务所在容器 storaged。
$ kubectl exec -it nebula-storaged-0 -c storaged -- /bin/bash

// 查看 Storage 服务的运行日志。
$ cd /usr/local/nebula/logs
```

## 清理日志

集群服务在运行期间生成的运行日志会占用磁盘空间，为避免占用过多磁盘空间，Operator 使用 sidecar 容器定期清理和归档日志。

为了方便日志的采集和管理，每个{{nebula.name}}服务都会部署一个 sidecar 容器，负责收集该服务容器产生的日志，并将其发送到指定的日志磁盘中。sidecar 容器使用 [logrotate](https://linux.die.net/man/8/logrotate) 工具自动清理和归档日志。

在集群实例的 YAML 配置文件中，通过`spec.logRotate`字段开启日志轮转功能，同时在`spec.<graphd|metad|storaged>.config`下设置`timestamp_in_logfile_name`值为`false`以自动对各服务日志进行清理和归档。默认情况下，日志轮转功能是关闭的。为{{nebula.name}}各个服务都开启日志轮转功能示例如下：

```yaml
...
spec:
  graphd:
    config:
      # 是否在日志文件名中包含时间戳，需设置为 false 以实现日志轮转。默认值为 true。
      "timestamp_in_logfile_name": "false"
  metad:
    config:
      "timestamp_in_logfile_name": "false"
  storaged:
    config:
      "timestamp_in_logfile_name": "false"
  logRotate: # 日志轮转配置
    # 日志文件在被删除前会被轮转的次数。默认值为 5，0 表示删除前不会被轮转。
    rotate: 5
    # 仅当日志文件增长超过定义的字节大小时才会轮转日志文件。默认值为 200M。
    size: "200M"
```


## 收集日志

如果不想挂载额外的日志磁盘备份日志文件，或者想通过诸如 [fluent-bit](https://fluentbit.io/) 之类的服务收集日志并将其发送到日志中心，可以配置日志至标准错误输出。Operator 使用 [glog](https://github.com/google/glog) 工具将日志记录到标准错误输出。

!!! caution

    目前 Operator 仅收集标准错误日志。

在集群实例的 YAML 配置文件中，可以在各个服务下的`config`和`env`字段中配置日志记录到标准错误输出。


```yaml
...
spec:
  graphd:
    config:
      # 是否将标准错误重定向到单独的输出文件。默认值为 false，表示不重定向。
      redirect_stdout: "false"
      # 日志内容的严重程度级别：INFO、WARNING、ERROR 和 FATAL。取值分别为 0、1、2 和 3。
      stderrthreshold: "0"
    env: 
    - name: GLOG_logtostderr # 日志写入标准错误而不是文件。
      value: "1" # 1 表示写入标准错误，0 表示写入文件中。
    image: vesoft/nebula-graphd
    replicas: 1
    resources:
      requests:
        cpu: 500m
        memory: 500Mi
    service:
      externalTrafficPolicy: Local
      type: NodePort
    version: v{{nebula.release}}
  metad:
    config:
      redirect_stdout: "false"
      stderrthreshold: "0"
    dataVolumeClaim:
      resources:
        requests:
          storage: 1Gi
      storageClassName: ebs-sc
    env:
    - name: GLOG_logtostderr
      value: "1"
    image: vesoft/nebula-metad
  ...
```