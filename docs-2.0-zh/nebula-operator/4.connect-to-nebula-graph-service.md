# 通过 Operator 连接{{nebula.name}}

使用 NebulaGraph Operator 创建{{nebula.name}} 集群后，用户可在{{nebula.name}}集群内部访问{{nebula.name}}，也可在集群外访问{{nebula.name}}。如果用户在集群内部，可以通过访问集群内的虚拟 IP 地址（ClusterIP）访问数据库。而如果用户在集群外部，则需要使用集群节点的公共 IP 地址或者通过部署 Nginx Ingress 控制器来访问{{nebula.name}}。IP 地址类型可以在创建集群的配置文件中，通过`spec.graphd.service`指定。关于 Service 的更多信息，参考 [Service](https://kubernetes.io/docs/concepts/services-networking/service/)。

## 前提条件

使用 NebulaGraph Operator 创建{{nebula.name}}集群。具体步骤参考[使用 Kubectl 部署{{nebula.name}}集群](3.deploy-nebula-graph-cluster/3.1create-cluster-with-kubectl.md) 或者[使用 Helm 部署{{nebula.name}}集群](3.deploy-nebula-graph-cluster/3.2create-cluster-with-helm.md)。

## 在{{nebula.name}}集群内连接{{nebula.name}}

用户也可以创建`ClusterIP`类型的 Service，为集群内的其他 Pod 提供访问{{nebula.name}}的入口。通过该 Service 的 IP 和数据库 Graph 服务的端口号（`9669`），可连接{{nebula.name}}。更多信息，请参考 [ClusterIP](https://kubernetes.io/docs/concepts/services-networking/service/)。

1. 创建名为`graphd-clusterip-service.yaml`的文件。示例内容如下：

  ```yaml
  apiVersion: v1
  kind: Service
  metadata:
    labels:
      app.kubernetes.io/cluster: nebula
      app.kubernetes.io/component: graphd
      app.kubernetes.io/managed-by: nebula-operator
      app.kubernetes.io/name: nebula-graph
    name: nebula-graphd-svc
    namespace: default
  spec:
    externalTrafficPolicy: Local
    ports:
    - name: thrift
      port: 9669
      protocol: TCP
      targetPort: 9669
    - name: http
      port: 19669
      protocol: TCP
      targetPort: 19669
    selector:
      app.kubernetes.io/cluster: nebula
      app.kubernetes.io/component: graphd
      app.kubernetes.io/managed-by: nebula-operator
      app.kubernetes.io/name: nebula-graph
    type: ClusterIP  # 设置 Service 类型为 ClusterIP。
  ```

  - {{nebula.name}}默认使用`9669`端口为客户端提供服务。`19669`为 Graph 服务的 HTTP 端口号。
  - `targetPort`的值为映射至 Pod 的端口，可自定义。

2. 执行以下命令使 Service 服务在集群中生效。
   
  ```bash	  
  kubectl create -f graphd-clusterip-service.yaml	
  ```	  

3. 查看 Service，命令如下：

  ```bash
  $ kubectl get service -l app.kubernetes.io/cluster=<nebula>  #<nebula>为变量值，请用实际集群名称替换。
  NAME                       TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                                          AGE
  nebula-graphd-svc          ClusterIP   10.98.213.34   <none>        9669/TCP,19669/TCP,19670/TCP                     23h
  ...
  ```

4. 使用上述`<cluster-name>-graphd-svc` Service 的 IP 连接{{nebula.name}}：

  ```bash
  kubectl run -ti --image vesoft/nebula-console:{{console.tag}} --restart=Never -- <nebula_console_name> -addr <cluster_ip>  -port <service_port> -u <username> -p <password>
  ```

  示例：

  ```bash
  kubectl run -ti --image vesoft/nebula-console:{{console.tag}} --restart=Never -- nebula-console -addr 10.98.213.34  -port 9669 -u root -p vesoft
  ```

  - `--image`：为连接{{nebula.name}}的工具 NebulaGraph Console 的镜像。
  - `<nebula-console>`：自定义的 Pod 名称。
  - `-addr`：连接 Graphd 服务的 IP 地址，即`ClusterIP`类型的 Service IP 地址。
  - `-port`：连接 Graphd 服务的端口。默认端口为`9669`。
  - `-u`：{{nebula.name}}账号的用户名。未启用身份认证时，可以使用任意已存在的用户名（默认为`root`）。
  - `-p`：用户名对应的密码。未启用身份认证时，密码可以填写任意字符。

  如果返回以下内容，说明成功连接数据库：

  ```bash
  If you don't see a command prompt, try pressing enter.

  (root@nebula) [(none)]>
  ```

用户还可以使用**完全限定域名（FQDN）**连接数据库，域名格式为`<cluster-name>-graphd.<cluster-namespace>.svc.<CLUSTER_DOMAIN>`，`CLUSTER_DOMAIN`的默认值为`cluster.local`。

```bash
kubectl run -ti --image vesoft/nebula-console:{{console.tag}} --restart=Never -- <nebula_console_name> -addr <cluster_name>-graphd-svc.default.svc.cluster.local -port <service_port> -u <username> -p <password>
```
  
- `<service_port>`为 Graph 服务默认的端口`9669`。

## 通过`NodePort`在{{nebula.name}}集群外部连接{{nebula.name}}

用户可创建`NodePort`类型的 Service，通过集群任一节点 IP 和暴露的节点端口，从集群外部访问集群内部的服务。用户也可以使用云厂商（例如 Azure、AWS 等）提供的负载均衡服务，设置 Service 的类型为`LoadBalancer`，通过云厂商提供的负载均衡器的公网 IP 和端口，从集群外部访问集群内部的服务。

`NodePort`类型的 Service 通过标签选择器`spec.selector`将前端的请求转发到带有标签`app.kubernetes.io/cluster: <cluster-name>`、`app.kubernetes.io/component: graphd`的 Graphd pod 中。

当根据集群[示例模板](https://github.com/vesoft-inc/nebula-operator/blob/v{{operator.release}}/config/samples/apps_v1alpha1_nebulacluster.yaml)，其中`spec.graphd.service.type=NodePort`，创建 NebulaGraph 集群后，NebulaGraph Operator 会自动在同一命名空间下，创建名为`<cluster-name>-graphd-svc`、类型为`NodePort`的 Service。通过任一节点 IP 和暴露的节点端口，可直接连接 NebulaGraph 数据库（参见下文第 4 步）。用户也可以根据自己的需求，创建自定义的 Service。

操作步骤如下：

1. 创建名为`graphd-nodeport-service.yaml`的文件。示例内容如下：

  ```yaml
  apiVersion: v1
  kind: Service
  metadata:
    labels:
      app.kubernetes.io/cluster: nebula
      app.kubernetes.io/component: graphd
      app.kubernetes.io/managed-by: nebula-operator
      app.kubernetes.io/name: nebula-graph
    name: nebula-graphd-svc-nodeport
    namespace: default
  spec:
    externalTrafficPolicy: Local
    ports:
    - name: thrift
      port: 9669
      protocol: TCP
      targetPort: 9669
    - name: http
      port: 19669
      protocol: TCP
      targetPort: 19669
    selector:
      app.kubernetes.io/cluster: nebula
      app.kubernetes.io/component: graphd
      app.kubernetes.io/managed-by: nebula-operator
      app.kubernetes.io/name: nebula-graph
    type: NodePort  # 设置 Service 类型为 NodePort。
  ```

  - {{nebula.name}}默认使用`9669`端口为客户端提供服务。`19669`为 Graph 服务的 HTTP 端口号。
  - `targetPort`的值为映射至 Pod 的端口，可自定义。

2. 执行以下命令使 Service 服务在集群中生效。

  ```bash
  kubectl create -f graphd-nodeport-service.yaml
  ```

3. 查看 Service 中{{nebula.name}}映射至集群节点的端口。

  ```bash
  kubectl get services -l app.kubernetes.io/cluster=<nebula>  #<nebula>为变量值，请用实际集群名称替换。
  ```

  返回：

  ```bash
  NAME                           TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                                          AGE
  nebula-graphd-svc-nodeport     NodePort    10.107.153.129 <none>        9669:32236/TCP,19669:31674/TCP,19670:31057/TCP   24h
  ...
  ```

  `NodePort`类型的 Service 中，映射至集群节点的端口为`32236`。

4. 使用节点 IP 和上述映射的节点端口连接 NebulaGraph。
  
  ```bash
  kubectl run -ti --image vesoft/nebula-console:{{console.tag}} --restart=Never -- <nebula_console_name> -addr <node_ip> -port <node_port> -u <username> -p <password>
  ```

  示例如下：

  ```bash
  kubectl run -ti --image vesoft/nebula-console:{{console.tag}} --restart=Never -- nebula-console -addr 192.168.8.24 -port 32236 -u root -p vesoft
  If you don't see a command prompt, try pressing enter.

  (root@nebula) [(none)]>
  ```

  - `--image`：为连接{{nebula.name}}的工具 NebulaGraph Console 的镜像。
  - `<nebula-console>`：自定义的 Pod 名称。本示例为`nebula-console`。
  - `-addr`：{{nebula.name}}集群中任一节点 IP 地址。本示例为`192.168.8.24`。
  - `-port`：{{nebula.name}}映射至节点的端口。本示例为`32236`。
  - `-u`：{{nebula.name}}账号的用户名。未启用身份认证时，可以使用任意已存在的用户名（默认为 root）。
  - `-p`：用户名对应的密码。未启用身份认证时，密码可以填写任意字符。

## 通过`Ingress`在{{nebula.name}}集群外部连接{{nebula.name}}

当集群中有多个 Pod 时，为每个 Pod 分别提供服务会变得非常困难和繁琐，而使用 Ingress 可以轻松解决这个问题。Ingress 可以将流量路由到集群内部的多个 Pod。

Nginx Ingress 是 Kubernetes 中的一个 Ingress 控制器（Controller），是对 Kubernetes Ingress 资源的一个实现，通过 Watch 机制感知 Kubernetes 集群中的 Ingress 资源。它将这些 Ingress 规则转换为 Nginx 配置并启动一个 Nginx 实例来处理流量。

用户可以通过 HostNetwork 和 DaemonSet 组合的模式使用 Nginx Ingress 从集群外部连接{{nebula.name}}集群。

由于使用 HostNetwork，Nginx Ingress 的 Pods 可能被调度在同一个节点上（当多个 Pod 尝试在同一个节点上监听相同的端口时，将会出现端口冲突）。为了避免这种情况，Nginx Ingress 以 DaemonSet 模式（确保集群中每个节点都运行一个 Pod 副本）部署在这些节点上，需先选择一些节点并为节点打上标签，专门用于部署 Nginx Ingress。 

由于 Ingress 不支持 TCP 或 UDP 服务，为此 nginx-ingress-controller 使用`--tcp-services-configmap`和`--udp-services-configmap`参数指向一个 ConfigMap，该 ConfigMap 中的键指需要使用的外部端口，值指要公开的服务的格式，值的格式为`<命名空间/服务名称>:<服务端口>`。

例如指向名为`tcp-services`的 ConfigMap 的配置如下：

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: tcp-services
  namespace: nginx-ingress
data:
  9769: "default/nebula-graphd-svc:9669"
```

操作步骤如下：

1. 创建名为`nginx-ingress-daemonset-hostnetwork.yaml`的文件。

  单击 [nginx-ingress-daemonset-hostnetwork.yaml](https://github.com/vesoft-inc/nebula-operator/blob/{{operator.tag}}/config/samples/nginx-ingress-daemonset-hostnetwork.yaml) 查看完整的 YAML 示例内容。

  !!! note
  
        上述 YAML 中的资源对象均使用`nginx-ingress`命名空间。用户可执行`kubectl create namesapce nginx-ingress`创建命名空间，或者自定义其他命名空间。

2. 为任一节点（本示例使用的节点名为`worker2`，IP 为`192.168.8.160`）打上标签，以运行上述 YAML 文件中名为`nginx-ingress-controller`的 DaemonSet。

  ```bash
  kubectl label node worker2 nginx-ingress=true
  ```

3. 执行以下命令使 Nginx Ingress 在集群中生效。

  ```bash
  kubectl create -f nginx-ingress-daemonset-hostnetwork.yaml
  ```

  返回：

  ```bash
  configmap/nginx-ingress-controller created
  configmap/tcp-services created
  serviceaccount/nginx-ingress created
  serviceaccount/nginx-ingress-backend created
  clusterrole.rbac.authorization.k8s.io/nginx-ingress created
  clusterrolebinding.rbac.authorization.k8s.io/nginx-ingress created
  role.rbac.authorization.k8s.io/nginx-ingress created
  rolebinding.rbac.authorization.k8s.io/nginx-ingress created
  service/nginx-ingress-controller-metrics created
  service/nginx-ingress-default-backend created
  service/nginx-ingress-proxy-tcp created
  daemonset.apps/nginx-ingress-controller created
  ```

  成功部署 Nginx Ingress 后，由于 Nginx Ingress 中配置的网络类型为`hostNetwork`，因此用户可通过部署了 Nginx Ingress 的节点的 IP（`192.168.8.160`）和外部端口（`9769`）访问{{nebula.name}}服务。

4. 执行以下命令部署连接{{nebula.name}}服务的 Console 并通过宿主机 IP（本示例为`192.168.8.160`）和上述配置的外部端口访问{{nebula.name}}服务。

  ```bash
  kubectl run -ti --image vesoft/nebula-console:{{console.tag}} --restart=Never -- <nebula_console_name> -addr <host_ip> -port <external_port> -u <username> -p <password>
  ```

  示例：

  ```bash
  kubectl run -ti --image vesoft/nebula-console:{{console.tag}} --restart=Never -- nebula-console -addr 192.168.8.160 -port 9769 -u root -p vesoft
  ```

  - `--image`：为连接{{nebula.name}}的工具 NebulaGraph Console 的镜像。
  - `<nebula-console>`：自定义的 Pod 名称。本示例为`nebula-console`。
  - `-addr`：部署 Nginx Ingress 的节点 IP，本示例为`192.168.8.160`。
  - `-port`：外网访问使用的的端口。本示例设置为`9769`。
  - `-u`：{{nebula.name}}账号的用户名。未启用身份认证时，可以使用任意已存在的用户名（默认为 root）。
  - `-p`：用户名对应的密码。未启用身份认证时，密码可以填写任意字符。

  如果返回以下内容，说明成功连接数据库：

  ```bash
  If you don't see a command prompt, try pressing enter.

  (root@nebula) [(none)]>
  ```
