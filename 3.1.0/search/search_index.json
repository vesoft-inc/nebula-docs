{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Nebula Graph 3.1.0 Documentation \u00b6 View release notes This manual is revised on 2022-4-25, with GitHub commit 1e02affa2 . Nebula Graph is a distributed, scalable, and lightning-fast graph database. It is the optimal solution in the world capable of hosting graphs with dozens of billions of vertices (nodes) and trillions of edges (relationships) with millisecond latency. Getting started \u00b6 Learning path What is Nebula Graph Quick start Preparations before deployment nGQL cheatsheet FAQ Ecosystem Tools Other Sources \u00b6 Nebula Graph Homepage Release notes Forum Blogs Videos Chinese Docs Symbols used in this manual \u00b6 Note Additional information or operation-related notes. Caution Cautions that need strict observation. If not, systematic breakdown, data loss, and security issues may happen. Danger Operations that may cause danger. If not observed, systematic breakdown, data loss, and security issues will happen. Performance Operations that merit attention as for performance enhancement. Faq Frequently asked questions. Compatibility The compatibility notes between nGQL and openCypher, or between the current version of nGQL and its prior ones. Enterpriseonly Differences between the Nebula Graph Community and Enterprise editions. Modify errors \u00b6 This Nebula Graph manual is written in the Markdown language. Users can click the pencil sign on the upper right side of each document title and modify errors.","title":"About"},{"location":"#welcome_to_nebula_graph_310_documentation","text":"View release notes This manual is revised on 2022-4-25, with GitHub commit 1e02affa2 . Nebula Graph is a distributed, scalable, and lightning-fast graph database. It is the optimal solution in the world capable of hosting graphs with dozens of billions of vertices (nodes) and trillions of edges (relationships) with millisecond latency.","title":"Welcome to Nebula Graph 3.1.0 Documentation"},{"location":"#getting_started","text":"Learning path What is Nebula Graph Quick start Preparations before deployment nGQL cheatsheet FAQ Ecosystem Tools","title":"Getting started"},{"location":"#other_sources","text":"Nebula Graph Homepage Release notes Forum Blogs Videos Chinese Docs","title":"Other Sources"},{"location":"#symbols_used_in_this_manual","text":"Note Additional information or operation-related notes. Caution Cautions that need strict observation. If not, systematic breakdown, data loss, and security issues may happen. Danger Operations that may cause danger. If not observed, systematic breakdown, data loss, and security issues will happen. Performance Operations that merit attention as for performance enhancement. Faq Frequently asked questions. Compatibility The compatibility notes between nGQL and openCypher, or between the current version of nGQL and its prior ones. Enterpriseonly Differences between the Nebula Graph Community and Enterprise editions.","title":"Symbols used in this manual"},{"location":"#modify_errors","text":"This Nebula Graph manual is written in the Markdown language. Users can click the pencil sign on the upper right side of each document title and modify errors.","title":"Modify errors"},{"location":"nebula-algorithm/","text":"Nebula Algorithm \u00b6 Nebula Algorithm (Algorithm) is a Spark application based on GraphX . It uses a complete algorithm tool to perform graph computing on the data in the Nebula Graph database by submitting a Spark task. You can also programmatically use the algorithm under the lib repository to perform graph computing on DataFrame. Version compatibility \u00b6 The correspondence between the Nebula Algorithm release and the Nebula Graph core release is as follows. Algorithm client Nebula Graph 3.0-SNAPSHOT nightly 3.0.0 3.1.0 2.6.x 2.6.x 2.5.0 2.5.0\u30012.5.1 2.1.0 2.0.0\u30012.0.1 Prerequisites \u00b6 Before using the Nebula Algorithm, users need to confirm the following information: The Nebula Graph services have been deployed and started. For details, see Nebula Installation . The Spark version is 2.4.x. The Scala version is 2.11. (Optional) If users need to clone, compile, and package the latest Algorithm in Github, install Maven . Limitations \u00b6 When submitting the algorithm package directly, the data of the vertex ID must be an integer. That is, the vertex ID can be INT or String, but the data itself is an integer. For non-integer String data, it is recommended to use the algorithm interface. You can use the dense_rank function of SparkSQL to encode the data as the Long type instead of the String type. Graph computing outputs vertex datasets, and the algorithm results are stored in DataFrames as the properties of vertices. You can do further operations such as statistics and filtering according to your business requirements. When writing the algorithm results into the Nebula Graph, make sure that the Tag in the corresponding graph space has properties corresponding to the algorithm result name. The corresponding properties of each algorithm are as follows. Algorithm Properties name Properties type pagerank pagerank double/string louvain louvain int/string kcore kcore int/string labelpropagation lpa int/string connectedcomponent cc int/string stronglyconnectedcomponent scc int/string betweenness betweenness double/string shortestpath shortestpath string degreestatic degree,inDegree,outDegree int/string trianglecount trianglecount int/string clusteringcoefficient clustercoefficient double/string closeness closeness double/string hanp hanp int/string bfs bfs string jaccard jaccard string node2vec node2vec string Supported algorithms \u00b6 The graph computing algorithms supported by Nebula Algorithm are as follows. Algorithm Description Scenario PageRank The rank of pages Web page ranking, key node mining Louvain Community discovery Community mining, hierarchical clustering KCore K core Community discovery, financial risk control LabelPropagation Label propagation Information spreading, advertising, and community discovery Hanp Label propagation advanced Community discovery, recommendation system ConnectedComponent Connected component Community discovery, island discovery StronglyConnectedComponent Strongly connected component Community discovery ShortestPath The shortest path Path planning, network planning TriangleCount Triangle counting Network structure analysis GraphTriangleCount Graph triangle counting Network structure and tightness analysis BetweennessCentrality Intermediate centrality Key node mining, node influence computing ClosenessCentrality Closeness centrality Key node mining, node influence computing DegreeStatic Degree of statistical Graph structure analysis ClusteringCoefficient Aggregation coefficient Recommendation system, telecom fraud analysis Jaccard Jaccard similarity Similarity computing, recommendation system BFS Breadth-First Search Sequence traversal, shortest path planning Node2Vec - Graph classification Implementation methods \u00b6 Nebula Algorithm implements the graph calculating as follows: Read the graph data of DataFrame from the Nebula Graph database using the Nebula Spark Connector. Transform the graph data of DataFrame to the GraphX graph. Use graph algorithms provided by GraphX (such as PageRank) or self-implemented algorithms (such as Louvain). For detailed implementation methods, see Scala file . Get Nebula Algorithm \u00b6 Compile and package \u00b6 Clone the repository nebula-algorithm . $ git clone -b v3.0.0 https://github.com/vesoft-inc/nebula-algorithm.git Enter the directory nebula-algorithm . $ cd nebula-algorithm Compile and package. $ mvn clean package -Dgpg.skip -Dmaven.javadoc.skip = true -Dmaven.test.skip = true After the compilation, a similar file nebula-algorithm-3.x.x.jar is generated in the directory nebula-algorithm/target . Download maven from the remote repository \u00b6 Download address How to use \u00b6 Use algorithm interface (recommended) \u00b6 The lib repository provides 10 common graph algorithms. Add dependencies to the file pom.xml . <dependency> <groupId>com.vesoft</groupId> <artifactId>nebula-algorithm</artifactId> <version>3.0.0</version> </dependency> Use the algorithm (take PageRank as an example) by filling in parameters. For more examples, see example . Note By default, the DataFrame that executes the algorithm sets the first column as the starting vertex, the second column as the destination vertex, and the third column as the edge weights (not the rank in the Nebula Graph). val prConfig = new PRConfig ( 5 , 1 .0 ) val louvainResult = PageRankAlgo.apply ( spark, data, prConfig, false ) If your vertex IDs are Strings, see Pagerank Example for how to encoding and decoding them. Submit the algorithm package directly \u00b6 Note There are limitations to use sealed packages. For example, when sinking a repository into Nebula Graph, the property name of the tag created in the sunk graph space must match the preset name in the code. The first method is recommended if the user has development skills. Set the Configuration file . { # Configurations related to Spark spark: { app: { name: LPA # The number of partitions of Spark partitionNum:100 } master:local } data: { # Data source. Optional values are nebula, csv, and json. source: csv # Data sink. The algorithm result will be written into this sink. Optional values are nebula, csv, and text. sink: nebula # Whether the algorithm has a weight. hasWeight: false } # Configurations related to Nebula Graph nebula: { # Data source. When Nebula Graph is the data source of the graph computing, the configuration of `nebula.read` is valid. read: { # The IP addresses and ports of all Meta services. Multiple addresses are separated by commas (,). Example: \"ip1:port1,ip2:port2\". # To deploy Nebula Graph by using Docker Compose, fill in the port with which Docker Compose maps to the outside. # Check the status with `docker-compose ps`. metaAddress: \"192.168.*.10:9559\" # The name of the graph space in Nebula Graph. space: basketballplayer # Edge types in Nebula Graph. When there are multiple labels, the data of multiple edges will be merged. labels: [ \"serve\" ] # The property name of each edge type in Nebula Graph. This property will be used as the weight column of the algorithm. Make sure that it corresponds to the edge type. weightCols: [ \"start_year\" ] } # Data sink. When the graph computing result sinks into Nebula Graph, the configuration of `nebula.write` is valid. write: { # The IP addresses and ports of all Graph services. Multiple addresses are separated by commas (,). Example: \"ip1:port1,ip2:port2\". # To deploy by using Docker Compose, fill in the port with which Docker Compose maps to the outside. # Check the status with `docker-compose ps`. graphAddress: \"192.168.*.11:9669\" # The IP addresses and ports of all Meta services. Multiple addresses are separated by commas (,). Example: \"ip1:port1,ip2:port2\". # To deploy Nebula Graph by using Docker Compose, fill in the port with which Docker Compose maps to the outside. # Check the staus with `docker-compose ps`. metaAddress: \"192.168.*.12:9559\" user:root pswd:nebula # Before submitting the graph computing task, create the graph space and tag. # The name of the graph space in Nebula Graph. space:nb # The name of the tag in Nebula Graph. The graph computing result will be written into this tag. The property name of this tag is as follows. # PageRank: pagerank # Louvain: louvain # ConnectedComponent: cc # StronglyConnectedComponent: scc # LabelPropagation: lpa # ShortestPath: shortestpath # DegreeStatic: degree\u3001inDegree\u3001outDegree # KCore: kcore # TriangleCount: tranglecpunt # BetweennessCentrality: betweennedss tag:pagerank } } local: { # Data source. When the data source is csv or json, the configuration of `local.read` is valid. read: { filePath: \"hdfs://127.0.0.1:9000/edge/work_for.csv\" # If the CSV file has a header or it is a json file, use the header. If not, use [_c0, _c1, _c2, ..., _cn] instead. # The header of the source VID column. srcId: \"_c0\" # The header of the destination VID column. dstId: \"_c1\" # The header of the weight column. weight: \"_c2\" # Whether the csv file has a header. header: false # The delimiter in the csv file. delimiter: \",\" } # Data sink. When the graph computing result sinks to the csv or text file, the configuration of `local.write` is valid. write: { resultPath:/tmp/ } } algorithm: { # The algorithm to execute. Optional values are as follow: # pagerank, louvain, connectedcomponent, labelpropagation, shortestpaths, # degreestatic, kcore, stronglyconnectedcomponent, trianglecount , # betweenness, graphtriangleCount. executeAlgo: pagerank # PageRank pagerank: { maxIter: 10 resetProb: 0 .15 } # Louvain louvain: { maxIter: 20 internalIter: 10 tol: 0 .5 } # ... } } Submit the graph computing task. ${ SPARK_HOME } /bin/spark-submit --master <mode> --class com.vesoft.nebula.algorithm.Main <nebula-algorithm-3.0.0.jar_path> -p <application.conf_path> Example: ${ SPARK_HOME } /bin/spark-submit --master \"local\" --class com.vesoft.nebula.algorithm.Main /root/nebula-algorithm/target/nebula-algorithm-3.0-SNAPSHOT.jar -p /root/nebula-algorithm/src/main/resources/application.conf","title":"Nebula Algorithm"},{"location":"nebula-algorithm/#nebula_algorithm","text":"Nebula Algorithm (Algorithm) is a Spark application based on GraphX . It uses a complete algorithm tool to perform graph computing on the data in the Nebula Graph database by submitting a Spark task. You can also programmatically use the algorithm under the lib repository to perform graph computing on DataFrame.","title":"Nebula Algorithm"},{"location":"nebula-algorithm/#version_compatibility","text":"The correspondence between the Nebula Algorithm release and the Nebula Graph core release is as follows. Algorithm client Nebula Graph 3.0-SNAPSHOT nightly 3.0.0 3.1.0 2.6.x 2.6.x 2.5.0 2.5.0\u30012.5.1 2.1.0 2.0.0\u30012.0.1","title":"Version compatibility"},{"location":"nebula-algorithm/#prerequisites","text":"Before using the Nebula Algorithm, users need to confirm the following information: The Nebula Graph services have been deployed and started. For details, see Nebula Installation . The Spark version is 2.4.x. The Scala version is 2.11. (Optional) If users need to clone, compile, and package the latest Algorithm in Github, install Maven .","title":"Prerequisites"},{"location":"nebula-algorithm/#limitations","text":"When submitting the algorithm package directly, the data of the vertex ID must be an integer. That is, the vertex ID can be INT or String, but the data itself is an integer. For non-integer String data, it is recommended to use the algorithm interface. You can use the dense_rank function of SparkSQL to encode the data as the Long type instead of the String type. Graph computing outputs vertex datasets, and the algorithm results are stored in DataFrames as the properties of vertices. You can do further operations such as statistics and filtering according to your business requirements. When writing the algorithm results into the Nebula Graph, make sure that the Tag in the corresponding graph space has properties corresponding to the algorithm result name. The corresponding properties of each algorithm are as follows. Algorithm Properties name Properties type pagerank pagerank double/string louvain louvain int/string kcore kcore int/string labelpropagation lpa int/string connectedcomponent cc int/string stronglyconnectedcomponent scc int/string betweenness betweenness double/string shortestpath shortestpath string degreestatic degree,inDegree,outDegree int/string trianglecount trianglecount int/string clusteringcoefficient clustercoefficient double/string closeness closeness double/string hanp hanp int/string bfs bfs string jaccard jaccard string node2vec node2vec string","title":"Limitations"},{"location":"nebula-algorithm/#supported_algorithms","text":"The graph computing algorithms supported by Nebula Algorithm are as follows. Algorithm Description Scenario PageRank The rank of pages Web page ranking, key node mining Louvain Community discovery Community mining, hierarchical clustering KCore K core Community discovery, financial risk control LabelPropagation Label propagation Information spreading, advertising, and community discovery Hanp Label propagation advanced Community discovery, recommendation system ConnectedComponent Connected component Community discovery, island discovery StronglyConnectedComponent Strongly connected component Community discovery ShortestPath The shortest path Path planning, network planning TriangleCount Triangle counting Network structure analysis GraphTriangleCount Graph triangle counting Network structure and tightness analysis BetweennessCentrality Intermediate centrality Key node mining, node influence computing ClosenessCentrality Closeness centrality Key node mining, node influence computing DegreeStatic Degree of statistical Graph structure analysis ClusteringCoefficient Aggregation coefficient Recommendation system, telecom fraud analysis Jaccard Jaccard similarity Similarity computing, recommendation system BFS Breadth-First Search Sequence traversal, shortest path planning Node2Vec - Graph classification","title":"Supported algorithms"},{"location":"nebula-algorithm/#implementation_methods","text":"Nebula Algorithm implements the graph calculating as follows: Read the graph data of DataFrame from the Nebula Graph database using the Nebula Spark Connector. Transform the graph data of DataFrame to the GraphX graph. Use graph algorithms provided by GraphX (such as PageRank) or self-implemented algorithms (such as Louvain). For detailed implementation methods, see Scala file .","title":"Implementation methods"},{"location":"nebula-algorithm/#get_nebula_algorithm","text":"","title":"Get Nebula Algorithm"},{"location":"nebula-algorithm/#compile_and_package","text":"Clone the repository nebula-algorithm . $ git clone -b v3.0.0 https://github.com/vesoft-inc/nebula-algorithm.git Enter the directory nebula-algorithm . $ cd nebula-algorithm Compile and package. $ mvn clean package -Dgpg.skip -Dmaven.javadoc.skip = true -Dmaven.test.skip = true After the compilation, a similar file nebula-algorithm-3.x.x.jar is generated in the directory nebula-algorithm/target .","title":"Compile and package"},{"location":"nebula-algorithm/#download_maven_from_the_remote_repository","text":"Download address","title":"Download maven from the remote repository"},{"location":"nebula-algorithm/#how_to_use","text":"","title":"How to use"},{"location":"nebula-algorithm/#use_algorithm_interface_recommended","text":"The lib repository provides 10 common graph algorithms. Add dependencies to the file pom.xml . <dependency> <groupId>com.vesoft</groupId> <artifactId>nebula-algorithm</artifactId> <version>3.0.0</version> </dependency> Use the algorithm (take PageRank as an example) by filling in parameters. For more examples, see example . Note By default, the DataFrame that executes the algorithm sets the first column as the starting vertex, the second column as the destination vertex, and the third column as the edge weights (not the rank in the Nebula Graph). val prConfig = new PRConfig ( 5 , 1 .0 ) val louvainResult = PageRankAlgo.apply ( spark, data, prConfig, false ) If your vertex IDs are Strings, see Pagerank Example for how to encoding and decoding them.","title":"Use algorithm interface (recommended)"},{"location":"nebula-algorithm/#submit_the_algorithm_package_directly","text":"Note There are limitations to use sealed packages. For example, when sinking a repository into Nebula Graph, the property name of the tag created in the sunk graph space must match the preset name in the code. The first method is recommended if the user has development skills. Set the Configuration file . { # Configurations related to Spark spark: { app: { name: LPA # The number of partitions of Spark partitionNum:100 } master:local } data: { # Data source. Optional values are nebula, csv, and json. source: csv # Data sink. The algorithm result will be written into this sink. Optional values are nebula, csv, and text. sink: nebula # Whether the algorithm has a weight. hasWeight: false } # Configurations related to Nebula Graph nebula: { # Data source. When Nebula Graph is the data source of the graph computing, the configuration of `nebula.read` is valid. read: { # The IP addresses and ports of all Meta services. Multiple addresses are separated by commas (,). Example: \"ip1:port1,ip2:port2\". # To deploy Nebula Graph by using Docker Compose, fill in the port with which Docker Compose maps to the outside. # Check the status with `docker-compose ps`. metaAddress: \"192.168.*.10:9559\" # The name of the graph space in Nebula Graph. space: basketballplayer # Edge types in Nebula Graph. When there are multiple labels, the data of multiple edges will be merged. labels: [ \"serve\" ] # The property name of each edge type in Nebula Graph. This property will be used as the weight column of the algorithm. Make sure that it corresponds to the edge type. weightCols: [ \"start_year\" ] } # Data sink. When the graph computing result sinks into Nebula Graph, the configuration of `nebula.write` is valid. write: { # The IP addresses and ports of all Graph services. Multiple addresses are separated by commas (,). Example: \"ip1:port1,ip2:port2\". # To deploy by using Docker Compose, fill in the port with which Docker Compose maps to the outside. # Check the status with `docker-compose ps`. graphAddress: \"192.168.*.11:9669\" # The IP addresses and ports of all Meta services. Multiple addresses are separated by commas (,). Example: \"ip1:port1,ip2:port2\". # To deploy Nebula Graph by using Docker Compose, fill in the port with which Docker Compose maps to the outside. # Check the staus with `docker-compose ps`. metaAddress: \"192.168.*.12:9559\" user:root pswd:nebula # Before submitting the graph computing task, create the graph space and tag. # The name of the graph space in Nebula Graph. space:nb # The name of the tag in Nebula Graph. The graph computing result will be written into this tag. The property name of this tag is as follows. # PageRank: pagerank # Louvain: louvain # ConnectedComponent: cc # StronglyConnectedComponent: scc # LabelPropagation: lpa # ShortestPath: shortestpath # DegreeStatic: degree\u3001inDegree\u3001outDegree # KCore: kcore # TriangleCount: tranglecpunt # BetweennessCentrality: betweennedss tag:pagerank } } local: { # Data source. When the data source is csv or json, the configuration of `local.read` is valid. read: { filePath: \"hdfs://127.0.0.1:9000/edge/work_for.csv\" # If the CSV file has a header or it is a json file, use the header. If not, use [_c0, _c1, _c2, ..., _cn] instead. # The header of the source VID column. srcId: \"_c0\" # The header of the destination VID column. dstId: \"_c1\" # The header of the weight column. weight: \"_c2\" # Whether the csv file has a header. header: false # The delimiter in the csv file. delimiter: \",\" } # Data sink. When the graph computing result sinks to the csv or text file, the configuration of `local.write` is valid. write: { resultPath:/tmp/ } } algorithm: { # The algorithm to execute. Optional values are as follow: # pagerank, louvain, connectedcomponent, labelpropagation, shortestpaths, # degreestatic, kcore, stronglyconnectedcomponent, trianglecount , # betweenness, graphtriangleCount. executeAlgo: pagerank # PageRank pagerank: { maxIter: 10 resetProb: 0 .15 } # Louvain louvain: { maxIter: 20 internalIter: 10 tol: 0 .5 } # ... } } Submit the graph computing task. ${ SPARK_HOME } /bin/spark-submit --master <mode> --class com.vesoft.nebula.algorithm.Main <nebula-algorithm-3.0.0.jar_path> -p <application.conf_path> Example: ${ SPARK_HOME } /bin/spark-submit --master \"local\" --class com.vesoft.nebula.algorithm.Main /root/nebula-algorithm/target/nebula-algorithm-3.0-SNAPSHOT.jar -p /root/nebula-algorithm/src/main/resources/application.conf","title":"Submit the algorithm package directly"},{"location":"nebula-analytics/","text":"Nebula Analytics \u00b6 Nebula Analytics is a high-performance graph computing framework tool that performs graph analysis of data in the Nebula Graph database. Enterpriseonly Only available for the Nebula Graph Enterprise Edition. Scenarios \u00b6 You can import data from data sources as Nebula Graph clusters, CSV files on HDFS, or local CSV files into Nebula Analytics and export the graph computation results to Nebula Graph clusters, CSV files on HDFS, or local CSV files from Nebula Analytics. Limitations \u00b6 When you import Nebula Graph cluster data into Nebula Analytics and export the graph computation results from Nebula Analytics to a Nebula Graph cluster, the graph computation results can only be exported to the graph space where the data source is located. Version compatibility \u00b6 The version correspondence between Nebula Analytics and Nebula Graph is as follows. Nebula Analytics Nebula Graph 1.1.0 3.1.0 1.0.x 3.0.x 0.9.0 2.6.x Graph algorithms \u00b6 Nebula Analytics supports the following graph algorithms. Algorithm Description Category APSP All Pair Shortest Path Path SSSP Single Source Shortest Path Path BFS Breadth-first search Path PageRank It is used to rank web pages. Node importance measurement KCore k-Cores Node importance measurement DegreeCentrality It is a simple count of the total number of connections linked to a vertex. Node importance measurement DegreeWithTime Neighbor statistics based on the time range of edge ranks Node importance measurement BetweennessCentrality Intermediate centrality Node importance measurement ClosenessCentrality Closeness centrality Node importance measurement TriangleCount It counts the number of triangles. Graph feature LPA Label Propagation Algorithm Community discovery WCC World Competitive Contests Community discovery LOUVAIN It detects communities in large networks. Community discovery HANP Hop attenuation & Node Preference Community discovery Clustering Coefficient It is a measure of the degree to which nodes in a graph tend to cluster together. Clustering Jaccard Jaccard similarity Similarity Install Nebula Analytics \u00b6 When installing a cluster of multiple Nebula Analytics on multiple nodes, you need to install Nebula Analytics to the same path and set up SSH-free login between nodes. sudo rpm -i nebula-analytics-1.0.0-centos.x86_64.rpm --prefix /home/xxx/nebula-analytics How to use Nebula Analytics \u00b6 After installation, you can set parameters of different algorithms and then execute a script to obtain the results of the algorithms and export them to the specified format. Select one node from the Nebula Analytics cluster and then access the scripts directory. $ cd scripts Confirm the data source and export path. Configuration steps are as follows. Nebula Graph clusters as the data source Modify the configuration file nebula.conf to configure the Nebula Graph cluster. # The number of retries connecting to Nebula Graph. --retry = 3 # The name of the graph space where you read or write data. --space = baskeyballplayer # Read data from Nebula Graph. # The metad process address. --meta_server_addrs = 192 .168.8.100:9559, 192 .168.8.101:9559, 192 .168.8.102:9559 # The name of edges. --edges = LIKES # The name of the property to be read as the weight of the edge. Can be either the attribute name or _rank. #--edge_data_fields # The number of rows read per scan. --read_batch_size = 10000 # Write data to Nebula Graph. # The graphd process address. --graph_server_addrs = 192 .168.8.100:9669 # The account to log into Nebula Graph. --user = root # The password to log into Nebula Graph. --password = nebula # The pattern used to write data back to Nebula Graph: insert or update. --mode = insert # The tag name written back to Nebula Graph. --tag = pagerank # The property name corresponding to the tag. --prop = pr # The property type corresponding the the tag. --type = double # The number of rows per write. --write_batch_size = 1000 # The file path where the data failed to be written back to Nebula Graph is stored. --err_file = /home/xxx/analytics/err.txt Modify the related parameters in the script to be used, such as run_pagerank.sh . # The sum of the number of processes running on all machines in the cluster. It is recommended to be the number of machines or the number of nodes in the NUMA architecture. WNUM = 3 # The number of threads per process. It is recommended to set the maximum value to be the number of hardware threads of the machine. WCORES = 4 # The path to the data source. # Set to read data from Nebula Graph via the nebula.conf file. INPUT = ${ INPUT := \"nebula: $PROJECT /scripts/nebula.conf\" } # Set to read data from the CSV files on HDFS or on local directories. # #INPUT=${INPUT:=\"$PROJECT/data/graph/v100_e2150_ua_c3.csv\"} # The export path to the graph computation results. # Data can be exported to a Nebula Graph. If the data source is also a Nebula Graph, the results will be exported to the graph space specified in nebula.conf. OUTPUT = ${ OUTPUT := \"nebula: $PROJECT /scripts/nebula.conf\" } # Data can also be exported to the CSV files on HDFS or on local directories. # OUTPUT=${OUTPUT:='hdfs://192.168.8.100:9000/_test/output'} # If the value is true, it is a directed graph, if false, it is an undirected graph. IS_DIRECTED = ${ IS_DIRECTED :=true } # Set whether to encode ID or not. NEED_ENCODE = ${ NEED_ENCODE :=true } # The ID type of the data source vertices. For example string, int32, and int64. VTYPE = ${ VTYPE :=int32 } # Encoding type. The value distributed specifies the distributed vertex ID encoding. The value single specifies the single-machine vertex ID encoding. ENCODER = ${ ENCODER := \"distributed\" } # The parameter for the PageRank algorithm. Algorithms differ in parameters. EPS = ${ EPS :=0.0001 } DAMPING = ${ DAMPING :=0.85 } # The number of iterations. ITERATIONS = ${ ITERATIONS :=100 } Local or HDFS CSV files as the data source Modify parameters in the script to be used, such as run_pagerank.sh . # The sum of the number of processes running on all machines in the cluster. It is recommended to be the number of machines or the number of nodes in the NUMA architecture. WNUM = 3 # The number of threads per process. It is recommended to set the maximum value to be the number of hardware threads of the machine. WCORES = 4 # The path to the data source. # Set to read data from Nebula Graph via the nebula.conf file. # INPUT=${INPUT:=\"nebula:$PROJECT/scripts/nebula.conf\"} # Set to read data from the CSV files on HDFS or on local directories. INPUT = ${ INPUT := \" $PROJECT /data/graph/v100_e2150_ua_c3.csv\" } # The export path to the graph computation results. # Data can be exported to a Nebula Graph. If the data source is also a Nebula Graph, the results will be exported to the graph space specified in nebula.conf. # OUTPUT=${OUTPUT:=\"nebula:$PROJECT/scripts/nebula.conf\"} # Data can also be exported to the CSV files on HDFS or on local directories. OUTPUT = ${ OUTPUT := 'hdfs://192.168.8.100:9000/_test/output' } # If the value is true, it is a directed graph, if false, it is an undirected graph. IS_DIRECTED = ${ IS_DIRECTED :=true } # Set whether to encode ID or not. NEED_ENCODE = ${ NEED_ENCODE :=true } # The ID type of the data source vertices. For example string, int32, and int64. VTYPE = ${ VTYPE :=int32 } # The value distributed specifies the distributed vertex ID encoding. The value single specifies the single-machine vertex ID encoding. ENCODER = ${ ENCODER := \"distributed\" } # The parameter for the PageRank algorithm. Algorithms differ in parameters. EPS = ${ EPS :=0.0001 } DAMPING = ${ DAMPING :=0.85 } # The number of iterations. ITERATIONS = ${ ITERATIONS :=100 } Modify the configuration file cluster to set the Nebula Analytics cluster nodes and task assignment weights for executing the algorithm. # Nebula Analytics Cluster Node IP Addresses: Task Assignment Weights 192 .168.8.200:1 192 .168.8.201:1 192 .168.8.202:1 Run the algorithm script. For example: ./run_pagerank.sh View the graph computation results in the export path. For exporting to a Nebula Graph cluster, check the results according to the settings in nebula.conf . For exporting the results to the CSV files on HDFS or on local directories, check the results according to the settings in OUTPUT , which is a compressed file in the .gz format.","title":"Nebula Analytics"},{"location":"nebula-analytics/#nebula_analytics","text":"Nebula Analytics is a high-performance graph computing framework tool that performs graph analysis of data in the Nebula Graph database. Enterpriseonly Only available for the Nebula Graph Enterprise Edition.","title":"Nebula Analytics"},{"location":"nebula-analytics/#scenarios","text":"You can import data from data sources as Nebula Graph clusters, CSV files on HDFS, or local CSV files into Nebula Analytics and export the graph computation results to Nebula Graph clusters, CSV files on HDFS, or local CSV files from Nebula Analytics.","title":"Scenarios"},{"location":"nebula-analytics/#limitations","text":"When you import Nebula Graph cluster data into Nebula Analytics and export the graph computation results from Nebula Analytics to a Nebula Graph cluster, the graph computation results can only be exported to the graph space where the data source is located.","title":"Limitations"},{"location":"nebula-analytics/#version_compatibility","text":"The version correspondence between Nebula Analytics and Nebula Graph is as follows. Nebula Analytics Nebula Graph 1.1.0 3.1.0 1.0.x 3.0.x 0.9.0 2.6.x","title":"Version compatibility"},{"location":"nebula-analytics/#graph_algorithms","text":"Nebula Analytics supports the following graph algorithms. Algorithm Description Category APSP All Pair Shortest Path Path SSSP Single Source Shortest Path Path BFS Breadth-first search Path PageRank It is used to rank web pages. Node importance measurement KCore k-Cores Node importance measurement DegreeCentrality It is a simple count of the total number of connections linked to a vertex. Node importance measurement DegreeWithTime Neighbor statistics based on the time range of edge ranks Node importance measurement BetweennessCentrality Intermediate centrality Node importance measurement ClosenessCentrality Closeness centrality Node importance measurement TriangleCount It counts the number of triangles. Graph feature LPA Label Propagation Algorithm Community discovery WCC World Competitive Contests Community discovery LOUVAIN It detects communities in large networks. Community discovery HANP Hop attenuation & Node Preference Community discovery Clustering Coefficient It is a measure of the degree to which nodes in a graph tend to cluster together. Clustering Jaccard Jaccard similarity Similarity","title":"Graph algorithms"},{"location":"nebula-analytics/#install_nebula_analytics","text":"When installing a cluster of multiple Nebula Analytics on multiple nodes, you need to install Nebula Analytics to the same path and set up SSH-free login between nodes. sudo rpm -i nebula-analytics-1.0.0-centos.x86_64.rpm --prefix /home/xxx/nebula-analytics","title":"Install Nebula Analytics"},{"location":"nebula-analytics/#how_to_use_nebula_analytics","text":"After installation, you can set parameters of different algorithms and then execute a script to obtain the results of the algorithms and export them to the specified format. Select one node from the Nebula Analytics cluster and then access the scripts directory. $ cd scripts Confirm the data source and export path. Configuration steps are as follows. Nebula Graph clusters as the data source Modify the configuration file nebula.conf to configure the Nebula Graph cluster. # The number of retries connecting to Nebula Graph. --retry = 3 # The name of the graph space where you read or write data. --space = baskeyballplayer # Read data from Nebula Graph. # The metad process address. --meta_server_addrs = 192 .168.8.100:9559, 192 .168.8.101:9559, 192 .168.8.102:9559 # The name of edges. --edges = LIKES # The name of the property to be read as the weight of the edge. Can be either the attribute name or _rank. #--edge_data_fields # The number of rows read per scan. --read_batch_size = 10000 # Write data to Nebula Graph. # The graphd process address. --graph_server_addrs = 192 .168.8.100:9669 # The account to log into Nebula Graph. --user = root # The password to log into Nebula Graph. --password = nebula # The pattern used to write data back to Nebula Graph: insert or update. --mode = insert # The tag name written back to Nebula Graph. --tag = pagerank # The property name corresponding to the tag. --prop = pr # The property type corresponding the the tag. --type = double # The number of rows per write. --write_batch_size = 1000 # The file path where the data failed to be written back to Nebula Graph is stored. --err_file = /home/xxx/analytics/err.txt Modify the related parameters in the script to be used, such as run_pagerank.sh . # The sum of the number of processes running on all machines in the cluster. It is recommended to be the number of machines or the number of nodes in the NUMA architecture. WNUM = 3 # The number of threads per process. It is recommended to set the maximum value to be the number of hardware threads of the machine. WCORES = 4 # The path to the data source. # Set to read data from Nebula Graph via the nebula.conf file. INPUT = ${ INPUT := \"nebula: $PROJECT /scripts/nebula.conf\" } # Set to read data from the CSV files on HDFS or on local directories. # #INPUT=${INPUT:=\"$PROJECT/data/graph/v100_e2150_ua_c3.csv\"} # The export path to the graph computation results. # Data can be exported to a Nebula Graph. If the data source is also a Nebula Graph, the results will be exported to the graph space specified in nebula.conf. OUTPUT = ${ OUTPUT := \"nebula: $PROJECT /scripts/nebula.conf\" } # Data can also be exported to the CSV files on HDFS or on local directories. # OUTPUT=${OUTPUT:='hdfs://192.168.8.100:9000/_test/output'} # If the value is true, it is a directed graph, if false, it is an undirected graph. IS_DIRECTED = ${ IS_DIRECTED :=true } # Set whether to encode ID or not. NEED_ENCODE = ${ NEED_ENCODE :=true } # The ID type of the data source vertices. For example string, int32, and int64. VTYPE = ${ VTYPE :=int32 } # Encoding type. The value distributed specifies the distributed vertex ID encoding. The value single specifies the single-machine vertex ID encoding. ENCODER = ${ ENCODER := \"distributed\" } # The parameter for the PageRank algorithm. Algorithms differ in parameters. EPS = ${ EPS :=0.0001 } DAMPING = ${ DAMPING :=0.85 } # The number of iterations. ITERATIONS = ${ ITERATIONS :=100 } Local or HDFS CSV files as the data source Modify parameters in the script to be used, such as run_pagerank.sh . # The sum of the number of processes running on all machines in the cluster. It is recommended to be the number of machines or the number of nodes in the NUMA architecture. WNUM = 3 # The number of threads per process. It is recommended to set the maximum value to be the number of hardware threads of the machine. WCORES = 4 # The path to the data source. # Set to read data from Nebula Graph via the nebula.conf file. # INPUT=${INPUT:=\"nebula:$PROJECT/scripts/nebula.conf\"} # Set to read data from the CSV files on HDFS or on local directories. INPUT = ${ INPUT := \" $PROJECT /data/graph/v100_e2150_ua_c3.csv\" } # The export path to the graph computation results. # Data can be exported to a Nebula Graph. If the data source is also a Nebula Graph, the results will be exported to the graph space specified in nebula.conf. # OUTPUT=${OUTPUT:=\"nebula:$PROJECT/scripts/nebula.conf\"} # Data can also be exported to the CSV files on HDFS or on local directories. OUTPUT = ${ OUTPUT := 'hdfs://192.168.8.100:9000/_test/output' } # If the value is true, it is a directed graph, if false, it is an undirected graph. IS_DIRECTED = ${ IS_DIRECTED :=true } # Set whether to encode ID or not. NEED_ENCODE = ${ NEED_ENCODE :=true } # The ID type of the data source vertices. For example string, int32, and int64. VTYPE = ${ VTYPE :=int32 } # The value distributed specifies the distributed vertex ID encoding. The value single specifies the single-machine vertex ID encoding. ENCODER = ${ ENCODER := \"distributed\" } # The parameter for the PageRank algorithm. Algorithms differ in parameters. EPS = ${ EPS :=0.0001 } DAMPING = ${ DAMPING :=0.85 } # The number of iterations. ITERATIONS = ${ ITERATIONS :=100 } Modify the configuration file cluster to set the Nebula Analytics cluster nodes and task assignment weights for executing the algorithm. # Nebula Analytics Cluster Node IP Addresses: Task Assignment Weights 192 .168.8.200:1 192 .168.8.201:1 192 .168.8.202:1 Run the algorithm script. For example: ./run_pagerank.sh View the graph computation results in the export path. For exporting to a Nebula Graph cluster, check the results according to the settings in nebula.conf . For exporting the results to the CSV files on HDFS or on local directories, check the results according to the settings in OUTPUT , which is a compressed file in the .gz format.","title":"How to use Nebula Analytics"},{"location":"nebula-bench/","text":"Nebula Bench \u00b6 Nebula Bench is a performance test tool for Nebula Graph using the LDBC data set. Scenario \u00b6 Generate test data and import Nebula Graph. Performance testing in the Nebula Graph cluster. Release note \u00b6 Release Test process \u00b6 Generate test data by using ldbc_snb_datagen. Import data to Nebula Graph by using the Importer. Performance testing by using K6 with the XK6-Nebula plug-in. For detailed usage instructions, see Nebula Bench .","title":"Nebula Bench"},{"location":"nebula-bench/#nebula_bench","text":"Nebula Bench is a performance test tool for Nebula Graph using the LDBC data set.","title":"Nebula Bench"},{"location":"nebula-bench/#scenario","text":"Generate test data and import Nebula Graph. Performance testing in the Nebula Graph cluster.","title":"Scenario"},{"location":"nebula-bench/#release_note","text":"Release","title":"Release note"},{"location":"nebula-bench/#test_process","text":"Generate test data by using ldbc_snb_datagen. Import data to Nebula Graph by using the Importer. Performance testing by using K6 with the XK6-Nebula plug-in. For detailed usage instructions, see Nebula Bench .","title":"Test process"},{"location":"nebula-console/","text":"Nebula Console \u00b6 Nebula Console is a native CLI client for Nebula Graph. It can be used to connect a Nebula Graph cluster and execute queries. It also supports special commands to manage parameters, export query results, import test datasets, etc. Obtain Nebula Console \u00b6 You can obtain Nebula Console in the following ways: Download the binary file from the GitHub releases page . Compile the source code to obtain the binary file. For more information, see Install from source code . Nebula Console functions \u00b6 Connect to Nebula Graph \u00b6 To connect to Nebula Graph with the nebula-console file, use the following syntax: <path_of_console> -addr <ip> -port <port> -u <username> -p <password> path_of_console indicates the storage path of the Nebula Console binary file. Parameter descriptions are as follows: Parameter Description -h/-help Shows the help menu. -addr/-address Sets the IP address of the Graph service. The default address is 127.0.0.1. If Nebula Graph is deployed on Nebula Cloud , you need to create a Private Link and set the IP address of the Private Endpoint as the parameter value. -P/-port Sets the port number of the graphd service. The default port number is 9669. -u/-user Sets the username of your Nebula Graph account. Before enabling authentication, you can use any existing username. The default username is root . -p/-password Sets the password of your Nebula Graph account. Before enabling authentication, you can use any characters as the password. -t/-timeout Sets an integer-type timeout threshold of the connection. The unit is second. The default value is 120. -e/-eval Sets a string-type nGQL statement. The nGQL statement is executed once the connection succeeds. The connection stops after the result is returned. -f/-file Sets the path of an nGQL file. The nGQL statements in the file are executed once the connection succeeds. The result will be returned and the connection stops then. -enable_ssl Enables SSL encryption when connecting to Nebula Graph. -ssl_root_ca_path Sets the storage path of the certification authority file. -ssl_cert_path Sets the storage path of the certificate file. -ssl_private_key_path Sets the storage path of the private key file. For information on more parameters, see the project repository . For example, to connect to the Graph Service deployed on 192.168.10.8, run the following command: ./nebula-console -addr 192 .168.10.8 -port 9669 -u root -p thisisapassword Manage parameters \u00b6 You can save parameters for parameterized queries. Note Setting a parameter as a VID in a query is not supported. Parameters are not supported in SAMPLE clauses. Parameters are deleted when their sessions are released. The command to save a parameter is as follows: nebula> :param <param_name> => <param_value>; The example is as follows: nebula> :param p1 => \"Tim Duncan\"; nebula> MATCH (v:player{name:$p1})-[:follow]->(n) RETURN v,n; +----------------------------------------------------+-------------------------------------------------------+ | v | n | +----------------------------------------------------+-------------------------------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | (\"player125\" :player{age: 41, name: \"Manu Ginobili\"}) | | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | (\"player101\" :player{age: 36, name: \"Tony Parker\"}) | +----------------------------------------------------+-------------------------------------------------------+ nebula> :param p2 => {\"a\":3,\"b\":false,\"c\":\"Tim Duncan\"}; nebula> RETURN $p2.b AS b; +-------+ | b | +-------+ | false | +-------+ The command to view the saved parameters is as follows: nebula> :params; The command to delete a specified parameter is as follows: nebula> :param <param_name> =>; Export query results \u00b6 Export query results, which can be saved as a CSV file or DOT file. Note The exported file is stored in the working directory, i.e., what the linux command pwd shows. This command only works for the next query statement. You can copy the contents of the DOT file and paste them in GraphvizOnline to generate a visualized execution plan. The command to export a csv file is as follows: nebula> :CSV <file_name.csv>; The command to export a DOT file is as follows: nebula> :dot <file_name.dot> The example is as follows: nebula> :dot a.dot nebula> PROFILE FORMAT=\"dot\" GO FROM \"player100\" OVER follow; Import a testing dataset \u00b6 The testing dataset is named basketballplayer . To view details about the schema and data, use the corresponding SHOW command. The command to import a testing dataset is as follows: nebula> :play basketballplayer Run a command multiple times \u00b6 To run a command multiple times, use the following command: nebula> :repeat N The example is as follows: nebula> :repeat 3 nebula> GO FROM \"player100\" OVER follow YIELD dst(edge); +-------------+ | dst(EDGE) | +-------------+ | \"player101\" | | \"player125\" | +-------------+ Got 2 rows (time spent 2602/3214 us) Fri, 20 Aug 2021 06:36:05 UTC +-------------+ | dst(EDGE) | +-------------+ | \"player101\" | | \"player125\" | +-------------+ Got 2 rows (time spent 583/849 us) Fri, 20 Aug 2021 06:36:05 UTC +-------------+ | dst(EDGE) | +-------------+ | \"player101\" | | \"player125\" | +-------------+ Got 2 rows (time spent 496/671 us) Fri, 20 Aug 2021 06:36:05 UTC Executed 3 times, (total time spent 3681/4734 us), (average time spent 1227/1578 us) Sleep \u00b6 This command will make Nebula Console sleep for N seconds. The schema is altered in an async way and takes effect in the next heartbeat cycle. Therefore, this command is usually used when altering schema. The command is as follows: nebula> :sleep N Disconnect Nebula Console from Nebula Graph \u00b6 You can use :EXIT or :QUIT to disconnect from Nebula Graph. For convenience, Nebula Console supports using these commands in lower case without the colon (\":\"), such as quit . The example is as follows: nebula> :QUIT Bye root!","title":"Nebula Console"},{"location":"nebula-console/#nebula_console","text":"Nebula Console is a native CLI client for Nebula Graph. It can be used to connect a Nebula Graph cluster and execute queries. It also supports special commands to manage parameters, export query results, import test datasets, etc.","title":"Nebula Console"},{"location":"nebula-console/#obtain_nebula_console","text":"You can obtain Nebula Console in the following ways: Download the binary file from the GitHub releases page . Compile the source code to obtain the binary file. For more information, see Install from source code .","title":"Obtain Nebula Console"},{"location":"nebula-console/#nebula_console_functions","text":"","title":"Nebula Console functions"},{"location":"nebula-console/#connect_to_nebula_graph","text":"To connect to Nebula Graph with the nebula-console file, use the following syntax: <path_of_console> -addr <ip> -port <port> -u <username> -p <password> path_of_console indicates the storage path of the Nebula Console binary file. Parameter descriptions are as follows: Parameter Description -h/-help Shows the help menu. -addr/-address Sets the IP address of the Graph service. The default address is 127.0.0.1. If Nebula Graph is deployed on Nebula Cloud , you need to create a Private Link and set the IP address of the Private Endpoint as the parameter value. -P/-port Sets the port number of the graphd service. The default port number is 9669. -u/-user Sets the username of your Nebula Graph account. Before enabling authentication, you can use any existing username. The default username is root . -p/-password Sets the password of your Nebula Graph account. Before enabling authentication, you can use any characters as the password. -t/-timeout Sets an integer-type timeout threshold of the connection. The unit is second. The default value is 120. -e/-eval Sets a string-type nGQL statement. The nGQL statement is executed once the connection succeeds. The connection stops after the result is returned. -f/-file Sets the path of an nGQL file. The nGQL statements in the file are executed once the connection succeeds. The result will be returned and the connection stops then. -enable_ssl Enables SSL encryption when connecting to Nebula Graph. -ssl_root_ca_path Sets the storage path of the certification authority file. -ssl_cert_path Sets the storage path of the certificate file. -ssl_private_key_path Sets the storage path of the private key file. For information on more parameters, see the project repository . For example, to connect to the Graph Service deployed on 192.168.10.8, run the following command: ./nebula-console -addr 192 .168.10.8 -port 9669 -u root -p thisisapassword","title":"Connect to Nebula Graph"},{"location":"nebula-console/#manage_parameters","text":"You can save parameters for parameterized queries. Note Setting a parameter as a VID in a query is not supported. Parameters are not supported in SAMPLE clauses. Parameters are deleted when their sessions are released. The command to save a parameter is as follows: nebula> :param <param_name> => <param_value>; The example is as follows: nebula> :param p1 => \"Tim Duncan\"; nebula> MATCH (v:player{name:$p1})-[:follow]->(n) RETURN v,n; +----------------------------------------------------+-------------------------------------------------------+ | v | n | +----------------------------------------------------+-------------------------------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | (\"player125\" :player{age: 41, name: \"Manu Ginobili\"}) | | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | (\"player101\" :player{age: 36, name: \"Tony Parker\"}) | +----------------------------------------------------+-------------------------------------------------------+ nebula> :param p2 => {\"a\":3,\"b\":false,\"c\":\"Tim Duncan\"}; nebula> RETURN $p2.b AS b; +-------+ | b | +-------+ | false | +-------+ The command to view the saved parameters is as follows: nebula> :params; The command to delete a specified parameter is as follows: nebula> :param <param_name> =>;","title":"Manage parameters"},{"location":"nebula-console/#export_query_results","text":"Export query results, which can be saved as a CSV file or DOT file. Note The exported file is stored in the working directory, i.e., what the linux command pwd shows. This command only works for the next query statement. You can copy the contents of the DOT file and paste them in GraphvizOnline to generate a visualized execution plan. The command to export a csv file is as follows: nebula> :CSV <file_name.csv>; The command to export a DOT file is as follows: nebula> :dot <file_name.dot> The example is as follows: nebula> :dot a.dot nebula> PROFILE FORMAT=\"dot\" GO FROM \"player100\" OVER follow;","title":"Export query results"},{"location":"nebula-console/#import_a_testing_dataset","text":"The testing dataset is named basketballplayer . To view details about the schema and data, use the corresponding SHOW command. The command to import a testing dataset is as follows: nebula> :play basketballplayer","title":"Import a testing dataset"},{"location":"nebula-console/#run_a_command_multiple_times","text":"To run a command multiple times, use the following command: nebula> :repeat N The example is as follows: nebula> :repeat 3 nebula> GO FROM \"player100\" OVER follow YIELD dst(edge); +-------------+ | dst(EDGE) | +-------------+ | \"player101\" | | \"player125\" | +-------------+ Got 2 rows (time spent 2602/3214 us) Fri, 20 Aug 2021 06:36:05 UTC +-------------+ | dst(EDGE) | +-------------+ | \"player101\" | | \"player125\" | +-------------+ Got 2 rows (time spent 583/849 us) Fri, 20 Aug 2021 06:36:05 UTC +-------------+ | dst(EDGE) | +-------------+ | \"player101\" | | \"player125\" | +-------------+ Got 2 rows (time spent 496/671 us) Fri, 20 Aug 2021 06:36:05 UTC Executed 3 times, (total time spent 3681/4734 us), (average time spent 1227/1578 us)","title":"Run a command multiple times"},{"location":"nebula-console/#sleep","text":"This command will make Nebula Console sleep for N seconds. The schema is altered in an async way and takes effect in the next heartbeat cycle. Therefore, this command is usually used when altering schema. The command is as follows: nebula> :sleep N","title":"Sleep"},{"location":"nebula-console/#disconnect_nebula_console_from_nebula_graph","text":"You can use :EXIT or :QUIT to disconnect from Nebula Graph. For convenience, Nebula Console supports using these commands in lower case without the colon (\":\"), such as quit . The example is as follows: nebula> :QUIT Bye root!","title":"Disconnect Nebula Console from Nebula Graph"},{"location":"nebula-flink-connector/","text":"Nebula Flink Connector \u00b6 Nebula Flink Connector is a connector that helps Flink users quickly access Nebula Graph. Nebula Flink Connector supports reading data from the Nebula Graph database or writing other external data to the Nebula Graph database. For more information, see Nebula Flink Connector . Use cases \u00b6 Nebula Flink Connector applies to the following scenarios: Migrate data between different Nebula Graph clusters. Migrate data between different graph spaces in the same Nebula Graph cluster. Migrate data between Nebula Graph and other data sources. Release note \u00b6 Release","title":"Nebula Flink Connector"},{"location":"nebula-flink-connector/#nebula_flink_connector","text":"Nebula Flink Connector is a connector that helps Flink users quickly access Nebula Graph. Nebula Flink Connector supports reading data from the Nebula Graph database or writing other external data to the Nebula Graph database. For more information, see Nebula Flink Connector .","title":"Nebula Flink Connector"},{"location":"nebula-flink-connector/#use_cases","text":"Nebula Flink Connector applies to the following scenarios: Migrate data between different Nebula Graph clusters. Migrate data between different graph spaces in the same Nebula Graph cluster. Migrate data between Nebula Graph and other data sources.","title":"Use cases"},{"location":"nebula-flink-connector/#release_note","text":"Release","title":"Release note"},{"location":"nebula-spark-connector/","text":"Nebula Spark Connector \u00b6 Nebula Spark Connector is a Spark connector application for reading and writing Nebula Graph data in Spark standard format. Nebula Spark Connector consists of two parts: Reader and Writer. Reader Provides a Spark SQL interface. This interface can be used to read Nebula Graph data. It reads one vertex or edge type data at a time and assemble the result into a Spark DataFrame. Writer Provides a Spark SQL interface. This interface can be used to write DataFrames into Nebula Graph in a row-by-row or batch-import way. For more information, see Nebula Spark Connector . Use cases \u00b6 Nebula Spark Connector applies to the following scenarios: Migrate data between different Nebula Graph clusters. Migrate data between different graph spaces in the same Nebula Graph cluster. Migrate data between Nebula Graph and other data sources. Graph computing with Nebula Algorithm . Benefits \u00b6 The features of Nebula Spark Connector 3.0.0 are as follows: Supports multiple connection settings, such as timeout period, number of connection retries, number of execution retries, etc. Supports multiple settings for data writing, such as setting the corresponding column as vertex ID, starting vertex ID, destination vertex ID or attributes. Supports non-attribute reading and full attribute reading. Supports reading Nebula Graph data into VertexRDD and EdgeRDD, and supports non-Long vertex IDs. Unifies the extended data source of SparkSQL, and uses DataSourceV2 to extend Nebula Graph data. Three write modes, insert , update and delete , are supported. insert mode will insert (overwrite) data, update mode will only update existing data, and delete mode will only delete data. Release note \u00b6 Release Get Nebula Spark Connector \u00b6 Compile package \u00b6 Note Install Nebula Spark Connector of version 2.4.x. Clone repository nebula-spark-connector . $ git clone -b v3.0.0 https://github.com/vesoft-inc/nebula-spark-connector.git Make the nebula-spark-connector directory the current working directory. $ cd nebula-spark-connector/nebula-spark-connector Compile package. $ mvn clean package -Dmaven.test.skip = true -Dgpg.skip -Dmaven.javadoc.skip = true After compilation, a similar file nebula-spark-connector-3.0.0-SHANPSHOT.jar is generated in the directory nebula-spark-connector/nebula-spark-connector/target/ . Download maven remote repository \u00b6 Download How to use \u00b6 When using Nebula Spark Connector to reading and writing Nebula Graph data, You can refer to the following code. # Read vertex and edge data from Nebula Graph . spark . read . nebula (). loadVerticesToDF () spark . read . nebula (). loadEdgesToDF () # Write dataframe data into Nebula Graph as vertex and edges . dataframe . write . nebula (). writeVertices () dataframe . write . nebula (). writeEdges () nebula() receives two configuration parameters, including connection configuration and read-write configuration. Reading data from Nebula Graph \u00b6 val config = NebulaConnectionConfig . builder () . withMetaAddress ( \"127.0.0.1:9559\" ) . withConenctionRetry ( 2 ) . withExecuteRetry ( 2 ) . withTimeout ( 6000 ) . build () val nebulaReadVertexConfig : ReadNebulaConfig = ReadNebulaConfig . builder () . withSpace ( \"test\" ) . withLabel ( \"person\" ) . withNoColumn ( false ) . withReturnCols ( List ( \"birthday\" )) . withLimit ( 10 ) . withPartitionNum ( 10 ) . build () val vertex = spark . read . nebula ( config , nebulaReadVertexConfig ). loadVerticesToDF () val nebulaReadEdgeConfig : ReadNebulaConfig = ReadNebulaConfig . builder () . withSpace ( \"test\" ) . withLabel ( \"knows\" ) . withNoColumn ( false ) . withReturnCols ( List ( \"degree\" )) . withLimit ( 10 ) . withPartitionNum ( 10 ) . build () val edge = spark . read . nebula ( config , nebulaReadEdgeConfig ). loadEdgesToDF () NebulaConnectionConfig is the configuration for connecting to the nebula graph, as described below. Parameter Required Description withMetaAddress Yes Specifies the IP addresses and ports of all Meta Services. Separate multiple addresses with commas. The format is ip1:port1,ip2:port2,... . Read data is no need to configure withGraphAddress . withConnectionRetry No The number of retries that the Nebula Java Client connected to the Nebula Graph. The default value is 1 . withExecuteRetry No The number of retries that the Nebula Java Client executed query statements. The default value is 1 . withTimeout No The timeout for the Nebula Java Client request response. The default value is 6000 , Unit: ms. ReadNebulaConfig is the configuration to read Nebula Graph data, as described below. Parameter Required Description withSpace Yes Nebula Graph space name. withLabel Yes The Tag or Edge type name within the Nebula Graph space. withNoColumn No Whether the property is not read. The default value is false , read property. If the value is true , the property is not read, the withReturnCols configuration is invalid. withReturnCols No Configures the set of properties for vertex or edges to read. the format is List(property1,property2,...) , The default value is List() , indicating that all properties are read. withLimit No Configure the number of rows of data read from the server by the Nebula Java Storage Client at a time. The default value is 1000 . withPartitionNum No Configures the number of Spark partitions to read the Nebula Graph data. The default value is 100 . This value should not exceed the number of slices in the graph space (partition_num). Write data into Nebula Graph \u00b6 Note The values of columns in a dataframe are automatically written to the Nebula Graph as property values. val config = NebulaConnectionConfig . builder () . withMetaAddress ( \"127.0.0.1:9559\" ) . withGraphAddress ( \"127.0.0.1:9669\" ) . withConenctionRetry ( 2 ) . build () val nebulaWriteVertexConfig : WriteNebulaVertexConfig = WriteNebulaVertexConfig . builder () . withSpace ( \"test\" ) . withTag ( \"person\" ) . withVidField ( \"id\" ) . withVidPolicy ( \"hash\" ) . withVidAsProp ( true ) . withUser ( \"root\" ) . withPasswd ( \"nebula\" ) . withBatch ( 1000 ) . build () df . write . nebula ( config , nebulaWriteVertexConfig ). writeVertices () val nebulaWriteEdgeConfig : WriteNebulaEdgeConfig = WriteNebulaEdgeConfig . builder () . withSpace ( \"test\" ) . withEdge ( \"friend\" ) . withSrcIdField ( \"src\" ) . withSrcPolicy ( null ) . withDstIdField ( \"dst\" ) . withDstPolicy ( null ) . withRankField ( \"degree\" ) . withSrcAsProperty ( true ) . withDstAsProperty ( true ) . withRankAsProperty ( true ) . withUser ( \"root\" ) . withPasswd ( \"nebula\" ) . withBatch ( 1000 ) . build () df . write . nebula ( config , nebulaWriteEdgeConfig ). writeEdges () The default write mode is insert , which can be changed to update via withWriteMode configuration: val config = NebulaConnectionConfig . builder () . withMetaAddress ( \"127.0.0.1:9559\" ) . withGraphAddress ( \"127.0.0.1:9669\" ) . build () val nebulaWriteVertexConfig = WriteNebulaVertexConfig . builder () . withSpace ( \"test\" ) . withTag ( \"person\" ) . withVidField ( \"id\" ) . withVidAsProp ( true ) . withBatch ( 1000 ) . withWriteMode ( WriteMode . UPDATE ) . build () df . write . nebula ( config , nebulaWriteVertexConfig ). writeVertices () NebulaConnectionConfig is the configuration for connecting to the nebula graph, as described below. Parameter Required Description withMetaAddress Yes Specifies the IP addresses and ports of all Meta Services. Separate multiple addresses with commas. The format is ip1:port1,ip2:port2,... . withGraphAddress Yes Specifies the IP addresses and ports of Graph Services. Separate multiple addresses with commas. The format is ip1:port1,ip2:port2,... . withConnectionRetry No Number of retries that the Nebula Java Client connected to the Nebula Graph. The default value is 1 . WriteNebulaVertexConfig is the configuration of the write vertex, as described below. Parameter Required Description withSpace Yes Nebula Graph space name. withTag Yes The Tag name that needs to be associated when a vertex is written. withVidField Yes The column in the DataFrame as the vertex ID. withVidPolicy No When writing the vertex ID, Nebula Graph use mapping function, supports HASH only. No mapping is performed by default. withVidAsProp No Whether the column in the DataFrame that is the vertex ID is also written as an property. The default value is false . If set to true , make sure the Tag has the same property name as VidField . withUser No Nebula Graph user name. If authentication is disabled, you do not need to configure the user name and password. withPasswd No The password for the Nebula Graph user name. withBatch Yes The number of rows of data written at a time. The default value is 1000 . withWriteMode No Write mode. The optional values are insert and update . The default value is insert . WriteNebulaEdgeConfig is the configuration of the write edge, as described below. Parameter Required Description withSpace Yes Nebula Graph space name. withEdge Yes The Edge type name that needs to be associated when a edge is written. withSrcIdField Yes The column in the DataFrame as the vertex ID. withSrcPolicy No When writing the starting vertex ID, Nebula Graph use mapping function, supports HASH only. No mapping is performed by default. withDstIdField Yes The column in the DataFrame that serves as the destination vertex. withDstPolicy No When writing the destination vertex ID, Nebula Graph use mapping function, supports HASH only. No mapping is performed by default. withRankField No The column in the DataFrame as the rank. Rank is not written by default. withSrcAsProperty No Whether the column in the DataFrame that is the starting vertex is also written as an property. The default value is false . If set to true , make sure Edge type has the same property name as SrcIdField . withDstAsProperty No Whether column that are destination vertex in the DataFrame are also written as property. The default value is false . If set to true , make sure Edge type has the same property name as DstIdField . withRankAsProperty No Whether column in the DataFrame that is the rank is also written as property.The default value is false . If set to true , make sure Edge type has the same property name as RankField . withUser No Nebula Graph user name. If authentication is disabled, you do not need to configure the user name and password. withPasswd No The password for the Nebula Graph user name. withBatch Yes The number of rows of data written at a time. The default value is 1000 . withWriteMode No Write mode. The optional values are insert and update . The default value is insert .","title":"Nebula Spark Connector"},{"location":"nebula-spark-connector/#nebula_spark_connector","text":"Nebula Spark Connector is a Spark connector application for reading and writing Nebula Graph data in Spark standard format. Nebula Spark Connector consists of two parts: Reader and Writer. Reader Provides a Spark SQL interface. This interface can be used to read Nebula Graph data. It reads one vertex or edge type data at a time and assemble the result into a Spark DataFrame. Writer Provides a Spark SQL interface. This interface can be used to write DataFrames into Nebula Graph in a row-by-row or batch-import way. For more information, see Nebula Spark Connector .","title":"Nebula Spark Connector"},{"location":"nebula-spark-connector/#use_cases","text":"Nebula Spark Connector applies to the following scenarios: Migrate data between different Nebula Graph clusters. Migrate data between different graph spaces in the same Nebula Graph cluster. Migrate data between Nebula Graph and other data sources. Graph computing with Nebula Algorithm .","title":"Use cases"},{"location":"nebula-spark-connector/#benefits","text":"The features of Nebula Spark Connector 3.0.0 are as follows: Supports multiple connection settings, such as timeout period, number of connection retries, number of execution retries, etc. Supports multiple settings for data writing, such as setting the corresponding column as vertex ID, starting vertex ID, destination vertex ID or attributes. Supports non-attribute reading and full attribute reading. Supports reading Nebula Graph data into VertexRDD and EdgeRDD, and supports non-Long vertex IDs. Unifies the extended data source of SparkSQL, and uses DataSourceV2 to extend Nebula Graph data. Three write modes, insert , update and delete , are supported. insert mode will insert (overwrite) data, update mode will only update existing data, and delete mode will only delete data.","title":"Benefits"},{"location":"nebula-spark-connector/#release_note","text":"Release","title":"Release note"},{"location":"nebula-spark-connector/#get_nebula_spark_connector","text":"","title":"Get Nebula Spark Connector"},{"location":"nebula-spark-connector/#compile_package","text":"Note Install Nebula Spark Connector of version 2.4.x. Clone repository nebula-spark-connector . $ git clone -b v3.0.0 https://github.com/vesoft-inc/nebula-spark-connector.git Make the nebula-spark-connector directory the current working directory. $ cd nebula-spark-connector/nebula-spark-connector Compile package. $ mvn clean package -Dmaven.test.skip = true -Dgpg.skip -Dmaven.javadoc.skip = true After compilation, a similar file nebula-spark-connector-3.0.0-SHANPSHOT.jar is generated in the directory nebula-spark-connector/nebula-spark-connector/target/ .","title":"Compile package"},{"location":"nebula-spark-connector/#download_maven_remote_repository","text":"Download","title":"Download maven remote repository"},{"location":"nebula-spark-connector/#how_to_use","text":"When using Nebula Spark Connector to reading and writing Nebula Graph data, You can refer to the following code. # Read vertex and edge data from Nebula Graph . spark . read . nebula (). loadVerticesToDF () spark . read . nebula (). loadEdgesToDF () # Write dataframe data into Nebula Graph as vertex and edges . dataframe . write . nebula (). writeVertices () dataframe . write . nebula (). writeEdges () nebula() receives two configuration parameters, including connection configuration and read-write configuration.","title":"How to use"},{"location":"nebula-spark-connector/#reading_data_from_nebula_graph","text":"val config = NebulaConnectionConfig . builder () . withMetaAddress ( \"127.0.0.1:9559\" ) . withConenctionRetry ( 2 ) . withExecuteRetry ( 2 ) . withTimeout ( 6000 ) . build () val nebulaReadVertexConfig : ReadNebulaConfig = ReadNebulaConfig . builder () . withSpace ( \"test\" ) . withLabel ( \"person\" ) . withNoColumn ( false ) . withReturnCols ( List ( \"birthday\" )) . withLimit ( 10 ) . withPartitionNum ( 10 ) . build () val vertex = spark . read . nebula ( config , nebulaReadVertexConfig ). loadVerticesToDF () val nebulaReadEdgeConfig : ReadNebulaConfig = ReadNebulaConfig . builder () . withSpace ( \"test\" ) . withLabel ( \"knows\" ) . withNoColumn ( false ) . withReturnCols ( List ( \"degree\" )) . withLimit ( 10 ) . withPartitionNum ( 10 ) . build () val edge = spark . read . nebula ( config , nebulaReadEdgeConfig ). loadEdgesToDF () NebulaConnectionConfig is the configuration for connecting to the nebula graph, as described below. Parameter Required Description withMetaAddress Yes Specifies the IP addresses and ports of all Meta Services. Separate multiple addresses with commas. The format is ip1:port1,ip2:port2,... . Read data is no need to configure withGraphAddress . withConnectionRetry No The number of retries that the Nebula Java Client connected to the Nebula Graph. The default value is 1 . withExecuteRetry No The number of retries that the Nebula Java Client executed query statements. The default value is 1 . withTimeout No The timeout for the Nebula Java Client request response. The default value is 6000 , Unit: ms. ReadNebulaConfig is the configuration to read Nebula Graph data, as described below. Parameter Required Description withSpace Yes Nebula Graph space name. withLabel Yes The Tag or Edge type name within the Nebula Graph space. withNoColumn No Whether the property is not read. The default value is false , read property. If the value is true , the property is not read, the withReturnCols configuration is invalid. withReturnCols No Configures the set of properties for vertex or edges to read. the format is List(property1,property2,...) , The default value is List() , indicating that all properties are read. withLimit No Configure the number of rows of data read from the server by the Nebula Java Storage Client at a time. The default value is 1000 . withPartitionNum No Configures the number of Spark partitions to read the Nebula Graph data. The default value is 100 . This value should not exceed the number of slices in the graph space (partition_num).","title":"Reading data from Nebula Graph"},{"location":"nebula-spark-connector/#write_data_into_nebula_graph","text":"Note The values of columns in a dataframe are automatically written to the Nebula Graph as property values. val config = NebulaConnectionConfig . builder () . withMetaAddress ( \"127.0.0.1:9559\" ) . withGraphAddress ( \"127.0.0.1:9669\" ) . withConenctionRetry ( 2 ) . build () val nebulaWriteVertexConfig : WriteNebulaVertexConfig = WriteNebulaVertexConfig . builder () . withSpace ( \"test\" ) . withTag ( \"person\" ) . withVidField ( \"id\" ) . withVidPolicy ( \"hash\" ) . withVidAsProp ( true ) . withUser ( \"root\" ) . withPasswd ( \"nebula\" ) . withBatch ( 1000 ) . build () df . write . nebula ( config , nebulaWriteVertexConfig ). writeVertices () val nebulaWriteEdgeConfig : WriteNebulaEdgeConfig = WriteNebulaEdgeConfig . builder () . withSpace ( \"test\" ) . withEdge ( \"friend\" ) . withSrcIdField ( \"src\" ) . withSrcPolicy ( null ) . withDstIdField ( \"dst\" ) . withDstPolicy ( null ) . withRankField ( \"degree\" ) . withSrcAsProperty ( true ) . withDstAsProperty ( true ) . withRankAsProperty ( true ) . withUser ( \"root\" ) . withPasswd ( \"nebula\" ) . withBatch ( 1000 ) . build () df . write . nebula ( config , nebulaWriteEdgeConfig ). writeEdges () The default write mode is insert , which can be changed to update via withWriteMode configuration: val config = NebulaConnectionConfig . builder () . withMetaAddress ( \"127.0.0.1:9559\" ) . withGraphAddress ( \"127.0.0.1:9669\" ) . build () val nebulaWriteVertexConfig = WriteNebulaVertexConfig . builder () . withSpace ( \"test\" ) . withTag ( \"person\" ) . withVidField ( \"id\" ) . withVidAsProp ( true ) . withBatch ( 1000 ) . withWriteMode ( WriteMode . UPDATE ) . build () df . write . nebula ( config , nebulaWriteVertexConfig ). writeVertices () NebulaConnectionConfig is the configuration for connecting to the nebula graph, as described below. Parameter Required Description withMetaAddress Yes Specifies the IP addresses and ports of all Meta Services. Separate multiple addresses with commas. The format is ip1:port1,ip2:port2,... . withGraphAddress Yes Specifies the IP addresses and ports of Graph Services. Separate multiple addresses with commas. The format is ip1:port1,ip2:port2,... . withConnectionRetry No Number of retries that the Nebula Java Client connected to the Nebula Graph. The default value is 1 . WriteNebulaVertexConfig is the configuration of the write vertex, as described below. Parameter Required Description withSpace Yes Nebula Graph space name. withTag Yes The Tag name that needs to be associated when a vertex is written. withVidField Yes The column in the DataFrame as the vertex ID. withVidPolicy No When writing the vertex ID, Nebula Graph use mapping function, supports HASH only. No mapping is performed by default. withVidAsProp No Whether the column in the DataFrame that is the vertex ID is also written as an property. The default value is false . If set to true , make sure the Tag has the same property name as VidField . withUser No Nebula Graph user name. If authentication is disabled, you do not need to configure the user name and password. withPasswd No The password for the Nebula Graph user name. withBatch Yes The number of rows of data written at a time. The default value is 1000 . withWriteMode No Write mode. The optional values are insert and update . The default value is insert . WriteNebulaEdgeConfig is the configuration of the write edge, as described below. Parameter Required Description withSpace Yes Nebula Graph space name. withEdge Yes The Edge type name that needs to be associated when a edge is written. withSrcIdField Yes The column in the DataFrame as the vertex ID. withSrcPolicy No When writing the starting vertex ID, Nebula Graph use mapping function, supports HASH only. No mapping is performed by default. withDstIdField Yes The column in the DataFrame that serves as the destination vertex. withDstPolicy No When writing the destination vertex ID, Nebula Graph use mapping function, supports HASH only. No mapping is performed by default. withRankField No The column in the DataFrame as the rank. Rank is not written by default. withSrcAsProperty No Whether the column in the DataFrame that is the starting vertex is also written as an property. The default value is false . If set to true , make sure Edge type has the same property name as SrcIdField . withDstAsProperty No Whether column that are destination vertex in the DataFrame are also written as property. The default value is false . If set to true , make sure Edge type has the same property name as DstIdField . withRankAsProperty No Whether column in the DataFrame that is the rank is also written as property.The default value is false . If set to true , make sure Edge type has the same property name as RankField . withUser No Nebula Graph user name. If authentication is disabled, you do not need to configure the user name and password. withPasswd No The password for the Nebula Graph user name. withBatch Yes The number of rows of data written at a time. The default value is 1000 . withWriteMode No Write mode. The optional values are insert and update . The default value is insert .","title":"Write data into Nebula Graph"},{"location":"1.introduction/0-0-graph/","text":"An introduction to graphs \u00b6 People from tech giants (such as Amazon and Facebook) to small research teams are devoting significant resources to exploring the potential of graph databases to solve data relationships problems. What exactly is a graph database? What can it do? Where does it fit in the database landscape? To answer these questions, we first need to understand graphs. Graphs are one of the main areas of research in computer science. Graphs can efficiently solve many of the problems that exist today. This topic will start with graphs to explain the advantages of graph databases and their great potential in modern application development, and then describe the differences between distributed graph databases and several other types of databases. What are graphs? \u00b6 Graphs are everywhere. When hearing the word graph, many people think of bar charts or line charts, because sometimes we call them graphs, which show the connections between two or more data systems. The simplest example is the following picture, which shows the number of Nebula Graph GitHub repository stars over time. This type of diagram is often called a line chart. As you can see, the number of starts rises over time. A line chart can show data changes over time (depending on the scale settings). Here we have given only examples of line charts. There are various graphs, such as pie charts, bar charts, etc. Another kind of diagram is often used in daily conversation, such as image recognition, retouched photos. This type of diagram is called a picture/photo/image. The diagram we discuss in this topic is a different concept, the graph in graph theory. In graph theory, a branch of mathematics, graphs are used to represent the relationships between entities. A graph consists of several small dots (called vertices or nodes) and lines or curves (called edges) that connect these dots. The term graph was proposed by Sylvester in 1878. The following picture is what this topic calls a graph. Simply put, graph theory is the study of graphs. Graph theory began in the early 18th century with the problem of the Seven Bridges of K\u00f6nigsberg. K\u00f6nigsberg was then a Prussian city (now part of Russia, renamed Kaliningrad). The river Preger crossed K\u00f6nigsberg and not only divided K\u00f6nigsberg into two parts, but also formed two small islands in the middle of the river. This divided the city into four areas, each connected by seven bridges. There was a game associated with K\u00f6nigsberg at the time, namely how to cross each bridge only once and navigate the entire four areas of the city. A simplified view of the seven bridges is shown below. Try to find the answer to this game if you are interested 1 . To solve this problem, the great mathematician Euler by abstracting the four regions of the city into points and the seven bridges connecting the city into edges connecting the points, proved that the problem was unsolvable. The simplified abstract diagram is as follows 2 . The four dots in the picture represent the four regions of K\u00f6nigsberg, and the lines between the dots represent the seven bridges connecting the four regions. It is easy to see that the area connected by the even-numbered bridges can be easily passed because different routes can be chosen to come and go. The areas connected by the odd-numbered bridges can only be used as starting or endings points because the same route can only be taken once. The number of edges associated with a node is called the node degree. Now it can be shown that the K\u00f6nigsberg problem can only be solved if two nodes have odd degrees and the other nodes have even degrees, i.e., two regions must have an even number of bridges and the remaining regions have an odd number of bridges. However, as we know from the above picture, there is no even number of bridges in any region of K\u00f6nigsberg, so this puzzle is unsolvable. Property graphs \u00b6 From a mathematical point of view, graph theory studies the relationships between modeled objects. However, it is common to extend the underlying graph model. The extended graphs are called the attribute graph model . A property graph usually consists of the following components. Node, an object or entity. In this topic, nodes are called vertices. Relationship between nodes. In this topic, relationships are called edges. Usually, the edges can be directed or undirected to indicate a relationship between two entities. There can be properties on nodes and edges. In real life, there are many examples of property graphs. For example, Qichacha or BOSS Zhipin use graphs to model business equity relationships. A vertex is usually a natural person or a business, and the edge is the equity relationship between a person and a business. The properties on vertices can be the name, age, ID number, etc. of the natural person. The properties on edges can be the investment amount, investment time, position such as director and supervisor. A vertex can be a listed company and an edge can be a correlation between listed companies. The vertex property can be a stock code, abbreviation, market capitalization, sector, etc. The edge property can be the time-series correlation coefficient of the stock price 3 . The graph relationship can also be similar to the character relationship in a TV series like Game of Thrones 4 . Vertices are the characters. Edges are the interactions between the characters. Vertex properties are the character's names, ages, camps, etc., and edge properties are the number of interactions between two characters. Graphs are also used for governance within IT systems. For example, a company like WeBank has a very large data warehouse and corresponding data warehouse management tools. These management tools record the ETL relationships between the Hive tables in the data warehouse through Job implementation 5 . Such ETL relationships can be very easily presented and managed in the form of graphs, and the root cause can be easily traced when problems arise. Graphs can also be used to document the invocation relationships between the intricate microservices within a large IT system 6 , which is used by operations teams for service governance. Here each point represents a microservice and the edge represents the invocation relationship between two microservices; thus, Ops can easily find invocation links with availability below a threshold (99.99%) or discover microservice nodes that would be particularly affected by a failure. Graphs are also used to record the invocation relationships between the intricate microservices 6 . Each vertex represents a microservice and an edge represents the invocation relationship between two microservices. This allows Ops to easily find call links with availability below a threshold (99.99%), or to discover microservice nodes where a failure would have a particularly large impact. Graphs can also be used to improve the efficiency of code development. Graphs store function call relationships between codes 6 to improve the efficiency of reviewing and testing the code. In such a graph, each vertex is a function or variable, each edge is a call relationship between functions or variables. When there is a new code commit, one can more easily see other interfaces that may be affected, which helps testers better assess potential go-live risks. In addition, we can discover more scenarios by adding some temporal information as opposed to a static property graph that does not change. For example, inside a network of interbank account fund flows 7 , a vertex is an account, an edge is the transfer record between accounts. Edge properties record the time, amount, etc. of the transfer. Companies can use graph technology to easily explore the graph to discover obvious misappropriation of funds, paying back a load to with the loan, loan gang scams, and other phenomena. The same approach can be used to explore the discovery of the flow of cryptocurrencies. In a network of accounts and devices 8 , vertices can be accounts, mobile devices, and WIFI networks, edges are the login relationships between these accounts and mobile devices, and the access relationships between mobile devices and WIFI networks. These graph data records the characteristic of the network black production operations. Some big companies such as 360 DigiTech 8 , Kuaishou 9 , WeChat 10 , Zhihu 11 , and Ctrip Finance all identified over a million crime groups through technology. In addition to the dimension of time, you can find more scenarios for property graphs by adding some geographic location information. For an example of tracing the source of the Coronavirus Disease (COVID-19) 12 , vertices are the person and edges are the contact between people. Vertex properties are the information of the person's ID card and onset time, and edge properties are the time and geographical location of the close contact between people, etc. It provides help for health prevention departments to quickly identify high-risk people and their behavioral trajectories. The combination of geographic location and graph is also used in some O2O scenarios, such as real-time food recommendation based on POI (Point-of-Interest) 13 , which enables local life service platform companies like Meituan to recommend more suitable businesses in real-time when consumers open the APP. A graph is also used for knowledge inference. Huawei, Vivo, OPPO, WeChat, Meituan, and other companies use graphs for the representation of the underlying knowledge relationships. Why do we use graph databases? \u00b6 Although relational databases and semi-structured databases such as XML/JSON can be used to describe a graph-structured data model, a graph (database) not only describes the graph structure and stores data itself but also focuses on handling the associative relationships between the data. Specifically, graph databases have several advantages: Graphs are a more visual and intuitive way of representing knowledge to human brains. This allows us to focus on the business problem itself rather than how to describe the problem as a particular structure of the database (e.g., a table structure). It is easier to show the characteristic of the data in graphs. Such as transfer paths and nearby communities. To analyze the relationships of characters and character importance in Game of Thrones, data displayed with tables is not as intuitive as with graphs. Especially when some central vertices are deleted: Adding an edge can completely change the entire topology. We can intuitively sense the importance of minor changes in graphs rather than in tables. Graph query language is designed based on graph structures. The following is a query example in LDBC. Requirements: Query the posts posted by a person, and query the corresponding replies (the replies themselves will also be replied multiple times). Since the posting time and reply time both meet certain conditions, you can sort the results according to the number of replies. Write querying statements using PostgreSQL: --PostgreSQL WITH RECURSIVE post_all ( psa_threadid , psa_thread_creatorid , psa_messageid , psa_creationdate , psa_messagetype ) AS ( SELECT m_messageid AS psa_threadid , m_creatorid AS psa_thread_creatorid , m_messageid AS psa_messageid , m_creationdate , 'Post' FROM message WHERE 1 = 1 AND m_c_replyof IS NULL -- post, not comment AND m_creationdate BETWEEN : startDate AND : endDate UNION ALL SELECT psa . psa_threadid AS psa_threadid , psa . psa_thread_creatorid AS psa_thread_creatorid , m_messageid , m_creationdate , 'Comment' FROM message p , post_all psa WHERE 1 = 1 AND p . m_c_replyof = psa . psa_messageid AND m_creationdate BETWEEN : startDate AND : endDate ) SELECT p . p_personid AS \"person.id\" , p . p_firstname AS \"person.firstName\" , p . p_lastname AS \"person.lastName\" , count ( DISTINCT psa . psa_threadid ) AS threadCount END ) AS messageCount , count ( DISTINCT psa . psa_messageid ) AS messageCount FROM person p left join post_all psa on ( 1 = 1 AND p . p_personid = psa . psa_thread_creatorid AND psa_creationdate BETWEEN : startDate AND : endDate ) GROUP BY p . p_personid , p . p_firstname , p . p_lastname ORDER BY messageCount DESC , p . p_personid LIMIT 100 ; Write querying statements using Cypher designed especially for graphs: -- Cypher MATCH ( person : Person ) <-[ : HAS_CREATOR ]- ( post : Post ) <-[ : REPLY_OF * 0 .. ]- ( reply : Message ) WHERE post . creationDate >= $ startDate AND post . creationDate <= $ endDate AND reply . creationDate >= $ startDate AND reply . creationDate <= $ endDate RETURN person . id , person . firstName , person . lastName , count ( DISTINCT post ) AS threadCount , count ( DISTINCT reply ) AS messageCount ORDER BY messageCount DESC , person . id ASC LIMIT 100 Graph traversal (corresponding to Join in SQL) is much more efficient because the storage and query engines are designed specifically for the structure of the graph. Graph databases have a wide range of application scenarios. Examples include data integration (knowledge graph), personalized recommendations, fraud, and threat detection, risk analysis, and compliance, identity (and control) verification, IT infrastructure management, supply chain, and logistics, social network research, etc. According to the literature 14 , the fields that use graph technology are (from the greatest to least): information technology (IT), research in academia, finance, laboratories in industry, government, healthcare, defense, pharmaceuticals, retail, and e-commerce, transportation, telecommunications, and insurance. In 2019, according to Gartner's questionnaire research, 27% of customers (500 groups) are using graph databases and 20% have plans to use them. RDF \u00b6 This topic does not discuss the RDF data model due to space limitations. Souce of the picture: https://medium.freecodecamp.org/i-dont-understand-graph-theory-1c96572a1401. \u21a9 Source of the picture: https://medium.freecodecamp.org/i-dont-understand-graph-theory-1c96572a1401 \u21a9 https://nebula-graph.com.cn/posts/stock-interrelation-analysis-jgrapht-nebula-graph/ \u21a9 https://nebula-graph.com.cn/posts/game-of-thrones-relationship-networkx-gephi-nebula-graph/ \u21a9 https://nebula-graph.com.cn/posts/practicing-nebula-graph-webank/ \u21a9 https://nebula-graph.com.cn/posts/meituan-graph-database-platform-practice/ \u21a9 \u21a9 \u21a9 https://zhuanlan.zhihu.com/p/90635957 \u21a9 https://nebula-graph.com.cn/posts/graph-database-data-connections-insight/ \u21a9 \u21a9 https://nebula-graph.com.cn/posts/kuaishou-security-intelligence-platform-with-nebula-graph/ \u21a9 https://nebula-graph.com.cn/posts/nebula-graph-for-social-networking/ \u21a9 https://mp.weixin.qq.com/s/K2QinpR5Rplw1teHpHtf4w \u21a9 https://nebula-graph.com.cn/posts/detect-corona-virus-spreading-with-graph-database/ \u21a9 https://nebula-graph.com.cn/posts/meituan-graph-database-platform-practice/ \u21a9 https://arxiv.org/abs/1709.03188 \u21a9","title":"Introduction to graphs"},{"location":"1.introduction/0-0-graph/#an_introduction_to_graphs","text":"People from tech giants (such as Amazon and Facebook) to small research teams are devoting significant resources to exploring the potential of graph databases to solve data relationships problems. What exactly is a graph database? What can it do? Where does it fit in the database landscape? To answer these questions, we first need to understand graphs. Graphs are one of the main areas of research in computer science. Graphs can efficiently solve many of the problems that exist today. This topic will start with graphs to explain the advantages of graph databases and their great potential in modern application development, and then describe the differences between distributed graph databases and several other types of databases.","title":"An introduction to graphs"},{"location":"1.introduction/0-0-graph/#what_are_graphs","text":"Graphs are everywhere. When hearing the word graph, many people think of bar charts or line charts, because sometimes we call them graphs, which show the connections between two or more data systems. The simplest example is the following picture, which shows the number of Nebula Graph GitHub repository stars over time. This type of diagram is often called a line chart. As you can see, the number of starts rises over time. A line chart can show data changes over time (depending on the scale settings). Here we have given only examples of line charts. There are various graphs, such as pie charts, bar charts, etc. Another kind of diagram is often used in daily conversation, such as image recognition, retouched photos. This type of diagram is called a picture/photo/image. The diagram we discuss in this topic is a different concept, the graph in graph theory. In graph theory, a branch of mathematics, graphs are used to represent the relationships between entities. A graph consists of several small dots (called vertices or nodes) and lines or curves (called edges) that connect these dots. The term graph was proposed by Sylvester in 1878. The following picture is what this topic calls a graph. Simply put, graph theory is the study of graphs. Graph theory began in the early 18th century with the problem of the Seven Bridges of K\u00f6nigsberg. K\u00f6nigsberg was then a Prussian city (now part of Russia, renamed Kaliningrad). The river Preger crossed K\u00f6nigsberg and not only divided K\u00f6nigsberg into two parts, but also formed two small islands in the middle of the river. This divided the city into four areas, each connected by seven bridges. There was a game associated with K\u00f6nigsberg at the time, namely how to cross each bridge only once and navigate the entire four areas of the city. A simplified view of the seven bridges is shown below. Try to find the answer to this game if you are interested 1 . To solve this problem, the great mathematician Euler by abstracting the four regions of the city into points and the seven bridges connecting the city into edges connecting the points, proved that the problem was unsolvable. The simplified abstract diagram is as follows 2 . The four dots in the picture represent the four regions of K\u00f6nigsberg, and the lines between the dots represent the seven bridges connecting the four regions. It is easy to see that the area connected by the even-numbered bridges can be easily passed because different routes can be chosen to come and go. The areas connected by the odd-numbered bridges can only be used as starting or endings points because the same route can only be taken once. The number of edges associated with a node is called the node degree. Now it can be shown that the K\u00f6nigsberg problem can only be solved if two nodes have odd degrees and the other nodes have even degrees, i.e., two regions must have an even number of bridges and the remaining regions have an odd number of bridges. However, as we know from the above picture, there is no even number of bridges in any region of K\u00f6nigsberg, so this puzzle is unsolvable.","title":"What are graphs?"},{"location":"1.introduction/0-0-graph/#property_graphs","text":"From a mathematical point of view, graph theory studies the relationships between modeled objects. However, it is common to extend the underlying graph model. The extended graphs are called the attribute graph model . A property graph usually consists of the following components. Node, an object or entity. In this topic, nodes are called vertices. Relationship between nodes. In this topic, relationships are called edges. Usually, the edges can be directed or undirected to indicate a relationship between two entities. There can be properties on nodes and edges. In real life, there are many examples of property graphs. For example, Qichacha or BOSS Zhipin use graphs to model business equity relationships. A vertex is usually a natural person or a business, and the edge is the equity relationship between a person and a business. The properties on vertices can be the name, age, ID number, etc. of the natural person. The properties on edges can be the investment amount, investment time, position such as director and supervisor. A vertex can be a listed company and an edge can be a correlation between listed companies. The vertex property can be a stock code, abbreviation, market capitalization, sector, etc. The edge property can be the time-series correlation coefficient of the stock price 3 . The graph relationship can also be similar to the character relationship in a TV series like Game of Thrones 4 . Vertices are the characters. Edges are the interactions between the characters. Vertex properties are the character's names, ages, camps, etc., and edge properties are the number of interactions between two characters. Graphs are also used for governance within IT systems. For example, a company like WeBank has a very large data warehouse and corresponding data warehouse management tools. These management tools record the ETL relationships between the Hive tables in the data warehouse through Job implementation 5 . Such ETL relationships can be very easily presented and managed in the form of graphs, and the root cause can be easily traced when problems arise. Graphs can also be used to document the invocation relationships between the intricate microservices within a large IT system 6 , which is used by operations teams for service governance. Here each point represents a microservice and the edge represents the invocation relationship between two microservices; thus, Ops can easily find invocation links with availability below a threshold (99.99%) or discover microservice nodes that would be particularly affected by a failure. Graphs are also used to record the invocation relationships between the intricate microservices 6 . Each vertex represents a microservice and an edge represents the invocation relationship between two microservices. This allows Ops to easily find call links with availability below a threshold (99.99%), or to discover microservice nodes where a failure would have a particularly large impact. Graphs can also be used to improve the efficiency of code development. Graphs store function call relationships between codes 6 to improve the efficiency of reviewing and testing the code. In such a graph, each vertex is a function or variable, each edge is a call relationship between functions or variables. When there is a new code commit, one can more easily see other interfaces that may be affected, which helps testers better assess potential go-live risks. In addition, we can discover more scenarios by adding some temporal information as opposed to a static property graph that does not change. For example, inside a network of interbank account fund flows 7 , a vertex is an account, an edge is the transfer record between accounts. Edge properties record the time, amount, etc. of the transfer. Companies can use graph technology to easily explore the graph to discover obvious misappropriation of funds, paying back a load to with the loan, loan gang scams, and other phenomena. The same approach can be used to explore the discovery of the flow of cryptocurrencies. In a network of accounts and devices 8 , vertices can be accounts, mobile devices, and WIFI networks, edges are the login relationships between these accounts and mobile devices, and the access relationships between mobile devices and WIFI networks. These graph data records the characteristic of the network black production operations. Some big companies such as 360 DigiTech 8 , Kuaishou 9 , WeChat 10 , Zhihu 11 , and Ctrip Finance all identified over a million crime groups through technology. In addition to the dimension of time, you can find more scenarios for property graphs by adding some geographic location information. For an example of tracing the source of the Coronavirus Disease (COVID-19) 12 , vertices are the person and edges are the contact between people. Vertex properties are the information of the person's ID card and onset time, and edge properties are the time and geographical location of the close contact between people, etc. It provides help for health prevention departments to quickly identify high-risk people and their behavioral trajectories. The combination of geographic location and graph is also used in some O2O scenarios, such as real-time food recommendation based on POI (Point-of-Interest) 13 , which enables local life service platform companies like Meituan to recommend more suitable businesses in real-time when consumers open the APP. A graph is also used for knowledge inference. Huawei, Vivo, OPPO, WeChat, Meituan, and other companies use graphs for the representation of the underlying knowledge relationships.","title":"Property graphs"},{"location":"1.introduction/0-0-graph/#why_do_we_use_graph_databases","text":"Although relational databases and semi-structured databases such as XML/JSON can be used to describe a graph-structured data model, a graph (database) not only describes the graph structure and stores data itself but also focuses on handling the associative relationships between the data. Specifically, graph databases have several advantages: Graphs are a more visual and intuitive way of representing knowledge to human brains. This allows us to focus on the business problem itself rather than how to describe the problem as a particular structure of the database (e.g., a table structure). It is easier to show the characteristic of the data in graphs. Such as transfer paths and nearby communities. To analyze the relationships of characters and character importance in Game of Thrones, data displayed with tables is not as intuitive as with graphs. Especially when some central vertices are deleted: Adding an edge can completely change the entire topology. We can intuitively sense the importance of minor changes in graphs rather than in tables. Graph query language is designed based on graph structures. The following is a query example in LDBC. Requirements: Query the posts posted by a person, and query the corresponding replies (the replies themselves will also be replied multiple times). Since the posting time and reply time both meet certain conditions, you can sort the results according to the number of replies. Write querying statements using PostgreSQL: --PostgreSQL WITH RECURSIVE post_all ( psa_threadid , psa_thread_creatorid , psa_messageid , psa_creationdate , psa_messagetype ) AS ( SELECT m_messageid AS psa_threadid , m_creatorid AS psa_thread_creatorid , m_messageid AS psa_messageid , m_creationdate , 'Post' FROM message WHERE 1 = 1 AND m_c_replyof IS NULL -- post, not comment AND m_creationdate BETWEEN : startDate AND : endDate UNION ALL SELECT psa . psa_threadid AS psa_threadid , psa . psa_thread_creatorid AS psa_thread_creatorid , m_messageid , m_creationdate , 'Comment' FROM message p , post_all psa WHERE 1 = 1 AND p . m_c_replyof = psa . psa_messageid AND m_creationdate BETWEEN : startDate AND : endDate ) SELECT p . p_personid AS \"person.id\" , p . p_firstname AS \"person.firstName\" , p . p_lastname AS \"person.lastName\" , count ( DISTINCT psa . psa_threadid ) AS threadCount END ) AS messageCount , count ( DISTINCT psa . psa_messageid ) AS messageCount FROM person p left join post_all psa on ( 1 = 1 AND p . p_personid = psa . psa_thread_creatorid AND psa_creationdate BETWEEN : startDate AND : endDate ) GROUP BY p . p_personid , p . p_firstname , p . p_lastname ORDER BY messageCount DESC , p . p_personid LIMIT 100 ; Write querying statements using Cypher designed especially for graphs: -- Cypher MATCH ( person : Person ) <-[ : HAS_CREATOR ]- ( post : Post ) <-[ : REPLY_OF * 0 .. ]- ( reply : Message ) WHERE post . creationDate >= $ startDate AND post . creationDate <= $ endDate AND reply . creationDate >= $ startDate AND reply . creationDate <= $ endDate RETURN person . id , person . firstName , person . lastName , count ( DISTINCT post ) AS threadCount , count ( DISTINCT reply ) AS messageCount ORDER BY messageCount DESC , person . id ASC LIMIT 100 Graph traversal (corresponding to Join in SQL) is much more efficient because the storage and query engines are designed specifically for the structure of the graph. Graph databases have a wide range of application scenarios. Examples include data integration (knowledge graph), personalized recommendations, fraud, and threat detection, risk analysis, and compliance, identity (and control) verification, IT infrastructure management, supply chain, and logistics, social network research, etc. According to the literature 14 , the fields that use graph technology are (from the greatest to least): information technology (IT), research in academia, finance, laboratories in industry, government, healthcare, defense, pharmaceuticals, retail, and e-commerce, transportation, telecommunications, and insurance. In 2019, according to Gartner's questionnaire research, 27% of customers (500 groups) are using graph databases and 20% have plans to use them.","title":"Why do we use graph databases?"},{"location":"1.introduction/0-0-graph/#rdf","text":"This topic does not discuss the RDF data model due to space limitations. Souce of the picture: https://medium.freecodecamp.org/i-dont-understand-graph-theory-1c96572a1401. \u21a9 Source of the picture: https://medium.freecodecamp.org/i-dont-understand-graph-theory-1c96572a1401 \u21a9 https://nebula-graph.com.cn/posts/stock-interrelation-analysis-jgrapht-nebula-graph/ \u21a9 https://nebula-graph.com.cn/posts/game-of-thrones-relationship-networkx-gephi-nebula-graph/ \u21a9 https://nebula-graph.com.cn/posts/practicing-nebula-graph-webank/ \u21a9 https://nebula-graph.com.cn/posts/meituan-graph-database-platform-practice/ \u21a9 \u21a9 \u21a9 https://zhuanlan.zhihu.com/p/90635957 \u21a9 https://nebula-graph.com.cn/posts/graph-database-data-connections-insight/ \u21a9 \u21a9 https://nebula-graph.com.cn/posts/kuaishou-security-intelligence-platform-with-nebula-graph/ \u21a9 https://nebula-graph.com.cn/posts/nebula-graph-for-social-networking/ \u21a9 https://mp.weixin.qq.com/s/K2QinpR5Rplw1teHpHtf4w \u21a9 https://nebula-graph.com.cn/posts/detect-corona-virus-spreading-with-graph-database/ \u21a9 https://nebula-graph.com.cn/posts/meituan-graph-database-platform-practice/ \u21a9 https://arxiv.org/abs/1709.03188 \u21a9","title":"RDF"},{"location":"1.introduction/0-1-graph-database/","text":"Market overview of graph databases \u00b6 Now that we have discussed what a graph is, let's move on to further understanding graph databases developed based on graph theory and the property graph model. Different graph databases may differ slightly in terms of terminology, but in the end, they all talk about vertices, edges, and properties. As for more advanced features such as labels, indexes, constraints, TTL, long tasks, stored procedures, and UDFs, these advanced features will vary significantly from one graph database to another. Graph databases use graphs to store data, and graphs are one of the closest to highly flexible, high-performance data structures. A graph database is a storage engine specifically designed to store and retrieve large information, which efficiently stores data as vertices and edges and allows high-performance retrieval and querying of these vertex-edge structures. We can also add properties to these vertices and edges. Third-party services market predictions \u00b6 DB-Engines ranking \u00b6 According to DB-Engines.com, the world's leading database ranking site, graph databases have been the fastest growing database category since 2013 1 . The site counts trends in the popularity of each category based on several metrics, including records and trends based on search engines such as Google, technical topics discussed on major IT technology forums and social networking sites, job posting changes on job boards. 371 database products are included in the site and are divided into 12 categories. Of these 12 categories, a category like graph databases is growing much faster than any of the others. Gartner\u2019s predictions \u00b6 Gartner, one of the world's top think tanks, identified graph databases as a major business intelligence and analytics technology trend long before 2013 2 . At that time, big data was hot as ever, and data scientists were in a hot position. Until recently, graph databases and related graph technologies were ranked in the Top 10 Data and Analytics Trends for 2021 3 . Trend 8: Graph Relates Everything Graphs form the foundation of many modern data and analytics capabilities to find relationships between people, places, things, events, and locations across diverse data assets. D&A leaders rely on graphs to quickly answer complex business questions which require contextual awareness and an understanding of the nature of connections and strengths across multiple entities. Gartner predicts that by 2025, graph technologies will be used in 80% of data and analytics innovations, up from 10% in 2021, facilitating rapid decision-making across the organization. It can be noted that Gartner's predictions match the DB-Engines ranking well. There is usually a period of rapid bubble development, then a plateau period, followed by a new bubble period due to the emergence of new technologies, and then a plateau period. And so on in a spiral. Market size of graph databases \u00b6 According to statistics and forecasts from Verifiedmarketresearc 4 , fnfresearch 5 , MarketsandMarkets 6 , and Gartner 7 , the global graph database market size to grow from about USD 0.8 billion in 2019 to USD 3-4 billion by 2026, at a Compound Annual Growth Rate (CAGR) of about 25%, which corresponds to about 5%-10% market share of the global database market. Market participants \u00b6 Neo4j, the pioneer of (first generation) graph databases \u00b6 Although some graph-like data models and products, and the corresponding graph language G/G+ had been proposed in the 1970s (e.g. CODASYL 8 ). But it is Neo4j, the main pioneer in this market, that has really made the concept of graph databases popular, and even the two main terms (labeled) property graphs and graph databases were first introduced and practiced by Neo4j. !!! Info \"This section on the history of Neo4j and the graph query language it created, Cypher, is largely excerpted from the ISO WG3 paper An overview of the recent history of Graph Query Languages 10 and 9 . To take into account the latest two years of development, the content mentioned in this topic has been abridged and updated by the authors of this book. About GQL (Graph Query Language) and the development of an International Standard Readers familiar with databases are probably aware of the Structured Query Language SQL. by using SQL, people access databases in a way that is close to natural language. Before SQL was widely adopted and standardized, the market for relational databases was very fragmented. Each vendor's product had a completely different way of accessing. Developers of the database product itself, developers of the tools surrounding the database product, and end-users of the database, all had to learn each product. When the SQL-89 standard was developed in 1989, the entire relational database market quickly focus on SQL-89. This greatly reduced the learning costs for the people mentioned above. GQL (Graph Query Language) assumes a role similar to SQL in the field of graph databases. Uses interacts with graphs with GQL. Unlike international standards such as SQL-89, there are no international standards for GQL. Two mainstream graph languages are Neo4j\u2019s Cypher and Apache TinkerPop's Gremlin. The former is often referred to as the DQL, Declarative Query Language. DQL tells the system \"what to do\", regardless of \"how to do\". The latter is referred to as the IQL, Imperative Query Language. IQL explicitly specifies the system's actions. The GQL International Standard is in the process of being developed. Overview of the recent history of graph databases \u00b6 In 2000, the idea of modeling data as a network came to the founders of Neo4j. In 2001, Neo4j developed the earliest core part of the code. In 2007, Neo4j started operating as a company. In 2009, Neo4j borrowed XPath as a graph query language. Gremlin 11 is also similar to XPath. In 2010, Marko Rodriguez, a Neo4j employee, used the term Property Graph to describe the data model of Neo4j and TinkerPop (Gremlin). In 2011, the first public version Neo4j 1.4 was released, and the first version of Cypher was released. In 2012, Neo4j 1.8 enabled you to write a Cypher. Neo4j 2.0 added labels and indexes. Cypher became a declarative graph query language. In 2015, Cypher was opened up by Neo4j through the openCypher project. In 2017, the ISO WG3 organization discussed how to use SQL to query property graph data. In 2018, Starting from the Neo4j 3.5 GA, the core of Neo4j only for the Enterprise Edition will no longer be open source. In 2019, ISO officially established two projects ISO/IEC JTC 1 N 14279 and ISO/IEC JTC 1/SC 32 N 3228 to develop an international standard for graph database language. In 2021, the $325 million Series F funding round for Neo4j marks the largest investment round in database history. The early history of Neo4j \u00b6 The data model property graph was first conceived in 2000. The founders of Neo4j were developing a media management system, and the schema of the system was often changed. To adapt to such changes, Peter Neubauer, one of the founders, wanted to enable the system to be modeled to a conceptually interconnected network. A group of graduate students at the Indian Institute of Technology Bombay implemented the earliest prototypes. Emil Eifr\u00e9m, the Neo4j co-founder, and these students spent a week extending Peter's idea into a more abstract model: vertices were connected by relationships, and key-values were used as properties of vertices and relationships. They developed a Java API to interact with this data model and implemented an abstraction layer on top of the relational database. Although this network model greatly improved productivity, its performance has been poor. So Johan Svensson, Neo4j co-founder, put a lot of effort into implementing a native data management system, that is Neo4j. For the first few years, Neo4j was successful as an in-house product. In 2007, the intellectual property of Neo4j was transferred to an independent database company. In the first public release of Neo4j ( Neo4j 1.4\uff0c2011), the data model was consisted of vertices and typed edges. Vertices and edges have properties. The early versions of Neo4j did not have indexes. Applications had to construct their search structure from the root vertex. Because this was very unwieldy for the applications, Neo4j 2.0 (2013.12) introduced a new concept label on vertices. Based on labels, Neo4j can index some predefined vertex properties. \"Vertex\", \"Relationship\", \"Property\", \"Relationships can only have one label.\", \"Vertices can have zero or multiple labels.\". All these concepts form the data model definitions for Neo4j property graphs. With the later addition of indexing, Cypher became the main way of interacting with Neo4j. This is because the application developer only needs to focus on the data itself, not on the search structure that the developer built himself as mentioned above. The creation of Gremlin \u00b6 Gremlin is a graph query language based on Apache TinkerPop, which is close in style to a sequence of function (procedure) calls. Initially, Neo4j was queried through the Java API. applications could embed the query engine as a library into the application and then use the API to query the graph. The early Neo4j employees Tobias Lindaaker, Ivarsson, Peter Neubauer, and Marko Rodriguez used XPath as a graph query. Groovy provides loop structures, branching, and computation. This was the original prototype of Gremlin, the first version of which was released in November 2009. Later, Marko found a lot of problems with using two different parsers (XPath and Groovy) at the same time and changed Gremlin to a Domain Specific Language (DSL) based on Groovy. The creation of Cypher \u00b6 Gremlin, like Neo4j's Java API, was originally intended to be a procedural way of expressing how to query databases. It uses shorter syntaxes to query and remotely access databases through the network. The procedural nature of Gremlin requires users to know the best way to query results, which is still burdensome for application developers. Over the last 30 years, the declarative language SQL has been a great success. SQL can separate the declarative way to get data from how the engine gets data. So the Neo4j engineers wanted to develop a declarative graph query language. In 2010, Andr\u00e9s Taylor joined Neo4j as an engineer. Inspired by SQL, he started a project to develop graph query language, which was released as Neo4j 1.4 in 2011. The language is the ancestor of most graph query languages today - Cypher. Cypher's syntax is based on the use of ASCII art to describe graph patterns. This approach originally came from the annotations on how to describe graph patterns in the source code. An example can be seen as follows. Simply put, ASCII art uses printable text to describe vertices and edges. Cypher syntax uses () for vertices and -[]-> for edges. (query)-[modeled as]->(drawing) is used to represent a simple graph relationship (which can also be called graph schema): the starting vertex - query , the destination vertex - drawing , and the edge - modeled as . The first version of Cypher implemented graph reading, but users should specify vertices from which to start querying. Only from these vertices could graph schema matching be supported. In a later version, Neo4j 1.8, released in October 2012, Cypher added the ability to modify graphs. However, queries still need to specify which nodes to start from. In December 2013, Neo4j 2.0 introduced the concept of a label, which is essentially an index. This allows the query engine to use the index to select the vertices matched by the schema, without requiring the user to specify the vertex to start the query. With the popularity of Neo4j, Cypher has a wide community of developers and is widely used in a variety of industries. It is still the most popular graph query language. In September 2015, Neo4j established the openCypher Implementors Group (oCIG) to open source Cypher to openCypher, to govern and advance the evolution of the language itself through open source. Subsequent events \u00b6 Cypher has inspired a series of graph query languages, including: 2015, Oracle released PGQL, a graph language used by the graph engine PGX. 2016, the Linked Data Benchmarking Council (short for LDBC) an industry-renowned benchmarking organization for graph performance, released G-CORE. 2018, RedisGraph, a Redis-based graph library, adopted Cypher as its graph language. 2019, the International Standards Organization ISO started two projects to initiate the process of developing an international standard for graph languages based on existing industry achievements such as openCypher, PGQL, GSQL 12 , and G-CORE. 2019, Nebula Graph released Nebula Graph Query Language (nGQL) based on openCypher. Distributed graph databases \u00b6 From 2005 to 2010, with the release of Google's cloud computing \"Troika\", various distributed architectures became increasingly popular, including Hadoop and Cassandra, which have been open-sourced. Several implications are as follows: The technical and cost advantages of distributed systems over single machines (e.g. Neo4j) or small machines are more obvious due to the increasing volume of data and computation. Distributed systems allow applications to access these thousands of machines as if they were local systems, without the need for much modification at the code level. The open-source approach allows more people to know emerging technologies and feedback to the community in a more cost-effective way, including code developers, data scientists, and product managers. Strictly speaking, Neo4j also offers several distributed capabilities, which are quite different from the industry's sense of the distributed system. Neo4j 3. x requires that the full amount of data must be stored on a single machine. Although it supports full replication and high availability between multiple machines, the data cannot be sliced into different subgraphs. Neo4j 4. x stores a part of data on different machines (subgraphs), and then the application layer assembles data in a certain way (called Fabric) 13 and distributes the reads and writes to each machine. This approach requires a log of involvement and work from the application layer code. For example, designing how to place different subgraphs on which machines they should be placed and how to assemble some of the results obtained from each machine into the final result. The style of its syntax is as follows: USE graphA MATCH ( movie : Movie ) Return movie . title AS title UNION USE graphB MATCH \uff08 move : Movie ) RETURN movie . title AS title The second generation (distributed) graph database: Titan and its successor JanusGraph \u00b6 In 2011, Aurelius was founded to develop an open-source distributed graph database called Titan 14 . By the first official release of Titan in 2015, the backend of Titan can support many major distributed storage architectures (e.g. Cassandra, HBase, Elasticsearch, BerkeleyDB) and can reuse many of the conveniences of the Hadoop ecosystem, with Gremlin as a unified query language on the frontend. It is easy for programmers to use, develop and participate in the community. Large-scale graphs could be sharded and stored on HBase or Cassandra (which were relatively mature distributed storage solutions at the time), and the Gremlin language was relatively full-featured though slightly lengthy. The whole solution was competitive at that time (2011-2015). The following picture shows the growth of Titan and Neo4j stars on Github.com from 2012 to 2015. After Aurelius (Titan) was acquired by DataStax in 2015, Titan was gradually transformed into a closed-source commercial product(DataStax Enterprise Graph). After the acquisition of Aurelius(Titan), there has been a strong demand for an open-source distributed graph database, and there were not many mature and active products in the market. In the era of big data, data is still being generated in a steady stream, far faster than Moore's Law. The Linux Foundation, along with some technology giants (Expero, Google, GRAKN.AI, Hortonworks, IBM, and Amazon) replicated and forked the original Titan project and started it as a new project JanusGraph 15 . Most of the community work including development, testing, release, and promotion, has been gradually shifted to the new JanusGraph\u3002 The following graph shows the evolution of daily code commits (pull requests) for the two projects, and we can see: Although Aurelius(Titan) still has some activity in its open-source code after its acquisition in 2015, the growth rate has slowed down significantly. This reflects the strength of the community. After the new project was started in January 2017, its community became active quickly, surpassing the number of pull requests accumulated by Titan in the past 5 years in just one year. At the same time, the open-source Titan came to a halt. Famous products of the same period OrientDB, TigerGraph, ArangoDB, and DGraph \u00b6 In addition to JanusGraph managed by the Linux Foundation, more vendors have been joined the overall market. Some distributed graph databases that were developed by commercial companies use different data models and access methods. The following table only lists the main differences. Vendors Creation time Core product Open source protocol Data model Query language OrientDB LTD (Acquired by SAP in 2017) 2011 OrientDB Open source Document + KV + Graph OrientDB SQL (SQL-based extended graph abilities) GraphSQL (was renamed TigerGraph) 2012 TigerGraph Commercial version Graph (Analysis) GraphSQL (similar to SQL) ArangoDB GmbH 2014 ArangoDB Apache License 2.0 Document + KV + Graph AQL (Simultaneous operation of documents, KVs and graphs) DGraph Labs 2016 DGraph Apache Public License 2.0 + Dgraph Community License Originally RDF, later changed to GraphQL GraphQL+- Traditional giants Microsoft, Amazon, and Oracle \u00b6 In addition to vendors focused on graph products, traditional giants have also entered the graph database field. Microsoft Azure Cosmos DB 16 is a multimodal database cloud service on the Microsoft cloud that provides SQL, document, graph, key-value, and other capabilities. Amazon AWS Neptune 17 is a graph database cloud service provided by AWS support property graphs and RDF two data models. Oracle Graph 18 is a product of the relational database giant Oracle in the direction of graph technology and graph databases. Nebula Graph, a new generation of open-source distributed graph databases \u00b6 In the following topics, we will formally introduce Nebula Graph, a new generation of open-source distributed graph databases. https://db-engines.com/en/ranking_categories \u21a9 https://www.yellowfinbi.com/blog/2014/06/yfcommunitynews-big-data-analytics-the-need-for-pragmatism-tangible-benefits-and-real-world-case-165305 \u21a9 https://www.gartner.com/smarterwithgartner/gartner-top-10-data-and-analytics-trends-for-2021/ \u21a9 https://www.verifiedmarketresearch.com/product/graph-database-market/ \u21a9 https://www.globenewswire.com/news-release/2021/01/28/2165742/0/en/Global-Graph-Database-Market-Size-Share-to-Exceed-USD-4-500-Million-By-2026-Facts-Factors.html \u21a9 https://www.marketsandmarkets.com/Market-Reports/graph-database-market-126230231.html \u21a9 https://www.gartner.com/en/newsroom/press-releases/2019-07-01-gartner-says-the-future-of-the-database-market-is-the \u21a9 https://www.amazon.com/Designing-Data-Intensive-Applications-Reliable-Maintainable/dp/1449373321 \u21a9 I. F. Cruz, A. O. Mendelzon, and P. T. Wood. A Graphical Query Language Supporting Recursion. In Proceedings of the Association for Computing Machinery Special Interest Group on Management of Data, pages 323\u2013330. ACM Press, May 1987. \u21a9 \"An overview of the recent history of Graph Query Languages\". Authors: Tobias Lindaaker, U.S. National Expert.Date: 2018-05-14 \u21a9 Gremlin is a graph language developed based on Apache TinkerPop . \u21a9 https://docs.tigergraph.com/dev/gsql-ref \u21a9 https://neo4j.com/fosdem20/ \u21a9 https://github.com/thinkaurelius/titan \u21a9 https://github.com/JanusGraph/janusgraph \u21a9 https://azure.microsoft.com/en-us/free/cosmos-db/ \u21a9 https://aws.amazon.com/cn/neptune/ \u21a9 https://www.oracle.com/database/graph/ \u21a9","title":"Graph databases"},{"location":"1.introduction/0-1-graph-database/#market_overview_of_graph_databases","text":"Now that we have discussed what a graph is, let's move on to further understanding graph databases developed based on graph theory and the property graph model. Different graph databases may differ slightly in terms of terminology, but in the end, they all talk about vertices, edges, and properties. As for more advanced features such as labels, indexes, constraints, TTL, long tasks, stored procedures, and UDFs, these advanced features will vary significantly from one graph database to another. Graph databases use graphs to store data, and graphs are one of the closest to highly flexible, high-performance data structures. A graph database is a storage engine specifically designed to store and retrieve large information, which efficiently stores data as vertices and edges and allows high-performance retrieval and querying of these vertex-edge structures. We can also add properties to these vertices and edges.","title":"Market overview of graph databases"},{"location":"1.introduction/0-1-graph-database/#third-party_services_market_predictions","text":"","title":"Third-party services market predictions"},{"location":"1.introduction/0-1-graph-database/#db-engines_ranking","text":"According to DB-Engines.com, the world's leading database ranking site, graph databases have been the fastest growing database category since 2013 1 . The site counts trends in the popularity of each category based on several metrics, including records and trends based on search engines such as Google, technical topics discussed on major IT technology forums and social networking sites, job posting changes on job boards. 371 database products are included in the site and are divided into 12 categories. Of these 12 categories, a category like graph databases is growing much faster than any of the others.","title":"DB-Engines ranking"},{"location":"1.introduction/0-1-graph-database/#gartners_predictions","text":"Gartner, one of the world's top think tanks, identified graph databases as a major business intelligence and analytics technology trend long before 2013 2 . At that time, big data was hot as ever, and data scientists were in a hot position. Until recently, graph databases and related graph technologies were ranked in the Top 10 Data and Analytics Trends for 2021 3 . Trend 8: Graph Relates Everything Graphs form the foundation of many modern data and analytics capabilities to find relationships between people, places, things, events, and locations across diverse data assets. D&A leaders rely on graphs to quickly answer complex business questions which require contextual awareness and an understanding of the nature of connections and strengths across multiple entities. Gartner predicts that by 2025, graph technologies will be used in 80% of data and analytics innovations, up from 10% in 2021, facilitating rapid decision-making across the organization. It can be noted that Gartner's predictions match the DB-Engines ranking well. There is usually a period of rapid bubble development, then a plateau period, followed by a new bubble period due to the emergence of new technologies, and then a plateau period. And so on in a spiral.","title":"Gartner\u2019s predictions"},{"location":"1.introduction/0-1-graph-database/#market_size_of_graph_databases","text":"According to statistics and forecasts from Verifiedmarketresearc 4 , fnfresearch 5 , MarketsandMarkets 6 , and Gartner 7 , the global graph database market size to grow from about USD 0.8 billion in 2019 to USD 3-4 billion by 2026, at a Compound Annual Growth Rate (CAGR) of about 25%, which corresponds to about 5%-10% market share of the global database market.","title":"Market size of graph databases"},{"location":"1.introduction/0-1-graph-database/#market_participants","text":"","title":"Market participants"},{"location":"1.introduction/0-1-graph-database/#neo4j_the_pioneer_of_first_generation_graph_databases","text":"Although some graph-like data models and products, and the corresponding graph language G/G+ had been proposed in the 1970s (e.g. CODASYL 8 ). But it is Neo4j, the main pioneer in this market, that has really made the concept of graph databases popular, and even the two main terms (labeled) property graphs and graph databases were first introduced and practiced by Neo4j. !!! Info \"This section on the history of Neo4j and the graph query language it created, Cypher, is largely excerpted from the ISO WG3 paper An overview of the recent history of Graph Query Languages 10 and 9 . To take into account the latest two years of development, the content mentioned in this topic has been abridged and updated by the authors of this book. About GQL (Graph Query Language) and the development of an International Standard Readers familiar with databases are probably aware of the Structured Query Language SQL. by using SQL, people access databases in a way that is close to natural language. Before SQL was widely adopted and standardized, the market for relational databases was very fragmented. Each vendor's product had a completely different way of accessing. Developers of the database product itself, developers of the tools surrounding the database product, and end-users of the database, all had to learn each product. When the SQL-89 standard was developed in 1989, the entire relational database market quickly focus on SQL-89. This greatly reduced the learning costs for the people mentioned above. GQL (Graph Query Language) assumes a role similar to SQL in the field of graph databases. Uses interacts with graphs with GQL. Unlike international standards such as SQL-89, there are no international standards for GQL. Two mainstream graph languages are Neo4j\u2019s Cypher and Apache TinkerPop's Gremlin. The former is often referred to as the DQL, Declarative Query Language. DQL tells the system \"what to do\", regardless of \"how to do\". The latter is referred to as the IQL, Imperative Query Language. IQL explicitly specifies the system's actions. The GQL International Standard is in the process of being developed.","title":"Neo4j, the pioneer of (first generation) graph databases"},{"location":"1.introduction/0-1-graph-database/#overview_of_the_recent_history_of_graph_databases","text":"In 2000, the idea of modeling data as a network came to the founders of Neo4j. In 2001, Neo4j developed the earliest core part of the code. In 2007, Neo4j started operating as a company. In 2009, Neo4j borrowed XPath as a graph query language. Gremlin 11 is also similar to XPath. In 2010, Marko Rodriguez, a Neo4j employee, used the term Property Graph to describe the data model of Neo4j and TinkerPop (Gremlin). In 2011, the first public version Neo4j 1.4 was released, and the first version of Cypher was released. In 2012, Neo4j 1.8 enabled you to write a Cypher. Neo4j 2.0 added labels and indexes. Cypher became a declarative graph query language. In 2015, Cypher was opened up by Neo4j through the openCypher project. In 2017, the ISO WG3 organization discussed how to use SQL to query property graph data. In 2018, Starting from the Neo4j 3.5 GA, the core of Neo4j only for the Enterprise Edition will no longer be open source. In 2019, ISO officially established two projects ISO/IEC JTC 1 N 14279 and ISO/IEC JTC 1/SC 32 N 3228 to develop an international standard for graph database language. In 2021, the $325 million Series F funding round for Neo4j marks the largest investment round in database history.","title":"Overview of the recent history of graph databases"},{"location":"1.introduction/0-1-graph-database/#the_early_history_of_neo4j","text":"The data model property graph was first conceived in 2000. The founders of Neo4j were developing a media management system, and the schema of the system was often changed. To adapt to such changes, Peter Neubauer, one of the founders, wanted to enable the system to be modeled to a conceptually interconnected network. A group of graduate students at the Indian Institute of Technology Bombay implemented the earliest prototypes. Emil Eifr\u00e9m, the Neo4j co-founder, and these students spent a week extending Peter's idea into a more abstract model: vertices were connected by relationships, and key-values were used as properties of vertices and relationships. They developed a Java API to interact with this data model and implemented an abstraction layer on top of the relational database. Although this network model greatly improved productivity, its performance has been poor. So Johan Svensson, Neo4j co-founder, put a lot of effort into implementing a native data management system, that is Neo4j. For the first few years, Neo4j was successful as an in-house product. In 2007, the intellectual property of Neo4j was transferred to an independent database company. In the first public release of Neo4j ( Neo4j 1.4\uff0c2011), the data model was consisted of vertices and typed edges. Vertices and edges have properties. The early versions of Neo4j did not have indexes. Applications had to construct their search structure from the root vertex. Because this was very unwieldy for the applications, Neo4j 2.0 (2013.12) introduced a new concept label on vertices. Based on labels, Neo4j can index some predefined vertex properties. \"Vertex\", \"Relationship\", \"Property\", \"Relationships can only have one label.\", \"Vertices can have zero or multiple labels.\". All these concepts form the data model definitions for Neo4j property graphs. With the later addition of indexing, Cypher became the main way of interacting with Neo4j. This is because the application developer only needs to focus on the data itself, not on the search structure that the developer built himself as mentioned above.","title":"The early history of Neo4j"},{"location":"1.introduction/0-1-graph-database/#the_creation_of_gremlin","text":"Gremlin is a graph query language based on Apache TinkerPop, which is close in style to a sequence of function (procedure) calls. Initially, Neo4j was queried through the Java API. applications could embed the query engine as a library into the application and then use the API to query the graph. The early Neo4j employees Tobias Lindaaker, Ivarsson, Peter Neubauer, and Marko Rodriguez used XPath as a graph query. Groovy provides loop structures, branching, and computation. This was the original prototype of Gremlin, the first version of which was released in November 2009. Later, Marko found a lot of problems with using two different parsers (XPath and Groovy) at the same time and changed Gremlin to a Domain Specific Language (DSL) based on Groovy.","title":"The creation of Gremlin"},{"location":"1.introduction/0-1-graph-database/#the_creation_of_cypher","text":"Gremlin, like Neo4j's Java API, was originally intended to be a procedural way of expressing how to query databases. It uses shorter syntaxes to query and remotely access databases through the network. The procedural nature of Gremlin requires users to know the best way to query results, which is still burdensome for application developers. Over the last 30 years, the declarative language SQL has been a great success. SQL can separate the declarative way to get data from how the engine gets data. So the Neo4j engineers wanted to develop a declarative graph query language. In 2010, Andr\u00e9s Taylor joined Neo4j as an engineer. Inspired by SQL, he started a project to develop graph query language, which was released as Neo4j 1.4 in 2011. The language is the ancestor of most graph query languages today - Cypher. Cypher's syntax is based on the use of ASCII art to describe graph patterns. This approach originally came from the annotations on how to describe graph patterns in the source code. An example can be seen as follows. Simply put, ASCII art uses printable text to describe vertices and edges. Cypher syntax uses () for vertices and -[]-> for edges. (query)-[modeled as]->(drawing) is used to represent a simple graph relationship (which can also be called graph schema): the starting vertex - query , the destination vertex - drawing , and the edge - modeled as . The first version of Cypher implemented graph reading, but users should specify vertices from which to start querying. Only from these vertices could graph schema matching be supported. In a later version, Neo4j 1.8, released in October 2012, Cypher added the ability to modify graphs. However, queries still need to specify which nodes to start from. In December 2013, Neo4j 2.0 introduced the concept of a label, which is essentially an index. This allows the query engine to use the index to select the vertices matched by the schema, without requiring the user to specify the vertex to start the query. With the popularity of Neo4j, Cypher has a wide community of developers and is widely used in a variety of industries. It is still the most popular graph query language. In September 2015, Neo4j established the openCypher Implementors Group (oCIG) to open source Cypher to openCypher, to govern and advance the evolution of the language itself through open source.","title":"The creation of Cypher"},{"location":"1.introduction/0-1-graph-database/#subsequent_events","text":"Cypher has inspired a series of graph query languages, including: 2015, Oracle released PGQL, a graph language used by the graph engine PGX. 2016, the Linked Data Benchmarking Council (short for LDBC) an industry-renowned benchmarking organization for graph performance, released G-CORE. 2018, RedisGraph, a Redis-based graph library, adopted Cypher as its graph language. 2019, the International Standards Organization ISO started two projects to initiate the process of developing an international standard for graph languages based on existing industry achievements such as openCypher, PGQL, GSQL 12 , and G-CORE. 2019, Nebula Graph released Nebula Graph Query Language (nGQL) based on openCypher.","title":"Subsequent events"},{"location":"1.introduction/0-1-graph-database/#distributed_graph_databases","text":"From 2005 to 2010, with the release of Google's cloud computing \"Troika\", various distributed architectures became increasingly popular, including Hadoop and Cassandra, which have been open-sourced. Several implications are as follows: The technical and cost advantages of distributed systems over single machines (e.g. Neo4j) or small machines are more obvious due to the increasing volume of data and computation. Distributed systems allow applications to access these thousands of machines as if they were local systems, without the need for much modification at the code level. The open-source approach allows more people to know emerging technologies and feedback to the community in a more cost-effective way, including code developers, data scientists, and product managers. Strictly speaking, Neo4j also offers several distributed capabilities, which are quite different from the industry's sense of the distributed system. Neo4j 3. x requires that the full amount of data must be stored on a single machine. Although it supports full replication and high availability between multiple machines, the data cannot be sliced into different subgraphs. Neo4j 4. x stores a part of data on different machines (subgraphs), and then the application layer assembles data in a certain way (called Fabric) 13 and distributes the reads and writes to each machine. This approach requires a log of involvement and work from the application layer code. For example, designing how to place different subgraphs on which machines they should be placed and how to assemble some of the results obtained from each machine into the final result. The style of its syntax is as follows: USE graphA MATCH ( movie : Movie ) Return movie . title AS title UNION USE graphB MATCH \uff08 move : Movie ) RETURN movie . title AS title","title":"Distributed graph databases"},{"location":"1.introduction/0-1-graph-database/#the_second_generation_distributed_graph_database_titan_and_its_successor_janusgraph","text":"In 2011, Aurelius was founded to develop an open-source distributed graph database called Titan 14 . By the first official release of Titan in 2015, the backend of Titan can support many major distributed storage architectures (e.g. Cassandra, HBase, Elasticsearch, BerkeleyDB) and can reuse many of the conveniences of the Hadoop ecosystem, with Gremlin as a unified query language on the frontend. It is easy for programmers to use, develop and participate in the community. Large-scale graphs could be sharded and stored on HBase or Cassandra (which were relatively mature distributed storage solutions at the time), and the Gremlin language was relatively full-featured though slightly lengthy. The whole solution was competitive at that time (2011-2015). The following picture shows the growth of Titan and Neo4j stars on Github.com from 2012 to 2015. After Aurelius (Titan) was acquired by DataStax in 2015, Titan was gradually transformed into a closed-source commercial product(DataStax Enterprise Graph). After the acquisition of Aurelius(Titan), there has been a strong demand for an open-source distributed graph database, and there were not many mature and active products in the market. In the era of big data, data is still being generated in a steady stream, far faster than Moore's Law. The Linux Foundation, along with some technology giants (Expero, Google, GRAKN.AI, Hortonworks, IBM, and Amazon) replicated and forked the original Titan project and started it as a new project JanusGraph 15 . Most of the community work including development, testing, release, and promotion, has been gradually shifted to the new JanusGraph\u3002 The following graph shows the evolution of daily code commits (pull requests) for the two projects, and we can see: Although Aurelius(Titan) still has some activity in its open-source code after its acquisition in 2015, the growth rate has slowed down significantly. This reflects the strength of the community. After the new project was started in January 2017, its community became active quickly, surpassing the number of pull requests accumulated by Titan in the past 5 years in just one year. At the same time, the open-source Titan came to a halt.","title":"The second generation (distributed) graph database: Titan and its successor JanusGraph"},{"location":"1.introduction/0-1-graph-database/#famous_products_of_the_same_period_orientdb_tigergraph_arangodb_and_dgraph","text":"In addition to JanusGraph managed by the Linux Foundation, more vendors have been joined the overall market. Some distributed graph databases that were developed by commercial companies use different data models and access methods. The following table only lists the main differences. Vendors Creation time Core product Open source protocol Data model Query language OrientDB LTD (Acquired by SAP in 2017) 2011 OrientDB Open source Document + KV + Graph OrientDB SQL (SQL-based extended graph abilities) GraphSQL (was renamed TigerGraph) 2012 TigerGraph Commercial version Graph (Analysis) GraphSQL (similar to SQL) ArangoDB GmbH 2014 ArangoDB Apache License 2.0 Document + KV + Graph AQL (Simultaneous operation of documents, KVs and graphs) DGraph Labs 2016 DGraph Apache Public License 2.0 + Dgraph Community License Originally RDF, later changed to GraphQL GraphQL+-","title":"Famous products of the same period OrientDB, TigerGraph, ArangoDB, and DGraph"},{"location":"1.introduction/0-1-graph-database/#traditional_giants_microsoft_amazon_and_oracle","text":"In addition to vendors focused on graph products, traditional giants have also entered the graph database field. Microsoft Azure Cosmos DB 16 is a multimodal database cloud service on the Microsoft cloud that provides SQL, document, graph, key-value, and other capabilities. Amazon AWS Neptune 17 is a graph database cloud service provided by AWS support property graphs and RDF two data models. Oracle Graph 18 is a product of the relational database giant Oracle in the direction of graph technology and graph databases.","title":"Traditional giants Microsoft, Amazon, and Oracle"},{"location":"1.introduction/0-1-graph-database/#nebula_graph_a_new_generation_of_open-source_distributed_graph_databases","text":"In the following topics, we will formally introduce Nebula Graph, a new generation of open-source distributed graph databases. https://db-engines.com/en/ranking_categories \u21a9 https://www.yellowfinbi.com/blog/2014/06/yfcommunitynews-big-data-analytics-the-need-for-pragmatism-tangible-benefits-and-real-world-case-165305 \u21a9 https://www.gartner.com/smarterwithgartner/gartner-top-10-data-and-analytics-trends-for-2021/ \u21a9 https://www.verifiedmarketresearch.com/product/graph-database-market/ \u21a9 https://www.globenewswire.com/news-release/2021/01/28/2165742/0/en/Global-Graph-Database-Market-Size-Share-to-Exceed-USD-4-500-Million-By-2026-Facts-Factors.html \u21a9 https://www.marketsandmarkets.com/Market-Reports/graph-database-market-126230231.html \u21a9 https://www.gartner.com/en/newsroom/press-releases/2019-07-01-gartner-says-the-future-of-the-database-market-is-the \u21a9 https://www.amazon.com/Designing-Data-Intensive-Applications-Reliable-Maintainable/dp/1449373321 \u21a9 I. F. Cruz, A. O. Mendelzon, and P. T. Wood. A Graphical Query Language Supporting Recursion. In Proceedings of the Association for Computing Machinery Special Interest Group on Management of Data, pages 323\u2013330. ACM Press, May 1987. \u21a9 \"An overview of the recent history of Graph Query Languages\". Authors: Tobias Lindaaker, U.S. National Expert.Date: 2018-05-14 \u21a9 Gremlin is a graph language developed based on Apache TinkerPop . \u21a9 https://docs.tigergraph.com/dev/gsql-ref \u21a9 https://neo4j.com/fosdem20/ \u21a9 https://github.com/thinkaurelius/titan \u21a9 https://github.com/JanusGraph/janusgraph \u21a9 https://azure.microsoft.com/en-us/free/cosmos-db/ \u21a9 https://aws.amazon.com/cn/neptune/ \u21a9 https://www.oracle.com/database/graph/ \u21a9","title":"Nebula Graph, a new generation of open-source distributed graph databases"},{"location":"1.introduction/0-2.relates/","text":"Related technologies \u00b6 This topic introduces databases and graph-related technologies that are closely related to distributed graph databases. Databases \u00b6 Relational databases \u00b6 A relational database is a database that uses a relational model to organize data. The relational model is a two-dimensional table model, and a relational database consists of two-dimensional tables and the relationships between them. When it comes to relational databases, most people think of MySQL, one of the most popular database management systems that support database operations using the most common structured query language (SQL) and stores data in the form of tables, rows, and columns. This approach to storing data is derived from the relational data model proposed by Edgar Frank Codd in 1970. In a relational database, a table can be created for each type of data to be stored. For example, the player table is used to store all player information, the team table is used to store team information. Each row of data in a SQL table must contain a primary key. The primary key is a unique identifier for the row of data. Generally, the primary key is self-incrementing with the number of rows as the field ID. Relational databases have served the computer industry very well since their inception and will continue to do so for a long time to come. If you have used Excel, WPS, or other similar applications, you have a rough idea of how relational databases work. First, you set up the columns, then you add rows of data under the corresponding columns. You can average or otherwise aggregate the data in a column, similar to averaging in a relational database MySQL. Pivot tables in Excel are the equivalent of querying data in a relational database MySQL using aggregation functions and CASE statements. An Excel file can have multiple tables, and a single table is equivalent to a single table in MySQL. An Excel file is similar to a MySQL database. Relationships in relational databases \u00b6 Unlike graph databases, edges in relational databases (or SQL-type databases) are also stored as entities in specialized edge tables. Two tables are created, player and team, and then player_team is created as an edge table. Edge tables are usually formed by joining related tables. For example, here the edge table player_team is made by joining the player table and the team table. The way of storing edges is not a big problem when associating small data sets, but problems arise when there are too many relationships in a relational database. Specifically, when you want to query just one player's teammates, you have to join all the data in the table and then filter out all the data you don't need, which puts a huge strain on the relational database when your dataset reaches a certain size. If you want to associate multiple different tables, the system may not be able to respond before the join bombs. Origins of relational databases \u00b6 As mentioned above, the relational data model was first proposed by Edgar Frank Codd, an IBM engineer, in 1970. Codd wrote several papers on database management systems that addressed the potential of the relational data model. The relational data model does not rely on linked lists of data (mesh or hierarchical data), but more on data sets. Using the mathematical method of tuple calculus, he argued that these datasets can perform the same tasks as a navigational database. The only requirement was that the relational data model needed a suitable query language to guarantee the consistency requirements of the database. This became the inspiration for declarative query languages such as Structured Query Language (SQL). IBM's System R was one of the first implementations of such a system. But Software Development Laboratories, a small company founded by ex-IBM people and one illustrious Mr.Larry Ellison, beat IBM to the market with the product that would become known as Oracle. Since the relational database was a trendy term at the time, many database vendors preferred to use it in their product names, even though their products were not actually relational. To prevent this and reduce the misuse of the relational data model, Codd introduced the famous Codd's 12 Rules. All relational data systems must follow Codd's 12 Rules. NoSQL databases \u00b6 Graph databases are not the only alternative that can overcome the shortcomings of relational databases. There are many non-relational database products on the market that can be called NoSQL. The term NoSQL was first introduced in the late 1990s and can be interpreted as \"not SQL\" or \"not only SQL\". For the sake of understanding, NoSQL can be interpreted as a \"non-relational database\" here. Unlike relational databases, the data storage and retrieval mechanisms provided by NoSQL databases are not modeled based on table relationships. NoSQL databases can be divided into four categories. Key-value Data Store Columnar Store Document Store Graph Store The following describes the four types of NoSQL databases. Key-value Data Store \u00b6 Key-value databases store data in unique key-value pairs. Unlike relational databases, key-value stores do not have tables and columns. A key-value database itself is like a large table with many columns (i.e., keys). In a key-value store database, data are stored and queried by means of keys, usually implemented as hash lists. This is much simpler than traditional SQL databases, and for some web applications, it is sufficient. The advantage of the key-value model for IT systems is that it is simple and easy to deploy. In most cases, this type of storage works well for unrelated data. If you are just storing data without querying it, there is no problem using this storage method. However, if the DBA only queries or updates some of the values, the key-value model becomes inefficient. Common key-value storage databases include Redis, Voldemort, and Oracle BDB. Columnar Store \u00b6 A NoSQL database's columnar store has many similarities to a NoSQL database's key-value store because the columnar store is still using keys for storage and retrieval. The difference is that in a columnar store database, the column is the smallest storage unit, and each column consists of a key, a value, and a timestamp for version control and conflict resolution. This is particularly useful when scaling in a distributed manner, as timestamps can be used to locate expired data when the database is updated. Because of the good scalability of columnar storage, the columnar store is suitable for very large data sets. Common columnar storage databases include HBase, Cassandra, HadoopDB, etc. Document Store \u00b6 A NoSQL database document store is a key-value-based database, but with enhanced functionality. Data is still stored as keys, but the values in a document store are structured documents, not just a string or a single value. That is, because of the increased information structure, document stores are able to perform more optimized queries and make data retrieval easier. Therefore, document stores are particularly well suited for storing, indexing, and managing document-oriented data or similar semi-structured data. Technically speaking, as a semi-structured unit of information, a document in a document store can be any form of document available, including XML, JSON, YAML, etc., depending on the design of the database vendor. For example, JSON is a common choice. While JSON is not the best choice for structured data, JSON-type data can be used in both front-end and back-end applications. Common document storage databases include MongoDB, CouchDB, Terrastore, etc. Graph Store \u00b6 The last class of NoSQL databases is graph databases. Nebula Graph, is also a graph database. Although graph databases are also NoSQL databases, graph databases are fundamentally different from the above-mentioned NoSQL databases. Graph databases store data in the form of points, edges, and attributes. Its advantages include high flexibility, support for complex graph algorithms, and can be used to build complex relational graphs. We will discuss graph databases in detail in the subsequent topics. But in this topic, you just need to know that a graph database is a NoSQL type of database. Common graph databases include Nebula Graph, Neo4j, OrientDB, etc. Graph-related technologies \u00b6 Take a look at a panoramic view of graph technology in 2020 1 . There are many technologies that are related to graphs, which can be broadly classified into these categories: Infrastructure: including graph databases, graph computing (processing) engines, graph deep learning, cloud services, etc. Applications: including visualization, knowledge graph, anti-fraud, cyber security, social network, etc. Development tools: including graph query languages, modeling tools, development frameworks, and libraries. E-books 2 and conferences, etc. Graph language \u00b6 In the previous topic, we introduced the history of graph languages. In this section, we make a classification of the functions of graph languages. Nearest neighbor query (NNS): Query the neighboring edges, neighbors, or K-hops neighbors. Find one/all subgraphs that satisfy a given graph pattern. This problem is very close to Subgraph Isomorphism - two seemingly different graphs that are actually identical [ ^subiso] as shown below. Reachability (connectivity) problems: The most common reachability problem is the shortest path problem. Such problems are usually described in terms of Regular Path Query - a series of connected groups of vertices forming a path that needs to satisfy some regular expression. Analytic problems: It is related to some convergent operators, such as Average, Count, Max, Vertex Degree. Measures the distance between all two vertices, the degree of interaction between a vertex and other vertices. Graph database and graph processing systems \u00b6 A graph system usually includes a complex data pipeline 4 . From the data source (the left side of the picture below) to the processing output (the right side), multiple data processing steps and systems are used, such as the ETL module, Graph OLTP module, OLAP module, BI, and knowledge graph. Graph databases and graph processing systems have different origins and specialties (and weaknesses). (Online) The graph database is designed for persistent storage management of graphs and efficient subgraph operations. Hard disks and network) are the target operating devices, physical/logical data mapping, data integrity, and (fault) consistency are the main goals. Each request typically involves only a small part of the full graph and can usually be done on a single server. Request latency is usually in milliseconds or seconds, and request concurrency is typically in the thousands or hundreds of thousands. The early Neo4j was one of the origins of the graph database space. (Offline) The graph processing system is for high-volume, parallel, iterative, processing, and analysis of the full graph. Memory and network are the target operating devices. Each request involves all graph vertices and requires all servers to be involved in its completion. The latency of a single request is in the range of minutes to hours (days). The request concurrency is in single digits. Google's Pregel 5 represents the typical origin of graph processing systems. Its point-centric programming abstraction and BSP's operational model constitute a programming paradigm that is a more graph-friendly API abstraction than the previous Hadoop Map-Reduce. 6 Graph sharding methods \u00b6 For large-scale graph data, it is difficult to store it in the memory of a single server, and even just storing the graph structure itself is not enough. By increasing the capacity of a single server, its cost price usually rises exponentially. As the volume of data increases, for example, 100 billion data already exceeds the capacity of all commercially available servers on the market. There is another option is to shard data and place each shard on a different server to increase reliability and performance. For NoSQL systems, such as key-value or document systems, the sharding method is intuitive and natural. Each record and data unit can usually be placed on a different server based on the key or docID. However, the sharding of data structures like graphs is usually less intuitive, because usually, graphs are \"fully connected\" and each vertex can be connected to any other vertex in usually 6 hops. And it has been theoretically proven that the graph sharding problem is NP. When distributing the entire graph data across multiple servers, the cross-server network access latency is 10 times higher than the hardware (memory) access time inside the same server. Therefore, for some depth-first traversal scenarios, a large number of cross-network accesses occur, resulting in extremely high overall latency. 7 Usually, graphs have a clear power-law distribution. A small number of vertices have much denser neighboring edges than the average vertices. While processing these vertices can usually be within the same server, reducing cross-network access, also means that these servers will be far more stressed than the average. The common graph sharding methods are as follows: Biased application-level sharding: The application layer senses and controls which shard each vertex and edge should locate on, which can generally be judged based on the type of points and edges. A set of vertices of the same type is placed on one sharding and another set of vertices of the same type is placed on another sharding. Of course, for high reliability, the sharding itself can also be made multiple copies. When used by the application, the desired vertices and edges are fetched from each shard, and then on the off-application side (or some proxy server-side), the fetched data is assembled into the final result. This is typically represented by the Neo4j 4. x Fabric. Using a distributed cache layer: Add a memory cache layer on the top of the hard disk and cache important portions of the sharding and data and preheat that cache. Adding read-only replicas or views: Add read-only replicas or create a view for some of the graph sharding, and pass the heavier load of read requests through these sharding servers. Performing fine-grained graph sharding: Form multiple small partitions of vertices and edges instead of one large sharding, and then place the more correlated partitions on the same server as much as possible. 8 \u3002 A mixture of these approaches is also used in specific engineering practices. Usually, offline graph processing systems perform some degree of graph preprocessing to improve the locality through an ETL process, while online graph database systems usually choose a periodic data rebalancing process to improve data locality. Technical challenges \u00b6 In the literature 9 , a thorough investigation of graphs and challenges is done, and the following lists the top ten graph technology challenges. Scalability: Loading and upgrading big graphs, performing graph computation and graph traversal, use of triggers and supernodes. Visualization: Customizable layouts, rendering and display big images, and display dynamic and updated display. Query language and programming API: Language expressiveness, standards compatibility, compatibility with existing systems, design of subqueries, and associative queries across multiple graphs. Faster graph algorithms. Easy to use (configuration and usage) Performance metrics and testing General graph technology software (e.g., to handle offline, online, streaming computations.\uff09 ETL Debug and test Open-source graph tools on single machines \u00b6 There is a common misconception about graph databases that any data access involving graph structure needs to be stored in a graph database. When the amount of data is not large, single machine memory is enough to store the data. You can use some single machine open-source tools to store tens of millions of vertices and edges. JGraphT 10 : A well-known open-source Java graph theory library, which implements a considerable number of efficient graph algorithms. igraph 11 : A lightweight and powerful library, supporting R, Python, and C++. NetworkX 12 : The first choice for data scientists doing graph theory analysis. Cytoscape 13 : A powerful visual open-source graph analysis tool. Gephi 14 : A powerful visual open-source graph analysis tool. arrows.app 15 : A simple brain mapping tool for visually generating Cypher statements. Industry databases and benchmarks \u00b6 LDBC \u00b6 LDBC 16 (Linked Data Benchmark Council\uff09is a non-profit organization composed of hardware and software giants such as Oracle, Intel and mainstream graph database vendors such as Neo4j and TigerGraph, which is the benchmark guide developer and test result publisher for graphs and has a high influence in the industry. SNB (Social Network Benchmark\uff09is one of the benchmarks developed by the Linked Data Benchmark Committee (LDBC) for graph databases and is divided into two scenarios: interactive query (Interactive) and business intelligence (BI). Its role is similar to that of TPC-C, TPC-H, and other tests in SQL-type databases, which can help users compare the functions, performance, and capacity of various graph database products. An SNB dataset simulates the relationship between people and posts of a social network, taking into account the distribution properties of the social network, the activity of people, and other social information. The standard data size ranges from 0.1 GB (scale factor 0.1) to 1000 GB (sf 1000). Larger data sets of 10 TB and 100 TB can also be generated. The number of vertices and edges is as shown below. Trends \u00b6 Graph technologies of different origins and goals are learning from and integrating with each other \u00b6 The trends in cloud computing place higher demands on scalability. \u00b6 According to Gartner's projections, cloud services have been growing at a rapid rate and penetration 17 . A large number of commercial software is gradually moving from a completely local and private model 10 years ago to a cloud services-based business model. One of the major advantages of cloud services is that they offer near-infinite scalability. It requires that various cloud infrastructure-based software must have a better ability to scale up and down quickly and elastically. Trends in hardware that SSD will be the mainstream persistent device \u00b6 Hardware determines software architecture. From the 1950s, when Moore's Law was discovered, to the 00s, when multi-core was introduced, hardware trends and speeds have profoundly determined software architecture. Database systems are mostly designed around \"hard disk + memory\", high-performance computing systems are mostly designed around \"memory + CPU\", and distributed systems are designed completely differently for 1 gigabit, 10 gigabits, and RDMA. Graph traversals are featured as random access. Early graph database systems adopted the large memory + HDD architecture. By designing some data structure in memory, random access can be achieved in memory (B+ trees, Hash tables) for the purpose of optimizing graph topology traversal. And then the random access was converted into sequential reads and writes suitable for HDDs. The entire software architecture (including the storage and compute layers) must be based on and built around such IO processes. With the decline in SSD prices 18 , SSDs are replacing HDDs as the dominant device. Friendly random access, deep IO queue, fast access are the features of SSD that differ from HDD's highly repetitive sequence, random latency, and easily damaged disk. The redesign for all software architectures becomes a heavy historical technical burden. https://graphaware.com/graphaware/2020/02/17/graph-technology-landscape-2020.html \u21a9 Electronic copies are available for learning purposes by contacting Author . \u21a9 https://en.wikipedia.org/wiki/Graph_isomorphism \u21a9 The Future is Big Graphs! A Community View on Graph Processing Systems. https://arxiv.org/abs/2012.06171 \u21a9 G. Malewicz, M. H. Austern, A. J. Bik, J. C. Dehnert, I. Horn, N. Leiser, and G. Czajkowski. Pregel: a system for large-scale graph processing. In Proceedings of the International Conference on Management of data (SIGMOD), pages 135\u2013146, New York, NY, USA, 2010. ACM \u21a9 https://neo4j.com/graphacademy/training-iga-40/02-iga-40-overview-of-graph-algorithms/ \u21a9 https://livebook.manning.com/book/graph-powered-machine-learning/welcome/v-8/ \u21a9 https://www.arangodb.com/learn/graphs/using-smartgraphs-arangodb/ \u21a9 https://arxiv.org/abs/1709.03188 \u21a9 https://jgrapht.org/ \u21a9 https://igraph.org/ \u21a9 https://networkx.org/ \u21a9 https://cytoscape.org/ \u21a9 https://gephi.org/ \u21a9 https://arrows.app/ \u21a9 https://github.com/ldbc/ldbc_snb_docs \u21a9 https://cloudcomputing-news.net/news/2019/apr/15/public-cloud-soaring-to-331b-by-2022-according-to-gartner/ \u21a9 https://blocksandfiles.com/2021/01/25/wikibon-ssds-vs-hard-drives-wrights-law/ \u21a9","title":"Related technologies"},{"location":"1.introduction/0-2.relates/#related_technologies","text":"This topic introduces databases and graph-related technologies that are closely related to distributed graph databases.","title":"Related technologies"},{"location":"1.introduction/0-2.relates/#databases","text":"","title":"Databases"},{"location":"1.introduction/0-2.relates/#relational_databases","text":"A relational database is a database that uses a relational model to organize data. The relational model is a two-dimensional table model, and a relational database consists of two-dimensional tables and the relationships between them. When it comes to relational databases, most people think of MySQL, one of the most popular database management systems that support database operations using the most common structured query language (SQL) and stores data in the form of tables, rows, and columns. This approach to storing data is derived from the relational data model proposed by Edgar Frank Codd in 1970. In a relational database, a table can be created for each type of data to be stored. For example, the player table is used to store all player information, the team table is used to store team information. Each row of data in a SQL table must contain a primary key. The primary key is a unique identifier for the row of data. Generally, the primary key is self-incrementing with the number of rows as the field ID. Relational databases have served the computer industry very well since their inception and will continue to do so for a long time to come. If you have used Excel, WPS, or other similar applications, you have a rough idea of how relational databases work. First, you set up the columns, then you add rows of data under the corresponding columns. You can average or otherwise aggregate the data in a column, similar to averaging in a relational database MySQL. Pivot tables in Excel are the equivalent of querying data in a relational database MySQL using aggregation functions and CASE statements. An Excel file can have multiple tables, and a single table is equivalent to a single table in MySQL. An Excel file is similar to a MySQL database.","title":"Relational databases"},{"location":"1.introduction/0-2.relates/#relationships_in_relational_databases","text":"Unlike graph databases, edges in relational databases (or SQL-type databases) are also stored as entities in specialized edge tables. Two tables are created, player and team, and then player_team is created as an edge table. Edge tables are usually formed by joining related tables. For example, here the edge table player_team is made by joining the player table and the team table. The way of storing edges is not a big problem when associating small data sets, but problems arise when there are too many relationships in a relational database. Specifically, when you want to query just one player's teammates, you have to join all the data in the table and then filter out all the data you don't need, which puts a huge strain on the relational database when your dataset reaches a certain size. If you want to associate multiple different tables, the system may not be able to respond before the join bombs.","title":"Relationships in relational databases"},{"location":"1.introduction/0-2.relates/#origins_of_relational_databases","text":"As mentioned above, the relational data model was first proposed by Edgar Frank Codd, an IBM engineer, in 1970. Codd wrote several papers on database management systems that addressed the potential of the relational data model. The relational data model does not rely on linked lists of data (mesh or hierarchical data), but more on data sets. Using the mathematical method of tuple calculus, he argued that these datasets can perform the same tasks as a navigational database. The only requirement was that the relational data model needed a suitable query language to guarantee the consistency requirements of the database. This became the inspiration for declarative query languages such as Structured Query Language (SQL). IBM's System R was one of the first implementations of such a system. But Software Development Laboratories, a small company founded by ex-IBM people and one illustrious Mr.Larry Ellison, beat IBM to the market with the product that would become known as Oracle. Since the relational database was a trendy term at the time, many database vendors preferred to use it in their product names, even though their products were not actually relational. To prevent this and reduce the misuse of the relational data model, Codd introduced the famous Codd's 12 Rules. All relational data systems must follow Codd's 12 Rules.","title":"Origins of relational databases"},{"location":"1.introduction/0-2.relates/#nosql_databases","text":"Graph databases are not the only alternative that can overcome the shortcomings of relational databases. There are many non-relational database products on the market that can be called NoSQL. The term NoSQL was first introduced in the late 1990s and can be interpreted as \"not SQL\" or \"not only SQL\". For the sake of understanding, NoSQL can be interpreted as a \"non-relational database\" here. Unlike relational databases, the data storage and retrieval mechanisms provided by NoSQL databases are not modeled based on table relationships. NoSQL databases can be divided into four categories. Key-value Data Store Columnar Store Document Store Graph Store The following describes the four types of NoSQL databases.","title":"NoSQL databases"},{"location":"1.introduction/0-2.relates/#key-value_data_store","text":"Key-value databases store data in unique key-value pairs. Unlike relational databases, key-value stores do not have tables and columns. A key-value database itself is like a large table with many columns (i.e., keys). In a key-value store database, data are stored and queried by means of keys, usually implemented as hash lists. This is much simpler than traditional SQL databases, and for some web applications, it is sufficient. The advantage of the key-value model for IT systems is that it is simple and easy to deploy. In most cases, this type of storage works well for unrelated data. If you are just storing data without querying it, there is no problem using this storage method. However, if the DBA only queries or updates some of the values, the key-value model becomes inefficient. Common key-value storage databases include Redis, Voldemort, and Oracle BDB.","title":"Key-value Data Store"},{"location":"1.introduction/0-2.relates/#columnar_store","text":"A NoSQL database's columnar store has many similarities to a NoSQL database's key-value store because the columnar store is still using keys for storage and retrieval. The difference is that in a columnar store database, the column is the smallest storage unit, and each column consists of a key, a value, and a timestamp for version control and conflict resolution. This is particularly useful when scaling in a distributed manner, as timestamps can be used to locate expired data when the database is updated. Because of the good scalability of columnar storage, the columnar store is suitable for very large data sets. Common columnar storage databases include HBase, Cassandra, HadoopDB, etc.","title":"Columnar Store"},{"location":"1.introduction/0-2.relates/#document_store","text":"A NoSQL database document store is a key-value-based database, but with enhanced functionality. Data is still stored as keys, but the values in a document store are structured documents, not just a string or a single value. That is, because of the increased information structure, document stores are able to perform more optimized queries and make data retrieval easier. Therefore, document stores are particularly well suited for storing, indexing, and managing document-oriented data or similar semi-structured data. Technically speaking, as a semi-structured unit of information, a document in a document store can be any form of document available, including XML, JSON, YAML, etc., depending on the design of the database vendor. For example, JSON is a common choice. While JSON is not the best choice for structured data, JSON-type data can be used in both front-end and back-end applications. Common document storage databases include MongoDB, CouchDB, Terrastore, etc.","title":"Document Store"},{"location":"1.introduction/0-2.relates/#graph_store","text":"The last class of NoSQL databases is graph databases. Nebula Graph, is also a graph database. Although graph databases are also NoSQL databases, graph databases are fundamentally different from the above-mentioned NoSQL databases. Graph databases store data in the form of points, edges, and attributes. Its advantages include high flexibility, support for complex graph algorithms, and can be used to build complex relational graphs. We will discuss graph databases in detail in the subsequent topics. But in this topic, you just need to know that a graph database is a NoSQL type of database. Common graph databases include Nebula Graph, Neo4j, OrientDB, etc.","title":"Graph Store"},{"location":"1.introduction/0-2.relates/#graph-related_technologies","text":"Take a look at a panoramic view of graph technology in 2020 1 . There are many technologies that are related to graphs, which can be broadly classified into these categories: Infrastructure: including graph databases, graph computing (processing) engines, graph deep learning, cloud services, etc. Applications: including visualization, knowledge graph, anti-fraud, cyber security, social network, etc. Development tools: including graph query languages, modeling tools, development frameworks, and libraries. E-books 2 and conferences, etc.","title":"Graph-related technologies"},{"location":"1.introduction/0-2.relates/#graph_language","text":"In the previous topic, we introduced the history of graph languages. In this section, we make a classification of the functions of graph languages. Nearest neighbor query (NNS): Query the neighboring edges, neighbors, or K-hops neighbors. Find one/all subgraphs that satisfy a given graph pattern. This problem is very close to Subgraph Isomorphism - two seemingly different graphs that are actually identical [ ^subiso] as shown below. Reachability (connectivity) problems: The most common reachability problem is the shortest path problem. Such problems are usually described in terms of Regular Path Query - a series of connected groups of vertices forming a path that needs to satisfy some regular expression. Analytic problems: It is related to some convergent operators, such as Average, Count, Max, Vertex Degree. Measures the distance between all two vertices, the degree of interaction between a vertex and other vertices.","title":"Graph language"},{"location":"1.introduction/0-2.relates/#graph_database_and_graph_processing_systems","text":"A graph system usually includes a complex data pipeline 4 . From the data source (the left side of the picture below) to the processing output (the right side), multiple data processing steps and systems are used, such as the ETL module, Graph OLTP module, OLAP module, BI, and knowledge graph. Graph databases and graph processing systems have different origins and specialties (and weaknesses). (Online) The graph database is designed for persistent storage management of graphs and efficient subgraph operations. Hard disks and network) are the target operating devices, physical/logical data mapping, data integrity, and (fault) consistency are the main goals. Each request typically involves only a small part of the full graph and can usually be done on a single server. Request latency is usually in milliseconds or seconds, and request concurrency is typically in the thousands or hundreds of thousands. The early Neo4j was one of the origins of the graph database space. (Offline) The graph processing system is for high-volume, parallel, iterative, processing, and analysis of the full graph. Memory and network are the target operating devices. Each request involves all graph vertices and requires all servers to be involved in its completion. The latency of a single request is in the range of minutes to hours (days). The request concurrency is in single digits. Google's Pregel 5 represents the typical origin of graph processing systems. Its point-centric programming abstraction and BSP's operational model constitute a programming paradigm that is a more graph-friendly API abstraction than the previous Hadoop Map-Reduce. 6","title":"Graph database and graph processing systems"},{"location":"1.introduction/0-2.relates/#graph_sharding_methods","text":"For large-scale graph data, it is difficult to store it in the memory of a single server, and even just storing the graph structure itself is not enough. By increasing the capacity of a single server, its cost price usually rises exponentially. As the volume of data increases, for example, 100 billion data already exceeds the capacity of all commercially available servers on the market. There is another option is to shard data and place each shard on a different server to increase reliability and performance. For NoSQL systems, such as key-value or document systems, the sharding method is intuitive and natural. Each record and data unit can usually be placed on a different server based on the key or docID. However, the sharding of data structures like graphs is usually less intuitive, because usually, graphs are \"fully connected\" and each vertex can be connected to any other vertex in usually 6 hops. And it has been theoretically proven that the graph sharding problem is NP. When distributing the entire graph data across multiple servers, the cross-server network access latency is 10 times higher than the hardware (memory) access time inside the same server. Therefore, for some depth-first traversal scenarios, a large number of cross-network accesses occur, resulting in extremely high overall latency. 7 Usually, graphs have a clear power-law distribution. A small number of vertices have much denser neighboring edges than the average vertices. While processing these vertices can usually be within the same server, reducing cross-network access, also means that these servers will be far more stressed than the average. The common graph sharding methods are as follows: Biased application-level sharding: The application layer senses and controls which shard each vertex and edge should locate on, which can generally be judged based on the type of points and edges. A set of vertices of the same type is placed on one sharding and another set of vertices of the same type is placed on another sharding. Of course, for high reliability, the sharding itself can also be made multiple copies. When used by the application, the desired vertices and edges are fetched from each shard, and then on the off-application side (or some proxy server-side), the fetched data is assembled into the final result. This is typically represented by the Neo4j 4. x Fabric. Using a distributed cache layer: Add a memory cache layer on the top of the hard disk and cache important portions of the sharding and data and preheat that cache. Adding read-only replicas or views: Add read-only replicas or create a view for some of the graph sharding, and pass the heavier load of read requests through these sharding servers. Performing fine-grained graph sharding: Form multiple small partitions of vertices and edges instead of one large sharding, and then place the more correlated partitions on the same server as much as possible. 8 \u3002 A mixture of these approaches is also used in specific engineering practices. Usually, offline graph processing systems perform some degree of graph preprocessing to improve the locality through an ETL process, while online graph database systems usually choose a periodic data rebalancing process to improve data locality.","title":"Graph sharding methods"},{"location":"1.introduction/0-2.relates/#technical_challenges","text":"In the literature 9 , a thorough investigation of graphs and challenges is done, and the following lists the top ten graph technology challenges. Scalability: Loading and upgrading big graphs, performing graph computation and graph traversal, use of triggers and supernodes. Visualization: Customizable layouts, rendering and display big images, and display dynamic and updated display. Query language and programming API: Language expressiveness, standards compatibility, compatibility with existing systems, design of subqueries, and associative queries across multiple graphs. Faster graph algorithms. Easy to use (configuration and usage) Performance metrics and testing General graph technology software (e.g., to handle offline, online, streaming computations.\uff09 ETL Debug and test","title":"Technical challenges"},{"location":"1.introduction/0-2.relates/#open-source_graph_tools_on_single_machines","text":"There is a common misconception about graph databases that any data access involving graph structure needs to be stored in a graph database. When the amount of data is not large, single machine memory is enough to store the data. You can use some single machine open-source tools to store tens of millions of vertices and edges. JGraphT 10 : A well-known open-source Java graph theory library, which implements a considerable number of efficient graph algorithms. igraph 11 : A lightweight and powerful library, supporting R, Python, and C++. NetworkX 12 : The first choice for data scientists doing graph theory analysis. Cytoscape 13 : A powerful visual open-source graph analysis tool. Gephi 14 : A powerful visual open-source graph analysis tool. arrows.app 15 : A simple brain mapping tool for visually generating Cypher statements.","title":"Open-source graph tools on single machines"},{"location":"1.introduction/0-2.relates/#industry_databases_and_benchmarks","text":"","title":"Industry databases and benchmarks"},{"location":"1.introduction/0-2.relates/#ldbc","text":"LDBC 16 (Linked Data Benchmark Council\uff09is a non-profit organization composed of hardware and software giants such as Oracle, Intel and mainstream graph database vendors such as Neo4j and TigerGraph, which is the benchmark guide developer and test result publisher for graphs and has a high influence in the industry. SNB (Social Network Benchmark\uff09is one of the benchmarks developed by the Linked Data Benchmark Committee (LDBC) for graph databases and is divided into two scenarios: interactive query (Interactive) and business intelligence (BI). Its role is similar to that of TPC-C, TPC-H, and other tests in SQL-type databases, which can help users compare the functions, performance, and capacity of various graph database products. An SNB dataset simulates the relationship between people and posts of a social network, taking into account the distribution properties of the social network, the activity of people, and other social information. The standard data size ranges from 0.1 GB (scale factor 0.1) to 1000 GB (sf 1000). Larger data sets of 10 TB and 100 TB can also be generated. The number of vertices and edges is as shown below.","title":"LDBC"},{"location":"1.introduction/0-2.relates/#trends","text":"","title":"Trends"},{"location":"1.introduction/0-2.relates/#graph_technologies_of_different_origins_and_goals_are_learning_from_and_integrating_with_each_other","text":"","title":"Graph technologies of different origins and goals are learning from and integrating with each other"},{"location":"1.introduction/0-2.relates/#the_trends_in_cloud_computing_place_higher_demands_on_scalability","text":"According to Gartner's projections, cloud services have been growing at a rapid rate and penetration 17 . A large number of commercial software is gradually moving from a completely local and private model 10 years ago to a cloud services-based business model. One of the major advantages of cloud services is that they offer near-infinite scalability. It requires that various cloud infrastructure-based software must have a better ability to scale up and down quickly and elastically.","title":"The trends in cloud computing place higher demands on scalability."},{"location":"1.introduction/0-2.relates/#trends_in_hardware_that_ssd_will_be_the_mainstream_persistent_device","text":"Hardware determines software architecture. From the 1950s, when Moore's Law was discovered, to the 00s, when multi-core was introduced, hardware trends and speeds have profoundly determined software architecture. Database systems are mostly designed around \"hard disk + memory\", high-performance computing systems are mostly designed around \"memory + CPU\", and distributed systems are designed completely differently for 1 gigabit, 10 gigabits, and RDMA. Graph traversals are featured as random access. Early graph database systems adopted the large memory + HDD architecture. By designing some data structure in memory, random access can be achieved in memory (B+ trees, Hash tables) for the purpose of optimizing graph topology traversal. And then the random access was converted into sequential reads and writes suitable for HDDs. The entire software architecture (including the storage and compute layers) must be based on and built around such IO processes. With the decline in SSD prices 18 , SSDs are replacing HDDs as the dominant device. Friendly random access, deep IO queue, fast access are the features of SSD that differ from HDD's highly repetitive sequence, random latency, and easily damaged disk. The redesign for all software architectures becomes a heavy historical technical burden. https://graphaware.com/graphaware/2020/02/17/graph-technology-landscape-2020.html \u21a9 Electronic copies are available for learning purposes by contacting Author . \u21a9 https://en.wikipedia.org/wiki/Graph_isomorphism \u21a9 The Future is Big Graphs! A Community View on Graph Processing Systems. https://arxiv.org/abs/2012.06171 \u21a9 G. Malewicz, M. H. Austern, A. J. Bik, J. C. Dehnert, I. Horn, N. Leiser, and G. Czajkowski. Pregel: a system for large-scale graph processing. In Proceedings of the International Conference on Management of data (SIGMOD), pages 135\u2013146, New York, NY, USA, 2010. ACM \u21a9 https://neo4j.com/graphacademy/training-iga-40/02-iga-40-overview-of-graph-algorithms/ \u21a9 https://livebook.manning.com/book/graph-powered-machine-learning/welcome/v-8/ \u21a9 https://www.arangodb.com/learn/graphs/using-smartgraphs-arangodb/ \u21a9 https://arxiv.org/abs/1709.03188 \u21a9 https://jgrapht.org/ \u21a9 https://igraph.org/ \u21a9 https://networkx.org/ \u21a9 https://cytoscape.org/ \u21a9 https://gephi.org/ \u21a9 https://arrows.app/ \u21a9 https://github.com/ldbc/ldbc_snb_docs \u21a9 https://cloudcomputing-news.net/news/2019/apr/15/public-cloud-soaring-to-331b-by-2022-according-to-gartner/ \u21a9 https://blocksandfiles.com/2021/01/25/wikibon-ssds-vs-hard-drives-wrights-law/ \u21a9","title":"Trends in hardware that SSD will be the mainstream persistent device"},{"location":"1.introduction/1.what-is-nebula-graph/","text":"What is Nebula Graph \u00b6 Nebula Graph is an open-source, distributed, easily scalable, and native graph database. It is capable of hosting graphs with hundreds of billions of vertices and trillions of edges, and serving queries with millisecond-latency. What is a graph database \u00b6 A graph database, such as Nebula Graph, is a database that specializes in storing vast graph networks and retrieving information from them. It efficiently stores data as vertices (nodes) and edges (relationships) in labeled property graphs. Properties can be attached to both vertices and edges. Each vertex can have one or multiple tags (labels). Graph databases are well suited for storing most kinds of data models abstracted from reality. Things are connected in almost all fields in the world. Modeling systems like relational databases extract the relationships between entities and squeeze them into table columns alone, with their types and properties stored in other columns or even other tables. This makes data management time-consuming and cost-ineffective. Nebula Graph, as a typical native graph database, allows you to store the rich relationships as edges with edge types and properties directly attached to them. Advantages of Nebula Graph \u00b6 Open source \u00b6 Nebula Graph is open under the Apache 2.0 License. More and more people such as database developers, data scientists, security experts, and algorithm engineers are participating in the designing and development of Nebula Graph. To join the opening of source code and ideas, surf the Nebula Graph GitHub page . Outstanding performance \u00b6 Written in C++ and born for graphs, Nebula Graph handles graph queries in milliseconds. Among most databases, Nebula Graph shows superior performance in providing graph data services. The larger the data size, the greater the superiority of Nebula Graph. For more information, see Nebula Graph benchmarking . High scalability \u00b6 Nebula Graph is designed in a shared-nothing architecture and supports scaling in and out without interrupting the database service. Developer friendly \u00b6 Nebula Graph supports clients in popular programming languages like Java, Python, C++, and Go, and more are under development. For more information, see Nebula Graph clients . Reliable access control \u00b6 Nebula Graph supports strict role-based access control and external authentication servers such as LDAP (Lightweight Directory Access Protocol) servers to enhance data security. For more information, see Authentication and authorization . Diversified ecosystem \u00b6 More and more native tools of Nebula Graph have been released, such as Nebula Graph Studio , Nebula Console , and Nebula Exchange . For more ecosystem tools, see Ecosystem tools overview . Besides, Nebula Graph has the ability to be integrated with many cutting-edge technologies, such as Spark, Flink, and HBase, for the purpose of mutual strengthening in a world of increasing challenges and chances. OpenCypher-compatible query language \u00b6 The native Nebula Graph Query Language, also known as nGQL, is a declarative, openCypher-compatible textual query language. It is easy to understand and easy to use. For more information, see nGQL guide . Future-oriented hardware with balanced reading and writing \u00b6 Solid-state drives have extremely high performance and they are getting cheaper . Nebula Graph is a product based on SSD. Compared with products based on HDD and large memory, it is more suitable for future hardware trends and easier to achieve balanced reading and writing. Easy data modeling and high flexibility \u00b6 You can easily model the connected data into Nebula Graph for your business without forcing them into a structure such as a relational table, and properties can be added, updated, and deleted freely. For more information, see Data modeling . High popularity \u00b6 Nebula Graph is being used by tech leaders such as Tencent, Vivo, Meituan, and JD Digits. For more information, visit the Nebula Graph official website . Use cases \u00b6 Nebula Graph can be used to support various graph-based scenarios. To spare the time spent on pushing the kinds of data mentioned in this section into relational databases and on bothering with join queries, use Nebula Graph. Fraud detection \u00b6 Financial institutions have to traverse countless transactions to piece together potential crimes and understand how combinations of transactions and devices might be related to a single fraud scheme. This kind of scenario can be modeled in graphs, and with the help of Nebula Graph, fraud rings and other sophisticated scams can be easily detected. Real-time recommendation \u00b6 Nebula Graph offers the ability to instantly process the real-time information produced by a visitor and make accurate recommendations on articles, videos, products, and services. Intelligent question-answer system \u00b6 Natural languages can be transformed into knowledge graphs and stored in Nebula Graph. A question organized in a natural language can be resolved by a semantic parser in an intelligent question-answer system and re-organized. Then, possible answers to the question can be retrieved from the knowledge graph and provided to the one who asked the question. Social networking \u00b6 Information on people and their relationships is typical graph data. Nebula Graph can easily handle the social networking information of billions of people and trillions of relationships, and provide lightning-fast queries for friend recommendations and job promotions in the case of massive concurrency. Related links \u00b6 Official website Docs Blogs Forum GitHub","title":"What is Nebula Graph"},{"location":"1.introduction/1.what-is-nebula-graph/#what_is_nebula_graph","text":"Nebula Graph is an open-source, distributed, easily scalable, and native graph database. It is capable of hosting graphs with hundreds of billions of vertices and trillions of edges, and serving queries with millisecond-latency.","title":"What is Nebula Graph"},{"location":"1.introduction/1.what-is-nebula-graph/#what_is_a_graph_database","text":"A graph database, such as Nebula Graph, is a database that specializes in storing vast graph networks and retrieving information from them. It efficiently stores data as vertices (nodes) and edges (relationships) in labeled property graphs. Properties can be attached to both vertices and edges. Each vertex can have one or multiple tags (labels). Graph databases are well suited for storing most kinds of data models abstracted from reality. Things are connected in almost all fields in the world. Modeling systems like relational databases extract the relationships between entities and squeeze them into table columns alone, with their types and properties stored in other columns or even other tables. This makes data management time-consuming and cost-ineffective. Nebula Graph, as a typical native graph database, allows you to store the rich relationships as edges with edge types and properties directly attached to them.","title":"What is a graph database"},{"location":"1.introduction/1.what-is-nebula-graph/#advantages_of_nebula_graph","text":"","title":"Advantages of Nebula Graph"},{"location":"1.introduction/1.what-is-nebula-graph/#open_source","text":"Nebula Graph is open under the Apache 2.0 License. More and more people such as database developers, data scientists, security experts, and algorithm engineers are participating in the designing and development of Nebula Graph. To join the opening of source code and ideas, surf the Nebula Graph GitHub page .","title":"Open source"},{"location":"1.introduction/1.what-is-nebula-graph/#outstanding_performance","text":"Written in C++ and born for graphs, Nebula Graph handles graph queries in milliseconds. Among most databases, Nebula Graph shows superior performance in providing graph data services. The larger the data size, the greater the superiority of Nebula Graph. For more information, see Nebula Graph benchmarking .","title":"Outstanding performance"},{"location":"1.introduction/1.what-is-nebula-graph/#high_scalability","text":"Nebula Graph is designed in a shared-nothing architecture and supports scaling in and out without interrupting the database service.","title":"High scalability"},{"location":"1.introduction/1.what-is-nebula-graph/#developer_friendly","text":"Nebula Graph supports clients in popular programming languages like Java, Python, C++, and Go, and more are under development. For more information, see Nebula Graph clients .","title":"Developer friendly"},{"location":"1.introduction/1.what-is-nebula-graph/#reliable_access_control","text":"Nebula Graph supports strict role-based access control and external authentication servers such as LDAP (Lightweight Directory Access Protocol) servers to enhance data security. For more information, see Authentication and authorization .","title":"Reliable access control"},{"location":"1.introduction/1.what-is-nebula-graph/#diversified_ecosystem","text":"More and more native tools of Nebula Graph have been released, such as Nebula Graph Studio , Nebula Console , and Nebula Exchange . For more ecosystem tools, see Ecosystem tools overview . Besides, Nebula Graph has the ability to be integrated with many cutting-edge technologies, such as Spark, Flink, and HBase, for the purpose of mutual strengthening in a world of increasing challenges and chances.","title":"Diversified ecosystem"},{"location":"1.introduction/1.what-is-nebula-graph/#opencypher-compatible_query_language","text":"The native Nebula Graph Query Language, also known as nGQL, is a declarative, openCypher-compatible textual query language. It is easy to understand and easy to use. For more information, see nGQL guide .","title":"OpenCypher-compatible query language"},{"location":"1.introduction/1.what-is-nebula-graph/#future-oriented_hardware_with_balanced_reading_and_writing","text":"Solid-state drives have extremely high performance and they are getting cheaper . Nebula Graph is a product based on SSD. Compared with products based on HDD and large memory, it is more suitable for future hardware trends and easier to achieve balanced reading and writing.","title":"Future-oriented hardware with balanced reading and writing"},{"location":"1.introduction/1.what-is-nebula-graph/#easy_data_modeling_and_high_flexibility","text":"You can easily model the connected data into Nebula Graph for your business without forcing them into a structure such as a relational table, and properties can be added, updated, and deleted freely. For more information, see Data modeling .","title":"Easy data modeling and high flexibility"},{"location":"1.introduction/1.what-is-nebula-graph/#high_popularity","text":"Nebula Graph is being used by tech leaders such as Tencent, Vivo, Meituan, and JD Digits. For more information, visit the Nebula Graph official website .","title":"High popularity"},{"location":"1.introduction/1.what-is-nebula-graph/#use_cases","text":"Nebula Graph can be used to support various graph-based scenarios. To spare the time spent on pushing the kinds of data mentioned in this section into relational databases and on bothering with join queries, use Nebula Graph.","title":"Use cases"},{"location":"1.introduction/1.what-is-nebula-graph/#fraud_detection","text":"Financial institutions have to traverse countless transactions to piece together potential crimes and understand how combinations of transactions and devices might be related to a single fraud scheme. This kind of scenario can be modeled in graphs, and with the help of Nebula Graph, fraud rings and other sophisticated scams can be easily detected.","title":"Fraud detection"},{"location":"1.introduction/1.what-is-nebula-graph/#real-time_recommendation","text":"Nebula Graph offers the ability to instantly process the real-time information produced by a visitor and make accurate recommendations on articles, videos, products, and services.","title":"Real-time recommendation"},{"location":"1.introduction/1.what-is-nebula-graph/#intelligent_question-answer_system","text":"Natural languages can be transformed into knowledge graphs and stored in Nebula Graph. A question organized in a natural language can be resolved by a semantic parser in an intelligent question-answer system and re-organized. Then, possible answers to the question can be retrieved from the knowledge graph and provided to the one who asked the question.","title":"Intelligent question-answer system"},{"location":"1.introduction/1.what-is-nebula-graph/#social_networking","text":"Information on people and their relationships is typical graph data. Nebula Graph can easily handle the social networking information of billions of people and trillions of relationships, and provide lightning-fast queries for friend recommendations and job promotions in the case of massive concurrency.","title":"Social networking"},{"location":"1.introduction/1.what-is-nebula-graph/#related_links","text":"Official website Docs Blogs Forum GitHub","title":"Related links"},{"location":"1.introduction/2.1.path/","text":"Path types \u00b6 In graph theory, a path in a graph is a finite or infinite sequence of edges which joins a sequence of vertices. Paths are fundamental concepts of graph theory. Paths can be categorized into 3 types: walk , trail , and path . For more information, see Wikipedia . The following figure is an example for a brief introduction. Walk \u00b6 A walk is a finite or infinite sequence of edges. Both vertices and edges can be repeatedly visited in graph traversal. In the above figure C, D, and E form a cycle. So, this figure contains infinite paths, such as A->B->C->D->E , A->B->C->D->E->C , and A->B->C->D->E->C->D . Note GO statements use walk . Trail \u00b6 A trail is a finite sequence of edges. Only vertices can be repeatedly visited in graph traversal. The Seven Bridges of K\u00f6nigsberg is a typical trail . In the above figure, edges cannot be repeatedly visited. So, this figure contains finite paths. The longest path in this figure consists of 5 edges: A->B->C->D->E->C . Note MATCH , FIND PATH , and GET SUBGRAPH statements use trail . There are two special cases of trail, cycle and circuit . The following figure is an example for a brief introduction. cycle A cycle refers to a closed trail . Only the terminal vertices can be repeatedly visited. The longest path in this figure consists of 3 edges: A->B->C->A or C->D->E->C . circuit A circuit refers to a closed trail . Edges cannot be repeatedly visited in graph traversal. Apart from the terminal vertices, other vertices can also be repeatedly visited. The longest path in this figure: A->B->C->D->E->C->A . Path \u00b6 A path is a finite sequence of edges. Neither vertices nor edges can be repeatedly visited in graph traversal. So, the above figure contains finite paths. The longest path in this figure consists of 4 edges: A->B->C->D->E .","title":"Path"},{"location":"1.introduction/2.1.path/#path_types","text":"In graph theory, a path in a graph is a finite or infinite sequence of edges which joins a sequence of vertices. Paths are fundamental concepts of graph theory. Paths can be categorized into 3 types: walk , trail , and path . For more information, see Wikipedia . The following figure is an example for a brief introduction.","title":"Path types"},{"location":"1.introduction/2.1.path/#walk","text":"A walk is a finite or infinite sequence of edges. Both vertices and edges can be repeatedly visited in graph traversal. In the above figure C, D, and E form a cycle. So, this figure contains infinite paths, such as A->B->C->D->E , A->B->C->D->E->C , and A->B->C->D->E->C->D . Note GO statements use walk .","title":"Walk"},{"location":"1.introduction/2.1.path/#trail","text":"A trail is a finite sequence of edges. Only vertices can be repeatedly visited in graph traversal. The Seven Bridges of K\u00f6nigsberg is a typical trail . In the above figure, edges cannot be repeatedly visited. So, this figure contains finite paths. The longest path in this figure consists of 5 edges: A->B->C->D->E->C . Note MATCH , FIND PATH , and GET SUBGRAPH statements use trail . There are two special cases of trail, cycle and circuit . The following figure is an example for a brief introduction. cycle A cycle refers to a closed trail . Only the terminal vertices can be repeatedly visited. The longest path in this figure consists of 3 edges: A->B->C->A or C->D->E->C . circuit A circuit refers to a closed trail . Edges cannot be repeatedly visited in graph traversal. Apart from the terminal vertices, other vertices can also be repeatedly visited. The longest path in this figure: A->B->C->D->E->C->A .","title":"Trail"},{"location":"1.introduction/2.1.path/#path","text":"A path is a finite sequence of edges. Neither vertices nor edges can be repeatedly visited in graph traversal. So, the above figure contains finite paths. The longest path in this figure consists of 4 edges: A->B->C->D->E .","title":"Path"},{"location":"1.introduction/2.data-model/","text":"Data modeling \u00b6 A data model is a model that organizes data and specifies how they are related to one another. This topic describes the Nebula Graph data model and provides suggestions for data modeling with Nebula Graph. Data structures \u00b6 Nebula Graph data model uses six data structures to store data. They are graph spaces, vertices, edges, tags, edge types and properties. Graph spaces : Graph spaces are used to isolate data from different teams or programs. Data stored in different graph spaces are securely isolated. Storage replications, privileges, and partitions can be assigned. Vertices : Vertices are used to store entities. In Nebula Graph, vertices are identified with vertex identifiers (i.e. VID ). The VID must be unique in the same graph space. VID should be int64, or fixed_string(N). A vertex has zero to multiple tags. Compatibility In Nebula Graph 2.x a vertex must have at least one tag. And in Nebula Graph 3.1.0, a tag is not required for a vertex. Edges : Edges are used to connect vertices. An edge is a connection or behavior between two vertices. There can be multiple edges between two vertices. Edges are directed. -> identifies the directions of edges. Edges can be traversed in either direction. An edge is identified uniquely with <a source vertex, an edge type, a rank value, and a destination vertex> . Edges have no EID. An edge must have one and only one edge type. The rank value is an immutable user-assigned 64-bit signed integer. It identifies the edges with the same edge type between two vertices. Edges are sorted by their rank values. The edge with the greatest rank value is listed first. The default rank value is zero. Tags : Tags are used to categorize vertices. Vertices that have the same tag share the same definition of properties. Edge types : Edge types are used to categorize edges. Edges that have the same edge type share the same definition of properties. Properties : Properties are key-value pairs. Both vertices and edges are containers for properties. Note Tags and Edge types are similar to \"vertex tables\" and \"edge tables\" in the relational databases. Directed property graph \u00b6 Nebula Graph stores data in directed property graphs. A directed property graph has a set of vertices connected by directed edges. Both vertices and edges can have properties. A directed property graph is represented as: G = < V, E, P V , P E > V is a set of vertices. E is a set of directed edges. P V is the property of vertices. P E is the property of edges. The following table is an example of the structure of the basketball player dataset. We have two types of vertices, that is player and team , and two types of edges, that is serve and follow . Element Name Property name (Data type) Description Tag player name (string) age (int) Represents players in the team. Tag team name (string) Represents the teams. Edge type serve start_year (int) end_year (int) Represents actions taken by players in the team. An action links a player with a team, and the direction is from a player to a team. Edge type follow degree (int) Represents actions taken by players in the team. An action links a player with another player, and the direction is from one player to the other player. Note Nebula Graph supports only directed edges. Compatibility Nebula Graph 3.1.0 allows dangling edges. Therefore, when adding or deleting, you need to ensure the corresponding source vertex and destination vertex of an edge exist. For details, see INSERT VERTEX , DELETE VERTEX , INSERT EDGE , and DELETE EDGE . The MERGE statement in openCypher is not supported.","title":"Data model"},{"location":"1.introduction/2.data-model/#data_modeling","text":"A data model is a model that organizes data and specifies how they are related to one another. This topic describes the Nebula Graph data model and provides suggestions for data modeling with Nebula Graph.","title":"Data modeling"},{"location":"1.introduction/2.data-model/#data_structures","text":"Nebula Graph data model uses six data structures to store data. They are graph spaces, vertices, edges, tags, edge types and properties. Graph spaces : Graph spaces are used to isolate data from different teams or programs. Data stored in different graph spaces are securely isolated. Storage replications, privileges, and partitions can be assigned. Vertices : Vertices are used to store entities. In Nebula Graph, vertices are identified with vertex identifiers (i.e. VID ). The VID must be unique in the same graph space. VID should be int64, or fixed_string(N). A vertex has zero to multiple tags. Compatibility In Nebula Graph 2.x a vertex must have at least one tag. And in Nebula Graph 3.1.0, a tag is not required for a vertex. Edges : Edges are used to connect vertices. An edge is a connection or behavior between two vertices. There can be multiple edges between two vertices. Edges are directed. -> identifies the directions of edges. Edges can be traversed in either direction. An edge is identified uniquely with <a source vertex, an edge type, a rank value, and a destination vertex> . Edges have no EID. An edge must have one and only one edge type. The rank value is an immutable user-assigned 64-bit signed integer. It identifies the edges with the same edge type between two vertices. Edges are sorted by their rank values. The edge with the greatest rank value is listed first. The default rank value is zero. Tags : Tags are used to categorize vertices. Vertices that have the same tag share the same definition of properties. Edge types : Edge types are used to categorize edges. Edges that have the same edge type share the same definition of properties. Properties : Properties are key-value pairs. Both vertices and edges are containers for properties. Note Tags and Edge types are similar to \"vertex tables\" and \"edge tables\" in the relational databases.","title":"Data structures"},{"location":"1.introduction/2.data-model/#directed_property_graph","text":"Nebula Graph stores data in directed property graphs. A directed property graph has a set of vertices connected by directed edges. Both vertices and edges can have properties. A directed property graph is represented as: G = < V, E, P V , P E > V is a set of vertices. E is a set of directed edges. P V is the property of vertices. P E is the property of edges. The following table is an example of the structure of the basketball player dataset. We have two types of vertices, that is player and team , and two types of edges, that is serve and follow . Element Name Property name (Data type) Description Tag player name (string) age (int) Represents players in the team. Tag team name (string) Represents the teams. Edge type serve start_year (int) end_year (int) Represents actions taken by players in the team. An action links a player with a team, and the direction is from a player to a team. Edge type follow degree (int) Represents actions taken by players in the team. An action links a player with another player, and the direction is from one player to the other player. Note Nebula Graph supports only directed edges. Compatibility Nebula Graph 3.1.0 allows dangling edges. Therefore, when adding or deleting, you need to ensure the corresponding source vertex and destination vertex of an edge exist. For details, see INSERT VERTEX , DELETE VERTEX , INSERT EDGE , and DELETE EDGE . The MERGE statement in openCypher is not supported.","title":"Directed property graph"},{"location":"1.introduction/3.vid/","text":"VID \u00b6 In Nebula Graph, a vertex is uniquely identified by its ID, which is called a VID or a Vertex ID. Features \u00b6 The data types of VIDs are restricted to FIXED_STRING(<N>) or INT64 . One graph space can only select one VID type. A VID in a graph space is unique. It functions just as a primary key in a relational database. VIDs in different graph spaces are independent. The VID generation method must be set by users, because Nebula Graph does not provide auto increasing ID, or UUID. Vertices with the same VID will be identified as the same one. For example: A VID is the unique identifier of an entity, like a person's ID card number. A tag means the type of an entity, such as driver, and boss. Different tags define two groups of different properties, such as driving license number, driving age, order amount, order taking alt, and job number, payroll, debt ceiling, business phone number. When two INSERT statements (neither uses a parameter of IF NOT EXISTS ) with the same VID and tag are operated at the same time, the latter INSERT will overwrite the former. When two INSERT statements with the same VID but different tags, like TAG A and TAG B , are operated at the same time, the operation of Tag A will not affect Tag B . VIDs will usually be indexed and stored into memory (in the way of LSM-tree). Thus, direct access to VIDs enjoys peak performance. VID Operation \u00b6 Nebula Graph 1.x only supports INT64 while Nebula Graph 2.x supports INT64 and FIXED_STRING(<N>) . In CREATE SPACE , VID types can be set via vid_type . id() function can be used to specify or locate a VID. LOOKUP or MATCH statements can be used to find a VID via property index. Direct access to vertices statements via VIDs enjoys peak performance, such as DELETE xxx WHERE id(xxx) == \"player100\" or GO FROM \"player100\" . Finding VIDs via properties and then operating the graph will cause poor performance, such as LOOKUP | GO FROM $-.ids , which will run both LOOKUP and | one more time. VID Generation \u00b6 VIDs can be generated via applications. Here are some tips: (Optimal) Directly take a unique primary key or property as a VID. Property access depends on the VID. Generate a VID via a unique combination of properties. Property access depends on property index. Generate a VID via algorithms like snowflake. Property access depends on property index. If short primary keys greatly outnumber long primary keys, do not enlarge the N of FIXED_STRING(<N>) too much. Otherwise, it will occupy a lot of memory and hard disks, and slow down performance. Generate VIDs via BASE64, MD5, hash by encoding and splicing. If you generate int64 VID via hash, the probability of collision is about 1/10 when there are 1 billion vertices. The number of edges has no concern with the probability of collision. Define and modify a VID and its data type \u00b6 The data type of a VID must be defined when you create the graph space . Once defined, it cannot be modified. A VID is set when you insert a vertex and cannot be modified. Query start vid and global scan \u00b6 In most cases, the execution plan of query statements in Nebula Graph ( MATCH , GO , and LOOKUP ) must query the start vid in a certain way. There are only two ways to locate start vid : For example, GO FROM \"player100\" OVER explicitly indicates in the statement that start vid is \"player100\". For example, LOOKUP ON player WHERE player.name == \"Tony Parker\" or MATCH (v:player {name:\"Tony Parker\"}) locates start vid by the index of the property player.name . Caution For example, match (n) return n; returns an error: Scan vertices or edges need to specify a limit number, or limit number can not push down. , because it is a global scan, you must use the LIMIT clause to limit the number of returns.","title":"VID"},{"location":"1.introduction/3.vid/#vid","text":"In Nebula Graph, a vertex is uniquely identified by its ID, which is called a VID or a Vertex ID.","title":"VID"},{"location":"1.introduction/3.vid/#features","text":"The data types of VIDs are restricted to FIXED_STRING(<N>) or INT64 . One graph space can only select one VID type. A VID in a graph space is unique. It functions just as a primary key in a relational database. VIDs in different graph spaces are independent. The VID generation method must be set by users, because Nebula Graph does not provide auto increasing ID, or UUID. Vertices with the same VID will be identified as the same one. For example: A VID is the unique identifier of an entity, like a person's ID card number. A tag means the type of an entity, such as driver, and boss. Different tags define two groups of different properties, such as driving license number, driving age, order amount, order taking alt, and job number, payroll, debt ceiling, business phone number. When two INSERT statements (neither uses a parameter of IF NOT EXISTS ) with the same VID and tag are operated at the same time, the latter INSERT will overwrite the former. When two INSERT statements with the same VID but different tags, like TAG A and TAG B , are operated at the same time, the operation of Tag A will not affect Tag B . VIDs will usually be indexed and stored into memory (in the way of LSM-tree). Thus, direct access to VIDs enjoys peak performance.","title":"Features"},{"location":"1.introduction/3.vid/#vid_operation","text":"Nebula Graph 1.x only supports INT64 while Nebula Graph 2.x supports INT64 and FIXED_STRING(<N>) . In CREATE SPACE , VID types can be set via vid_type . id() function can be used to specify or locate a VID. LOOKUP or MATCH statements can be used to find a VID via property index. Direct access to vertices statements via VIDs enjoys peak performance, such as DELETE xxx WHERE id(xxx) == \"player100\" or GO FROM \"player100\" . Finding VIDs via properties and then operating the graph will cause poor performance, such as LOOKUP | GO FROM $-.ids , which will run both LOOKUP and | one more time.","title":"VID Operation"},{"location":"1.introduction/3.vid/#vid_generation","text":"VIDs can be generated via applications. Here are some tips: (Optimal) Directly take a unique primary key or property as a VID. Property access depends on the VID. Generate a VID via a unique combination of properties. Property access depends on property index. Generate a VID via algorithms like snowflake. Property access depends on property index. If short primary keys greatly outnumber long primary keys, do not enlarge the N of FIXED_STRING(<N>) too much. Otherwise, it will occupy a lot of memory and hard disks, and slow down performance. Generate VIDs via BASE64, MD5, hash by encoding and splicing. If you generate int64 VID via hash, the probability of collision is about 1/10 when there are 1 billion vertices. The number of edges has no concern with the probability of collision.","title":"VID Generation"},{"location":"1.introduction/3.vid/#define_and_modify_a_vid_and_its_data_type","text":"The data type of a VID must be defined when you create the graph space . Once defined, it cannot be modified. A VID is set when you insert a vertex and cannot be modified.","title":"Define and modify a VID and its data type"},{"location":"1.introduction/3.vid/#query_start_vid_and_global_scan","text":"In most cases, the execution plan of query statements in Nebula Graph ( MATCH , GO , and LOOKUP ) must query the start vid in a certain way. There are only two ways to locate start vid : For example, GO FROM \"player100\" OVER explicitly indicates in the statement that start vid is \"player100\". For example, LOOKUP ON player WHERE player.name == \"Tony Parker\" or MATCH (v:player {name:\"Tony Parker\"}) locates start vid by the index of the property player.name . Caution For example, match (n) return n; returns an error: Scan vertices or edges need to specify a limit number, or limit number can not push down. , because it is a global scan, you must use the LIMIT clause to limit the number of returns.","title":"Query start vid and global scan"},{"location":"1.introduction/3.nebula-graph-architecture/1.architecture-overview/","text":"Architecture overview \u00b6 Nebula Graph consists of three services: the Graph Service, the Storage Service, and the Meta Service. It applies the separation of storage and computing architecture. Each service has its executable binaries and processes launched from the binaries. Users can deploy a Nebula Graph cluster on a single machine or multiple machines using these binaries. The following figure shows the architecture of a typical Nebula Graph cluster. The Meta Service \u00b6 The Meta Service in the Nebula Graph architecture is run by the nebula-metad processes. It is responsible for metadata management, such as schema operations, cluster administration, and user privilege management. For details on the Meta Service, see Meta Service . The Graph Service and the Storage Service \u00b6 Nebula Graph applies the separation of storage and computing architecture. The Graph Service is responsible for querying. The Storage Service is responsible for storage. They are run by different processes, i.e., nebula-graphd and nebula-storaged. The benefits of the separation of storage and computing architecture are as follows: Great scalability The separated structure makes both the Graph Service and the Storage Service flexible and easy to scale in or out. High availability If part of the Graph Service fails, the data stored by the Storage Service suffers no loss. And if the rest part of the Graph Service is still able to serve the clients, service recovery can be performed quickly, even unfelt by the users. Cost-effective The separation of storage and computing architecture provides a higher resource utilization rate, and it enables clients to manage the cost flexibly according to business demands. Open to more possibilities With the ability to run separately, the Graph Service may work with multiple types of storage engines, and the Storage Service may also serve more types of computing engines. For details on the Graph Service and the Storage Service, see Graph Service and Storage Service .","title":"Architecture overview"},{"location":"1.introduction/3.nebula-graph-architecture/1.architecture-overview/#architecture_overview","text":"Nebula Graph consists of three services: the Graph Service, the Storage Service, and the Meta Service. It applies the separation of storage and computing architecture. Each service has its executable binaries and processes launched from the binaries. Users can deploy a Nebula Graph cluster on a single machine or multiple machines using these binaries. The following figure shows the architecture of a typical Nebula Graph cluster.","title":"Architecture overview"},{"location":"1.introduction/3.nebula-graph-architecture/1.architecture-overview/#the_meta_service","text":"The Meta Service in the Nebula Graph architecture is run by the nebula-metad processes. It is responsible for metadata management, such as schema operations, cluster administration, and user privilege management. For details on the Meta Service, see Meta Service .","title":"The Meta Service"},{"location":"1.introduction/3.nebula-graph-architecture/1.architecture-overview/#the_graph_service_and_the_storage_service","text":"Nebula Graph applies the separation of storage and computing architecture. The Graph Service is responsible for querying. The Storage Service is responsible for storage. They are run by different processes, i.e., nebula-graphd and nebula-storaged. The benefits of the separation of storage and computing architecture are as follows: Great scalability The separated structure makes both the Graph Service and the Storage Service flexible and easy to scale in or out. High availability If part of the Graph Service fails, the data stored by the Storage Service suffers no loss. And if the rest part of the Graph Service is still able to serve the clients, service recovery can be performed quickly, even unfelt by the users. Cost-effective The separation of storage and computing architecture provides a higher resource utilization rate, and it enables clients to manage the cost flexibly according to business demands. Open to more possibilities With the ability to run separately, the Graph Service may work with multiple types of storage engines, and the Storage Service may also serve more types of computing engines. For details on the Graph Service and the Storage Service, see Graph Service and Storage Service .","title":"The Graph Service and the Storage Service"},{"location":"1.introduction/3.nebula-graph-architecture/2.meta-service/","text":"Meta Service \u00b6 This topic introduces the architecture and functions of the Meta Service. The architecture of the Meta Service \u00b6 The architecture of the Meta Service is as follows: The Meta Service is run by nebula-metad processes. Users can deploy nebula-metad processes according to the scenario: In a test environment, users can deploy one or three nebula-metad processes on different machines or a single machine. In a production environment, we recommend that users deploy three nebula-metad processes on different machines for high availability. All the nebula-metad processes form a Raft-based cluster, with one process as the leader and the others as the followers. The leader is elected by the majorities and only the leader can provide service to the clients or other components of Nebula Graph. The followers will be run in a standby way and each has a data replication of the leader. Once the leader fails, one of the followers will be elected as the new leader. Note The data of the leader and the followers will keep consistent through Raft. Thus the breakdown and election of the leader will not cause data inconsistency. For more information on Raft, see Storage service architecture . Functions of the Meta Service \u00b6 Manages user accounts \u00b6 The Meta Service stores the information of user accounts and the privileges granted to the accounts. When the clients send queries to the Meta Service through an account, the Meta Service checks the account information and whether the account has the right privileges to execute the queries or not. For more information on Nebula Graph access control, see Authentication . Manages partitions \u00b6 The Meta Service stores and manages the locations of the storage partitions and helps balance the partitions. Manages graph spaces \u00b6 Nebula Graph supports multiple graph spaces. Data stored in different graph spaces are securely isolated. The Meta Service stores the metadata of all graph spaces and tracks the changes of them, such as adding or dropping a graph space. Manages schema information \u00b6 Nebula Graph is a strong-typed graph database. Its schema contains tags (i.e., the vertex types), edge types, tag properties, and edge type properties. The Meta Service stores the schema information. Besides, it performs the addition, modification, and deletion of the schema, and logs the versions of them. For more information on Nebula Graph schema, see Data model . Manages TTL information \u00b6 The Meta Service stores the definition of TTL (Time to Live) options which are used to control data expiration. The Storage Service takes care of the expiring and evicting processes. For more information, see TTL . Manages jobs \u00b6 The Job Management module in the Meta Service is responsible for the creation, queuing, querying, and deletion of jobs.","title":"Meta Service"},{"location":"1.introduction/3.nebula-graph-architecture/2.meta-service/#meta_service","text":"This topic introduces the architecture and functions of the Meta Service.","title":"Meta Service"},{"location":"1.introduction/3.nebula-graph-architecture/2.meta-service/#the_architecture_of_the_meta_service","text":"The architecture of the Meta Service is as follows: The Meta Service is run by nebula-metad processes. Users can deploy nebula-metad processes according to the scenario: In a test environment, users can deploy one or three nebula-metad processes on different machines or a single machine. In a production environment, we recommend that users deploy three nebula-metad processes on different machines for high availability. All the nebula-metad processes form a Raft-based cluster, with one process as the leader and the others as the followers. The leader is elected by the majorities and only the leader can provide service to the clients or other components of Nebula Graph. The followers will be run in a standby way and each has a data replication of the leader. Once the leader fails, one of the followers will be elected as the new leader. Note The data of the leader and the followers will keep consistent through Raft. Thus the breakdown and election of the leader will not cause data inconsistency. For more information on Raft, see Storage service architecture .","title":"The architecture of the Meta Service"},{"location":"1.introduction/3.nebula-graph-architecture/2.meta-service/#functions_of_the_meta_service","text":"","title":"Functions of the Meta Service"},{"location":"1.introduction/3.nebula-graph-architecture/2.meta-service/#manages_user_accounts","text":"The Meta Service stores the information of user accounts and the privileges granted to the accounts. When the clients send queries to the Meta Service through an account, the Meta Service checks the account information and whether the account has the right privileges to execute the queries or not. For more information on Nebula Graph access control, see Authentication .","title":"Manages user accounts"},{"location":"1.introduction/3.nebula-graph-architecture/2.meta-service/#manages_partitions","text":"The Meta Service stores and manages the locations of the storage partitions and helps balance the partitions.","title":"Manages partitions"},{"location":"1.introduction/3.nebula-graph-architecture/2.meta-service/#manages_graph_spaces","text":"Nebula Graph supports multiple graph spaces. Data stored in different graph spaces are securely isolated. The Meta Service stores the metadata of all graph spaces and tracks the changes of them, such as adding or dropping a graph space.","title":"Manages graph spaces"},{"location":"1.introduction/3.nebula-graph-architecture/2.meta-service/#manages_schema_information","text":"Nebula Graph is a strong-typed graph database. Its schema contains tags (i.e., the vertex types), edge types, tag properties, and edge type properties. The Meta Service stores the schema information. Besides, it performs the addition, modification, and deletion of the schema, and logs the versions of them. For more information on Nebula Graph schema, see Data model .","title":"Manages schema information"},{"location":"1.introduction/3.nebula-graph-architecture/2.meta-service/#manages_ttl_information","text":"The Meta Service stores the definition of TTL (Time to Live) options which are used to control data expiration. The Storage Service takes care of the expiring and evicting processes. For more information, see TTL .","title":"Manages TTL information"},{"location":"1.introduction/3.nebula-graph-architecture/2.meta-service/#manages_jobs","text":"The Job Management module in the Meta Service is responsible for the creation, queuing, querying, and deletion of jobs.","title":"Manages jobs"},{"location":"1.introduction/3.nebula-graph-architecture/3.graph-service/","text":"Graph Service \u00b6 The Graph Service is used to process the query. It has four submodules: Parser, Validator, Planner, and Executor. This topic will describe the Graph Service accordingly. The architecture of the Graph Service \u00b6 After a query is sent to the Graph Service, it will be processed by the following four submodules: Parser : Performs lexical analysis and syntax analysis. Validator : Validates the statements. Planner : Generates and optimizes the execution plans. Executor : Executes the operators. Parser \u00b6 After receiving a request, the statements will be parsed by Parser composed of Flex (lexical analysis tool) and Bison (syntax analysis tool), and its corresponding AST will be generated. Statements will be directly intercepted in this stage because of their invalid syntax. For example, the structure of the AST of GO FROM \"Tim\" OVER like WHERE properties(edge).likeness > 8.0 YIELD dst(edge) is shown in the following figure. Validator \u00b6 Validator performs a series of validations on the AST. It mainly works on these tasks: Validating metadata Validator will validate whether the metadata is correct or not. When parsing the OVER , WHERE , and YIELD clauses, Validator looks up the Schema and verifies whether the edge type and tag data exist or not. For an INSERT statement, Validator verifies whether the types of the inserted data are the same as the ones defined in the Schema. Validating contextual reference Validator will verify whether the cited variable exists or not, or whether the cited property is variable or not. For composite statements, like $var = GO FROM \"Tim\" OVER like YIELD dst(edge) AS ID; GO FROM $var.ID OVER serve YIELD dst(edge) , Validator verifies first to see if var is defined, and then to check if the ID property is attached to the var variable. Validating type inference Validator infers what type the result of an expression is and verifies the type against the specified clause. For example, the WHERE clause requires the result to be a bool value, a NULL value, or empty . Validating the information of * Validator needs to verify all the Schema that involves * when verifying the clause if there is a * in the statement. Take a statement like GO FROM \"Tim\" OVER * YIELD dst(edge), properties(edge).likeness, dst(edge) as an example. When verifying the OVER clause, Validator needs to verify all the edge types. If the edge type includes like and serve , the statement would be GO FROM \"Tim\" OVER like,serve YIELD dst(edge), properties(edge).likeness, dst(edge) . Validating input and output Validator will check the consistency of the clauses before and after the | . In the statement GO FROM \"Tim\" OVER like YIELD dst(edge) AS ID | GO FROM $-.ID OVER serve YIELD dst(edge) , Validator will verify whether $-.ID is defined in the clause before the | . When the validation succeeds, an execution plan will be generated. Its data structure will be stored in the src/planner directory. Planner \u00b6 In the nebula-graphd.conf file, when enable_optimizer is set to be false , Planner will not optimize the execution plans generated by Validator. It will be executed by Executor directly. In the nebula-graphd.conf file, when enable_optimizer is set to be true , Planner will optimize the execution plans generated by Validator. The structure is as follows. Before optimization In the execution plan on the right side of the preceding figure, each node directly depends on other nodes. For example, the root node Project depends on the Filter node, the Filter node depends on the GetNeighbor node, and so on, up to the leaf node Start . Then the execution plan is (not truly) executed. During this stage, every node has its input and output variables, which are stored in a hash table. The execution plan is not truly executed, so the value of each key in the associated hash table is empty (except for the Start node, where the input variables hold the starting data), and the hash table is defined in src/context/ExecutionContext.cpp under the nebula-graph repository. For example, if the hash table is named as ResultMap when creating the Filter node, users can determine that the node takes data from ResultMap[\"GN1\"] , then puts the result into ResultMap[\"Filter2\"] , and so on. All these work as the input and output of each node. Process of optimization The optimization rules that Planner has implemented so far are considered RBO (Rule-Based Optimization), namely the pre-defined optimization rules. The CBO (Cost-Based Optimization) feature is under development. The optimized code is in the src/optimizer/ directory under the nebula-graph repository. RBO is a \u201cbottom-up\u201d exploration process. For each rule, the root node of the execution plan (in this case, the Project node) is the entry point, and step by step along with the node dependencies, it reaches the node at the bottom to see if it matches the rule. As shown in the preceding figure, when the Filter node is explored, it is found that its children node is GetNeighbors , which matches successfully with the pre-defined rules, so a transformation is initiated to integrate the Filter node into the GetNeighbors node, the Filter node is removed, and then the process continues to the next rule. Therefore, when the GetNeighbor operator calls interfaces of the Storage layer to get the neighboring edges of a vertex during the execution stage, the Storage layer will directly filter out the unqualified edges internally. Such optimization greatly reduces the amount of data transfer, which is commonly known as filter pushdown. Executor \u00b6 The Executor module consists of Scheduler and Executor. The Scheduler generates the corresponding execution operators against the execution plan, starting from the leaf nodes and ending at the root node. The structure is as follows. Each node of the execution plan has one execution operator node, whose input and output have been determined in the execution plan. Each operator only needs to get the values for the input variables, compute them, and finally put the results into the corresponding output variables. Therefore, it is only necessary to execute step by step from Start , and the result of the last operator is returned to the user as the final result. Source code hierarchy \u00b6 The source code hierarchy under the nebula-graph repository is as follows. | --src | --context //contexts for validation and execution | --daemons | --executor //execution operators | --mock | --optimizer //optimization rules | --parser //lexical analysis and syntax analysis | --planner //structure of the execution plans | --scheduler //scheduler | --service | --util //basic components | --validator //validation of the statements | --visitor","title":"Graph Service"},{"location":"1.introduction/3.nebula-graph-architecture/3.graph-service/#graph_service","text":"The Graph Service is used to process the query. It has four submodules: Parser, Validator, Planner, and Executor. This topic will describe the Graph Service accordingly.","title":"Graph Service"},{"location":"1.introduction/3.nebula-graph-architecture/3.graph-service/#the_architecture_of_the_graph_service","text":"After a query is sent to the Graph Service, it will be processed by the following four submodules: Parser : Performs lexical analysis and syntax analysis. Validator : Validates the statements. Planner : Generates and optimizes the execution plans. Executor : Executes the operators.","title":"The architecture of the Graph Service"},{"location":"1.introduction/3.nebula-graph-architecture/3.graph-service/#parser","text":"After receiving a request, the statements will be parsed by Parser composed of Flex (lexical analysis tool) and Bison (syntax analysis tool), and its corresponding AST will be generated. Statements will be directly intercepted in this stage because of their invalid syntax. For example, the structure of the AST of GO FROM \"Tim\" OVER like WHERE properties(edge).likeness > 8.0 YIELD dst(edge) is shown in the following figure.","title":"Parser"},{"location":"1.introduction/3.nebula-graph-architecture/3.graph-service/#validator","text":"Validator performs a series of validations on the AST. It mainly works on these tasks: Validating metadata Validator will validate whether the metadata is correct or not. When parsing the OVER , WHERE , and YIELD clauses, Validator looks up the Schema and verifies whether the edge type and tag data exist or not. For an INSERT statement, Validator verifies whether the types of the inserted data are the same as the ones defined in the Schema. Validating contextual reference Validator will verify whether the cited variable exists or not, or whether the cited property is variable or not. For composite statements, like $var = GO FROM \"Tim\" OVER like YIELD dst(edge) AS ID; GO FROM $var.ID OVER serve YIELD dst(edge) , Validator verifies first to see if var is defined, and then to check if the ID property is attached to the var variable. Validating type inference Validator infers what type the result of an expression is and verifies the type against the specified clause. For example, the WHERE clause requires the result to be a bool value, a NULL value, or empty . Validating the information of * Validator needs to verify all the Schema that involves * when verifying the clause if there is a * in the statement. Take a statement like GO FROM \"Tim\" OVER * YIELD dst(edge), properties(edge).likeness, dst(edge) as an example. When verifying the OVER clause, Validator needs to verify all the edge types. If the edge type includes like and serve , the statement would be GO FROM \"Tim\" OVER like,serve YIELD dst(edge), properties(edge).likeness, dst(edge) . Validating input and output Validator will check the consistency of the clauses before and after the | . In the statement GO FROM \"Tim\" OVER like YIELD dst(edge) AS ID | GO FROM $-.ID OVER serve YIELD dst(edge) , Validator will verify whether $-.ID is defined in the clause before the | . When the validation succeeds, an execution plan will be generated. Its data structure will be stored in the src/planner directory.","title":"Validator"},{"location":"1.introduction/3.nebula-graph-architecture/3.graph-service/#planner","text":"In the nebula-graphd.conf file, when enable_optimizer is set to be false , Planner will not optimize the execution plans generated by Validator. It will be executed by Executor directly. In the nebula-graphd.conf file, when enable_optimizer is set to be true , Planner will optimize the execution plans generated by Validator. The structure is as follows. Before optimization In the execution plan on the right side of the preceding figure, each node directly depends on other nodes. For example, the root node Project depends on the Filter node, the Filter node depends on the GetNeighbor node, and so on, up to the leaf node Start . Then the execution plan is (not truly) executed. During this stage, every node has its input and output variables, which are stored in a hash table. The execution plan is not truly executed, so the value of each key in the associated hash table is empty (except for the Start node, where the input variables hold the starting data), and the hash table is defined in src/context/ExecutionContext.cpp under the nebula-graph repository. For example, if the hash table is named as ResultMap when creating the Filter node, users can determine that the node takes data from ResultMap[\"GN1\"] , then puts the result into ResultMap[\"Filter2\"] , and so on. All these work as the input and output of each node. Process of optimization The optimization rules that Planner has implemented so far are considered RBO (Rule-Based Optimization), namely the pre-defined optimization rules. The CBO (Cost-Based Optimization) feature is under development. The optimized code is in the src/optimizer/ directory under the nebula-graph repository. RBO is a \u201cbottom-up\u201d exploration process. For each rule, the root node of the execution plan (in this case, the Project node) is the entry point, and step by step along with the node dependencies, it reaches the node at the bottom to see if it matches the rule. As shown in the preceding figure, when the Filter node is explored, it is found that its children node is GetNeighbors , which matches successfully with the pre-defined rules, so a transformation is initiated to integrate the Filter node into the GetNeighbors node, the Filter node is removed, and then the process continues to the next rule. Therefore, when the GetNeighbor operator calls interfaces of the Storage layer to get the neighboring edges of a vertex during the execution stage, the Storage layer will directly filter out the unqualified edges internally. Such optimization greatly reduces the amount of data transfer, which is commonly known as filter pushdown.","title":"Planner"},{"location":"1.introduction/3.nebula-graph-architecture/3.graph-service/#executor","text":"The Executor module consists of Scheduler and Executor. The Scheduler generates the corresponding execution operators against the execution plan, starting from the leaf nodes and ending at the root node. The structure is as follows. Each node of the execution plan has one execution operator node, whose input and output have been determined in the execution plan. Each operator only needs to get the values for the input variables, compute them, and finally put the results into the corresponding output variables. Therefore, it is only necessary to execute step by step from Start , and the result of the last operator is returned to the user as the final result.","title":"Executor"},{"location":"1.introduction/3.nebula-graph-architecture/3.graph-service/#source_code_hierarchy","text":"The source code hierarchy under the nebula-graph repository is as follows. | --src | --context //contexts for validation and execution | --daemons | --executor //execution operators | --mock | --optimizer //optimization rules | --parser //lexical analysis and syntax analysis | --planner //structure of the execution plans | --scheduler //scheduler | --service | --util //basic components | --validator //validation of the statements | --visitor","title":"Source code hierarchy"},{"location":"1.introduction/3.nebula-graph-architecture/4.storage-service/","text":"Storage Service \u00b6 The persistent data of Nebula Graph have two parts. One is the Meta Service that stores the meta-related data. The other is the Storage Service that stores the data, which is run by the nebula-storaged process. This topic will describe the architecture of the Storage Service. Advantages \u00b6 High performance (Customized built-in KVStore) Great scalability (Shared-nothing architecture, not rely on NAS/SAN-like devices) Strong consistency (Raft) High availability (Raft) Supports synchronizing with the third party systems, such as Elasticsearch . The architecture of the Storage Service \u00b6 The Storage Service is run by the nebula-storaged process. Users can deploy nebula-storaged processes on different occasions. For example, users can deploy 1 nebula-storaged process in a test environment and deploy 3 nebula-storaged processes in a production environment. All the nebula-storaged processes consist of a Raft-based cluster. There are three layers in the Storage Service: Storage interface The top layer is the storage interface. It defines a set of APIs that are related to the graph concepts. These API requests will be translated into a set of KV operations targeting the corresponding Partition . For example: getNeighbors : queries the in-edge or out-edge of a set of vertices, returns the edges and the corresponding properties, and supports conditional filtering. insert vertex/edge : inserts a vertex or edge and its properties. getProps : gets the properties of a vertex or an edge. It is this layer that makes the Storage Service a real graph storage. Otherwise, it is just a KV storage. Consensus Below the storage interface is the consensus layer that implements Multi Group Raft , which ensures the strong consistency and high availability of the Storage Service. Store engine The bottom layer is the local storage engine library, providing operations like get , put , and scan on local disks. The related interfaces are stored in KVStore.h and KVEngine.h files. You can develop your own local store plugins based on your needs. The following will describe some features of the Storage Service based on the above architecture. Storage writing process \u00b6 KVStore \u00b6 Nebula Graph develops and customizes its built-in KVStore for the following reasons. It is a high-performance KVStore. It is provided as a (kv) library and can be easily developed for the filter pushdown purpose. As a strong-typed database, how to provide Schema during pushdown is the key to efficiency for Nebula Graph. It has strong data consistency. Therefore, Nebula Graph develops its own KVStore with RocksDB as the local storage engine. The advantages are as follows. For multiple local hard disks, Nebula Graph can make full use of its concurrent capacities through deploying multiple data directories. The Meta Service manages all the Storage servers. All the partition distribution data and current machine status can be found in the meta service. Accordingly, users can execute a manual load balancing plan in meta service. Note Nebula Graph does not support auto load balancing because auto data transfer will affect online business. Nebula Graph provides its own WAL mode so one can customize the WAL. Each partition owns its WAL. One Nebula Graph KVStore cluster supports multiple graph spaces, and each graph space has its own partition number and replica copies. Different graph spaces are isolated physically from each other in the same cluster. Data storage structure \u00b6 Graphs consist of vertices and edges. Nebula Graph uses key-value pairs to store vertices, edges, and their properties. Vertices and edges are stored in keys and their properties are stored in values. Such structure enables efficient property filtering. The storage structure of vertices Different from Nebula Graph version 2.x, version 3.x added a new key for each vertex. Compared to the old key that still exists, the new key has no TagID field and no value. Vertices in Nebula Graph can now live without tags owing to the new key. Field Description Type One byte, used to indicate the key type. PartID Three bytes, used to indicate the sharding partition and to scan the partition data based on the prefix when re-balancing the partition. VertexID The vertex ID. For an integer VertexID, it occupies eight bytes. However, for a string VertexID, it is changed to fixed_string of a fixed length which needs to be specified by users when they create the space. TagID Four bytes, used to indicate the tags that vertex relate with. SerializedValue The serialized value of the key. It stores the property information of the vertex. The storage structure of edges Field Description Type One byte, used to indicate the key type. PartID Three bytes, used to indicate the partition ID. This field can be used to scan the partition data based on the prefix when re-balancing the partition. VertexID Used to indicate vertex ID. The former VID refers to the source VID in the outgoing edge and the dest VID in the incoming edge, while the latter VID refers to the dest VID in the outgoing edge and the source VID in the incoming edge. Edge Type Four bytes, used to indicate the edge type. Greater than zero indicates out-edge, less than zero means in-edge. Rank Eight bytes, used to indicate multiple edges in one edge type. Users can set the field based on needs and store weight, such as transaction time and transaction number. PlaceHolder One byte. Reserved. SerializedValue The serialized value of the key. It stores the property information of the edge. Property descriptions \u00b6 Nebula Graph uses strong-typed Schema. Nebula Graph will store the properties of vertex and edges in order after encoding them. Since the length of properties is fixed, queries can be made in no time according to offset. Before decoding, Nebula Graph needs to get (and cache) the schema information in the Meta Service. In addition, when encoding properties, Nebula Graph will add the corresponding schema version to support online schema change. Data partitioning \u00b6 Since in an ultra-large-scale relational network, vertices can be as many as tens to hundreds of billions, and edges are even more than trillions. Even if only vertices and edges are stored, the storage capacity of both exceeds that of ordinary servers. Therefore, Nebula Graph uses hash to shard the graph elements and store them in different partitions. Edge partitioning and storage amplification \u00b6 In Nebula Graph, an edge corresponds to two key-value pairs on the hard disk. When there are lots of edges and each has many properties, storage amplification will be obvious. The storage format of edges is shown in the figure below. In this example, ScrVertex connects DstVertex via EdgeA, forming the path of (SrcVertex)-[EdgeA]->(DstVertex) . ScrVertex, DstVertex, and EdgeA will all be stored in Partition x and Partition y as four key-value pairs in the storage layer. Details are as follows: The key value of SrcVertex is stored in Partition x. Key fields include Type, PartID(x), VID(Src), and TagID. SerializedValue, namely Value, refers to serialized vertex properties. The first key value of EdgeA, namely EdgeA_Out, is stored in the same partition as the ScrVertex. Key fields include Type, PartID(x), VID(Src), EdgeType(+ means out-edge), Rank(0), VID(Dst), and PlaceHolder. SerializedValue, namely Value, refers to serialized edge properties. The key value of DstVertex is stored in Partition y. Key fields include Type, PartID(y), VID(Dst), and TagID. SerializedValue, namely Value, refers to serialized vertex properties. The second key value of EdgeA, namely EdgeA_In, is stored in the same partition as the DstVertex. Key fields include Type, PartID(y), VID(Dst), EdgeType(- means in-edge), Rank(0), VID(Src), and PlaceHolder. SerializedValue, namely Value, refers to serialized edge properties, which is exactly the same as that in EdgeA_Out. EdgeA_Out and EdgeA_In are stored in storage layer with opposite directions, constituting EdgeA logically. EdgeA_Out is used for traversal requests starting from SrcVertex, such as (a)-[]->() ; EdgeA_In is used for traversal requests starting from DstVertex, such as ()-[]->(a) . Like EdgeA_Out and EdgeA_In, Nebula Graph redundantly stores the information of each edge, which doubles the actual capacities needed for edge storage. The key corresponding to the edge occupies a small hard disk space, but the space occupied by Value is proportional to the length and amount of the property value. Therefore, it will occupy a relatively large hard disk space if the property value of the edge is large or there are many edge property values. To ensure the final consistency of the two key-value pairs when operating on edges, enable the TOSS function . After that, the operation will be performed in Partition x first where the out-edge is located, and then in Partition y where the in-edge is located. Finally, the result is returned. --> Partition algorithm \u00b6 Nebula Graph uses a static Hash strategy to shard data through a modulo operation on vertex ID. All the out-keys, in-keys, and tag data will be placed in the same partition. In this way, query efficiency is increased dramatically. Note The number of partitions needs to be determined when users are creating a graph space since it cannot be changed afterward. Users are supposed to take into consideration the demands of future business when setting it. When inserting into Nebula Graph, vertices and edges are distributed across different partitions. And the partitions are located on different machines. The number of partitions is set in the CREATE SPACE statement and cannot be changed afterward. If certain vertices need to be placed on the same partition (i.e., on the same machine), see Formula/code . The following code will briefly describe the relationship between VID and partition. // If VertexID occupies 8 bytes, it will be stored in int64 to be compatible with the version 1.0. uint64_t vid = 0; if (id.size() == 8) { memcpy(static_cast<void*>(&vid), id.data(), 8); } else { MurmurHash2 hash; vid = hash(id.data()); } PartitionID pId = vid % numParts + 1; Roughly speaking, after hashing a fixed string to int64, (the hashing of int64 is the number itself), do modulo, and then plus one, namely: pId = vid % numParts + 1 ; Parameters and descriptions of the preceding formula are as follows: Parameter Description % The modulo operation. numParts The number of partitions for the graph space where the VID is located, namely the value of partition_num in the CREATE SPACE statement. pId The ID for the partition where the VID is located. Suppose there are 100 partitions, the vertices with VID 1, 101, and 1001 will be stored on the same partition. But, the mapping between the partition ID and the machine address is random. Therefore, we cannot assume that any two partitions are located on the same machine. Raft \u00b6 Raft implementation \u00b6 In a distributed system, one data usually has multiple replicas so that the system can still run normally even if a few copies fail. It requires certain technical means to ensure consistency between replicas. Basic principle: Raft is designed to ensure consistency between replicas. Raft uses election between replicas, and the (candidate) replica that wins more than half of the votes will become the Leader, providing external services on behalf of all replicas. The rest Followers will play backups. When the Leader fails (due to communication failure, operation and maintenance commands, etc.), the rest Followers will conduct a new round of elections and vote for a new Leader. The Leader and Followers will detect each other's survival through heartbeats and write them to the hard disk in Raft-wal mode. Replicas that do not respond to more than multiple heartbeats will be considered faulty. Note Raft-wal needs to be written into the hard disk periodically. If hard disk bottlenecks to write, Raft will fail to send a heartbeat and conduct a new round of elections. If the hard disk IO is severely blocked, there will be no Leader for a long time. Read and write: For every writing request of the clients, the Leader will initiate a Raft-wal and synchronize it with the Followers. Only after over half replicas have received the Raft-wal will it return to the clients successfully. For every reading request of the clients, it will get to the Leader directly, while Followers will not be involved. Failure: Scenario 1: Take a (space) cluster of a single replica as an example. If the system has only one replica, the Leader will be itself. If failure happens, the system will be completely unavailable. Scenario 2: Take a (space) cluster of three replicas as an example. If the system has three replicas, one of them will be the Leader and the rest will be the Followers. If the Leader fails, the rest two can still vote for a new Leader (and a Follower), and the system is still available. But if any of the two Followers fails again, the system will be completely unavailable due to inadequate voters. Note Raft and HDFS have different modes of duplication. Raft is based on a quorum vote, so the number of replicas cannot be even. Multi Group Raft \u00b6 The Storage Service supports a distributed cluster architecture, so Nebula Graph implements Multi Group Raft according to Raft protocol. Each Raft group stores all the replicas of each partition. One replica is the leader, while others are followers. In this way, Nebula Graph achieves strong consistency and high availability. The functions of Raft are as follows. Nebula Graph uses Multi Group Raft to improve performance when there are many partitions because Raft-wal cannot be NULL. When there are too many partitions, costs will increase, such as storing information in Raft group, WAL files, or batch operation in low load. There are two key points to implement the Multi Raft Group: To share transport layer Each Raft Group sends messages to its corresponding peers. So if the transport layer cannot be shared, the connection costs will be very high. To share thread pool Raft Groups share the same thread pool to prevent starting too many threads and a high context switch cost. Batch \u00b6 For each partition, it is necessary to do a batch to improve throughput when writing the WAL serially. As Nebula Graph uses WAL to implement some special functions, batches need to be grouped, which is a feature of Nebula Graph. For example, lock-free CAS operations will execute after all the previous WALs are committed. So for a batch, if there are several WALs in CAS type, we need to divide this batch into several smaller groups and make sure they are committed serially. Transfer Leadership \u00b6 Transfer leadership is extremely important for balance. When moving a partition from one machine to another, Nebula Graph first checks if the source is a leader. If so, it should be moved to another peer. After data migration is completed, it is important to balance leader distribution again. When a transfer leadership command is committed, the leader will abandon its leadership and the followers will start a leader election. Peer changes \u00b6 To avoid split-brain, when members in a Raft Group change, an intermediate state is required. In such a state, the quorum of the old group and new group always have an overlap. Thus it prevents the old or new group from making decisions unilaterally. To make it even simpler, in his doctoral thesis Diego Ongaro suggests adding or removing a peer once to ensure the overlap between the quorum of the new group and the old group. Nebula Graph also uses this approach, except that the way to add or remove a member is different. For details, please refer to addPeer/removePeer in the Raft Part class. Cache \u00b6 The cache management of RocksDB can not cache vertices or edges on demand. Nebula Graph implements its own cache management for Storage, allowing you to set the storage cache size, content, etc. For more information, see Storage cache configurations . Differences with HDFS \u00b6 The Storage Service is a Raft-based distributed architecture, which has certain differences with that of HDFS. For example: The Storage Service ensures consistency through Raft. Usually, the number of its replicas is odd to elect a leader. However, DataNode used by HDFS ensures consistency through NameNode, which has no limit on the number of replicas. In the Storage Service, only the replicas of the leader can read and write, while in HDFS all the replicas can do so. In the Storage Service, the number of replicas needs to be determined when creating a space, since it cannot be changed afterward. But in HDFS, the number of replicas can be changed freely. The Storage Service can access the file system directly. While the applications of HDFS (such as HBase) have to access HDFS before the file system, which requires more RPC times. In a word, the Storage Service is more lightweight with some functions simplified and its architecture is simpler than HDFS, which can effectively improve the read and write performance of a smaller block of data.","title":"Storage Service"},{"location":"1.introduction/3.nebula-graph-architecture/4.storage-service/#storage_service","text":"The persistent data of Nebula Graph have two parts. One is the Meta Service that stores the meta-related data. The other is the Storage Service that stores the data, which is run by the nebula-storaged process. This topic will describe the architecture of the Storage Service.","title":"Storage Service"},{"location":"1.introduction/3.nebula-graph-architecture/4.storage-service/#advantages","text":"High performance (Customized built-in KVStore) Great scalability (Shared-nothing architecture, not rely on NAS/SAN-like devices) Strong consistency (Raft) High availability (Raft) Supports synchronizing with the third party systems, such as Elasticsearch .","title":"Advantages"},{"location":"1.introduction/3.nebula-graph-architecture/4.storage-service/#the_architecture_of_the_storage_service","text":"The Storage Service is run by the nebula-storaged process. Users can deploy nebula-storaged processes on different occasions. For example, users can deploy 1 nebula-storaged process in a test environment and deploy 3 nebula-storaged processes in a production environment. All the nebula-storaged processes consist of a Raft-based cluster. There are three layers in the Storage Service: Storage interface The top layer is the storage interface. It defines a set of APIs that are related to the graph concepts. These API requests will be translated into a set of KV operations targeting the corresponding Partition . For example: getNeighbors : queries the in-edge or out-edge of a set of vertices, returns the edges and the corresponding properties, and supports conditional filtering. insert vertex/edge : inserts a vertex or edge and its properties. getProps : gets the properties of a vertex or an edge. It is this layer that makes the Storage Service a real graph storage. Otherwise, it is just a KV storage. Consensus Below the storage interface is the consensus layer that implements Multi Group Raft , which ensures the strong consistency and high availability of the Storage Service. Store engine The bottom layer is the local storage engine library, providing operations like get , put , and scan on local disks. The related interfaces are stored in KVStore.h and KVEngine.h files. You can develop your own local store plugins based on your needs. The following will describe some features of the Storage Service based on the above architecture.","title":"The architecture of the Storage Service"},{"location":"1.introduction/3.nebula-graph-architecture/4.storage-service/#storage_writing_process","text":"","title":"Storage writing process"},{"location":"1.introduction/3.nebula-graph-architecture/4.storage-service/#kvstore","text":"Nebula Graph develops and customizes its built-in KVStore for the following reasons. It is a high-performance KVStore. It is provided as a (kv) library and can be easily developed for the filter pushdown purpose. As a strong-typed database, how to provide Schema during pushdown is the key to efficiency for Nebula Graph. It has strong data consistency. Therefore, Nebula Graph develops its own KVStore with RocksDB as the local storage engine. The advantages are as follows. For multiple local hard disks, Nebula Graph can make full use of its concurrent capacities through deploying multiple data directories. The Meta Service manages all the Storage servers. All the partition distribution data and current machine status can be found in the meta service. Accordingly, users can execute a manual load balancing plan in meta service. Note Nebula Graph does not support auto load balancing because auto data transfer will affect online business. Nebula Graph provides its own WAL mode so one can customize the WAL. Each partition owns its WAL. One Nebula Graph KVStore cluster supports multiple graph spaces, and each graph space has its own partition number and replica copies. Different graph spaces are isolated physically from each other in the same cluster.","title":"KVStore"},{"location":"1.introduction/3.nebula-graph-architecture/4.storage-service/#data_storage_structure","text":"Graphs consist of vertices and edges. Nebula Graph uses key-value pairs to store vertices, edges, and their properties. Vertices and edges are stored in keys and their properties are stored in values. Such structure enables efficient property filtering. The storage structure of vertices Different from Nebula Graph version 2.x, version 3.x added a new key for each vertex. Compared to the old key that still exists, the new key has no TagID field and no value. Vertices in Nebula Graph can now live without tags owing to the new key. Field Description Type One byte, used to indicate the key type. PartID Three bytes, used to indicate the sharding partition and to scan the partition data based on the prefix when re-balancing the partition. VertexID The vertex ID. For an integer VertexID, it occupies eight bytes. However, for a string VertexID, it is changed to fixed_string of a fixed length which needs to be specified by users when they create the space. TagID Four bytes, used to indicate the tags that vertex relate with. SerializedValue The serialized value of the key. It stores the property information of the vertex. The storage structure of edges Field Description Type One byte, used to indicate the key type. PartID Three bytes, used to indicate the partition ID. This field can be used to scan the partition data based on the prefix when re-balancing the partition. VertexID Used to indicate vertex ID. The former VID refers to the source VID in the outgoing edge and the dest VID in the incoming edge, while the latter VID refers to the dest VID in the outgoing edge and the source VID in the incoming edge. Edge Type Four bytes, used to indicate the edge type. Greater than zero indicates out-edge, less than zero means in-edge. Rank Eight bytes, used to indicate multiple edges in one edge type. Users can set the field based on needs and store weight, such as transaction time and transaction number. PlaceHolder One byte. Reserved. SerializedValue The serialized value of the key. It stores the property information of the edge.","title":"Data storage structure"},{"location":"1.introduction/3.nebula-graph-architecture/4.storage-service/#property_descriptions","text":"Nebula Graph uses strong-typed Schema. Nebula Graph will store the properties of vertex and edges in order after encoding them. Since the length of properties is fixed, queries can be made in no time according to offset. Before decoding, Nebula Graph needs to get (and cache) the schema information in the Meta Service. In addition, when encoding properties, Nebula Graph will add the corresponding schema version to support online schema change.","title":"Property descriptions"},{"location":"1.introduction/3.nebula-graph-architecture/4.storage-service/#data_partitioning","text":"Since in an ultra-large-scale relational network, vertices can be as many as tens to hundreds of billions, and edges are even more than trillions. Even if only vertices and edges are stored, the storage capacity of both exceeds that of ordinary servers. Therefore, Nebula Graph uses hash to shard the graph elements and store them in different partitions.","title":"Data partitioning"},{"location":"1.introduction/3.nebula-graph-architecture/4.storage-service/#edge_partitioning_and_storage_amplification","text":"In Nebula Graph, an edge corresponds to two key-value pairs on the hard disk. When there are lots of edges and each has many properties, storage amplification will be obvious. The storage format of edges is shown in the figure below. In this example, ScrVertex connects DstVertex via EdgeA, forming the path of (SrcVertex)-[EdgeA]->(DstVertex) . ScrVertex, DstVertex, and EdgeA will all be stored in Partition x and Partition y as four key-value pairs in the storage layer. Details are as follows: The key value of SrcVertex is stored in Partition x. Key fields include Type, PartID(x), VID(Src), and TagID. SerializedValue, namely Value, refers to serialized vertex properties. The first key value of EdgeA, namely EdgeA_Out, is stored in the same partition as the ScrVertex. Key fields include Type, PartID(x), VID(Src), EdgeType(+ means out-edge), Rank(0), VID(Dst), and PlaceHolder. SerializedValue, namely Value, refers to serialized edge properties. The key value of DstVertex is stored in Partition y. Key fields include Type, PartID(y), VID(Dst), and TagID. SerializedValue, namely Value, refers to serialized vertex properties. The second key value of EdgeA, namely EdgeA_In, is stored in the same partition as the DstVertex. Key fields include Type, PartID(y), VID(Dst), EdgeType(- means in-edge), Rank(0), VID(Src), and PlaceHolder. SerializedValue, namely Value, refers to serialized edge properties, which is exactly the same as that in EdgeA_Out. EdgeA_Out and EdgeA_In are stored in storage layer with opposite directions, constituting EdgeA logically. EdgeA_Out is used for traversal requests starting from SrcVertex, such as (a)-[]->() ; EdgeA_In is used for traversal requests starting from DstVertex, such as ()-[]->(a) . Like EdgeA_Out and EdgeA_In, Nebula Graph redundantly stores the information of each edge, which doubles the actual capacities needed for edge storage. The key corresponding to the edge occupies a small hard disk space, but the space occupied by Value is proportional to the length and amount of the property value. Therefore, it will occupy a relatively large hard disk space if the property value of the edge is large or there are many edge property values. To ensure the final consistency of the two key-value pairs when operating on edges, enable the TOSS function . After that, the operation will be performed in Partition x first where the out-edge is located, and then in Partition y where the in-edge is located. Finally, the result is returned. -->","title":"Edge partitioning and storage amplification"},{"location":"1.introduction/3.nebula-graph-architecture/4.storage-service/#partition_algorithm","text":"Nebula Graph uses a static Hash strategy to shard data through a modulo operation on vertex ID. All the out-keys, in-keys, and tag data will be placed in the same partition. In this way, query efficiency is increased dramatically. Note The number of partitions needs to be determined when users are creating a graph space since it cannot be changed afterward. Users are supposed to take into consideration the demands of future business when setting it. When inserting into Nebula Graph, vertices and edges are distributed across different partitions. And the partitions are located on different machines. The number of partitions is set in the CREATE SPACE statement and cannot be changed afterward. If certain vertices need to be placed on the same partition (i.e., on the same machine), see Formula/code . The following code will briefly describe the relationship between VID and partition. // If VertexID occupies 8 bytes, it will be stored in int64 to be compatible with the version 1.0. uint64_t vid = 0; if (id.size() == 8) { memcpy(static_cast<void*>(&vid), id.data(), 8); } else { MurmurHash2 hash; vid = hash(id.data()); } PartitionID pId = vid % numParts + 1; Roughly speaking, after hashing a fixed string to int64, (the hashing of int64 is the number itself), do modulo, and then plus one, namely: pId = vid % numParts + 1 ; Parameters and descriptions of the preceding formula are as follows: Parameter Description % The modulo operation. numParts The number of partitions for the graph space where the VID is located, namely the value of partition_num in the CREATE SPACE statement. pId The ID for the partition where the VID is located. Suppose there are 100 partitions, the vertices with VID 1, 101, and 1001 will be stored on the same partition. But, the mapping between the partition ID and the machine address is random. Therefore, we cannot assume that any two partitions are located on the same machine.","title":"Partition algorithm"},{"location":"1.introduction/3.nebula-graph-architecture/4.storage-service/#raft","text":"","title":"Raft"},{"location":"1.introduction/3.nebula-graph-architecture/4.storage-service/#raft_implementation","text":"In a distributed system, one data usually has multiple replicas so that the system can still run normally even if a few copies fail. It requires certain technical means to ensure consistency between replicas. Basic principle: Raft is designed to ensure consistency between replicas. Raft uses election between replicas, and the (candidate) replica that wins more than half of the votes will become the Leader, providing external services on behalf of all replicas. The rest Followers will play backups. When the Leader fails (due to communication failure, operation and maintenance commands, etc.), the rest Followers will conduct a new round of elections and vote for a new Leader. The Leader and Followers will detect each other's survival through heartbeats and write them to the hard disk in Raft-wal mode. Replicas that do not respond to more than multiple heartbeats will be considered faulty. Note Raft-wal needs to be written into the hard disk periodically. If hard disk bottlenecks to write, Raft will fail to send a heartbeat and conduct a new round of elections. If the hard disk IO is severely blocked, there will be no Leader for a long time. Read and write: For every writing request of the clients, the Leader will initiate a Raft-wal and synchronize it with the Followers. Only after over half replicas have received the Raft-wal will it return to the clients successfully. For every reading request of the clients, it will get to the Leader directly, while Followers will not be involved. Failure: Scenario 1: Take a (space) cluster of a single replica as an example. If the system has only one replica, the Leader will be itself. If failure happens, the system will be completely unavailable. Scenario 2: Take a (space) cluster of three replicas as an example. If the system has three replicas, one of them will be the Leader and the rest will be the Followers. If the Leader fails, the rest two can still vote for a new Leader (and a Follower), and the system is still available. But if any of the two Followers fails again, the system will be completely unavailable due to inadequate voters. Note Raft and HDFS have different modes of duplication. Raft is based on a quorum vote, so the number of replicas cannot be even.","title":"Raft implementation"},{"location":"1.introduction/3.nebula-graph-architecture/4.storage-service/#multi_group_raft","text":"The Storage Service supports a distributed cluster architecture, so Nebula Graph implements Multi Group Raft according to Raft protocol. Each Raft group stores all the replicas of each partition. One replica is the leader, while others are followers. In this way, Nebula Graph achieves strong consistency and high availability. The functions of Raft are as follows. Nebula Graph uses Multi Group Raft to improve performance when there are many partitions because Raft-wal cannot be NULL. When there are too many partitions, costs will increase, such as storing information in Raft group, WAL files, or batch operation in low load. There are two key points to implement the Multi Raft Group: To share transport layer Each Raft Group sends messages to its corresponding peers. So if the transport layer cannot be shared, the connection costs will be very high. To share thread pool Raft Groups share the same thread pool to prevent starting too many threads and a high context switch cost.","title":"Multi Group Raft"},{"location":"1.introduction/3.nebula-graph-architecture/4.storage-service/#batch","text":"For each partition, it is necessary to do a batch to improve throughput when writing the WAL serially. As Nebula Graph uses WAL to implement some special functions, batches need to be grouped, which is a feature of Nebula Graph. For example, lock-free CAS operations will execute after all the previous WALs are committed. So for a batch, if there are several WALs in CAS type, we need to divide this batch into several smaller groups and make sure they are committed serially.","title":"Batch"},{"location":"1.introduction/3.nebula-graph-architecture/4.storage-service/#transfer_leadership","text":"Transfer leadership is extremely important for balance. When moving a partition from one machine to another, Nebula Graph first checks if the source is a leader. If so, it should be moved to another peer. After data migration is completed, it is important to balance leader distribution again. When a transfer leadership command is committed, the leader will abandon its leadership and the followers will start a leader election.","title":"Transfer Leadership"},{"location":"1.introduction/3.nebula-graph-architecture/4.storage-service/#peer_changes","text":"To avoid split-brain, when members in a Raft Group change, an intermediate state is required. In such a state, the quorum of the old group and new group always have an overlap. Thus it prevents the old or new group from making decisions unilaterally. To make it even simpler, in his doctoral thesis Diego Ongaro suggests adding or removing a peer once to ensure the overlap between the quorum of the new group and the old group. Nebula Graph also uses this approach, except that the way to add or remove a member is different. For details, please refer to addPeer/removePeer in the Raft Part class.","title":"Peer changes"},{"location":"1.introduction/3.nebula-graph-architecture/4.storage-service/#cache","text":"The cache management of RocksDB can not cache vertices or edges on demand. Nebula Graph implements its own cache management for Storage, allowing you to set the storage cache size, content, etc. For more information, see Storage cache configurations .","title":"Cache"},{"location":"1.introduction/3.nebula-graph-architecture/4.storage-service/#differences_with_hdfs","text":"The Storage Service is a Raft-based distributed architecture, which has certain differences with that of HDFS. For example: The Storage Service ensures consistency through Raft. Usually, the number of its replicas is odd to elect a leader. However, DataNode used by HDFS ensures consistency through NameNode, which has no limit on the number of replicas. In the Storage Service, only the replicas of the leader can read and write, while in HDFS all the replicas can do so. In the Storage Service, the number of replicas needs to be determined when creating a space, since it cannot be changed afterward. But in HDFS, the number of replicas can be changed freely. The Storage Service can access the file system directly. While the applications of HDFS (such as HBase) have to access HDFS before the file system, which requires more RPC times. In a word, the Storage Service is more lightweight with some functions simplified and its architecture is simpler than HDFS, which can effectively improve the read and write performance of a smaller block of data.","title":"Differences with HDFS"},{"location":"14.client/1.nebula-client/","text":"Clients overview \u00b6 Nebula Graph supports multiple types of clients for users to connect to and manage the Nebula Graph database. Nebula Console : the native CLI client Nebula CPP : the Nebula Graph client for C++ Nebula Java : the Nebula Graph client for Java Nebula Python : the Nebula Graph client for Python Nebula Go : the Nebula Graph client for Golang Note For now, only Nebula Java is thread-safe. Caution Other clients\uff08such as Nebula PHP , Nebula Node , Nebula .net , Nebula JDBC , and NORM - Nebula Graph\u2018s Golang ORM \uff09can also be used to connect to and manage Nebula Graph, but there is no uptime guarantee.","title":"Clients overview"},{"location":"14.client/1.nebula-client/#clients_overview","text":"Nebula Graph supports multiple types of clients for users to connect to and manage the Nebula Graph database. Nebula Console : the native CLI client Nebula CPP : the Nebula Graph client for C++ Nebula Java : the Nebula Graph client for Java Nebula Python : the Nebula Graph client for Python Nebula Go : the Nebula Graph client for Golang Note For now, only Nebula Java is thread-safe. Caution Other clients\uff08such as Nebula PHP , Nebula Node , Nebula .net , Nebula JDBC , and NORM - Nebula Graph\u2018s Golang ORM \uff09can also be used to connect to and manage Nebula Graph, but there is no uptime guarantee.","title":"Clients overview"},{"location":"14.client/3.nebula-cpp-client/","text":"Nebula CPP \u00b6 Nebula CPP is a C++ client for connecting to and managing the Nebula Graph database. Prerequisites \u00b6 You have installed C++ and GCC 4.8 or later versions. You have prepared the correct resources . Compatibility with Nebula Graph \u00b6 Nebula Graph version Nebula CPP version 3.1.0 3.0.0 2.6.x 2.5.0 2.5.x 2.5.0 2.0.x 2.0.0 Install Nebula CPP \u00b6 Clone the Nebula CPP source code to the host. (Recommended) To install a specific version of Nebula CPP, use the Git option --branch to specify the branch. For example, to install v3.0.0, run the following command: $ git clone --branch v3.0.0 https://github.com/vesoft-inc/nebula-cpp.git To install the daily development version, run the following command to download the source code from the master branch: $ git clone https://github.com/vesoft-inc/nebula-cpp.git Change the working directory to nebula-cpp . $ cd nebula-cpp Create a directory named build and change the working directory to it. $ mkdir build && cd build Generate the makefile file with CMake. Note The default installation path is /usr/local/nebula . To modify it, add the -DCMAKE_INSTALL_PREFIX=<installation_path> option while running the following command. $ cmake -DCMAKE_BUILD_TYPE = Release .. Note If G++ does not support C++ 11, add the option -DDISABLE_CXX11_ABI=ON . Compile Nebula CPP. To speed up the compiling, use the -j option to set a concurrent number N . It should be \\(\\min(\\text{CPU}core number,\\frac{the_memory_size(GB)}{2})\\) . $ make -j { N } Install Nebula CPP. $ sudo make install Update the dynamic link library. $ sudo ldconfig Use Nebula CPP \u00b6 Compile the CPP file to an executable file, then you can use it. The following steps take using SessionExample.cpp for example. Use the example code to create the SessionExample.cpp file. Run the following command to compile the file. $ LIBRARY_PATH = <library_folder_path>: $LIBRARY_PATH g++ -std = c++11 SessionExample.cpp -I<include_folder_path> -lnebula_graph_client -o session_example library_folder_path : The storage path of the Nebula Graph dynamic libraries. The default path is /usr/local/nebula/lib64 . include_folder_path : The storage of the Nebula Graph header files. The default path is /usr/local/nebula/include . For example: $ LIBRARY_PATH = /usr/local/nebula/lib64: $LIBRARY_PATH g++ -std = c++11 SessionExample.cpp -I/usr/local/nebula/include -lnebula_graph_client -o session_example Core of the example code \u00b6 This sub-section shows the core of the example code. For all the code, see SessionExample .","title":"Nebula CPP"},{"location":"14.client/3.nebula-cpp-client/#nebula_cpp","text":"Nebula CPP is a C++ client for connecting to and managing the Nebula Graph database.","title":"Nebula CPP"},{"location":"14.client/3.nebula-cpp-client/#prerequisites","text":"You have installed C++ and GCC 4.8 or later versions. You have prepared the correct resources .","title":"Prerequisites"},{"location":"14.client/3.nebula-cpp-client/#compatibility_with_nebula_graph","text":"Nebula Graph version Nebula CPP version 3.1.0 3.0.0 2.6.x 2.5.0 2.5.x 2.5.0 2.0.x 2.0.0","title":"Compatibility with Nebula Graph"},{"location":"14.client/3.nebula-cpp-client/#install_nebula_cpp","text":"Clone the Nebula CPP source code to the host. (Recommended) To install a specific version of Nebula CPP, use the Git option --branch to specify the branch. For example, to install v3.0.0, run the following command: $ git clone --branch v3.0.0 https://github.com/vesoft-inc/nebula-cpp.git To install the daily development version, run the following command to download the source code from the master branch: $ git clone https://github.com/vesoft-inc/nebula-cpp.git Change the working directory to nebula-cpp . $ cd nebula-cpp Create a directory named build and change the working directory to it. $ mkdir build && cd build Generate the makefile file with CMake. Note The default installation path is /usr/local/nebula . To modify it, add the -DCMAKE_INSTALL_PREFIX=<installation_path> option while running the following command. $ cmake -DCMAKE_BUILD_TYPE = Release .. Note If G++ does not support C++ 11, add the option -DDISABLE_CXX11_ABI=ON . Compile Nebula CPP. To speed up the compiling, use the -j option to set a concurrent number N . It should be \\(\\min(\\text{CPU}core number,\\frac{the_memory_size(GB)}{2})\\) . $ make -j { N } Install Nebula CPP. $ sudo make install Update the dynamic link library. $ sudo ldconfig","title":"Install Nebula CPP"},{"location":"14.client/3.nebula-cpp-client/#use_nebula_cpp","text":"Compile the CPP file to an executable file, then you can use it. The following steps take using SessionExample.cpp for example. Use the example code to create the SessionExample.cpp file. Run the following command to compile the file. $ LIBRARY_PATH = <library_folder_path>: $LIBRARY_PATH g++ -std = c++11 SessionExample.cpp -I<include_folder_path> -lnebula_graph_client -o session_example library_folder_path : The storage path of the Nebula Graph dynamic libraries. The default path is /usr/local/nebula/lib64 . include_folder_path : The storage of the Nebula Graph header files. The default path is /usr/local/nebula/include . For example: $ LIBRARY_PATH = /usr/local/nebula/lib64: $LIBRARY_PATH g++ -std = c++11 SessionExample.cpp -I/usr/local/nebula/include -lnebula_graph_client -o session_example","title":"Use Nebula CPP"},{"location":"14.client/3.nebula-cpp-client/#core_of_the_example_code","text":"This sub-section shows the core of the example code. For all the code, see SessionExample .","title":"Core of the example code"},{"location":"14.client/4.nebula-java-client/","text":"Nebula Java \u00b6 Nebula Java is a Java client for connecting to and managing the Nebula Graph database. Prerequisites \u00b6 You have installed Java 8.0 or later versions. Compatibility with Nebula Graph \u00b6 Nebula Graph version Nebula Java version 3.1.0 3.0.0 2.6.x 2.6.1 2.0.x 2.0.0 2.0.0-rc1 2.0.0-rc1 Download Nebula Java \u00b6 (Recommended) To install a specific version of Nebula Java, use the Git option --branch to specify the branch. For example, to install v3.0.0, run the following command: $ git clone --branch v3.0.0 https://github.com/vesoft-inc/nebula-java.git To install the daily development version, run the following command to download the source code from the master branch: $ git clone https://github.com/vesoft-inc/nebula-java.git Use Nebula Java \u00b6 Note We recommend that each thread uses one session. If multiple threads use the same session, the performance will be reduced. When importing a Maven project with tools such as IDEA, set the following dependency in pom.xml . Note 3.0.0-SNAPSHOT indicates the daily development version that may have unknown issues. We recommend that you replace 3.0.0-SNAPSHOT with a released version number to use a table version. <dependency> <groupId>com.vesoft</groupId> <artifactId>client</artifactId> <version>3.0.0-SNAPSHOT</version> </dependency> If you cannot download the dependency for the daily development version, set the following content in pom.xml . Released versions have no such issue. <repositories> <repository> <id>snapshots</id> <url>https://oss.sonatype.org/content/repositories/snapshots/</url> </repository> </repositories> If there is no Maven to manage the project, manually download the JAR file to install Nebula Java. Core of the example code \u00b6 This sub-section shows the core of the example code. For all the code, see GraphClientExample .","title":"Nebula Java"},{"location":"14.client/4.nebula-java-client/#nebula_java","text":"Nebula Java is a Java client for connecting to and managing the Nebula Graph database.","title":"Nebula Java"},{"location":"14.client/4.nebula-java-client/#prerequisites","text":"You have installed Java 8.0 or later versions.","title":"Prerequisites"},{"location":"14.client/4.nebula-java-client/#compatibility_with_nebula_graph","text":"Nebula Graph version Nebula Java version 3.1.0 3.0.0 2.6.x 2.6.1 2.0.x 2.0.0 2.0.0-rc1 2.0.0-rc1","title":"Compatibility with Nebula Graph"},{"location":"14.client/4.nebula-java-client/#download_nebula_java","text":"(Recommended) To install a specific version of Nebula Java, use the Git option --branch to specify the branch. For example, to install v3.0.0, run the following command: $ git clone --branch v3.0.0 https://github.com/vesoft-inc/nebula-java.git To install the daily development version, run the following command to download the source code from the master branch: $ git clone https://github.com/vesoft-inc/nebula-java.git","title":"Download Nebula Java"},{"location":"14.client/4.nebula-java-client/#use_nebula_java","text":"Note We recommend that each thread uses one session. If multiple threads use the same session, the performance will be reduced. When importing a Maven project with tools such as IDEA, set the following dependency in pom.xml . Note 3.0.0-SNAPSHOT indicates the daily development version that may have unknown issues. We recommend that you replace 3.0.0-SNAPSHOT with a released version number to use a table version. <dependency> <groupId>com.vesoft</groupId> <artifactId>client</artifactId> <version>3.0.0-SNAPSHOT</version> </dependency> If you cannot download the dependency for the daily development version, set the following content in pom.xml . Released versions have no such issue. <repositories> <repository> <id>snapshots</id> <url>https://oss.sonatype.org/content/repositories/snapshots/</url> </repository> </repositories> If there is no Maven to manage the project, manually download the JAR file to install Nebula Java.","title":"Use Nebula Java"},{"location":"14.client/4.nebula-java-client/#core_of_the_example_code","text":"This sub-section shows the core of the example code. For all the code, see GraphClientExample .","title":"Core of the example code"},{"location":"14.client/5.nebula-python-client/","text":"Nebula Python \u00b6 Nebula Python is a Python client for connecting to and managing the Nebula Graph database. Prerequisites \u00b6 You have installed Python 3.6 or later versions. Compatibility with Nebula Graph \u00b6 Nebula Graph version Nebula Python version 3.1.0 3.1.0 2.6.x 2.6.0 2.0.x 2.0.0 2.0.0-rc1 2.0.0rc1 Install Nebula Python \u00b6 Install Nebula Python with pip \u00b6 $ pip install nebula3-python == <version> Install Nebula Python from the source code \u00b6 Clone the Nebula Python source code to the host. (Recommended) To install a specific version of Nebula Python, use the Git option --branch to specify the branch. For example, to install v3.1.0, run the following command: $ git clone --branch release-3.1 https://github.com/vesoft-inc/nebula-python.git To install the daily development version, run the following command to download the source code from the master branch: $ git clone https://github.com/vesoft-inc/nebula-python.git Change the working directory to nebula-python. $ cd nebula-python Run the following command to install Nebula Python. $ pip install . Core of the example code \u00b6 This section shows the core of the example code. For all the code, see Example .","title":"Nebula Python"},{"location":"14.client/5.nebula-python-client/#nebula_python","text":"Nebula Python is a Python client for connecting to and managing the Nebula Graph database.","title":"Nebula Python"},{"location":"14.client/5.nebula-python-client/#prerequisites","text":"You have installed Python 3.6 or later versions.","title":"Prerequisites"},{"location":"14.client/5.nebula-python-client/#compatibility_with_nebula_graph","text":"Nebula Graph version Nebula Python version 3.1.0 3.1.0 2.6.x 2.6.0 2.0.x 2.0.0 2.0.0-rc1 2.0.0rc1","title":"Compatibility with Nebula Graph"},{"location":"14.client/5.nebula-python-client/#install_nebula_python","text":"","title":"Install Nebula Python"},{"location":"14.client/5.nebula-python-client/#install_nebula_python_with_pip","text":"$ pip install nebula3-python == <version>","title":"Install Nebula Python with pip"},{"location":"14.client/5.nebula-python-client/#install_nebula_python_from_the_source_code","text":"Clone the Nebula Python source code to the host. (Recommended) To install a specific version of Nebula Python, use the Git option --branch to specify the branch. For example, to install v3.1.0, run the following command: $ git clone --branch release-3.1 https://github.com/vesoft-inc/nebula-python.git To install the daily development version, run the following command to download the source code from the master branch: $ git clone https://github.com/vesoft-inc/nebula-python.git Change the working directory to nebula-python. $ cd nebula-python Run the following command to install Nebula Python. $ pip install .","title":"Install Nebula Python from the source code"},{"location":"14.client/5.nebula-python-client/#core_of_the_example_code","text":"This section shows the core of the example code. For all the code, see Example .","title":"Core of the example code"},{"location":"14.client/6.nebula-go-client/","text":"Nebula Go \u00b6 Nebula Go is a Golang client for connecting to and managing the Nebula Graph database. Prerequisites \u00b6 You have installed Golang 1.13 or later versions. Compatibility with Nebula Graph \u00b6 Nebula Graph version Nebula Go version 3.1.0 3.1.0 2.6.x 2.6.0 2.0.x 2.0.0-GA Download Nebula Go \u00b6 (Recommended) To install a specific version of Nebula Go, use the Git option --branch to specify the branch. For example, to install v3.1.0, run the following command: $ git clone --branch release-v3.1 https://github.com/vesoft-inc/nebula-go.git To install the daily development version, run the following command to download the source code from the master branch: $ git clone https://github.com/vesoft-inc/nebula-go.git Install or update \u00b6 Run the following command to install or update Nebula Go: $ go get -u -v github.com/vesoft-inc/nebula-go/v3@v3.1.0 Core of the example code \u00b6 This section shows the core of the example code. For all the code, see graph_client_basic_example and graph_client_goroutines_example .","title":"Nebula Go"},{"location":"14.client/6.nebula-go-client/#nebula_go","text":"Nebula Go is a Golang client for connecting to and managing the Nebula Graph database.","title":"Nebula Go"},{"location":"14.client/6.nebula-go-client/#prerequisites","text":"You have installed Golang 1.13 or later versions.","title":"Prerequisites"},{"location":"14.client/6.nebula-go-client/#compatibility_with_nebula_graph","text":"Nebula Graph version Nebula Go version 3.1.0 3.1.0 2.6.x 2.6.0 2.0.x 2.0.0-GA","title":"Compatibility with Nebula Graph"},{"location":"14.client/6.nebula-go-client/#download_nebula_go","text":"(Recommended) To install a specific version of Nebula Go, use the Git option --branch to specify the branch. For example, to install v3.1.0, run the following command: $ git clone --branch release-v3.1 https://github.com/vesoft-inc/nebula-go.git To install the daily development version, run the following command to download the source code from the master branch: $ git clone https://github.com/vesoft-inc/nebula-go.git","title":"Download Nebula Go"},{"location":"14.client/6.nebula-go-client/#install_or_update","text":"Run the following command to install or update Nebula Go: $ go get -u -v github.com/vesoft-inc/nebula-go/v3@v3.1.0","title":"Install or update"},{"location":"14.client/6.nebula-go-client/#core_of_the_example_code","text":"This section shows the core of the example code. For all the code, see graph_client_basic_example and graph_client_goroutines_example .","title":"Core of the example code"},{"location":"15.contribution/how-to-contribute/","text":"How to Contribute \u00b6 Before you get started \u00b6 Commit an issue on the github or forum \u00b6 You are welcome to contribute any code or files to the project. But firstly we suggest you raise an issue on the github or the forum to start a discussion with the community. Check through the topic for Github. Sign the Contributor License Agreement (CLA) \u00b6 What is CLA ? Here is the vesoft inc. Contributor License Agreement . Click the Sign in with GitHub to agree button to sign the CLA. If you have any questions, send an email to info@vesoft.com . Modify a single document \u00b6 This manual is written in the Markdown language. Click the pencil icon on the right of the document title to commit the modification. This method applies to modify a single document only. Batch modify or add files \u00b6 This method applies to contribute codes, modify multiple documents in batches, or add new documents. Step 1: Fork in the github.com \u00b6 The Nebula Graph project has many repositories . Take the nebul repository for example: Visit https://github.com/vesoft-inc/nebula . Click the Fork button to establish an online fork. Step 2: Clone Fork to Local Storage \u00b6 Define a local working directory. # Define the working directory. working_dir = $HOME /Workspace Set user to match the Github profile name. user ={ the Github profile name } Create your clone. mkdir -p $working_dir cd $working_dir git clone https://github.com/ $user /nebula.git # or: git clone git@github.com:$user/nebula.git cd $working_dir /nebula git remote add upstream https://github.com/vesoft-inc/nebula.git # or: git remote add upstream git@github.com:vesoft-inc/nebula.git # Never push to upstream master since you do not have write access. git remote set-url --push upstream no_push # Confirm that the remote branch is valid. # The correct format is: # origin git@github.com:$(user)/nebula.git (fetch) # origin git@github.com:$(user)/nebula.git (push) # upstream https://github.com/vesoft-inc/nebula (fetch) # upstream no_push (push) git remote -v (Optional) Define a pre-commit hook. Please link the Nebula Graph pre-commit hook into the .git directory. This hook checks the commits for formatting, building, doc generation, etc. cd $working_dir /nebula/.git/hooks ln -s $working_dir /nebulah/.linters/cpp/hooks/pre-commit.sh . Sometimes, the pre-commit hook cannot be executed. You have to execute it manually. cd $working_dir /nebula/.git/hooks chmod +x pre-commit Step 3: Branch \u00b6 Get your local master up to date. cd $working_dir /nebula git fetch upstream git checkout master git rebase upstream/master Checkout a new branch from master. git checkout -b myfeature Note Because the PR often consists of several commits, which might be squashed while being merged into upstream. We strongly suggest you to open a separate topic branch to make your changes on. After merged, this topic branch can be just abandoned, thus you could synchronize your master branch with upstream easily with a rebase like above. Otherwise, if you commit your changes directly into master, you need to use a hard reset on the master branch. For example: git fetch upstream git checkout master git reset --hard upstream/master git push --force origin master Step 4: Develop \u00b6 Code style Nebula Graph adopts cpplint to make sure that the project conforms to Google's coding style guides. The checker will be implemented before the code is committed. Unit tests requirements Please add unit tests for the new features or bug fixes. Build your code with unit tests enabled For more information, see Install Nebula Graph by compiling the source code . Note Make sure you have enabled the building of unit tests by setting -DENABLE_TESTING=ON . Run tests In the root directory of nebula , run the following command: cd nebula/build ctest -j $( nproc ) Step 5: Bring Your Branch Update to Date \u00b6 # While on your myfeature branch. git fetch upstream git rebase upstream/master Users need to bring the head branch up to date after other contributors merge PR to the base branch. Step 6: Commit \u00b6 Commit your changes. git commit -a Users can use the command --amend to re-edit the previous code. Step 7: Push \u00b6 When ready to review or just to establish an offsite backup, push your branch to your fork on github.com : git push origin myfeature Step 8: Create a Pull Request \u00b6 Visit your fork at https://github.com/$user/nebula (replace $user here). Click the Compare & pull request button next to your myfeature branch. Step 9: Get a Code Review \u00b6 Once your pull request has been created, it will be assigned to at least two reviewers. Those reviewers will do a thorough code review to make sure that the changes meet the repository's contributing guidelines and other quality standards. Add test cases \u00b6 For detailed methods, see How to add test cases . Donation \u00b6 Step 1: Confirm the project donation \u00b6 Contact the official Nebula Graph staff via email, WeChat, Slack, etc. to confirm the donation project. The project will be donated to the Nebula Contrib organization . Email address: info@vesoft.com WeChat: NebulaGraphbot Slack: Join Slack Step 2: Get the information of the project recipient \u00b6 The Nebula Graph official staff will give the recipient ID of the Nebula Contrib project. Step 3: Donate a project \u00b6 The user transfers the project to the recipient of this donation, and the recipient transfers the project to the Nebula Contrib organization. After the donation, the user will continue to lead the development of community projects as a Maintainer. For operations of transferring a repository on GitHub, see Transferring a repository owned by your user account .","title":"How to contribute"},{"location":"15.contribution/how-to-contribute/#how_to_contribute","text":"","title":"How to Contribute"},{"location":"15.contribution/how-to-contribute/#before_you_get_started","text":"","title":"Before you get started"},{"location":"15.contribution/how-to-contribute/#commit_an_issue_on_the_github_or_forum","text":"You are welcome to contribute any code or files to the project. But firstly we suggest you raise an issue on the github or the forum to start a discussion with the community. Check through the topic for Github.","title":"Commit an issue on the github or forum"},{"location":"15.contribution/how-to-contribute/#sign_the_contributor_license_agreement_cla","text":"What is CLA ? Here is the vesoft inc. Contributor License Agreement . Click the Sign in with GitHub to agree button to sign the CLA. If you have any questions, send an email to info@vesoft.com .","title":"Sign the Contributor License Agreement (CLA)"},{"location":"15.contribution/how-to-contribute/#modify_a_single_document","text":"This manual is written in the Markdown language. Click the pencil icon on the right of the document title to commit the modification. This method applies to modify a single document only.","title":"Modify a single document"},{"location":"15.contribution/how-to-contribute/#batch_modify_or_add_files","text":"This method applies to contribute codes, modify multiple documents in batches, or add new documents.","title":"Batch modify or add files"},{"location":"15.contribution/how-to-contribute/#step_1_fork_in_the_githubcom","text":"The Nebula Graph project has many repositories . Take the nebul repository for example: Visit https://github.com/vesoft-inc/nebula . Click the Fork button to establish an online fork.","title":"Step 1: Fork in the github.com"},{"location":"15.contribution/how-to-contribute/#step_2_clone_fork_to_local_storage","text":"Define a local working directory. # Define the working directory. working_dir = $HOME /Workspace Set user to match the Github profile name. user ={ the Github profile name } Create your clone. mkdir -p $working_dir cd $working_dir git clone https://github.com/ $user /nebula.git # or: git clone git@github.com:$user/nebula.git cd $working_dir /nebula git remote add upstream https://github.com/vesoft-inc/nebula.git # or: git remote add upstream git@github.com:vesoft-inc/nebula.git # Never push to upstream master since you do not have write access. git remote set-url --push upstream no_push # Confirm that the remote branch is valid. # The correct format is: # origin git@github.com:$(user)/nebula.git (fetch) # origin git@github.com:$(user)/nebula.git (push) # upstream https://github.com/vesoft-inc/nebula (fetch) # upstream no_push (push) git remote -v (Optional) Define a pre-commit hook. Please link the Nebula Graph pre-commit hook into the .git directory. This hook checks the commits for formatting, building, doc generation, etc. cd $working_dir /nebula/.git/hooks ln -s $working_dir /nebulah/.linters/cpp/hooks/pre-commit.sh . Sometimes, the pre-commit hook cannot be executed. You have to execute it manually. cd $working_dir /nebula/.git/hooks chmod +x pre-commit","title":"Step 2: Clone Fork to Local Storage"},{"location":"15.contribution/how-to-contribute/#step_3_branch","text":"Get your local master up to date. cd $working_dir /nebula git fetch upstream git checkout master git rebase upstream/master Checkout a new branch from master. git checkout -b myfeature Note Because the PR often consists of several commits, which might be squashed while being merged into upstream. We strongly suggest you to open a separate topic branch to make your changes on. After merged, this topic branch can be just abandoned, thus you could synchronize your master branch with upstream easily with a rebase like above. Otherwise, if you commit your changes directly into master, you need to use a hard reset on the master branch. For example: git fetch upstream git checkout master git reset --hard upstream/master git push --force origin master","title":"Step 3: Branch"},{"location":"15.contribution/how-to-contribute/#step_4_develop","text":"Code style Nebula Graph adopts cpplint to make sure that the project conforms to Google's coding style guides. The checker will be implemented before the code is committed. Unit tests requirements Please add unit tests for the new features or bug fixes. Build your code with unit tests enabled For more information, see Install Nebula Graph by compiling the source code . Note Make sure you have enabled the building of unit tests by setting -DENABLE_TESTING=ON . Run tests In the root directory of nebula , run the following command: cd nebula/build ctest -j $( nproc )","title":"Step 4: Develop"},{"location":"15.contribution/how-to-contribute/#step_5_bring_your_branch_update_to_date","text":"# While on your myfeature branch. git fetch upstream git rebase upstream/master Users need to bring the head branch up to date after other contributors merge PR to the base branch.","title":"Step 5: Bring Your Branch Update to Date"},{"location":"15.contribution/how-to-contribute/#step_6_commit","text":"Commit your changes. git commit -a Users can use the command --amend to re-edit the previous code.","title":"Step 6: Commit"},{"location":"15.contribution/how-to-contribute/#step_7_push","text":"When ready to review or just to establish an offsite backup, push your branch to your fork on github.com : git push origin myfeature","title":"Step 7: Push"},{"location":"15.contribution/how-to-contribute/#step_8_create_a_pull_request","text":"Visit your fork at https://github.com/$user/nebula (replace $user here). Click the Compare & pull request button next to your myfeature branch.","title":"Step 8: Create a Pull Request"},{"location":"15.contribution/how-to-contribute/#step_9_get_a_code_review","text":"Once your pull request has been created, it will be assigned to at least two reviewers. Those reviewers will do a thorough code review to make sure that the changes meet the repository's contributing guidelines and other quality standards.","title":"Step 9: Get a Code Review"},{"location":"15.contribution/how-to-contribute/#add_test_cases","text":"For detailed methods, see How to add test cases .","title":"Add test cases"},{"location":"15.contribution/how-to-contribute/#donation","text":"","title":"Donation"},{"location":"15.contribution/how-to-contribute/#step_1_confirm_the_project_donation","text":"Contact the official Nebula Graph staff via email, WeChat, Slack, etc. to confirm the donation project. The project will be donated to the Nebula Contrib organization . Email address: info@vesoft.com WeChat: NebulaGraphbot Slack: Join Slack","title":"Step 1: Confirm the project donation"},{"location":"15.contribution/how-to-contribute/#step_2_get_the_information_of_the_project_recipient","text":"The Nebula Graph official staff will give the recipient ID of the Nebula Contrib project.","title":"Step 2: Get the information of the project recipient"},{"location":"15.contribution/how-to-contribute/#step_3_donate_a_project","text":"The user transfers the project to the recipient of this donation, and the recipient transfers the project to the Nebula Contrib organization. After the donation, the user will continue to lead the development of community projects as a Maintainer. For operations of transferring a repository on GitHub, see Transferring a repository owned by your user account .","title":"Step 3: Donate a project"},{"location":"2.quick-start/1.quick-start-workflow/","text":"Quick start workflow \u00b6 The quick start introduces the simplest workflow to use Nebula Graph, including deploying Nebula Graph, connecting to Nebula Graph, and doing basic CRUD. Steps \u00b6 Users can quickly deploy and use Nebula Graph in the following steps. Deploy Nebula Graph Users can use the RPM or DEB file to quickly deploy Nebula Graph. For other deployment methods and the corresponding preparations, see the Deployment and installation chapter. Start Nebula Graph Users need to start Nebula Graph after deployment. Connect to Nebula Graph Then users can use clients to connect to Nebula Graph. Nebula Graph supports a variety of clients. This topic will describe how to use Nebula Console to connect to Nebula Graph. Register the Storage Service When connecting to Nebula Graph for the first time, users must register the Storage Service before querying data. CRUD in Nebula Graph Users can use nGQL (Nebula Graph Query Language) to run CRUD after connecting to Nebula Graph.","title":"Quick start workflow"},{"location":"2.quick-start/1.quick-start-workflow/#quick_start_workflow","text":"The quick start introduces the simplest workflow to use Nebula Graph, including deploying Nebula Graph, connecting to Nebula Graph, and doing basic CRUD.","title":"Quick start workflow"},{"location":"2.quick-start/1.quick-start-workflow/#steps","text":"Users can quickly deploy and use Nebula Graph in the following steps. Deploy Nebula Graph Users can use the RPM or DEB file to quickly deploy Nebula Graph. For other deployment methods and the corresponding preparations, see the Deployment and installation chapter. Start Nebula Graph Users need to start Nebula Graph after deployment. Connect to Nebula Graph Then users can use clients to connect to Nebula Graph. Nebula Graph supports a variety of clients. This topic will describe how to use Nebula Console to connect to Nebula Graph. Register the Storage Service When connecting to Nebula Graph for the first time, users must register the Storage Service before querying data. CRUD in Nebula Graph Users can use nGQL (Nebula Graph Query Language) to run CRUD after connecting to Nebula Graph.","title":"Steps"},{"location":"2.quick-start/2.install-nebula-graph/","text":"Step 1: Install Nebula Graph \u00b6 RPM and DEB are common package formats on Linux systems. This topic shows how to quickly install Nebula Graph with the RPM or DEB package. Prerequisites \u00b6 Prepare the right resources . Note The console is not complied or packaged with Nebula Graph server binaries. You can install nebula-console by yourself. Enterpriseonly For the Enterprise Edition, please send email to inquiry@vesoft.com. Download the package from cloud service \u00b6 Download the released version. URL: //Centos 6 https://oss-cdn.nebula-graph.io/package/<release_version>/nebula-graph-<release_version>.el6.x86_64.rpm //Centos 7 https://oss-cdn.nebula-graph.io/package/<release_version>/nebula-graph-<release_version>.el7.x86_64.rpm //Centos 8 https://oss-cdn.nebula-graph.io/package/<release_version>/nebula-graph-<release_version>.el8.x86_64.rpm //Ubuntu 1604 https://oss-cdn.nebula-graph.io/package/<release_version>/nebula-graph-<release_version>.ubuntu1604.amd64.deb //Ubuntu 1804 https://oss-cdn.nebula-graph.io/package/<release_version>/nebula-graph-<release_version>.ubuntu1804.amd64.deb //Ubuntu 2004 https://oss-cdn.nebula-graph.io/package/<release_version>/nebula-graph-<release_version>.ubuntu2004.amd64.deb For example, download the release package 3.1.0 for Centos 7.5 : wget https://oss-cdn.nebula-graph.io/package/3.1.0/nebula-graph-3.1.0.el7.x86_64.rpm wget https://oss-cdn.nebula-graph.io/package/3.1.0/nebula-graph-3.1.0.el7.x86_64.rpm.sha256sum.txt Download the release package 3.1.0 for Ubuntu 1804 : wget https://oss-cdn.nebula-graph.io/package/3.1.0/nebula-graph-3.1.0.ubuntu1804.amd64.deb wget https://oss-cdn.nebula-graph.io/package/3.1.0/nebula-graph-3.1.0.ubuntu1804.amd64.deb.sha256sum.txt Download the nightly version. Danger Nightly versions are usually used to test new features. Do not use it in a production environment. Nightly versions may not be built successfully every night. And the names may change from day to day. URL: //Centos 6 https://oss-cdn.nebula-graph.io/package/nightly/<yyyy.mm.dd>/nebula-graph-<yyyy.mm.dd>-nightly.el6.x86_64.rpm //Centos 7 https://oss-cdn.nebula-graph.io/package/nightly/<yyyy.mm.dd>/nebula-graph-<yyyy.mm.dd>-nightly.el7.x86_64.rpm //Centos 8 https://oss-cdn.nebula-graph.io/package/nightly/<yyyy.mm.dd>/nebula-graph-<yyyy.mm.dd>-nightly.el8.x86_64.rpm //Ubuntu 1604 https://oss-cdn.nebula-graph.io/package/nightly/<yyyy.mm.dd>/nebula-graph-<yyyy.mm.dd>-nightly.ubuntu1604.amd64.deb //Ubuntu 1804 https://oss-cdn.nebula-graph.io/package/nightly/<yyyy.mm.dd>/nebula-graph-<yyyy.mm.dd>-nightly.ubuntu1804.amd64.deb //Ubuntu 2004 https://oss-cdn.nebula-graph.io/package/nightly/<yyyy.mm.dd>/nebula-graph-<yyyy.mm.dd>-nightly.ubuntu2004.amd64.deb For example, download the Centos 7.5 package developed and built in 2021.11.28 : wget https://oss-cdn.nebula-graph.io/package/nightly/2021.11.28/nebula-graph-2021.11.28-nightly.el7.x86_64.rpm wget https://oss-cdn.nebula-graph.io/package/nightly/2021.11.28/nebula-graph-2021.11.28-nightly.el7.x86_64.rpm.sha256sum.txt For example, download the Ubuntu 1804 package developed and built in 2021.11.28 : wget https://oss-cdn.nebula-graph.io/package/nightly/2021.11.28/nebula-graph-2021.11.28-nightly.ubuntu1804.amd64.deb wget https://oss-cdn.nebula-graph.io/package/nightly/2021.11.28/nebula-graph-2021.11.28-nightly.ubuntu1804.amd64.deb.sha256sum.txt Install Nebula Graph \u00b6 Use the following syntax to install with an RPM package. $ sudo rpm -ivh --prefix = <installation_path> <package_name> The option --prefix indicates the installation path. The default path is /usr/local/nebula/ . For example, to install an RPM package in the default path for the 3.1.0 version, run the following command. sudo rpm -ivh nebula-graph-3.1.0.el7.x86_64.rpm Use the following syntax to install with a DEB package. $ sudo dpkg -i <package_name> Note Customizing the installation path is not supported when installing Nebula Graph with a DEB package. The default installation path is /usr/local/nebula/ . For example, to install a DEB package for the 3.1.0 version, run the following command. sudo dpkg -i nebula-graph-3.1.0.ubuntu1804.amd64.deb Note The default installation path is /usr/local/nebula/ . Next to do \u00b6 (Enterprise Edition) Deploy license Start Nebula Graph Connect to Nebula Graph","title":"Step 1 Install Nebula Graph"},{"location":"2.quick-start/2.install-nebula-graph/#step_1_install_nebula_graph","text":"RPM and DEB are common package formats on Linux systems. This topic shows how to quickly install Nebula Graph with the RPM or DEB package.","title":"Step 1: Install Nebula Graph"},{"location":"2.quick-start/2.install-nebula-graph/#prerequisites","text":"Prepare the right resources . Note The console is not complied or packaged with Nebula Graph server binaries. You can install nebula-console by yourself. Enterpriseonly For the Enterprise Edition, please send email to inquiry@vesoft.com.","title":"Prerequisites"},{"location":"2.quick-start/2.install-nebula-graph/#download_the_package_from_cloud_service","text":"Download the released version. URL: //Centos 6 https://oss-cdn.nebula-graph.io/package/<release_version>/nebula-graph-<release_version>.el6.x86_64.rpm //Centos 7 https://oss-cdn.nebula-graph.io/package/<release_version>/nebula-graph-<release_version>.el7.x86_64.rpm //Centos 8 https://oss-cdn.nebula-graph.io/package/<release_version>/nebula-graph-<release_version>.el8.x86_64.rpm //Ubuntu 1604 https://oss-cdn.nebula-graph.io/package/<release_version>/nebula-graph-<release_version>.ubuntu1604.amd64.deb //Ubuntu 1804 https://oss-cdn.nebula-graph.io/package/<release_version>/nebula-graph-<release_version>.ubuntu1804.amd64.deb //Ubuntu 2004 https://oss-cdn.nebula-graph.io/package/<release_version>/nebula-graph-<release_version>.ubuntu2004.amd64.deb For example, download the release package 3.1.0 for Centos 7.5 : wget https://oss-cdn.nebula-graph.io/package/3.1.0/nebula-graph-3.1.0.el7.x86_64.rpm wget https://oss-cdn.nebula-graph.io/package/3.1.0/nebula-graph-3.1.0.el7.x86_64.rpm.sha256sum.txt Download the release package 3.1.0 for Ubuntu 1804 : wget https://oss-cdn.nebula-graph.io/package/3.1.0/nebula-graph-3.1.0.ubuntu1804.amd64.deb wget https://oss-cdn.nebula-graph.io/package/3.1.0/nebula-graph-3.1.0.ubuntu1804.amd64.deb.sha256sum.txt Download the nightly version. Danger Nightly versions are usually used to test new features. Do not use it in a production environment. Nightly versions may not be built successfully every night. And the names may change from day to day. URL: //Centos 6 https://oss-cdn.nebula-graph.io/package/nightly/<yyyy.mm.dd>/nebula-graph-<yyyy.mm.dd>-nightly.el6.x86_64.rpm //Centos 7 https://oss-cdn.nebula-graph.io/package/nightly/<yyyy.mm.dd>/nebula-graph-<yyyy.mm.dd>-nightly.el7.x86_64.rpm //Centos 8 https://oss-cdn.nebula-graph.io/package/nightly/<yyyy.mm.dd>/nebula-graph-<yyyy.mm.dd>-nightly.el8.x86_64.rpm //Ubuntu 1604 https://oss-cdn.nebula-graph.io/package/nightly/<yyyy.mm.dd>/nebula-graph-<yyyy.mm.dd>-nightly.ubuntu1604.amd64.deb //Ubuntu 1804 https://oss-cdn.nebula-graph.io/package/nightly/<yyyy.mm.dd>/nebula-graph-<yyyy.mm.dd>-nightly.ubuntu1804.amd64.deb //Ubuntu 2004 https://oss-cdn.nebula-graph.io/package/nightly/<yyyy.mm.dd>/nebula-graph-<yyyy.mm.dd>-nightly.ubuntu2004.amd64.deb For example, download the Centos 7.5 package developed and built in 2021.11.28 : wget https://oss-cdn.nebula-graph.io/package/nightly/2021.11.28/nebula-graph-2021.11.28-nightly.el7.x86_64.rpm wget https://oss-cdn.nebula-graph.io/package/nightly/2021.11.28/nebula-graph-2021.11.28-nightly.el7.x86_64.rpm.sha256sum.txt For example, download the Ubuntu 1804 package developed and built in 2021.11.28 : wget https://oss-cdn.nebula-graph.io/package/nightly/2021.11.28/nebula-graph-2021.11.28-nightly.ubuntu1804.amd64.deb wget https://oss-cdn.nebula-graph.io/package/nightly/2021.11.28/nebula-graph-2021.11.28-nightly.ubuntu1804.amd64.deb.sha256sum.txt","title":"Download the package from cloud service"},{"location":"2.quick-start/2.install-nebula-graph/#install_nebula_graph","text":"Use the following syntax to install with an RPM package. $ sudo rpm -ivh --prefix = <installation_path> <package_name> The option --prefix indicates the installation path. The default path is /usr/local/nebula/ . For example, to install an RPM package in the default path for the 3.1.0 version, run the following command. sudo rpm -ivh nebula-graph-3.1.0.el7.x86_64.rpm Use the following syntax to install with a DEB package. $ sudo dpkg -i <package_name> Note Customizing the installation path is not supported when installing Nebula Graph with a DEB package. The default installation path is /usr/local/nebula/ . For example, to install a DEB package for the 3.1.0 version, run the following command. sudo dpkg -i nebula-graph-3.1.0.ubuntu1804.amd64.deb Note The default installation path is /usr/local/nebula/ .","title":"Install Nebula Graph"},{"location":"2.quick-start/2.install-nebula-graph/#next_to_do","text":"(Enterprise Edition) Deploy license Start Nebula Graph Connect to Nebula Graph","title":"Next to do"},{"location":"2.quick-start/3.1add-storage-hosts/","text":"Register the Storage Service \u00b6 When connecting to Nebula Graph for the first time, you have to add the Storage hosts, and confirm that all the hosts are online. Compatibility Starting from Nebula Graph 3.0.0, you have to run ADD HOSTS before reading or writing data into the Storage Service. In earlier versions, ADD HOSTS is neither needed nor supported. Prerequisites \u00b6 You have connnected to Nebula Graph . Steps \u00b6 Add the Storage hosts. Run the following command to add hosts: ADD HOSTS <ip>:<port> [,<ip>:<port> ...]; Example\uff1a nebula> ADD HOSTS 192.168.10.100:9779, 192.168.10.101:9779, 192.168.10.102:9779; Check the status of the hosts to make sure that they are all online. nebula> SHOW HOSTS; +------------------+------+-----------+----------+--------------+---------------------- +------------------------+---------+ | Host | Port | HTTP port | Status | Leader count | Leader distribution | Partition distribution | Version | +------------------+------+-----------+----------+--------------+---------------------- +------------------------+---------+ | \"192.168.10.100\" | 9779 | 19669 | \"ONLINE\" | 0 | \"No valid partition\" | \"No valid partition\" | \"3.1.0\" | | \"192.168.10.101\" | 9779 | 19669 | \"ONLINE\" | 0 | \"No valid partition\" | \"No valid partition\" | \"3.1.0\" | | \"192.168.10.102\" | 9779 | 19669 | \"ONLINE\" | 0 | \"No valid partition\" | \"No valid partition\" | \"3.1.0\" | +------------------+------+-----------+----------+--------------+---------------------- +------------------------+---------+ The Status column of the result above shows that all Storage hosts are online.","title":"Step 4 Register the Storage Service"},{"location":"2.quick-start/3.1add-storage-hosts/#register_the_storage_service","text":"When connecting to Nebula Graph for the first time, you have to add the Storage hosts, and confirm that all the hosts are online. Compatibility Starting from Nebula Graph 3.0.0, you have to run ADD HOSTS before reading or writing data into the Storage Service. In earlier versions, ADD HOSTS is neither needed nor supported.","title":"Register the Storage Service"},{"location":"2.quick-start/3.1add-storage-hosts/#prerequisites","text":"You have connnected to Nebula Graph .","title":"Prerequisites"},{"location":"2.quick-start/3.1add-storage-hosts/#steps","text":"Add the Storage hosts. Run the following command to add hosts: ADD HOSTS <ip>:<port> [,<ip>:<port> ...]; Example\uff1a nebula> ADD HOSTS 192.168.10.100:9779, 192.168.10.101:9779, 192.168.10.102:9779; Check the status of the hosts to make sure that they are all online. nebula> SHOW HOSTS; +------------------+------+-----------+----------+--------------+---------------------- +------------------------+---------+ | Host | Port | HTTP port | Status | Leader count | Leader distribution | Partition distribution | Version | +------------------+------+-----------+----------+--------------+---------------------- +------------------------+---------+ | \"192.168.10.100\" | 9779 | 19669 | \"ONLINE\" | 0 | \"No valid partition\" | \"No valid partition\" | \"3.1.0\" | | \"192.168.10.101\" | 9779 | 19669 | \"ONLINE\" | 0 | \"No valid partition\" | \"No valid partition\" | \"3.1.0\" | | \"192.168.10.102\" | 9779 | 19669 | \"ONLINE\" | 0 | \"No valid partition\" | \"No valid partition\" | \"3.1.0\" | +------------------+------+-----------+----------+--------------+---------------------- +------------------------+---------+ The Status column of the result above shows that all Storage hosts are online.","title":"Steps"},{"location":"2.quick-start/3.connect-to-nebula-graph/","text":"Step 3: Connect to Nebula Graph \u00b6 This topic provides basic instruction on how to use the native CLI client Nebula Console to connect to Nebula Graph. Caution When connecting to Nebula Graph for the first time, you must register the Storage Service before querying data. Nebula Graph supports multiple types of clients, including a CLI client, a GUI client, and clients developed in popular programming languages. For more information, see the client list . Prerequisites \u00b6 You have started Nebula Graph services . The machine on which you plan to run Nebula Console has network access to the Graph Service of Nebula Graph. The Nebula Console version is compatible with the Nebula Graph version. Note Nebula Console and Nebula Graph of the same version number are the most compatible. There may be compatibility issues when connecting to Nebula Graph with a different version of Nebula Console. The error message incompatible version between client and server is displayed when there is such an issue. Steps \u00b6 On the Nebula Console releases page , select a Nebula Console version and click Assets . In the Assets area, find the correct binary file for the machine where you want to run Nebula Console and download the file to the machine. (Optional) Rename the binary file to nebula-console for convenience. Note For Windows, rename the file to nebula-console.exe . On the machine to run Nebula Console, grant the execute permission of the nebula-console binary file to the user. Note For Windows, skip this step. $ chmod 111 nebula-console In the command line interface, change the working directory to the one where the nebula-console binary file is stored. Run the following command to connect to Nebula Graph. For Linux or macOS: $ ./nebula-console -addr <ip> -port <port> -u <username> -p <password> [ -t 120 ] [ -e \"nGQL_statement\" | -f filename.nGQL ] For Windows: > nebula-console . exe -addr < ip > -port < port > -u < username > -p < password > [ -t 120 ] [ -e \"nGQL_statement\" | -f filename . nGQL ] Parameter descriptions are as follows: Parameter Description -h/-help Shows the help menu. -addr/-address Sets the IP address of the Graph service. The default address is 127.0.0.1. If Nebula Graph is deployed on Nebula Cloud , you need to create a Private Link and set the IP address of the Private Endpoint as the parameter value. -P/-port Sets the port number of the graphd service. The default port number is 9669. -u/-user Sets the username of your Nebula Graph account. Before enabling authentication, you can use any existing username. The default username is root . -p/-password Sets the password of your Nebula Graph account. Before enabling authentication, you can use any characters as the password. -t/-timeout Sets an integer-type timeout threshold of the connection. The unit is second. The default value is 120. -e/-eval Sets a string-type nGQL statement. The nGQL statement is executed once the connection succeeds. The connection stops after the result is returned. -f/-file Sets the path of an nGQL file. The nGQL statements in the file are executed once the connection succeeds. The result will be returned and the connection stops then. -enable_ssl Enables SSL encryption when connecting to Nebula Graph. -ssl_root_ca_path Sets the storage path of the certification authority file. -ssl_cert_path Sets the storage path of the certificate file. -ssl_private_key_path Sets the storage path of the private key file. For information on more parameters, see the project repository .","title":"Step 3 Connect to Nebula Graph"},{"location":"2.quick-start/3.connect-to-nebula-graph/#step_3_connect_to_nebula_graph","text":"This topic provides basic instruction on how to use the native CLI client Nebula Console to connect to Nebula Graph. Caution When connecting to Nebula Graph for the first time, you must register the Storage Service before querying data. Nebula Graph supports multiple types of clients, including a CLI client, a GUI client, and clients developed in popular programming languages. For more information, see the client list .","title":"Step 3: Connect to Nebula Graph"},{"location":"2.quick-start/3.connect-to-nebula-graph/#prerequisites","text":"You have started Nebula Graph services . The machine on which you plan to run Nebula Console has network access to the Graph Service of Nebula Graph. The Nebula Console version is compatible with the Nebula Graph version. Note Nebula Console and Nebula Graph of the same version number are the most compatible. There may be compatibility issues when connecting to Nebula Graph with a different version of Nebula Console. The error message incompatible version between client and server is displayed when there is such an issue.","title":"Prerequisites"},{"location":"2.quick-start/3.connect-to-nebula-graph/#steps","text":"On the Nebula Console releases page , select a Nebula Console version and click Assets . In the Assets area, find the correct binary file for the machine where you want to run Nebula Console and download the file to the machine. (Optional) Rename the binary file to nebula-console for convenience. Note For Windows, rename the file to nebula-console.exe . On the machine to run Nebula Console, grant the execute permission of the nebula-console binary file to the user. Note For Windows, skip this step. $ chmod 111 nebula-console In the command line interface, change the working directory to the one where the nebula-console binary file is stored. Run the following command to connect to Nebula Graph. For Linux or macOS: $ ./nebula-console -addr <ip> -port <port> -u <username> -p <password> [ -t 120 ] [ -e \"nGQL_statement\" | -f filename.nGQL ] For Windows: > nebula-console . exe -addr < ip > -port < port > -u < username > -p < password > [ -t 120 ] [ -e \"nGQL_statement\" | -f filename . nGQL ] Parameter descriptions are as follows: Parameter Description -h/-help Shows the help menu. -addr/-address Sets the IP address of the Graph service. The default address is 127.0.0.1. If Nebula Graph is deployed on Nebula Cloud , you need to create a Private Link and set the IP address of the Private Endpoint as the parameter value. -P/-port Sets the port number of the graphd service. The default port number is 9669. -u/-user Sets the username of your Nebula Graph account. Before enabling authentication, you can use any existing username. The default username is root . -p/-password Sets the password of your Nebula Graph account. Before enabling authentication, you can use any characters as the password. -t/-timeout Sets an integer-type timeout threshold of the connection. The unit is second. The default value is 120. -e/-eval Sets a string-type nGQL statement. The nGQL statement is executed once the connection succeeds. The connection stops after the result is returned. -f/-file Sets the path of an nGQL file. The nGQL statements in the file are executed once the connection succeeds. The result will be returned and the connection stops then. -enable_ssl Enables SSL encryption when connecting to Nebula Graph. -ssl_root_ca_path Sets the storage path of the certification authority file. -ssl_cert_path Sets the storage path of the certificate file. -ssl_private_key_path Sets the storage path of the private key file. For information on more parameters, see the project repository .","title":"Steps"},{"location":"2.quick-start/4.nebula-graph-crud/","text":"Step 4: Use nGQL (CRUD) \u00b6 This topic will describe the basic CRUD operations in Nebula Graph. For more information, see nGQL guide . Graph space and Nebula Graph schema \u00b6 A Nebula Graph instance consists of one or more graph spaces. Graph spaces are physically isolated from each other. You can use different graph spaces in the same instance to store different datasets. To insert data into a graph space, define a schema for the graph database. Nebula Graph schema is based on the following components. Schema component Description Vertex Represents an entity in the real world. A vertex can have zero to multiple tags. Tag The type of the same group of vertices. It defines a set of properties that describes the types of vertices. Edge Represents a directed relationship between two vertices. Edge type The type of an edge. It defines a group of properties that describes the types of edges. For more information, see Data modeling . In this topic, we will use the following dataset to demonstrate basic CRUD operations. Async implementation of CREATE and ALTER \u00b6 Caution In Nebula Graph, the following CREATE or ALTER commands are implemented in an async way and take effect in the next heartbeat cycle. Otherwise, an error will be returned. To make sure the follow-up operations work as expected, Wait for two heartbeat cycles, i.e., 20 seconds. CREATE SPACE CREATE TAG CREATE EDGE ALTER TAG ALTER EDGE CREATE TAG INDEX CREATE EDGE INDEX Note The default heartbeat interval is 10 seconds. To change the heartbeat interval, modify the heartbeat_interval_secs parameter in the configuration files for all services. Create and use a graph space \u00b6 nGQL syntax \u00b6 Create a graph space: CREATE SPACE [IF NOT EXISTS] <graph_space_name> ( [partition_num = <partition_number>,] [replica_factor = <replica_number>,] vid_type = {FIXED_STRING(<N>) | INT64} ) [COMMENT = '<comment>']; For more information on parameters, see CREATE SPACE . List graph spaces and check if the creation is successful: nebula> SHOW SPACES; Use a graph space: USE <graph_space_name>; Examples \u00b6 Use the following statement to create a graph space named basketballplayer . nebula> CREATE SPACE basketballplayer(partition_num=15, replica_factor=1, vid_type=fixed_string(30)); Check the partition distribution with SHOW HOSTS to make sure that the partitions are distributed in a balanced way. nebula> SHOW HOSTS; +-------------+-----------+-----------+-----------+--------------+----------------------------------+------------------------+---------+ | Host | Port | HTTP port | Status | Leader count | Leader distribution | Partition distribution | Version | +-------------+-----------+-----------+-----------+--------------+----------------------------------+------------------------+---------+ | \"storaged0\" | 9779 | 19669 | \"ONLINE\" | 5 | \"basketballplayer:5\" | \"basketballplayer:5\" | \"3.1.0\" | | \"storaged1\" | 9779 | 19669 | \"ONLINE\" | 5 | \"basketballplayer:5\" | \"basketballplayer:5\" | \"3.1.0\" | | \"storaged2\" | 9779 | 19669 | \"ONLINE\" | 5 | \"basketballplayer:5\" | \"basketballplayer:5\" | \"3.1.0\" | +-------------+-----------+-----------+-----------+--------------+----------------------------------+------------------------+---------+ If the Leader distribution is uneven, use BALANCE LEADER to redistribute the partitions. For more information, see BALANCE . Use the basketballplayer graph space. nebula[(none)]> USE basketballplayer; You can use SHOW SPACES to check the graph space you created. nebula> SHOW SPACES; +--------------------+ | Name | +--------------------+ | \"basketballplayer\" | +--------------------+ Create tags and edge types \u00b6 nGQL syntax \u00b6 CREATE {TAG | EDGE} [IF NOT EXISTS] {<tag_name> | <edge_type_name>} ( <prop_name> <data_type> [NULL | NOT NULL] [DEFAULT <default_value>] [COMMENT '<comment>'] [{, <prop_name> <data_type> [NULL | NOT NULL] [DEFAULT <default_value>] [COMMENT '<comment>']} ...] ) [TTL_DURATION = <ttl_duration>] [TTL_COL = <prop_name>] [COMMENT = '<comment>']; For more information on parameters, see CREATE TAG and CREATE EDGE . Examples \u00b6 Create tags player and team , and edge types follow and serve . Descriptions are as follows. Component name Type Property player Tag name (string), age (int) team Tag name (string) follow Edge type degree (int) serve Edge type start_year (int), end_year (int) nebula> CREATE TAG player(name string, age int); nebula> CREATE TAG team(name string); nebula> CREATE EDGE follow(degree int); nebula> CREATE EDGE serve(start_year int, end_year int); Insert vertices and edges \u00b6 You can use the INSERT statement to insert vertices or edges based on existing tags or edge types. nGQL syntax \u00b6 Insert vertices: INSERT VERTEX [IF NOT EXISTS] [tag_props, [tag_props] ...] VALUES <vid>: ([prop_value_list]) tag_props: tag_name ([prop_name_list]) prop_name_list: [prop_name [, prop_name] ...] prop_value_list: [prop_value [, prop_value] ...] vid is short for Vertex ID. A vid must be a unique string value in a graph space. For details, see INSERT VERTEX . Insert edges: INSERT EDGE [IF NOT EXISTS] <edge_type> ( <prop_name_list> ) VALUES <src_vid> -> <dst_vid>[@<rank>] : ( <prop_value_list> ) [, <src_vid> -> <dst_vid>[@<rank>] : ( <prop_value_list> ), ...]; <prop_name_list> ::= [ <prop_name> [, <prop_name> ] ...] <prop_value_list> ::= [ <prop_value> [, <prop_value> ] ...] For more information on parameters, see INSERT EDGE . Examples \u00b6 Insert vertices representing basketball players and teams: nebula> INSERT VERTEX player(name, age) VALUES \"player100\":(\"Tim Duncan\", 42); nebula> INSERT VERTEX player(name, age) VALUES \"player101\":(\"Tony Parker\", 36); nebula> INSERT VERTEX player(name, age) VALUES \"player102\":(\"LaMarcus Aldridge\", 33); nebula> INSERT VERTEX team(name) VALUES \"team203\":(\"Trail Blazers\"), \"team204\":(\"Spurs\"); Insert edges representing the relations between basketball players and teams: nebula> INSERT EDGE follow(degree) VALUES \"player101\" -> \"player100\":(95); nebula> INSERT EDGE follow(degree) VALUES \"player101\" -> \"player102\":(90); nebula> INSERT EDGE follow(degree) VALUES \"player102\" -> \"player100\":(75); nebula> INSERT EDGE serve(start_year, end_year) VALUES \"player101\" -> \"team204\":(1999, 2018),\"player102\" -> \"team203\":(2006, 2015); Read data \u00b6 The GO statement can traverse the database based on specific conditions. A GO traversal starts from one or more vertices, along one or more edges, and returns information in a form specified in the YIELD clause. The FETCH statement is used to get properties from vertices or edges. The LOOKUP statement is based on indexes . It is used together with the WHERE clause to search for the data that meet the specific conditions. The MATCH statement is the most commonly used statement for graph data querying. It can describe all kinds of graph patterns, but it relies on indexes to match data patterns in Nebula Graph. Therefore, its performance still needs optimization. nGQL syntax \u00b6 GO GO [[<M> TO] <N> STEPS ] FROM <vertex_list> OVER <edge_type_list> [{REVERSELY | BIDIRECT}] [ WHERE <conditions> ] YIELD [DISTINCT] <return_list> [{ SAMPLE <sample_list> | <limit_by_list_clause> }] [| GROUP BY {<col_name> | expression> | <position>} YIELD <col_name>] [| ORDER BY <expression> [{ASC | DESC}]] [| LIMIT [<offset>,] <number_rows>]; FETCH Fetch properties on tags: FETCH PROP ON {<tag_name>[, tag_name ...] | *} <vid> [, vid ...] YIELD <return_list> [AS <alias>]; Fetch properties on edges: FETCH PROP ON <edge_type> <src_vid> -> <dst_vid>[@<rank>] [, <src_vid> -> <dst_vid> ...] YIELD <output>; LOOKUP LOOKUP ON {<vertex_tag> | <edge_type>} [WHERE <expression> [AND <expression> ...]] YIELD <return_list> [AS <alias>]; <return_list> <prop_name> [AS <col_alias>] [, <prop_name> [AS <prop_alias>] ...]; MATCH MATCH <pattern> [<clause_1>] RETURN <output> [<clause_2>]; Examples of GO statement \u00b6 Search for the players that the player with VID player101 follows. nebula> GO FROM \"player101\" OVER follow YIELD id($$); +-------------+ | id($$) | +-------------+ | \"player100\" | | \"player102\" | | \"player125\" | +-------------+ Filter the players that the player with VID player101 follows whose age is equal to or greater than 35. Rename the corresponding columns in the results with Teammate and Age . nebula> GO FROM \"player101\" OVER follow WHERE properties($$).age >= 35 \\ YIELD properties($$).name AS Teammate, properties($$).age AS Age; +-----------------+-----+ | Teammate | Age | +-----------------+-----+ | \"Tim Duncan\" | 42 | | \"Manu Ginobili\" | 41 | +-----------------+-----+ | Clause/Sign | Description | |-------------+---------------------------------------------------------------------| | YIELD | Specifies what values or results you want to return from the query. | | $$ | Represents the target vertices. | | \\ | A line-breaker. | Search for the players that the player with VID player101 follows. Then retrieve the teams of the players that the player with VID player100 follows. To combine the two queries, use a pipe or a temporary variable. With a pipe: nebula> GO FROM \"player101\" OVER follow YIELD dst(edge) AS id | \\ GO FROM $-.id OVER serve YIELD properties($$).name AS Team, \\ properties($^).name AS Player; +-----------------+---------------------+ | Team | Player | +-----------------+---------------------+ | \"Spurs\" | \"Tim Duncan\" | | \"Trail Blazers\" | \"LaMarcus Aldridge\" | | \"Spurs\" | \"LaMarcus Aldridge\" | | \"Spurs\" | \"Manu Ginobili\" | +-----------------+---------------------+ Clause/Sign Description $^ Represents the source vertex of the edge. | A pipe symbol can combine multiple queries. $- Represents the outputs of the query before the pipe symbol. With a temporary variable: Note Once a composite statement is submitted to the server as a whole, the life cycle of the temporary variables in the statement ends. nebula> $var = GO FROM \"player101\" OVER follow YIELD dst(edge) AS id; \\ GO FROM $var.id OVER serve YIELD properties($$).name AS Team, \\ properties($^).name AS Player; +-----------------+---------------------+ | Team | Player | +-----------------+---------------------+ | \"Spurs\" | \"Tim Duncan\" | | \"Trail Blazers\" | \"LaMarcus Aldridge\" | | \"Spurs\" | \"LaMarcus Aldridge\" | | \"Spurs\" | \"Manu Ginobili\" | +-----------------+---------------------+ Example of FETCH statement \u00b6 Use FETCH : Fetch the properties of the player with VID player100 . nebula> FETCH PROP ON player \"player100\" YIELD properties(vertex); +-------------------------------+ | properties(VERTEX) | +-------------------------------+ | {age: 42, name: \"Tim Duncan\"} | +-------------------------------+ Note The examples of LOOKUP and MATCH statements are in indexes . Update vertices and edges \u00b6 Users can use the UPDATE or the UPSERT statements to update existing data. UPSERT is the combination of UPDATE and INSERT . If you update a vertex or an edge with UPSERT , the database will insert a new vertex or edge if it does not exist. Note UPSERT operates serially in a partition-based order. Therefore, it is slower than INSERT OR UPDATE . And UPSERT has concurrency only between multiple partitions. nGQL syntax \u00b6 UPDATE vertices: UPDATE VERTEX <vid> SET <properties to be updated> [WHEN <condition>] [YIELD <columns>]; UPDATE edges: UPDATE EDGE <source vid> -> <destination vid> [@rank] OF <edge_type> SET <properties to be updated> [WHEN <condition>] [YIELD <columns to be output>]; UPSERT vertices or edges: UPSERT {VERTEX <vid> | EDGE <edge_type>} SET <update_columns> [WHEN <condition>] [YIELD <columns>]; Examples \u00b6 UPDATE the name property of the vertex with VID player100 and check the result with the FETCH statement. nebula> UPDATE VERTEX \"player100\" SET player.name = \"Tim\"; nebula> FETCH PROP ON player \"player100\" YIELD properties(vertex); +------------------------+ | properties(VERTEX) | +------------------------+ | {age: 42, name: \"Tim\"} | +------------------------+ UPDATE the degree property of an edge and check the result with the FETCH statement. nebula> UPDATE EDGE \"player101\" -> \"player100\" OF follow SET degree = 96; nebula> FETCH PROP ON follow \"player101\" -> \"player100\" YIELD properties(edge); +------------------+ | properties(EDGE) | +------------------+ | {degree: 96} | +------------------+ Insert a vertex with VID player111 and UPSERT it. nebula> INSERT VERTEX player(name,age) VALUES \"player111\":(\"David West\", 38); nebula> UPSERT VERTEX \"player111\" SET player.name = \"David\", player.age = $^.player.age + 11 \\ WHEN $^.player.name == \"David West\" AND $^.player.age > 20 \\ YIELD $^.player.name AS Name, $^.player.age AS Age; +---------+-----+ | Name | Age | +---------+-----+ | \"David\" | 49 | +---------+-----+ Delete vertices and edges \u00b6 nGQL syntax \u00b6 Delete vertices: DELETE VERTEX <vid1>[, <vid2>...] Delete edges: DELETE EDGE <edge_type> <src_vid> -> <dst_vid>[@<rank>] [, <src_vid> -> <dst_vid>...] Examples \u00b6 Delete vertices: nebula> DELETE VERTEX \"player111\", \"team203\"; Delete edges: nebula> DELETE EDGE follow \"player101\" -> \"team204\"; About indexes \u00b6 Users can add indexes to tags and edge types with the CREATE INDEX statement. Must-read for using indexes Both MATCH and LOOKUP statements depend on the indexes. But indexes can dramatically reduce the write performance (as much as 90% or even more). DO NOT use indexes in production environments unless you are fully aware of their influences on your service. Users MUST rebuild indexes for pre-existing data. Otherwise, the pre-existing data cannot be indexed and therefore cannot be returned in MATCH or LOOKUP statements. For more information, see REBUILD INDEX . nGQL syntax \u00b6 Create an index: CREATE {TAG | EDGE} INDEX [IF NOT EXISTS] <index_name> ON {<tag_name> | <edge_name>} ([<prop_name_list>]) [COMMENT = '<comment>']; Rebuild an index: REBUILD {TAG | EDGE} INDEX <index_name>; Note Define the index length when creating an index for a variable-length property. In UTF-8 encoding, a non-ascii character occupies 3 bytes. You should set an appropriate index length according to the variable-length property. For example, the index should be 30 bytes for 10 non-ascii characters. For more information, see CREATE INDEX Examples of LOOKUP and MATCH (index-based) \u00b6 Make sure there is an index for LOOKUP or MATCH to use. If there is not, create an index first. Find the information of the vertex with the tag player and its value of the name property is Tony Parker . This example creates the index player_index_1 on the name property. nebula> CREATE TAG INDEX IF NOT EXISTS player_index_1 ON player(name(20)); This example rebuilds the index to make sure it takes effect on pre-existing data. nebula> REBUILD TAG INDEX player_index_1 +------------+ | New Job Id | +------------+ | 31 | +------------+ This example uses the LOOKUP statement to retrieve the vertex property. nebula> LOOKUP ON player WHERE player.name == \"Tony Parker\" \\ YIELD properties(vertex).name AS name, properties(vertex).age AS age; +---------------+-----+ | name | age | +---------------+-----+ | \"Tony Parker\" | 36 | +---------------+-----+ This example uses the MATCH statement to retrieve the vertex property. nebula> MATCH (v:player{name:\"Tony Parker\"}) RETURN v; +-----------------------------------------------------+ | v | +-----------------------------------------------------+ | (\"player101\" :player{age: 36, name: \"Tony Parker\"}) | +-----------------------------------------------------+","title":"Step 5 Use nGQL (CRUD)"},{"location":"2.quick-start/4.nebula-graph-crud/#step_4_use_ngql_crud","text":"This topic will describe the basic CRUD operations in Nebula Graph. For more information, see nGQL guide .","title":"Step 4: Use nGQL (CRUD)"},{"location":"2.quick-start/4.nebula-graph-crud/#graph_space_and_nebula_graph_schema","text":"A Nebula Graph instance consists of one or more graph spaces. Graph spaces are physically isolated from each other. You can use different graph spaces in the same instance to store different datasets. To insert data into a graph space, define a schema for the graph database. Nebula Graph schema is based on the following components. Schema component Description Vertex Represents an entity in the real world. A vertex can have zero to multiple tags. Tag The type of the same group of vertices. It defines a set of properties that describes the types of vertices. Edge Represents a directed relationship between two vertices. Edge type The type of an edge. It defines a group of properties that describes the types of edges. For more information, see Data modeling . In this topic, we will use the following dataset to demonstrate basic CRUD operations.","title":"Graph space and Nebula Graph schema"},{"location":"2.quick-start/4.nebula-graph-crud/#async_implementation_of_create_and_alter","text":"Caution In Nebula Graph, the following CREATE or ALTER commands are implemented in an async way and take effect in the next heartbeat cycle. Otherwise, an error will be returned. To make sure the follow-up operations work as expected, Wait for two heartbeat cycles, i.e., 20 seconds. CREATE SPACE CREATE TAG CREATE EDGE ALTER TAG ALTER EDGE CREATE TAG INDEX CREATE EDGE INDEX Note The default heartbeat interval is 10 seconds. To change the heartbeat interval, modify the heartbeat_interval_secs parameter in the configuration files for all services.","title":"Async implementation of CREATE and ALTER"},{"location":"2.quick-start/4.nebula-graph-crud/#create_and_use_a_graph_space","text":"","title":"Create and use a graph space"},{"location":"2.quick-start/4.nebula-graph-crud/#ngql_syntax","text":"Create a graph space: CREATE SPACE [IF NOT EXISTS] <graph_space_name> ( [partition_num = <partition_number>,] [replica_factor = <replica_number>,] vid_type = {FIXED_STRING(<N>) | INT64} ) [COMMENT = '<comment>']; For more information on parameters, see CREATE SPACE . List graph spaces and check if the creation is successful: nebula> SHOW SPACES; Use a graph space: USE <graph_space_name>;","title":"nGQL syntax"},{"location":"2.quick-start/4.nebula-graph-crud/#examples","text":"Use the following statement to create a graph space named basketballplayer . nebula> CREATE SPACE basketballplayer(partition_num=15, replica_factor=1, vid_type=fixed_string(30)); Check the partition distribution with SHOW HOSTS to make sure that the partitions are distributed in a balanced way. nebula> SHOW HOSTS; +-------------+-----------+-----------+-----------+--------------+----------------------------------+------------------------+---------+ | Host | Port | HTTP port | Status | Leader count | Leader distribution | Partition distribution | Version | +-------------+-----------+-----------+-----------+--------------+----------------------------------+------------------------+---------+ | \"storaged0\" | 9779 | 19669 | \"ONLINE\" | 5 | \"basketballplayer:5\" | \"basketballplayer:5\" | \"3.1.0\" | | \"storaged1\" | 9779 | 19669 | \"ONLINE\" | 5 | \"basketballplayer:5\" | \"basketballplayer:5\" | \"3.1.0\" | | \"storaged2\" | 9779 | 19669 | \"ONLINE\" | 5 | \"basketballplayer:5\" | \"basketballplayer:5\" | \"3.1.0\" | +-------------+-----------+-----------+-----------+--------------+----------------------------------+------------------------+---------+ If the Leader distribution is uneven, use BALANCE LEADER to redistribute the partitions. For more information, see BALANCE . Use the basketballplayer graph space. nebula[(none)]> USE basketballplayer; You can use SHOW SPACES to check the graph space you created. nebula> SHOW SPACES; +--------------------+ | Name | +--------------------+ | \"basketballplayer\" | +--------------------+","title":"Examples"},{"location":"2.quick-start/4.nebula-graph-crud/#create_tags_and_edge_types","text":"","title":"Create tags and edge types"},{"location":"2.quick-start/4.nebula-graph-crud/#ngql_syntax_1","text":"CREATE {TAG | EDGE} [IF NOT EXISTS] {<tag_name> | <edge_type_name>} ( <prop_name> <data_type> [NULL | NOT NULL] [DEFAULT <default_value>] [COMMENT '<comment>'] [{, <prop_name> <data_type> [NULL | NOT NULL] [DEFAULT <default_value>] [COMMENT '<comment>']} ...] ) [TTL_DURATION = <ttl_duration>] [TTL_COL = <prop_name>] [COMMENT = '<comment>']; For more information on parameters, see CREATE TAG and CREATE EDGE .","title":"nGQL syntax"},{"location":"2.quick-start/4.nebula-graph-crud/#examples_1","text":"Create tags player and team , and edge types follow and serve . Descriptions are as follows. Component name Type Property player Tag name (string), age (int) team Tag name (string) follow Edge type degree (int) serve Edge type start_year (int), end_year (int) nebula> CREATE TAG player(name string, age int); nebula> CREATE TAG team(name string); nebula> CREATE EDGE follow(degree int); nebula> CREATE EDGE serve(start_year int, end_year int);","title":"Examples"},{"location":"2.quick-start/4.nebula-graph-crud/#insert_vertices_and_edges","text":"You can use the INSERT statement to insert vertices or edges based on existing tags or edge types.","title":"Insert vertices and edges"},{"location":"2.quick-start/4.nebula-graph-crud/#ngql_syntax_2","text":"Insert vertices: INSERT VERTEX [IF NOT EXISTS] [tag_props, [tag_props] ...] VALUES <vid>: ([prop_value_list]) tag_props: tag_name ([prop_name_list]) prop_name_list: [prop_name [, prop_name] ...] prop_value_list: [prop_value [, prop_value] ...] vid is short for Vertex ID. A vid must be a unique string value in a graph space. For details, see INSERT VERTEX . Insert edges: INSERT EDGE [IF NOT EXISTS] <edge_type> ( <prop_name_list> ) VALUES <src_vid> -> <dst_vid>[@<rank>] : ( <prop_value_list> ) [, <src_vid> -> <dst_vid>[@<rank>] : ( <prop_value_list> ), ...]; <prop_name_list> ::= [ <prop_name> [, <prop_name> ] ...] <prop_value_list> ::= [ <prop_value> [, <prop_value> ] ...] For more information on parameters, see INSERT EDGE .","title":"nGQL syntax"},{"location":"2.quick-start/4.nebula-graph-crud/#examples_2","text":"Insert vertices representing basketball players and teams: nebula> INSERT VERTEX player(name, age) VALUES \"player100\":(\"Tim Duncan\", 42); nebula> INSERT VERTEX player(name, age) VALUES \"player101\":(\"Tony Parker\", 36); nebula> INSERT VERTEX player(name, age) VALUES \"player102\":(\"LaMarcus Aldridge\", 33); nebula> INSERT VERTEX team(name) VALUES \"team203\":(\"Trail Blazers\"), \"team204\":(\"Spurs\"); Insert edges representing the relations between basketball players and teams: nebula> INSERT EDGE follow(degree) VALUES \"player101\" -> \"player100\":(95); nebula> INSERT EDGE follow(degree) VALUES \"player101\" -> \"player102\":(90); nebula> INSERT EDGE follow(degree) VALUES \"player102\" -> \"player100\":(75); nebula> INSERT EDGE serve(start_year, end_year) VALUES \"player101\" -> \"team204\":(1999, 2018),\"player102\" -> \"team203\":(2006, 2015);","title":"Examples"},{"location":"2.quick-start/4.nebula-graph-crud/#read_data","text":"The GO statement can traverse the database based on specific conditions. A GO traversal starts from one or more vertices, along one or more edges, and returns information in a form specified in the YIELD clause. The FETCH statement is used to get properties from vertices or edges. The LOOKUP statement is based on indexes . It is used together with the WHERE clause to search for the data that meet the specific conditions. The MATCH statement is the most commonly used statement for graph data querying. It can describe all kinds of graph patterns, but it relies on indexes to match data patterns in Nebula Graph. Therefore, its performance still needs optimization.","title":"Read data"},{"location":"2.quick-start/4.nebula-graph-crud/#ngql_syntax_3","text":"GO GO [[<M> TO] <N> STEPS ] FROM <vertex_list> OVER <edge_type_list> [{REVERSELY | BIDIRECT}] [ WHERE <conditions> ] YIELD [DISTINCT] <return_list> [{ SAMPLE <sample_list> | <limit_by_list_clause> }] [| GROUP BY {<col_name> | expression> | <position>} YIELD <col_name>] [| ORDER BY <expression> [{ASC | DESC}]] [| LIMIT [<offset>,] <number_rows>]; FETCH Fetch properties on tags: FETCH PROP ON {<tag_name>[, tag_name ...] | *} <vid> [, vid ...] YIELD <return_list> [AS <alias>]; Fetch properties on edges: FETCH PROP ON <edge_type> <src_vid> -> <dst_vid>[@<rank>] [, <src_vid> -> <dst_vid> ...] YIELD <output>; LOOKUP LOOKUP ON {<vertex_tag> | <edge_type>} [WHERE <expression> [AND <expression> ...]] YIELD <return_list> [AS <alias>]; <return_list> <prop_name> [AS <col_alias>] [, <prop_name> [AS <prop_alias>] ...]; MATCH MATCH <pattern> [<clause_1>] RETURN <output> [<clause_2>];","title":"nGQL syntax"},{"location":"2.quick-start/4.nebula-graph-crud/#examples_of_go_statement","text":"Search for the players that the player with VID player101 follows. nebula> GO FROM \"player101\" OVER follow YIELD id($$); +-------------+ | id($$) | +-------------+ | \"player100\" | | \"player102\" | | \"player125\" | +-------------+ Filter the players that the player with VID player101 follows whose age is equal to or greater than 35. Rename the corresponding columns in the results with Teammate and Age . nebula> GO FROM \"player101\" OVER follow WHERE properties($$).age >= 35 \\ YIELD properties($$).name AS Teammate, properties($$).age AS Age; +-----------------+-----+ | Teammate | Age | +-----------------+-----+ | \"Tim Duncan\" | 42 | | \"Manu Ginobili\" | 41 | +-----------------+-----+ | Clause/Sign | Description | |-------------+---------------------------------------------------------------------| | YIELD | Specifies what values or results you want to return from the query. | | $$ | Represents the target vertices. | | \\ | A line-breaker. | Search for the players that the player with VID player101 follows. Then retrieve the teams of the players that the player with VID player100 follows. To combine the two queries, use a pipe or a temporary variable. With a pipe: nebula> GO FROM \"player101\" OVER follow YIELD dst(edge) AS id | \\ GO FROM $-.id OVER serve YIELD properties($$).name AS Team, \\ properties($^).name AS Player; +-----------------+---------------------+ | Team | Player | +-----------------+---------------------+ | \"Spurs\" | \"Tim Duncan\" | | \"Trail Blazers\" | \"LaMarcus Aldridge\" | | \"Spurs\" | \"LaMarcus Aldridge\" | | \"Spurs\" | \"Manu Ginobili\" | +-----------------+---------------------+ Clause/Sign Description $^ Represents the source vertex of the edge. | A pipe symbol can combine multiple queries. $- Represents the outputs of the query before the pipe symbol. With a temporary variable: Note Once a composite statement is submitted to the server as a whole, the life cycle of the temporary variables in the statement ends. nebula> $var = GO FROM \"player101\" OVER follow YIELD dst(edge) AS id; \\ GO FROM $var.id OVER serve YIELD properties($$).name AS Team, \\ properties($^).name AS Player; +-----------------+---------------------+ | Team | Player | +-----------------+---------------------+ | \"Spurs\" | \"Tim Duncan\" | | \"Trail Blazers\" | \"LaMarcus Aldridge\" | | \"Spurs\" | \"LaMarcus Aldridge\" | | \"Spurs\" | \"Manu Ginobili\" | +-----------------+---------------------+","title":"Examples of GO statement"},{"location":"2.quick-start/4.nebula-graph-crud/#example_of_fetch_statement","text":"Use FETCH : Fetch the properties of the player with VID player100 . nebula> FETCH PROP ON player \"player100\" YIELD properties(vertex); +-------------------------------+ | properties(VERTEX) | +-------------------------------+ | {age: 42, name: \"Tim Duncan\"} | +-------------------------------+ Note The examples of LOOKUP and MATCH statements are in indexes .","title":"Example of FETCH statement"},{"location":"2.quick-start/4.nebula-graph-crud/#update_vertices_and_edges","text":"Users can use the UPDATE or the UPSERT statements to update existing data. UPSERT is the combination of UPDATE and INSERT . If you update a vertex or an edge with UPSERT , the database will insert a new vertex or edge if it does not exist. Note UPSERT operates serially in a partition-based order. Therefore, it is slower than INSERT OR UPDATE . And UPSERT has concurrency only between multiple partitions.","title":"Update vertices and edges"},{"location":"2.quick-start/4.nebula-graph-crud/#ngql_syntax_4","text":"UPDATE vertices: UPDATE VERTEX <vid> SET <properties to be updated> [WHEN <condition>] [YIELD <columns>]; UPDATE edges: UPDATE EDGE <source vid> -> <destination vid> [@rank] OF <edge_type> SET <properties to be updated> [WHEN <condition>] [YIELD <columns to be output>]; UPSERT vertices or edges: UPSERT {VERTEX <vid> | EDGE <edge_type>} SET <update_columns> [WHEN <condition>] [YIELD <columns>];","title":"nGQL syntax"},{"location":"2.quick-start/4.nebula-graph-crud/#examples_3","text":"UPDATE the name property of the vertex with VID player100 and check the result with the FETCH statement. nebula> UPDATE VERTEX \"player100\" SET player.name = \"Tim\"; nebula> FETCH PROP ON player \"player100\" YIELD properties(vertex); +------------------------+ | properties(VERTEX) | +------------------------+ | {age: 42, name: \"Tim\"} | +------------------------+ UPDATE the degree property of an edge and check the result with the FETCH statement. nebula> UPDATE EDGE \"player101\" -> \"player100\" OF follow SET degree = 96; nebula> FETCH PROP ON follow \"player101\" -> \"player100\" YIELD properties(edge); +------------------+ | properties(EDGE) | +------------------+ | {degree: 96} | +------------------+ Insert a vertex with VID player111 and UPSERT it. nebula> INSERT VERTEX player(name,age) VALUES \"player111\":(\"David West\", 38); nebula> UPSERT VERTEX \"player111\" SET player.name = \"David\", player.age = $^.player.age + 11 \\ WHEN $^.player.name == \"David West\" AND $^.player.age > 20 \\ YIELD $^.player.name AS Name, $^.player.age AS Age; +---------+-----+ | Name | Age | +---------+-----+ | \"David\" | 49 | +---------+-----+","title":"Examples"},{"location":"2.quick-start/4.nebula-graph-crud/#delete_vertices_and_edges","text":"","title":"Delete vertices and edges"},{"location":"2.quick-start/4.nebula-graph-crud/#ngql_syntax_5","text":"Delete vertices: DELETE VERTEX <vid1>[, <vid2>...] Delete edges: DELETE EDGE <edge_type> <src_vid> -> <dst_vid>[@<rank>] [, <src_vid> -> <dst_vid>...]","title":"nGQL syntax"},{"location":"2.quick-start/4.nebula-graph-crud/#examples_4","text":"Delete vertices: nebula> DELETE VERTEX \"player111\", \"team203\"; Delete edges: nebula> DELETE EDGE follow \"player101\" -> \"team204\";","title":"Examples"},{"location":"2.quick-start/4.nebula-graph-crud/#about_indexes","text":"Users can add indexes to tags and edge types with the CREATE INDEX statement. Must-read for using indexes Both MATCH and LOOKUP statements depend on the indexes. But indexes can dramatically reduce the write performance (as much as 90% or even more). DO NOT use indexes in production environments unless you are fully aware of their influences on your service. Users MUST rebuild indexes for pre-existing data. Otherwise, the pre-existing data cannot be indexed and therefore cannot be returned in MATCH or LOOKUP statements. For more information, see REBUILD INDEX .","title":"About indexes"},{"location":"2.quick-start/4.nebula-graph-crud/#ngql_syntax_6","text":"Create an index: CREATE {TAG | EDGE} INDEX [IF NOT EXISTS] <index_name> ON {<tag_name> | <edge_name>} ([<prop_name_list>]) [COMMENT = '<comment>']; Rebuild an index: REBUILD {TAG | EDGE} INDEX <index_name>; Note Define the index length when creating an index for a variable-length property. In UTF-8 encoding, a non-ascii character occupies 3 bytes. You should set an appropriate index length according to the variable-length property. For example, the index should be 30 bytes for 10 non-ascii characters. For more information, see CREATE INDEX","title":"nGQL syntax"},{"location":"2.quick-start/4.nebula-graph-crud/#examples_of_lookup_and_match_index-based","text":"Make sure there is an index for LOOKUP or MATCH to use. If there is not, create an index first. Find the information of the vertex with the tag player and its value of the name property is Tony Parker . This example creates the index player_index_1 on the name property. nebula> CREATE TAG INDEX IF NOT EXISTS player_index_1 ON player(name(20)); This example rebuilds the index to make sure it takes effect on pre-existing data. nebula> REBUILD TAG INDEX player_index_1 +------------+ | New Job Id | +------------+ | 31 | +------------+ This example uses the LOOKUP statement to retrieve the vertex property. nebula> LOOKUP ON player WHERE player.name == \"Tony Parker\" \\ YIELD properties(vertex).name AS name, properties(vertex).age AS age; +---------------+-----+ | name | age | +---------------+-----+ | \"Tony Parker\" | 36 | +---------------+-----+ This example uses the MATCH statement to retrieve the vertex property. nebula> MATCH (v:player{name:\"Tony Parker\"}) RETURN v; +-----------------------------------------------------+ | v | +-----------------------------------------------------+ | (\"player101\" :player{age: 36, name: \"Tony Parker\"}) | +-----------------------------------------------------+","title":"Examples of LOOKUP and MATCH (index-based)"},{"location":"2.quick-start/5.start-stop-service/","text":"Step 2: Manage Nebula Graph Service \u00b6 Nebula Graph supports managing services with scripts or systemd. This topic will describe the two methods in detail. Enterpriseonly Managing Nebula Graph with systemd is only available in the Nebula Graph Enterprise Edition. Danger The two methods are incompatible. It is recommended to use only one method in a cluster. Manage services with script \u00b6 You can use the nebula.service script to start, stop, restart, terminate, and check the Nebula Graph services. Note nebula.service is stored in the /usr/local/nebula/scripts directory by default. If you have customized the path, use the actual path in your environment. Syntax \u00b6 $ sudo /usr/local/nebula/scripts/nebula.service [ -v ] [ -c <config_file_path> ] <start | stop | restart | kill | status> <metad | graphd | storaged | all> Parameter Description -v Display detailed debugging information. -c Specify the configuration file path. The default path is /usr/local/nebula/etc/ . start Start the target services. stop Stop the target services. restart Restart the target services. kill Terminate the target services. status Check the status of the target services. metad Set the Meta Service as the target service. graphd Set the Graph Service as the target service. storaged Set the Storage Service as the target service. all Set all the Nebula Graph services as the target services. Manage services with systemd \u00b6 For easy maintenance, Nebula Graph supports managing services with systemd. You can start, stop, restart, and check services with systemctl commands. Note After installing Nebula Graph, the .service files required by systemd are located in the etc/unit path in the installation directory. Nebula Graph installed with the RPM/DEB package automatically places the .service files into the path /usr/lib/systemd/system and the parameter ExecStart is generated based on the specified Nebula Graph installation path, so you can use systemctl commands directly. Otherwise, users need to move the .service files manually into the directory /usr/lib/systemd/system , and modify the file path of the parameter ExecStart in the .service files. Syntax \u00b6 $ systemctl <start | stop | restart | status > <nebula | nebula-metad | nebula-graphd | nebula-storaged> Parameter Description start Start the target services. stop Stop the target services. restart Restart the target services. status Check the status of the target services. nebula Set all the Nebula Graph services as the target services. nebula-metad Set the Meta Service as the target service. nebula-graphd Set the Graph Service as the target service. nebula-storaged Set the Storage Service as the target service. Start Nebula Graph \u00b6 In non-container environment \u00b6 Run the following command to start Nebula Graph. $ sudo /usr/local/nebula/scripts/nebula.service start all [ INFO ] Starting nebula-metad... [ INFO ] Done [ INFO ] Starting nebula-graphd... [ INFO ] Done [ INFO ] Starting nebula-storaged... [ INFO ] Done Users can also run the following command: $ systemctl start nebula If users want to automatically start Nebula Graph when the machine starts, run the following command: $ systemctl enable nebula In docker container (deployed with docker-compose) \u00b6 Run the following command in the nebula-docker-compose/ directory to start Nebula Graph. [ nebula-docker-compose ] $ docker-compose up -d Building with native build. Learn about native build in Compose here: https://docs.docker.com/go/compose-native-build/ Creating network \"nebula-docker-compose_nebula-net\" with the default driver Creating nebula-docker-compose_metad0_1 ... done Creating nebula-docker-compose_metad2_1 ... done Creating nebula-docker-compose_metad1_1 ... done Creating nebula-docker-compose_storaged2_1 ... done Creating nebula-docker-compose_graphd1_1 ... done Creating nebula-docker-compose_storaged1_1 ... done Creating nebula-docker-compose_storaged0_1 ... done Creating nebula-docker-compose_graphd2_1 ... done Creating nebula-docker-compose_graphd_1 ... done Stop Nebula Graph \u00b6 Danger Do not run kill -9 to forcibly terminate the processes. Otherwise, there is a low probability of data loss. In non-container environment \u00b6 Run the following command to stop Nebula Graph. $ sudo /usr/local/nebula/scripts/nebula.service stop all [ INFO ] Stopping nebula-metad... [ INFO ] Done [ INFO ] Stopping nebula-graphd... [ INFO ] Done [ INFO ] Stopping nebula-storaged... [ INFO ] Done Users can also run the following command: $ systemctl stop nebula In docker container (deployed with docker-compose) \u00b6 Run the following command in the nebula-docker-compose/ directory to stop Nebula Graph. nebula-docker-compose ] $ docker-compose down Stopping nebula-docker-compose_graphd_1 ... done Stopping nebula-docker-compose_graphd2_1 ... done Stopping nebula-docker-compose_storaged0_1 ... done Stopping nebula-docker-compose_storaged1_1 ... done Stopping nebula-docker-compose_graphd1_1 ... done Stopping nebula-docker-compose_storaged2_1 ... done Stopping nebula-docker-compose_metad1_1 ... done Stopping nebula-docker-compose_metad2_1 ... done Stopping nebula-docker-compose_metad0_1 ... done Removing nebula-docker-compose_graphd_1 ... done Removing nebula-docker-compose_graphd2_1 ... done Removing nebula-docker-compose_storaged0_1 ... done Removing nebula-docker-compose_storaged1_1 ... done Removing nebula-docker-compose_graphd1_1 ... done Removing nebula-docker-compose_storaged2_1 ... done Removing nebula-docker-compose_metad1_1 ... done Removing nebula-docker-compose_metad2_1 ... done Removing nebula-docker-compose_metad0_1 ... done Removing network nebula-docker-compose_nebula-net Note If you are using a developing or nightly version for testing and have compatibility issues, try to run docker-compose down -v to DELETE all data stored in Nebula Graph and import data again. Check the service status \u00b6 In non-container environment \u00b6 Run the following command to check the service status of Nebula Graph. $ sudo /usr/local/nebula/scripts/nebula.service status all Nebula Graph is running normally if the following information is returned. [ INFO ] nebula-metad ( 02b2091 ) : Running as 26601 , Listening on 9559 [ INFO ] nebula-graphd ( 02b2091 ) : Running as 26644 , Listening on 9669 [ INFO ] nebula-storaged ( 02b2091 ) : Running as 26709 , Listening on 9779 Note After starting Nebula Graph, the port of the nebula-storaged process is shown in red. Because the nebula-storaged process waits for the nebula-metad to add the current Storage service during the startup process. The Storage works after it receives the ready signal. Starting from Nebula Graph 3.0.0, the Meta service cannot directly read or write data in the Storage service that you add in the configuration file. The configuration file only registers the Storage service to the Meta service. You must run the ADD HOSTS command to enable the Meta to read and write data in the Storage service. For more information, see Manage Storage hosts . If the returned result is similar to the following one, there is a problem. You may also go to the Nebula Graph community for help. [ INFO ] nebula-metad: Running as 25600 , Listening on 9559 [ INFO ] nebula-graphd: Exited [ INFO ] nebula-storaged: Running as 25646 , Listening on 9779 Users can also run the following command: $ systemctl status nebula \u25cf nebula.service Loaded: loaded ( /usr/lib/systemd/system/nebula.service ; disabled ; vendor preset: disabled ) Active: active ( exited ) since \u4e00 2022 -03-28 04 :13:24 UTC ; 1h 47min ago Process: 21772 ExecStart = /usr/local/ent-nightly/scripts/nebula.service start all ( code = exited, status = 0 /SUCCESS ) Main PID: 21772 ( code = exited, status = 0 /SUCCESS ) Tasks: 325 Memory: 424 .5M CGroup: /system.slice/nebula.service \u251c\u250021789 /usr/local/ent-nightly/bin/nebula-metad --flagfile /usr/local/ent-nightly/etc/nebula-metad.conf \u251c\u250021827 /usr/local/ent-nightly/bin/nebula-graphd --flagfile /usr/local/ent-nightly/etc/nebula-graphd.conf \u2514\u250021900 /usr/local/ent-nightly/bin/nebula-storaged --flagfile /usr/local/ent-nightly/etc/nebula-storaged.conf 3\u6708 28 04 :13:24 xxxxxx systemd [ 1 ] : Started nebula.service. ... The Nebula Graph services consist of the Meta Service, Graph Service, and Storage Service. The configuration files for all three services are stored in the /usr/local/nebula/etc/ directory by default. You can check the configuration files according to the returned result to troubleshoot problems. In docker container (deployed with docker-compose) \u00b6 Run the following command in the nebula-docker-compose/ directory to check the service status of Nebula Graph. nebula-docker-compose ] $ docker-compose ps Name Command State Ports --------------------------------------------------------------------------------------------------------------------------------------------------------------------- nebula-docker-compose_graphd1_1 /usr/local/nebula/bin/nebu ... Up ( healthy ) 0 .0.0.0:49223->19669/tcp, 0 .0.0.0:49222->19670/tcp, 0 .0.0.0:49224->9669/tcp nebula-docker-compose_graphd2_1 /usr/local/nebula/bin/nebu ... Up ( healthy ) 0 .0.0.0:49229->19669/tcp, 0 .0.0.0:49228->19670/tcp, 0 .0.0.0:49230->9669/tcp nebula-docker-compose_graphd_1 /usr/local/nebula/bin/nebu ... Up ( healthy ) 0 .0.0.0:49221->19669/tcp, 0 .0.0.0:49220->19670/tcp, 0 .0.0.0:9669->9669/tcp nebula-docker-compose_metad0_1 ./bin/nebula-metad --flagf ... Up ( healthy ) 0 .0.0.0:49212->19559/tcp, 0 .0.0.0:49211->19560/tcp, 0 .0.0.0:49213->9559/tcp, 9560 /tcp nebula-docker-compose_metad1_1 ./bin/nebula-metad --flagf ... Up ( healthy ) 0 .0.0.0:49209->19559/tcp, 0 .0.0.0:49208->19560/tcp, 0 .0.0.0:49210->9559/tcp, 9560 /tcp nebula-docker-compose_metad2_1 ./bin/nebula-metad --flagf ... Up ( healthy ) 0 .0.0.0:49206->19559/tcp, 0 .0.0.0:49205->19560/tcp, 0 .0.0.0:49207->9559/tcp, 9560 /tcp nebula-docker-compose_storaged0_1 ./bin/nebula-storaged --fl ... Up ( healthy ) 0 .0.0.0:49218->19779/tcp, 0 .0.0.0:49217->19780/tcp, 9777 /tcp, 9778 /tcp, 0 .0.0.0:49219->9779/tcp, 9780 /tcp nebula-docker-compose_storaged1_1 ./bin/nebula-storaged --fl ... Up ( healthy ) 0 .0.0.0:49215->19779/tcp, 0 .0.0.0:49214->19780/tcp, 9777 /tcp, 9778 /tcp, 0 .0.0.0:49216->9779/tcp, 9780 /tcp nebula-docker-compose_storaged2_1 ./bin/nebula-storaged --fl ... Up ( healthy ) 0 .0.0.0:49226->19779/tcp, 0 .0.0.0:49225->19780/tcp, 9777 /tcp, 9778 /tcp, 0 .0.0.0:49227->9779/tcp, 9780 /tcp If the service is abnormal, you can first confirm the abnormal container name (such as nebula-docker-compose_graphd2_1 ). Then you can execute docker ps to view the corresponding CONTAINER ID (such as 2a6c56c405f5 ). [ nebula-docker-compose ] $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 2a6c56c405f5 vesoft/nebula-graphd:nightly \"/usr/local/nebula/b\u2026\" 36 minutes ago Up 36 minutes ( healthy ) 0 .0.0.0:49230->9669/tcp, 0 .0.0.0:49229->19669/tcp, 0 .0.0.0:49228->19670/tcp nebula-docker-compose_graphd2_1 7042e0a8e83d vesoft/nebula-storaged:nightly \"./bin/nebula-storag\u2026\" 36 minutes ago Up 36 minutes ( healthy ) 9777 -9778/tcp, 9780 /tcp, 0 .0.0.0:49227->9779/tcp, 0 .0.0.0:49226->19779/tcp, 0 .0.0.0:49225->19780/tcp nebula-docker-compose_storaged2_1 18e3ea63ad65 vesoft/nebula-storaged:nightly \"./bin/nebula-storag\u2026\" 36 minutes ago Up 36 minutes ( healthy ) 9777 -9778/tcp, 9780 /tcp, 0 .0.0.0:49219->9779/tcp, 0 .0.0.0:49218->19779/tcp, 0 .0.0.0:49217->19780/tcp nebula-docker-compose_storaged0_1 4dcabfe8677a vesoft/nebula-graphd:nightly \"/usr/local/nebula/b\u2026\" 36 minutes ago Up 36 minutes ( healthy ) 0 .0.0.0:49224->9669/tcp, 0 .0.0.0:49223->19669/tcp, 0 .0.0.0:49222->19670/tcp nebula-docker-compose_graphd1_1 a74054c6ae25 vesoft/nebula-graphd:nightly \"/usr/local/nebula/b\u2026\" 36 minutes ago Up 36 minutes ( healthy ) 0 .0.0.0:9669->9669/tcp, 0 .0.0.0:49221->19669/tcp, 0 .0.0.0:49220->19670/tcp nebula-docker-compose_graphd_1 880025a3858c vesoft/nebula-storaged:nightly \"./bin/nebula-storag\u2026\" 36 minutes ago Up 36 minutes ( healthy ) 9777 -9778/tcp, 9780 /tcp, 0 .0.0.0:49216->9779/tcp, 0 .0.0.0:49215->19779/tcp, 0 .0.0.0:49214->19780/tcp nebula-docker-compose_storaged1_1 45736a32a23a vesoft/nebula-metad:nightly \"./bin/nebula-metad \u2026\" 36 minutes ago Up 36 minutes ( healthy ) 9560 /tcp, 0 .0.0.0:49213->9559/tcp, 0 .0.0.0:49212->19559/tcp, 0 .0.0.0:49211->19560/tcp nebula-docker-compose_metad0_1 3b2c90eb073e vesoft/nebula-metad:nightly \"./bin/nebula-metad \u2026\" 36 minutes ago Up 36 minutes ( healthy ) 9560 /tcp, 0 .0.0.0:49207->9559/tcp, 0 .0.0.0:49206->19559/tcp, 0 .0.0.0:49205->19560/tcp nebula-docker-compose_metad2_1 7bb31b7a5b3f vesoft/nebula-metad:nightly \"./bin/nebula-metad \u2026\" 36 minutes ago Up 36 minutes ( healthy ) 9560 /tcp, 0 .0.0.0:49210->9559/tcp, 0 .0.0.0:49209->19559/tcp, 0 .0.0.0:49208->19560/tcp nebula-docker-compose_metad1_1 Use the CONTAINER ID to log in the container and troubleshoot. nebula-docker-compose ] $ docker exec -it 2a6c56c405f5 bash [ root@2a6c56c405f5 nebula ] # Next to do \u00b6 Connect to Nebula Graph","title":"Step 2 Manage Nebula Graph Service"},{"location":"2.quick-start/5.start-stop-service/#step_2_manage_nebula_graph_service","text":"Nebula Graph supports managing services with scripts or systemd. This topic will describe the two methods in detail. Enterpriseonly Managing Nebula Graph with systemd is only available in the Nebula Graph Enterprise Edition. Danger The two methods are incompatible. It is recommended to use only one method in a cluster.","title":"Step 2: Manage Nebula Graph Service"},{"location":"2.quick-start/5.start-stop-service/#manage_services_with_script","text":"You can use the nebula.service script to start, stop, restart, terminate, and check the Nebula Graph services. Note nebula.service is stored in the /usr/local/nebula/scripts directory by default. If you have customized the path, use the actual path in your environment.","title":"Manage services with script"},{"location":"2.quick-start/5.start-stop-service/#syntax","text":"$ sudo /usr/local/nebula/scripts/nebula.service [ -v ] [ -c <config_file_path> ] <start | stop | restart | kill | status> <metad | graphd | storaged | all> Parameter Description -v Display detailed debugging information. -c Specify the configuration file path. The default path is /usr/local/nebula/etc/ . start Start the target services. stop Stop the target services. restart Restart the target services. kill Terminate the target services. status Check the status of the target services. metad Set the Meta Service as the target service. graphd Set the Graph Service as the target service. storaged Set the Storage Service as the target service. all Set all the Nebula Graph services as the target services.","title":"Syntax"},{"location":"2.quick-start/5.start-stop-service/#manage_services_with_systemd","text":"For easy maintenance, Nebula Graph supports managing services with systemd. You can start, stop, restart, and check services with systemctl commands. Note After installing Nebula Graph, the .service files required by systemd are located in the etc/unit path in the installation directory. Nebula Graph installed with the RPM/DEB package automatically places the .service files into the path /usr/lib/systemd/system and the parameter ExecStart is generated based on the specified Nebula Graph installation path, so you can use systemctl commands directly. Otherwise, users need to move the .service files manually into the directory /usr/lib/systemd/system , and modify the file path of the parameter ExecStart in the .service files.","title":"Manage services with systemd"},{"location":"2.quick-start/5.start-stop-service/#syntax_1","text":"$ systemctl <start | stop | restart | status > <nebula | nebula-metad | nebula-graphd | nebula-storaged> Parameter Description start Start the target services. stop Stop the target services. restart Restart the target services. status Check the status of the target services. nebula Set all the Nebula Graph services as the target services. nebula-metad Set the Meta Service as the target service. nebula-graphd Set the Graph Service as the target service. nebula-storaged Set the Storage Service as the target service.","title":"Syntax"},{"location":"2.quick-start/5.start-stop-service/#start_nebula_graph","text":"","title":"Start Nebula Graph"},{"location":"2.quick-start/5.start-stop-service/#in_non-container_environment","text":"Run the following command to start Nebula Graph. $ sudo /usr/local/nebula/scripts/nebula.service start all [ INFO ] Starting nebula-metad... [ INFO ] Done [ INFO ] Starting nebula-graphd... [ INFO ] Done [ INFO ] Starting nebula-storaged... [ INFO ] Done Users can also run the following command: $ systemctl start nebula If users want to automatically start Nebula Graph when the machine starts, run the following command: $ systemctl enable nebula","title":"In non-container environment"},{"location":"2.quick-start/5.start-stop-service/#in_docker_container_deployed_with_docker-compose","text":"Run the following command in the nebula-docker-compose/ directory to start Nebula Graph. [ nebula-docker-compose ] $ docker-compose up -d Building with native build. Learn about native build in Compose here: https://docs.docker.com/go/compose-native-build/ Creating network \"nebula-docker-compose_nebula-net\" with the default driver Creating nebula-docker-compose_metad0_1 ... done Creating nebula-docker-compose_metad2_1 ... done Creating nebula-docker-compose_metad1_1 ... done Creating nebula-docker-compose_storaged2_1 ... done Creating nebula-docker-compose_graphd1_1 ... done Creating nebula-docker-compose_storaged1_1 ... done Creating nebula-docker-compose_storaged0_1 ... done Creating nebula-docker-compose_graphd2_1 ... done Creating nebula-docker-compose_graphd_1 ... done","title":"In docker container (deployed with docker-compose)"},{"location":"2.quick-start/5.start-stop-service/#stop_nebula_graph","text":"Danger Do not run kill -9 to forcibly terminate the processes. Otherwise, there is a low probability of data loss.","title":"Stop Nebula Graph"},{"location":"2.quick-start/5.start-stop-service/#in_non-container_environment_1","text":"Run the following command to stop Nebula Graph. $ sudo /usr/local/nebula/scripts/nebula.service stop all [ INFO ] Stopping nebula-metad... [ INFO ] Done [ INFO ] Stopping nebula-graphd... [ INFO ] Done [ INFO ] Stopping nebula-storaged... [ INFO ] Done Users can also run the following command: $ systemctl stop nebula","title":"In non-container environment"},{"location":"2.quick-start/5.start-stop-service/#in_docker_container_deployed_with_docker-compose_1","text":"Run the following command in the nebula-docker-compose/ directory to stop Nebula Graph. nebula-docker-compose ] $ docker-compose down Stopping nebula-docker-compose_graphd_1 ... done Stopping nebula-docker-compose_graphd2_1 ... done Stopping nebula-docker-compose_storaged0_1 ... done Stopping nebula-docker-compose_storaged1_1 ... done Stopping nebula-docker-compose_graphd1_1 ... done Stopping nebula-docker-compose_storaged2_1 ... done Stopping nebula-docker-compose_metad1_1 ... done Stopping nebula-docker-compose_metad2_1 ... done Stopping nebula-docker-compose_metad0_1 ... done Removing nebula-docker-compose_graphd_1 ... done Removing nebula-docker-compose_graphd2_1 ... done Removing nebula-docker-compose_storaged0_1 ... done Removing nebula-docker-compose_storaged1_1 ... done Removing nebula-docker-compose_graphd1_1 ... done Removing nebula-docker-compose_storaged2_1 ... done Removing nebula-docker-compose_metad1_1 ... done Removing nebula-docker-compose_metad2_1 ... done Removing nebula-docker-compose_metad0_1 ... done Removing network nebula-docker-compose_nebula-net Note If you are using a developing or nightly version for testing and have compatibility issues, try to run docker-compose down -v to DELETE all data stored in Nebula Graph and import data again.","title":"In docker container (deployed with docker-compose)"},{"location":"2.quick-start/5.start-stop-service/#check_the_service_status","text":"","title":"Check the service status"},{"location":"2.quick-start/5.start-stop-service/#in_non-container_environment_2","text":"Run the following command to check the service status of Nebula Graph. $ sudo /usr/local/nebula/scripts/nebula.service status all Nebula Graph is running normally if the following information is returned. [ INFO ] nebula-metad ( 02b2091 ) : Running as 26601 , Listening on 9559 [ INFO ] nebula-graphd ( 02b2091 ) : Running as 26644 , Listening on 9669 [ INFO ] nebula-storaged ( 02b2091 ) : Running as 26709 , Listening on 9779 Note After starting Nebula Graph, the port of the nebula-storaged process is shown in red. Because the nebula-storaged process waits for the nebula-metad to add the current Storage service during the startup process. The Storage works after it receives the ready signal. Starting from Nebula Graph 3.0.0, the Meta service cannot directly read or write data in the Storage service that you add in the configuration file. The configuration file only registers the Storage service to the Meta service. You must run the ADD HOSTS command to enable the Meta to read and write data in the Storage service. For more information, see Manage Storage hosts . If the returned result is similar to the following one, there is a problem. You may also go to the Nebula Graph community for help. [ INFO ] nebula-metad: Running as 25600 , Listening on 9559 [ INFO ] nebula-graphd: Exited [ INFO ] nebula-storaged: Running as 25646 , Listening on 9779 Users can also run the following command: $ systemctl status nebula \u25cf nebula.service Loaded: loaded ( /usr/lib/systemd/system/nebula.service ; disabled ; vendor preset: disabled ) Active: active ( exited ) since \u4e00 2022 -03-28 04 :13:24 UTC ; 1h 47min ago Process: 21772 ExecStart = /usr/local/ent-nightly/scripts/nebula.service start all ( code = exited, status = 0 /SUCCESS ) Main PID: 21772 ( code = exited, status = 0 /SUCCESS ) Tasks: 325 Memory: 424 .5M CGroup: /system.slice/nebula.service \u251c\u250021789 /usr/local/ent-nightly/bin/nebula-metad --flagfile /usr/local/ent-nightly/etc/nebula-metad.conf \u251c\u250021827 /usr/local/ent-nightly/bin/nebula-graphd --flagfile /usr/local/ent-nightly/etc/nebula-graphd.conf \u2514\u250021900 /usr/local/ent-nightly/bin/nebula-storaged --flagfile /usr/local/ent-nightly/etc/nebula-storaged.conf 3\u6708 28 04 :13:24 xxxxxx systemd [ 1 ] : Started nebula.service. ... The Nebula Graph services consist of the Meta Service, Graph Service, and Storage Service. The configuration files for all three services are stored in the /usr/local/nebula/etc/ directory by default. You can check the configuration files according to the returned result to troubleshoot problems.","title":"In non-container environment"},{"location":"2.quick-start/5.start-stop-service/#in_docker_container_deployed_with_docker-compose_2","text":"Run the following command in the nebula-docker-compose/ directory to check the service status of Nebula Graph. nebula-docker-compose ] $ docker-compose ps Name Command State Ports --------------------------------------------------------------------------------------------------------------------------------------------------------------------- nebula-docker-compose_graphd1_1 /usr/local/nebula/bin/nebu ... Up ( healthy ) 0 .0.0.0:49223->19669/tcp, 0 .0.0.0:49222->19670/tcp, 0 .0.0.0:49224->9669/tcp nebula-docker-compose_graphd2_1 /usr/local/nebula/bin/nebu ... Up ( healthy ) 0 .0.0.0:49229->19669/tcp, 0 .0.0.0:49228->19670/tcp, 0 .0.0.0:49230->9669/tcp nebula-docker-compose_graphd_1 /usr/local/nebula/bin/nebu ... Up ( healthy ) 0 .0.0.0:49221->19669/tcp, 0 .0.0.0:49220->19670/tcp, 0 .0.0.0:9669->9669/tcp nebula-docker-compose_metad0_1 ./bin/nebula-metad --flagf ... Up ( healthy ) 0 .0.0.0:49212->19559/tcp, 0 .0.0.0:49211->19560/tcp, 0 .0.0.0:49213->9559/tcp, 9560 /tcp nebula-docker-compose_metad1_1 ./bin/nebula-metad --flagf ... Up ( healthy ) 0 .0.0.0:49209->19559/tcp, 0 .0.0.0:49208->19560/tcp, 0 .0.0.0:49210->9559/tcp, 9560 /tcp nebula-docker-compose_metad2_1 ./bin/nebula-metad --flagf ... Up ( healthy ) 0 .0.0.0:49206->19559/tcp, 0 .0.0.0:49205->19560/tcp, 0 .0.0.0:49207->9559/tcp, 9560 /tcp nebula-docker-compose_storaged0_1 ./bin/nebula-storaged --fl ... Up ( healthy ) 0 .0.0.0:49218->19779/tcp, 0 .0.0.0:49217->19780/tcp, 9777 /tcp, 9778 /tcp, 0 .0.0.0:49219->9779/tcp, 9780 /tcp nebula-docker-compose_storaged1_1 ./bin/nebula-storaged --fl ... Up ( healthy ) 0 .0.0.0:49215->19779/tcp, 0 .0.0.0:49214->19780/tcp, 9777 /tcp, 9778 /tcp, 0 .0.0.0:49216->9779/tcp, 9780 /tcp nebula-docker-compose_storaged2_1 ./bin/nebula-storaged --fl ... Up ( healthy ) 0 .0.0.0:49226->19779/tcp, 0 .0.0.0:49225->19780/tcp, 9777 /tcp, 9778 /tcp, 0 .0.0.0:49227->9779/tcp, 9780 /tcp If the service is abnormal, you can first confirm the abnormal container name (such as nebula-docker-compose_graphd2_1 ). Then you can execute docker ps to view the corresponding CONTAINER ID (such as 2a6c56c405f5 ). [ nebula-docker-compose ] $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 2a6c56c405f5 vesoft/nebula-graphd:nightly \"/usr/local/nebula/b\u2026\" 36 minutes ago Up 36 minutes ( healthy ) 0 .0.0.0:49230->9669/tcp, 0 .0.0.0:49229->19669/tcp, 0 .0.0.0:49228->19670/tcp nebula-docker-compose_graphd2_1 7042e0a8e83d vesoft/nebula-storaged:nightly \"./bin/nebula-storag\u2026\" 36 minutes ago Up 36 minutes ( healthy ) 9777 -9778/tcp, 9780 /tcp, 0 .0.0.0:49227->9779/tcp, 0 .0.0.0:49226->19779/tcp, 0 .0.0.0:49225->19780/tcp nebula-docker-compose_storaged2_1 18e3ea63ad65 vesoft/nebula-storaged:nightly \"./bin/nebula-storag\u2026\" 36 minutes ago Up 36 minutes ( healthy ) 9777 -9778/tcp, 9780 /tcp, 0 .0.0.0:49219->9779/tcp, 0 .0.0.0:49218->19779/tcp, 0 .0.0.0:49217->19780/tcp nebula-docker-compose_storaged0_1 4dcabfe8677a vesoft/nebula-graphd:nightly \"/usr/local/nebula/b\u2026\" 36 minutes ago Up 36 minutes ( healthy ) 0 .0.0.0:49224->9669/tcp, 0 .0.0.0:49223->19669/tcp, 0 .0.0.0:49222->19670/tcp nebula-docker-compose_graphd1_1 a74054c6ae25 vesoft/nebula-graphd:nightly \"/usr/local/nebula/b\u2026\" 36 minutes ago Up 36 minutes ( healthy ) 0 .0.0.0:9669->9669/tcp, 0 .0.0.0:49221->19669/tcp, 0 .0.0.0:49220->19670/tcp nebula-docker-compose_graphd_1 880025a3858c vesoft/nebula-storaged:nightly \"./bin/nebula-storag\u2026\" 36 minutes ago Up 36 minutes ( healthy ) 9777 -9778/tcp, 9780 /tcp, 0 .0.0.0:49216->9779/tcp, 0 .0.0.0:49215->19779/tcp, 0 .0.0.0:49214->19780/tcp nebula-docker-compose_storaged1_1 45736a32a23a vesoft/nebula-metad:nightly \"./bin/nebula-metad \u2026\" 36 minutes ago Up 36 minutes ( healthy ) 9560 /tcp, 0 .0.0.0:49213->9559/tcp, 0 .0.0.0:49212->19559/tcp, 0 .0.0.0:49211->19560/tcp nebula-docker-compose_metad0_1 3b2c90eb073e vesoft/nebula-metad:nightly \"./bin/nebula-metad \u2026\" 36 minutes ago Up 36 minutes ( healthy ) 9560 /tcp, 0 .0.0.0:49207->9559/tcp, 0 .0.0.0:49206->19559/tcp, 0 .0.0.0:49205->19560/tcp nebula-docker-compose_metad2_1 7bb31b7a5b3f vesoft/nebula-metad:nightly \"./bin/nebula-metad \u2026\" 36 minutes ago Up 36 minutes ( healthy ) 9560 /tcp, 0 .0.0.0:49210->9559/tcp, 0 .0.0.0:49209->19559/tcp, 0 .0.0.0:49208->19560/tcp nebula-docker-compose_metad1_1 Use the CONTAINER ID to log in the container and troubleshoot. nebula-docker-compose ] $ docker exec -it 2a6c56c405f5 bash [ root@2a6c56c405f5 nebula ] #","title":"In docker container (deployed with docker-compose)"},{"location":"2.quick-start/5.start-stop-service/#next_to_do","text":"Connect to Nebula Graph","title":"Next to do"},{"location":"2.quick-start/6.cheatsheet-for-ngql/","text":"nGQL cheatsheet \u00b6 Functions \u00b6 Math functions Function Description double abs(double x) Returns the absolute value of the argument. double floor(double x) Returns the largest integer value smaller than or equal to the argument. (Rounds down) double ceil(double x) Returns the smallest integer greater than or equal to the argument. (Rounds up) double round(double x) Returns the integer value nearest to the argument. Returns a number farther away from 0 if the argument is in the middle. double sqrt(double x) Returns the square root of the argument. double cbrt(double x) Returns the cubic root of the argument. double hypot(double x, double y) Returns the hypotenuse of a right-angled triangle. double pow(double x, double y) Returns the result of \\(x^y\\) . double exp(double x) Returns the result of \\(e^x\\) . double exp2(double x) Returns the result of \\(2^x\\) . double log(double x) Returns the base-e logarithm of the argument. double log2(double x) Returns the base-2 logarithm of the argument. double log10(double x) Returns the base-10 logarithm of the argument. double sin(double x) Returns the sine of the argument. double asin(double x) Returns the inverse sine of the argument. double cos(double x) Returns the cosine of the argument. double acos(double x) Returns the inverse cosine of the argument. double tan(double x) Returns the tangent of the argument. double atan(double x) Returns the inverse tangent of the argument. double rand() Returns a random floating point number in the range from 0 (inclusive) to 1 (exclusive); i.e.[0,1). int rand32(int min, int max) Returns a random 32-bit integer in [min, max) . If you set only one argument, it is parsed as max and min is 0 by default. If you set no argument, the system returns a random signed 32-bit integer. int rand64(int min, int max) Returns a random 64-bit integer in [min, max) . If you set only one argument, it is parsed as max and min is 0 by default. If you set no argument, the system returns a random signed 64-bit integer. collect() Puts all the collected values into a list. avg() Returns the average value of the argument. count() Returns the number of records. max() Returns the maximum value. min() Returns the minimum value. std() Returns the population standard deviation. sum() Returns the sum value. bit_and() Bitwise AND. bit_or() Bitwise OR. bit_xor() Bitwise XOR. int size() Returns the number of elements in a list or a map. int range(int start, int end, int step) Returns a list of integers from [start,end] in the specified steps. step is 1 by default. int sign(double x) Returns the signum of the given number. If the number is 0 , the system returns 0 . If the number is negative, the system returns -1 . If the number is positive, the system returns 1 . double e() Returns the base of the natural logarithm, e (2.718281828459045). double pi() Returns the mathematical constant pi (3.141592653589793). double radians() Converts degrees to radians. radians(180) returns 3.141592653589793 . String functions Function Description int strcasecmp(string a, string b) Compares string a and b without case sensitivity. When a = b, the return value is 0. When a > b, the return value is greater than 0. When a < b, the return value is less than 0. string lower(string a) Returns the argument in lowercase. string toLower(string a) The same as lower() . string upper(string a) Returns the argument in uppercase. string toUpper(string a) The same as upper() . int length(string a) Returns the length of the given string in bytes. string trim(string a) Removes leading and trailing spaces. string ltrim(string a) Removes leading spaces. string rtrim(string a) Removes trailing spaces. string left(string a, int count) Returns a substring consisting of count characters from the left side of string a. If string a is shorter than count , the system returns string a. string right(string a, int count) Returns a substring consisting of count characters from the right side of string a. If string a is shorter than count , the system returns string a. string lpad(string a, int size, string letters) Left-pads string a with string letters and returns a substring with the length of size . string rpad(string a, int size, string letters) Right-pads string a with string letters and returns a substring with the length of size . string substr(string a, int pos, int count) Returns a substring extracting count characters starting from the specified position pos of string a. string substring(string a, int pos, int count) The same as substr() . string reverse(string) Returns a string in reverse order. string replace(string a, string b, string c) Replaces string b in string a with string c. list split(string a, string b) Splits string a at string b and returns a list of strings. string toString() Takes in any data type and converts it into a string. int hash() Takes in any data type and encodes it into a hash value. Data and time functions Function Description int now() Returns the current date and time of the system timezone. timestamp timestamp() Returns the current date and time of the system timezone. date date() Returns the current UTC date based on the current system. time time() Returns the current UTC time based on the current system. datetime datetime() Returns the current UTC date and time based on the current system. Schema functions For nGQL statements Function Description id(vertex) Returns the ID of a vertex. The data type of the result is the same as the vertex ID. map properties(vertex) Returns the properties of a vertex. map properties(edge) Returns the properties of an edge. string type(edge) Returns the edge type of an edge. src(edge) Returns the source vertex ID of an edge. The data type of the result is the same as the vertex ID. dst(edge) Returns the destination vertex ID of an edge. The data type of the result is the same as the vertex ID. int rank(edge) Returns the rank value of an edge. vertex Returns the information of vertices, including VIDs, tags, properties, and values. edge Returns the information of edges, including edge types, source vertices, destination vertices, ranks, properties, and values. vertices Returns the information of vertices in a subgraph. For more information, see GET SUBGRAPH \u3002 edges Returns the information of edges in a subgraph. For more information, see GET SUBGRAPH \u3002 path Returns the information of a path. For more information, see FIND PATH \u3002 For statements compatible with openCypher Function Description id(<vertex>) Returns the ID of a vertex. The data type of the result is the same as the vertex ID. list tags(<vertex>) Returns the Tag of a vertex, which serves the same purpose as labels(). list labels(<vertex>) Returns the Tag of a vertex, which serves the same purpose as tags(). This function is used for compatibility with openCypher syntax. map properties(<vertex_or_edge>) Returns the properties of a vertex or an edge. string type(<edge>) Returns the edge type of an edge. src(<edge>) Returns the source vertex ID of an edge. The data type of the result is the same as the vertex ID. dst(<edge>) Returns the destination vertex ID of an edge. The data type of the result is the same as the vertex ID. vertex startNode(<path>) Visits an edge or a path and returns its source vertex ID. string endNode(<path>) Visits an edge or a path and returns its destination vertex ID. int rank(<edge>) Returns the rank value of an edge. List functions Function Description keys(expr) Returns a list containing the string representations for all the property names of vertices, edges, or maps. labels(vertex) Returns the list containing all the tags of a vertex. nodes(path) Returns the list containing all the vertices in a path. range(start, end [, step]) Returns the list containing all the fixed-length steps in [start,end] . step is 1 by default. relationships(path) Returns the list containing all the relationships in a path. reverse(list) Returns the list reversing the order of all elements in the original list. tail(list) Returns all the elements of the original list, excluding the first one. head(list) Returns the first element of a list. last(list) Returns the last element of a list. coalesce(list) Returns the first not null value in a list. reduce() See reduce() function \u3002 count() function Function Description count() Syntax: count({expr | *}) . count() returns the number of rows (including NULL). count(expr) returns the number of non-NULL values that meet the expression. count() and size() are different. collect() function Function Description collect() The collect() function returns a list containing the values returned by an expression. Using this function aggregates data by merging multiple records or values into a single list. reduce() function Function Syntax Description reduce() reduce(<accumulator> = <initial>, <variable> IN <list> | <expression>) The reduce() function applies an expression to each element in a list one by one, chains the result to the next iteration by taking it as the initial value, and returns the final result. hash() function Function Description hash() The hash() function returns the hash value of the argument. The argument can be a number, a string, a list, a boolean, null, or an expression that evaluates to a value of the preceding data types. The source code of the hash() function (MurmurHash2), seed ( 0xc70f6907UL ), and other parameters can be found in MurmurHash2.h . concat() function Function Description concat() The concat() function requires at least two or more strings. All the parameters are concatenated into one string. Syntax: concat(string1,string2,...) concat_ws() function Function Description concat_ws() The concat_ws() function connects two or more strings with a predefined separator. Predicate functions Predicate functions return true or false . They are most commonly used in WHERE clauses. <predicate>(<variable> IN <list> WHERE <condition>) Function Description exists() Returns true if the specified property exists in the vertex, edge or map. Otherwise, returns false . any() Returns true if the specified predicate holds for at least one element in the given list. Otherwise, returns false . all() Returns true if the specified predicate holds for all elements in the given list. Otherwise, returns false . none() Returns true if the specified predicate holds for no element in the given list. Otherwise, returns false . single() Returns true if the specified predicate holds for exactly one of the elements in the given list. Otherwise, returns false . CASE expressions The CASE expression uses conditions to filter the result of an nGQL query statement. It is usually used in the YIELD and RETURN clauses. The CASE expression will traverse all the conditions. When the first condition is met, the CASE expression stops reading the conditions and returns the result. If no conditions are met, it returns the result in the ELSE clause. If there is no ELSE clause and no conditions are met, it returns NULL . Syntax: CASE <comparer> WHEN <value> THEN <result> [WHEN ...] [ELSE <default>] END Parameter Description comparer A value or a valid expression that outputs a value. This value is used to compare with the value . value It will be compared with the comparer . If the value matches the comparer , then this condition is met. result The result is returned by the CASE expression if the value matches the comparer . default The default is returned by the CASE expression if no conditions are met. General queries statements \u00b6 MATCH MATCH <pattern> [<clause_1>] RETURN <output> [<clause_2>]; Pattern Example Description Match vertices (v) You can use a user-defined variable in a pair of parentheses to represent a vertex in a pattern. For example: (v) . Match tags MATCH (v:player) RETURN v You can specify a tag with :<tag_name> after the vertex in a pattern. Match multiple tags MATCH (v:player:team) RETURN v LIMIT 10 To match vertices with multiple tags, use colons (:). Match vertex properties MATCH (v:player{name:\"Tim Duncan\"}) RETURN v You can specify a vertex property with {<prop_name>: <prop_value>} after the tag in a pattern. Match a VID. MATCH (v) WHERE id(v) == 'player101' RETURN v You can use the VID to match a vertex. The id() function can retrieve the VID of a vertex. Match multiple VIDs. MATCH (v:player { name: 'Tim Duncan' })--(v2) WHERE id(v2) IN [\"player101\", \"player102\"] RETURN v2 To match multiple VIDs, use WHERE id(v) IN [vid_list] . Match connected vertices MATCH (v:player{name:\"Tim Duncan\"})--(v2) RETURN v2.player.name AS Name You can use the -- symbol to represent edges of both directions and match vertices connected by these edges. You can add a > or < to the -- symbol to specify the direction of an edge. Match paths MATCH p=(v:player{name:\"Tim Duncan\"})-->(v2) RETURN p Connected vertices and edges form a path. You can use a user-defined variable to name a path as follows. Match edges MATCH (v:player{name:\"Tim Duncan\"})-[e]-(v2) RETURN e Besides using -- , --> , or <-- to indicate a nameless edge, you can use a user-defined variable in a pair of square brackets to represent a named edge. For example: -[e]- . Match an edge type MATCH ()-[e:follow]-() RETURN e Just like vertices, you can specify an edge type with :<edge_type> in a pattern. For example: -[e:follow]- . Match edge type properties MATCH (v:player{name:\"Tim Duncan\"})-[e:follow{degree:95}]->(v2) RETURN e You can specify edge type properties with {<prop_name>: <prop_value>} in a pattern. For example: [e:follow{likeness:95}] . Match multiple edge types MATCH (v:player{name:\"Tim Duncan\"})-[e:follow | :serve]->(v2) RETURN e The | symbol can help matching multiple edge types. For example: [e:follow|:serve] . The English colon (:) before the first edge type cannot be omitted, but the English colon before the subsequent edge type can be omitted, such as [e:follow|serve] . Match multiple edges MATCH (v:player{name:\"Tim Duncan\"})-[]->(v2)<-[e:serve]-(v3) RETURN v2, v3 You can extend a pattern to match multiple edges in a path. Match fixed-length paths MATCH p=(v:player{name:\"Tim Duncan\"})-[e:follow*2]->(v2) RETURN DISTINCT v2 AS Friends You can use the :<edge_type>*<hop> pattern to match a fixed-length path. hop must be a non-negative integer. The data type of e is the list. Match variable-length paths MATCH p=(v:player{name:\"Tim Duncan\"})-[e:follow*1..3]->(v2) RETURN v2 AS Friends minHop : Optional. It represents the minimum length of the path. minHop : must be a non-negative integer. The default value is 1. minHop and maxHop are optional and the default value is 1 and infinity respectively. The data type of e is the list. Match variable-length paths with multiple edge types MATCH p=(v:player{name:\"Tim Duncan\"})-[e:follow | serve*2]->(v2) RETURN DISTINCT v2 You can specify multiple edge types in a fixed-length or variable-length pattern. In this case, hop , minHop , and maxHop take effect on all edge types. The data type of e is the list. Retrieve vertex or edge information MATCH (v:player{name:\"Tim Duncan\"}) RETURN v MATCH (v:player{name:\"Tim Duncan\"})-[e]->(v2) RETURN e Use RETURN {<vertex_name> | <edge_name>} to retrieve all the information of a vertex or an edge. Retrieve VIDs MATCH (v:player{name:\"Tim Duncan\"}) RETURN id(v) Use the id() function to retrieve VIDs. Retrieve tags MATCH (v:player{name:\"Tim Duncan\"}) RETURN labels(v) Use the labels() function to retrieve the list of tags on a vertex. To retrieve the nth element in the labels(v) list, use labels(v)[n-1] . Retrieve a single property on a vertex or an edge MATCH (v:player{name:\"Tim Duncan\"}) RETURN v.player.age Use RETURN {<vertex_name> | <edge_name>}.<property> to retrieve a single property. Use AS to specify an alias for a property. Retrieve all properties on a vertex or an edge MATCH p=(v:player{name:\"Tim Duncan\"})-[]->(v2) RETURN properties(v2) Use the properties() function to retrieve all properties on a vertex or an edge. Retrieve edge types MATCH p=(v:player{name:\"Tim Duncan\"})-[e]->() RETURN DISTINCT type(e) Use the type() function to retrieve the matched edge types. Retrieve paths MATCH p=(v:player{name:\"Tim Duncan\"})-[*3]->() RETURN p Use RETURN <path_name> to retrieve all the information of the matched paths. Retrieve vertices in a path MATCH p=(v:player{name:\"Tim Duncan\"})-[]->(v2) RETURN nodes(p) Use the nodes() function to retrieve all vertices in a path. Retrieve edges in a path MATCH p=(v:player{name:\"Tim Duncan\"})-[]->(v2) RETURN relationships(p) Use the relationships() function to retrieve all edges in a path. Retrieve path length MATCH p=(v:player{name:\"Tim Duncan\">})-[*..2]->(v2) RETURN p AS Paths, length(p) AS Length Use the length() function to retrieve the length of a path. OPTIONAL MATCH Pattern Example Description Matches patterns against your graph database, just like MATCH does. MATCH (m)-[]->(n) WHERE id(m)==\"player100\" OPTIONAL MATCH (n)-[]->(l) WHERE id(n)==\"player125\" RETURN id(m),id(n),id(l) If no matches are found, OPTIONAL MATCH will use a null for missing parts of the pattern. LOOKUP LOOKUP ON {<vertex_tag> | <edge_type>} [WHERE <expression> [AND <expression> ...]] YIELD <return_list> [AS <alias>] Pattern Example Description Retrieve vertices LOOKUP ON player WHERE player.name == \"Tony Parker\" YIELD player.name AS name, player.age AS age The following example returns vertices whose name is Tony Parker and the tag is player . Retrieve edges LOOKUP ON follow WHERE follow.degree == 90 YIELD follow.degree Returns edges whose degree is 90 and the edge type is follow . List vertices with a tag LOOKUP ON player YIELD properties(vertex),id(vertex) Shows how to retrieve the VID of all vertices tagged with player . List edges with an edge types LOOKUP ON like YIELD edge AS e Shows how to retrieve the source Vertex IDs, destination vertex IDs, and ranks of all edges of the like edge type. Count the numbers of vertices or edges LOOKUP ON player YIELD id(vertex)| YIELD COUNT(*) AS Player_Count Shows how to count the number of vertices tagged with player . Count the numbers of edges LOOKUP ON like YIELD id(vertex)| YIELD COUNT(*) AS Like_Count Shows how to count the number of edges of the like edge type. GO GO [[<M> TO] <N> STEPS ] FROM <vertex_list> OVER <edge_type_list> [{REVERSELY | BIDIRECT}] [ WHERE <conditions> ] YIELD [DISTINCT] <return_list> [{SAMPLE <sample_list> | LIMIT <limit_list>}] [| GROUP BY {col_name | expr | position} YIELD <col_name>] [| ORDER BY <expression> [{ASC | DESC}]] [| LIMIT [<offset_value>,] <number_rows>] Example Description GO FROM \"player102\" OVER serve YIELD dst(edge) Returns the teams that player 102 serves. GO 2 STEPS FROM \"player102\" OVER follow YIELD dst(edge) Returns the friends of player 102 with 2 hops. GO FROM \"player100\", \"player102\" OVER serve WHERE properties(edge).start_year > 1995 YIELD DISTINCT properties($$).name AS team_name, properties(edge).start_year AS start_year, properties($^).name AS player_name Adds a filter for the traversal. GO FROM \"player100\" OVER follow, serve YIELD properties(edge).degree, properties(edge).start_year The following example traverses along with multiple edge types. If there is no value for a property, the output is UNKNOWN_PROP . GO FROM \"player100\" OVER follow REVERSELY YIELD src(edge) AS destination The following example returns the neighbor vertices in the incoming direction of player 100. GO FROM \"player100\" OVER follow REVERSELY YIELD src(edge) AS id | GO FROM $-.id OVER serve WHERE properties($^).age > 20 YIELD properties($^).name AS FriendOf, properties($$).name AS Team The following example retrieves the friends of player 100 and the teams that they serve. GO FROM \"player102\" OVER follow YIELD dst(edge) AS both The following example returns all the neighbor vertices of player 102. GO 2 STEPS FROM \"player100\" OVER follow YIELD src(edge) AS src, dst(edge) AS dst, properties($$).age AS age | GROUP BY $-.dst YIELD $-.dst AS dst, collect_set($-.src) AS src, collect($-.age) AS age The following example the outputs according to age. FETCH Fetch vertex properties FETCH PROP ON {<tag_name>[, tag_name ...] | *} <vid> [, vid ...] YIELD <return_list> [AS <alias>] Example Description FETCH PROP ON player \"player100\" YIELD properties(vertex) Specify a tag in the FETCH statement to fetch the vertex properties by that tag. FETCH PROP ON player \"player100\" YIELD player.name AS name Use a YIELD clause to specify the properties to be returned. FETCH PROP ON player \"player101\", \"player102\", \"player103\" YIELD properties(vertex) Specify multiple VIDs (vertex IDs) to fetch properties of multiple vertices. Separate the VIDs with commas. FETCH PROP ON player, t1 \"player100\", \"player103\" YIELD properties(vertex) Specify multiple tags in the FETCH statement to fetch the vertex properties by the tags. Separate the tags with commas. FETCH PROP ON * \"player100\", \"player106\", \"team200\" YIELD properties(vertex) Set an asterisk symbol * to fetch properties by all tags in the current graph space. Fetch edge properties FETCH PROP ON <edge_type> <src_vid> -> <dst_vid>[@<rank>] [, <src_vid> -> <dst_vid> ...] YIELD <output>; Example Description FETCH PROP ON serve \"player100\" -> \"team204\" YIELD properties(edge) The following statement fetches all the properties of the serve edge that connects vertex \"player100\" and vertex \"team204\" . FETCH PROP ON serve \"player100\" -> \"team204\" YIELD serve.start_year Use a YIELD clause to fetch specific properties of an edge. FETCH PROP ON serve \"player100\" -> \"team204\", \"player133\" -> \"team202\" YIELD properties(edge) Specify multiple edge patterns ( <src_vid> -> <dst_vid>[@<rank>] ) to fetch properties of multiple edges. Separate the edge patterns with commas. FETCH PROP ON serve \"player100\" -> \"team204\"@1 YIELD properties(edge) To fetch on an edge whose rank is not 0, set its rank in the FETCH statement. GO FROM \"player101\" OVER follow YIELD follow._src AS s, follow._dst AS d | FETCH PROP ON follow $-.s -> $-.d YIELD follow.degree The following statement returns the degree values of the follow edges that start from vertex \"player101\" . $var = GO FROM \"player101\" OVER follow YIELD follow._src AS s, follow._dst AS d; FETCH PROP ON follow $var.s -> $var.d YIELD follow.degree You can use user-defined variables to construct similar queries. SHOW Statement Syntax Example Description SHOW CHARSET SHOW CHARSET SHOW CHARSET Shows the available character sets. SHOW COLLATION SHOW COLLATION SHOW COLLATION Shows the collations supported by Nebula Graph. SHOW CREATE SPACE SHOW CREATE SPACE <space_name> SHOW CREATE SPACE basketballplayer Shows the creating statement of the specified graph space. SHOW CREATE TAG/EDGE SHOW CREATE {TAG <tag_name> | EDGE <edge_name>} SHOW CREATE TAG player Shows the basic information of the specified tag. SHOW HOSTS SHOW HOSTS [GRAPH | STORAGE | META] SHOW HOSTS SHOW HOSTS GRAPH Shows the host and version information of Graph Service, Storage Service, and Meta Service. SHOW INDEX STATUS SHOW {TAG | EDGE} INDEX STATUS SHOW TAG INDEX STATUS Shows the status of jobs that rebuild native indexes, which helps check whether a native index is successfully rebuilt or not. SHOW INDEXES SHOW {TAG | EDGE} INDEXES SHOW TAG INDEXES Shows the names of existing native indexes. SHOW PARTS SHOW PARTS [<part_id>] SHOW PARTS Shows the information of a specified partition or all partitions in a graph space. SHOW ROLES SHOW ROLES IN <space_name> SHOW ROLES in basketballplayer Shows the roles that are assigned to a user account. SHOW SNAPSHOTS SHOW SNAPSHOTS SHOW SNAPSHOTS Shows the information of all the snapshots. SHOW SPACES SHOW SPACES SHOW SPACES Shows existing graph spaces in Nebula Graph. SHOW STATS SHOW STATS SHOW STATS Shows the statistics of the graph space collected by the latest STATS job. SHOW TAGS/EDGES SHOW TAGS | EDGES SHOW TAGS \u3001 SHOW EDGES Shows all the tags in the current graph space. SHOW USERS SHOW USERS SHOW USERS Shows the user information. SHOW SESSIONS SHOW SESSIONS SHOW SESSIONS Shows the information of all the sessions. SHOW SESSIONS SHOW SESSION <Session_Id> SHOW SESSION 1623304491050858 Shows a specified session with its ID. SHOW QUERIES SHOW [ALL] QUERIES SHOW QUERIES Shows the information of working queries in the current session. SHOW META LEADER SHOW META LEADER SHOW META LEADER Shows the information of the leader in the current Meta cluster. Clauses and options \u00b6 Clause Syntax Example Description GROUP BY GROUP BY <var> YIELD <var>, <aggregation_function(var)> GO FROM \"player100\" OVER follow BIDIRECT YIELD $$.player.name as Name | GROUP BY $-.Name YIELD $-.Name as Player, count(*) AS Name_Count Finds all the vertices connected directly to vertex \"player100\" , groups the result set by player names, and counts how many times the name shows up in the result set. LIMIT YIELD <var> [| LIMIT [<offset_value>,] <number_rows>] O FROM \"player100\" OVER follow REVERSELY YIELD $$.player.name AS Friend, $$.player.age AS Age | ORDER BY $-.Age, $-.Friend | LIMIT 1, 3 Returns the 3 rows of data starting from the second row of the sorted output. SKIP RETURN <var> [SKIP <offset>] [LIMIT <number_rows>] MATCH (v:player{name:\"Tim Duncan\"}) --> (v2) RETURN v2.player.name AS Name, v2.player.age AS Age ORDER BY Age DESC SKIP 1 SKIP can be used alone to set the offset and return the data after the specified position. SAMPLE <go_statement> SAMPLE <sample_list>; GO 3 STEPS FROM \"player100\" OVER * YIELD properties($$).name AS NAME, properties($$).age AS Age SAMPLE [1,2,3]; Takes samples evenly in the result set and returns the specified amount of data. ORDER BY <YIELD clause> ORDER BY <expression> [ASC | DESC] [, <expression> [ASC | DESC] ...] FETCH PROP ON player \"player100\", \"player101\", \"player102\", \"player103\" YIELD player.age AS age, player.name AS name | ORDER BY $-.age ASC, $-.name DESC The ORDER BY clause specifies the order of the rows in the output. RETURN RETURN {<vertex_name>|<edge_name>|<vertex_name>.<property>|<edge_name>.<property>|...} MATCH (v:player) RETURN v.player.name, v.player.age LIMIT 3 Returns the first three rows with values of the vertex properties name and age . TTL CREATE TAG <tag_name>(<property_name_1> <property_value_1>, <property_name_2> <property_value_2>, ...) ttl_duration= <value_int>, ttl_col = <property_name> CREATE TAG t2(a int, b int, c string) ttl_duration= 100, ttl_col = \"a\" Create a tag and set the TTL options. WHERE WHERE {<vertex|edge_alias>.<property_name> {>|==|<|...} <value>...} MATCH (v:player) WHERE v.player.name == \"Tim Duncan\" XOR (v.player.age < 30 AND v.player.name == \"Yao Ming\") OR NOT (v.player.name == \"Yao Ming\" OR v.player.name == \"Tim Duncan\") RETURN v.player.name, v.player.age The WHERE clause filters the output by conditions. The WHERE clause usually works in Native nGQL GO and LOOKUP statements, and OpenCypher MATCH and WITH statements. YIELD YIELD [DISTINCT] <col> [AS <alias>] [, <col> [AS <alias>] ...] [WHERE <conditions>]; GO FROM \"player100\" OVER follow YIELD dst(edge) AS ID | FETCH PROP ON player $-.ID YIELD player.age AS Age | YIELD AVG($-.Age) as Avg_age, count(*)as Num_friends Finds the players that \"player100\" follows and calculates their average age. WITH MATCH $expressions WITH {nodes()|labels()|...} MATCH p=(v:player{name:\"Tim Duncan\"})--() WITH nodes(p) AS n UNWIND n AS n1 RETURN DISTINCT n1 The WITH clause can retrieve the output from a query part, process it, and pass it to the next query part as the input. UNWIND UNWIND <list> AS <alias> <RETURN clause> UNWIND [1,2,3] AS n RETURN n Splits a list into rows. Space statements \u00b6 Statement Syntax Example Description CREATE SPACE CREATE SPACE [IF NOT EXISTS] <graph_space_name> ( [partition_num = <partition_number>,] [replica_factor = <replica_number>,] vid_type = {FIXED_STRING(<N>) | INT[64]} ) [COMMENT = '<comment>'] CREATE SPACE my_space_1 (vid_type=FIXED_STRING(30)) Creates a graph space with CREATE SPACE CREATE SPACE <new_graph_space_name> AS <old_graph_space_name> CREATE SPACE my_space_4 as my_space_3 Clone a graph. space. USE USE <graph_space_name> USE space1 Specifies a graph space as the current working graph space for subsequent queries. SHOW SPACES SHOW SPACES SHOW SPACES Lists all the graph spaces in the Nebula Graph examples. DESCRIBE SPACE DESC[RIBE] SPACE <graph_space_name> DESCRIBE SPACE basketballplayer Returns the information about the specified graph space.\u606f\u3002 DROP SPACE DROP SPACE [IF EXISTS] <graph_space_name> DROP SPACE basketballplayer Deletes everything in the specified graph space. TAG statements \u00b6 Statement Syntax Example Description CREATE TAG CREATE TAG [IF NOT EXISTS] <tag_name> ( <prop_name> <data_type> [NULL | NOT NULL] [DEFAULT <default_value>] [COMMENT '<comment>'] [{, <prop_name> <data_type> [NULL | NOT NULL] [DEFAULT <default_value>] [COMMENT '<comment>']} ...] ) [TTL_DURATION = <ttl_duration>] [TTL_COL = <prop_name>] [COMMENT = '<comment>'] CREATE TAG woman(name string, age int, married bool, salary double, create_time timestamp) TTL_DURATION = 100, TTL_COL = \"create_time\" Creates a tag with the given name in a graph space. DROP TAG DROP TAG [IF EXISTS] <tag_name> CREATE TAG test(p1 string, p2 int) Drops a tag with the given name in the current working graph space. ALTER TAG ALTER TAG <tag_name> <alter_definition> [, alter_definition] ...] [ttl_definition [, ttl_definition] ... ] [COMMENT = '<comment>'] ALTER TAG t1 ADD (p3 int, p4 string) Alters the structure of a tag with the given name in a graph space. You can add or drop properties, and change the data type of an existing property. You can also set a TTL \uff08Time-To-Live\uff09on a property, or change its TTL duration. SHOW TAGS SHOW TAGS SHOW TAGS Shows the name of all tags in the current graph space. DESCRIBE TAG DESC[RIBE] TAG <tag_name> DESCRIBE TAG player Returns the information about a tag with the given name in a graph space, such as field names, data type, and so on. DELETE TAG DELETE TAG <tag_name_list> FROM <VID> DELETE TAG test1 FROM \"test\" Deletes a tag with the given name on a specified vertex. Edge type statements \u00b6 Statement Syntax Example Description CREATE EDGE CREATE EDGE [IF NOT EXISTS] <edge_type_name> ( <prop_name> <data_type> [NULL | NOT NULL] [DEFAULT <default_value>] [COMMENT '<comment>'] [{, <prop_name> <data_type> [NULL | NOT NULL] [DEFAULT <default_value>] [COMMENT '<comment>']} ...] ) [TTL_DURATION = <ttl_duration>] [TTL_COL = <prop_name>] [COMMENT = '<comment>'] CREATE EDGE e1(p1 string, p2 int, p3 timestamp) TTL_DURATION = 100, TTL_COL = \"p2\" Creates an edge type with the given name in a graph space.type\u3002 DROP EDGE DROP EDGE [IF EXISTS] <edge_type_name> DROP EDGE e1 Drops an edge type with the given name in a graph space. ALTER EDGE ALTER EDGE <edge_type_name> <alter_definition> [, alter_definition] ...] [ttl_definition [, ttl_definition] ... ] [COMMENT = '<comment>'] ALTER EDGE e1 ADD (p3 int, p4 string) Alters the structure of an edge type with the given name in a graph space. SHOW EDGES SHOW EDGES SHOW EDGES Shows all edge types in the current graph space. DESCRIBE EDGE DESC[RIBE] EDGE <edge_type_name> DESCRIBE EDGE follow Returns the information about an edge type with the given name in a graph space, such as field names, data type, and so on. Vertex statements \u00b6 Statement Syntax Example Description INSERT VERTEX INSERT VERTEX [IF NOT EXISTS] [tag_props, [tag_props] ...] VALUES <vid>: ([prop_value_list]) INSERT VERTEX t2 (name, age) VALUES \"13\":(\"n3\", 12), \"14\":(\"n4\", 8) Inserts one or more vertices into a graph space in Nebula Graph. DELETE VERTEX DELETE VERTEX <vid> [, <vid> ...] DELETE VERTEX \"team1\" Deletes vertices and the related incoming and outgoing edges of the vertices. UPDATE VERTEX UPDATE VERTEX ON <tag_name> <vid> SET <update_prop> [WHEN <condition>] [YIELD <output>] UPDATE VERTEX ON player \"player101\" SET age = age + 2 Updates properties on tags of a vertex. UPSERT VERTEX UPSERT VERTEX ON <tag> <vid> SET <update_prop> [WHEN <condition>] [YIELD <output>] UPSERT VERTEX ON player \"player667\" SET age = 31 The UPSERT statement is a combination of UPDATE and INSERT . You can use UPSERT VERTEX to update the properties of a vertex if it exists or insert a new vertex if it does not exist. Edge statements \u00b6 Statement Syntax Example Description INSERT EDGE INSERT EDGE [IF NOT EXISTS] <edge_type> ( <prop_name_list> ) VALUES <src_vid> -> <dst_vid>[@<rank>] : ( <prop_value_list> ) [, <src_vid> -> <dst_vid>[@<rank>] : ( <prop_value_list> ), ...] INSERT EDGE e2 (name, age) VALUES \"11\"->\"13\":(\"n1\", 1) Inserts an edge or multiple edges into a graph space from a source vertex (given by src_vid) to a destination vertex (given by dst_vid) with a specific rank in Nebula Graph. DELETE EDGE DELETE EDGE <edge_type> <src_vid> -> <dst_vid>[@<rank>] [, <src_vid> -> <dst_vid>[@<rank>] ...] DELETE EDGE serve \"player100\" -> \"team204\"@0 Deletes one edge or multiple edges at a time. UPDATE EDGE UPDATE EDGE ON <edge_type> <src_vid> -> <dst_vid> [@<rank>] SET <update_prop> [WHEN <condition>] [YIELD <output>] UPDATE EDGE ON serve \"player100\" -> \"team204\"@0 SET start_year = start_year + 1 Updates properties on an edge. UPSERT EDGE UPSERT EDGE ON <edge_type> <src_vid> -> <dst_vid> [@rank] SET <update_prop> [WHEN <condition>] [YIELD <properties>] UPSERT EDGE on serve \"player666\" -> \"team200\"@0 SET end_year = 2021 The UPSERT statement is a combination of UPDATE and INSERT . You can use UPSERT EDGE to update the properties of an edge if it exists or insert a new edge if it does not exist. Index \u00b6 Native index You can use native indexes together with LOOKUP and MATCH statements. Statement Syntax Example Description CREATE INDEX CREATE {TAG | EDGE} INDEX [IF NOT EXISTS] <index_name> ON {<tag_name> | <edge_name>} ([<prop_name_list>]) [COMMENT = '<comment>'] CREATE TAG INDEX player_index on player() Add native indexes for the existing tags, edge types, or properties. SHOW CREATE INDEX SHOW CREATE {TAG | EDGE} INDEX <index_name> show create tag index index_2 Shows the statement used when creating a tag or an edge type. It contains detailed information about the index, such as its associated properties. SHOW INDEXES SHOW {TAG | EDGE} INDEXES SHOW TAG INDEXES Shows the defined tag or edge type indexes names in the current graph space. DESCRIBE INDEX DESCRIBE {TAG | EDGE} INDEX <index_name> DESCRIBE TAG INDEX player_index_0 Gets the information about the index with a given name, including the property name (Field) and the property type (Type) of the index. REBUILD INDEX REBUILD {TAG | EDGE} INDEX [<index_name_list>] REBUILD TAG INDEX single_person_index Rebuilds the created tag or edge type index. If data is updated or inserted before the creation of the index, you must rebuild the indexes manually to make sure that the indexes contain the previously added data. SHOW INDEX STATUS SHOW {TAG | EDGE} INDEX STATUS SHOW TAG INDEX STATUS Returns the name of the created tag or edge type index and its status. DROP INDEX DROP {TAG | EDGE} INDEX [IF EXISTS] <index_name> DROP TAG INDEX player_index_0 Removes an existing index from the current graph space. Full-text index Syntax Example Description SIGN IN TEXT SERVICE [(<elastic_ip:port> [,<username>, <password>]), (<elastic_ip:port>), ...] SIGN IN TEXT SERVICE (127.0.0.1:9200) The full-text indexes is implemented based on Elasticsearch . After deploying an Elasticsearch cluster, you can use the SIGN IN statement to log in to the Elasticsearch client. SHOW TEXT SEARCH CLIENTS SHOW TEXT SEARCH CLIENTS Shows text search clients. SIGN OUT TEXT SERVICE SIGN OUT TEXT SERVICE Signs out to the text search clients. CREATE FULLTEXT {TAG | EDGE} INDEX <index_name> ON {<tag_name> | <edge_name>} ([<prop_name_list>]) CREATE FULLTEXT TAG INDEX nebula_index_1 ON player(name) Creates full-text indexes. SHOW FULLTEXT INDEXES SHOW FULLTEXT INDEXES Show full-text indexes. REBUILD FULLTEXT INDEX REBUILD FULLTEXT INDEX Rebuild full-text indexes. DROP FULLTEXT INDEX <index_name> DROP FULLTEXT INDEX nebula_index_1 Drop full-text indexes. LOOKUP ON {<tag> | <edge_type>} WHERE <expression> [YIELD <return_list>] LOOKUP ON player WHERE FUZZY(player.name, \"Tim Dunncan\", AUTO, OR) YIELD player.name Use query options. Subgraph and path statements \u00b6 Type Syntax Example Description GET SUBGRAPH GET SUBGRAPH [WITH PROP] [<step_count> STEPS] FROM {<vid>, <vid>...} [{IN | OUT | BOTH} <edge_type>, <edge_type>...] YIELD [VERTICES AS <vertex_alias>] [,EDGES AS <edge_alias>] GET SUBGRAPH 1 STEPS FROM \"player100\" YIELD VERTICES AS nodes, EDGES AS relationships Retrieves information of vertices and edges reachable from the source vertices of the specified edge types and returns information of the subgraph. FIND PATH FIND { SHORTEST | ALL | NOLOOP } PATH [WITH PROP] FROM <vertex_id_list> TO <vertex_id_list> OVER <edge_type_list> [REVERSELY | BIDIRECT] [<WHERE clause>] [UPTO <N> STEPS] YIELD path as <alias> [| ORDER BY $-.path] [| LIMIT <M>] FIND SHORTEST PATH FROM \"player102\" TO \"team204\" OVER * YIELD path as p Finds the paths between the selected source vertices and destination vertices. A returned path is like (<vertex_id>)-[:<edge_type_name>@<rank>]->(<vertex_id) . Query tuning statements \u00b6 Type Syntax Example Description EXPLAIN EXPLAIN [format=\"row\" | \"dot\"] <your_nGQL_statement> EXPLAIN format=\"row\" SHOW TAGS EXPLAIN format=\"dot\" SHOW TAGS Helps output the execution plan of an nGQL statement without executing the statement. PROFILE PROFILE [format=\"row\" | \"dot\"] <your_nGQL_statement> PROFILE format=\"row\" SHOW TAGS EXPLAIN format=\"dot\" SHOW TAGS Executes the statement, then outputs the execution plan as well as the execution profile. Operation and maintenance statements \u00b6 BALANCE Syntax Description BALANCE LEADER Starts a job to balance the distribution of storage leaders in the current graph space. It returns the job ID. Job statements Syntax Description SUBMIT JOB COMPACT Triggers the long-term RocksDB compact operation. SUBMIT JOB FLUSH Writes the RocksDB memfile in the memory to the hard disk. SUBMIT JOB STATS Starts a job that makes the statistics of the current graph space. Once this job succeeds, you can use the SHOW STATS statement to list the statistics. SHOW JOB <job_id> Shows the information about a specific job and all its tasks in the current graph space. The Meta Service parses a SUBMIT JOB request into multiple tasks and assigns them to the nebula-storaged processes. SHOW JOBS Lists all the unexpired jobs in the current graph space. STOP JOB Stops jobs that are not finished in the current graph space. RECOVER JOB Re-executes the failed jobs in the current graph space and returns the number of recovered jobs. Kill queries Syntax Example Description KILL QUERY (session=<session_id>, plan=<plan_id>) KILL QUERY(SESSION=1625553545984255,PLAN=163) Terminates the query being executed, and is often used to terminate slow queries.","title":"nGQL cheatsheet"},{"location":"2.quick-start/6.cheatsheet-for-ngql/#ngql_cheatsheet","text":"","title":"nGQL cheatsheet"},{"location":"2.quick-start/6.cheatsheet-for-ngql/#functions","text":"Math functions Function Description double abs(double x) Returns the absolute value of the argument. double floor(double x) Returns the largest integer value smaller than or equal to the argument. (Rounds down) double ceil(double x) Returns the smallest integer greater than or equal to the argument. (Rounds up) double round(double x) Returns the integer value nearest to the argument. Returns a number farther away from 0 if the argument is in the middle. double sqrt(double x) Returns the square root of the argument. double cbrt(double x) Returns the cubic root of the argument. double hypot(double x, double y) Returns the hypotenuse of a right-angled triangle. double pow(double x, double y) Returns the result of \\(x^y\\) . double exp(double x) Returns the result of \\(e^x\\) . double exp2(double x) Returns the result of \\(2^x\\) . double log(double x) Returns the base-e logarithm of the argument. double log2(double x) Returns the base-2 logarithm of the argument. double log10(double x) Returns the base-10 logarithm of the argument. double sin(double x) Returns the sine of the argument. double asin(double x) Returns the inverse sine of the argument. double cos(double x) Returns the cosine of the argument. double acos(double x) Returns the inverse cosine of the argument. double tan(double x) Returns the tangent of the argument. double atan(double x) Returns the inverse tangent of the argument. double rand() Returns a random floating point number in the range from 0 (inclusive) to 1 (exclusive); i.e.[0,1). int rand32(int min, int max) Returns a random 32-bit integer in [min, max) . If you set only one argument, it is parsed as max and min is 0 by default. If you set no argument, the system returns a random signed 32-bit integer. int rand64(int min, int max) Returns a random 64-bit integer in [min, max) . If you set only one argument, it is parsed as max and min is 0 by default. If you set no argument, the system returns a random signed 64-bit integer. collect() Puts all the collected values into a list. avg() Returns the average value of the argument. count() Returns the number of records. max() Returns the maximum value. min() Returns the minimum value. std() Returns the population standard deviation. sum() Returns the sum value. bit_and() Bitwise AND. bit_or() Bitwise OR. bit_xor() Bitwise XOR. int size() Returns the number of elements in a list or a map. int range(int start, int end, int step) Returns a list of integers from [start,end] in the specified steps. step is 1 by default. int sign(double x) Returns the signum of the given number. If the number is 0 , the system returns 0 . If the number is negative, the system returns -1 . If the number is positive, the system returns 1 . double e() Returns the base of the natural logarithm, e (2.718281828459045). double pi() Returns the mathematical constant pi (3.141592653589793). double radians() Converts degrees to radians. radians(180) returns 3.141592653589793 . String functions Function Description int strcasecmp(string a, string b) Compares string a and b without case sensitivity. When a = b, the return value is 0. When a > b, the return value is greater than 0. When a < b, the return value is less than 0. string lower(string a) Returns the argument in lowercase. string toLower(string a) The same as lower() . string upper(string a) Returns the argument in uppercase. string toUpper(string a) The same as upper() . int length(string a) Returns the length of the given string in bytes. string trim(string a) Removes leading and trailing spaces. string ltrim(string a) Removes leading spaces. string rtrim(string a) Removes trailing spaces. string left(string a, int count) Returns a substring consisting of count characters from the left side of string a. If string a is shorter than count , the system returns string a. string right(string a, int count) Returns a substring consisting of count characters from the right side of string a. If string a is shorter than count , the system returns string a. string lpad(string a, int size, string letters) Left-pads string a with string letters and returns a substring with the length of size . string rpad(string a, int size, string letters) Right-pads string a with string letters and returns a substring with the length of size . string substr(string a, int pos, int count) Returns a substring extracting count characters starting from the specified position pos of string a. string substring(string a, int pos, int count) The same as substr() . string reverse(string) Returns a string in reverse order. string replace(string a, string b, string c) Replaces string b in string a with string c. list split(string a, string b) Splits string a at string b and returns a list of strings. string toString() Takes in any data type and converts it into a string. int hash() Takes in any data type and encodes it into a hash value. Data and time functions Function Description int now() Returns the current date and time of the system timezone. timestamp timestamp() Returns the current date and time of the system timezone. date date() Returns the current UTC date based on the current system. time time() Returns the current UTC time based on the current system. datetime datetime() Returns the current UTC date and time based on the current system. Schema functions For nGQL statements Function Description id(vertex) Returns the ID of a vertex. The data type of the result is the same as the vertex ID. map properties(vertex) Returns the properties of a vertex. map properties(edge) Returns the properties of an edge. string type(edge) Returns the edge type of an edge. src(edge) Returns the source vertex ID of an edge. The data type of the result is the same as the vertex ID. dst(edge) Returns the destination vertex ID of an edge. The data type of the result is the same as the vertex ID. int rank(edge) Returns the rank value of an edge. vertex Returns the information of vertices, including VIDs, tags, properties, and values. edge Returns the information of edges, including edge types, source vertices, destination vertices, ranks, properties, and values. vertices Returns the information of vertices in a subgraph. For more information, see GET SUBGRAPH \u3002 edges Returns the information of edges in a subgraph. For more information, see GET SUBGRAPH \u3002 path Returns the information of a path. For more information, see FIND PATH \u3002 For statements compatible with openCypher Function Description id(<vertex>) Returns the ID of a vertex. The data type of the result is the same as the vertex ID. list tags(<vertex>) Returns the Tag of a vertex, which serves the same purpose as labels(). list labels(<vertex>) Returns the Tag of a vertex, which serves the same purpose as tags(). This function is used for compatibility with openCypher syntax. map properties(<vertex_or_edge>) Returns the properties of a vertex or an edge. string type(<edge>) Returns the edge type of an edge. src(<edge>) Returns the source vertex ID of an edge. The data type of the result is the same as the vertex ID. dst(<edge>) Returns the destination vertex ID of an edge. The data type of the result is the same as the vertex ID. vertex startNode(<path>) Visits an edge or a path and returns its source vertex ID. string endNode(<path>) Visits an edge or a path and returns its destination vertex ID. int rank(<edge>) Returns the rank value of an edge. List functions Function Description keys(expr) Returns a list containing the string representations for all the property names of vertices, edges, or maps. labels(vertex) Returns the list containing all the tags of a vertex. nodes(path) Returns the list containing all the vertices in a path. range(start, end [, step]) Returns the list containing all the fixed-length steps in [start,end] . step is 1 by default. relationships(path) Returns the list containing all the relationships in a path. reverse(list) Returns the list reversing the order of all elements in the original list. tail(list) Returns all the elements of the original list, excluding the first one. head(list) Returns the first element of a list. last(list) Returns the last element of a list. coalesce(list) Returns the first not null value in a list. reduce() See reduce() function \u3002 count() function Function Description count() Syntax: count({expr | *}) . count() returns the number of rows (including NULL). count(expr) returns the number of non-NULL values that meet the expression. count() and size() are different. collect() function Function Description collect() The collect() function returns a list containing the values returned by an expression. Using this function aggregates data by merging multiple records or values into a single list. reduce() function Function Syntax Description reduce() reduce(<accumulator> = <initial>, <variable> IN <list> | <expression>) The reduce() function applies an expression to each element in a list one by one, chains the result to the next iteration by taking it as the initial value, and returns the final result. hash() function Function Description hash() The hash() function returns the hash value of the argument. The argument can be a number, a string, a list, a boolean, null, or an expression that evaluates to a value of the preceding data types. The source code of the hash() function (MurmurHash2), seed ( 0xc70f6907UL ), and other parameters can be found in MurmurHash2.h . concat() function Function Description concat() The concat() function requires at least two or more strings. All the parameters are concatenated into one string. Syntax: concat(string1,string2,...) concat_ws() function Function Description concat_ws() The concat_ws() function connects two or more strings with a predefined separator. Predicate functions Predicate functions return true or false . They are most commonly used in WHERE clauses. <predicate>(<variable> IN <list> WHERE <condition>) Function Description exists() Returns true if the specified property exists in the vertex, edge or map. Otherwise, returns false . any() Returns true if the specified predicate holds for at least one element in the given list. Otherwise, returns false . all() Returns true if the specified predicate holds for all elements in the given list. Otherwise, returns false . none() Returns true if the specified predicate holds for no element in the given list. Otherwise, returns false . single() Returns true if the specified predicate holds for exactly one of the elements in the given list. Otherwise, returns false . CASE expressions The CASE expression uses conditions to filter the result of an nGQL query statement. It is usually used in the YIELD and RETURN clauses. The CASE expression will traverse all the conditions. When the first condition is met, the CASE expression stops reading the conditions and returns the result. If no conditions are met, it returns the result in the ELSE clause. If there is no ELSE clause and no conditions are met, it returns NULL . Syntax: CASE <comparer> WHEN <value> THEN <result> [WHEN ...] [ELSE <default>] END Parameter Description comparer A value or a valid expression that outputs a value. This value is used to compare with the value . value It will be compared with the comparer . If the value matches the comparer , then this condition is met. result The result is returned by the CASE expression if the value matches the comparer . default The default is returned by the CASE expression if no conditions are met.","title":"Functions"},{"location":"2.quick-start/6.cheatsheet-for-ngql/#general_queries_statements","text":"MATCH MATCH <pattern> [<clause_1>] RETURN <output> [<clause_2>]; Pattern Example Description Match vertices (v) You can use a user-defined variable in a pair of parentheses to represent a vertex in a pattern. For example: (v) . Match tags MATCH (v:player) RETURN v You can specify a tag with :<tag_name> after the vertex in a pattern. Match multiple tags MATCH (v:player:team) RETURN v LIMIT 10 To match vertices with multiple tags, use colons (:). Match vertex properties MATCH (v:player{name:\"Tim Duncan\"}) RETURN v You can specify a vertex property with {<prop_name>: <prop_value>} after the tag in a pattern. Match a VID. MATCH (v) WHERE id(v) == 'player101' RETURN v You can use the VID to match a vertex. The id() function can retrieve the VID of a vertex. Match multiple VIDs. MATCH (v:player { name: 'Tim Duncan' })--(v2) WHERE id(v2) IN [\"player101\", \"player102\"] RETURN v2 To match multiple VIDs, use WHERE id(v) IN [vid_list] . Match connected vertices MATCH (v:player{name:\"Tim Duncan\"})--(v2) RETURN v2.player.name AS Name You can use the -- symbol to represent edges of both directions and match vertices connected by these edges. You can add a > or < to the -- symbol to specify the direction of an edge. Match paths MATCH p=(v:player{name:\"Tim Duncan\"})-->(v2) RETURN p Connected vertices and edges form a path. You can use a user-defined variable to name a path as follows. Match edges MATCH (v:player{name:\"Tim Duncan\"})-[e]-(v2) RETURN e Besides using -- , --> , or <-- to indicate a nameless edge, you can use a user-defined variable in a pair of square brackets to represent a named edge. For example: -[e]- . Match an edge type MATCH ()-[e:follow]-() RETURN e Just like vertices, you can specify an edge type with :<edge_type> in a pattern. For example: -[e:follow]- . Match edge type properties MATCH (v:player{name:\"Tim Duncan\"})-[e:follow{degree:95}]->(v2) RETURN e You can specify edge type properties with {<prop_name>: <prop_value>} in a pattern. For example: [e:follow{likeness:95}] . Match multiple edge types MATCH (v:player{name:\"Tim Duncan\"})-[e:follow | :serve]->(v2) RETURN e The | symbol can help matching multiple edge types. For example: [e:follow|:serve] . The English colon (:) before the first edge type cannot be omitted, but the English colon before the subsequent edge type can be omitted, such as [e:follow|serve] . Match multiple edges MATCH (v:player{name:\"Tim Duncan\"})-[]->(v2)<-[e:serve]-(v3) RETURN v2, v3 You can extend a pattern to match multiple edges in a path. Match fixed-length paths MATCH p=(v:player{name:\"Tim Duncan\"})-[e:follow*2]->(v2) RETURN DISTINCT v2 AS Friends You can use the :<edge_type>*<hop> pattern to match a fixed-length path. hop must be a non-negative integer. The data type of e is the list. Match variable-length paths MATCH p=(v:player{name:\"Tim Duncan\"})-[e:follow*1..3]->(v2) RETURN v2 AS Friends minHop : Optional. It represents the minimum length of the path. minHop : must be a non-negative integer. The default value is 1. minHop and maxHop are optional and the default value is 1 and infinity respectively. The data type of e is the list. Match variable-length paths with multiple edge types MATCH p=(v:player{name:\"Tim Duncan\"})-[e:follow | serve*2]->(v2) RETURN DISTINCT v2 You can specify multiple edge types in a fixed-length or variable-length pattern. In this case, hop , minHop , and maxHop take effect on all edge types. The data type of e is the list. Retrieve vertex or edge information MATCH (v:player{name:\"Tim Duncan\"}) RETURN v MATCH (v:player{name:\"Tim Duncan\"})-[e]->(v2) RETURN e Use RETURN {<vertex_name> | <edge_name>} to retrieve all the information of a vertex or an edge. Retrieve VIDs MATCH (v:player{name:\"Tim Duncan\"}) RETURN id(v) Use the id() function to retrieve VIDs. Retrieve tags MATCH (v:player{name:\"Tim Duncan\"}) RETURN labels(v) Use the labels() function to retrieve the list of tags on a vertex. To retrieve the nth element in the labels(v) list, use labels(v)[n-1] . Retrieve a single property on a vertex or an edge MATCH (v:player{name:\"Tim Duncan\"}) RETURN v.player.age Use RETURN {<vertex_name> | <edge_name>}.<property> to retrieve a single property. Use AS to specify an alias for a property. Retrieve all properties on a vertex or an edge MATCH p=(v:player{name:\"Tim Duncan\"})-[]->(v2) RETURN properties(v2) Use the properties() function to retrieve all properties on a vertex or an edge. Retrieve edge types MATCH p=(v:player{name:\"Tim Duncan\"})-[e]->() RETURN DISTINCT type(e) Use the type() function to retrieve the matched edge types. Retrieve paths MATCH p=(v:player{name:\"Tim Duncan\"})-[*3]->() RETURN p Use RETURN <path_name> to retrieve all the information of the matched paths. Retrieve vertices in a path MATCH p=(v:player{name:\"Tim Duncan\"})-[]->(v2) RETURN nodes(p) Use the nodes() function to retrieve all vertices in a path. Retrieve edges in a path MATCH p=(v:player{name:\"Tim Duncan\"})-[]->(v2) RETURN relationships(p) Use the relationships() function to retrieve all edges in a path. Retrieve path length MATCH p=(v:player{name:\"Tim Duncan\">})-[*..2]->(v2) RETURN p AS Paths, length(p) AS Length Use the length() function to retrieve the length of a path. OPTIONAL MATCH Pattern Example Description Matches patterns against your graph database, just like MATCH does. MATCH (m)-[]->(n) WHERE id(m)==\"player100\" OPTIONAL MATCH (n)-[]->(l) WHERE id(n)==\"player125\" RETURN id(m),id(n),id(l) If no matches are found, OPTIONAL MATCH will use a null for missing parts of the pattern. LOOKUP LOOKUP ON {<vertex_tag> | <edge_type>} [WHERE <expression> [AND <expression> ...]] YIELD <return_list> [AS <alias>] Pattern Example Description Retrieve vertices LOOKUP ON player WHERE player.name == \"Tony Parker\" YIELD player.name AS name, player.age AS age The following example returns vertices whose name is Tony Parker and the tag is player . Retrieve edges LOOKUP ON follow WHERE follow.degree == 90 YIELD follow.degree Returns edges whose degree is 90 and the edge type is follow . List vertices with a tag LOOKUP ON player YIELD properties(vertex),id(vertex) Shows how to retrieve the VID of all vertices tagged with player . List edges with an edge types LOOKUP ON like YIELD edge AS e Shows how to retrieve the source Vertex IDs, destination vertex IDs, and ranks of all edges of the like edge type. Count the numbers of vertices or edges LOOKUP ON player YIELD id(vertex)| YIELD COUNT(*) AS Player_Count Shows how to count the number of vertices tagged with player . Count the numbers of edges LOOKUP ON like YIELD id(vertex)| YIELD COUNT(*) AS Like_Count Shows how to count the number of edges of the like edge type. GO GO [[<M> TO] <N> STEPS ] FROM <vertex_list> OVER <edge_type_list> [{REVERSELY | BIDIRECT}] [ WHERE <conditions> ] YIELD [DISTINCT] <return_list> [{SAMPLE <sample_list> | LIMIT <limit_list>}] [| GROUP BY {col_name | expr | position} YIELD <col_name>] [| ORDER BY <expression> [{ASC | DESC}]] [| LIMIT [<offset_value>,] <number_rows>] Example Description GO FROM \"player102\" OVER serve YIELD dst(edge) Returns the teams that player 102 serves. GO 2 STEPS FROM \"player102\" OVER follow YIELD dst(edge) Returns the friends of player 102 with 2 hops. GO FROM \"player100\", \"player102\" OVER serve WHERE properties(edge).start_year > 1995 YIELD DISTINCT properties($$).name AS team_name, properties(edge).start_year AS start_year, properties($^).name AS player_name Adds a filter for the traversal. GO FROM \"player100\" OVER follow, serve YIELD properties(edge).degree, properties(edge).start_year The following example traverses along with multiple edge types. If there is no value for a property, the output is UNKNOWN_PROP . GO FROM \"player100\" OVER follow REVERSELY YIELD src(edge) AS destination The following example returns the neighbor vertices in the incoming direction of player 100. GO FROM \"player100\" OVER follow REVERSELY YIELD src(edge) AS id | GO FROM $-.id OVER serve WHERE properties($^).age > 20 YIELD properties($^).name AS FriendOf, properties($$).name AS Team The following example retrieves the friends of player 100 and the teams that they serve. GO FROM \"player102\" OVER follow YIELD dst(edge) AS both The following example returns all the neighbor vertices of player 102. GO 2 STEPS FROM \"player100\" OVER follow YIELD src(edge) AS src, dst(edge) AS dst, properties($$).age AS age | GROUP BY $-.dst YIELD $-.dst AS dst, collect_set($-.src) AS src, collect($-.age) AS age The following example the outputs according to age. FETCH Fetch vertex properties FETCH PROP ON {<tag_name>[, tag_name ...] | *} <vid> [, vid ...] YIELD <return_list> [AS <alias>] Example Description FETCH PROP ON player \"player100\" YIELD properties(vertex) Specify a tag in the FETCH statement to fetch the vertex properties by that tag. FETCH PROP ON player \"player100\" YIELD player.name AS name Use a YIELD clause to specify the properties to be returned. FETCH PROP ON player \"player101\", \"player102\", \"player103\" YIELD properties(vertex) Specify multiple VIDs (vertex IDs) to fetch properties of multiple vertices. Separate the VIDs with commas. FETCH PROP ON player, t1 \"player100\", \"player103\" YIELD properties(vertex) Specify multiple tags in the FETCH statement to fetch the vertex properties by the tags. Separate the tags with commas. FETCH PROP ON * \"player100\", \"player106\", \"team200\" YIELD properties(vertex) Set an asterisk symbol * to fetch properties by all tags in the current graph space. Fetch edge properties FETCH PROP ON <edge_type> <src_vid> -> <dst_vid>[@<rank>] [, <src_vid> -> <dst_vid> ...] YIELD <output>; Example Description FETCH PROP ON serve \"player100\" -> \"team204\" YIELD properties(edge) The following statement fetches all the properties of the serve edge that connects vertex \"player100\" and vertex \"team204\" . FETCH PROP ON serve \"player100\" -> \"team204\" YIELD serve.start_year Use a YIELD clause to fetch specific properties of an edge. FETCH PROP ON serve \"player100\" -> \"team204\", \"player133\" -> \"team202\" YIELD properties(edge) Specify multiple edge patterns ( <src_vid> -> <dst_vid>[@<rank>] ) to fetch properties of multiple edges. Separate the edge patterns with commas. FETCH PROP ON serve \"player100\" -> \"team204\"@1 YIELD properties(edge) To fetch on an edge whose rank is not 0, set its rank in the FETCH statement. GO FROM \"player101\" OVER follow YIELD follow._src AS s, follow._dst AS d | FETCH PROP ON follow $-.s -> $-.d YIELD follow.degree The following statement returns the degree values of the follow edges that start from vertex \"player101\" . $var = GO FROM \"player101\" OVER follow YIELD follow._src AS s, follow._dst AS d; FETCH PROP ON follow $var.s -> $var.d YIELD follow.degree You can use user-defined variables to construct similar queries. SHOW Statement Syntax Example Description SHOW CHARSET SHOW CHARSET SHOW CHARSET Shows the available character sets. SHOW COLLATION SHOW COLLATION SHOW COLLATION Shows the collations supported by Nebula Graph. SHOW CREATE SPACE SHOW CREATE SPACE <space_name> SHOW CREATE SPACE basketballplayer Shows the creating statement of the specified graph space. SHOW CREATE TAG/EDGE SHOW CREATE {TAG <tag_name> | EDGE <edge_name>} SHOW CREATE TAG player Shows the basic information of the specified tag. SHOW HOSTS SHOW HOSTS [GRAPH | STORAGE | META] SHOW HOSTS SHOW HOSTS GRAPH Shows the host and version information of Graph Service, Storage Service, and Meta Service. SHOW INDEX STATUS SHOW {TAG | EDGE} INDEX STATUS SHOW TAG INDEX STATUS Shows the status of jobs that rebuild native indexes, which helps check whether a native index is successfully rebuilt or not. SHOW INDEXES SHOW {TAG | EDGE} INDEXES SHOW TAG INDEXES Shows the names of existing native indexes. SHOW PARTS SHOW PARTS [<part_id>] SHOW PARTS Shows the information of a specified partition or all partitions in a graph space. SHOW ROLES SHOW ROLES IN <space_name> SHOW ROLES in basketballplayer Shows the roles that are assigned to a user account. SHOW SNAPSHOTS SHOW SNAPSHOTS SHOW SNAPSHOTS Shows the information of all the snapshots. SHOW SPACES SHOW SPACES SHOW SPACES Shows existing graph spaces in Nebula Graph. SHOW STATS SHOW STATS SHOW STATS Shows the statistics of the graph space collected by the latest STATS job. SHOW TAGS/EDGES SHOW TAGS | EDGES SHOW TAGS \u3001 SHOW EDGES Shows all the tags in the current graph space. SHOW USERS SHOW USERS SHOW USERS Shows the user information. SHOW SESSIONS SHOW SESSIONS SHOW SESSIONS Shows the information of all the sessions. SHOW SESSIONS SHOW SESSION <Session_Id> SHOW SESSION 1623304491050858 Shows a specified session with its ID. SHOW QUERIES SHOW [ALL] QUERIES SHOW QUERIES Shows the information of working queries in the current session. SHOW META LEADER SHOW META LEADER SHOW META LEADER Shows the information of the leader in the current Meta cluster.","title":"General queries statements"},{"location":"2.quick-start/6.cheatsheet-for-ngql/#clauses_and_options","text":"Clause Syntax Example Description GROUP BY GROUP BY <var> YIELD <var>, <aggregation_function(var)> GO FROM \"player100\" OVER follow BIDIRECT YIELD $$.player.name as Name | GROUP BY $-.Name YIELD $-.Name as Player, count(*) AS Name_Count Finds all the vertices connected directly to vertex \"player100\" , groups the result set by player names, and counts how many times the name shows up in the result set. LIMIT YIELD <var> [| LIMIT [<offset_value>,] <number_rows>] O FROM \"player100\" OVER follow REVERSELY YIELD $$.player.name AS Friend, $$.player.age AS Age | ORDER BY $-.Age, $-.Friend | LIMIT 1, 3 Returns the 3 rows of data starting from the second row of the sorted output. SKIP RETURN <var> [SKIP <offset>] [LIMIT <number_rows>] MATCH (v:player{name:\"Tim Duncan\"}) --> (v2) RETURN v2.player.name AS Name, v2.player.age AS Age ORDER BY Age DESC SKIP 1 SKIP can be used alone to set the offset and return the data after the specified position. SAMPLE <go_statement> SAMPLE <sample_list>; GO 3 STEPS FROM \"player100\" OVER * YIELD properties($$).name AS NAME, properties($$).age AS Age SAMPLE [1,2,3]; Takes samples evenly in the result set and returns the specified amount of data. ORDER BY <YIELD clause> ORDER BY <expression> [ASC | DESC] [, <expression> [ASC | DESC] ...] FETCH PROP ON player \"player100\", \"player101\", \"player102\", \"player103\" YIELD player.age AS age, player.name AS name | ORDER BY $-.age ASC, $-.name DESC The ORDER BY clause specifies the order of the rows in the output. RETURN RETURN {<vertex_name>|<edge_name>|<vertex_name>.<property>|<edge_name>.<property>|...} MATCH (v:player) RETURN v.player.name, v.player.age LIMIT 3 Returns the first three rows with values of the vertex properties name and age . TTL CREATE TAG <tag_name>(<property_name_1> <property_value_1>, <property_name_2> <property_value_2>, ...) ttl_duration= <value_int>, ttl_col = <property_name> CREATE TAG t2(a int, b int, c string) ttl_duration= 100, ttl_col = \"a\" Create a tag and set the TTL options. WHERE WHERE {<vertex|edge_alias>.<property_name> {>|==|<|...} <value>...} MATCH (v:player) WHERE v.player.name == \"Tim Duncan\" XOR (v.player.age < 30 AND v.player.name == \"Yao Ming\") OR NOT (v.player.name == \"Yao Ming\" OR v.player.name == \"Tim Duncan\") RETURN v.player.name, v.player.age The WHERE clause filters the output by conditions. The WHERE clause usually works in Native nGQL GO and LOOKUP statements, and OpenCypher MATCH and WITH statements. YIELD YIELD [DISTINCT] <col> [AS <alias>] [, <col> [AS <alias>] ...] [WHERE <conditions>]; GO FROM \"player100\" OVER follow YIELD dst(edge) AS ID | FETCH PROP ON player $-.ID YIELD player.age AS Age | YIELD AVG($-.Age) as Avg_age, count(*)as Num_friends Finds the players that \"player100\" follows and calculates their average age. WITH MATCH $expressions WITH {nodes()|labels()|...} MATCH p=(v:player{name:\"Tim Duncan\"})--() WITH nodes(p) AS n UNWIND n AS n1 RETURN DISTINCT n1 The WITH clause can retrieve the output from a query part, process it, and pass it to the next query part as the input. UNWIND UNWIND <list> AS <alias> <RETURN clause> UNWIND [1,2,3] AS n RETURN n Splits a list into rows.","title":"Clauses and options"},{"location":"2.quick-start/6.cheatsheet-for-ngql/#space_statements","text":"Statement Syntax Example Description CREATE SPACE CREATE SPACE [IF NOT EXISTS] <graph_space_name> ( [partition_num = <partition_number>,] [replica_factor = <replica_number>,] vid_type = {FIXED_STRING(<N>) | INT[64]} ) [COMMENT = '<comment>'] CREATE SPACE my_space_1 (vid_type=FIXED_STRING(30)) Creates a graph space with CREATE SPACE CREATE SPACE <new_graph_space_name> AS <old_graph_space_name> CREATE SPACE my_space_4 as my_space_3 Clone a graph. space. USE USE <graph_space_name> USE space1 Specifies a graph space as the current working graph space for subsequent queries. SHOW SPACES SHOW SPACES SHOW SPACES Lists all the graph spaces in the Nebula Graph examples. DESCRIBE SPACE DESC[RIBE] SPACE <graph_space_name> DESCRIBE SPACE basketballplayer Returns the information about the specified graph space.\u606f\u3002 DROP SPACE DROP SPACE [IF EXISTS] <graph_space_name> DROP SPACE basketballplayer Deletes everything in the specified graph space.","title":"Space statements"},{"location":"2.quick-start/6.cheatsheet-for-ngql/#tag_statements","text":"Statement Syntax Example Description CREATE TAG CREATE TAG [IF NOT EXISTS] <tag_name> ( <prop_name> <data_type> [NULL | NOT NULL] [DEFAULT <default_value>] [COMMENT '<comment>'] [{, <prop_name> <data_type> [NULL | NOT NULL] [DEFAULT <default_value>] [COMMENT '<comment>']} ...] ) [TTL_DURATION = <ttl_duration>] [TTL_COL = <prop_name>] [COMMENT = '<comment>'] CREATE TAG woman(name string, age int, married bool, salary double, create_time timestamp) TTL_DURATION = 100, TTL_COL = \"create_time\" Creates a tag with the given name in a graph space. DROP TAG DROP TAG [IF EXISTS] <tag_name> CREATE TAG test(p1 string, p2 int) Drops a tag with the given name in the current working graph space. ALTER TAG ALTER TAG <tag_name> <alter_definition> [, alter_definition] ...] [ttl_definition [, ttl_definition] ... ] [COMMENT = '<comment>'] ALTER TAG t1 ADD (p3 int, p4 string) Alters the structure of a tag with the given name in a graph space. You can add or drop properties, and change the data type of an existing property. You can also set a TTL \uff08Time-To-Live\uff09on a property, or change its TTL duration. SHOW TAGS SHOW TAGS SHOW TAGS Shows the name of all tags in the current graph space. DESCRIBE TAG DESC[RIBE] TAG <tag_name> DESCRIBE TAG player Returns the information about a tag with the given name in a graph space, such as field names, data type, and so on. DELETE TAG DELETE TAG <tag_name_list> FROM <VID> DELETE TAG test1 FROM \"test\" Deletes a tag with the given name on a specified vertex.","title":"TAG statements"},{"location":"2.quick-start/6.cheatsheet-for-ngql/#edge_type_statements","text":"Statement Syntax Example Description CREATE EDGE CREATE EDGE [IF NOT EXISTS] <edge_type_name> ( <prop_name> <data_type> [NULL | NOT NULL] [DEFAULT <default_value>] [COMMENT '<comment>'] [{, <prop_name> <data_type> [NULL | NOT NULL] [DEFAULT <default_value>] [COMMENT '<comment>']} ...] ) [TTL_DURATION = <ttl_duration>] [TTL_COL = <prop_name>] [COMMENT = '<comment>'] CREATE EDGE e1(p1 string, p2 int, p3 timestamp) TTL_DURATION = 100, TTL_COL = \"p2\" Creates an edge type with the given name in a graph space.type\u3002 DROP EDGE DROP EDGE [IF EXISTS] <edge_type_name> DROP EDGE e1 Drops an edge type with the given name in a graph space. ALTER EDGE ALTER EDGE <edge_type_name> <alter_definition> [, alter_definition] ...] [ttl_definition [, ttl_definition] ... ] [COMMENT = '<comment>'] ALTER EDGE e1 ADD (p3 int, p4 string) Alters the structure of an edge type with the given name in a graph space. SHOW EDGES SHOW EDGES SHOW EDGES Shows all edge types in the current graph space. DESCRIBE EDGE DESC[RIBE] EDGE <edge_type_name> DESCRIBE EDGE follow Returns the information about an edge type with the given name in a graph space, such as field names, data type, and so on.","title":"Edge type statements"},{"location":"2.quick-start/6.cheatsheet-for-ngql/#vertex_statements","text":"Statement Syntax Example Description INSERT VERTEX INSERT VERTEX [IF NOT EXISTS] [tag_props, [tag_props] ...] VALUES <vid>: ([prop_value_list]) INSERT VERTEX t2 (name, age) VALUES \"13\":(\"n3\", 12), \"14\":(\"n4\", 8) Inserts one or more vertices into a graph space in Nebula Graph. DELETE VERTEX DELETE VERTEX <vid> [, <vid> ...] DELETE VERTEX \"team1\" Deletes vertices and the related incoming and outgoing edges of the vertices. UPDATE VERTEX UPDATE VERTEX ON <tag_name> <vid> SET <update_prop> [WHEN <condition>] [YIELD <output>] UPDATE VERTEX ON player \"player101\" SET age = age + 2 Updates properties on tags of a vertex. UPSERT VERTEX UPSERT VERTEX ON <tag> <vid> SET <update_prop> [WHEN <condition>] [YIELD <output>] UPSERT VERTEX ON player \"player667\" SET age = 31 The UPSERT statement is a combination of UPDATE and INSERT . You can use UPSERT VERTEX to update the properties of a vertex if it exists or insert a new vertex if it does not exist.","title":"Vertex statements"},{"location":"2.quick-start/6.cheatsheet-for-ngql/#edge_statements","text":"Statement Syntax Example Description INSERT EDGE INSERT EDGE [IF NOT EXISTS] <edge_type> ( <prop_name_list> ) VALUES <src_vid> -> <dst_vid>[@<rank>] : ( <prop_value_list> ) [, <src_vid> -> <dst_vid>[@<rank>] : ( <prop_value_list> ), ...] INSERT EDGE e2 (name, age) VALUES \"11\"->\"13\":(\"n1\", 1) Inserts an edge or multiple edges into a graph space from a source vertex (given by src_vid) to a destination vertex (given by dst_vid) with a specific rank in Nebula Graph. DELETE EDGE DELETE EDGE <edge_type> <src_vid> -> <dst_vid>[@<rank>] [, <src_vid> -> <dst_vid>[@<rank>] ...] DELETE EDGE serve \"player100\" -> \"team204\"@0 Deletes one edge or multiple edges at a time. UPDATE EDGE UPDATE EDGE ON <edge_type> <src_vid> -> <dst_vid> [@<rank>] SET <update_prop> [WHEN <condition>] [YIELD <output>] UPDATE EDGE ON serve \"player100\" -> \"team204\"@0 SET start_year = start_year + 1 Updates properties on an edge. UPSERT EDGE UPSERT EDGE ON <edge_type> <src_vid> -> <dst_vid> [@rank] SET <update_prop> [WHEN <condition>] [YIELD <properties>] UPSERT EDGE on serve \"player666\" -> \"team200\"@0 SET end_year = 2021 The UPSERT statement is a combination of UPDATE and INSERT . You can use UPSERT EDGE to update the properties of an edge if it exists or insert a new edge if it does not exist.","title":"Edge statements"},{"location":"2.quick-start/6.cheatsheet-for-ngql/#index","text":"Native index You can use native indexes together with LOOKUP and MATCH statements. Statement Syntax Example Description CREATE INDEX CREATE {TAG | EDGE} INDEX [IF NOT EXISTS] <index_name> ON {<tag_name> | <edge_name>} ([<prop_name_list>]) [COMMENT = '<comment>'] CREATE TAG INDEX player_index on player() Add native indexes for the existing tags, edge types, or properties. SHOW CREATE INDEX SHOW CREATE {TAG | EDGE} INDEX <index_name> show create tag index index_2 Shows the statement used when creating a tag or an edge type. It contains detailed information about the index, such as its associated properties. SHOW INDEXES SHOW {TAG | EDGE} INDEXES SHOW TAG INDEXES Shows the defined tag or edge type indexes names in the current graph space. DESCRIBE INDEX DESCRIBE {TAG | EDGE} INDEX <index_name> DESCRIBE TAG INDEX player_index_0 Gets the information about the index with a given name, including the property name (Field) and the property type (Type) of the index. REBUILD INDEX REBUILD {TAG | EDGE} INDEX [<index_name_list>] REBUILD TAG INDEX single_person_index Rebuilds the created tag or edge type index. If data is updated or inserted before the creation of the index, you must rebuild the indexes manually to make sure that the indexes contain the previously added data. SHOW INDEX STATUS SHOW {TAG | EDGE} INDEX STATUS SHOW TAG INDEX STATUS Returns the name of the created tag or edge type index and its status. DROP INDEX DROP {TAG | EDGE} INDEX [IF EXISTS] <index_name> DROP TAG INDEX player_index_0 Removes an existing index from the current graph space. Full-text index Syntax Example Description SIGN IN TEXT SERVICE [(<elastic_ip:port> [,<username>, <password>]), (<elastic_ip:port>), ...] SIGN IN TEXT SERVICE (127.0.0.1:9200) The full-text indexes is implemented based on Elasticsearch . After deploying an Elasticsearch cluster, you can use the SIGN IN statement to log in to the Elasticsearch client. SHOW TEXT SEARCH CLIENTS SHOW TEXT SEARCH CLIENTS Shows text search clients. SIGN OUT TEXT SERVICE SIGN OUT TEXT SERVICE Signs out to the text search clients. CREATE FULLTEXT {TAG | EDGE} INDEX <index_name> ON {<tag_name> | <edge_name>} ([<prop_name_list>]) CREATE FULLTEXT TAG INDEX nebula_index_1 ON player(name) Creates full-text indexes. SHOW FULLTEXT INDEXES SHOW FULLTEXT INDEXES Show full-text indexes. REBUILD FULLTEXT INDEX REBUILD FULLTEXT INDEX Rebuild full-text indexes. DROP FULLTEXT INDEX <index_name> DROP FULLTEXT INDEX nebula_index_1 Drop full-text indexes. LOOKUP ON {<tag> | <edge_type>} WHERE <expression> [YIELD <return_list>] LOOKUP ON player WHERE FUZZY(player.name, \"Tim Dunncan\", AUTO, OR) YIELD player.name Use query options.","title":"Index"},{"location":"2.quick-start/6.cheatsheet-for-ngql/#subgraph_and_path_statements","text":"Type Syntax Example Description GET SUBGRAPH GET SUBGRAPH [WITH PROP] [<step_count> STEPS] FROM {<vid>, <vid>...} [{IN | OUT | BOTH} <edge_type>, <edge_type>...] YIELD [VERTICES AS <vertex_alias>] [,EDGES AS <edge_alias>] GET SUBGRAPH 1 STEPS FROM \"player100\" YIELD VERTICES AS nodes, EDGES AS relationships Retrieves information of vertices and edges reachable from the source vertices of the specified edge types and returns information of the subgraph. FIND PATH FIND { SHORTEST | ALL | NOLOOP } PATH [WITH PROP] FROM <vertex_id_list> TO <vertex_id_list> OVER <edge_type_list> [REVERSELY | BIDIRECT] [<WHERE clause>] [UPTO <N> STEPS] YIELD path as <alias> [| ORDER BY $-.path] [| LIMIT <M>] FIND SHORTEST PATH FROM \"player102\" TO \"team204\" OVER * YIELD path as p Finds the paths between the selected source vertices and destination vertices. A returned path is like (<vertex_id>)-[:<edge_type_name>@<rank>]->(<vertex_id) .","title":"Subgraph and path statements"},{"location":"2.quick-start/6.cheatsheet-for-ngql/#query_tuning_statements","text":"Type Syntax Example Description EXPLAIN EXPLAIN [format=\"row\" | \"dot\"] <your_nGQL_statement> EXPLAIN format=\"row\" SHOW TAGS EXPLAIN format=\"dot\" SHOW TAGS Helps output the execution plan of an nGQL statement without executing the statement. PROFILE PROFILE [format=\"row\" | \"dot\"] <your_nGQL_statement> PROFILE format=\"row\" SHOW TAGS EXPLAIN format=\"dot\" SHOW TAGS Executes the statement, then outputs the execution plan as well as the execution profile.","title":"Query tuning statements"},{"location":"2.quick-start/6.cheatsheet-for-ngql/#operation_and_maintenance_statements","text":"BALANCE Syntax Description BALANCE LEADER Starts a job to balance the distribution of storage leaders in the current graph space. It returns the job ID. Job statements Syntax Description SUBMIT JOB COMPACT Triggers the long-term RocksDB compact operation. SUBMIT JOB FLUSH Writes the RocksDB memfile in the memory to the hard disk. SUBMIT JOB STATS Starts a job that makes the statistics of the current graph space. Once this job succeeds, you can use the SHOW STATS statement to list the statistics. SHOW JOB <job_id> Shows the information about a specific job and all its tasks in the current graph space. The Meta Service parses a SUBMIT JOB request into multiple tasks and assigns them to the nebula-storaged processes. SHOW JOBS Lists all the unexpired jobs in the current graph space. STOP JOB Stops jobs that are not finished in the current graph space. RECOVER JOB Re-executes the failed jobs in the current graph space and returns the number of recovered jobs. Kill queries Syntax Example Description KILL QUERY (session=<session_id>, plan=<plan_id>) KILL QUERY(SESSION=1625553545984255,PLAN=163) Terminates the query being executed, and is often used to terminate slow queries.","title":"Operation and maintenance statements"},{"location":"20.appendix/0.FAQ/","text":"FAQ \u00b6 This topic lists the frequently asked questions for using Nebula Graph 3.1.0. You can use the search box in the help center or the search function of the browser to match the questions you are looking for. If the solutions described in this topic cannot solve your problems, ask for help on the Nebula Graph forum or submit an issue on GitHub issue . About manual updates \u00b6 \"Why is the behavior in the manual not consistent with the system?\" \u00b6 Nebula Graph is still under development. Its behavior changes from time to time. Users can submit an issue to inform the team if the manual and the system are not consistent. Note If you find some errors in this topic: Click the pencil button at the top right side of this page. Use markdown to fix this error. Then click \"Commit changes\" at the bottom, which will start a Github pull request. Sign the CLA . This pull request will be merged after the acceptance of at least two reviewers. About legacy version compatibility \u00b6 X version compatibility Neubla Graph 3.1.0 is not compatible with Nebula Graph 1.x nor 2.0-RC in both data formats and RPC-protocols, and vice versa . The service process may quit if using an lower version client to connect to a higher version server. To upgrade data formats, see Upgrade Nebula Graph to the current version . Users must upgrade all clients . About execution errors \u00b6 \"How to resolve the error SemanticError: Missing yield clause. ?\" \u00b6 Starting with Nebula Graph 3.0.0, the statements LOOKUP , GO , and FETCH must output results with the YIELD clause. For more information, see YIELD . \"How to resolve the error Host not enough! ?\" \u00b6 From Nebula Graph version 3.0.0, the Storage services added in the configuration files CANNOT be read or written directly. The configuration files only register the Storage services into the Meta services. You must run the ADD HOSTS command to read and write data on Storage servers. For more information, see Manage Storage hosts . \"How to resolve the error To get the property of the vertex in 'v.age', should use the format 'var.tag.prop' ?\" \u00b6 From Nebula Graph version 3.0.0, patterns support matching multiple tags at the same time, so you need to specify a tag name when querying properties. The original statement RETURN variable_name.property_name is changed to RETURN variable_name.<tag_name>.property_name . \"How to resolve [ERROR (-1005)]: Used memory hits the high watermark(0.800000) of total system memory. ?\" \u00b6 The reason for this error may be that system_memory_high_watermark_ratio specifies the trigger threshold of the memory high watermark alarm mechanism. The default value is 0.8 . If the system memory usage is higher than this value, an alarm mechanism will be triggered, and Nebula Graph will stop querying. Possible solutions are as follows: Clean the system memory to make it below the threshold. Modify the Graph configuration . Add the system_memory_high_watermark_ratio parameter to the configuration files of all Graph servers, and set it greater than 0.8 , such as 0.9 . \"How to resolve the error Storage Error E_RPC_FAILURE ?\" \u00b6 The reason for this error is usually that the storaged process returns too many data back to the graphd process. Possible solutions are as follows: Modify configuration files : Modify the value of --storage_client_timeout_ms in the nebula-graphd.conf file to extend the connection timeout of the Storage client. This configuration is measured in milliseconds (ms). For example, set --storage_client_timeout_ms=60000 . If this parameter is not specified in the nebula-graphd.conf file, specify it manually. Tip: Add --local_config=true at the beginning of the configuration file and restart the service. Optimize the query statement: Reduce queries that scan the entire database. No matter whether LIMIT is used to limit the number of returned results, use the GO statement to rewrite the MATCH statement (the former is optimized, while the latter is not). Check whether the Storaged process has OOM. ( dmesg |grep nebula ). Use better SSD or memory for the Storage Server. Retry. \"How to resolve the error The leader has changed. Try again later ?\" \u00b6 It is a known issue. Just retry 1 to N times, where N is the partition number. The reason is that the meta client needs some heartbeats to update or errors to trigger the new leader information. \"How to resolve [ERROR (-1005)]: Schema not exist: xxx ?\" \u00b6 If the system returns Schema not exist when querying, make sure that: Whether there is a tag or an edge type in the Schema. Whether the name of the tag or the edge type is a keyword. If it is a keyword, enclose them with backquotes (`). For more information, see Keywords . Unable to download SNAPSHOT packages when compiling Exchange, Connectors, or Algorithm \u00b6 Problem description: The system reports Could not find artifact com.vesoft:client:jar:xxx-SNAPSHOT when compiling. Cause: There is no local Maven repository for storing or downloading SNAPSHOT packages. The default central repository in Maven only stores official releases, not development versions (SNAPSHOTs). Solution: Add the following configuration in the profiles scope of Maven's setting.xml file: <profile> <activation> <activeByDefault> true </activeByDefault> </activation> <repositories> <repository> <id> snapshots </id> <url> https://oss.sonatype.org/content/repositories/snapshots/ </url> <snapshots> <enabled> true </enabled> </snapshots> </repository> </repositories> </profile> \"How to resolve [ERROR (-7)]: SyntaxError: syntax error near ?\" \u00b6 In most cases, a query statement requires a YIELD or a RETURN . Check your query statement to see if YIELD or RETURN is provided. \"How to resolve the error can\u2019t solve the start vids from the sentence ?\" \u00b6 The graphd process requires start vids to begin a graph traversal. The start vids can be specified by the user. For example: > GO FROM ${vids} ... > MATCH (src) WHERE id(src) == ${vids} # The \"start vids\" are explicitly given by ${vids}. It can also be found from a property index. For example: # CREATE TAG INDEX IF NOT EXISTS i_player ON player(name(20)); # REBUILD TAG INDEX i_player; > LOOKUP ON player WHERE player.name == \"abc\" | ... YIELD ... > MATCH (src) WHERE src.name == \"abc\" ... # The \"start vids\" are found from the property index \"name\". Otherwise, an error like can\u2019t solve the start vids from the sentence will be returned. \"How to resolve the error Wrong vertex id type: 1001 ?\" \u00b6 Check whether the VID is INT64 or FIXED_STRING(N) set by create space . For more information, see create space . \"How to resolve the error The VID must be a 64-bit integer or a string fitting space vertex id length limit. ?\" \u00b6 Check whether the length of the VID exceeds the limitation. For more information, see create space . \"How to resolve the error edge conflict or vertex conflict ?\" \u00b6 Nebula Graph may return such errors when the Storage service receives multiple requests to insert or update the same vertex or edge within milliseconds. Try the failed requests again later. \"How to resolve the error RPC failure in MetaClient: Connection refused ?\" \u00b6 The reason for this error is usually that the metad service status is unusual, or the network of the machine where the metad and graphd services are located is disconnected. Possible solutions are as follows: Check the metad service status on the server where the metad is located. If the service status is unusual, restart the metad service. Use telnet meta-ip:port to check the network status under the server that returns an error. Check the port information in the configuration file. If the port is different from the one used when connecting, use the port in the configuration file or modify the configuration. \"How to resolve the error StorageClientBase.inl:214] Request to \"x.x.x.x\":9779 failed: N6apache6thrift9transport19TTransportExceptionE: Timed Out in nebula-graph.INFO ?\" \u00b6 The reason for this error may be that the amount of data to be queried is too large, and the storaged process has timed out. Possible solutions are as follows: When importing data, set Compaction manually to make read faster. Extend the RPC connection timeout of the Graph service and the Storage service. Modify the value of --storage_client_timeout_ms in the nebula-storaged.conf file. This configuration is measured in milliseconds (ms). The default value is 60000ms. \"How to resolve the error MetaClient.cpp:65] Heartbeat failed, status:Wrong cluster! in nebula-storaged.INFO , or HBProcessor.cpp:54] Reject wrong cluster host \"x.x.x.x\":9771! in nebula-metad.INFO ? \u00b6 The reason for this error may be that the user has modified the IP or the port information of the metad process, or the storage service has joined other clusters before. Possible solutions are as follows: Delete the cluster.id file in the installation directory where the storage machine is deployed (the default installation directory is /usr/local/nebula ), and restart the storaged service. About design and functions \u00b6 \"How is the time spent value at the end of each return message calculated?\" \u00b6 Take the returned message of SHOW SPACES as an example: nebula> SHOW SPACES; +--------------------+ | Name | +--------------------+ | \"basketballplayer\" | +--------------------+ Got 1 rows (time spent 1235/1934 us) The first number 1235 shows the time spent by the database itself, that is, the time it takes for the query engine to receive a query from the client, fetch the data from the storage server, and perform a series of calculations. The second number 1934 shows the time spent from the client's perspective, that is, the time it takes for the client from sending a request, receiving a response, and displaying the result on the screen. Why does the port number of the nebula-storaged process keep showing red after connecting to Nebula Graph? \u00b6 Because the nebula-storaged process waits for nebula-metad to add the current Storage service during the startup process. The Storage works after it receives the ready signal. Starting from Nebula Graph 3.0.0, the Meta service cannot directly read or write data in the Storage service that you add in the configuration file. The configuration file only registers the Storage service to the Meta service. You must run the ADD HOSTS command to enable the Meta to read and write data in the Storage service. For more information, see Manage Storage hosts . Why is there no line separating each row in the returned result of Nebula Graph 2.6.0? \u00b6 This is caused by the release of Nebula Console 2.6.0, not the change of Nebula Graph core. And it will not affect the content of the returned data itself. About dangling edges \u00b6 A dangling edge is an edge that only connects to a single vertex and only one part of the edge connects to the vertex. Dangling edges may appear in Nebula Graph 3.1.0 as the design. And there is no MERGE statements of openCypher. The guarantee for dangling edges depends entirely on the application level. For more information, see INSERT VERTEX , DELETE VERTEX , INSERT EDGE , DELETE EDGE . \"Can I set replica_factor as an even number in CREATE SPACE statements, e.g., replica_factor = 2 ?\" \u00b6 NO. The Storage service guarantees its availability based on the Raft consensus protocol. The number of failed replicas must not exceed half of the total replica number. When the number of machines is 1, replica_factor can only be set to 1 . When there are enough machines and replica_factor=2 , if one replica fails, the Storage service fails. No matter replica_factor=3 or replica_factor=4 , if more than one replica fails, the Storage Service fails. To prevent unnecessary waste of resources, we recommend that you set an odd replica number. We suggest that you set replica_factor=3 for a production environment and replica_factor=1 for a test environment. Do not use an even number. \"Is stopping or killing slow queries supported?\" \u00b6 Yes. For more information, see Kill query . \"Why are the query results different when using GO and MATCH to execute the same semantic query?\" \u00b6 The possible reasons are listed as follows. GO statements find the dangling edges. RETURN commands do not specify the sequence. The dense vertex truncation limitation defined by max_edge_returned_per_vertex in the Storage service is triggered. Using different types of paths may cause different query results. GO statements use walk . Both vertices and edges can be repeatedly visited in graph traversal. MATCH statements are compatible with openCypher and use trail . Only vertices can be repeatedly visited in graph traversal. The example is as follows. All queries that start from A with 5 hops will end at C ( A->B->C->D->E->C ). If it is 6 hops, the GO statement will end at D ( A->B->C->D->E->C->D ), because the edge C->D can be visited repeatedly. However, the MATCH statement returns empty, because edges cannot be visited repeatedly. Therefore, using GO and MATCH to execute the same semantic query may cause different query results. For more information, see Wikipedia . \"How to count the vertices/edges number of each tag/edge type?\" \u00b6 See show-stats . \"How to get all the vertices/edge of each tag/edge type?\" \u00b6 Create and rebuild the index. > CREATE TAG INDEX IF NOT EXISTS i_player ON player(); > REBUILD TAG INDEX IF NOT EXISTS i_player; Use LOOKUP or MATCH . For example: > LOOKUP ON player; > MATCH (n:player) RETURN n; For more information, see INDEX , LOOKUP , and MATCH . \"How to get all the vertices/edges without specifying the types?\" \u00b6 By nGQL, you CAN NOT directly getting all the vertices without specifying the tags, neither the edges, or you can use the LIMIT clause to limit the number of returns. E.g., You CAN NOT run MATCH (n) RETURN (n) . An error like Scan vertices or edges need to specify a limit number, or limit number can not push down. will be returned. You can use Nebula Algorithm . Or get vertices by each tag, and then group them by yourself. Can non-English characters be used as identifiers, such as the names of graph spaces, tags, edge types, properties, and indexes? \u00b6 Yes, for more information, see Keywords and reserved words . \"How to get the out-degree/the in-degree of a vertex with a given name\"? \u00b6 The out-degree of a vertex refers to the number of edges starting from that vertex, while the in-degree refers to the number of edges pointing to that vertex. nebula > MATCH (s)-[e]->() WHERE id(s) == \"given\" RETURN count(e); #Out-degree nebula > MATCH (s)<-[e]-() WHERE id(s) == \"given\" RETURN count(e); #In-degree This is a very slow operation to get the out/in degree since no accelaration can be applied (no indices or caches). It also could be out-of-memory when hitting a supper-node . \"How to quickly get the out-degree and in-degree of all vertices?\" \u00b6 There is no such command. You can use Nebula Algorithm . About operation and maintenance \u00b6 \"The runtime log files are too large. How to recycle the logs?\" \u00b6 By default, the runtime logs of Nebula Graph are stored in /usr/local/nebula/logs/ . The INFO level log files are nebula-graphd.INFO, nebula-storaged.INFO, nebula-metad.INFO . If an alarm or error occurs, the suffixes are modified as .WARNING or .ERROR . Nebula Graph uses glog to print logs. glog cannot recycle the outdated files. To rotate logs, you can: Use crontab to delete logs periodically. For more information, see Glog should delete old log files automatically . Use logrotate to manage log files. Before using logrotate, modify the configurations of corresponding services and set timestamp_in_logfile_name to false . \"How to check the Nebula Graph version?\" \u00b6 If the service is running: run command SHOW HOSTS META in nebula-console . See SHOW HOSTS . If the service is not running: Different installation methods make the method of checking the version different. The instructions are as follows: If the service is not running, run the command ./<binary_name> --version to get the version and the Git commit IDs of the Nebula Graph binary files. For example: $ ./nebula-graphd --version If you deploy Nebula Graph with Docker Compose Check the version of Nebula Graph deployed by Docker Compose. The method is similar to the previous method, except that you have to enter the container first. The commands are as follows: docker exec -it nebula-docker-compose_graphd_1 bash cd bin/ ./nebula-graphd --version If you install Nebula Graph with RPM/DEB package Run rpm -qa |grep nebula to check the version of Nebula Graph. \"How to scale out or scale in? (Enterprise Edition only)\" \u00b6 You can scale Graph and Storage services with Dashboard Enterprise Edition. For details, see Scale . You can also use Nebula Operator to scale Graph and Storage services. For details, see Deploy Nebula Graph clusters with Kubectl and Deploy Nebula Graph clusters with Helm . Nebula Graph 3.1.0 does not provide any commands or tools to support automatic scale out/in. You can refer to the following steps: Scale out and scale in metad: The metad process can not be scaled out or scale in. The process cannot be moved to a new machine. You cannot add a new metad process to the service. Note You can use the Meta transfer script tool to migrate Meta services. Note that the Meta-related settings in the configuration files of Storage and Graph services need to be modified correspondingly. Scale in graphd: Remove the IP of the graphd process from the code in the client. Close this graphd process. Scale out graphd: Prepare the binary and config files of the graphd process in the new host. Modify the config files and add all existing addresses of the metad processes. Then start the new graphd process. Scale in storaged: (The number of replicas must be greater than 1) See Balance remove command . After the command is finished, stop this storaged process. Scale out storaged: (The number of replicas must be greater than 1) Prepare the binary and config files of the storaged process in the new host, Modify the config files and add all existing addresses of the metad processes. Then register the storaged process to the metad, and then start the new storaged process. For details, see Register storaged services . You also need to run Balance Data and Balance leader after scaling in/out storaged. \"After changing the name of the host, the old one keeps displaying OFFLINE . What should I do?\" \u00b6 Hosts with the status of OFFLINE will be automatically deleted after one day. About connections \u00b6 \"Which ports should be opened on the firewalls?\" \u00b6 If you have not modified the predefined ports in the Configurations , open the following ports for the Nebula Graph services: Service Port Meta 9559, 9560, 19559, 19560 Graph 9669, 19669, 19670 Storage 9777 ~ 9780, 19779, 19780 If you have customized the configuration files and changed the predefined ports, find the port numbers in your configuration files and open them on the firewalls. For those eco-tools, see the corresponding document. \"How to test whether a port is open or closed?\" \u00b6 You can use telnet as follows to check for port status. telnet <ip> <port> Note If you cannot use the telnet command, check if telnet is installed or enabled on your host. For example: // If the port is open: $ telnet 192 .168.1.10 9669 Trying 192 .168.1.10... Connected to 192 .168.1.10. Escape character is '^]' . // If the port is closed or blocked: $ telnet 192 .168.1.10 9777 Trying 192 .168.1.10... telnet: connect to address 192 .168.1.10: Connection refused About license \u00b6 Are the Dashboard/Explorer/Nebula Graph Enterprise Edition licenses the same? \u00b6 No, the licenses of Dashboard, Explorer, and Nebula Graph Enterprise Editions are independent of each other and cannot be used interchangeably. During the validity period of the Nebula Graph Enterprise Edition license, after replacing the enterprise edition Meta with the community edition Meta, can the community edition Meta be used with the enterprise edition Graph and Storage? \u00b6 No, mixed deployments of the enterprise edition services and the community edition services are not supported. After the Nebula Graph Enterprise Edition license expires, is it possible that copy the data in the data directory and paste it to the same directory of Nebula Graph Community Edition, and then use Nebula Graph services as normal? \u00b6 Yes, it is possible. The data of the Enterprise Edition can be used in the Community Edition. The pasted data will only work properly in the services deployed in the Community Edition. Mixed deployments of the enterprise edition services and the community edition services are not supported. For example, the mixed deployment of the enterprise edition Meta service and the community edition Graph and Storage services is not supported. Is there any message before the license expires, and how to renew the license after it expires? \u00b6 The system will send expiration notifications before the license expires. The notification time before the license expires is different for the full version license and the trial version license. For the full version license: Within 30 days before the license expires or on the day the license expires, there is an expiration reminder when Nebula Graph/Dashboard/Explorer is started. There is a 14-day buffer period after expiration. During the buffer period, you will receive expiration notifications and can continue using Nebula Graph/Dashboard/Explorer. After the buffer period ends, the corresponding service will be down and cannot be started. For the trial version license: Within 7 days before the license expires or on the day the license expires, there is an expiration reminder when Nebula Graph/Dashboard/Explorer is started. There is no buffer period after expiration. Once the license expires, the corresponding service will be down and cannot be started. After your license expires, contact us via inqury@vesoft.com to renew it.","title":"FAQ"},{"location":"20.appendix/0.FAQ/#faq","text":"This topic lists the frequently asked questions for using Nebula Graph 3.1.0. You can use the search box in the help center or the search function of the browser to match the questions you are looking for. If the solutions described in this topic cannot solve your problems, ask for help on the Nebula Graph forum or submit an issue on GitHub issue .","title":"FAQ"},{"location":"20.appendix/0.FAQ/#about_manual_updates","text":"","title":"About manual updates"},{"location":"20.appendix/0.FAQ/#why_is_the_behavior_in_the_manual_not_consistent_with_the_system","text":"Nebula Graph is still under development. Its behavior changes from time to time. Users can submit an issue to inform the team if the manual and the system are not consistent. Note If you find some errors in this topic: Click the pencil button at the top right side of this page. Use markdown to fix this error. Then click \"Commit changes\" at the bottom, which will start a Github pull request. Sign the CLA . This pull request will be merged after the acceptance of at least two reviewers.","title":"\"Why is the behavior in the manual not consistent with the system?\""},{"location":"20.appendix/0.FAQ/#about_legacy_version_compatibility","text":"X version compatibility Neubla Graph 3.1.0 is not compatible with Nebula Graph 1.x nor 2.0-RC in both data formats and RPC-protocols, and vice versa . The service process may quit if using an lower version client to connect to a higher version server. To upgrade data formats, see Upgrade Nebula Graph to the current version . Users must upgrade all clients .","title":"About legacy version compatibility"},{"location":"20.appendix/0.FAQ/#about_execution_errors","text":"","title":"About execution errors"},{"location":"20.appendix/0.FAQ/#how_to_resolve_the_error_semanticerror_missing_yield_clause","text":"Starting with Nebula Graph 3.0.0, the statements LOOKUP , GO , and FETCH must output results with the YIELD clause. For more information, see YIELD .","title":"\"How to resolve the error SemanticError: Missing yield clause.?\""},{"location":"20.appendix/0.FAQ/#how_to_resolve_the_error_host_not_enough","text":"From Nebula Graph version 3.0.0, the Storage services added in the configuration files CANNOT be read or written directly. The configuration files only register the Storage services into the Meta services. You must run the ADD HOSTS command to read and write data on Storage servers. For more information, see Manage Storage hosts .","title":"\"How to resolve the error Host not enough!?\""},{"location":"20.appendix/0.FAQ/#how_to_resolve_the_error_to_get_the_property_of_the_vertex_in_vage_should_use_the_format_vartagprop","text":"From Nebula Graph version 3.0.0, patterns support matching multiple tags at the same time, so you need to specify a tag name when querying properties. The original statement RETURN variable_name.property_name is changed to RETURN variable_name.<tag_name>.property_name .","title":"\"How to resolve the error To get the property of the vertex in 'v.age', should use the format 'var.tag.prop'?\""},{"location":"20.appendix/0.FAQ/#how_to_resolve_error_-1005_used_memory_hits_the_high_watermark0800000_of_total_system_memory","text":"The reason for this error may be that system_memory_high_watermark_ratio specifies the trigger threshold of the memory high watermark alarm mechanism. The default value is 0.8 . If the system memory usage is higher than this value, an alarm mechanism will be triggered, and Nebula Graph will stop querying. Possible solutions are as follows: Clean the system memory to make it below the threshold. Modify the Graph configuration . Add the system_memory_high_watermark_ratio parameter to the configuration files of all Graph servers, and set it greater than 0.8 , such as 0.9 .","title":"\"How to resolve [ERROR (-1005)]: Used memory hits the high watermark(0.800000) of total system memory.?\""},{"location":"20.appendix/0.FAQ/#how_to_resolve_the_error_storage_error_e_rpc_failure","text":"The reason for this error is usually that the storaged process returns too many data back to the graphd process. Possible solutions are as follows: Modify configuration files : Modify the value of --storage_client_timeout_ms in the nebula-graphd.conf file to extend the connection timeout of the Storage client. This configuration is measured in milliseconds (ms). For example, set --storage_client_timeout_ms=60000 . If this parameter is not specified in the nebula-graphd.conf file, specify it manually. Tip: Add --local_config=true at the beginning of the configuration file and restart the service. Optimize the query statement: Reduce queries that scan the entire database. No matter whether LIMIT is used to limit the number of returned results, use the GO statement to rewrite the MATCH statement (the former is optimized, while the latter is not). Check whether the Storaged process has OOM. ( dmesg |grep nebula ). Use better SSD or memory for the Storage Server. Retry.","title":"\"How to resolve the error Storage Error E_RPC_FAILURE?\""},{"location":"20.appendix/0.FAQ/#how_to_resolve_the_error_the_leader_has_changed_try_again_later","text":"It is a known issue. Just retry 1 to N times, where N is the partition number. The reason is that the meta client needs some heartbeats to update or errors to trigger the new leader information.","title":"\"How to resolve the error The leader has changed. Try again later?\""},{"location":"20.appendix/0.FAQ/#how_to_resolve_error_-1005_schema_not_exist_xxx","text":"If the system returns Schema not exist when querying, make sure that: Whether there is a tag or an edge type in the Schema. Whether the name of the tag or the edge type is a keyword. If it is a keyword, enclose them with backquotes (`). For more information, see Keywords .","title":"\"How to resolve [ERROR (-1005)]: Schema not exist: xxx?\""},{"location":"20.appendix/0.FAQ/#unable_to_download_snapshot_packages_when_compiling_exchange_connectors_or_algorithm","text":"Problem description: The system reports Could not find artifact com.vesoft:client:jar:xxx-SNAPSHOT when compiling. Cause: There is no local Maven repository for storing or downloading SNAPSHOT packages. The default central repository in Maven only stores official releases, not development versions (SNAPSHOTs). Solution: Add the following configuration in the profiles scope of Maven's setting.xml file: <profile> <activation> <activeByDefault> true </activeByDefault> </activation> <repositories> <repository> <id> snapshots </id> <url> https://oss.sonatype.org/content/repositories/snapshots/ </url> <snapshots> <enabled> true </enabled> </snapshots> </repository> </repositories> </profile>","title":"Unable to download SNAPSHOT packages when compiling Exchange, Connectors, or Algorithm"},{"location":"20.appendix/0.FAQ/#how_to_resolve_error_-7_syntaxerror_syntax_error_near","text":"In most cases, a query statement requires a YIELD or a RETURN . Check your query statement to see if YIELD or RETURN is provided.","title":"\"How to resolve [ERROR (-7)]: SyntaxError: syntax error near?\""},{"location":"20.appendix/0.FAQ/#how_to_resolve_the_error_cant_solve_the_start_vids_from_the_sentence","text":"The graphd process requires start vids to begin a graph traversal. The start vids can be specified by the user. For example: > GO FROM ${vids} ... > MATCH (src) WHERE id(src) == ${vids} # The \"start vids\" are explicitly given by ${vids}. It can also be found from a property index. For example: # CREATE TAG INDEX IF NOT EXISTS i_player ON player(name(20)); # REBUILD TAG INDEX i_player; > LOOKUP ON player WHERE player.name == \"abc\" | ... YIELD ... > MATCH (src) WHERE src.name == \"abc\" ... # The \"start vids\" are found from the property index \"name\". Otherwise, an error like can\u2019t solve the start vids from the sentence will be returned.","title":"\"How to resolve the error can\u2019t solve the start vids from the sentence?\""},{"location":"20.appendix/0.FAQ/#how_to_resolve_the_error_wrong_vertex_id_type_1001","text":"Check whether the VID is INT64 or FIXED_STRING(N) set by create space . For more information, see create space .","title":"\"How to resolve the error Wrong vertex id type: 1001?\""},{"location":"20.appendix/0.FAQ/#how_to_resolve_the_error_the_vid_must_be_a_64-bit_integer_or_a_string_fitting_space_vertex_id_length_limit","text":"Check whether the length of the VID exceeds the limitation. For more information, see create space .","title":"\"How to resolve the error The VID must be a 64-bit integer or a string fitting space vertex id length limit.?\""},{"location":"20.appendix/0.FAQ/#how_to_resolve_the_error_edge_conflict_or_vertex_conflict","text":"Nebula Graph may return such errors when the Storage service receives multiple requests to insert or update the same vertex or edge within milliseconds. Try the failed requests again later.","title":"\"How to resolve the error edge conflict or vertex conflict?\""},{"location":"20.appendix/0.FAQ/#how_to_resolve_the_error_rpc_failure_in_metaclient_connection_refused","text":"The reason for this error is usually that the metad service status is unusual, or the network of the machine where the metad and graphd services are located is disconnected. Possible solutions are as follows: Check the metad service status on the server where the metad is located. If the service status is unusual, restart the metad service. Use telnet meta-ip:port to check the network status under the server that returns an error. Check the port information in the configuration file. If the port is different from the one used when connecting, use the port in the configuration file or modify the configuration.","title":"\"How to resolve the error RPC failure in MetaClient: Connection refused?\""},{"location":"20.appendix/0.FAQ/#how_to_resolve_the_error_storageclientbaseinl214_request_to_xxxx9779_failed_n6apache6thrift9transport19ttransportexceptione_timed_out_in_nebula-graphinfo","text":"The reason for this error may be that the amount of data to be queried is too large, and the storaged process has timed out. Possible solutions are as follows: When importing data, set Compaction manually to make read faster. Extend the RPC connection timeout of the Graph service and the Storage service. Modify the value of --storage_client_timeout_ms in the nebula-storaged.conf file. This configuration is measured in milliseconds (ms). The default value is 60000ms.","title":"\"How to resolve the error StorageClientBase.inl:214] Request to \"x.x.x.x\":9779 failed: N6apache6thrift9transport19TTransportExceptionE: Timed Out in nebula-graph.INFO?\""},{"location":"20.appendix/0.FAQ/#how_to_resolve_the_error_metaclientcpp65_heartbeat_failed_statuswrong_cluster_in_nebula-storagedinfo_or_hbprocessorcpp54_reject_wrong_cluster_host_xxxx9771_in_nebula-metadinfo","text":"The reason for this error may be that the user has modified the IP or the port information of the metad process, or the storage service has joined other clusters before. Possible solutions are as follows: Delete the cluster.id file in the installation directory where the storage machine is deployed (the default installation directory is /usr/local/nebula ), and restart the storaged service.","title":"\"How to resolve the error MetaClient.cpp:65] Heartbeat failed, status:Wrong cluster! in nebula-storaged.INFO, or HBProcessor.cpp:54] Reject wrong cluster host \"x.x.x.x\":9771! in nebula-metad.INFO?"},{"location":"20.appendix/0.FAQ/#about_design_and_functions","text":"","title":"About design and functions"},{"location":"20.appendix/0.FAQ/#how_is_the_time_spent_value_at_the_end_of_each_return_message_calculated","text":"Take the returned message of SHOW SPACES as an example: nebula> SHOW SPACES; +--------------------+ | Name | +--------------------+ | \"basketballplayer\" | +--------------------+ Got 1 rows (time spent 1235/1934 us) The first number 1235 shows the time spent by the database itself, that is, the time it takes for the query engine to receive a query from the client, fetch the data from the storage server, and perform a series of calculations. The second number 1934 shows the time spent from the client's perspective, that is, the time it takes for the client from sending a request, receiving a response, and displaying the result on the screen.","title":"\"How is the time spent value at the end of each return message calculated?\""},{"location":"20.appendix/0.FAQ/#why_does_the_port_number_of_the_nebula-storaged_process_keep_showing_red_after_connecting_to_nebula_graph","text":"Because the nebula-storaged process waits for nebula-metad to add the current Storage service during the startup process. The Storage works after it receives the ready signal. Starting from Nebula Graph 3.0.0, the Meta service cannot directly read or write data in the Storage service that you add in the configuration file. The configuration file only registers the Storage service to the Meta service. You must run the ADD HOSTS command to enable the Meta to read and write data in the Storage service. For more information, see Manage Storage hosts .","title":"Why does the port number of the nebula-storaged process keep showing red after connecting to Nebula Graph?"},{"location":"20.appendix/0.FAQ/#why_is_there_no_line_separating_each_row_in_the_returned_result_of_nebula_graph_260","text":"This is caused by the release of Nebula Console 2.6.0, not the change of Nebula Graph core. And it will not affect the content of the returned data itself.","title":"Why is there no line separating each row in the returned result of Nebula Graph 2.6.0?"},{"location":"20.appendix/0.FAQ/#about_dangling_edges","text":"A dangling edge is an edge that only connects to a single vertex and only one part of the edge connects to the vertex. Dangling edges may appear in Nebula Graph 3.1.0 as the design. And there is no MERGE statements of openCypher. The guarantee for dangling edges depends entirely on the application level. For more information, see INSERT VERTEX , DELETE VERTEX , INSERT EDGE , DELETE EDGE .","title":"About dangling edges"},{"location":"20.appendix/0.FAQ/#can_i_set_replica_factor_as_an_even_number_in_create_space_statements_eg_replica_factor_2","text":"NO. The Storage service guarantees its availability based on the Raft consensus protocol. The number of failed replicas must not exceed half of the total replica number. When the number of machines is 1, replica_factor can only be set to 1 . When there are enough machines and replica_factor=2 , if one replica fails, the Storage service fails. No matter replica_factor=3 or replica_factor=4 , if more than one replica fails, the Storage Service fails. To prevent unnecessary waste of resources, we recommend that you set an odd replica number. We suggest that you set replica_factor=3 for a production environment and replica_factor=1 for a test environment. Do not use an even number.","title":"\"Can I set replica_factor as an even number in CREATE SPACE statements, e.g., replica_factor = 2?\""},{"location":"20.appendix/0.FAQ/#is_stopping_or_killing_slow_queries_supported","text":"Yes. For more information, see Kill query .","title":"\"Is stopping or killing slow queries supported?\""},{"location":"20.appendix/0.FAQ/#why_are_the_query_results_different_when_using_go_and_match_to_execute_the_same_semantic_query","text":"The possible reasons are listed as follows. GO statements find the dangling edges. RETURN commands do not specify the sequence. The dense vertex truncation limitation defined by max_edge_returned_per_vertex in the Storage service is triggered. Using different types of paths may cause different query results. GO statements use walk . Both vertices and edges can be repeatedly visited in graph traversal. MATCH statements are compatible with openCypher and use trail . Only vertices can be repeatedly visited in graph traversal. The example is as follows. All queries that start from A with 5 hops will end at C ( A->B->C->D->E->C ). If it is 6 hops, the GO statement will end at D ( A->B->C->D->E->C->D ), because the edge C->D can be visited repeatedly. However, the MATCH statement returns empty, because edges cannot be visited repeatedly. Therefore, using GO and MATCH to execute the same semantic query may cause different query results. For more information, see Wikipedia .","title":"\"Why are the query results different when using GO and MATCH to execute the same semantic query?\""},{"location":"20.appendix/0.FAQ/#how_to_count_the_verticesedges_number_of_each_tagedge_type","text":"See show-stats .","title":"\"How to count the vertices/edges number of each tag/edge type?\""},{"location":"20.appendix/0.FAQ/#how_to_get_all_the_verticesedge_of_each_tagedge_type","text":"Create and rebuild the index. > CREATE TAG INDEX IF NOT EXISTS i_player ON player(); > REBUILD TAG INDEX IF NOT EXISTS i_player; Use LOOKUP or MATCH . For example: > LOOKUP ON player; > MATCH (n:player) RETURN n; For more information, see INDEX , LOOKUP , and MATCH .","title":"\"How to get all the vertices/edge of each tag/edge type?\""},{"location":"20.appendix/0.FAQ/#how_to_get_all_the_verticesedges_without_specifying_the_types","text":"By nGQL, you CAN NOT directly getting all the vertices without specifying the tags, neither the edges, or you can use the LIMIT clause to limit the number of returns. E.g., You CAN NOT run MATCH (n) RETURN (n) . An error like Scan vertices or edges need to specify a limit number, or limit number can not push down. will be returned. You can use Nebula Algorithm . Or get vertices by each tag, and then group them by yourself.","title":"\"How to get all the vertices/edges without specifying the types?\""},{"location":"20.appendix/0.FAQ/#can_non-english_characters_be_used_as_identifiers_such_as_the_names_of_graph_spaces_tags_edge_types_properties_and_indexes","text":"Yes, for more information, see Keywords and reserved words .","title":"Can non-English characters be used as identifiers, such as the names of graph spaces, tags, edge types, properties, and indexes?"},{"location":"20.appendix/0.FAQ/#how_to_get_the_out-degreethe_in-degree_of_a_vertex_with_a_given_name","text":"The out-degree of a vertex refers to the number of edges starting from that vertex, while the in-degree refers to the number of edges pointing to that vertex. nebula > MATCH (s)-[e]->() WHERE id(s) == \"given\" RETURN count(e); #Out-degree nebula > MATCH (s)<-[e]-() WHERE id(s) == \"given\" RETURN count(e); #In-degree This is a very slow operation to get the out/in degree since no accelaration can be applied (no indices or caches). It also could be out-of-memory when hitting a supper-node .","title":"\"How to get the out-degree/the in-degree of a vertex with a given name\"?"},{"location":"20.appendix/0.FAQ/#how_to_quickly_get_the_out-degree_and_in-degree_of_all_vertices","text":"There is no such command. You can use Nebula Algorithm .","title":"\"How to quickly get the out-degree and in-degree of all vertices?\""},{"location":"20.appendix/0.FAQ/#about_operation_and_maintenance","text":"","title":"About operation and maintenance"},{"location":"20.appendix/0.FAQ/#the_runtime_log_files_are_too_large_how_to_recycle_the_logs","text":"By default, the runtime logs of Nebula Graph are stored in /usr/local/nebula/logs/ . The INFO level log files are nebula-graphd.INFO, nebula-storaged.INFO, nebula-metad.INFO . If an alarm or error occurs, the suffixes are modified as .WARNING or .ERROR . Nebula Graph uses glog to print logs. glog cannot recycle the outdated files. To rotate logs, you can: Use crontab to delete logs periodically. For more information, see Glog should delete old log files automatically . Use logrotate to manage log files. Before using logrotate, modify the configurations of corresponding services and set timestamp_in_logfile_name to false .","title":"\"The runtime log files are too large. How to recycle the logs?\""},{"location":"20.appendix/0.FAQ/#how_to_check_the_nebula_graph_version","text":"If the service is running: run command SHOW HOSTS META in nebula-console . See SHOW HOSTS . If the service is not running: Different installation methods make the method of checking the version different. The instructions are as follows: If the service is not running, run the command ./<binary_name> --version to get the version and the Git commit IDs of the Nebula Graph binary files. For example: $ ./nebula-graphd --version If you deploy Nebula Graph with Docker Compose Check the version of Nebula Graph deployed by Docker Compose. The method is similar to the previous method, except that you have to enter the container first. The commands are as follows: docker exec -it nebula-docker-compose_graphd_1 bash cd bin/ ./nebula-graphd --version If you install Nebula Graph with RPM/DEB package Run rpm -qa |grep nebula to check the version of Nebula Graph.","title":"\"How to check the Nebula Graph version?\""},{"location":"20.appendix/0.FAQ/#how_to_scale_out_or_scale_in_enterprise_edition_only","text":"You can scale Graph and Storage services with Dashboard Enterprise Edition. For details, see Scale . You can also use Nebula Operator to scale Graph and Storage services. For details, see Deploy Nebula Graph clusters with Kubectl and Deploy Nebula Graph clusters with Helm . Nebula Graph 3.1.0 does not provide any commands or tools to support automatic scale out/in. You can refer to the following steps: Scale out and scale in metad: The metad process can not be scaled out or scale in. The process cannot be moved to a new machine. You cannot add a new metad process to the service. Note You can use the Meta transfer script tool to migrate Meta services. Note that the Meta-related settings in the configuration files of Storage and Graph services need to be modified correspondingly. Scale in graphd: Remove the IP of the graphd process from the code in the client. Close this graphd process. Scale out graphd: Prepare the binary and config files of the graphd process in the new host. Modify the config files and add all existing addresses of the metad processes. Then start the new graphd process. Scale in storaged: (The number of replicas must be greater than 1) See Balance remove command . After the command is finished, stop this storaged process. Scale out storaged: (The number of replicas must be greater than 1) Prepare the binary and config files of the storaged process in the new host, Modify the config files and add all existing addresses of the metad processes. Then register the storaged process to the metad, and then start the new storaged process. For details, see Register storaged services . You also need to run Balance Data and Balance leader after scaling in/out storaged.","title":"\"How to scale out or scale in? (Enterprise Edition only)\""},{"location":"20.appendix/0.FAQ/#after_changing_the_name_of_the_host_the_old_one_keeps_displaying_offline_what_should_i_do","text":"Hosts with the status of OFFLINE will be automatically deleted after one day.","title":"\"After changing the name of the host, the old one keeps displaying OFFLINE. What should I do?\""},{"location":"20.appendix/0.FAQ/#about_connections","text":"","title":"About connections"},{"location":"20.appendix/0.FAQ/#which_ports_should_be_opened_on_the_firewalls","text":"If you have not modified the predefined ports in the Configurations , open the following ports for the Nebula Graph services: Service Port Meta 9559, 9560, 19559, 19560 Graph 9669, 19669, 19670 Storage 9777 ~ 9780, 19779, 19780 If you have customized the configuration files and changed the predefined ports, find the port numbers in your configuration files and open them on the firewalls. For those eco-tools, see the corresponding document.","title":"\"Which ports should be opened on the firewalls?\""},{"location":"20.appendix/0.FAQ/#how_to_test_whether_a_port_is_open_or_closed","text":"You can use telnet as follows to check for port status. telnet <ip> <port> Note If you cannot use the telnet command, check if telnet is installed or enabled on your host. For example: // If the port is open: $ telnet 192 .168.1.10 9669 Trying 192 .168.1.10... Connected to 192 .168.1.10. Escape character is '^]' . // If the port is closed or blocked: $ telnet 192 .168.1.10 9777 Trying 192 .168.1.10... telnet: connect to address 192 .168.1.10: Connection refused","title":"\"How to test whether a port is open or closed?\""},{"location":"20.appendix/0.FAQ/#about_license","text":"","title":"About license"},{"location":"20.appendix/0.FAQ/#are_the_dashboardexplorernebula_graph_enterprise_edition_licenses_the_same","text":"No, the licenses of Dashboard, Explorer, and Nebula Graph Enterprise Editions are independent of each other and cannot be used interchangeably.","title":"Are the Dashboard/Explorer/Nebula Graph Enterprise Edition licenses the same?"},{"location":"20.appendix/0.FAQ/#during_the_validity_period_of_the_nebula_graph_enterprise_edition_license_after_replacing_the_enterprise_edition_meta_with_the_community_edition_meta_can_the_community_edition_meta_be_used_with_the_enterprise_edition_graph_and_storage","text":"No, mixed deployments of the enterprise edition services and the community edition services are not supported.","title":"During the validity period of the Nebula Graph Enterprise Edition license, after replacing the enterprise edition Meta with the community edition Meta, can the community edition Meta be used with the enterprise edition Graph and Storage?"},{"location":"20.appendix/0.FAQ/#after_the_nebula_graph_enterprise_edition_license_expires_is_it_possible_that_copy_the_data_in_the_data_directory_and_paste_it_to_the_same_directory_of_nebula_graph_community_edition_and_then_use_nebula_graph_services_as_normal","text":"Yes, it is possible. The data of the Enterprise Edition can be used in the Community Edition. The pasted data will only work properly in the services deployed in the Community Edition. Mixed deployments of the enterprise edition services and the community edition services are not supported. For example, the mixed deployment of the enterprise edition Meta service and the community edition Graph and Storage services is not supported.","title":"After the Nebula Graph Enterprise Edition license expires, is it possible that copy the data in the data directory and paste it to the same directory of Nebula Graph Community Edition, and then use Nebula Graph services as normal?"},{"location":"20.appendix/0.FAQ/#is_there_any_message_before_the_license_expires_and_how_to_renew_the_license_after_it_expires","text":"The system will send expiration notifications before the license expires. The notification time before the license expires is different for the full version license and the trial version license. For the full version license: Within 30 days before the license expires or on the day the license expires, there is an expiration reminder when Nebula Graph/Dashboard/Explorer is started. There is a 14-day buffer period after expiration. During the buffer period, you will receive expiration notifications and can continue using Nebula Graph/Dashboard/Explorer. After the buffer period ends, the corresponding service will be down and cannot be started. For the trial version license: Within 7 days before the license expires or on the day the license expires, there is an expiration reminder when Nebula Graph/Dashboard/Explorer is started. There is no buffer period after expiration. Once the license expires, the corresponding service will be down and cannot be started. After your license expires, contact us via inqury@vesoft.com to renew it.","title":"Is there any message before the license expires, and how to renew the license after it expires?"},{"location":"20.appendix/6.eco-tool-version/","text":"Ecosystem tools overview \u00b6 Compatibility The core release number naming rule is X.Y.Z , which means Major version X , Medium version Y , and Minor version Z . The upgrade requirements for the client are: Upgrade the core from X.Y.Z1 to X.Y.Z2 : It means that the core is fully forward compatible and is usually used for bugfixes. It is recommended to upgrade the minor version of the core as soon as possible. At this time, the client can stay not upgraded . Upgrade the core from X.Y1.* to X.Y2.* : It means that there is some incompatibility of API, syntax, and return value. It is usually used to add functions, improve performance, and optimize code. The client needs to be upgraded to X.Y2.* . Upgrade the core from X1.*.* to X2.*.* : It means that there is a major incompatibility in storage formats, API, syntax, etc. You need to use tools to upgrade the core data. The client must be upgraded. The default core and client do not support downgrade: You cannot downgrade from X.Y.Z2 to X.Y.Z1 . The release cycle of a Y version is about 6 months, and its maintenance and support cycle is 6 months. The version released at the beginning of the year is usually named X.0.0 , and in the middle of the year, it is named X.5.0 . The file name contains RC to indicate an unofficial version ( Release Candidate ) that is only used for preview. Its maintenance period is only until the next RC or official version is released. Its client, data compatibility, etc. are not guaranteed. The files with nightly , SNAPSHOT , or date are the nightly versions. There is no quality assurance and maintenance period. Nebula Graph Studio \u00b6 Nebula Graph Studio (Studio for short) is a graph database visualization tool that can be accessed through the Web. It can be used with Nebula Graph DBMS to provide one-stop services such as composition, data import, writing nGQL queries, and graph exploration. For details, see What is Nebula Graph Studio . Note The release of the Studio is independent of Nebula Graph core, and its naming method is also not the same as the core naming rules. Nebula Graph version Studio version 3.1.0 3.3.0 Nebula Dashboard Community Edition \u00b6 Nebula Dashboard Community Edition (Dashboard for short) is a visualization tool for monitoring the status of machines and services in the Nebula Graph cluster. For details, see What is Nebula Dashboard . Nebula Graph version Dashboard Community version 3.1.0 3.0.0 Nebula Dashboard Enterprise Edition \u00b6 Nebula Dashboard Enterprise Edition (Dashboard for short) is a visualization tool that monitors and manages the status of machines and services in Nebula Graph cluster. For details, see What is Nebula Dashboard . Nebula Graph version Dashboard Enterprise version 3.1.0 3.0.0 Nebula Explorer \u00b6 Nebula Explorer (Explorer for short) is a graph exploration visualization tool that can be accessed through the Web. It is used with the Nebula Graph core to visualize interaction with graph data. Users can quickly become map experts, even without experience in map data manipulation. For details, see What is Nebula Explorer . Nebula Graph version Explorer Enterprise version 3.1.0 3.0.0 Nebula Stats Exporter \u00b6 Nebula-stats-exporter exports monitor metrics to Promethus. Nebula Graph version Stats Exporter version 3.1.0 3.0.0 Nebula Exchange \u00b6 Nebula Exchange (Exchange for short) is an Apache Spark&trade application for batch migration of data in a cluster to Nebula Graph in a distributed environment. It can support the migration of batch data and streaming data in a variety of different formats. For details, see What is Nebula Exchange . Nebula Graph version Exchange Community version Exchange Enterprise version 3.1.0 3.0.0 3.0.0 Nebula Operator \u00b6 Nebula Operator (Operator for short) is a tool to automate the deployment, operation, and maintenance of Nebula Graph clusters on Kubernetes. Building upon the excellent scalability mechanism of Kubernetes, Nebula Graph introduced its operation and maintenance knowledge into the Kubernetes system, which makes Nebula Graph a real cloud-native graph database. For more information, see What is Nebula Operator . Nebula Graph version Operator version 3.1.0 1.1.0 Nebula Importer \u00b6 Nebula Importer (Importer for short) is a CSV file import tool for Nebula Graph. The Importer can read the local CSV file, and then import the data into the Nebula Graph database. For details, see What is Nebula Importer . Nebula Graph version Importer version 3.1.0 3.1.0 Nebula Spark Connector \u00b6 Nebula Spark Connector is a Spark connector that provides the ability to read and write Nebula Graph data in the Spark standard format. Nebula Spark Connector consists of two parts, Reader and Writer. For details, see What is Nebula Spark Connector . Nebula Graph version Spark Connector version 3.1.0 3.0.0 Nebula Flink Connector \u00b6 Nebula Flink Connector is a connector that helps Flink users quickly access Nebula Graph. It supports reading data from the Nebula Graph database or writing data read from other external data sources to the Nebula Graph database. For details, see What is Nebula Flink Connector . Nebula Graph version Flink Connector version 3.1.0 3.0.0 Nebula Algorithm \u00b6 Nebula Algorithm (Algorithm for short) is a Spark application based on GraphX , which uses a complete algorithm tool to analyze data in the Nebula Graph database by submitting a Spark task To perform graph computing, use the algorithm under the lib repository through programming to perform graph computing for DataFrame. For details, see What is Nebula Algorithm . Nebula Graph version Algorithm version 3.1.0 3.0.0 Nebula Analytics \u00b6 Nebula Analytics is an application that integrates the open-source Plato Graph Computing Framework, with which Nebula Analytics performs graph computations on Nebula Graph database data. For details, see What is Nebula Analytics . Nebula Graph version Analytics version 3.1.0 1.1.0 Nebula Console \u00b6 Nebula Console is the native CLI client of Nebula Graph. For how to use it, see Nebula Console . Nebula Graph version Console version 3.1.0 3.0.0 Nebula Docker Compose \u00b6 Docker Compose can quickly deploy Nebula Graph clusters. For how to use it, please refer to Docker Compose Deployment Nebula Graph . Nebula Graph version Docker Compose version 3.1.0 3.1.0 Backup & Restore \u00b6 Backup&Restore (BR for short) is a command line interface (CLI) tool that can help back up the graph space data of Nebula Graph, or restore it through a backup file data. Nebula Graph version BR version 3.1.0 0.7.0 Nebula Bench \u00b6 Nebula Bench is used to test the baseline performance data of Nebula Graph. It uses the standard data set of LDBC. Nebula Graph version Bench version 3.1.0 1.1.0 API, SDK \u00b6 Compatibility Select the latest version of X.Y.* which is the same as the core version. Nebula Graph version Language (commit id) 3.1.0 C++ 3.1.0 Go 3.1.0 Python 3.1.0 Java 3.1.0 HTTP Not Released \u00b6 API Rust Client Node.js Client [Object Graph Mapping Library (OGM, or ORM)] Java, Python (TODO: in design) Test Chaos Test","title":"Ecosystem tools"},{"location":"20.appendix/6.eco-tool-version/#ecosystem_tools_overview","text":"Compatibility The core release number naming rule is X.Y.Z , which means Major version X , Medium version Y , and Minor version Z . The upgrade requirements for the client are: Upgrade the core from X.Y.Z1 to X.Y.Z2 : It means that the core is fully forward compatible and is usually used for bugfixes. It is recommended to upgrade the minor version of the core as soon as possible. At this time, the client can stay not upgraded . Upgrade the core from X.Y1.* to X.Y2.* : It means that there is some incompatibility of API, syntax, and return value. It is usually used to add functions, improve performance, and optimize code. The client needs to be upgraded to X.Y2.* . Upgrade the core from X1.*.* to X2.*.* : It means that there is a major incompatibility in storage formats, API, syntax, etc. You need to use tools to upgrade the core data. The client must be upgraded. The default core and client do not support downgrade: You cannot downgrade from X.Y.Z2 to X.Y.Z1 . The release cycle of a Y version is about 6 months, and its maintenance and support cycle is 6 months. The version released at the beginning of the year is usually named X.0.0 , and in the middle of the year, it is named X.5.0 . The file name contains RC to indicate an unofficial version ( Release Candidate ) that is only used for preview. Its maintenance period is only until the next RC or official version is released. Its client, data compatibility, etc. are not guaranteed. The files with nightly , SNAPSHOT , or date are the nightly versions. There is no quality assurance and maintenance period.","title":"Ecosystem tools overview"},{"location":"20.appendix/6.eco-tool-version/#nebula_graph_studio","text":"Nebula Graph Studio (Studio for short) is a graph database visualization tool that can be accessed through the Web. It can be used with Nebula Graph DBMS to provide one-stop services such as composition, data import, writing nGQL queries, and graph exploration. For details, see What is Nebula Graph Studio . Note The release of the Studio is independent of Nebula Graph core, and its naming method is also not the same as the core naming rules. Nebula Graph version Studio version 3.1.0 3.3.0","title":"Nebula Graph Studio"},{"location":"20.appendix/6.eco-tool-version/#nebula_dashboard_community_edition","text":"Nebula Dashboard Community Edition (Dashboard for short) is a visualization tool for monitoring the status of machines and services in the Nebula Graph cluster. For details, see What is Nebula Dashboard . Nebula Graph version Dashboard Community version 3.1.0 3.0.0","title":"Nebula Dashboard Community Edition"},{"location":"20.appendix/6.eco-tool-version/#nebula_dashboard_enterprise_edition","text":"Nebula Dashboard Enterprise Edition (Dashboard for short) is a visualization tool that monitors and manages the status of machines and services in Nebula Graph cluster. For details, see What is Nebula Dashboard . Nebula Graph version Dashboard Enterprise version 3.1.0 3.0.0","title":"Nebula Dashboard Enterprise Edition"},{"location":"20.appendix/6.eco-tool-version/#nebula_explorer","text":"Nebula Explorer (Explorer for short) is a graph exploration visualization tool that can be accessed through the Web. It is used with the Nebula Graph core to visualize interaction with graph data. Users can quickly become map experts, even without experience in map data manipulation. For details, see What is Nebula Explorer . Nebula Graph version Explorer Enterprise version 3.1.0 3.0.0","title":"Nebula Explorer"},{"location":"20.appendix/6.eco-tool-version/#nebula_stats_exporter","text":"Nebula-stats-exporter exports monitor metrics to Promethus. Nebula Graph version Stats Exporter version 3.1.0 3.0.0","title":"Nebula Stats Exporter"},{"location":"20.appendix/6.eco-tool-version/#nebula_exchange","text":"Nebula Exchange (Exchange for short) is an Apache Spark&trade application for batch migration of data in a cluster to Nebula Graph in a distributed environment. It can support the migration of batch data and streaming data in a variety of different formats. For details, see What is Nebula Exchange . Nebula Graph version Exchange Community version Exchange Enterprise version 3.1.0 3.0.0 3.0.0","title":"Nebula Exchange"},{"location":"20.appendix/6.eco-tool-version/#nebula_operator","text":"Nebula Operator (Operator for short) is a tool to automate the deployment, operation, and maintenance of Nebula Graph clusters on Kubernetes. Building upon the excellent scalability mechanism of Kubernetes, Nebula Graph introduced its operation and maintenance knowledge into the Kubernetes system, which makes Nebula Graph a real cloud-native graph database. For more information, see What is Nebula Operator . Nebula Graph version Operator version 3.1.0 1.1.0","title":"Nebula Operator"},{"location":"20.appendix/6.eco-tool-version/#nebula_importer","text":"Nebula Importer (Importer for short) is a CSV file import tool for Nebula Graph. The Importer can read the local CSV file, and then import the data into the Nebula Graph database. For details, see What is Nebula Importer . Nebula Graph version Importer version 3.1.0 3.1.0","title":"Nebula Importer"},{"location":"20.appendix/6.eco-tool-version/#nebula_spark_connector","text":"Nebula Spark Connector is a Spark connector that provides the ability to read and write Nebula Graph data in the Spark standard format. Nebula Spark Connector consists of two parts, Reader and Writer. For details, see What is Nebula Spark Connector . Nebula Graph version Spark Connector version 3.1.0 3.0.0","title":"Nebula Spark Connector"},{"location":"20.appendix/6.eco-tool-version/#nebula_flink_connector","text":"Nebula Flink Connector is a connector that helps Flink users quickly access Nebula Graph. It supports reading data from the Nebula Graph database or writing data read from other external data sources to the Nebula Graph database. For details, see What is Nebula Flink Connector . Nebula Graph version Flink Connector version 3.1.0 3.0.0","title":"Nebula Flink Connector"},{"location":"20.appendix/6.eco-tool-version/#nebula_algorithm","text":"Nebula Algorithm (Algorithm for short) is a Spark application based on GraphX , which uses a complete algorithm tool to analyze data in the Nebula Graph database by submitting a Spark task To perform graph computing, use the algorithm under the lib repository through programming to perform graph computing for DataFrame. For details, see What is Nebula Algorithm . Nebula Graph version Algorithm version 3.1.0 3.0.0","title":"Nebula Algorithm"},{"location":"20.appendix/6.eco-tool-version/#nebula_analytics","text":"Nebula Analytics is an application that integrates the open-source Plato Graph Computing Framework, with which Nebula Analytics performs graph computations on Nebula Graph database data. For details, see What is Nebula Analytics . Nebula Graph version Analytics version 3.1.0 1.1.0","title":"Nebula Analytics"},{"location":"20.appendix/6.eco-tool-version/#nebula_console","text":"Nebula Console is the native CLI client of Nebula Graph. For how to use it, see Nebula Console . Nebula Graph version Console version 3.1.0 3.0.0","title":"Nebula Console"},{"location":"20.appendix/6.eco-tool-version/#nebula_docker_compose","text":"Docker Compose can quickly deploy Nebula Graph clusters. For how to use it, please refer to Docker Compose Deployment Nebula Graph . Nebula Graph version Docker Compose version 3.1.0 3.1.0","title":"Nebula Docker Compose"},{"location":"20.appendix/6.eco-tool-version/#backup_restore","text":"Backup&Restore (BR for short) is a command line interface (CLI) tool that can help back up the graph space data of Nebula Graph, or restore it through a backup file data. Nebula Graph version BR version 3.1.0 0.7.0","title":"Backup &amp; Restore"},{"location":"20.appendix/6.eco-tool-version/#nebula_bench","text":"Nebula Bench is used to test the baseline performance data of Nebula Graph. It uses the standard data set of LDBC. Nebula Graph version Bench version 3.1.0 1.1.0","title":"Nebula Bench"},{"location":"20.appendix/6.eco-tool-version/#api_sdk","text":"Compatibility Select the latest version of X.Y.* which is the same as the core version. Nebula Graph version Language (commit id) 3.1.0 C++ 3.1.0 Go 3.1.0 Python 3.1.0 Java 3.1.0 HTTP","title":"API, SDK"},{"location":"20.appendix/6.eco-tool-version/#not_released","text":"API Rust Client Node.js Client [Object Graph Mapping Library (OGM, or ORM)] Java, Python (TODO: in design) Test Chaos Test","title":"Not Released"},{"location":"20.appendix/error-code/","text":"Error code \u00b6 Nebula Graph returns an error code when an error occurs. This topic describes the details of the error code returned. Note If an error occurs but no error code is returned, or if the error code description is unclear, we welcome your feedback or suggestions on the forum or GitHub . When the code returned is 0 , it means that the operation is successful. Error Code Description -1 Lost connection -2 Unable to establish connection -3 RPC failure -4 Raft leader has been changed -5 Graph space does not exist -6 Tag does not exist -7 Edge type does not exist -8 Index does not exist -9 Edge type property does not exist -10 Tag property does not exist -11 The current role does not exist -12 The current configuration does not exist -13 The current host does not exist -15 Listener does not exist -16 The current partition does not exist -17 Key does not exist -18 User does not exist -19 Statistics do not exist -20 No current service found -21 Drainer does not exist -22 Drainer client does not exist -24 Backup failed -25 The backed-up table is empty -26 Table backup failure -27 MultiGet could not get all data -28 Index rebuild failed -29 Password is invalid -30 Unable to get absolute path -1001 Authentication failed -1002 Invalid session -1003 Session timeout -1004 Syntax error -1005 Execution error -1006 Statement is empty -1008 Permission denied -1009 Semantic error -1010 Maximum number of connections exceeded -1011 Access to storage failed (only some requests succeeded) -2001 Host does not exist -2002 Host already exists -2003 Invalid host -2004 The current command, statement, or function is not supported -2007 Configuration items cannot be changed -2008 Parameters conflict with meta data -2009 Invalid parameter -2010 Wrong cluster -2011 Listener conflicts -2021 Failed to store data -2022 Illegal storage segment -2023 Invalid data balancing plan -2024 The cluster is already in the data balancing status -2025 There is no running data balancing plan -2026 Lack of valid hosts -2027 A data balancing plan that has been corrupted -2029 Lack of valid drainers -2030 Failed to recover user role -2031 Number of invalid partitions -2032 Invalid replica factor -2033 Invalid character set -2034 Invalid character sorting rules -2035 Character set and character sorting rule mismatch -2040 Failed to generate a snapshot -2041 Failed to write block data -2044 Failed to add new task -2045 Failed to stop task -2046 Failed to save task information -2047 Data balancing failed -2048 The current task has not been completed -2049 Task report failed -2050 The current task is not in the graph space -2051 The current task needs to be resumed -2065 Invalid task -2066 Backup terminated (index being created) -2067 Graph space does not exist at the time of backup -2068 Backup recovery failed -2069 Session does not exist -2070 Failed to get cluster information -2071 Failed to get absolute path when getting cluster information -2072 Unable to get an agent when getting cluster information -2073 Query not found -2074 Failed to receive heartbeat from agent -2080 Invalid variable -2081 Variable value and type do not match -3001 Consensus cannot be reached during an election -3002 Key already exists -3003 Data type mismatch -3004 Invalid field value -3005 Invalid operation -3006 Current value is not allowed to be empty -3007 Field value must be set if the field value is NOT NULL or has no default value -3008 The value is out of the range of the current type -3010 Data conflict -3011 Writes are delayed -3021 Incorrect data type -3022 Invalid VID length -3031 Invalid filter -3032 Invalid field update -3033 Invalid KV storage -3034 Peer invalid -3035 Out of retries -3036 Leader change failed -3037 Invalid stat type -3038 VID is invalid -3040 Failed to load meta information -3041 Failed to generate checkpoint -3042 Generating checkpoint is blocked -3043 Data is filtered -3044 Invalid data -3045 Concurrent write conflicts on the same edge -3046 Concurrent write conflict on the same vertex -3047 Lock is invalid -3051 Invalid task parameter -3052 The user canceled the task -3053 Task execution failed -3060 Execution plan was cleared -3061 Client and server versions are not compatible -3062 Failed to get ID serial number -3070 The heartbeat process was not completed when the request was received -3071 Out-of-date heartbeat received from the old leader (the new leader has been elected) -3073 Concurrent write conflicts with later requests -3500 Unknown partition -3501 Raft logs lag behind -3502 Raft logs are out of date -3503 Heartbeat messages are out of date -3504 Unknown additional logs -3511 Waiting for the snapshot to complete -3512 There was an error sending the snapshot -3513 Invalid receiver -3514 Raft did not start -3515 Raft has stopped -3516 Wrong role -3521 Write to a WAL failed -3522 The host has stopped -3523 Too many requests -3524 Persistent snapshot failed -3525 RPC exception -3526 No WAL logs found -3527 Host suspended -3528 Writes are blocked -3529 Cache overflow -3530 Atomic operation failed -3531 Leader lease expired -3532 Data has been synchronized on Raft -4001 Drainer logs lag behind -4002 Drainer logs are out of date -4003 The drainer data storage is invalid -4004 Graph space mismatch -4005 Partition mismatch -4006 Data conflict -4007 Request conflict -4008 Illegal data -5001 Cache configuration error -5002 Insufficient space -5003 No cache hit -5005 Write cache failed -7001 Number of machines exceeded the limit -7002 Failed to resolve certificate -8000 Unknown error","title":"Error code"},{"location":"20.appendix/error-code/#error_code","text":"Nebula Graph returns an error code when an error occurs. This topic describes the details of the error code returned. Note If an error occurs but no error code is returned, or if the error code description is unclear, we welcome your feedback or suggestions on the forum or GitHub . When the code returned is 0 , it means that the operation is successful. Error Code Description -1 Lost connection -2 Unable to establish connection -3 RPC failure -4 Raft leader has been changed -5 Graph space does not exist -6 Tag does not exist -7 Edge type does not exist -8 Index does not exist -9 Edge type property does not exist -10 Tag property does not exist -11 The current role does not exist -12 The current configuration does not exist -13 The current host does not exist -15 Listener does not exist -16 The current partition does not exist -17 Key does not exist -18 User does not exist -19 Statistics do not exist -20 No current service found -21 Drainer does not exist -22 Drainer client does not exist -24 Backup failed -25 The backed-up table is empty -26 Table backup failure -27 MultiGet could not get all data -28 Index rebuild failed -29 Password is invalid -30 Unable to get absolute path -1001 Authentication failed -1002 Invalid session -1003 Session timeout -1004 Syntax error -1005 Execution error -1006 Statement is empty -1008 Permission denied -1009 Semantic error -1010 Maximum number of connections exceeded -1011 Access to storage failed (only some requests succeeded) -2001 Host does not exist -2002 Host already exists -2003 Invalid host -2004 The current command, statement, or function is not supported -2007 Configuration items cannot be changed -2008 Parameters conflict with meta data -2009 Invalid parameter -2010 Wrong cluster -2011 Listener conflicts -2021 Failed to store data -2022 Illegal storage segment -2023 Invalid data balancing plan -2024 The cluster is already in the data balancing status -2025 There is no running data balancing plan -2026 Lack of valid hosts -2027 A data balancing plan that has been corrupted -2029 Lack of valid drainers -2030 Failed to recover user role -2031 Number of invalid partitions -2032 Invalid replica factor -2033 Invalid character set -2034 Invalid character sorting rules -2035 Character set and character sorting rule mismatch -2040 Failed to generate a snapshot -2041 Failed to write block data -2044 Failed to add new task -2045 Failed to stop task -2046 Failed to save task information -2047 Data balancing failed -2048 The current task has not been completed -2049 Task report failed -2050 The current task is not in the graph space -2051 The current task needs to be resumed -2065 Invalid task -2066 Backup terminated (index being created) -2067 Graph space does not exist at the time of backup -2068 Backup recovery failed -2069 Session does not exist -2070 Failed to get cluster information -2071 Failed to get absolute path when getting cluster information -2072 Unable to get an agent when getting cluster information -2073 Query not found -2074 Failed to receive heartbeat from agent -2080 Invalid variable -2081 Variable value and type do not match -3001 Consensus cannot be reached during an election -3002 Key already exists -3003 Data type mismatch -3004 Invalid field value -3005 Invalid operation -3006 Current value is not allowed to be empty -3007 Field value must be set if the field value is NOT NULL or has no default value -3008 The value is out of the range of the current type -3010 Data conflict -3011 Writes are delayed -3021 Incorrect data type -3022 Invalid VID length -3031 Invalid filter -3032 Invalid field update -3033 Invalid KV storage -3034 Peer invalid -3035 Out of retries -3036 Leader change failed -3037 Invalid stat type -3038 VID is invalid -3040 Failed to load meta information -3041 Failed to generate checkpoint -3042 Generating checkpoint is blocked -3043 Data is filtered -3044 Invalid data -3045 Concurrent write conflicts on the same edge -3046 Concurrent write conflict on the same vertex -3047 Lock is invalid -3051 Invalid task parameter -3052 The user canceled the task -3053 Task execution failed -3060 Execution plan was cleared -3061 Client and server versions are not compatible -3062 Failed to get ID serial number -3070 The heartbeat process was not completed when the request was received -3071 Out-of-date heartbeat received from the old leader (the new leader has been elected) -3073 Concurrent write conflicts with later requests -3500 Unknown partition -3501 Raft logs lag behind -3502 Raft logs are out of date -3503 Heartbeat messages are out of date -3504 Unknown additional logs -3511 Waiting for the snapshot to complete -3512 There was an error sending the snapshot -3513 Invalid receiver -3514 Raft did not start -3515 Raft has stopped -3516 Wrong role -3521 Write to a WAL failed -3522 The host has stopped -3523 Too many requests -3524 Persistent snapshot failed -3525 RPC exception -3526 No WAL logs found -3527 Host suspended -3528 Writes are blocked -3529 Cache overflow -3530 Atomic operation failed -3531 Leader lease expired -3532 Data has been synchronized on Raft -4001 Drainer logs lag behind -4002 Drainer logs are out of date -4003 The drainer data storage is invalid -4004 Graph space mismatch -4005 Partition mismatch -4006 Data conflict -4007 Request conflict -4008 Illegal data -5001 Cache configuration error -5002 Insufficient space -5003 No cache hit -5005 Write cache failed -7001 Number of machines exceeded the limit -7002 Failed to resolve certificate -8000 Unknown error","title":"Error code"},{"location":"20.appendix/history/","text":"History timeline for Nebula Graph \u00b6 2018.9: dutor wrote and submitted the first line of Nebula Graph database code. 2019.5: Nebula Graph v0.1.0-alpha was released as open-source. Nebula Graph v1.0.0-beta, v1.0.0-rc1, v1.0.0-rc2, v1.0.0-rc3, and v1.0.0-rc4 were released one after another within a year thereafter. 2019.7: Nebula Graph's debut at HBaseCon 1 . @ dangleptr 2020.3: Nebula Graph v2.0 was starting developed in the final stage of v1.0 development. 2020.6: The first major version of Nebula Graph v1.0.0 GA was released. 2021.3: The second major version of Nebula Graph v2.0 GA was released. 2021.8: Nebula Graph v2.5.0 was released. 2021.10: Nebula Graph v2.6.0 was released. For more information about release notes, see Releases . 2022.2: Nebula Graph v3.0.0 was released. For more information about release notes, see Releases . Nebula Graph v1.x supports both RocksDB and HBase as its storage engines. Nebula Graph v2.x removes HBase supports. \u21a9","title":"History timeline"},{"location":"20.appendix/history/#history_timeline_for_nebula_graph","text":"2018.9: dutor wrote and submitted the first line of Nebula Graph database code. 2019.5: Nebula Graph v0.1.0-alpha was released as open-source. Nebula Graph v1.0.0-beta, v1.0.0-rc1, v1.0.0-rc2, v1.0.0-rc3, and v1.0.0-rc4 were released one after another within a year thereafter. 2019.7: Nebula Graph's debut at HBaseCon 1 . @ dangleptr 2020.3: Nebula Graph v2.0 was starting developed in the final stage of v1.0 development. 2020.6: The first major version of Nebula Graph v1.0.0 GA was released. 2021.3: The second major version of Nebula Graph v2.0 GA was released. 2021.8: Nebula Graph v2.5.0 was released. 2021.10: Nebula Graph v2.6.0 was released. For more information about release notes, see Releases . 2022.2: Nebula Graph v3.0.0 was released. For more information about release notes, see Releases . Nebula Graph v1.x supports both RocksDB and HBase as its storage engines. Nebula Graph v2.x removes HBase supports. \u21a9","title":"History timeline for Nebula Graph"},{"location":"20.appendix/learning-path/","text":"Nebula Graph learning path \u00b6 This topic is for anyone interested in learning more about Nebula Graph. You can master Nebula Graph from zero to hero through the documentation and videos in Nebula Graph learning path. 1. About Nebula Graph \u00b6 1.1 What is Nebula Graph? \u00b6 Document Video What is Nebula Graph Nebula Graph 1.2 Data models \u00b6 Document Data modeling 1.3 Path \u00b6 Document Path 1.4 Nebula Graph architecture \u00b6 Document Meta service Graph service Storage service 2. Quick start \u00b6 2.1 Install Nebula Graph \u00b6 Document Video Install with a RPM or DEB package - Install with a TAR package - Install with Docker Install Nebula Graph with Docker and Docker Compose Install from source Install Nebula Graph with Source Code 2.2 Start Nebula Graph \u00b6 Document Start and stop Nebula Graph 2.3 Connect to Nebula Graph \u00b6 Document Connect to Nebula Graph 2.4 Use nGQL statements \u00b6 Document nGQL cheatsheet 3. Hands-on practices \u00b6 3.1 Deploy a multi-machine cluster \u00b6 Document Deploy a Nebula Graph cluster with RPM/DEB 3.2 Upgrade Nebula Graph \u00b6 Document Upgrade Nebula Graph to release-3.1 Upgrade Nebula Graph from v2.0.x to release-3.1 3.3 Configure Nebula Graph \u00b6 Document Configure Meta Configure Graph Configure Storage Configure Linux kernel 3.4 Configure logs \u00b6 Document Log managements 3.5 O&M and Management \u00b6 Account authentication and authorization Document Local authentication OpenLDAP User management Roles and privileges Balance the distribution of partitions Document Storage load balancing Monitoring Document Nebula Graph metrics RocksDB statistics Data snapshot Document Create snapshots SSL encryption Document SSL 3.6 Performance tuning \u00b6 Document Graph data modeling suggestions System design suggestions Compaction 3.7 Derivative software \u00b6 Visualization Visualization tools Document Video Data visualization Nebula Graph Studio Nebula Studio Data monitoring and O&M Nebula Dashboard Community Edition \u548c Nebula Dashboard Enterprise Edition - Data analysis Nebula Explorer Enterprise Edition - Data import and export Import and export Document Video Data import Nebula Importer Nebula Importer Data import Nebula Spark Connector - Data import Nebula Flink Connector - Data import Nebula Exchange Community Edition - Data export Nebula Exchange Enterprise Edition - Performance test Document Nebula Bench Cluster O&M Document Nebula Operator Graph algorithm Document Nebula Algorithm Clients Document Nebula Console Nebula CPP Nebula Java Nebula Python Nebula Go 4. API & SDK \u00b6 Document API & SDK 5. Best practices \u00b6 Document Handling Tens of Billions of Threat Intelligence Data with Graph Database at Kuaishou Import data from Neo4j to Nebula Graph via Nebula Exchange: Best Practices Hands-On Experience: Import Data to Nebula Graph with Spark How to Select a Graph Database: Best Practices at RoyalFlush Practicing Nebula Operator on Cloud Using Ansible to Automate Deployment of Nebula Graph Cluster 6. FAQ \u00b6 Document FAQ 7. Practical tasks \u00b6 You can check if you have mastered Nebula Graph by completing the following practical tasks. Task Reference Compile the source code of Nebula Graph Install Nebula Graph by compiling the source code Deploy Studio, Dashboard, and Explorer Deploy Studio , Deploy Dashboard , and Deploy Explorer Load test Nebula Graph with K6 Nebula Bench Import LDBC data Nebula Graph 1.0 Benchmark Report based on the LDBC Dataset Query LDBC data\uff08such as queries for vertices, paths, or subgraphs.\uff09 LDBC and interactive-short-1.cypher","title":"Learning path"},{"location":"20.appendix/learning-path/#nebula_graph_learning_path","text":"This topic is for anyone interested in learning more about Nebula Graph. You can master Nebula Graph from zero to hero through the documentation and videos in Nebula Graph learning path.","title":"Nebula Graph learning path"},{"location":"20.appendix/learning-path/#1_about_nebula_graph","text":"","title":"1. About Nebula Graph"},{"location":"20.appendix/learning-path/#11_what_is_nebula_graph","text":"Document Video What is Nebula Graph Nebula Graph","title":"1.1 What is Nebula Graph?"},{"location":"20.appendix/learning-path/#12_data_models","text":"Document Data modeling","title":"1.2 Data models"},{"location":"20.appendix/learning-path/#13_path","text":"Document Path","title":"1.3 Path"},{"location":"20.appendix/learning-path/#14_nebula_graph_architecture","text":"Document Meta service Graph service Storage service","title":"1.4 Nebula Graph architecture"},{"location":"20.appendix/learning-path/#2_quick_start","text":"","title":"2. Quick start"},{"location":"20.appendix/learning-path/#21_install_nebula_graph","text":"Document Video Install with a RPM or DEB package - Install with a TAR package - Install with Docker Install Nebula Graph with Docker and Docker Compose Install from source Install Nebula Graph with Source Code","title":"2.1 Install Nebula Graph"},{"location":"20.appendix/learning-path/#22_start_nebula_graph","text":"Document Start and stop Nebula Graph","title":"2.2 Start Nebula Graph"},{"location":"20.appendix/learning-path/#23_connect_to_nebula_graph","text":"Document Connect to Nebula Graph","title":"2.3 Connect to Nebula Graph"},{"location":"20.appendix/learning-path/#24_use_ngql_statements","text":"Document nGQL cheatsheet","title":"2.4 Use nGQL statements"},{"location":"20.appendix/learning-path/#3_hands-on_practices","text":"","title":"3. Hands-on practices"},{"location":"20.appendix/learning-path/#31_deploy_a_multi-machine_cluster","text":"Document Deploy a Nebula Graph cluster with RPM/DEB","title":"3.1 Deploy a multi-machine cluster"},{"location":"20.appendix/learning-path/#32_upgrade_nebula_graph","text":"Document Upgrade Nebula Graph to release-3.1 Upgrade Nebula Graph from v2.0.x to release-3.1","title":"3.2 Upgrade Nebula Graph"},{"location":"20.appendix/learning-path/#33_configure_nebula_graph","text":"Document Configure Meta Configure Graph Configure Storage Configure Linux kernel","title":"3.3 Configure Nebula Graph"},{"location":"20.appendix/learning-path/#34_configure_logs","text":"Document Log managements","title":"3.4 Configure logs"},{"location":"20.appendix/learning-path/#35_om_and_management","text":"Account authentication and authorization Document Local authentication OpenLDAP User management Roles and privileges Balance the distribution of partitions Document Storage load balancing Monitoring Document Nebula Graph metrics RocksDB statistics Data snapshot Document Create snapshots SSL encryption Document SSL","title":"3.5 O&amp;M and Management"},{"location":"20.appendix/learning-path/#36_performance_tuning","text":"Document Graph data modeling suggestions System design suggestions Compaction","title":"3.6 Performance tuning"},{"location":"20.appendix/learning-path/#37_derivative_software","text":"Visualization Visualization tools Document Video Data visualization Nebula Graph Studio Nebula Studio Data monitoring and O&M Nebula Dashboard Community Edition \u548c Nebula Dashboard Enterprise Edition - Data analysis Nebula Explorer Enterprise Edition - Data import and export Import and export Document Video Data import Nebula Importer Nebula Importer Data import Nebula Spark Connector - Data import Nebula Flink Connector - Data import Nebula Exchange Community Edition - Data export Nebula Exchange Enterprise Edition - Performance test Document Nebula Bench Cluster O&M Document Nebula Operator Graph algorithm Document Nebula Algorithm Clients Document Nebula Console Nebula CPP Nebula Java Nebula Python Nebula Go","title":"3.7 Derivative software"},{"location":"20.appendix/learning-path/#4_api_sdk","text":"Document API & SDK","title":"4. API &amp; SDK"},{"location":"20.appendix/learning-path/#5_best_practices","text":"Document Handling Tens of Billions of Threat Intelligence Data with Graph Database at Kuaishou Import data from Neo4j to Nebula Graph via Nebula Exchange: Best Practices Hands-On Experience: Import Data to Nebula Graph with Spark How to Select a Graph Database: Best Practices at RoyalFlush Practicing Nebula Operator on Cloud Using Ansible to Automate Deployment of Nebula Graph Cluster","title":"5. Best practices"},{"location":"20.appendix/learning-path/#6_faq","text":"Document FAQ","title":"6. FAQ"},{"location":"20.appendix/learning-path/#7_practical_tasks","text":"You can check if you have mastered Nebula Graph by completing the following practical tasks. Task Reference Compile the source code of Nebula Graph Install Nebula Graph by compiling the source code Deploy Studio, Dashboard, and Explorer Deploy Studio , Deploy Dashboard , and Deploy Explorer Load test Nebula Graph with K6 Nebula Bench Import LDBC data Nebula Graph 1.0 Benchmark Report based on the LDBC Dataset Query LDBC data\uff08such as queries for vertices, paths, or subgraphs.\uff09 LDBC and interactive-short-1.cypher","title":"7. Practical tasks"},{"location":"20.appendix/releasenote/","text":"Nebula Graph 3.1.0 release notes \u00b6 Enhancement \u00b6 Patterns can now be used in WHERE statements. For example: MATCH (v:player) WHERE (v)-[:like]->() RETURN v . #3997 CLEAR SPACE can be used to clear graph space and index data, but the graph space schema and index names are reserved. #3989 The vertex alias can be repeated in match patterns, like MATCH (v)-->(v) . #3929 Optimized SUBGRAPH and FIND PATH for better performance. #3871 #4095 Optimized query paths to reduce redundant paths and time complexity. 4126 Optimized the method to get properties for better performance of MATCH statements. #3750 Optimized GO and YIELD clauses to avoid extracting redundant properties. #3974 Support for filter and limit pushdown when getting properties. 3844 3839 Support for aggregation pushdown in LOOKUP statements. #3504 maxHop is optional in MATCH variable-length paths. #3881 Graph spaces are physically deleted after using DROP SPACE . #3913 Optimized number parsing in date time, date, time. #3797 Added the toSet function which converts LIST or SET to SET . #3594 nGQL statements can be used to display the HTTP port of Nebula Graph services and the HTTP2 port has been disabled. #3808 The number of sessions for connections to each graphd with the same client IP and the same user is limited. #3729 Optimized the waiting mechanism to ensure a timely connection to the metad after the storaged starts. #3971 When a node has multiple paths and an error of the disk corresponding to a particular path occurs, it is no longer to rebuild the node. #4131 Optimized the job manager. #3976 #4045 #4001 The DOWNLOAD and INGEST SST files are now managed with the job manager. #3994 Support for error code display when a job fails. #4067 The OS page cache can be disabled and the block cache and Nebula Graph storage cache can only be used in a shared environment, to avoid memory usage interference between applications. #3890 Updated the default value of the KV separation threshold from 0 to 100. #3879 Support for using gflag to set the upper limit of expression depth for a better fit of different machine environments. #3722 Added a permission check for KILL QUERY . When the authorization is enabled, the GOD user can kill any query and the users with other roles can only kill queries that they own. #3896 Support for more complier launchers, including distcc and sccache. #3896 More dumping tables are supported with the meta dump tool. #3870 The storage layer controls the concurrency of write operations (INSERT VERTEX or EDGE) from reporting an error and requiring a client retry to using the internal queueing mechanism. #3926 Bugfix \u00b6 Fixed the crash when using a function call as part of a filter in a LOOKUP statement. #4111 Fixed the crash when there were non-indexed properties in an IN clause. #3986 Fixed the storage service crash when concurrently scanning vertices and edges. #4190 Fixed the crash when performing aggregation queries with patterns in a MATCH statement. #4180 Fixed the crash when getting the JSON results of a profile query. #3998 Fixed the crash when the async interface in the Lambda function finished running and the task in threadManager was not executed. #4000 Fixed the GROUP BY output bug. #4128 Fixed the bug that the version wasn't displayed with SHOW HOSTS sometimes. #4116 Fixed the bug on parameters for id(n) == $var , id(n) IN [$var] , id(n) == $var.foo.bar , and id(n) IN $var.foo.bar . #4024 Fixed the bug that an incorrect path direction occurred in MATCH...WHERE . #4091 Fixed the bug that the result of referencing multiple MATCH variables in a WHERE clause was incorrect. #4143 Fixed the optimizer bug. #4146 Fixed the bug that the storage service failed to handle Raft snapshots. #4019 Fixed the bug that the storage service would not accept more logs after receiving a snapshot. #3909 Fixed the bug that snapshots did not contain the vertices without tags. #4189 Fixed the latest schema version read failure when the schema version is greater than 255. #4023 Fixed the bug that SHOW STATS did not count the vertices that had no tags. #3967 Fixed the bug that the timestamp was fetched incorrectly sometimes. #3958 Fixed the bug that the root user could be granted with other roles in the graph space. #3868 Fixed the duplicate count of column indexes in the lexical parser bug. #3626 Legacy versions \u00b6 Release notes of legacy versions","title":"Release Note"},{"location":"20.appendix/releasenote/#nebula_graph_310_release_notes","text":"","title":"Nebula Graph 3.1.0 release notes"},{"location":"20.appendix/releasenote/#enhancement","text":"Patterns can now be used in WHERE statements. For example: MATCH (v:player) WHERE (v)-[:like]->() RETURN v . #3997 CLEAR SPACE can be used to clear graph space and index data, but the graph space schema and index names are reserved. #3989 The vertex alias can be repeated in match patterns, like MATCH (v)-->(v) . #3929 Optimized SUBGRAPH and FIND PATH for better performance. #3871 #4095 Optimized query paths to reduce redundant paths and time complexity. 4126 Optimized the method to get properties for better performance of MATCH statements. #3750 Optimized GO and YIELD clauses to avoid extracting redundant properties. #3974 Support for filter and limit pushdown when getting properties. 3844 3839 Support for aggregation pushdown in LOOKUP statements. #3504 maxHop is optional in MATCH variable-length paths. #3881 Graph spaces are physically deleted after using DROP SPACE . #3913 Optimized number parsing in date time, date, time. #3797 Added the toSet function which converts LIST or SET to SET . #3594 nGQL statements can be used to display the HTTP port of Nebula Graph services and the HTTP2 port has been disabled. #3808 The number of sessions for connections to each graphd with the same client IP and the same user is limited. #3729 Optimized the waiting mechanism to ensure a timely connection to the metad after the storaged starts. #3971 When a node has multiple paths and an error of the disk corresponding to a particular path occurs, it is no longer to rebuild the node. #4131 Optimized the job manager. #3976 #4045 #4001 The DOWNLOAD and INGEST SST files are now managed with the job manager. #3994 Support for error code display when a job fails. #4067 The OS page cache can be disabled and the block cache and Nebula Graph storage cache can only be used in a shared environment, to avoid memory usage interference between applications. #3890 Updated the default value of the KV separation threshold from 0 to 100. #3879 Support for using gflag to set the upper limit of expression depth for a better fit of different machine environments. #3722 Added a permission check for KILL QUERY . When the authorization is enabled, the GOD user can kill any query and the users with other roles can only kill queries that they own. #3896 Support for more complier launchers, including distcc and sccache. #3896 More dumping tables are supported with the meta dump tool. #3870 The storage layer controls the concurrency of write operations (INSERT VERTEX or EDGE) from reporting an error and requiring a client retry to using the internal queueing mechanism. #3926","title":"Enhancement"},{"location":"20.appendix/releasenote/#bugfix","text":"Fixed the crash when using a function call as part of a filter in a LOOKUP statement. #4111 Fixed the crash when there were non-indexed properties in an IN clause. #3986 Fixed the storage service crash when concurrently scanning vertices and edges. #4190 Fixed the crash when performing aggregation queries with patterns in a MATCH statement. #4180 Fixed the crash when getting the JSON results of a profile query. #3998 Fixed the crash when the async interface in the Lambda function finished running and the task in threadManager was not executed. #4000 Fixed the GROUP BY output bug. #4128 Fixed the bug that the version wasn't displayed with SHOW HOSTS sometimes. #4116 Fixed the bug on parameters for id(n) == $var , id(n) IN [$var] , id(n) == $var.foo.bar , and id(n) IN $var.foo.bar . #4024 Fixed the bug that an incorrect path direction occurred in MATCH...WHERE . #4091 Fixed the bug that the result of referencing multiple MATCH variables in a WHERE clause was incorrect. #4143 Fixed the optimizer bug. #4146 Fixed the bug that the storage service failed to handle Raft snapshots. #4019 Fixed the bug that the storage service would not accept more logs after receiving a snapshot. #3909 Fixed the bug that snapshots did not contain the vertices without tags. #4189 Fixed the latest schema version read failure when the schema version is greater than 255. #4023 Fixed the bug that SHOW STATS did not count the vertices that had no tags. #3967 Fixed the bug that the timestamp was fetched incorrectly sometimes. #3958 Fixed the bug that the root user could be granted with other roles in the graph space. #3868 Fixed the duplicate count of column indexes in the lexical parser bug. #3626","title":"Bugfix"},{"location":"20.appendix/releasenote/#legacy_versions","text":"Release notes of legacy versions","title":"Legacy versions"},{"location":"20.appendix/write-tools/","text":"Import tools \u00b6 There are many ways to write Nebula Graph 3.1.0: Import with the command -f : This method imports a small number of prepared nGQL files, which is suitable to prepare for a small amount of manual test data. Import with Studio : This method uses a browser to import multiple csv files of this machine. A single file cannot exceed 100 MB, and its format is limited. Import with Importer : This method imports multiple csv files on a single machine with unlimited size and flexible format. Import with Exchange : This method imports from various distribution sources, such as Neo4j, Hive, MySQL, etc., which requires a Spark cluster. Import with Spark-connector / Flink-connector : This method has corresponding components (Spark/Flink) and writes a small amount of code. Import with C++/GO/Java/Python SDK : This method imports in the way of writing programs, which requires certain programming and tuning skills. The following figure shows the positions of these ways:","title":"Write tools"},{"location":"20.appendix/write-tools/#import_tools","text":"There are many ways to write Nebula Graph 3.1.0: Import with the command -f : This method imports a small number of prepared nGQL files, which is suitable to prepare for a small amount of manual test data. Import with Studio : This method uses a browser to import multiple csv files of this machine. A single file cannot exceed 100 MB, and its format is limited. Import with Importer : This method imports multiple csv files on a single machine with unlimited size and flexible format. Import with Exchange : This method imports from various distribution sources, such as Neo4j, Hive, MySQL, etc., which requires a Spark cluster. Import with Spark-connector / Flink-connector : This method has corresponding components (Spark/Flink) and writes a small amount of code. Import with C++/GO/Java/Python SDK : This method imports in the way of writing programs, which requires certain programming and tuning skills. The following figure shows the positions of these ways:","title":"Import tools"},{"location":"3.ngql-guide/4.job-statements/","text":"Job manager and the JOB statements \u00b6 The long-term tasks run by the Storage Service are called jobs, such as COMPACT , FLUSH , and STATS . These jobs can be time-consuming if the data amount in the graph space is large. The job manager helps you run, show, stop, and recover jobs. Note All job management commands can be executed only after selecting a graph space. SUBMIT JOB BALANCE DATA \u00b6 Enterpriseonly Only available for the Nebula Graph Enterprise Edition. Caution Before performing the job, it is recommended to create a snapshot . During job execution, do not execute other jobs, such as SUBMIT JOB STATS , REBUILD INDEX , etc. During job execution, it is recommended not to write or read data in large batches. The SUBMIT JOB BALANCE DATA statement starts a job to balance the distribution of storage partitions in the current graph space. It returns the job ID. For example: nebula> SUBMIT JOB BALANCE DATA; +------------+ | New Job Id | +------------+ | 28 | +------------+ SUBMIT JOB COMPACT \u00b6 The SUBMIT JOB COMPACT statement triggers the long-term RocksDB compact operation in the current graph space. For more information about compact configuration, see Storage Service configuration . For example: nebula> SUBMIT JOB COMPACT; +------------+ | New Job Id | +------------+ | 40 | +------------+ SUBMIT JOB FLUSH \u00b6 The SUBMIT JOB FLUSH statement writes the RocksDB memfile in the memory to the hard disk in the current graph space. For example: nebula> SUBMIT JOB FLUSH; +------------+ | New Job Id | +------------+ | 96 | +------------+ SUBMIT JOB STATS \u00b6 The SUBMIT JOB STATS statement starts a job that makes the statistics of the current graph space. Once this job succeeds, you can use the SHOW STATS statement to list the statistics. For more information, see SHOW STATS . Note If the data stored in the graph space changes, in order to get the latest statistics, you have to run SUBMIT JOB STATS again. For example: nebula> SUBMIT JOB STATS; +------------+ | New Job Id | +------------+ | 9 | +------------+ SUBMIT JOB DOWNLOAD/INGEST \u00b6 The SUBMIT JOB DOWNLOAD HDFS and SUBMIT JOB INGEST commands are used to import the SST file into Nebula Graph. For detail, see Import data from SST files \u3002 The SUBMIT JOB DOWNLOAD HDFS command will download the SST file on the specified HDFS. The SUBMIT JOB INGEST command will import the downloaded SST file into Nebula Graph. For example: nebula> SUBMIT JOB DOWNLOAD HDFS \"hdfs://192.168.10.100:9000/sst\"; +------------+ | New Job Id | +------------+ | 10 | +------------+ nebula> SUBMIT JOB INGEST; +------------+ | New Job Id | +------------+ | 11 | +------------+ SHOW JOB \u00b6 The Meta Service parses a SUBMIT JOB request into multiple tasks and assigns them to the nebula-storaged processes. The SHOW JOB <job_id> statement shows the information about a specific job and all its tasks in the current graph space. job_id is returned when you run the SUBMIT JOB statement. For example: nebula> SHOW JOB 9; +----------------+-----------------+------------+----------------------------+----------------------------+-------------+ | Job Id(TaskId) | Command(Dest) | Status | Start Time | Stop Time | Error Code | +----------------+-----------------+------------+----------------------------+----------------------------+-------------+ | 9 | \"STATS\" | \"FINISHED\" | 2022-04-12T08:47:33.000000 | 2022-04-12T08:47:33.000000 | \"SUCCEEDED\" | | 0 | \"192.168.8.100\" | \"FINISHED\" | 2022-04-12T08:47:33.000000 | 2022-04-12T08:47:34.000000 | \"SUCCEEDED\" | | 1 | \"192.168.8.101\" | \"FINISHED\" | 2022-04-12T08:47:33.000000 | 2022-04-12T08:47:34.000000 | \"SUCCEEDED\" | +----------------+-----------------+------------+----------------------------+----------------------------+-------------+ The descriptions are as follows. Parameter Description Job Id(TaskId) The first row shows the job ID and the other rows show the task IDs. Command(Dest) The first row shows the command executed and the other rows show on which storaged processes the task is running. Status Shows the status of the job or task. For more information, see Job status . Start Time Shows a timestamp indicating the time when the job or task enters the RUNNING phase. Stop Time Shows a timestamp indicating the time when the job or task gets FINISHED , FAILED , or STOPPED . Error Code The error code of job. Job status \u00b6 The descriptions are as follows. Status Description QUEUE The job or task is waiting in a queue. The Start Time is empty in this phase. RUNNING The job or task is running. The Start Time shows the beginning time of this phase. FINISHED The job or task is successfully finished. The Stop Time shows the time when the job or task enters this phase. FAILED The job or task has failed. The Stop Time shows the time when the job or task enters this phase. STOPPED The job or task is stopped without running. The Stop Time shows the time when the job or task enters this phase. REMOVED The job or task is removed. The description of switching the status is described as follows. Queue -- running -- finished -- removed \\ \\ / \\ \\ -- failed -- / \\ \\ / \\ ---------- stopped -/ SHOW JOBS \u00b6 The SHOW JOBS statement lists all the unexpired jobs in the current graph space. The default job expiration interval is one week. You can change it by modifying the job_expired_secs parameter of the Meta Service. For how to modify job_expired_secs , see Meta Service configuration . For example: nebula> SHOW JOBS; +--------+---------------------+------------+----------------------------+----------------------------+ | Job Id | Command | Status | Start Time | Stop Time | +--------+---------------------+------------+----------------------------+----------------------------+ | 34 | \"STATS\" | \"FINISHED\" | 2021-11-01T03:32:27.000000 | 2021-11-01T03:32:27.000000 | | 33 | \"FLUSH\" | \"FINISHED\" | 2021-11-01T03:32:15.000000 | 2021-11-01T03:32:15.000000 | | 32 | \"COMPACT\" | \"FINISHED\" | 2021-11-01T03:32:06.000000 | 2021-11-01T03:32:06.000000 | | 31 | \"REBUILD_TAG_INDEX\" | \"FINISHED\" | 2021-10-29T05:39:16.000000 | 2021-10-29T05:39:17.000000 | | 10 | \"COMPACT\" | \"FINISHED\" | 2021-10-26T02:27:05.000000 | 2021-10-26T02:27:05.000000 | +--------+---------------------+------------+----------------------------+----------------------------+ STOP JOB \u00b6 The STOP JOB <job_id> statement stops jobs that are not finished in the current graph space. For example: nebula> STOP JOB 22; +---------------+ | Result | +---------------+ | \"Job stopped\" | +---------------+ RECOVER JOB \u00b6 The RECOVER JOB [<job_id>] statement re-executes the jobs that status is QUEUE , FAILED or STOPPED in the current graph space and returns the number of recovered jobs. If <job_id> is not specified, re-execution is performed from the earliest job and the number of jobs that have been recovered is returned. For example: nebula> RECOVER JOB; +-------------------+ | Recovered job num | +-------------------+ | 5 job recovered | +-------------------+ FAQ \u00b6 How to troubleshoot job problems? \u00b6 The SUBMIT JOB operations use the HTTP port. Please check if the HTTP ports on the machines where the Storage Service is running are working well. You can use the following command to debug. curl \"http://{storaged-ip}:19779/admin?space={space_name}&op=compact\"","title":"Job statements"},{"location":"3.ngql-guide/4.job-statements/#job_manager_and_the_job_statements","text":"The long-term tasks run by the Storage Service are called jobs, such as COMPACT , FLUSH , and STATS . These jobs can be time-consuming if the data amount in the graph space is large. The job manager helps you run, show, stop, and recover jobs. Note All job management commands can be executed only after selecting a graph space.","title":"Job manager and the JOB statements"},{"location":"3.ngql-guide/4.job-statements/#submit_job_balance_data","text":"Enterpriseonly Only available for the Nebula Graph Enterprise Edition. Caution Before performing the job, it is recommended to create a snapshot . During job execution, do not execute other jobs, such as SUBMIT JOB STATS , REBUILD INDEX , etc. During job execution, it is recommended not to write or read data in large batches. The SUBMIT JOB BALANCE DATA statement starts a job to balance the distribution of storage partitions in the current graph space. It returns the job ID. For example: nebula> SUBMIT JOB BALANCE DATA; +------------+ | New Job Id | +------------+ | 28 | +------------+","title":"SUBMIT JOB BALANCE DATA"},{"location":"3.ngql-guide/4.job-statements/#submit_job_compact","text":"The SUBMIT JOB COMPACT statement triggers the long-term RocksDB compact operation in the current graph space. For more information about compact configuration, see Storage Service configuration . For example: nebula> SUBMIT JOB COMPACT; +------------+ | New Job Id | +------------+ | 40 | +------------+","title":"SUBMIT JOB COMPACT"},{"location":"3.ngql-guide/4.job-statements/#submit_job_flush","text":"The SUBMIT JOB FLUSH statement writes the RocksDB memfile in the memory to the hard disk in the current graph space. For example: nebula> SUBMIT JOB FLUSH; +------------+ | New Job Id | +------------+ | 96 | +------------+","title":"SUBMIT JOB FLUSH"},{"location":"3.ngql-guide/4.job-statements/#submit_job_stats","text":"The SUBMIT JOB STATS statement starts a job that makes the statistics of the current graph space. Once this job succeeds, you can use the SHOW STATS statement to list the statistics. For more information, see SHOW STATS . Note If the data stored in the graph space changes, in order to get the latest statistics, you have to run SUBMIT JOB STATS again. For example: nebula> SUBMIT JOB STATS; +------------+ | New Job Id | +------------+ | 9 | +------------+","title":"SUBMIT JOB STATS"},{"location":"3.ngql-guide/4.job-statements/#submit_job_downloadingest","text":"The SUBMIT JOB DOWNLOAD HDFS and SUBMIT JOB INGEST commands are used to import the SST file into Nebula Graph. For detail, see Import data from SST files \u3002 The SUBMIT JOB DOWNLOAD HDFS command will download the SST file on the specified HDFS. The SUBMIT JOB INGEST command will import the downloaded SST file into Nebula Graph. For example: nebula> SUBMIT JOB DOWNLOAD HDFS \"hdfs://192.168.10.100:9000/sst\"; +------------+ | New Job Id | +------------+ | 10 | +------------+ nebula> SUBMIT JOB INGEST; +------------+ | New Job Id | +------------+ | 11 | +------------+","title":"SUBMIT JOB DOWNLOAD/INGEST"},{"location":"3.ngql-guide/4.job-statements/#show_job","text":"The Meta Service parses a SUBMIT JOB request into multiple tasks and assigns them to the nebula-storaged processes. The SHOW JOB <job_id> statement shows the information about a specific job and all its tasks in the current graph space. job_id is returned when you run the SUBMIT JOB statement. For example: nebula> SHOW JOB 9; +----------------+-----------------+------------+----------------------------+----------------------------+-------------+ | Job Id(TaskId) | Command(Dest) | Status | Start Time | Stop Time | Error Code | +----------------+-----------------+------------+----------------------------+----------------------------+-------------+ | 9 | \"STATS\" | \"FINISHED\" | 2022-04-12T08:47:33.000000 | 2022-04-12T08:47:33.000000 | \"SUCCEEDED\" | | 0 | \"192.168.8.100\" | \"FINISHED\" | 2022-04-12T08:47:33.000000 | 2022-04-12T08:47:34.000000 | \"SUCCEEDED\" | | 1 | \"192.168.8.101\" | \"FINISHED\" | 2022-04-12T08:47:33.000000 | 2022-04-12T08:47:34.000000 | \"SUCCEEDED\" | +----------------+-----------------+------------+----------------------------+----------------------------+-------------+ The descriptions are as follows. Parameter Description Job Id(TaskId) The first row shows the job ID and the other rows show the task IDs. Command(Dest) The first row shows the command executed and the other rows show on which storaged processes the task is running. Status Shows the status of the job or task. For more information, see Job status . Start Time Shows a timestamp indicating the time when the job or task enters the RUNNING phase. Stop Time Shows a timestamp indicating the time when the job or task gets FINISHED , FAILED , or STOPPED . Error Code The error code of job.","title":"SHOW JOB"},{"location":"3.ngql-guide/4.job-statements/#job_status","text":"The descriptions are as follows. Status Description QUEUE The job or task is waiting in a queue. The Start Time is empty in this phase. RUNNING The job or task is running. The Start Time shows the beginning time of this phase. FINISHED The job or task is successfully finished. The Stop Time shows the time when the job or task enters this phase. FAILED The job or task has failed. The Stop Time shows the time when the job or task enters this phase. STOPPED The job or task is stopped without running. The Stop Time shows the time when the job or task enters this phase. REMOVED The job or task is removed. The description of switching the status is described as follows. Queue -- running -- finished -- removed \\ \\ / \\ \\ -- failed -- / \\ \\ / \\ ---------- stopped -/","title":"Job status"},{"location":"3.ngql-guide/4.job-statements/#show_jobs","text":"The SHOW JOBS statement lists all the unexpired jobs in the current graph space. The default job expiration interval is one week. You can change it by modifying the job_expired_secs parameter of the Meta Service. For how to modify job_expired_secs , see Meta Service configuration . For example: nebula> SHOW JOBS; +--------+---------------------+------------+----------------------------+----------------------------+ | Job Id | Command | Status | Start Time | Stop Time | +--------+---------------------+------------+----------------------------+----------------------------+ | 34 | \"STATS\" | \"FINISHED\" | 2021-11-01T03:32:27.000000 | 2021-11-01T03:32:27.000000 | | 33 | \"FLUSH\" | \"FINISHED\" | 2021-11-01T03:32:15.000000 | 2021-11-01T03:32:15.000000 | | 32 | \"COMPACT\" | \"FINISHED\" | 2021-11-01T03:32:06.000000 | 2021-11-01T03:32:06.000000 | | 31 | \"REBUILD_TAG_INDEX\" | \"FINISHED\" | 2021-10-29T05:39:16.000000 | 2021-10-29T05:39:17.000000 | | 10 | \"COMPACT\" | \"FINISHED\" | 2021-10-26T02:27:05.000000 | 2021-10-26T02:27:05.000000 | +--------+---------------------+------------+----------------------------+----------------------------+","title":"SHOW JOBS"},{"location":"3.ngql-guide/4.job-statements/#stop_job","text":"The STOP JOB <job_id> statement stops jobs that are not finished in the current graph space. For example: nebula> STOP JOB 22; +---------------+ | Result | +---------------+ | \"Job stopped\" | +---------------+","title":"STOP JOB"},{"location":"3.ngql-guide/4.job-statements/#recover_job","text":"The RECOVER JOB [<job_id>] statement re-executes the jobs that status is QUEUE , FAILED or STOPPED in the current graph space and returns the number of recovered jobs. If <job_id> is not specified, re-execution is performed from the earliest job and the number of jobs that have been recovered is returned. For example: nebula> RECOVER JOB; +-------------------+ | Recovered job num | +-------------------+ | 5 job recovered | +-------------------+","title":"RECOVER JOB"},{"location":"3.ngql-guide/4.job-statements/#faq","text":"","title":"FAQ"},{"location":"3.ngql-guide/4.job-statements/#how_to_troubleshoot_job_problems","text":"The SUBMIT JOB operations use the HTTP port. Please check if the HTTP ports on the machines where the Storage Service is running are working well. You can use the following command to debug. curl \"http://{storaged-ip}:19779/admin?space={space_name}&op=compact\"","title":"How to troubleshoot job problems?"},{"location":"3.ngql-guide/1.nGQL-overview/1.overview/","text":"Nebula Graph Query Language (nGQL) \u00b6 This topic gives an introduction to the query language of Nebula Graph, nGQL. What is nGQL \u00b6 nGQL is a declarative graph query language for Nebula Graph. It allows expressive and efficient graph patterns . nGQL is designed for both developers and operations professionals. nGQL is an SQL-like query language, so it's easy to learn. nGQL is a project in progress. New features and optimizations are done steadily. There can be differences between syntax and implementation. Submit an issue to inform the Nebula Graph team if you find a new issue of this type. Nebula Graph 3.0 or later releases will support openCypher 9 . What can nGQL do \u00b6 Supports graph traversals Supports pattern match Supports aggregation Supports graph mutation Supports access control Supports composite queries Supports index Supports most openCypher 9 graph query syntax (but mutations and controls syntax are not supported) Example data Basketballplayer \u00b6 Users can download the example data Basketballplayer in Nebula Graph. After downloading the example data, you can import it to Nebula Graph by using the -f option in Nebula Graph Console . Note Ensure that you have executed the ADD HOSTS command to add the Storage service to your Nebula Graph cluster before importing the example data. For more information, see Manage Storage hosts . Placeholder identifiers and values \u00b6 Refer to the following standards in nGQL: (Draft) ISO/IEC JTC1 N14279 SC 32 - Database_Languages - GQL (Draft) ISO/IEC JTC1 SC32 N3228 - SQL_Property_Graph_Queries - SQLPGQ OpenCypher 9 In template code, any token that is not a keyword, a literal value, or punctuation is a placeholder identifier or a placeholder value. For details of the symbols in nGQL syntax, see the following table: Token Meaning < > name of a syntactic element : formula that defines an element [ ] optional elements { } explicitly specified elements | complete alternative elements ... may be repeated any number of times For example, create vertices in nGQL syntax: INSERT VERTEX [IF NOT EXISTS] [tag_props, [tag_props] ...] VALUES <vid>: ([prop_value_list]) tag_props: tag_name ([prop_name_list]) prop_name_list: [prop_name [, prop_name] ...] prop_value_list: [prop_value [, prop_value] ...] Example statement: nebula> CREATE TAG IF NOT EXISTS player(name string, age int); About openCypher compatibility \u00b6 Native nGQL and openCypher \u00b6 Native nGQL is the part of a graph query language designed and implemented by Nebula Graph. OpenCypher is a graph query language maintained by openCypher Implementers Group. The latest release is openCypher 9. The compatible parts of openCypher in nGQL are called openCypher compatible sentences (short as openCypher). Note nGQL = native nGQL + openCypher compatible sentences Undefined behavior Do not put together native nGQL and openCypher compatible sentences in one composite statement because this behavior is undefined. Is nGQL compatible with openCypher 9 completely? \u00b6 NO. nGQL is partially compatible with DQL in openCypher 9 nGQL is designed to be compatible with part of DQL (match) and is not planned to be compatible with any DDL, DML, or DCL. Multiple known incompatible items are listed in Nebula Graph Issues . Submit an issue with the incompatible tag if you find a new issue of this type. Users can search in this manual with the keyword compatibility to find major compatibility issues. What are the major differences between nGQL and openCypher 9? \u00b6 The following are some major differences (by design incompatible) between nGQL and openCypher. Category openCypher 9 nGQL Schema Optional Schema Strong Schema Equality operator = == Math exponentiation ^ ^ is not supported. Use pow(x, y) instead. Edge rank No such concept. edge rank (reference by @) Statement - All DMLs ( CREATE , MERGE , etc) of openCypher 9. Label and tag A label is used for searching a vertex, namely an index of vertex. A tag defines the type of a vertex and its corresponding properties. It cannot be used as an index. Pre-compiling and parameterized queries Support Parameterized queries are supported, but precompiling is not. Compatibility OpenCypher 9 and Cypher have some differences in grammar and licence. For example, Cypher requires that All Cypher statements are explicitly run within a transaction . While openCypher has no such requirement. And nGQL does not support transactions. Cypher has a variety of constraints, including Unique node property constraints, Node property existence constraints, Relationship property existence constraints, and Node key constraints. While OpenCypher has no such constraints. As a strong schema system, most of the constraints mentioned above can be solved through schema definitions (including NOT NULL) in nGQL. The only function that cannot be supported is the UNIQUE constraint. Cypher has APoC, while openCypher 9 does not have APoC. Cypher has Blot protocol support requirements, while openCypher 9 does not. Where can I find more nGQL examples? \u00b6 Users can find more than 2500 nGQL examples in the features directory on the Nebula Graph GitHub page. The features directory consists of .feature files. Each file records scenarios that you can use as nGQL examples. Here is an example: Feature: Basic match Background: Given a graph with space named \"basketballplayer\" Scenario: Single node When executing query: \"\"\" MATCH (v:player {name: \"Yao Ming\"}) RETURN v; \"\"\" Then the result should be, in any order, with relax comparison: | v | | (\"player133\" :player{age: 38, name: \"Yao Ming\"}) | Scenario: One step When executing query: \"\"\" MATCH (v1:player{name: \"LeBron James\"}) -[r]-> (v2) RETURN type(r) AS Type, v2.player.name AS Name \"\"\" Then the result should be, in any order: | Type | Name | | \"follow\" | \"Ray Allen\" | | \"serve\" | \"Lakers\" | | \"serve\" | \"Heat\" | | \"serve\" | \"Cavaliers\" | Feature: Comparison of where clause Background: Given a graph with space named \"basketballplayer\" Scenario: push edge props filter down When profiling query: \"\"\" GO FROM \"player100\" OVER follow WHERE properties(edge).degree IN [v IN [95,99] WHERE v > 0] YIELD dst(edge), properties(edge).degree \"\"\" Then the result should be, in any order: | follow._dst | follow.degree | | \"player101\" | 95 | | \"player125\" | 95 | And the execution plan should be: | id | name | dependencies | operator info | | 0 | Project | 1 | | | 1 | GetNeighbors | 2 | {\"filter\": \"(properties(edge).degree IN [v IN [95,99] WHERE (v>0)])\"} | | 2 | Start | | | The keywords in the preceding example are described as follows. Keyword Description Feature Describes the topic of the current .feature file. Background Describes the background information of the current .feature file. Given Describes the prerequisites of running the test statements in the current .feature file. Scenario Describes the scenarios. If there is the @skip before one Scenario , this scenario may not work and do not use it as a working example in a production environment. When Describes the nGQL statement to be executed. It can be a executing query or profiling query . Then Describes the expected return results of running the statement in the When clause. If the return results in your environment do not match the results described in the .feature file, submit an issue to inform the Nebula Graph team. And Describes the side effects of running the statement in the When clause. @skip This test case will be skipped. Commonly, the to-be-tested code is not ready. Welcome to add more tck case and return automatically to the using statements in CI/CD. Does it support TinkerPop Gremlin? \u00b6 No. And no plan to support that. Does Nebula Graph support W3C RDF (SPARQL) or GraphQL? \u00b6 No. And no plan to support that. The data model of Nebula Graph is the property graph. And as a strong schema system, Nebula Graph does not support RDF. Nebula Graph Query Language does not support SPARQL nor GraphQL .","title":"Overview"},{"location":"3.ngql-guide/1.nGQL-overview/1.overview/#nebula_graph_query_language_ngql","text":"This topic gives an introduction to the query language of Nebula Graph, nGQL.","title":"Nebula Graph Query Language (nGQL)"},{"location":"3.ngql-guide/1.nGQL-overview/1.overview/#what_is_ngql","text":"nGQL is a declarative graph query language for Nebula Graph. It allows expressive and efficient graph patterns . nGQL is designed for both developers and operations professionals. nGQL is an SQL-like query language, so it's easy to learn. nGQL is a project in progress. New features and optimizations are done steadily. There can be differences between syntax and implementation. Submit an issue to inform the Nebula Graph team if you find a new issue of this type. Nebula Graph 3.0 or later releases will support openCypher 9 .","title":"What is nGQL"},{"location":"3.ngql-guide/1.nGQL-overview/1.overview/#what_can_ngql_do","text":"Supports graph traversals Supports pattern match Supports aggregation Supports graph mutation Supports access control Supports composite queries Supports index Supports most openCypher 9 graph query syntax (but mutations and controls syntax are not supported)","title":"What can nGQL do"},{"location":"3.ngql-guide/1.nGQL-overview/1.overview/#example_data_basketballplayer","text":"Users can download the example data Basketballplayer in Nebula Graph. After downloading the example data, you can import it to Nebula Graph by using the -f option in Nebula Graph Console . Note Ensure that you have executed the ADD HOSTS command to add the Storage service to your Nebula Graph cluster before importing the example data. For more information, see Manage Storage hosts .","title":"Example data Basketballplayer"},{"location":"3.ngql-guide/1.nGQL-overview/1.overview/#placeholder_identifiers_and_values","text":"Refer to the following standards in nGQL: (Draft) ISO/IEC JTC1 N14279 SC 32 - Database_Languages - GQL (Draft) ISO/IEC JTC1 SC32 N3228 - SQL_Property_Graph_Queries - SQLPGQ OpenCypher 9 In template code, any token that is not a keyword, a literal value, or punctuation is a placeholder identifier or a placeholder value. For details of the symbols in nGQL syntax, see the following table: Token Meaning < > name of a syntactic element : formula that defines an element [ ] optional elements { } explicitly specified elements | complete alternative elements ... may be repeated any number of times For example, create vertices in nGQL syntax: INSERT VERTEX [IF NOT EXISTS] [tag_props, [tag_props] ...] VALUES <vid>: ([prop_value_list]) tag_props: tag_name ([prop_name_list]) prop_name_list: [prop_name [, prop_name] ...] prop_value_list: [prop_value [, prop_value] ...] Example statement: nebula> CREATE TAG IF NOT EXISTS player(name string, age int);","title":"Placeholder identifiers and values"},{"location":"3.ngql-guide/1.nGQL-overview/1.overview/#about_opencypher_compatibility","text":"","title":"About openCypher compatibility"},{"location":"3.ngql-guide/1.nGQL-overview/1.overview/#native_ngql_and_opencypher","text":"Native nGQL is the part of a graph query language designed and implemented by Nebula Graph. OpenCypher is a graph query language maintained by openCypher Implementers Group. The latest release is openCypher 9. The compatible parts of openCypher in nGQL are called openCypher compatible sentences (short as openCypher). Note nGQL = native nGQL + openCypher compatible sentences Undefined behavior Do not put together native nGQL and openCypher compatible sentences in one composite statement because this behavior is undefined.","title":"Native nGQL and openCypher"},{"location":"3.ngql-guide/1.nGQL-overview/1.overview/#is_ngql_compatible_with_opencypher_9_completely","text":"NO. nGQL is partially compatible with DQL in openCypher 9 nGQL is designed to be compatible with part of DQL (match) and is not planned to be compatible with any DDL, DML, or DCL. Multiple known incompatible items are listed in Nebula Graph Issues . Submit an issue with the incompatible tag if you find a new issue of this type. Users can search in this manual with the keyword compatibility to find major compatibility issues.","title":"Is nGQL compatible with openCypher 9 completely?"},{"location":"3.ngql-guide/1.nGQL-overview/1.overview/#what_are_the_major_differences_between_ngql_and_opencypher_9","text":"The following are some major differences (by design incompatible) between nGQL and openCypher. Category openCypher 9 nGQL Schema Optional Schema Strong Schema Equality operator = == Math exponentiation ^ ^ is not supported. Use pow(x, y) instead. Edge rank No such concept. edge rank (reference by @) Statement - All DMLs ( CREATE , MERGE , etc) of openCypher 9. Label and tag A label is used for searching a vertex, namely an index of vertex. A tag defines the type of a vertex and its corresponding properties. It cannot be used as an index. Pre-compiling and parameterized queries Support Parameterized queries are supported, but precompiling is not. Compatibility OpenCypher 9 and Cypher have some differences in grammar and licence. For example, Cypher requires that All Cypher statements are explicitly run within a transaction . While openCypher has no such requirement. And nGQL does not support transactions. Cypher has a variety of constraints, including Unique node property constraints, Node property existence constraints, Relationship property existence constraints, and Node key constraints. While OpenCypher has no such constraints. As a strong schema system, most of the constraints mentioned above can be solved through schema definitions (including NOT NULL) in nGQL. The only function that cannot be supported is the UNIQUE constraint. Cypher has APoC, while openCypher 9 does not have APoC. Cypher has Blot protocol support requirements, while openCypher 9 does not.","title":"What are the major differences between nGQL and openCypher 9?"},{"location":"3.ngql-guide/1.nGQL-overview/1.overview/#where_can_i_find_more_ngql_examples","text":"Users can find more than 2500 nGQL examples in the features directory on the Nebula Graph GitHub page. The features directory consists of .feature files. Each file records scenarios that you can use as nGQL examples. Here is an example: Feature: Basic match Background: Given a graph with space named \"basketballplayer\" Scenario: Single node When executing query: \"\"\" MATCH (v:player {name: \"Yao Ming\"}) RETURN v; \"\"\" Then the result should be, in any order, with relax comparison: | v | | (\"player133\" :player{age: 38, name: \"Yao Ming\"}) | Scenario: One step When executing query: \"\"\" MATCH (v1:player{name: \"LeBron James\"}) -[r]-> (v2) RETURN type(r) AS Type, v2.player.name AS Name \"\"\" Then the result should be, in any order: | Type | Name | | \"follow\" | \"Ray Allen\" | | \"serve\" | \"Lakers\" | | \"serve\" | \"Heat\" | | \"serve\" | \"Cavaliers\" | Feature: Comparison of where clause Background: Given a graph with space named \"basketballplayer\" Scenario: push edge props filter down When profiling query: \"\"\" GO FROM \"player100\" OVER follow WHERE properties(edge).degree IN [v IN [95,99] WHERE v > 0] YIELD dst(edge), properties(edge).degree \"\"\" Then the result should be, in any order: | follow._dst | follow.degree | | \"player101\" | 95 | | \"player125\" | 95 | And the execution plan should be: | id | name | dependencies | operator info | | 0 | Project | 1 | | | 1 | GetNeighbors | 2 | {\"filter\": \"(properties(edge).degree IN [v IN [95,99] WHERE (v>0)])\"} | | 2 | Start | | | The keywords in the preceding example are described as follows. Keyword Description Feature Describes the topic of the current .feature file. Background Describes the background information of the current .feature file. Given Describes the prerequisites of running the test statements in the current .feature file. Scenario Describes the scenarios. If there is the @skip before one Scenario , this scenario may not work and do not use it as a working example in a production environment. When Describes the nGQL statement to be executed. It can be a executing query or profiling query . Then Describes the expected return results of running the statement in the When clause. If the return results in your environment do not match the results described in the .feature file, submit an issue to inform the Nebula Graph team. And Describes the side effects of running the statement in the When clause. @skip This test case will be skipped. Commonly, the to-be-tested code is not ready. Welcome to add more tck case and return automatically to the using statements in CI/CD.","title":"Where can I find more nGQL examples?"},{"location":"3.ngql-guide/1.nGQL-overview/1.overview/#does_it_support_tinkerpop_gremlin","text":"No. And no plan to support that.","title":"Does it support TinkerPop Gremlin?"},{"location":"3.ngql-guide/1.nGQL-overview/1.overview/#does_nebula_graph_support_w3c_rdf_sparql_or_graphql","text":"No. And no plan to support that. The data model of Nebula Graph is the property graph. And as a strong schema system, Nebula Graph does not support RDF. Nebula Graph Query Language does not support SPARQL nor GraphQL .","title":"Does Nebula Graph support W3C RDF (SPARQL) or GraphQL?"},{"location":"3.ngql-guide/1.nGQL-overview/3.graph-patterns/","text":"Patterns \u00b6 Patterns and graph pattern matching are the very heart of a graph query language. This topic will describe the patterns in Nebula Graph, some of which have not yet been implemented. Patterns for vertices \u00b6 A vertex is described using a pair of parentheses and is typically given a name. For example: (a) This simple pattern describes a single vertex and names that vertex using the variable a . Patterns for related vertices \u00b6 A more powerful construct is a pattern that describes multiple vertices and edges between them. Patterns describe an edge by employing an arrow between two vertices. For example: (a)-[]->(b) This pattern describes a very simple data structure: two vertices and a single edge from one to the other. In this example, the two vertices are named as a and b respectively and the edge is directed : it goes from a to b . This manner of describing vertices and edges can be extended to cover an arbitrary number of vertices and the edges between them, for example: (a)-[]->(b)<-[]-(c) Such a series of connected vertices and edges is called a path . Note that the naming of the vertices in these patterns is only necessary when one needs to refer to the same vertex again, either later in the pattern or elsewhere in the query. If not, the name may be omitted as follows: (a)-[]->()<-[]-(c) Patterns for tags \u00b6 Note The concept of tag in nGQL has a few differences from that of label in openCypher. For example, users must create a tag before using it. And a tag also defines the type of properties. In addition to simply describing the vertices in the graphs, patterns can also describe the tags of the vertices. For example: (a:User)-[]->(b) Patterns can also describe a vertex that has multiple tags. For example: (a:User:Admin)-[]->(b) Patterns for properties \u00b6 Vertices and edges are the fundamental elements in a graph. In nGQL, properties are added to them for richer models. In the patterns, the properties can be expressed as follows: some key-value pairs are enclosed in curly brackets and separated by commas. For example, a vertex with two properties will be like: (a {name: 'Andres', sport: 'Brazilian Ju-Jitsu'}) One of the edges that connect to this vertex can be like: (a)-[{blocked: false}]->(b) Patterns for edges \u00b6 The simplest way to describe an edge is by using the arrow between two vertices, as in the previous examples. Users can describe an edge and its direction using the following statement. If users do not care about its direction, the arrowhead can be omitted. For example: (a)-[]-(b) Like vertices, edges can also be named. A pair of square brackets will be used to separate the arrow and the variable will be placed between them. For example: (a)-[r]->(b) Like the tags on vertices, edges can also have types. To describe an edge with a specific type, use the pattern as follows: (a)-[r:REL_TYPE]->(b) An edge can only have one edge type. But if we'd like to describe some data such that the edge could have a set of types, then they can all be listed in the pattern, separating them with the pipe symbol | like this: (a)-[r:TYPE1|TYPE2]->(b) Like vertices, the name of an edge can be omitted. For example: (a)-[:REL_TYPE]->(b) Variable-length pattern \u00b6 Rather than describing a long path using a sequence of many vertex and edge descriptions in a pattern, many edges (and the intermediate vertices) can be described by specifying a length in the edge description of a pattern. For example: (a)-[*2]->(b) The following pattern describes a graph of three vertices and two edges, all in one path (a path of length 2). It is equivalent to: (a)-[]->()-[]->(b) The range of lengths can also be specified. Such edge patterns are called variable-length edges . For example: (a)-[*3..5]->(b) The preceding example defines a path with a minimum length of 3 and a maximum length of 5. It describes a graph of either 4 vertices and 3 edges, 5 vertices and 4 edges, or 6 vertices and 5 edges, all connected in a single path. The lower bound can be omitted. For example, to describe paths of length 5 or less, use: (a)-[*..5]->(b) Note The upper bound must be specified. The following are NOT accepted. (a)-[*3..]->(b) (a)-[*]->(b) Assigning to path variables \u00b6 As described above, a series of connected vertices and edges is called a path . nGQL allows paths to be named using variables. For example: p = (a)-[*3..5]->(b) Users can do this in the MATCH statement.","title":"Graph patterns"},{"location":"3.ngql-guide/1.nGQL-overview/3.graph-patterns/#patterns","text":"Patterns and graph pattern matching are the very heart of a graph query language. This topic will describe the patterns in Nebula Graph, some of which have not yet been implemented.","title":"Patterns"},{"location":"3.ngql-guide/1.nGQL-overview/3.graph-patterns/#patterns_for_vertices","text":"A vertex is described using a pair of parentheses and is typically given a name. For example: (a) This simple pattern describes a single vertex and names that vertex using the variable a .","title":"Patterns for vertices"},{"location":"3.ngql-guide/1.nGQL-overview/3.graph-patterns/#patterns_for_related_vertices","text":"A more powerful construct is a pattern that describes multiple vertices and edges between them. Patterns describe an edge by employing an arrow between two vertices. For example: (a)-[]->(b) This pattern describes a very simple data structure: two vertices and a single edge from one to the other. In this example, the two vertices are named as a and b respectively and the edge is directed : it goes from a to b . This manner of describing vertices and edges can be extended to cover an arbitrary number of vertices and the edges between them, for example: (a)-[]->(b)<-[]-(c) Such a series of connected vertices and edges is called a path . Note that the naming of the vertices in these patterns is only necessary when one needs to refer to the same vertex again, either later in the pattern or elsewhere in the query. If not, the name may be omitted as follows: (a)-[]->()<-[]-(c)","title":"Patterns for related vertices"},{"location":"3.ngql-guide/1.nGQL-overview/3.graph-patterns/#patterns_for_tags","text":"Note The concept of tag in nGQL has a few differences from that of label in openCypher. For example, users must create a tag before using it. And a tag also defines the type of properties. In addition to simply describing the vertices in the graphs, patterns can also describe the tags of the vertices. For example: (a:User)-[]->(b) Patterns can also describe a vertex that has multiple tags. For example: (a:User:Admin)-[]->(b)","title":"Patterns for tags"},{"location":"3.ngql-guide/1.nGQL-overview/3.graph-patterns/#patterns_for_properties","text":"Vertices and edges are the fundamental elements in a graph. In nGQL, properties are added to them for richer models. In the patterns, the properties can be expressed as follows: some key-value pairs are enclosed in curly brackets and separated by commas. For example, a vertex with two properties will be like: (a {name: 'Andres', sport: 'Brazilian Ju-Jitsu'}) One of the edges that connect to this vertex can be like: (a)-[{blocked: false}]->(b)","title":"Patterns for properties"},{"location":"3.ngql-guide/1.nGQL-overview/3.graph-patterns/#patterns_for_edges","text":"The simplest way to describe an edge is by using the arrow between two vertices, as in the previous examples. Users can describe an edge and its direction using the following statement. If users do not care about its direction, the arrowhead can be omitted. For example: (a)-[]-(b) Like vertices, edges can also be named. A pair of square brackets will be used to separate the arrow and the variable will be placed between them. For example: (a)-[r]->(b) Like the tags on vertices, edges can also have types. To describe an edge with a specific type, use the pattern as follows: (a)-[r:REL_TYPE]->(b) An edge can only have one edge type. But if we'd like to describe some data such that the edge could have a set of types, then they can all be listed in the pattern, separating them with the pipe symbol | like this: (a)-[r:TYPE1|TYPE2]->(b) Like vertices, the name of an edge can be omitted. For example: (a)-[:REL_TYPE]->(b)","title":"Patterns for edges"},{"location":"3.ngql-guide/1.nGQL-overview/3.graph-patterns/#variable-length_pattern","text":"Rather than describing a long path using a sequence of many vertex and edge descriptions in a pattern, many edges (and the intermediate vertices) can be described by specifying a length in the edge description of a pattern. For example: (a)-[*2]->(b) The following pattern describes a graph of three vertices and two edges, all in one path (a path of length 2). It is equivalent to: (a)-[]->()-[]->(b) The range of lengths can also be specified. Such edge patterns are called variable-length edges . For example: (a)-[*3..5]->(b) The preceding example defines a path with a minimum length of 3 and a maximum length of 5. It describes a graph of either 4 vertices and 3 edges, 5 vertices and 4 edges, or 6 vertices and 5 edges, all connected in a single path. The lower bound can be omitted. For example, to describe paths of length 5 or less, use: (a)-[*..5]->(b) Note The upper bound must be specified. The following are NOT accepted. (a)-[*3..]->(b) (a)-[*]->(b)","title":"Variable-length pattern"},{"location":"3.ngql-guide/1.nGQL-overview/3.graph-patterns/#assigning_to_path_variables","text":"As described above, a series of connected vertices and edges is called a path . nGQL allows paths to be named using variables. For example: p = (a)-[*3..5]->(b) Users can do this in the MATCH statement.","title":"Assigning to path variables"},{"location":"3.ngql-guide/1.nGQL-overview/comments/","text":"Comments \u00b6 This topic will describe the comments in nGQL. Legacy version compatibility \u00b6 In Nebula Graph 1.x, there are four comment styles: # , -- , // , /* */ . Since Nebula Graph 2.x, -- cannot be used as comments. Examples \u00b6 nebula> # Do nothing in this line nebula> RETURN 1+1; # This comment continues to the end of this line. nebula> RETURN 1+1; // This comment continues to the end of this line. nebula> RETURN 1 /* This is an in-line comment. */ + 1 == 2; nebula> RETURN 11 + \\ /* Multi-line comment. \\ Use a backslash as a line break. \\ */ 12; In nGQL statement, the backslash \\ in a line indicates a line break. OpenCypher compatibility \u00b6 In nGQL, you must add a \\ at the end of every line, even in multi-line comments /* */ . In openCypher, there is no need to use a \\ as a line break. /* openCypher style: The following comment spans more than one line */ MATCH (n:label) RETURN n; /* nGQL style: \\ The following comment \\ spans more than \\ one line */ \\ MATCH (n:tag) \\ RETURN n;","title":"Comments"},{"location":"3.ngql-guide/1.nGQL-overview/comments/#comments","text":"This topic will describe the comments in nGQL.","title":"Comments"},{"location":"3.ngql-guide/1.nGQL-overview/comments/#legacy_version_compatibility","text":"In Nebula Graph 1.x, there are four comment styles: # , -- , // , /* */ . Since Nebula Graph 2.x, -- cannot be used as comments.","title":"Legacy version compatibility"},{"location":"3.ngql-guide/1.nGQL-overview/comments/#examples","text":"nebula> # Do nothing in this line nebula> RETURN 1+1; # This comment continues to the end of this line. nebula> RETURN 1+1; // This comment continues to the end of this line. nebula> RETURN 1 /* This is an in-line comment. */ + 1 == 2; nebula> RETURN 11 + \\ /* Multi-line comment. \\ Use a backslash as a line break. \\ */ 12; In nGQL statement, the backslash \\ in a line indicates a line break.","title":"Examples"},{"location":"3.ngql-guide/1.nGQL-overview/comments/#opencypher_compatibility","text":"In nGQL, you must add a \\ at the end of every line, even in multi-line comments /* */ . In openCypher, there is no need to use a \\ as a line break. /* openCypher style: The following comment spans more than one line */ MATCH (n:label) RETURN n; /* nGQL style: \\ The following comment \\ spans more than \\ one line */ \\ MATCH (n:tag) \\ RETURN n;","title":"OpenCypher compatibility"},{"location":"3.ngql-guide/1.nGQL-overview/identifier-case-sensitivity/","text":"Identifier case sensitivity \u00b6 Identifiers are Case-Sensitive \u00b6 The following statements will not work because they refer to two different spaces, i.e. my_space and MY_SPACE . nebula> CREATE SPACE IF NOT EXISTS my_space (vid_type=FIXED_STRING(30)); nebula> use MY_SPACE; [ERROR (-8)]: SpaceNotFound: Keywords and Reserved Words are Case-Insensitive \u00b6 The following statements are equivalent since show and spaces are keywords. nebula> show spaces; nebula> SHOW SPACES; nebula> SHOW spaces; nebula> show SPACES; Functions are Case-Insensitive \u00b6 Functions are case-insensitive. For example, count() , COUNT() , and couNT() are equivalent. nebula> WITH [NULL, 1, 1, 2, 2] As a \\ UNWIND a AS b \\ RETURN count(b), COUNT(*), couNT(DISTINCT b); +----------+----------+-------------------+ | count(b) | COUNT(*) | couNT(distinct b) | +----------+----------+-------------------+ | 4 | 5 | 2 | +----------+----------+-------------------+","title":"Identifier case sensitivity"},{"location":"3.ngql-guide/1.nGQL-overview/identifier-case-sensitivity/#identifier_case_sensitivity","text":"","title":"Identifier case sensitivity"},{"location":"3.ngql-guide/1.nGQL-overview/identifier-case-sensitivity/#identifiers_are_case-sensitive","text":"The following statements will not work because they refer to two different spaces, i.e. my_space and MY_SPACE . nebula> CREATE SPACE IF NOT EXISTS my_space (vid_type=FIXED_STRING(30)); nebula> use MY_SPACE; [ERROR (-8)]: SpaceNotFound:","title":"Identifiers are Case-Sensitive"},{"location":"3.ngql-guide/1.nGQL-overview/identifier-case-sensitivity/#keywords_and_reserved_words_are_case-insensitive","text":"The following statements are equivalent since show and spaces are keywords. nebula> show spaces; nebula> SHOW SPACES; nebula> SHOW spaces; nebula> show SPACES;","title":"Keywords and Reserved Words are Case-Insensitive"},{"location":"3.ngql-guide/1.nGQL-overview/identifier-case-sensitivity/#functions_are_case-insensitive","text":"Functions are case-insensitive. For example, count() , COUNT() , and couNT() are equivalent. nebula> WITH [NULL, 1, 1, 2, 2] As a \\ UNWIND a AS b \\ RETURN count(b), COUNT(*), couNT(DISTINCT b); +----------+----------+-------------------+ | count(b) | COUNT(*) | couNT(distinct b) | +----------+----------+-------------------+ | 4 | 5 | 2 | +----------+----------+-------------------+","title":"Functions are Case-Insensitive"},{"location":"3.ngql-guide/1.nGQL-overview/keywords-and-reserved-words/","text":"Keywords \u00b6 Keywords have significance in nGQL. It can be classified into reserved keywords and non-reserved keywords. It is not recommend to use keywords in schema. If you must use keywords in schema: Non-reserved keywords are permitted as identifiers without quoting. To use special characters or reserved keywords as identifiers, quote them with backticks such as AND . Note Keywords are case-insensitive. nebula> CREATE TAG TAG(name string); [ERROR (-7)]: SyntaxError: syntax error near `TAG' nebula> CREATE TAG `TAG` (name string); Execution succeeded nebula> CREATE TAG SPACE(name string); Execution succeeded nebula> CREATE TAG \u4e2d\u6587(\u7b80\u4f53 string); Execution succeeded nebula> CREATE TAG `\uffe5%special characters&*+-*/` (`q~\uff01\uff08\uff09= wer` string); Execution succeeded Reserved keywords \u00b6 ACROSS ADD ALTER AND AS ASC ASCENDING BALANCE BOOL BY CASE CHANGE COMPACT CREATE DATE DATETIME DELETE DESC DESCENDING DESCRIBE DISTINCT DOUBLE DOWNLOAD DROP EDGE EDGES EXISTS EXPLAIN FETCH FIND FIXED_STRING FLOAT FLUSH FORMAT FROM GET GO GRANT IF IGNORE_EXISTED_INDEX IN INDEX INDEXES INGEST INSERT INT INT16 INT32 INT64 INT8 INTERSECT IS LIMIT LIST LOOKUP MAP MATCH MINUS NO NOT NOT_IN NULL OF OFFSET ON OR ORDER OVER OVERWRITE PROFILE PROP REBUILD RECOVER REMOVE RETURN REVERSELY REVOKE SET SHOW STEP STEPS STOP STRING SUBMIT TAG TAGS TIME TIMESTAMP TO UNION UPDATE UPSERT UPTO USE VERTEX VERTICES WHEN WHERE WITH XOR YIELD Non-reserved keywords \u00b6 ACCOUNT ADMIN ALL ANY ATOMIC_EDGE AUTO BIDIRECT BOTH CHARSET CLIENTS COLLATE COLLATION COMMENT CONFIGS CONTAINS DATA DBA DEFAULT ELASTICSEARCH ELSE END ENDS ENDS_WITH FORCE FULLTEXT FUZZY GOD GRAPH GROUP GROUPS GUEST HDFS HOST HOSTS INTO IS_EMPTY IS_NOT_EMPTY IS_NOT_NULL IS_NULL JOB JOBS KILL LEADER LISTENER META NOLOOP NONE NOT_CONTAINS NOT_ENDS_WITH NOT_STARTS_WITH OPTIONAL OUT PART PARTITION_NUM PARTS PASSWORD PATH PLAN PREFIX QUERIES QUERY REDUCE REGEXP REPLICA_FACTOR RESET ROLE ROLES SAMPLE SEARCH SERVICE SESSION SESSIONS SHORTEST SIGN SINGLE SKIP SNAPSHOT SNAPSHOTS SPACE SPACES STARTS STARTS_WITH STATS STATUS STORAGE SUBGRAPH TEXT TEXT_SEARCH THEN TOP TTL_COL TTL_DURATION UNWIND USER USERS UUID VALUE VALUES VID_TYPE WILDCARD ZONE ZONES FALSE TRUE","title":"Keywords"},{"location":"3.ngql-guide/1.nGQL-overview/keywords-and-reserved-words/#keywords","text":"Keywords have significance in nGQL. It can be classified into reserved keywords and non-reserved keywords. It is not recommend to use keywords in schema. If you must use keywords in schema: Non-reserved keywords are permitted as identifiers without quoting. To use special characters or reserved keywords as identifiers, quote them with backticks such as AND . Note Keywords are case-insensitive. nebula> CREATE TAG TAG(name string); [ERROR (-7)]: SyntaxError: syntax error near `TAG' nebula> CREATE TAG `TAG` (name string); Execution succeeded nebula> CREATE TAG SPACE(name string); Execution succeeded nebula> CREATE TAG \u4e2d\u6587(\u7b80\u4f53 string); Execution succeeded nebula> CREATE TAG `\uffe5%special characters&*+-*/` (`q~\uff01\uff08\uff09= wer` string); Execution succeeded","title":"Keywords"},{"location":"3.ngql-guide/1.nGQL-overview/keywords-and-reserved-words/#reserved_keywords","text":"ACROSS ADD ALTER AND AS ASC ASCENDING BALANCE BOOL BY CASE CHANGE COMPACT CREATE DATE DATETIME DELETE DESC DESCENDING DESCRIBE DISTINCT DOUBLE DOWNLOAD DROP EDGE EDGES EXISTS EXPLAIN FETCH FIND FIXED_STRING FLOAT FLUSH FORMAT FROM GET GO GRANT IF IGNORE_EXISTED_INDEX IN INDEX INDEXES INGEST INSERT INT INT16 INT32 INT64 INT8 INTERSECT IS LIMIT LIST LOOKUP MAP MATCH MINUS NO NOT NOT_IN NULL OF OFFSET ON OR ORDER OVER OVERWRITE PROFILE PROP REBUILD RECOVER REMOVE RETURN REVERSELY REVOKE SET SHOW STEP STEPS STOP STRING SUBMIT TAG TAGS TIME TIMESTAMP TO UNION UPDATE UPSERT UPTO USE VERTEX VERTICES WHEN WHERE WITH XOR YIELD","title":"Reserved keywords"},{"location":"3.ngql-guide/1.nGQL-overview/keywords-and-reserved-words/#non-reserved_keywords","text":"ACCOUNT ADMIN ALL ANY ATOMIC_EDGE AUTO BIDIRECT BOTH CHARSET CLIENTS COLLATE COLLATION COMMENT CONFIGS CONTAINS DATA DBA DEFAULT ELASTICSEARCH ELSE END ENDS ENDS_WITH FORCE FULLTEXT FUZZY GOD GRAPH GROUP GROUPS GUEST HDFS HOST HOSTS INTO IS_EMPTY IS_NOT_EMPTY IS_NOT_NULL IS_NULL JOB JOBS KILL LEADER LISTENER META NOLOOP NONE NOT_CONTAINS NOT_ENDS_WITH NOT_STARTS_WITH OPTIONAL OUT PART PARTITION_NUM PARTS PASSWORD PATH PLAN PREFIX QUERIES QUERY REDUCE REGEXP REPLICA_FACTOR RESET ROLE ROLES SAMPLE SEARCH SERVICE SESSION SESSIONS SHORTEST SIGN SINGLE SKIP SNAPSHOT SNAPSHOTS SPACE SPACES STARTS STARTS_WITH STATS STATUS STORAGE SUBGRAPH TEXT TEXT_SEARCH THEN TOP TTL_COL TTL_DURATION UNWIND USER USERS UUID VALUE VALUES VID_TYPE WILDCARD ZONE ZONES FALSE TRUE","title":"Non-reserved keywords"},{"location":"3.ngql-guide/1.nGQL-overview/ngql-style-guide/","text":"nGQL style guide \u00b6 nGQL does not have strict formatting requirements, but creating nGQL statements according to an appropriate and uniform style can improve readability and avoid ambiguity. Using the same nGQL style in the same organization or project helps reduce maintenance costs and avoid problems caused by format confusion or misunderstanding. This topic will provide a style guide for writing nGQL statements. Compatibility The styles of nGQL and Cypher Style Guide are different. Newline \u00b6 Start a new line to write a clause. Not recommended: GO FROM \"player100\" OVER follow REVERSELY YIELD src(edge) AS id; Recommended: GO FROM \"player100\" \\ OVER follow REVERSELY \\ YIELD src(edge) AS id; Start a new line to write different statements in a composite statement. Not recommended: GO FROM \"player100\" OVER follow REVERSELY YIELD src(edge) AS id | GO FROM $-.id \\ OVER serve WHERE properties($^).age > 20 YIELD properties($^).name AS FriendOf, properties($$).name AS Team; Recommended: GO FROM \"player100\" \\ OVER follow REVERSELY \\ YIELD src(edge) AS id | \\ GO FROM $-.id OVER serve \\ WHERE properties($^).age > 20 \\ YIELD properties($^).name AS FriendOf, properties($$).name AS Team; If the clause exceeds 80 characters, start a new line at the appropriate place. Not recommended: MATCH (v:player{name:\"Tim Duncan\"})-[e]->(v2) \\ WHERE (v2.player.name STARTS WITH \"Y\" AND v2.player.age > 35 AND v2.player.age < v.player.age) OR (v2.player.name STARTS WITH \"T\" AND v2.player.age < 45 AND v2.player.age > v.player.age) \\ RETURN v2; Recommended: MATCH (v:player{name:\"Tim Duncan\"})-[e]->(v2) \\ WHERE (v2.player.name STARTS WITH \"Y\" AND v2.player.age > 35 AND v2.player.age < v.player.age) \\ OR (v2.player.name STARTS WITH \"T\" AND v2.player.age < 45 AND v2.player.age > v.player.age) \\ RETURN v2; Note If needed, you can also start a new line for better understanding, even if the clause does not exceed 80 characters. Identifier naming \u00b6 In nGQL statements, characters other than keywords, punctuation marks, and blanks are all identifiers. Recommended methods to name the identifiers are as follows. Use singular nouns to name tags, and use the base form of verbs or verb phrases to form Edge types. Not recommended: MATCH p=(v:players)-[e:are_following]-(v2) \\ RETURN nodes(p); Recommended: MATCH p=(v:player)-[e:follow]-(v2) \\ RETURN nodes(p); Use the snake case to name identifiers, and connect words with underscores (_) with all the letters lowercase. Not recommended: MATCH (v:basketballTeam) \\ RETURN v; Recommended: MATCH (v:basketball_team) \\ RETURN v; Use uppercase keywords and lowercase variables. Not recommended: go from \"player100\" over Follow Recommended: GO FROM \"player100\" OVER follow Pattern \u00b6 Start a new line on the right side of the arrow indicating an edge when writing patterns. Not recommended: MATCH (v:player{name: \"Tim Duncan\", age: 42}) \\ -[e:follow]->()-[e:serve]->()<--(v2) \\ RETURN v, e, v2; Recommended: MATCH (v:player{name: \"Tim Duncan\", age: 42})-[e:follow]-> \\ ()-[e:serve]->()<--(v2) \\ RETURN v, e, v2; Anonymize the vertices and edges that do not need to be queried. Not recommended: MATCH (v:player)-[e:follow]->(v2) \\ RETURN v; Recommended: MATCH (v:player)-[:follow]->() \\ RETURN v; Place named vertices in front of anonymous vertices. Not recommended: MATCH ()-[:follow]->(v) \\ RETURN v; Recommended: MATCH (v)<-[:follow]-() \\ RETURN v; String \u00b6 The strings should be surrounded by double quotes. Not recommended: RETURN 'Hello Nebula!'; Recommended: RETURN \"Hello Nebula!\\\"123\\\"\"; Note When single or double quotes need to be nested in a string, use a backslash () to escape. For example: RETURN \"\\\"Nebula Graph is amazing,\\\" the user says.\"; Statement termination \u00b6 End the nGQL statements with an English semicolon (;). Not recommended: FETCH PROP ON player \"player100\" Recommended: FETCH PROP ON player \"player100\"; Use a pipe (|) to separate a composite statement, and end the statement with an English semicolon at the end of the last line. Using an English semicolon before a pipe will cause the statement to fail. Not supported: GO FROM \"player100\" \\ OVER follow \\ YIELD dst(edge) AS id; | \\ GO FROM $-.id \\ OVER serve \\ YIELD properties($$).name AS Team, properties($^).name AS Player; Supported: GO FROM \"player100\" \\ OVER follow \\ YIELD dst(edge) AS id | \\ GO FROM $-.id \\ OVER serve \\ YIELD properties($$).name AS Team, properties($^).name AS Player; In a composite statement that contains user-defined variables, use an English semicolon to end the statements that define the variables. If you do not follow the rules to add a semicolon or use a pipe to end the composite statement, the execution will fail. Not supported: $var = GO FROM \"player100\" \\ OVER follow \\ YIELD follow._dst AS id \\ GO FROM $var.id \\ OVER serve \\ YIELD $$.team.name AS Team, $^.player.name AS Player; Not supported: $var = GO FROM \"player100\" \\ OVER follow \\ YIELD follow._dst AS id | \\ GO FROM $var.id \\ OVER serve \\ YIELD $$.team.name AS Team, $^.player.name AS Player; Supported: $var = GO FROM \"player100\" \\ OVER follow \\ YIELD follow._dst AS id; \\ GO FROM $var.id \\ OVER serve \\ YIELD $$.team.name AS Team, $^.player.name AS Player;","title":"nGQL style guide"},{"location":"3.ngql-guide/1.nGQL-overview/ngql-style-guide/#ngql_style_guide","text":"nGQL does not have strict formatting requirements, but creating nGQL statements according to an appropriate and uniform style can improve readability and avoid ambiguity. Using the same nGQL style in the same organization or project helps reduce maintenance costs and avoid problems caused by format confusion or misunderstanding. This topic will provide a style guide for writing nGQL statements. Compatibility The styles of nGQL and Cypher Style Guide are different.","title":"nGQL style guide"},{"location":"3.ngql-guide/1.nGQL-overview/ngql-style-guide/#newline","text":"Start a new line to write a clause. Not recommended: GO FROM \"player100\" OVER follow REVERSELY YIELD src(edge) AS id; Recommended: GO FROM \"player100\" \\ OVER follow REVERSELY \\ YIELD src(edge) AS id; Start a new line to write different statements in a composite statement. Not recommended: GO FROM \"player100\" OVER follow REVERSELY YIELD src(edge) AS id | GO FROM $-.id \\ OVER serve WHERE properties($^).age > 20 YIELD properties($^).name AS FriendOf, properties($$).name AS Team; Recommended: GO FROM \"player100\" \\ OVER follow REVERSELY \\ YIELD src(edge) AS id | \\ GO FROM $-.id OVER serve \\ WHERE properties($^).age > 20 \\ YIELD properties($^).name AS FriendOf, properties($$).name AS Team; If the clause exceeds 80 characters, start a new line at the appropriate place. Not recommended: MATCH (v:player{name:\"Tim Duncan\"})-[e]->(v2) \\ WHERE (v2.player.name STARTS WITH \"Y\" AND v2.player.age > 35 AND v2.player.age < v.player.age) OR (v2.player.name STARTS WITH \"T\" AND v2.player.age < 45 AND v2.player.age > v.player.age) \\ RETURN v2; Recommended: MATCH (v:player{name:\"Tim Duncan\"})-[e]->(v2) \\ WHERE (v2.player.name STARTS WITH \"Y\" AND v2.player.age > 35 AND v2.player.age < v.player.age) \\ OR (v2.player.name STARTS WITH \"T\" AND v2.player.age < 45 AND v2.player.age > v.player.age) \\ RETURN v2; Note If needed, you can also start a new line for better understanding, even if the clause does not exceed 80 characters.","title":"Newline"},{"location":"3.ngql-guide/1.nGQL-overview/ngql-style-guide/#identifier_naming","text":"In nGQL statements, characters other than keywords, punctuation marks, and blanks are all identifiers. Recommended methods to name the identifiers are as follows. Use singular nouns to name tags, and use the base form of verbs or verb phrases to form Edge types. Not recommended: MATCH p=(v:players)-[e:are_following]-(v2) \\ RETURN nodes(p); Recommended: MATCH p=(v:player)-[e:follow]-(v2) \\ RETURN nodes(p); Use the snake case to name identifiers, and connect words with underscores (_) with all the letters lowercase. Not recommended: MATCH (v:basketballTeam) \\ RETURN v; Recommended: MATCH (v:basketball_team) \\ RETURN v; Use uppercase keywords and lowercase variables. Not recommended: go from \"player100\" over Follow Recommended: GO FROM \"player100\" OVER follow","title":"Identifier naming"},{"location":"3.ngql-guide/1.nGQL-overview/ngql-style-guide/#pattern","text":"Start a new line on the right side of the arrow indicating an edge when writing patterns. Not recommended: MATCH (v:player{name: \"Tim Duncan\", age: 42}) \\ -[e:follow]->()-[e:serve]->()<--(v2) \\ RETURN v, e, v2; Recommended: MATCH (v:player{name: \"Tim Duncan\", age: 42})-[e:follow]-> \\ ()-[e:serve]->()<--(v2) \\ RETURN v, e, v2; Anonymize the vertices and edges that do not need to be queried. Not recommended: MATCH (v:player)-[e:follow]->(v2) \\ RETURN v; Recommended: MATCH (v:player)-[:follow]->() \\ RETURN v; Place named vertices in front of anonymous vertices. Not recommended: MATCH ()-[:follow]->(v) \\ RETURN v; Recommended: MATCH (v)<-[:follow]-() \\ RETURN v;","title":"Pattern"},{"location":"3.ngql-guide/1.nGQL-overview/ngql-style-guide/#string","text":"The strings should be surrounded by double quotes. Not recommended: RETURN 'Hello Nebula!'; Recommended: RETURN \"Hello Nebula!\\\"123\\\"\"; Note When single or double quotes need to be nested in a string, use a backslash () to escape. For example: RETURN \"\\\"Nebula Graph is amazing,\\\" the user says.\";","title":"String"},{"location":"3.ngql-guide/1.nGQL-overview/ngql-style-guide/#statement_termination","text":"End the nGQL statements with an English semicolon (;). Not recommended: FETCH PROP ON player \"player100\" Recommended: FETCH PROP ON player \"player100\"; Use a pipe (|) to separate a composite statement, and end the statement with an English semicolon at the end of the last line. Using an English semicolon before a pipe will cause the statement to fail. Not supported: GO FROM \"player100\" \\ OVER follow \\ YIELD dst(edge) AS id; | \\ GO FROM $-.id \\ OVER serve \\ YIELD properties($$).name AS Team, properties($^).name AS Player; Supported: GO FROM \"player100\" \\ OVER follow \\ YIELD dst(edge) AS id | \\ GO FROM $-.id \\ OVER serve \\ YIELD properties($$).name AS Team, properties($^).name AS Player; In a composite statement that contains user-defined variables, use an English semicolon to end the statements that define the variables. If you do not follow the rules to add a semicolon or use a pipe to end the composite statement, the execution will fail. Not supported: $var = GO FROM \"player100\" \\ OVER follow \\ YIELD follow._dst AS id \\ GO FROM $var.id \\ OVER serve \\ YIELD $$.team.name AS Team, $^.player.name AS Player; Not supported: $var = GO FROM \"player100\" \\ OVER follow \\ YIELD follow._dst AS id | \\ GO FROM $var.id \\ OVER serve \\ YIELD $$.team.name AS Team, $^.player.name AS Player; Supported: $var = GO FROM \"player100\" \\ OVER follow \\ YIELD follow._dst AS id; \\ GO FROM $var.id \\ OVER serve \\ YIELD $$.team.name AS Team, $^.player.name AS Player;","title":"Statement termination"},{"location":"3.ngql-guide/10.tag-statements/1.create-tag/","text":"CREATE TAG \u00b6 CREATE TAG creates a tag with the given name in a graph space. OpenCypher compatibility \u00b6 Tags in nGQL are similar to labels in openCypher. But they are also quite different. For example, the ways to create them are different. In openCypher, labels are created together with vertices in CREATE statements. In nGQL, tags are created separately using CREATE TAG statements. Tags in nGQL are more like tables in MySQL. Prerequisites \u00b6 Running the CREATE TAG statement requires some privileges for the graph space. Otherwise, Nebula Graph throws an error. Syntax \u00b6 To create a tag in a specific graph space, you must specify the current working space with the USE statement. CREATE TAG [IF NOT EXISTS] <tag_name> ( <prop_name> <data_type> [NULL | NOT NULL] [DEFAULT <default_value>] [COMMENT '<comment>'] [{, <prop_name> <data_type> [NULL | NOT NULL] [DEFAULT <default_value>] [COMMENT '<comment>']} ...] ) [TTL_DURATION = <ttl_duration>] [TTL_COL = <prop_name>] [COMMENT = '<comment>']; Parameter Description IF NOT EXISTS Detects if the tag that you want to create exists. If it does not exist, a new one will be created. The tag existence detection here only compares the tag names (excluding properties). <tag_name> The tag name must be unique in a graph space. Once the tag name is set, it can not be altered. The name of the tag starts with a letter, supports 1 to 4 bytes UTF-8 encoded characters, such as English letters (case-sensitive), digits, and Chinese characters, but does not support special characters except underscores. To use special characters or reserved keywords as identifiers, quote them with backticks. For more information, see Keywords and reserved words . <prop_name> The name of the property. It must be unique for each tag. The rules for permitted property names are the same as those for tag names. <data_type> Shows the data type of each property. For a full description of the property data types, see Data types and Boolean . NULL \\| NOT NULL Specifies if the property supports NULL | NOT NULL . The default value is NULL . DEFAULT Specifies a default value for a property. The default value can be a literal value or an expression supported by Nebula Graph. If no value is specified, the default value is used when inserting a new vertex. COMMENT The remarks of a certain property or the tag itself. The maximum length is 256 bytes. By default, there will be no comments on a tag. TTL_DURATION Specifies the life cycle for the property. The property that exceeds the specified TTL expires. The expiration threshold is the TTL_COL value plus the TTL_DURATION . The default value of TTL_DURATION is 0 . It means the data never expires. TTL_COL Specifies the property to set a timeout on. The data type of the property must be int or timestamp . A tag can only specify one field as TTL_COL . For more information on TTL, see TTL options . Examples \u00b6 nebula> CREATE TAG IF NOT EXISTS player(name string, age int); # The following example creates a tag with no properties. nebula> CREATE TAG IF NOT EXISTS no_property(); # The following example creates a tag with a default value. nebula> CREATE TAG IF NOT EXISTS player_with_default(name string, age int DEFAULT 20); # In the following example, the TTL of the create_time field is set to be 100 seconds. nebula> CREATE TAG IF NOT EXISTS woman(name string, age int, \\ married bool, salary double, create_time timestamp) \\ TTL_DURATION = 100, TTL_COL = \"create_time\"; Implementation of the operation \u00b6 Trying to use a newly created tag may fail because the creation of the tag is implemented asynchronously. To make sure the follow-up operations work as expected, Wait for two heartbeat cycles, i.e., 20 seconds. To change the heartbeat interval, modify the heartbeat_interval_secs parameter in the configuration files for all services.","title":"CREATE TAG"},{"location":"3.ngql-guide/10.tag-statements/1.create-tag/#create_tag","text":"CREATE TAG creates a tag with the given name in a graph space.","title":"CREATE TAG"},{"location":"3.ngql-guide/10.tag-statements/1.create-tag/#opencypher_compatibility","text":"Tags in nGQL are similar to labels in openCypher. But they are also quite different. For example, the ways to create them are different. In openCypher, labels are created together with vertices in CREATE statements. In nGQL, tags are created separately using CREATE TAG statements. Tags in nGQL are more like tables in MySQL.","title":"OpenCypher compatibility"},{"location":"3.ngql-guide/10.tag-statements/1.create-tag/#prerequisites","text":"Running the CREATE TAG statement requires some privileges for the graph space. Otherwise, Nebula Graph throws an error.","title":"Prerequisites"},{"location":"3.ngql-guide/10.tag-statements/1.create-tag/#syntax","text":"To create a tag in a specific graph space, you must specify the current working space with the USE statement. CREATE TAG [IF NOT EXISTS] <tag_name> ( <prop_name> <data_type> [NULL | NOT NULL] [DEFAULT <default_value>] [COMMENT '<comment>'] [{, <prop_name> <data_type> [NULL | NOT NULL] [DEFAULT <default_value>] [COMMENT '<comment>']} ...] ) [TTL_DURATION = <ttl_duration>] [TTL_COL = <prop_name>] [COMMENT = '<comment>']; Parameter Description IF NOT EXISTS Detects if the tag that you want to create exists. If it does not exist, a new one will be created. The tag existence detection here only compares the tag names (excluding properties). <tag_name> The tag name must be unique in a graph space. Once the tag name is set, it can not be altered. The name of the tag starts with a letter, supports 1 to 4 bytes UTF-8 encoded characters, such as English letters (case-sensitive), digits, and Chinese characters, but does not support special characters except underscores. To use special characters or reserved keywords as identifiers, quote them with backticks. For more information, see Keywords and reserved words . <prop_name> The name of the property. It must be unique for each tag. The rules for permitted property names are the same as those for tag names. <data_type> Shows the data type of each property. For a full description of the property data types, see Data types and Boolean . NULL \\| NOT NULL Specifies if the property supports NULL | NOT NULL . The default value is NULL . DEFAULT Specifies a default value for a property. The default value can be a literal value or an expression supported by Nebula Graph. If no value is specified, the default value is used when inserting a new vertex. COMMENT The remarks of a certain property or the tag itself. The maximum length is 256 bytes. By default, there will be no comments on a tag. TTL_DURATION Specifies the life cycle for the property. The property that exceeds the specified TTL expires. The expiration threshold is the TTL_COL value plus the TTL_DURATION . The default value of TTL_DURATION is 0 . It means the data never expires. TTL_COL Specifies the property to set a timeout on. The data type of the property must be int or timestamp . A tag can only specify one field as TTL_COL . For more information on TTL, see TTL options .","title":"Syntax"},{"location":"3.ngql-guide/10.tag-statements/1.create-tag/#examples","text":"nebula> CREATE TAG IF NOT EXISTS player(name string, age int); # The following example creates a tag with no properties. nebula> CREATE TAG IF NOT EXISTS no_property(); # The following example creates a tag with a default value. nebula> CREATE TAG IF NOT EXISTS player_with_default(name string, age int DEFAULT 20); # In the following example, the TTL of the create_time field is set to be 100 seconds. nebula> CREATE TAG IF NOT EXISTS woman(name string, age int, \\ married bool, salary double, create_time timestamp) \\ TTL_DURATION = 100, TTL_COL = \"create_time\";","title":"Examples"},{"location":"3.ngql-guide/10.tag-statements/1.create-tag/#implementation_of_the_operation","text":"Trying to use a newly created tag may fail because the creation of the tag is implemented asynchronously. To make sure the follow-up operations work as expected, Wait for two heartbeat cycles, i.e., 20 seconds. To change the heartbeat interval, modify the heartbeat_interval_secs parameter in the configuration files for all services.","title":"Implementation of the operation"},{"location":"3.ngql-guide/10.tag-statements/2.drop-tag/","text":"DROP TAG \u00b6 DROP TAG drops a tag with the given name in the current working graph space. A vertex can have one or more tags. If a vertex has only one tag, the vertex CANNOT be accessed after you drop it. But its edges are available. The vertex will be dropped in the next compaction. If a vertex has multiple tags, the vertex is still accessible after you drop one of them. But all the properties defined by this dropped tag CANNOT be accessed. This operation only deletes the Schema data. All the files or directories in the disk will not be deleted directly until the next compaction. Prerequisites \u00b6 Running the DROP TAG statement requires some privileges for the graph space. Otherwise, Nebula Graph throws an error. Before you drop a tag, make sure that the tag does not have any indexes. Otherwise, the conflict error ( [ERROR (-8)]: Conflict! ) will be returned when you run the DROP TAG statement. To drop an index, see DROP INDEX . Syntax \u00b6 DROP TAG [IF EXISTS] <tag_name>; IF NOT EXISTS : Detects if the tag that you want to drop exists. Only when it exists will it be dropped. tag_name : Specifies the tag name that you want to drop. You can drop only one tag in one statement. Example \u00b6 nebula> CREATE TAG IF NOT EXISTS test(p1 string, p2 int); nebula> DROP TAG test; Note In nGQL, there is no such statement to drop a certain tag of a vertex with the given name. In openCypher, you can use the statement REMOVE v:LABEL to drop the tag LABLE of the vertex v . In nGQL, after CREATE TAG and INSERT VERTEX , you can add a TAG on the vertex. But there is no way to drop the TAG afterward. We recommend you to add a field to identify the logical deletion in the schema. For example, add removed to the schema of each tag.","title":"DROP TAGS"},{"location":"3.ngql-guide/10.tag-statements/2.drop-tag/#drop_tag","text":"DROP TAG drops a tag with the given name in the current working graph space. A vertex can have one or more tags. If a vertex has only one tag, the vertex CANNOT be accessed after you drop it. But its edges are available. The vertex will be dropped in the next compaction. If a vertex has multiple tags, the vertex is still accessible after you drop one of them. But all the properties defined by this dropped tag CANNOT be accessed. This operation only deletes the Schema data. All the files or directories in the disk will not be deleted directly until the next compaction.","title":"DROP TAG"},{"location":"3.ngql-guide/10.tag-statements/2.drop-tag/#prerequisites","text":"Running the DROP TAG statement requires some privileges for the graph space. Otherwise, Nebula Graph throws an error. Before you drop a tag, make sure that the tag does not have any indexes. Otherwise, the conflict error ( [ERROR (-8)]: Conflict! ) will be returned when you run the DROP TAG statement. To drop an index, see DROP INDEX .","title":"Prerequisites"},{"location":"3.ngql-guide/10.tag-statements/2.drop-tag/#syntax","text":"DROP TAG [IF EXISTS] <tag_name>; IF NOT EXISTS : Detects if the tag that you want to drop exists. Only when it exists will it be dropped. tag_name : Specifies the tag name that you want to drop. You can drop only one tag in one statement.","title":"Syntax"},{"location":"3.ngql-guide/10.tag-statements/2.drop-tag/#example","text":"nebula> CREATE TAG IF NOT EXISTS test(p1 string, p2 int); nebula> DROP TAG test; Note In nGQL, there is no such statement to drop a certain tag of a vertex with the given name. In openCypher, you can use the statement REMOVE v:LABEL to drop the tag LABLE of the vertex v . In nGQL, after CREATE TAG and INSERT VERTEX , you can add a TAG on the vertex. But there is no way to drop the TAG afterward. We recommend you to add a field to identify the logical deletion in the schema. For example, add removed to the schema of each tag.","title":"Example"},{"location":"3.ngql-guide/10.tag-statements/3.alter-tag/","text":"ALTER TAG \u00b6 ALTER TAG alters the structure of a tag with the given name in a graph space. You can add or drop properties, and change the data type of an existing property. You can also set a TTL (Time-To-Live) on a property, or change its TTL duration. Prerequisites \u00b6 Running the ALTER TAG statement requires some privileges for the graph space. Otherwise, Nebula Graph throws an error. Before you alter properties for a tag, make sure that the properties are not indexed. If the properties contain any indexes, the conflict error [ERROR (-8)]: Conflict! will occur when you ALTER TAG . For more information on dropping an index, see DROP INDEX . Syntax \u00b6 ALTER TAG <tag_name> <alter_definition> [[, alter_definition] ...] [ttl_definition [, ttl_definition] ... ] [COMMENT '<comment>']; alter_definition: | ADD (prop_name data_type [NULL | NOT NULL] [DEFAULT <default_value>] [COMMENT '<comment>']) | DROP (prop_name) | CHANGE (prop_name data_type [NULL | NOT NULL] [DEFAULT <default_value>] [COMMENT '<comment>']) ttl_definition: TTL_DURATION = ttl_duration, TTL_COL = prop_name tag_name : Specifies the tag name that you want to alter. You can alter only one tag in one statement. Before you alter a tag, make sure that the tag exists in the current working graph space. If the tag does not exist, an error will occur when you alter it. Multiple ADD , DROP , and CHANGE clauses are permitted in a single ALTER TAG statement, separated by commas. Examples \u00b6 nebula> CREATE TAG IF NOT EXISTS t1 (p1 string, p2 int); nebula> ALTER TAG t1 ADD (p3 int, p4 string); nebula> ALTER TAG t1 TTL_DURATION = 2, TTL_COL = \"p2\"; nebula> ALTER TAG t1 COMMENT = 'test1'; nebula> ALTER TAG t1 ADD (p5 double NOT NULL DEFAULT 0.4 COMMENT 'p5') COMMENT='test2'; Implementation of the operation \u00b6 Trying to use a newly altered tag may fail because the alteration of the tag is implemented asynchronously. To make sure the follow-up operations work as expected, Wait for two heartbeat cycles, i.e., 20 seconds. To change the heartbeat interval, modify the heartbeat_interval_secs parameter in the configuration files for all services.","title":"ALTER TAG"},{"location":"3.ngql-guide/10.tag-statements/3.alter-tag/#alter_tag","text":"ALTER TAG alters the structure of a tag with the given name in a graph space. You can add or drop properties, and change the data type of an existing property. You can also set a TTL (Time-To-Live) on a property, or change its TTL duration.","title":"ALTER TAG"},{"location":"3.ngql-guide/10.tag-statements/3.alter-tag/#prerequisites","text":"Running the ALTER TAG statement requires some privileges for the graph space. Otherwise, Nebula Graph throws an error. Before you alter properties for a tag, make sure that the properties are not indexed. If the properties contain any indexes, the conflict error [ERROR (-8)]: Conflict! will occur when you ALTER TAG . For more information on dropping an index, see DROP INDEX .","title":"Prerequisites"},{"location":"3.ngql-guide/10.tag-statements/3.alter-tag/#syntax","text":"ALTER TAG <tag_name> <alter_definition> [[, alter_definition] ...] [ttl_definition [, ttl_definition] ... ] [COMMENT '<comment>']; alter_definition: | ADD (prop_name data_type [NULL | NOT NULL] [DEFAULT <default_value>] [COMMENT '<comment>']) | DROP (prop_name) | CHANGE (prop_name data_type [NULL | NOT NULL] [DEFAULT <default_value>] [COMMENT '<comment>']) ttl_definition: TTL_DURATION = ttl_duration, TTL_COL = prop_name tag_name : Specifies the tag name that you want to alter. You can alter only one tag in one statement. Before you alter a tag, make sure that the tag exists in the current working graph space. If the tag does not exist, an error will occur when you alter it. Multiple ADD , DROP , and CHANGE clauses are permitted in a single ALTER TAG statement, separated by commas.","title":"Syntax"},{"location":"3.ngql-guide/10.tag-statements/3.alter-tag/#examples","text":"nebula> CREATE TAG IF NOT EXISTS t1 (p1 string, p2 int); nebula> ALTER TAG t1 ADD (p3 int, p4 string); nebula> ALTER TAG t1 TTL_DURATION = 2, TTL_COL = \"p2\"; nebula> ALTER TAG t1 COMMENT = 'test1'; nebula> ALTER TAG t1 ADD (p5 double NOT NULL DEFAULT 0.4 COMMENT 'p5') COMMENT='test2';","title":"Examples"},{"location":"3.ngql-guide/10.tag-statements/3.alter-tag/#implementation_of_the_operation","text":"Trying to use a newly altered tag may fail because the alteration of the tag is implemented asynchronously. To make sure the follow-up operations work as expected, Wait for two heartbeat cycles, i.e., 20 seconds. To change the heartbeat interval, modify the heartbeat_interval_secs parameter in the configuration files for all services.","title":"Implementation of the operation"},{"location":"3.ngql-guide/10.tag-statements/4.show-tags/","text":"SHOW TAGS \u00b6 The SHOW TAGS statement shows the name of all tags in the current graph space. You do not need any privileges for the graph space to run the SHOW TAGS statement. But the returned results are different based on role privileges . Syntax \u00b6 SHOW TAGS; Examples \u00b6 nebula> SHOW TAGS; +----------+ | Name | +----------+ | \"player\" | | \"team\" | +----------+","title":"SHOW TAGS"},{"location":"3.ngql-guide/10.tag-statements/4.show-tags/#show_tags","text":"The SHOW TAGS statement shows the name of all tags in the current graph space. You do not need any privileges for the graph space to run the SHOW TAGS statement. But the returned results are different based on role privileges .","title":"SHOW TAGS"},{"location":"3.ngql-guide/10.tag-statements/4.show-tags/#syntax","text":"SHOW TAGS;","title":"Syntax"},{"location":"3.ngql-guide/10.tag-statements/4.show-tags/#examples","text":"nebula> SHOW TAGS; +----------+ | Name | +----------+ | \"player\" | | \"team\" | +----------+","title":"Examples"},{"location":"3.ngql-guide/10.tag-statements/5.describe-tag/","text":"DESCRIBE TAG \u00b6 DESCRIBE TAG returns the information about a tag with the given name in a graph space, such as field names, data type, and so on. Prerequisite \u00b6 Running the DESCRIBE TAG statement requires some privileges for the graph space. Otherwise, Nebula Graph throws an error. Syntax \u00b6 DESC[RIBE] TAG <tag_name>; You can use DESC instead of DESCRIBE for short. Example \u00b6 nebula> DESCRIBE TAG player; +--------+----------+-------+---------+---------+ | Field | Type | Null | Default | Comment | +--------+----------+-------+---------+---------+ | \"name\" | \"string\" | \"YES\" | | | | \"age\" | \"int64\" | \"YES\" | | | +--------+----------+-------+---------+---------+","title":"DESCRIBE TAG"},{"location":"3.ngql-guide/10.tag-statements/5.describe-tag/#describe_tag","text":"DESCRIBE TAG returns the information about a tag with the given name in a graph space, such as field names, data type, and so on.","title":"DESCRIBE TAG"},{"location":"3.ngql-guide/10.tag-statements/5.describe-tag/#prerequisite","text":"Running the DESCRIBE TAG statement requires some privileges for the graph space. Otherwise, Nebula Graph throws an error.","title":"Prerequisite"},{"location":"3.ngql-guide/10.tag-statements/5.describe-tag/#syntax","text":"DESC[RIBE] TAG <tag_name>; You can use DESC instead of DESCRIBE for short.","title":"Syntax"},{"location":"3.ngql-guide/10.tag-statements/5.describe-tag/#example","text":"nebula> DESCRIBE TAG player; +--------+----------+-------+---------+---------+ | Field | Type | Null | Default | Comment | +--------+----------+-------+---------+---------+ | \"name\" | \"string\" | \"YES\" | | | | \"age\" | \"int64\" | \"YES\" | | | +--------+----------+-------+---------+---------+","title":"Example"},{"location":"3.ngql-guide/10.tag-statements/6.delete-tag/","text":"DELETE TAG \u00b6 DELETE TAG deletes a tag with the given name on a specified vertex. Prerequisites \u00b6 Running the DELETE TAG statement requires some privileges for the graph space. Otherwise, Nebula Graph throws an error. Syntax \u00b6 DELETE TAG <tag_name_list> FROM <VID>; tag_name_list : Specifies the name of the tag. Multiple tags are separated with commas (,). * means all tags. VID : Specifies the VID of the tag to delete. Example \u00b6 nebula> CREATE TAG IF NOT EXISTS test1(p1 string, p2 int); nebula> CREATE TAG IF NOT EXISTS test2(p3 string, p4 int); nebula> INSERT VERTEX test1(p1, p2),test2(p3, p4) VALUES \"test\":(\"123\", 1, \"456\", 2); nebula> FETCH PROP ON * \"test\" YIELD vertex AS v; +------------------------------------------------------------+ | v | +------------------------------------------------------------+ | (\"test\" :test1{p1: \"123\", p2: 1} :test2{p3: \"456\", p4: 2}) | +------------------------------------------------------------+ nebula> DELETE TAG test1 FROM \"test\"; nebula> FETCH PROP ON * \"test\" YIELD vertex AS v; +-----------------------------------+ | v | +-----------------------------------+ | (\"test\" :test2{p3: \"456\", p4: 2}) | +-----------------------------------+ nebula> DELETE TAG * FROM \"test\"; nebula> FETCH PROP ON * \"test\" YIELD vertex AS v; +---+ | v | +---+ +---+ Compatibility In openCypher, you can use the statement REMOVE v:LABEL to delete the tag LABEL of the vertex v . DELETE TAG and DROP TAG have the same semantics but different syntax. In nGQL, use DELETE TAG .","title":"DELETE TAG"},{"location":"3.ngql-guide/10.tag-statements/6.delete-tag/#delete_tag","text":"DELETE TAG deletes a tag with the given name on a specified vertex.","title":"DELETE TAG"},{"location":"3.ngql-guide/10.tag-statements/6.delete-tag/#prerequisites","text":"Running the DELETE TAG statement requires some privileges for the graph space. Otherwise, Nebula Graph throws an error.","title":"Prerequisites"},{"location":"3.ngql-guide/10.tag-statements/6.delete-tag/#syntax","text":"DELETE TAG <tag_name_list> FROM <VID>; tag_name_list : Specifies the name of the tag. Multiple tags are separated with commas (,). * means all tags. VID : Specifies the VID of the tag to delete.","title":"Syntax"},{"location":"3.ngql-guide/10.tag-statements/6.delete-tag/#example","text":"nebula> CREATE TAG IF NOT EXISTS test1(p1 string, p2 int); nebula> CREATE TAG IF NOT EXISTS test2(p3 string, p4 int); nebula> INSERT VERTEX test1(p1, p2),test2(p3, p4) VALUES \"test\":(\"123\", 1, \"456\", 2); nebula> FETCH PROP ON * \"test\" YIELD vertex AS v; +------------------------------------------------------------+ | v | +------------------------------------------------------------+ | (\"test\" :test1{p1: \"123\", p2: 1} :test2{p3: \"456\", p4: 2}) | +------------------------------------------------------------+ nebula> DELETE TAG test1 FROM \"test\"; nebula> FETCH PROP ON * \"test\" YIELD vertex AS v; +-----------------------------------+ | v | +-----------------------------------+ | (\"test\" :test2{p3: \"456\", p4: 2}) | +-----------------------------------+ nebula> DELETE TAG * FROM \"test\"; nebula> FETCH PROP ON * \"test\" YIELD vertex AS v; +---+ | v | +---+ +---+ Compatibility In openCypher, you can use the statement REMOVE v:LABEL to delete the tag LABEL of the vertex v . DELETE TAG and DROP TAG have the same semantics but different syntax. In nGQL, use DELETE TAG .","title":"Example"},{"location":"3.ngql-guide/10.tag-statements/improve-query-by-tag-index/","text":"Add and delete tags \u00b6 OpenCypher has the features of SET label and REMOVE label to speed up the process of querying or labeling. Nebula Graph achieves the same operations by creating and inserting tags to an existing vertex, which can quickly query vertices based on the tag name. Users can also run DELETE TAG to delete some vertices that are no longer needed. Examples \u00b6 For example, in the basketballplayer data set, some basketball players are also team shareholders. Users can create an index for the shareholder tag shareholder for quick search. If the player is no longer a shareholder, users can delete the shareholder tag of the corresponding player by DELETE TAG . //This example creates the shareholder tag and index. nebula> CREATE TAG IF NOT EXISTS shareholder(); nebula> CREATE TAG INDEX IF NOT EXISTS shareholder_tag on shareholder(); //This example adds a tag on the vertex. nebula> INSERT VERTEX shareholder() VALUES \"player100\":(); nebula> INSERT VERTEX shareholder() VALUES \"player101\":(); //This example queries all the shareholders. nebula> MATCH (v:shareholder) RETURN v; +--------------------------------------------------------------------+ | v | +--------------------------------------------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"} :shareholder{}) | | (\"player101\" :player{age: 36, name: \"Tony Parker\"} :shareholder{}) | +--------------------------------------------------------------------+ nebula> LOOKUP ON shareholder YIELD id(vertex); +-------------+ | id(VERTEX) | +-------------+ | \"player100\" | | \"player101\" | +-------------+ //In this example, the \"player100\" is no longer a shareholder. nebula> DELETE TAG shareholder FROM \"player100\"; nebula> LOOKUP ON shareholder YIELD id(vertex); +-------------+ | id(VERTEX) | +-------------+ | \"player101\" | +-------------+ Note If the index is created after inserting the test data, use the REBUILD TAG INDEX <index_name_list>; statement to rebuild the index.","title":"Add or delete tag"},{"location":"3.ngql-guide/10.tag-statements/improve-query-by-tag-index/#add_and_delete_tags","text":"OpenCypher has the features of SET label and REMOVE label to speed up the process of querying or labeling. Nebula Graph achieves the same operations by creating and inserting tags to an existing vertex, which can quickly query vertices based on the tag name. Users can also run DELETE TAG to delete some vertices that are no longer needed.","title":"Add and delete tags"},{"location":"3.ngql-guide/10.tag-statements/improve-query-by-tag-index/#examples","text":"For example, in the basketballplayer data set, some basketball players are also team shareholders. Users can create an index for the shareholder tag shareholder for quick search. If the player is no longer a shareholder, users can delete the shareholder tag of the corresponding player by DELETE TAG . //This example creates the shareholder tag and index. nebula> CREATE TAG IF NOT EXISTS shareholder(); nebula> CREATE TAG INDEX IF NOT EXISTS shareholder_tag on shareholder(); //This example adds a tag on the vertex. nebula> INSERT VERTEX shareholder() VALUES \"player100\":(); nebula> INSERT VERTEX shareholder() VALUES \"player101\":(); //This example queries all the shareholders. nebula> MATCH (v:shareholder) RETURN v; +--------------------------------------------------------------------+ | v | +--------------------------------------------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"} :shareholder{}) | | (\"player101\" :player{age: 36, name: \"Tony Parker\"} :shareholder{}) | +--------------------------------------------------------------------+ nebula> LOOKUP ON shareholder YIELD id(vertex); +-------------+ | id(VERTEX) | +-------------+ | \"player100\" | | \"player101\" | +-------------+ //In this example, the \"player100\" is no longer a shareholder. nebula> DELETE TAG shareholder FROM \"player100\"; nebula> LOOKUP ON shareholder YIELD id(vertex); +-------------+ | id(VERTEX) | +-------------+ | \"player101\" | +-------------+ Note If the index is created after inserting the test data, use the REBUILD TAG INDEX <index_name_list>; statement to rebuild the index.","title":"Examples"},{"location":"3.ngql-guide/11.edge-type-statements/1.create-edge/","text":"CREATE EDGE \u00b6 CREATE EDGE creates an edge type with the given name in a graph space. OpenCypher compatibility \u00b6 Edge types in nGQL are similar to relationship types in openCypher. But they are also quite different. For example, the ways to create them are different. In openCypher, relationship types are created together with vertices in CREATE statements. In nGQL, edge types are created separately using CREATE EDGE statements. Edge types in nGQL are more like tables in MySQL. Prerequisites \u00b6 Running the CREATE EDGE statement requires some privileges for the graph space. Otherwise, Nebula Graph throws an error. Syntax \u00b6 To create an edge type in a specific graph space, you must specify the current working space with the USE statement. CREATE EDGE [IF NOT EXISTS] <edge_type_name> ( <prop_name> <data_type> [NULL | NOT NULL] [DEFAULT <default_value>] [COMMENT '<comment>'] [{, <prop_name> <data_type> [NULL | NOT NULL] [DEFAULT <default_value>] [COMMENT '<comment>']} ...] ) [TTL_DURATION = <ttl_duration>] [TTL_COL = <prop_name>] [COMMENT = '<comment>']; Parameter Description IF NOT EXISTS Detects if the edge type that you want to create exists. If it does not exist, a new one will be created. The edge type existence detection here only compares the edge type names (excluding properties). <edge_type_name> The edge type name must be unique in a graph space. Once the edge type name is set, it can not be altered. The name of the edge type starts with a letter, supports 1 to 4 bytes UTF-8 encoded characters, such as English letters (case-sensitive), digits, and Chinese characters, but does not support special characters except underscores. To use special characters or reserved keywords as identifiers, quote them with backticks. For more information, see Keywords and reserved words . <prop_name> The name of the property. It must be unique for each edge type. The rules for permitted property names are the same as those for edge type names. <data_type> Shows the data type of each property. For a full description of the property data types, see Data types and Boolean . NULL \\| NOT NULL Specifies if the property supports NULL | NOT NULL . The default value is NULL . DEFAULT Specifies a default value for a property. The default value can be a literal value or an expression supported by Nebula Graph. If no value is specified, the default value is used when inserting a new edge. COMMENT The remarks of a certain property or the edge type itself. The maximum length is 256 bytes. By default, there will be no comments on an edge type. TTL_DURATION Specifies the life cycle for the property. The property that exceeds the specified TTL expires. The expiration threshold is the TTL_COL value plus the TTL_DURATION . The default value of TTL_DURATION is 0 . It means the data never expires. TTL_COL Specifies the property to set a timeout on. The data type of the property must be int or timestamp . An edge type can only specify one field as TTL_COL . For more information on TTL, see TTL options . Examples \u00b6 nebula> CREATE EDGE IF NOT EXISTS follow(degree int); # The following example creates an edge type with no properties. nebula> CREATE EDGE IF NOT EXISTS no_property(); # The following example creates an edge type with a default value. nebula> CREATE EDGE IF NOT EXISTS follow_with_default(degree int DEFAULT 20); # In the following example, the TTL of the p2 field is set to be 100 seconds. nebula> CREATE EDGE IF NOT EXISTS e1(p1 string, p2 int, p3 timestamp) \\ TTL_DURATION = 100, TTL_COL = \"p2\"; Implementation of the operation \u00b6 Trying to use a newly created edge type may fail because the creation of the edge type is implemented asynchronously. To make sure the follow-up operations work as expected, Wait for two heartbeat cycles, i.e., 20 seconds. To change the heartbeat interval, modify the heartbeat_interval_secs parameter in the configuration files for all services.","title":"CREATE EDGE"},{"location":"3.ngql-guide/11.edge-type-statements/1.create-edge/#create_edge","text":"CREATE EDGE creates an edge type with the given name in a graph space.","title":"CREATE EDGE"},{"location":"3.ngql-guide/11.edge-type-statements/1.create-edge/#opencypher_compatibility","text":"Edge types in nGQL are similar to relationship types in openCypher. But they are also quite different. For example, the ways to create them are different. In openCypher, relationship types are created together with vertices in CREATE statements. In nGQL, edge types are created separately using CREATE EDGE statements. Edge types in nGQL are more like tables in MySQL.","title":"OpenCypher compatibility"},{"location":"3.ngql-guide/11.edge-type-statements/1.create-edge/#prerequisites","text":"Running the CREATE EDGE statement requires some privileges for the graph space. Otherwise, Nebula Graph throws an error.","title":"Prerequisites"},{"location":"3.ngql-guide/11.edge-type-statements/1.create-edge/#syntax","text":"To create an edge type in a specific graph space, you must specify the current working space with the USE statement. CREATE EDGE [IF NOT EXISTS] <edge_type_name> ( <prop_name> <data_type> [NULL | NOT NULL] [DEFAULT <default_value>] [COMMENT '<comment>'] [{, <prop_name> <data_type> [NULL | NOT NULL] [DEFAULT <default_value>] [COMMENT '<comment>']} ...] ) [TTL_DURATION = <ttl_duration>] [TTL_COL = <prop_name>] [COMMENT = '<comment>']; Parameter Description IF NOT EXISTS Detects if the edge type that you want to create exists. If it does not exist, a new one will be created. The edge type existence detection here only compares the edge type names (excluding properties). <edge_type_name> The edge type name must be unique in a graph space. Once the edge type name is set, it can not be altered. The name of the edge type starts with a letter, supports 1 to 4 bytes UTF-8 encoded characters, such as English letters (case-sensitive), digits, and Chinese characters, but does not support special characters except underscores. To use special characters or reserved keywords as identifiers, quote them with backticks. For more information, see Keywords and reserved words . <prop_name> The name of the property. It must be unique for each edge type. The rules for permitted property names are the same as those for edge type names. <data_type> Shows the data type of each property. For a full description of the property data types, see Data types and Boolean . NULL \\| NOT NULL Specifies if the property supports NULL | NOT NULL . The default value is NULL . DEFAULT Specifies a default value for a property. The default value can be a literal value or an expression supported by Nebula Graph. If no value is specified, the default value is used when inserting a new edge. COMMENT The remarks of a certain property or the edge type itself. The maximum length is 256 bytes. By default, there will be no comments on an edge type. TTL_DURATION Specifies the life cycle for the property. The property that exceeds the specified TTL expires. The expiration threshold is the TTL_COL value plus the TTL_DURATION . The default value of TTL_DURATION is 0 . It means the data never expires. TTL_COL Specifies the property to set a timeout on. The data type of the property must be int or timestamp . An edge type can only specify one field as TTL_COL . For more information on TTL, see TTL options .","title":"Syntax"},{"location":"3.ngql-guide/11.edge-type-statements/1.create-edge/#examples","text":"nebula> CREATE EDGE IF NOT EXISTS follow(degree int); # The following example creates an edge type with no properties. nebula> CREATE EDGE IF NOT EXISTS no_property(); # The following example creates an edge type with a default value. nebula> CREATE EDGE IF NOT EXISTS follow_with_default(degree int DEFAULT 20); # In the following example, the TTL of the p2 field is set to be 100 seconds. nebula> CREATE EDGE IF NOT EXISTS e1(p1 string, p2 int, p3 timestamp) \\ TTL_DURATION = 100, TTL_COL = \"p2\";","title":"Examples"},{"location":"3.ngql-guide/11.edge-type-statements/1.create-edge/#implementation_of_the_operation","text":"Trying to use a newly created edge type may fail because the creation of the edge type is implemented asynchronously. To make sure the follow-up operations work as expected, Wait for two heartbeat cycles, i.e., 20 seconds. To change the heartbeat interval, modify the heartbeat_interval_secs parameter in the configuration files for all services.","title":"Implementation of the operation"},{"location":"3.ngql-guide/11.edge-type-statements/2.drop-edge/","text":"DROP EDGE \u00b6 DROP EDGE drops an edge type with the given name in a graph space. An edge can have only one edge type. After you drop it, the edge CANNOT be accessed. The edge will be deleted in the next compaction. This operation only deletes the Schema data. All the files or directories in the disk will not be deleted directly until the next compaction. Prerequisites \u00b6 Running the DROP EDGE statement requires some privileges for the graph space. Otherwise, Nebula Graph throws an error. Before you drop an edge type, make sure that the edge type does not have any indexes. Otherwise, the conflict error ( [ERROR (-8)]: Conflict! ) will be returned. To drop an index, see DROP INDEX . Syntax \u00b6 DROP EDGE [IF EXISTS] <edge_type_name> Edge type name \u00b6 IF NOT EXISTS : Detects if the edge type that you want to drop exists. Only when it exists will it be dropped. edge_type_name : Specifies the edge type name that you want to drop. You can drop only one edge type in one statement. Example \u00b6 nebula> CREATE EDGE IF NOT EXISTS e1(p1 string, p2 int); nebula> DROP EDGE e1;","title":"DROP EDGE"},{"location":"3.ngql-guide/11.edge-type-statements/2.drop-edge/#drop_edge","text":"DROP EDGE drops an edge type with the given name in a graph space. An edge can have only one edge type. After you drop it, the edge CANNOT be accessed. The edge will be deleted in the next compaction. This operation only deletes the Schema data. All the files or directories in the disk will not be deleted directly until the next compaction.","title":"DROP EDGE"},{"location":"3.ngql-guide/11.edge-type-statements/2.drop-edge/#prerequisites","text":"Running the DROP EDGE statement requires some privileges for the graph space. Otherwise, Nebula Graph throws an error. Before you drop an edge type, make sure that the edge type does not have any indexes. Otherwise, the conflict error ( [ERROR (-8)]: Conflict! ) will be returned. To drop an index, see DROP INDEX .","title":"Prerequisites"},{"location":"3.ngql-guide/11.edge-type-statements/2.drop-edge/#syntax","text":"DROP EDGE [IF EXISTS] <edge_type_name>","title":"Syntax"},{"location":"3.ngql-guide/11.edge-type-statements/2.drop-edge/#edge_type_name","text":"IF NOT EXISTS : Detects if the edge type that you want to drop exists. Only when it exists will it be dropped. edge_type_name : Specifies the edge type name that you want to drop. You can drop only one edge type in one statement.","title":"Edge type name"},{"location":"3.ngql-guide/11.edge-type-statements/2.drop-edge/#example","text":"nebula> CREATE EDGE IF NOT EXISTS e1(p1 string, p2 int); nebula> DROP EDGE e1;","title":"Example"},{"location":"3.ngql-guide/11.edge-type-statements/3.alter-edge/","text":"ALTER EDGE \u00b6 ALTER EDGE alters the structure of an edge type with the given name in a graph space. You can add or drop properties, and change the data type of an existing property. You can also set a TTL (Time-To-Live) on a property, or change its TTL duration. Prerequisites \u00b6 Running the ALTER EDGE statement requires some privileges for the graph space. Otherwise, Nebula Graph throws an error. Before you alter properties for an edge type, make sure that the properties are not indexed. If the properties contain any indexes, the conflict error [ERROR (-8)]: Conflict! will occur when you ALTER EDGE . For more information on dropping an index, see DROP INDEX . Syntax \u00b6 ALTER EDGE <edge_type_name> <alter_definition> [, alter_definition] ...] [ttl_definition [, ttl_definition] ... ] [COMMENT = '<comment>']; alter_definition: | ADD (prop_name data_type) | DROP (prop_name) | CHANGE (prop_name data_type) ttl_definition: TTL_DURATION = ttl_duration, TTL_COL = prop_name edge_type_name : Specifies the edge type name that you want to alter. You can alter only one edge type in one statement. Before you alter an edge type, make sure that the edge type exists in the graph space. If the edge type does not exist, an error occurs when you alter it. Multiple ADD , DROP , and CHANGE clauses are permitted in a single ALTER EDGE statement, separated by commas. Example \u00b6 nebula> CREATE EDGE IF NOT EXISTS e1(p1 string, p2 int); nebula> ALTER EDGE e1 ADD (p3 int, p4 string); nebula> ALTER EDGE e1 TTL_DURATION = 2, TTL_COL = \"p2\"; nebula> ALTER EDGE e1 COMMENT = 'edge1'; Implementation of the operation \u00b6 Trying to use a newly altered edge type may fail because the alteration of the edge type is implemented asynchronously. To make sure the follow-up operations work as expected, Wait for two heartbeat cycles, i.e., 20 seconds. To change the heartbeat interval, modify the heartbeat_interval_secs parameter in the configuration files for all services.","title":"ALTER EDGE"},{"location":"3.ngql-guide/11.edge-type-statements/3.alter-edge/#alter_edge","text":"ALTER EDGE alters the structure of an edge type with the given name in a graph space. You can add or drop properties, and change the data type of an existing property. You can also set a TTL (Time-To-Live) on a property, or change its TTL duration.","title":"ALTER EDGE"},{"location":"3.ngql-guide/11.edge-type-statements/3.alter-edge/#prerequisites","text":"Running the ALTER EDGE statement requires some privileges for the graph space. Otherwise, Nebula Graph throws an error. Before you alter properties for an edge type, make sure that the properties are not indexed. If the properties contain any indexes, the conflict error [ERROR (-8)]: Conflict! will occur when you ALTER EDGE . For more information on dropping an index, see DROP INDEX .","title":"Prerequisites"},{"location":"3.ngql-guide/11.edge-type-statements/3.alter-edge/#syntax","text":"ALTER EDGE <edge_type_name> <alter_definition> [, alter_definition] ...] [ttl_definition [, ttl_definition] ... ] [COMMENT = '<comment>']; alter_definition: | ADD (prop_name data_type) | DROP (prop_name) | CHANGE (prop_name data_type) ttl_definition: TTL_DURATION = ttl_duration, TTL_COL = prop_name edge_type_name : Specifies the edge type name that you want to alter. You can alter only one edge type in one statement. Before you alter an edge type, make sure that the edge type exists in the graph space. If the edge type does not exist, an error occurs when you alter it. Multiple ADD , DROP , and CHANGE clauses are permitted in a single ALTER EDGE statement, separated by commas.","title":"Syntax"},{"location":"3.ngql-guide/11.edge-type-statements/3.alter-edge/#example","text":"nebula> CREATE EDGE IF NOT EXISTS e1(p1 string, p2 int); nebula> ALTER EDGE e1 ADD (p3 int, p4 string); nebula> ALTER EDGE e1 TTL_DURATION = 2, TTL_COL = \"p2\"; nebula> ALTER EDGE e1 COMMENT = 'edge1';","title":"Example"},{"location":"3.ngql-guide/11.edge-type-statements/3.alter-edge/#implementation_of_the_operation","text":"Trying to use a newly altered edge type may fail because the alteration of the edge type is implemented asynchronously. To make sure the follow-up operations work as expected, Wait for two heartbeat cycles, i.e., 20 seconds. To change the heartbeat interval, modify the heartbeat_interval_secs parameter in the configuration files for all services.","title":"Implementation of the operation"},{"location":"3.ngql-guide/11.edge-type-statements/4.show-edges/","text":"SHOW EDGES \u00b6 SHOW EDGES shows all edge types in the current graph space. You do not need any privileges for the graph space to run the SHOW EDGES statement. But the returned results are different based on role privileges . Syntax \u00b6 SHOW EDGES; Example \u00b6 nebula> SHOW EDGES; +----------+ | Name | +----------+ | \"follow\" | | \"serve\" | +----------+","title":"SHOW EDGES"},{"location":"3.ngql-guide/11.edge-type-statements/4.show-edges/#show_edges","text":"SHOW EDGES shows all edge types in the current graph space. You do not need any privileges for the graph space to run the SHOW EDGES statement. But the returned results are different based on role privileges .","title":"SHOW EDGES"},{"location":"3.ngql-guide/11.edge-type-statements/4.show-edges/#syntax","text":"SHOW EDGES;","title":"Syntax"},{"location":"3.ngql-guide/11.edge-type-statements/4.show-edges/#example","text":"nebula> SHOW EDGES; +----------+ | Name | +----------+ | \"follow\" | | \"serve\" | +----------+","title":"Example"},{"location":"3.ngql-guide/11.edge-type-statements/5.describe-edge/","text":"DESCRIBE EDGE \u00b6 DESCRIBE EDGE returns the information about an edge type with the given name in a graph space, such as field names, data type, and so on. Prerequisites \u00b6 Running the DESCRIBE EDGE statement requires some privileges for the graph space. Otherwise, Nebula Graph throws an error. Syntax \u00b6 DESC[RIBE] EDGE <edge_type_name> You can use DESC instead of DESCRIBE for short. Example \u00b6 nebula> DESCRIBE EDGE follow; +----------+---------+-------+---------+---------+ | Field | Type | Null | Default | Comment | +----------+---------+-------+---------+---------+ | \"degree\" | \"int64\" | \"YES\" | | | +----------+---------+-------+---------+---------+","title":"DESCRIBE EDGE"},{"location":"3.ngql-guide/11.edge-type-statements/5.describe-edge/#describe_edge","text":"DESCRIBE EDGE returns the information about an edge type with the given name in a graph space, such as field names, data type, and so on.","title":"DESCRIBE EDGE"},{"location":"3.ngql-guide/11.edge-type-statements/5.describe-edge/#prerequisites","text":"Running the DESCRIBE EDGE statement requires some privileges for the graph space. Otherwise, Nebula Graph throws an error.","title":"Prerequisites"},{"location":"3.ngql-guide/11.edge-type-statements/5.describe-edge/#syntax","text":"DESC[RIBE] EDGE <edge_type_name> You can use DESC instead of DESCRIBE for short.","title":"Syntax"},{"location":"3.ngql-guide/11.edge-type-statements/5.describe-edge/#example","text":"nebula> DESCRIBE EDGE follow; +----------+---------+-------+---------+---------+ | Field | Type | Null | Default | Comment | +----------+---------+-------+---------+---------+ | \"degree\" | \"int64\" | \"YES\" | | | +----------+---------+-------+---------+---------+","title":"Example"},{"location":"3.ngql-guide/12.vertex-statements/1.insert-vertex/","text":"INSERT VERTEX \u00b6 The INSERT VERTEX statement inserts one or more vertices into a graph space in Nebula Graph. Prerequisites \u00b6 Running the INSERT VERTEX statement requires some privileges for the graph space. Otherwise, Nebula Graph throws an error. Syntax \u00b6 INSERT VERTEX [IF NOT EXISTS] [tag_props, [tag_props] ...] VALUES VID: ([prop_value_list]) tag_props: tag_name ([prop_name_list]) prop_name_list: [prop_name [, prop_name] ...] prop_value_list: [prop_value [, prop_value] ...] IF NOT EXISTS detects if the VID that you want to insert exists. If it does not exist, a new one will be inserted. Note IF NOT EXISTS only compares the names of the VID and the tag (excluding properties). IF NOT EXISTS will read to check whether the data exists, which will have a significant impact on performance. tag_name denotes the tag (vertex type), which must be created before INSERT VERTEX . For more information, see CREATE TAG . Caution Nebula Graph 3.1.0 supports inserting vertices without tags. prop_name_list contains the names of the properties on the tag. VID is the vertex ID. In Nebula Graph 2.0, string and integer VID types are supported. The VID type is set when a graph space is created. For more information, see CREATE SPACE . prop_value_list must provide the property values according to the prop_name_list . When the NOT NULL constraint is set for a given property, an error is returned if no property is given. When the default value for a property is NULL , you can omit to specify the property value. For details, see CREATE TAG . Caution INSERT VERTEX and CREATE have different semantics. The semantics of INSERT VERTEX is closer to that of INSERT in NoSQL (key-value), or UPSERT ( UPDATE or INSERT ) in SQL. When two INSERT statements (neither uses IF NOT EXISTS ) with the same VID and TAG are operated at the same time, the latter INSERT will overwrite the former. When two INSERT statements with the same VID but different TAGS are operated at the same time, the operation of different tags will not overwrite each other. Examples are as follows. Examples \u00b6 # Insert a vertex without tag. nebula> INSERT VERTEX VALUES \"1\":(); # The following examples create tag t1 with no property and inserts vertex \"10\" with no property. nebula> CREATE TAG IF NOT EXISTS t1(); nebula> INSERT VERTEX t1() VALUES \"10\":(); nebula> CREATE TAG IF NOT EXISTS t2 (name string, age int); nebula> INSERT VERTEX t2 (name, age) VALUES \"11\":(\"n1\", 12); # In the following example, the insertion fails because \"a13\" is not int. nebula> INSERT VERTEX t2 (name, age) VALUES \"12\":(\"n1\", \"a13\"); # The following example inserts two vertices at one time. nebula> INSERT VERTEX t2 (name, age) VALUES \"13\":(\"n3\", 12), \"14\":(\"n4\", 8); nebula> CREATE TAG IF NOT EXISTS t3(p1 int); nebula> CREATE TAG IF NOT EXISTS t4(p2 string); # The following example inserts vertex \"21\" with two tags. nebula> INSERT VERTEX t3 (p1), t4(p2) VALUES \"21\": (321, \"hello\"); A vertex can be inserted/written with new values multiple times. Only the last written values can be read. # The following examples insert vertex \"11\" with new values for multiple times. nebula> INSERT VERTEX t2 (name, age) VALUES \"11\":(\"n2\", 13); nebula> INSERT VERTEX t2 (name, age) VALUES \"11\":(\"n3\", 14); nebula> INSERT VERTEX t2 (name, age) VALUES \"11\":(\"n4\", 15); nebula> FETCH PROP ON t2 \"11\" YIELD properties(vertex); +-----------------------+ | properties(VERTEX) | +-----------------------+ | {age: 15, name: \"n4\"} | +-----------------------+ nebula> CREATE TAG IF NOT EXISTS t5(p1 fixed_string(5) NOT NULL, p2 int, p3 int DEFAULT NULL); nebula> INSERT VERTEX t5(p1, p2, p3) VALUES \"001\":(\"Abe\", 2, 3); # In the following example, the insertion fails because the value of p1 cannot be NULL. nebula> INSERT VERTEX t5(p1, p2, p3) VALUES \"002\":(NULL, 4, 5); [ERROR (-1005)]: Storage Error: The not null field cannot be null. # In the following example, the value of p3 is the default NULL. nebula> INSERT VERTEX t5(p1, p2) VALUES \"003\":(\"cd\", 5); nebula> FETCH PROP ON t5 \"003\" YIELD properties(vertex); +---------------------------------+ | properties(VERTEX) | +---------------------------------+ | {p1: \"cd\", p2: 5, p3: __NULL__} | +---------------------------------+ # In the following example, the allowed maximum length of p1 is 5. nebula> INSERT VERTEX t5(p1, p2) VALUES \"004\":(\"shalalalala\", 4); nebula> FETCH PROP on t5 \"004\" YIELD properties(vertex); +------------------------------------+ | properties(VERTEX) | +------------------------------------+ | {p1: \"shala\", p2: 4, p3: __NULL__} | +------------------------------------+ If you insert a vertex that already exists with IF NOT EXISTS , there will be no modification. # The following example inserts vertex \"1\". nebula> INSERT VERTEX t2 (name, age) VALUES \"1\":(\"n2\", 13); # Modify vertex \"1\" with IF NOT EXISTS. But there will be no modification as vertex \"1\" already exists. nebula> INSERT VERTEX IF NOT EXISTS t2 (name, age) VALUES \"1\":(\"n3\", 14); nebula> FETCH PROP ON t2 \"1\" YIELD properties(vertex); +-----------------------+ | properties(VERTEX) | +-----------------------+ | {age: 13, name: \"n2\"} | +-----------------------+","title":"INSERT VERTEX"},{"location":"3.ngql-guide/12.vertex-statements/1.insert-vertex/#insert_vertex","text":"The INSERT VERTEX statement inserts one or more vertices into a graph space in Nebula Graph.","title":"INSERT VERTEX"},{"location":"3.ngql-guide/12.vertex-statements/1.insert-vertex/#prerequisites","text":"Running the INSERT VERTEX statement requires some privileges for the graph space. Otherwise, Nebula Graph throws an error.","title":"Prerequisites"},{"location":"3.ngql-guide/12.vertex-statements/1.insert-vertex/#syntax","text":"INSERT VERTEX [IF NOT EXISTS] [tag_props, [tag_props] ...] VALUES VID: ([prop_value_list]) tag_props: tag_name ([prop_name_list]) prop_name_list: [prop_name [, prop_name] ...] prop_value_list: [prop_value [, prop_value] ...] IF NOT EXISTS detects if the VID that you want to insert exists. If it does not exist, a new one will be inserted. Note IF NOT EXISTS only compares the names of the VID and the tag (excluding properties). IF NOT EXISTS will read to check whether the data exists, which will have a significant impact on performance. tag_name denotes the tag (vertex type), which must be created before INSERT VERTEX . For more information, see CREATE TAG . Caution Nebula Graph 3.1.0 supports inserting vertices without tags. prop_name_list contains the names of the properties on the tag. VID is the vertex ID. In Nebula Graph 2.0, string and integer VID types are supported. The VID type is set when a graph space is created. For more information, see CREATE SPACE . prop_value_list must provide the property values according to the prop_name_list . When the NOT NULL constraint is set for a given property, an error is returned if no property is given. When the default value for a property is NULL , you can omit to specify the property value. For details, see CREATE TAG . Caution INSERT VERTEX and CREATE have different semantics. The semantics of INSERT VERTEX is closer to that of INSERT in NoSQL (key-value), or UPSERT ( UPDATE or INSERT ) in SQL. When two INSERT statements (neither uses IF NOT EXISTS ) with the same VID and TAG are operated at the same time, the latter INSERT will overwrite the former. When two INSERT statements with the same VID but different TAGS are operated at the same time, the operation of different tags will not overwrite each other. Examples are as follows.","title":"Syntax"},{"location":"3.ngql-guide/12.vertex-statements/1.insert-vertex/#examples","text":"# Insert a vertex without tag. nebula> INSERT VERTEX VALUES \"1\":(); # The following examples create tag t1 with no property and inserts vertex \"10\" with no property. nebula> CREATE TAG IF NOT EXISTS t1(); nebula> INSERT VERTEX t1() VALUES \"10\":(); nebula> CREATE TAG IF NOT EXISTS t2 (name string, age int); nebula> INSERT VERTEX t2 (name, age) VALUES \"11\":(\"n1\", 12); # In the following example, the insertion fails because \"a13\" is not int. nebula> INSERT VERTEX t2 (name, age) VALUES \"12\":(\"n1\", \"a13\"); # The following example inserts two vertices at one time. nebula> INSERT VERTEX t2 (name, age) VALUES \"13\":(\"n3\", 12), \"14\":(\"n4\", 8); nebula> CREATE TAG IF NOT EXISTS t3(p1 int); nebula> CREATE TAG IF NOT EXISTS t4(p2 string); # The following example inserts vertex \"21\" with two tags. nebula> INSERT VERTEX t3 (p1), t4(p2) VALUES \"21\": (321, \"hello\"); A vertex can be inserted/written with new values multiple times. Only the last written values can be read. # The following examples insert vertex \"11\" with new values for multiple times. nebula> INSERT VERTEX t2 (name, age) VALUES \"11\":(\"n2\", 13); nebula> INSERT VERTEX t2 (name, age) VALUES \"11\":(\"n3\", 14); nebula> INSERT VERTEX t2 (name, age) VALUES \"11\":(\"n4\", 15); nebula> FETCH PROP ON t2 \"11\" YIELD properties(vertex); +-----------------------+ | properties(VERTEX) | +-----------------------+ | {age: 15, name: \"n4\"} | +-----------------------+ nebula> CREATE TAG IF NOT EXISTS t5(p1 fixed_string(5) NOT NULL, p2 int, p3 int DEFAULT NULL); nebula> INSERT VERTEX t5(p1, p2, p3) VALUES \"001\":(\"Abe\", 2, 3); # In the following example, the insertion fails because the value of p1 cannot be NULL. nebula> INSERT VERTEX t5(p1, p2, p3) VALUES \"002\":(NULL, 4, 5); [ERROR (-1005)]: Storage Error: The not null field cannot be null. # In the following example, the value of p3 is the default NULL. nebula> INSERT VERTEX t5(p1, p2) VALUES \"003\":(\"cd\", 5); nebula> FETCH PROP ON t5 \"003\" YIELD properties(vertex); +---------------------------------+ | properties(VERTEX) | +---------------------------------+ | {p1: \"cd\", p2: 5, p3: __NULL__} | +---------------------------------+ # In the following example, the allowed maximum length of p1 is 5. nebula> INSERT VERTEX t5(p1, p2) VALUES \"004\":(\"shalalalala\", 4); nebula> FETCH PROP on t5 \"004\" YIELD properties(vertex); +------------------------------------+ | properties(VERTEX) | +------------------------------------+ | {p1: \"shala\", p2: 4, p3: __NULL__} | +------------------------------------+ If you insert a vertex that already exists with IF NOT EXISTS , there will be no modification. # The following example inserts vertex \"1\". nebula> INSERT VERTEX t2 (name, age) VALUES \"1\":(\"n2\", 13); # Modify vertex \"1\" with IF NOT EXISTS. But there will be no modification as vertex \"1\" already exists. nebula> INSERT VERTEX IF NOT EXISTS t2 (name, age) VALUES \"1\":(\"n3\", 14); nebula> FETCH PROP ON t2 \"1\" YIELD properties(vertex); +-----------------------+ | properties(VERTEX) | +-----------------------+ | {age: 13, name: \"n2\"} | +-----------------------+","title":"Examples"},{"location":"3.ngql-guide/12.vertex-statements/2.update-vertex/","text":"UPDATE VERTEX \u00b6 The UPDATE VERTEX statement updates properties on tags of a vertex. In Nebula Graph, UPDATE VERTEX supports compare-and-set (CAS). Note An UPDATE VERTEX statement can only update properties on ONE TAG of a vertex. Syntax \u00b6 UPDATE VERTEX ON <tag_name> <vid> SET <update_prop> [WHEN <condition>] [YIELD <output>] Parameter Required Description Example ON <tag_name> Yes Specifies the tag of the vertex. The properties to be updated must be on this tag. ON player <vid> Yes Specifies the ID of the vertex to be updated. \"player100\" SET <update_prop> Yes Specifies the properties to be updated and how they will be updated. SET age = age +1 WHEN <condition> No Specifies the filter conditions. If <condition> evaluates to false , the SET clause will not take effect. WHEN name == \"Tim\" YIELD <output> No Specifies the output format of the statement. YIELD name AS Name Example \u00b6 // This query checks the properties of vertex \"player101\". nebula> FETCH PROP ON player \"player101\" YIELD properties(vertex); +--------------------------------+ | properties(VERTEX) | +--------------------------------+ | {age: 36, name: \"Tony Parker\"} | +--------------------------------+ // This query updates the age property and returns name and the new age. nebula> UPDATE VERTEX ON player \"player101\" \\ SET age = age + 2 \\ WHEN name == \"Tony Parker\" \\ YIELD name AS Name, age AS Age; +---------------+-----+ | Name | Age | +---------------+-----+ | \"Tony Parker\" | 38 | +---------------+-----+","title":"UPDATE VERTEX"},{"location":"3.ngql-guide/12.vertex-statements/2.update-vertex/#update_vertex","text":"The UPDATE VERTEX statement updates properties on tags of a vertex. In Nebula Graph, UPDATE VERTEX supports compare-and-set (CAS). Note An UPDATE VERTEX statement can only update properties on ONE TAG of a vertex.","title":"UPDATE VERTEX"},{"location":"3.ngql-guide/12.vertex-statements/2.update-vertex/#syntax","text":"UPDATE VERTEX ON <tag_name> <vid> SET <update_prop> [WHEN <condition>] [YIELD <output>] Parameter Required Description Example ON <tag_name> Yes Specifies the tag of the vertex. The properties to be updated must be on this tag. ON player <vid> Yes Specifies the ID of the vertex to be updated. \"player100\" SET <update_prop> Yes Specifies the properties to be updated and how they will be updated. SET age = age +1 WHEN <condition> No Specifies the filter conditions. If <condition> evaluates to false , the SET clause will not take effect. WHEN name == \"Tim\" YIELD <output> No Specifies the output format of the statement. YIELD name AS Name","title":"Syntax"},{"location":"3.ngql-guide/12.vertex-statements/2.update-vertex/#example","text":"// This query checks the properties of vertex \"player101\". nebula> FETCH PROP ON player \"player101\" YIELD properties(vertex); +--------------------------------+ | properties(VERTEX) | +--------------------------------+ | {age: 36, name: \"Tony Parker\"} | +--------------------------------+ // This query updates the age property and returns name and the new age. nebula> UPDATE VERTEX ON player \"player101\" \\ SET age = age + 2 \\ WHEN name == \"Tony Parker\" \\ YIELD name AS Name, age AS Age; +---------------+-----+ | Name | Age | +---------------+-----+ | \"Tony Parker\" | 38 | +---------------+-----+","title":"Example"},{"location":"3.ngql-guide/12.vertex-statements/3.upsert-vertex/","text":"UPSERT VERTEX \u00b6 The UPSERT statement is a combination of UPDATE and INSERT . You can use UPSERT VERTEX to update the properties of a vertex if it exists or insert a new vertex if it does not exist. Note An UPSERT VERTEX statement can only update the properties on ONE TAG of a vertex. The performance of UPSERT is much lower than that of INSERT because UPSERT is a read-modify-write serialization operation at the partition level. Danger Don't use UPSERT for scenarios with highly concurrent writes. You can use UPDATE or INSERT instead. Syntax \u00b6 UPSERT VERTEX ON <tag> <vid> SET <update_prop> [WHEN <condition>] [YIELD <output>] Parameter Required Description Example ON <tag> Yes Specifies the tag of the vertex. The properties to be updated must be on this tag. ON player <vid> Yes Specifies the ID of the vertex to be updated or inserted. \"player100\" SET <update_prop> Yes Specifies the properties to be updated and how they will be updated. SET age = age +1 WHEN <condition> No Specifies the filter conditions. WHEN name == \"Tim\" YIELD <output> No Specifies the output format of the statement. YIELD name AS Name Insert a vertex if it does not exist \u00b6 If a vertex does not exist, it is created no matter the conditions in the WHEN clause are met or not, and the SET clause always takes effect. The property values of the new vertex depend on: How the SET clause is defined. Whether the property has a default value. For example, if: The vertex to be inserted will have properties name and age based on the tag player . The SET clause specifies that age = 30 . Then the property values in different cases are listed as follows: Are WHEN conditions met If properties have default values Value of name Value of age Yes Yes The default value 30 Yes No NULL 30 No Yes The default value 30 No No NULL 30 Here are some examples: // This query checks if the following three vertices exist. The result \"Empty set\" indicates that the vertices do not exist. nebula> FETCH PROP ON * \"player666\", \"player667\", \"player668\" YIELD properties(vertex); +--------------------+ | properties(VERTEX) | +--------------------+ +--------------------+ Empty set nebula> UPSERT VERTEX ON player \"player666\" \\ SET age = 30 \\ WHEN name == \"Joe\" \\ YIELD name AS Name, age AS Age; +----------+----------+ | Name | Age | +----------+----------+ | __NULL__ | 30 | +----------+----------+ nebula> UPSERT VERTEX ON player \"player666\" \\ SET age = 31 \\ WHEN name == \"Joe\" \\ YIELD name AS Name, age AS Age; +----------+-----+ | Name | Age | +----------+-----+ | __NULL__ | 30 | +----------+-----+ nebula> UPSERT VERTEX ON player \"player667\" \\ SET age = 31 \\ YIELD name AS Name, age AS Age; +----------+-----+ | Name | Age | +----------+-----+ | __NULL__ | 31 | +----------+-----+ nebula> UPSERT VERTEX ON player \"player668\" \\ SET name = \"Amber\", age = age + 1 \\ YIELD name AS Name, age AS Age; +---------+----------+ | Name | Age | +---------+----------+ | \"Amber\" | __NULL__ | +---------+----------+ In the last query of the preceding examples, since age has no default value, when the vertex is created, age is NULL , and age = age + 1 does not take effect. But if age has a default value, age = age + 1 will take effect. For example: nebula> CREATE TAG IF NOT EXISTS player_with_default(name string, age int DEFAULT 20); Execution succeeded nebula> UPSERT VERTEX ON player_with_default \"player101\" \\ SET age = age + 1 \\ YIELD name AS Name, age AS Age; +----------+-----+ | Name | Age | +----------+-----+ | __NULL__ | 21 | +----------+-----+ Update a vertex if it exists \u00b6 If the vertex exists and the WHEN conditions are met, the vertex is updated. nebula> FETCH PROP ON player \"player101\" YIELD properties(vertex); +--------------------------------+ | properties(VERTEX) | +--------------------------------+ | {age: 36, name: \"Tony Parker\"} | +--------------------------------+ nebula> UPSERT VERTEX ON player \"player101\" \\ SET age = age + 2 \\ WHEN name == \"Tony Parker\" \\ YIELD name AS Name, age AS Age; +---------------+-----+ | Name | Age | +---------------+-----+ | \"Tony Parker\" | 38 | +---------------+-----+ If the vertex exists and the WHEN conditions are not met, the update does not take effect. nebula> FETCH PROP ON player \"player101\" YIELD properties(vertex); +--------------------------------+ | properties(VERTEX) | +--------------------------------+ | {age: 38, name: \"Tony Parker\"} | +--------------------------------+ nebula> UPSERT VERTEX ON player \"player101\" \\ SET age = age + 2 \\ WHEN name == \"Someone else\" \\ YIELD name AS Name, age AS Age; +---------------+-----+ | Name | Age | +---------------+-----+ | \"Tony Parker\" | 38 | +---------------+-----+","title":"UPSERT VERTEX"},{"location":"3.ngql-guide/12.vertex-statements/3.upsert-vertex/#upsert_vertex","text":"The UPSERT statement is a combination of UPDATE and INSERT . You can use UPSERT VERTEX to update the properties of a vertex if it exists or insert a new vertex if it does not exist. Note An UPSERT VERTEX statement can only update the properties on ONE TAG of a vertex. The performance of UPSERT is much lower than that of INSERT because UPSERT is a read-modify-write serialization operation at the partition level. Danger Don't use UPSERT for scenarios with highly concurrent writes. You can use UPDATE or INSERT instead.","title":"UPSERT VERTEX"},{"location":"3.ngql-guide/12.vertex-statements/3.upsert-vertex/#syntax","text":"UPSERT VERTEX ON <tag> <vid> SET <update_prop> [WHEN <condition>] [YIELD <output>] Parameter Required Description Example ON <tag> Yes Specifies the tag of the vertex. The properties to be updated must be on this tag. ON player <vid> Yes Specifies the ID of the vertex to be updated or inserted. \"player100\" SET <update_prop> Yes Specifies the properties to be updated and how they will be updated. SET age = age +1 WHEN <condition> No Specifies the filter conditions. WHEN name == \"Tim\" YIELD <output> No Specifies the output format of the statement. YIELD name AS Name","title":"Syntax"},{"location":"3.ngql-guide/12.vertex-statements/3.upsert-vertex/#insert_a_vertex_if_it_does_not_exist","text":"If a vertex does not exist, it is created no matter the conditions in the WHEN clause are met or not, and the SET clause always takes effect. The property values of the new vertex depend on: How the SET clause is defined. Whether the property has a default value. For example, if: The vertex to be inserted will have properties name and age based on the tag player . The SET clause specifies that age = 30 . Then the property values in different cases are listed as follows: Are WHEN conditions met If properties have default values Value of name Value of age Yes Yes The default value 30 Yes No NULL 30 No Yes The default value 30 No No NULL 30 Here are some examples: // This query checks if the following three vertices exist. The result \"Empty set\" indicates that the vertices do not exist. nebula> FETCH PROP ON * \"player666\", \"player667\", \"player668\" YIELD properties(vertex); +--------------------+ | properties(VERTEX) | +--------------------+ +--------------------+ Empty set nebula> UPSERT VERTEX ON player \"player666\" \\ SET age = 30 \\ WHEN name == \"Joe\" \\ YIELD name AS Name, age AS Age; +----------+----------+ | Name | Age | +----------+----------+ | __NULL__ | 30 | +----------+----------+ nebula> UPSERT VERTEX ON player \"player666\" \\ SET age = 31 \\ WHEN name == \"Joe\" \\ YIELD name AS Name, age AS Age; +----------+-----+ | Name | Age | +----------+-----+ | __NULL__ | 30 | +----------+-----+ nebula> UPSERT VERTEX ON player \"player667\" \\ SET age = 31 \\ YIELD name AS Name, age AS Age; +----------+-----+ | Name | Age | +----------+-----+ | __NULL__ | 31 | +----------+-----+ nebula> UPSERT VERTEX ON player \"player668\" \\ SET name = \"Amber\", age = age + 1 \\ YIELD name AS Name, age AS Age; +---------+----------+ | Name | Age | +---------+----------+ | \"Amber\" | __NULL__ | +---------+----------+ In the last query of the preceding examples, since age has no default value, when the vertex is created, age is NULL , and age = age + 1 does not take effect. But if age has a default value, age = age + 1 will take effect. For example: nebula> CREATE TAG IF NOT EXISTS player_with_default(name string, age int DEFAULT 20); Execution succeeded nebula> UPSERT VERTEX ON player_with_default \"player101\" \\ SET age = age + 1 \\ YIELD name AS Name, age AS Age; +----------+-----+ | Name | Age | +----------+-----+ | __NULL__ | 21 | +----------+-----+","title":"Insert a vertex if it does not exist"},{"location":"3.ngql-guide/12.vertex-statements/3.upsert-vertex/#update_a_vertex_if_it_exists","text":"If the vertex exists and the WHEN conditions are met, the vertex is updated. nebula> FETCH PROP ON player \"player101\" YIELD properties(vertex); +--------------------------------+ | properties(VERTEX) | +--------------------------------+ | {age: 36, name: \"Tony Parker\"} | +--------------------------------+ nebula> UPSERT VERTEX ON player \"player101\" \\ SET age = age + 2 \\ WHEN name == \"Tony Parker\" \\ YIELD name AS Name, age AS Age; +---------------+-----+ | Name | Age | +---------------+-----+ | \"Tony Parker\" | 38 | +---------------+-----+ If the vertex exists and the WHEN conditions are not met, the update does not take effect. nebula> FETCH PROP ON player \"player101\" YIELD properties(vertex); +--------------------------------+ | properties(VERTEX) | +--------------------------------+ | {age: 38, name: \"Tony Parker\"} | +--------------------------------+ nebula> UPSERT VERTEX ON player \"player101\" \\ SET age = age + 2 \\ WHEN name == \"Someone else\" \\ YIELD name AS Name, age AS Age; +---------------+-----+ | Name | Age | +---------------+-----+ | \"Tony Parker\" | 38 | +---------------+-----+","title":"Update a vertex if it exists"},{"location":"3.ngql-guide/12.vertex-statements/4.delete-vertex/","text":"DELETE VERTEX \u00b6 By default, the DELETE VERTEX statement deletes vertices but the incoming and outgoing edges of the vertices. Compatibility Nebula Graph 2.x deletes vertices and their incoming and outgoing edges. Nebula Graph 3.1.0 only deletes the vertices, and does not delete the related outgoing and incoming edges of the vertices. At this time, there will be dangling edges by default. The DELETE VERTEX statement deletes one vertex or multiple vertices at a time. You can use DELETE VERTEX together with pipes. For more information about pipe, see Pipe operator . Note DELETE VERTEX deletes vertices directly. DELETE TAG deletes a tag with the given name on a specified vertex. Syntax \u00b6 DELETE VERTEX <vid> [, <vid> ...] [WITH EDGE]; WITH EDGE: deletes vertices and the related incoming and outgoing edges of the vertices. Examples \u00b6 This query deletes the vertex whose ID is \"team1\". # Delete the vertex whose VID is `team1` but the related incoming and outgoing edges are not deleted. nebula> DELETE VERTEX \"team1\"; # Delete the vertex whose VID is `team1` and the related incoming and outgoing edges. nebula> DELETE VERTEX \"team1\" WITH EDGE; This query shows that you can use DELETE VERTEX together with pipe to delete vertices. nebula> GO FROM \"player100\" OVER serve WHERE properties(edge).start_year == \"2021\" YIELD dst(edge) AS id | DELETE VERTEX $-.id; Process of deleting vertices \u00b6 Once Nebula Graph deletes the vertices, all edges (incoming and outgoing edges) of the target vertex will become dangling edges. When Nebula Graph deletes the vertices WITH EDGE , Nebula Graph traverses the incoming and outgoing edges related to the vertices and deletes them all. Then Nebula Graph deletes the vertices. Caution Atomic deletion is not supported during the entire process for now. Please retry when a failure occurs to avoid partial deletion, which will cause pendent edges. Deleting a supernode takes a lot of time. To avoid connection timeout before the deletion is complete, you can modify the parameter --storage_client_timeout_ms in nebula-graphd.conf to extend the timeout period.","title":"DELETE VERTEX"},{"location":"3.ngql-guide/12.vertex-statements/4.delete-vertex/#delete_vertex","text":"By default, the DELETE VERTEX statement deletes vertices but the incoming and outgoing edges of the vertices. Compatibility Nebula Graph 2.x deletes vertices and their incoming and outgoing edges. Nebula Graph 3.1.0 only deletes the vertices, and does not delete the related outgoing and incoming edges of the vertices. At this time, there will be dangling edges by default. The DELETE VERTEX statement deletes one vertex or multiple vertices at a time. You can use DELETE VERTEX together with pipes. For more information about pipe, see Pipe operator . Note DELETE VERTEX deletes vertices directly. DELETE TAG deletes a tag with the given name on a specified vertex.","title":"DELETE VERTEX"},{"location":"3.ngql-guide/12.vertex-statements/4.delete-vertex/#syntax","text":"DELETE VERTEX <vid> [, <vid> ...] [WITH EDGE]; WITH EDGE: deletes vertices and the related incoming and outgoing edges of the vertices.","title":"Syntax"},{"location":"3.ngql-guide/12.vertex-statements/4.delete-vertex/#examples","text":"This query deletes the vertex whose ID is \"team1\". # Delete the vertex whose VID is `team1` but the related incoming and outgoing edges are not deleted. nebula> DELETE VERTEX \"team1\"; # Delete the vertex whose VID is `team1` and the related incoming and outgoing edges. nebula> DELETE VERTEX \"team1\" WITH EDGE; This query shows that you can use DELETE VERTEX together with pipe to delete vertices. nebula> GO FROM \"player100\" OVER serve WHERE properties(edge).start_year == \"2021\" YIELD dst(edge) AS id | DELETE VERTEX $-.id;","title":"Examples"},{"location":"3.ngql-guide/12.vertex-statements/4.delete-vertex/#process_of_deleting_vertices","text":"Once Nebula Graph deletes the vertices, all edges (incoming and outgoing edges) of the target vertex will become dangling edges. When Nebula Graph deletes the vertices WITH EDGE , Nebula Graph traverses the incoming and outgoing edges related to the vertices and deletes them all. Then Nebula Graph deletes the vertices. Caution Atomic deletion is not supported during the entire process for now. Please retry when a failure occurs to avoid partial deletion, which will cause pendent edges. Deleting a supernode takes a lot of time. To avoid connection timeout before the deletion is complete, you can modify the parameter --storage_client_timeout_ms in nebula-graphd.conf to extend the timeout period.","title":"Process of deleting vertices"},{"location":"3.ngql-guide/13.edge-statements/1.insert-edge/","text":"INSERT EDGE \u00b6 The INSERT EDGE statement inserts an edge or multiple edges into a graph space from a source vertex (given by src_vid) to a destination vertex (given by dst_vid) with a specific rank in Nebula Graph. When inserting an edge that already exists, INSERT VERTEX overrides the edge. Syntax \u00b6 INSERT EDGE [IF NOT EXISTS] <edge_type> ( <prop_name_list> ) VALUES <src_vid> -> <dst_vid>[@<rank>] : ( <prop_value_list> ) [, <src_vid> -> <dst_vid>[@<rank>] : ( <prop_value_list> ), ...]; <prop_name_list> ::= [ <prop_name> [, <prop_name> ] ...] <prop_value_list> ::= [ <prop_value> [, <prop_value> ] ...] IF NOT EXISTS detects if the edge that you want to insert exists. If it does not exist, a new one will be inserted. Note IF NOT EXISTS only detects whether exist and does not detect whether the property values overlap. IF NOT EXISTS will read to check whether the data exists, which will have a significant impact on performance. <edge_type> denotes the edge type, which must be created before INSERT EDGE . Only one edge type can be specified in this statement. <prop_name_list> is the property name list in the given <edge_type> . src_vid is the VID of the source vertex. It specifies the start of an edge. dst_vid is the VID of the destination vertex. It specifies the end of an edge. rank is optional. It specifies the edge rank of the same edge type. If not specified, the default value is 0 . You can insert many edges with the same edge type, source vertex, and destination vertex by using different rank values. OpenCypher compatibility OpenCypher has no such concept as rank. <prop_value_list> must provide the value list according to <prop_name_list> . If the property values do not match the data type in the edge type, an error is returned. When the NOT NULL constraint is set for a given property, an error is returned if no property is given. When the default value for a property is NULL , you can omit to specify the property value. For details, see CREATE EDGE . Examples \u00b6 # The following example creates edge type e1 with no property and inserts an edge from vertex \"10\" to vertex \"11\" with no property. nebula> CREATE EDGE IF NOT EXISTS e1(); nebula> INSERT EDGE e1 () VALUES \"10\"->\"11\":(); # The following example inserts an edge from vertex \"10\" to vertex \"11\" with no property. The edge rank is 1. nebula> INSERT EDGE e1 () VALUES \"10\"->\"11\"@1:(); nebula> CREATE EDGE IF NOT EXISTS e2 (name string, age int); nebula> INSERT EDGE e2 (name, age) VALUES \"11\"->\"13\":(\"n1\", 1); # The following example creates edge type e2 with two properties. nebula> INSERT EDGE e2 (name, age) VALUES \\ \"12\"->\"13\":(\"n1\", 1), \"13\"->\"14\":(\"n2\", 2); # In the following example, the insertion fails because \"a13\" is not int. nebula> INSERT EDGE e2 (name, age) VALUES \"11\"->\"13\":(\"n1\", \"a13\"); An edge can be inserted/written with property values multiple times. Only the last written values can be read. The following examples insert edge e2 with the new values for multiple times. nebula> INSERT EDGE e2 (name, age) VALUES \"11\"->\"13\":(\"n1\", 12); nebula> INSERT EDGE e2 (name, age) VALUES \"11\"->\"13\":(\"n1\", 13); nebula> INSERT EDGE e2 (name, age) VALUES \"11\"->\"13\":(\"n1\", 14); nebula> FETCH PROP ON e2 \"11\"->\"13\" YIELD edge AS e; +-------------------------------------------+ | e | +-------------------------------------------+ | [:e2 \"11\"->\"13\" @0 {age: 14, name: \"n1\"}] | +-------------------------------------------+ If you insert an edge that already exists with IF NOT EXISTS , there will be no modification. # The following example inserts edge e2 from vertex \"14\" to vertex \"15\". nebula> INSERT EDGE e2 (name, age) VALUES \"14\"->\"15\"@1:(\"n1\", 12); # The following example alters the edge with IF NOT EXISTS. But there will be no alteration because edge e2 already exists. nebula> INSERT EDGE IF NOT EXISTS e2 (name, age) VALUES \"14\"->\"15\"@1:(\"n2\", 13); nebula> FETCH PROP ON e2 \"14\"->\"15\"@1 YIELD edge AS e; +-------------------------------------------+ | e | +-------------------------------------------+ | [:e2 \"14\"->\"15\" @1 {age: 12, name: \"n1\"}] | +-------------------------------------------+ Note Nebula Graph 3.1.0 allows dangling edges. Therefore, you can write the edge before the source vertex or the destination vertex exists. At this time, you can get the (not written) vertex VID through <edgetype>._src or <edgetype>._dst (which is not recommended). Atomic operation is not guaranteed during the entire process for now. If it fails, please try again. Otherwise, partial writing will occur. At this time, the behavior of reading the data is undefined. Concurrently writing the same edge will cause an edge conflict error, so please try again later. The inserting speed of an edge is about half that of a vertex. Because in the storaged process, the insertion of an edge involves two tasks, while the insertion of a vertex involves only one task.","title":"INSERT EDGE"},{"location":"3.ngql-guide/13.edge-statements/1.insert-edge/#insert_edge","text":"The INSERT EDGE statement inserts an edge or multiple edges into a graph space from a source vertex (given by src_vid) to a destination vertex (given by dst_vid) with a specific rank in Nebula Graph. When inserting an edge that already exists, INSERT VERTEX overrides the edge.","title":"INSERT EDGE"},{"location":"3.ngql-guide/13.edge-statements/1.insert-edge/#syntax","text":"INSERT EDGE [IF NOT EXISTS] <edge_type> ( <prop_name_list> ) VALUES <src_vid> -> <dst_vid>[@<rank>] : ( <prop_value_list> ) [, <src_vid> -> <dst_vid>[@<rank>] : ( <prop_value_list> ), ...]; <prop_name_list> ::= [ <prop_name> [, <prop_name> ] ...] <prop_value_list> ::= [ <prop_value> [, <prop_value> ] ...] IF NOT EXISTS detects if the edge that you want to insert exists. If it does not exist, a new one will be inserted. Note IF NOT EXISTS only detects whether exist and does not detect whether the property values overlap. IF NOT EXISTS will read to check whether the data exists, which will have a significant impact on performance. <edge_type> denotes the edge type, which must be created before INSERT EDGE . Only one edge type can be specified in this statement. <prop_name_list> is the property name list in the given <edge_type> . src_vid is the VID of the source vertex. It specifies the start of an edge. dst_vid is the VID of the destination vertex. It specifies the end of an edge. rank is optional. It specifies the edge rank of the same edge type. If not specified, the default value is 0 . You can insert many edges with the same edge type, source vertex, and destination vertex by using different rank values. OpenCypher compatibility OpenCypher has no such concept as rank. <prop_value_list> must provide the value list according to <prop_name_list> . If the property values do not match the data type in the edge type, an error is returned. When the NOT NULL constraint is set for a given property, an error is returned if no property is given. When the default value for a property is NULL , you can omit to specify the property value. For details, see CREATE EDGE .","title":"Syntax"},{"location":"3.ngql-guide/13.edge-statements/1.insert-edge/#examples","text":"# The following example creates edge type e1 with no property and inserts an edge from vertex \"10\" to vertex \"11\" with no property. nebula> CREATE EDGE IF NOT EXISTS e1(); nebula> INSERT EDGE e1 () VALUES \"10\"->\"11\":(); # The following example inserts an edge from vertex \"10\" to vertex \"11\" with no property. The edge rank is 1. nebula> INSERT EDGE e1 () VALUES \"10\"->\"11\"@1:(); nebula> CREATE EDGE IF NOT EXISTS e2 (name string, age int); nebula> INSERT EDGE e2 (name, age) VALUES \"11\"->\"13\":(\"n1\", 1); # The following example creates edge type e2 with two properties. nebula> INSERT EDGE e2 (name, age) VALUES \\ \"12\"->\"13\":(\"n1\", 1), \"13\"->\"14\":(\"n2\", 2); # In the following example, the insertion fails because \"a13\" is not int. nebula> INSERT EDGE e2 (name, age) VALUES \"11\"->\"13\":(\"n1\", \"a13\"); An edge can be inserted/written with property values multiple times. Only the last written values can be read. The following examples insert edge e2 with the new values for multiple times. nebula> INSERT EDGE e2 (name, age) VALUES \"11\"->\"13\":(\"n1\", 12); nebula> INSERT EDGE e2 (name, age) VALUES \"11\"->\"13\":(\"n1\", 13); nebula> INSERT EDGE e2 (name, age) VALUES \"11\"->\"13\":(\"n1\", 14); nebula> FETCH PROP ON e2 \"11\"->\"13\" YIELD edge AS e; +-------------------------------------------+ | e | +-------------------------------------------+ | [:e2 \"11\"->\"13\" @0 {age: 14, name: \"n1\"}] | +-------------------------------------------+ If you insert an edge that already exists with IF NOT EXISTS , there will be no modification. # The following example inserts edge e2 from vertex \"14\" to vertex \"15\". nebula> INSERT EDGE e2 (name, age) VALUES \"14\"->\"15\"@1:(\"n1\", 12); # The following example alters the edge with IF NOT EXISTS. But there will be no alteration because edge e2 already exists. nebula> INSERT EDGE IF NOT EXISTS e2 (name, age) VALUES \"14\"->\"15\"@1:(\"n2\", 13); nebula> FETCH PROP ON e2 \"14\"->\"15\"@1 YIELD edge AS e; +-------------------------------------------+ | e | +-------------------------------------------+ | [:e2 \"14\"->\"15\" @1 {age: 12, name: \"n1\"}] | +-------------------------------------------+ Note Nebula Graph 3.1.0 allows dangling edges. Therefore, you can write the edge before the source vertex or the destination vertex exists. At this time, you can get the (not written) vertex VID through <edgetype>._src or <edgetype>._dst (which is not recommended). Atomic operation is not guaranteed during the entire process for now. If it fails, please try again. Otherwise, partial writing will occur. At this time, the behavior of reading the data is undefined. Concurrently writing the same edge will cause an edge conflict error, so please try again later. The inserting speed of an edge is about half that of a vertex. Because in the storaged process, the insertion of an edge involves two tasks, while the insertion of a vertex involves only one task.","title":"Examples"},{"location":"3.ngql-guide/13.edge-statements/2.update-edge/","text":"UPDATE EDGE \u00b6 The UPDATE EDGE statement updates properties on an edge. In Nebula Graph, UPDATE EDGE supports compare-and-swap (CAS). Syntax \u00b6 UPDATE EDGE ON <edge_type> <src_vid> -> <dst_vid> [@<rank>] SET <update_prop> [WHEN <condition>] [YIELD <output>] Parameter Required Description Example ON <edge_type> Yes Specifies the edge type. The properties to be updated must be on this edge type. ON serve <src_vid> Yes Specifies the source vertex ID of the edge. \"player100\" <dst_vid> Yes Specifies the destination vertex ID of the edge. \"team204\" <rank> No Specifies the rank of the edge. 10 SET <update_prop> Yes Specifies the properties to be updated and how they will be updated. SET start_year = start_year +1 WHEN <condition> No Specifies the filter conditions. If <condition> evaluates to false , the SET clause does not take effect. WHEN end_year < 2010 YIELD <output> No Specifies the output format of the statement. YIELD start_year AS Start_Year Example \u00b6 The following example checks the properties of the edge with the GO statement. nebula> GO FROM \"player100\" \\ OVER serve \\ YIELD properties(edge).start_year, properties(edge).end_year; +------------------+----------------+ | serve.start_year | serve.end_year | +------------------+----------------+ | 1997 | 2016 | +------------------+----------------+ The following example updates the start_year property and returns the end_year and the new start_year . nebula> UPDATE EDGE on serve \"player100\" -> \"team204\"@0 \\ SET start_year = start_year + 1 \\ WHEN end_year > 2010 \\ YIELD start_year, end_year; +------------+----------+ | start_year | end_year | +------------+----------+ | 1998 | 2016 | +------------+----------+","title":"UPDATE EDGE"},{"location":"3.ngql-guide/13.edge-statements/2.update-edge/#update_edge","text":"The UPDATE EDGE statement updates properties on an edge. In Nebula Graph, UPDATE EDGE supports compare-and-swap (CAS).","title":"UPDATE EDGE"},{"location":"3.ngql-guide/13.edge-statements/2.update-edge/#syntax","text":"UPDATE EDGE ON <edge_type> <src_vid> -> <dst_vid> [@<rank>] SET <update_prop> [WHEN <condition>] [YIELD <output>] Parameter Required Description Example ON <edge_type> Yes Specifies the edge type. The properties to be updated must be on this edge type. ON serve <src_vid> Yes Specifies the source vertex ID of the edge. \"player100\" <dst_vid> Yes Specifies the destination vertex ID of the edge. \"team204\" <rank> No Specifies the rank of the edge. 10 SET <update_prop> Yes Specifies the properties to be updated and how they will be updated. SET start_year = start_year +1 WHEN <condition> No Specifies the filter conditions. If <condition> evaluates to false , the SET clause does not take effect. WHEN end_year < 2010 YIELD <output> No Specifies the output format of the statement. YIELD start_year AS Start_Year","title":"Syntax"},{"location":"3.ngql-guide/13.edge-statements/2.update-edge/#example","text":"The following example checks the properties of the edge with the GO statement. nebula> GO FROM \"player100\" \\ OVER serve \\ YIELD properties(edge).start_year, properties(edge).end_year; +------------------+----------------+ | serve.start_year | serve.end_year | +------------------+----------------+ | 1997 | 2016 | +------------------+----------------+ The following example updates the start_year property and returns the end_year and the new start_year . nebula> UPDATE EDGE on serve \"player100\" -> \"team204\"@0 \\ SET start_year = start_year + 1 \\ WHEN end_year > 2010 \\ YIELD start_year, end_year; +------------+----------+ | start_year | end_year | +------------+----------+ | 1998 | 2016 | +------------+----------+","title":"Example"},{"location":"3.ngql-guide/13.edge-statements/3.upsert-edge/","text":"UPSERT EDGE \u00b6 The UPSERT statement is a combination of UPDATE and INSERT . You can use UPSERT EDGE to update the properties of an edge if it exists or insert a new edge if it does not exist. The performance of UPSERT is much lower than that of INSERT because UPSERT is a read-modify-write serialization operation at the partition level. Danger Do not use UPSERT for scenarios with highly concurrent writes. You can use UPDATE or INSERT instead. Syntax \u00b6 UPSERT EDGE ON <edge_type> <src_vid> -> <dst_vid> [@rank] SET <update_prop> [WHEN <condition>] [YIELD <properties>] Parameter Required Description Example ON <edge_type> Yes Specifies the edge type. The properties to be updated must be on this edge type. ON serve <src_vid> Yes Specifies the source vertex ID of the edge. \"player100\" <dst_vid> Yes Specifies the destination vertex ID of the edge. \"team204\" <rank> No Specifies the rank of the edge. 10 SET <update_prop> Yes Specifies the properties to be updated and how they will be updated. SET start_year = start_year +1 WHEN <condition> No Specifies the filter conditions. WHEN end_year < 2010 YIELD <output> No Specifies the output format of the statement. YIELD start_year AS Start_Year Insert an edge if it does not exist \u00b6 If an edge does not exist, it is created no matter the conditions in the WHEN clause are met or not, and the SET clause takes effect. The property values of the new edge depend on: How the SET clause is defined. Whether the property has a default value. For example, if: The edge to be inserted will have properties start_year and end_year based on the edge type serve . The SET clause specifies that end_year = 2021 . Then the property values in different cases are listed as follows: Are WHEN conditions met If properties have default values Value of start_year Value of end_year Yes Yes The default value 2021 Yes No NULL 2021 No Yes The default value 2021 No No NULL 2021 Here are some examples: // This example checks if the following three vertices have any outgoing serve edge. The result \"Empty set\" indicates that such an edge does not exist. nebula> GO FROM \"player666\", \"player667\", \"player668\" \\ OVER serve \\ YIELD properties(edge).start_year, properties(edge).end_year; +-----------------------------+---------------------------+ | properties(EDGE).start_year | properties(EDGE).end_year | +-----------------------------+---------------------------+ +-----------------------------+---------------------------+ Empty set nebula> UPSERT EDGE on serve \\ \"player666\" -> \"team200\"@0 \\ SET end_year = 2021 \\ WHEN end_year == 2010 \\ YIELD start_year, end_year; +------------+----------+ | start_year | end_year | +------------+----------+ | __NULL__ | 2021 | +------------+----------+ nebula> UPSERT EDGE on serve \\ \"player666\" -> \"team200\"@0 \\ SET end_year = 2022 \\ WHEN end_year == 2010 \\ YIELD start_year, end_year; +------------+----------+ | start_year | end_year | +------------+----------+ | __NULL__ | 2021 | +------------+----------+ nebula> UPSERT EDGE on serve \\ \"player667\" -> \"team200\"@0 \\ SET end_year = 2022 \\ YIELD start_year, end_year; +------------+----------+ | start_year | end_year | +------------+----------+ | __NULL__ | 2022 | +------------+----------+ nebula> UPSERT EDGE on serve \\ \"player668\" -> \"team200\"@0 \\ SET start_year = 2000, end_year = end_year + 1 \\ YIELD start_year, end_year; +------------+----------+ | start_year | end_year | +------------+----------+ | 2000 | __NULL__ | +------------+----------+ In the last query of the preceding example, since end_year has no default value, when the edge is created, end_year is NULL , and end_year = end_year + 1 does not take effect. But if end_year has a default value, end_year = end_year + 1 will take effect. For example: nebula> CREATE EDGE IF NOT EXISTS serve_with_default(start_year int, end_year int DEFAULT 2010); Execution succeeded nebula> UPSERT EDGE on serve_with_default \\ \"player668\" -> \"team200\" \\ SET end_year = end_year + 1 \\ YIELD start_year, end_year; +------------+----------+ | start_year | end_year | +------------+----------+ | __NULL__ | 2011 | +------------+----------+ Update an edge if it exists \u00b6 If the edge exists and the WHEN conditions are met, the edge is updated. nebula> MATCH (v:player{name:\"Ben Simmons\"})-[e:serve]-(v2) \\ RETURN e; +-----------------------------------------------------------------------+ | e | +-----------------------------------------------------------------------+ | [:serve \"player149\"->\"team219\" @0 {end_year: 2019, start_year: 2016}] | +-----------------------------------------------------------------------+ nebula> UPSERT EDGE on serve \\ \"player149\" -> \"team219\" \\ SET end_year = end_year + 1 \\ WHEN start_year == 2016 \\ YIELD start_year, end_year; +------------+----------+ | start_year | end_year | +------------+----------+ | 2016 | 2020 | +------------+----------+ If the edge exists and the WHEN conditions are not met, the update does not take effect. nebula> MATCH (v:player{name:\"Ben Simmons\"})-[e:serve]-(v2) \\ RETURN e; +-----------------------------------------------------------------------+ | e | +-----------------------------------------------------------------------+ | [:serve \"player149\"->\"team219\" @0 {end_year: 2020, start_year: 2016}] | +-----------------------------------------------------------------------+ nebula> UPSERT EDGE on serve \\ \"player149\" -> \"team219\" \\ SET end_year = end_year + 1 \\ WHEN start_year != 2016 \\ YIELD start_year, end_year; +------------+----------+ | start_year | end_year | +------------+----------+ | 2016 | 2020 | +------------+----------+","title":"UPSERT EDGE"},{"location":"3.ngql-guide/13.edge-statements/3.upsert-edge/#upsert_edge","text":"The UPSERT statement is a combination of UPDATE and INSERT . You can use UPSERT EDGE to update the properties of an edge if it exists or insert a new edge if it does not exist. The performance of UPSERT is much lower than that of INSERT because UPSERT is a read-modify-write serialization operation at the partition level. Danger Do not use UPSERT for scenarios with highly concurrent writes. You can use UPDATE or INSERT instead.","title":"UPSERT EDGE"},{"location":"3.ngql-guide/13.edge-statements/3.upsert-edge/#syntax","text":"UPSERT EDGE ON <edge_type> <src_vid> -> <dst_vid> [@rank] SET <update_prop> [WHEN <condition>] [YIELD <properties>] Parameter Required Description Example ON <edge_type> Yes Specifies the edge type. The properties to be updated must be on this edge type. ON serve <src_vid> Yes Specifies the source vertex ID of the edge. \"player100\" <dst_vid> Yes Specifies the destination vertex ID of the edge. \"team204\" <rank> No Specifies the rank of the edge. 10 SET <update_prop> Yes Specifies the properties to be updated and how they will be updated. SET start_year = start_year +1 WHEN <condition> No Specifies the filter conditions. WHEN end_year < 2010 YIELD <output> No Specifies the output format of the statement. YIELD start_year AS Start_Year","title":"Syntax"},{"location":"3.ngql-guide/13.edge-statements/3.upsert-edge/#insert_an_edge_if_it_does_not_exist","text":"If an edge does not exist, it is created no matter the conditions in the WHEN clause are met or not, and the SET clause takes effect. The property values of the new edge depend on: How the SET clause is defined. Whether the property has a default value. For example, if: The edge to be inserted will have properties start_year and end_year based on the edge type serve . The SET clause specifies that end_year = 2021 . Then the property values in different cases are listed as follows: Are WHEN conditions met If properties have default values Value of start_year Value of end_year Yes Yes The default value 2021 Yes No NULL 2021 No Yes The default value 2021 No No NULL 2021 Here are some examples: // This example checks if the following three vertices have any outgoing serve edge. The result \"Empty set\" indicates that such an edge does not exist. nebula> GO FROM \"player666\", \"player667\", \"player668\" \\ OVER serve \\ YIELD properties(edge).start_year, properties(edge).end_year; +-----------------------------+---------------------------+ | properties(EDGE).start_year | properties(EDGE).end_year | +-----------------------------+---------------------------+ +-----------------------------+---------------------------+ Empty set nebula> UPSERT EDGE on serve \\ \"player666\" -> \"team200\"@0 \\ SET end_year = 2021 \\ WHEN end_year == 2010 \\ YIELD start_year, end_year; +------------+----------+ | start_year | end_year | +------------+----------+ | __NULL__ | 2021 | +------------+----------+ nebula> UPSERT EDGE on serve \\ \"player666\" -> \"team200\"@0 \\ SET end_year = 2022 \\ WHEN end_year == 2010 \\ YIELD start_year, end_year; +------------+----------+ | start_year | end_year | +------------+----------+ | __NULL__ | 2021 | +------------+----------+ nebula> UPSERT EDGE on serve \\ \"player667\" -> \"team200\"@0 \\ SET end_year = 2022 \\ YIELD start_year, end_year; +------------+----------+ | start_year | end_year | +------------+----------+ | __NULL__ | 2022 | +------------+----------+ nebula> UPSERT EDGE on serve \\ \"player668\" -> \"team200\"@0 \\ SET start_year = 2000, end_year = end_year + 1 \\ YIELD start_year, end_year; +------------+----------+ | start_year | end_year | +------------+----------+ | 2000 | __NULL__ | +------------+----------+ In the last query of the preceding example, since end_year has no default value, when the edge is created, end_year is NULL , and end_year = end_year + 1 does not take effect. But if end_year has a default value, end_year = end_year + 1 will take effect. For example: nebula> CREATE EDGE IF NOT EXISTS serve_with_default(start_year int, end_year int DEFAULT 2010); Execution succeeded nebula> UPSERT EDGE on serve_with_default \\ \"player668\" -> \"team200\" \\ SET end_year = end_year + 1 \\ YIELD start_year, end_year; +------------+----------+ | start_year | end_year | +------------+----------+ | __NULL__ | 2011 | +------------+----------+","title":"Insert an edge if it does not exist"},{"location":"3.ngql-guide/13.edge-statements/3.upsert-edge/#update_an_edge_if_it_exists","text":"If the edge exists and the WHEN conditions are met, the edge is updated. nebula> MATCH (v:player{name:\"Ben Simmons\"})-[e:serve]-(v2) \\ RETURN e; +-----------------------------------------------------------------------+ | e | +-----------------------------------------------------------------------+ | [:serve \"player149\"->\"team219\" @0 {end_year: 2019, start_year: 2016}] | +-----------------------------------------------------------------------+ nebula> UPSERT EDGE on serve \\ \"player149\" -> \"team219\" \\ SET end_year = end_year + 1 \\ WHEN start_year == 2016 \\ YIELD start_year, end_year; +------------+----------+ | start_year | end_year | +------------+----------+ | 2016 | 2020 | +------------+----------+ If the edge exists and the WHEN conditions are not met, the update does not take effect. nebula> MATCH (v:player{name:\"Ben Simmons\"})-[e:serve]-(v2) \\ RETURN e; +-----------------------------------------------------------------------+ | e | +-----------------------------------------------------------------------+ | [:serve \"player149\"->\"team219\" @0 {end_year: 2020, start_year: 2016}] | +-----------------------------------------------------------------------+ nebula> UPSERT EDGE on serve \\ \"player149\" -> \"team219\" \\ SET end_year = end_year + 1 \\ WHEN start_year != 2016 \\ YIELD start_year, end_year; +------------+----------+ | start_year | end_year | +------------+----------+ | 2016 | 2020 | +------------+----------+","title":"Update an edge if it exists"},{"location":"3.ngql-guide/13.edge-statements/4.delete-edge/","text":"DELETE EDGE \u00b6 The DELETE EDGE statement deletes one edge or multiple edges at a time. You can use DELETE EDGE together with pipe operators. For more information, see PIPE OPERATORS . To delete all the outgoing edges for a vertex, please delete the vertex. For more information, see DELETE VERTEX . Syntax \u00b6 DELETE EDGE <edge_type> <src_vid> -> <dst_vid>[@<rank>] [, <src_vid> -> <dst_vid>[@<rank>] ...] Examples \u00b6 nebula> DELETE EDGE serve \"player100\" -> \"team204\"@0; The following example shows that you can use DELETE EDGE together with pipe operators to delete edges that meet the conditions. nebula> GO FROM \"player100\" OVER follow \\ WHERE dst(edge) == \"team204\" \\ YIELD src(edge) AS src, dst(edge) AS dst, rank(edge) AS rank \\ | DELETE EDGE follow $-.src->$-.dst @ $-.rank;","title":"DELETE EDGE"},{"location":"3.ngql-guide/13.edge-statements/4.delete-edge/#delete_edge","text":"The DELETE EDGE statement deletes one edge or multiple edges at a time. You can use DELETE EDGE together with pipe operators. For more information, see PIPE OPERATORS . To delete all the outgoing edges for a vertex, please delete the vertex. For more information, see DELETE VERTEX .","title":"DELETE EDGE"},{"location":"3.ngql-guide/13.edge-statements/4.delete-edge/#syntax","text":"DELETE EDGE <edge_type> <src_vid> -> <dst_vid>[@<rank>] [, <src_vid> -> <dst_vid>[@<rank>] ...]","title":"Syntax"},{"location":"3.ngql-guide/13.edge-statements/4.delete-edge/#examples","text":"nebula> DELETE EDGE serve \"player100\" -> \"team204\"@0; The following example shows that you can use DELETE EDGE together with pipe operators to delete edges that meet the conditions. nebula> GO FROM \"player100\" OVER follow \\ WHERE dst(edge) == \"team204\" \\ YIELD src(edge) AS src, dst(edge) AS dst, rank(edge) AS rank \\ | DELETE EDGE follow $-.src->$-.dst @ $-.rank;","title":"Examples"},{"location":"3.ngql-guide/14.native-index-statements/","text":"Index overview \u00b6 Indexes are built to fast process graph queries. Nebula Graph supports two kinds of indexes: native indexes and full-text indexes. This topic introduces the index types and helps choose the right index. Native indexes \u00b6 Native indexes allow querying data based on a given property. Features are as follows. There are two kinds of native indexes: tag index and edge type index. Native indexes must be updated manually. You can use the REBUILD INDEX statement to update native indexes. Native indexes support indexing multiple properties on a tag or an edge type (composite indexes), but do not support indexing across multiple tags or edge types. Operations on native indexes \u00b6 CREATE INDEX SHOW CREATE INDEX SHOW INDEXES DESCRIBE INDEX REBUILD INDEX SHOW INDEX STATUS DROP INDEX LOOKUP MATCH Full-text indexes \u00b6 Full-text indexes are used to do prefix, wildcard, regexp, and fuzzy search on a string property. Features are as follows. Full-text indexes allow indexing just one property. Only strings within a specified length (no longer than 256 bytes) are indexed. Full-text indexes do not support logical operations such as AND , OR , and NOT . Note To do complete string matches, use native indexes. Operations on full-text indexes \u00b6 Before doing any operations on full-text indexes, please make sure that you deploy full-text indexes. Details on full-text indexes deployment, see Deploy Elasticsearch and Deploy Listener . At this time, full-text indexes are created automatically on the Elasticsearch cluster. And rebuilding or altering full-text indexes are not supported. To drop full-text indexes, you need to drop them on the Elasticsearch cluster manually. To query full-text indexes, see Search with full-text indexes . Null values \u00b6 Indexes do not support indexing null values. Range queries \u00b6 In addition to querying single results from native indexes, you can also do range queries. Not all the native indexes support range queries. You can only do range searches for numeric, date, and time type properties.","title":"Index overview"},{"location":"3.ngql-guide/14.native-index-statements/#index_overview","text":"Indexes are built to fast process graph queries. Nebula Graph supports two kinds of indexes: native indexes and full-text indexes. This topic introduces the index types and helps choose the right index.","title":"Index overview"},{"location":"3.ngql-guide/14.native-index-statements/#native_indexes","text":"Native indexes allow querying data based on a given property. Features are as follows. There are two kinds of native indexes: tag index and edge type index. Native indexes must be updated manually. You can use the REBUILD INDEX statement to update native indexes. Native indexes support indexing multiple properties on a tag or an edge type (composite indexes), but do not support indexing across multiple tags or edge types.","title":"Native indexes"},{"location":"3.ngql-guide/14.native-index-statements/#operations_on_native_indexes","text":"CREATE INDEX SHOW CREATE INDEX SHOW INDEXES DESCRIBE INDEX REBUILD INDEX SHOW INDEX STATUS DROP INDEX LOOKUP MATCH","title":"Operations on native indexes"},{"location":"3.ngql-guide/14.native-index-statements/#full-text_indexes","text":"Full-text indexes are used to do prefix, wildcard, regexp, and fuzzy search on a string property. Features are as follows. Full-text indexes allow indexing just one property. Only strings within a specified length (no longer than 256 bytes) are indexed. Full-text indexes do not support logical operations such as AND , OR , and NOT . Note To do complete string matches, use native indexes.","title":"Full-text indexes"},{"location":"3.ngql-guide/14.native-index-statements/#operations_on_full-text_indexes","text":"Before doing any operations on full-text indexes, please make sure that you deploy full-text indexes. Details on full-text indexes deployment, see Deploy Elasticsearch and Deploy Listener . At this time, full-text indexes are created automatically on the Elasticsearch cluster. And rebuilding or altering full-text indexes are not supported. To drop full-text indexes, you need to drop them on the Elasticsearch cluster manually. To query full-text indexes, see Search with full-text indexes .","title":"Operations on full-text indexes"},{"location":"3.ngql-guide/14.native-index-statements/#null_values","text":"Indexes do not support indexing null values.","title":"Null values"},{"location":"3.ngql-guide/14.native-index-statements/#range_queries","text":"In addition to querying single results from native indexes, you can also do range queries. Not all the native indexes support range queries. You can only do range searches for numeric, date, and time type properties.","title":"Range queries"},{"location":"3.ngql-guide/14.native-index-statements/1.create-native-index/","text":"CREATE INDEX \u00b6 Prerequisites \u00b6 Before you create an index, make sure that the relative tag or edge type is created. For how to create tags or edge types, see CREATE TAG and CREATE EDGE . For how to create full-text indexes, see Deploy full-text index . Must-read for using indexes \u00b6 The concept and using restrictions of indexes are comparatively complex. You can use it together with LOOKUP and MATCH statements. You can use CREATE INDEX to add native indexes for the existing tags, edge types, or properties. They are usually called as tag indexes, edge type indexes, and property indexes. Tag indexes and edge type indexes apply to queries related to the tag and the edge type, but do not apply to queries that are based on certain properties on the tag. For example, you can use LOOKUP to retrieve all the vertices with the tag player . Property indexes apply to property-based queries. For example, you can use the age property to retrieve the VID of all vertices that meet age == 19 . If a property index i_TA is created for the property A of the tag T , the indexes can be replaced as follows (the same for edge type indexes): The query engine can use i_TA to replace i_T . In the MATCH statement, i_T cannot replace i_TA for querying properties. In the LOOKUP statement, i_T may replace i_TA for querying properties. Legacy version compatibility In previous releases, the tag or edge type index in the LOOKUP statement cannot replace the property index for property queries. Although the same results can be obtained by using alternative indexes for queries, the query performance varies according to the selected index. Caution Indexes can dramatically reduce the write performance. The performance reduction can be as much as 90% or even more. DO NOT use indexes in production environments unless you are fully aware of their influences on your service. Indexes cannot make queries faster. It can only locate a vertex or an edge according to properties or count the number of vertices or edges. Long indexes decrease the scan performance of the Storage Service and use more memory. We suggest that you set the indexing length the same as that of the longest string to be indexed. The longest index length is 256 bytes. If you must use indexes, we suggest that you: Import the data into Nebula Graph. Create indexes. Rebuild indexes . After the index is created and the data is imported, you can use LOOKUP or MATCH to retrieve the data. You do not need to specify which indexes to use in a query, Nebula Graph figures that out by itself. Note If you create an index before importing the data, the importing speed will be extremely slow due to the reduction in the write performance. Keep --disable_auto_compaction = false during daily incremental writing. The newly created index will not take effect immediately. Trying to use a newly created index (such as LOOKUP or REBUILD INDEX ) may fail and return can't find xxx in the space because the creation is implemented asynchronously. To make sure the follow-up operations work as expected, Wait for two heartbeat cycles, i.e., 20 seconds. To change the heartbeat interval, modify the heartbeat_interval_secs in the configuration files for all services. Danger After creating a new index, or dropping the old index and creating a new one with the same name again, you must REBUILD INDEX . Otherwise, these data cannot be returned in the MATCH and LOOKUP statements. Syntax \u00b6 CREATE {TAG | EDGE} INDEX [IF NOT EXISTS] <index_name> ON {<tag_name> | <edge_name>} ([<prop_name_list>]) [COMMENT '<comment>']; Parameter Description TAG \\| EDGE Specifies the index type that you want to create. IF NOT EXISTS Detects if the index that you want to create exists. If it does not exist, a new one will be created. <index_name> The name of the index. It must be unique in a graph space. A recommended way of naming is i_tagName_propName . The name of the index starts with a letter, supports 1 to 4 bytes UTF-8 encoded characters, such as English letters (case-sensitive), digits, and Chinese characters, but does not support special characters except underscores. To use special characters or reserved keywords as identifiers, quote them with backticks. For more information, see Keywords and reserved words . <tag_name> \\| <edge_name> Specifies the name of the tag or edge associated with the index. <prop_name_list> To index a variable-length string property, you must use prop_name(length) to specify the index length. To index a tag or an edge type, ignore the prop_name_list . COMMENT The remarks of the index. The maximum length is 256 bytes. By default, there will be no comments on an index. Create tag/edge type indexes \u00b6 nebula> CREATE TAG INDEX IF NOT EXISTS player_index on player(); nebula> CREATE EDGE INDEX IF NOT EXISTS follow_index on follow(); After indexing a tag or an edge type, you can use the LOOKUP statement to retrieve the VID of all vertices with the tag , or the source vertex ID, destination vertex ID, and ranks of all edges with the edge type . For more information, see LOOKUP . Create single-property indexes \u00b6 nebula> CREATE TAG INDEX IF NOT EXISTS player_index_0 on player(name(10)); The preceding example creates an index for the name property on all vertices carrying the player tag. This example creates an index using the first 10 characters of the name property. # To index a variable-length string property, you need to specify the index length. nebula> CREATE TAG IF NOT EXISTS var_string(p1 string); nebula> CREATE TAG INDEX IF NOT EXISTS var ON var_string(p1(10)); # To index a fixed-length string property, you do not need to specify the index length. nebula> CREATE TAG IF NOT EXISTS fix_string(p1 FIXED_STRING(10)); nebula> CREATE TAG INDEX IF NOT EXISTS fix ON fix_string(p1); nebula> CREATE EDGE INDEX IF NOT EXISTS follow_index_0 on follow(degree); Create composite property indexes \u00b6 An index on multiple properties on a tag (or an edge type) is called a composite property index. nebula> CREATE TAG INDEX IF NOT EXISTS player_index_1 on player(name(10), age); Caution Creating composite property indexes across multiple tags or edge types is not supported. Note Nebula Graph follows the left matching principle to select indexes.","title":"CREATE INDEX"},{"location":"3.ngql-guide/14.native-index-statements/1.create-native-index/#create_index","text":"","title":"CREATE INDEX"},{"location":"3.ngql-guide/14.native-index-statements/1.create-native-index/#prerequisites","text":"Before you create an index, make sure that the relative tag or edge type is created. For how to create tags or edge types, see CREATE TAG and CREATE EDGE . For how to create full-text indexes, see Deploy full-text index .","title":"Prerequisites"},{"location":"3.ngql-guide/14.native-index-statements/1.create-native-index/#must-read_for_using_indexes","text":"The concept and using restrictions of indexes are comparatively complex. You can use it together with LOOKUP and MATCH statements. You can use CREATE INDEX to add native indexes for the existing tags, edge types, or properties. They are usually called as tag indexes, edge type indexes, and property indexes. Tag indexes and edge type indexes apply to queries related to the tag and the edge type, but do not apply to queries that are based on certain properties on the tag. For example, you can use LOOKUP to retrieve all the vertices with the tag player . Property indexes apply to property-based queries. For example, you can use the age property to retrieve the VID of all vertices that meet age == 19 . If a property index i_TA is created for the property A of the tag T , the indexes can be replaced as follows (the same for edge type indexes): The query engine can use i_TA to replace i_T . In the MATCH statement, i_T cannot replace i_TA for querying properties. In the LOOKUP statement, i_T may replace i_TA for querying properties. Legacy version compatibility In previous releases, the tag or edge type index in the LOOKUP statement cannot replace the property index for property queries. Although the same results can be obtained by using alternative indexes for queries, the query performance varies according to the selected index. Caution Indexes can dramatically reduce the write performance. The performance reduction can be as much as 90% or even more. DO NOT use indexes in production environments unless you are fully aware of their influences on your service. Indexes cannot make queries faster. It can only locate a vertex or an edge according to properties or count the number of vertices or edges. Long indexes decrease the scan performance of the Storage Service and use more memory. We suggest that you set the indexing length the same as that of the longest string to be indexed. The longest index length is 256 bytes. If you must use indexes, we suggest that you: Import the data into Nebula Graph. Create indexes. Rebuild indexes . After the index is created and the data is imported, you can use LOOKUP or MATCH to retrieve the data. You do not need to specify which indexes to use in a query, Nebula Graph figures that out by itself. Note If you create an index before importing the data, the importing speed will be extremely slow due to the reduction in the write performance. Keep --disable_auto_compaction = false during daily incremental writing. The newly created index will not take effect immediately. Trying to use a newly created index (such as LOOKUP or REBUILD INDEX ) may fail and return can't find xxx in the space because the creation is implemented asynchronously. To make sure the follow-up operations work as expected, Wait for two heartbeat cycles, i.e., 20 seconds. To change the heartbeat interval, modify the heartbeat_interval_secs in the configuration files for all services. Danger After creating a new index, or dropping the old index and creating a new one with the same name again, you must REBUILD INDEX . Otherwise, these data cannot be returned in the MATCH and LOOKUP statements.","title":"Must-read for using indexes"},{"location":"3.ngql-guide/14.native-index-statements/1.create-native-index/#syntax","text":"CREATE {TAG | EDGE} INDEX [IF NOT EXISTS] <index_name> ON {<tag_name> | <edge_name>} ([<prop_name_list>]) [COMMENT '<comment>']; Parameter Description TAG \\| EDGE Specifies the index type that you want to create. IF NOT EXISTS Detects if the index that you want to create exists. If it does not exist, a new one will be created. <index_name> The name of the index. It must be unique in a graph space. A recommended way of naming is i_tagName_propName . The name of the index starts with a letter, supports 1 to 4 bytes UTF-8 encoded characters, such as English letters (case-sensitive), digits, and Chinese characters, but does not support special characters except underscores. To use special characters or reserved keywords as identifiers, quote them with backticks. For more information, see Keywords and reserved words . <tag_name> \\| <edge_name> Specifies the name of the tag or edge associated with the index. <prop_name_list> To index a variable-length string property, you must use prop_name(length) to specify the index length. To index a tag or an edge type, ignore the prop_name_list . COMMENT The remarks of the index. The maximum length is 256 bytes. By default, there will be no comments on an index.","title":"Syntax"},{"location":"3.ngql-guide/14.native-index-statements/1.create-native-index/#create_tagedge_type_indexes","text":"nebula> CREATE TAG INDEX IF NOT EXISTS player_index on player(); nebula> CREATE EDGE INDEX IF NOT EXISTS follow_index on follow(); After indexing a tag or an edge type, you can use the LOOKUP statement to retrieve the VID of all vertices with the tag , or the source vertex ID, destination vertex ID, and ranks of all edges with the edge type . For more information, see LOOKUP .","title":"Create tag/edge type indexes"},{"location":"3.ngql-guide/14.native-index-statements/1.create-native-index/#create_single-property_indexes","text":"nebula> CREATE TAG INDEX IF NOT EXISTS player_index_0 on player(name(10)); The preceding example creates an index for the name property on all vertices carrying the player tag. This example creates an index using the first 10 characters of the name property. # To index a variable-length string property, you need to specify the index length. nebula> CREATE TAG IF NOT EXISTS var_string(p1 string); nebula> CREATE TAG INDEX IF NOT EXISTS var ON var_string(p1(10)); # To index a fixed-length string property, you do not need to specify the index length. nebula> CREATE TAG IF NOT EXISTS fix_string(p1 FIXED_STRING(10)); nebula> CREATE TAG INDEX IF NOT EXISTS fix ON fix_string(p1); nebula> CREATE EDGE INDEX IF NOT EXISTS follow_index_0 on follow(degree);","title":"Create single-property indexes"},{"location":"3.ngql-guide/14.native-index-statements/1.create-native-index/#create_composite_property_indexes","text":"An index on multiple properties on a tag (or an edge type) is called a composite property index. nebula> CREATE TAG INDEX IF NOT EXISTS player_index_1 on player(name(10), age); Caution Creating composite property indexes across multiple tags or edge types is not supported. Note Nebula Graph follows the left matching principle to select indexes.","title":"Create composite property indexes"},{"location":"3.ngql-guide/14.native-index-statements/2.1.show-create-index/","text":"SHOW CREATE INDEX \u00b6 SHOW CREATE INDEX shows the statement used when creating a tag or an edge type. It contains detailed information about the index, such as its associated properties. Syntax \u00b6 SHOW CREATE {TAG | EDGE} INDEX <index_name>; Examples \u00b6 You can run SHOW TAG INDEXES to list all tag indexes, and then use SHOW CREATE TAG INDEX to show the information about the creation of the specified index. nebula> SHOW TAG INDEXES; +------------------+----------+----------+ | Index Name | By Tag | Columns | +------------------+----------+----------+ | \"player_index_0\" | \"player\" | [] | | \"player_index_1\" | \"player\" | [\"name\"] | +------------------+----------+----------+ nebula> SHOW CREATE TAG INDEX player_index_1; +------------------+--------------------------------------------------+ | Tag Index Name | Create Tag Index | +------------------+--------------------------------------------------+ | \"player_index_1\" | \"CREATE TAG INDEX `player_index_1` ON `player` ( | | | `name`(20) | | | )\" | +------------------+--------------------------------------------------+ Edge indexes can be queried through a similar approach. nebula> SHOW EDGE INDEXES; +----------------+----------+---------+ | Index Name | By Edge | Columns | +----------------+----------+---------+ | \"follow_index\" | \"follow\" | [] | +----------------+----------+---------+ nebula> SHOW CREATE EDGE INDEX follow_index; +-----------------+-------------------------------------------------+ | Edge Index Name | Create Edge Index | +-----------------+-------------------------------------------------+ | \"follow_index\" | \"CREATE EDGE INDEX `follow_index` ON `follow` ( | | | )\" | +-----------------+-------------------------------------------------+ Legacy version compatibility In Nebula Graph 2.0.1, the SHOW TAG/EDGE INDEXES statement only returns Names .","title":"SHOW CREATE INDEX"},{"location":"3.ngql-guide/14.native-index-statements/2.1.show-create-index/#show_create_index","text":"SHOW CREATE INDEX shows the statement used when creating a tag or an edge type. It contains detailed information about the index, such as its associated properties.","title":"SHOW CREATE INDEX"},{"location":"3.ngql-guide/14.native-index-statements/2.1.show-create-index/#syntax","text":"SHOW CREATE {TAG | EDGE} INDEX <index_name>;","title":"Syntax"},{"location":"3.ngql-guide/14.native-index-statements/2.1.show-create-index/#examples","text":"You can run SHOW TAG INDEXES to list all tag indexes, and then use SHOW CREATE TAG INDEX to show the information about the creation of the specified index. nebula> SHOW TAG INDEXES; +------------------+----------+----------+ | Index Name | By Tag | Columns | +------------------+----------+----------+ | \"player_index_0\" | \"player\" | [] | | \"player_index_1\" | \"player\" | [\"name\"] | +------------------+----------+----------+ nebula> SHOW CREATE TAG INDEX player_index_1; +------------------+--------------------------------------------------+ | Tag Index Name | Create Tag Index | +------------------+--------------------------------------------------+ | \"player_index_1\" | \"CREATE TAG INDEX `player_index_1` ON `player` ( | | | `name`(20) | | | )\" | +------------------+--------------------------------------------------+ Edge indexes can be queried through a similar approach. nebula> SHOW EDGE INDEXES; +----------------+----------+---------+ | Index Name | By Edge | Columns | +----------------+----------+---------+ | \"follow_index\" | \"follow\" | [] | +----------------+----------+---------+ nebula> SHOW CREATE EDGE INDEX follow_index; +-----------------+-------------------------------------------------+ | Edge Index Name | Create Edge Index | +-----------------+-------------------------------------------------+ | \"follow_index\" | \"CREATE EDGE INDEX `follow_index` ON `follow` ( | | | )\" | +-----------------+-------------------------------------------------+ Legacy version compatibility In Nebula Graph 2.0.1, the SHOW TAG/EDGE INDEXES statement only returns Names .","title":"Examples"},{"location":"3.ngql-guide/14.native-index-statements/2.show-native-indexes/","text":"SHOW INDEXES \u00b6 SHOW INDEXES shows the defined tag or edge type indexes names in the current graph space. Syntax \u00b6 SHOW {TAG | EDGE} INDEXES Examples \u00b6 nebula> SHOW TAG INDEXES; +------------------+--------------+-----------------+ | Index Name | By Tag | Columns | +------------------+--------------+-----------------+ | \"fix\" | \"fix_string\" | [\"p1\"] | | \"player_index_0\" | \"player\" | [\"name\"] | | \"player_index_1\" | \"player\" | [\"name\", \"age\"] | | \"var\" | \"var_string\" | [\"p1\"] | +------------------+--------------+-----------------+ nebula> SHOW EDGE INDEXES; +----------------+----------+---------+ | Index Name | By Edge | Columns | | \"follow_index\" | \"follow\" | [] | +----------------+----------+---------+ Legacy version compatibility In Nebula Graph 2.x, the SHOW TAG/EDGE INDEXES statement only returns Names .","title":"SHOW INDEX"},{"location":"3.ngql-guide/14.native-index-statements/2.show-native-indexes/#show_indexes","text":"SHOW INDEXES shows the defined tag or edge type indexes names in the current graph space.","title":"SHOW INDEXES"},{"location":"3.ngql-guide/14.native-index-statements/2.show-native-indexes/#syntax","text":"SHOW {TAG | EDGE} INDEXES","title":"Syntax"},{"location":"3.ngql-guide/14.native-index-statements/2.show-native-indexes/#examples","text":"nebula> SHOW TAG INDEXES; +------------------+--------------+-----------------+ | Index Name | By Tag | Columns | +------------------+--------------+-----------------+ | \"fix\" | \"fix_string\" | [\"p1\"] | | \"player_index_0\" | \"player\" | [\"name\"] | | \"player_index_1\" | \"player\" | [\"name\", \"age\"] | | \"var\" | \"var_string\" | [\"p1\"] | +------------------+--------------+-----------------+ nebula> SHOW EDGE INDEXES; +----------------+----------+---------+ | Index Name | By Edge | Columns | | \"follow_index\" | \"follow\" | [] | +----------------+----------+---------+ Legacy version compatibility In Nebula Graph 2.x, the SHOW TAG/EDGE INDEXES statement only returns Names .","title":"Examples"},{"location":"3.ngql-guide/14.native-index-statements/3.describe-native-index/","text":"DESCRIBE INDEX \u00b6 DESCRIBE INDEX can get the information about the index with a given name, including the property name (Field) and the property type (Type) of the index. Syntax \u00b6 DESCRIBE {TAG | EDGE} INDEX <index_name>; Examples \u00b6 nebula> DESCRIBE TAG INDEX player_index_0; +--------+--------------------+ | Field | Type | +--------+--------------------+ | \"name\" | \"fixed_string(30)\" | +--------+--------------------+ nebula> DESCRIBE TAG INDEX player_index_1; +--------+--------------------+ | Field | Type | +--------+--------------------+ | \"name\" | \"fixed_string(10)\" | | \"age\" | \"int64\" | +--------+--------------------+","title":"DESCRIBE INDEX"},{"location":"3.ngql-guide/14.native-index-statements/3.describe-native-index/#describe_index","text":"DESCRIBE INDEX can get the information about the index with a given name, including the property name (Field) and the property type (Type) of the index.","title":"DESCRIBE INDEX"},{"location":"3.ngql-guide/14.native-index-statements/3.describe-native-index/#syntax","text":"DESCRIBE {TAG | EDGE} INDEX <index_name>;","title":"Syntax"},{"location":"3.ngql-guide/14.native-index-statements/3.describe-native-index/#examples","text":"nebula> DESCRIBE TAG INDEX player_index_0; +--------+--------------------+ | Field | Type | +--------+--------------------+ | \"name\" | \"fixed_string(30)\" | +--------+--------------------+ nebula> DESCRIBE TAG INDEX player_index_1; +--------+--------------------+ | Field | Type | +--------+--------------------+ | \"name\" | \"fixed_string(10)\" | | \"age\" | \"int64\" | +--------+--------------------+","title":"Examples"},{"location":"3.ngql-guide/14.native-index-statements/4.rebuild-native-index/","text":"REBUILD INDEX \u00b6 Danger If data is updated or inserted before the creation of the index, you must rebuild the indexes manually to make sure that the indexes contain the previously added data. Otherwise, you cannot use LOOKUP and MATCH to query the data based on the index. If the index is created before any data insertion, there is no need to rebuild the index. During the rebuilding, all queries skip the index and perform sequential scans. This means that the return results can be different because not all the data is indexed during rebuilding. You can use REBUILD INDEX to rebuild the created tag or edge type index. For details on how to create an index, see CREATE INDEX . Syntax \u00b6 REBUILD {TAG | EDGE} INDEX [<index_name_list>]; <index_name_list>::= [index_name [, index_name] ...] Multiple indexes are permitted in a single REBUILD statement, separated by commas. When the index name is not specified, all tag or edge indexes are rebuilt. After the rebuilding is complete, you can use the SHOW {TAG | EDGE} INDEX STATUS command to check if the index is successfully rebuilt. For details on index status, see SHOW INDEX STATUS . Examples \u00b6 nebula> CREATE TAG IF NOT EXISTS person(name string, age int, gender string, email string); nebula> CREATE TAG INDEX IF NOT EXISTS single_person_index ON person(name(10)); # The following example rebuilds an index and returns the job ID. nebula> REBUILD TAG INDEX single_person_index; +------------+ | New Job Id | +------------+ | 31 | +------------+ # The following example checks the index status. nebula> SHOW TAG INDEX STATUS; +-----------------------+--------------+ | Name | Index Status | +-----------------------+--------------+ | \"single_person_index\" | \"FINISHED\" | +-----------------------+--------------+ # You can also use \"SHOW JOB <job_id>\" to check if the rebuilding process is complete. nebula> SHOW JOB 31; +----------------+---------------------+------------+-------------------------+-------------------------+-------------+ | Job Id(TaskId) | Command(Dest) | Status | Start Time | Stop Time | Error Code | +----------------+---------------------+------------+-------------------------+-------------------------+-------------+ | 31 | \"REBUILD_TAG_INDEX\" | \"FINISHED\" | 2021-07-07T09:04:24.000 | 2021-07-07T09:04:24.000 | \"SUCCEEDED\" | | 0 | \"storaged1\" | \"FINISHED\" | 2021-07-07T09:04:24.000 | 2021-07-07T09:04:28.000 | \"SUCCEEDED\" | | 1 | \"storaged2\" | \"FINISHED\" | 2021-07-07T09:04:24.000 | 2021-07-07T09:04:28.000 | \"SUCCEEDED\" | | 2 | \"storaged0\" | \"FINISHED\" | 2021-07-07T09:04:24.000 | 2021-07-07T09:04:28.000 | \"SUCCEEDED\" | +----------------+---------------------+------------+-------------------------+-------------------------+-------------+ Nebula Graph creates a job to rebuild the index. The job ID is displayed in the preceding return message. To check if the rebuilding process is complete, use the SHOW JOB <job_id> statement. For more information, see SHOW JOB .","title":"REBUILD INDEX"},{"location":"3.ngql-guide/14.native-index-statements/4.rebuild-native-index/#rebuild_index","text":"Danger If data is updated or inserted before the creation of the index, you must rebuild the indexes manually to make sure that the indexes contain the previously added data. Otherwise, you cannot use LOOKUP and MATCH to query the data based on the index. If the index is created before any data insertion, there is no need to rebuild the index. During the rebuilding, all queries skip the index and perform sequential scans. This means that the return results can be different because not all the data is indexed during rebuilding. You can use REBUILD INDEX to rebuild the created tag or edge type index. For details on how to create an index, see CREATE INDEX .","title":"REBUILD INDEX"},{"location":"3.ngql-guide/14.native-index-statements/4.rebuild-native-index/#syntax","text":"REBUILD {TAG | EDGE} INDEX [<index_name_list>]; <index_name_list>::= [index_name [, index_name] ...] Multiple indexes are permitted in a single REBUILD statement, separated by commas. When the index name is not specified, all tag or edge indexes are rebuilt. After the rebuilding is complete, you can use the SHOW {TAG | EDGE} INDEX STATUS command to check if the index is successfully rebuilt. For details on index status, see SHOW INDEX STATUS .","title":"Syntax"},{"location":"3.ngql-guide/14.native-index-statements/4.rebuild-native-index/#examples","text":"nebula> CREATE TAG IF NOT EXISTS person(name string, age int, gender string, email string); nebula> CREATE TAG INDEX IF NOT EXISTS single_person_index ON person(name(10)); # The following example rebuilds an index and returns the job ID. nebula> REBUILD TAG INDEX single_person_index; +------------+ | New Job Id | +------------+ | 31 | +------------+ # The following example checks the index status. nebula> SHOW TAG INDEX STATUS; +-----------------------+--------------+ | Name | Index Status | +-----------------------+--------------+ | \"single_person_index\" | \"FINISHED\" | +-----------------------+--------------+ # You can also use \"SHOW JOB <job_id>\" to check if the rebuilding process is complete. nebula> SHOW JOB 31; +----------------+---------------------+------------+-------------------------+-------------------------+-------------+ | Job Id(TaskId) | Command(Dest) | Status | Start Time | Stop Time | Error Code | +----------------+---------------------+------------+-------------------------+-------------------------+-------------+ | 31 | \"REBUILD_TAG_INDEX\" | \"FINISHED\" | 2021-07-07T09:04:24.000 | 2021-07-07T09:04:24.000 | \"SUCCEEDED\" | | 0 | \"storaged1\" | \"FINISHED\" | 2021-07-07T09:04:24.000 | 2021-07-07T09:04:28.000 | \"SUCCEEDED\" | | 1 | \"storaged2\" | \"FINISHED\" | 2021-07-07T09:04:24.000 | 2021-07-07T09:04:28.000 | \"SUCCEEDED\" | | 2 | \"storaged0\" | \"FINISHED\" | 2021-07-07T09:04:24.000 | 2021-07-07T09:04:28.000 | \"SUCCEEDED\" | +----------------+---------------------+------------+-------------------------+-------------------------+-------------+ Nebula Graph creates a job to rebuild the index. The job ID is displayed in the preceding return message. To check if the rebuilding process is complete, use the SHOW JOB <job_id> statement. For more information, see SHOW JOB .","title":"Examples"},{"location":"3.ngql-guide/14.native-index-statements/5.show-native-index-status/","text":"SHOW INDEX STATUS \u00b6 SHOW INDEX STATUS returns the name of the created tag or edge type index and its status of job. The index status includes: QUEUE : The job is in a queue. RUNNING : The job is running. FINISHED : The job is finished. FAILED : The job has failed. STOPPED : The job has stopped. INVALID : The job is invalid. Note For details on how to create an index, see CREATE INDEX . Syntax \u00b6 SHOW {TAG | EDGE} INDEX STATUS; Example \u00b6 nebula> SHOW TAG INDEX STATUS; +----------------------+--------------+ | Name | Index Status | +----------------------+--------------+ | \"player_index_0\" | \"FINISHED\" | | \"player_index_1\" | \"FINISHED\" | +----------------------+--------------+","title":"SHOW INDEX STATUS"},{"location":"3.ngql-guide/14.native-index-statements/5.show-native-index-status/#show_index_status","text":"SHOW INDEX STATUS returns the name of the created tag or edge type index and its status of job. The index status includes: QUEUE : The job is in a queue. RUNNING : The job is running. FINISHED : The job is finished. FAILED : The job has failed. STOPPED : The job has stopped. INVALID : The job is invalid. Note For details on how to create an index, see CREATE INDEX .","title":"SHOW INDEX STATUS"},{"location":"3.ngql-guide/14.native-index-statements/5.show-native-index-status/#syntax","text":"SHOW {TAG | EDGE} INDEX STATUS;","title":"Syntax"},{"location":"3.ngql-guide/14.native-index-statements/5.show-native-index-status/#example","text":"nebula> SHOW TAG INDEX STATUS; +----------------------+--------------+ | Name | Index Status | +----------------------+--------------+ | \"player_index_0\" | \"FINISHED\" | | \"player_index_1\" | \"FINISHED\" | +----------------------+--------------+","title":"Example"},{"location":"3.ngql-guide/14.native-index-statements/6.drop-native-index/","text":"DROP INDEX \u00b6 DROP INDEX removes an existing index from the current graph space. Prerequisite \u00b6 Running the DROP INDEX statement requires some privileges of DROP TAG INDEX and DROP EDGE INDEX in the given graph space. Otherwise, Nebula Graph throws an error. Syntax \u00b6 DROP {TAG | EDGE} INDEX [IF EXISTS] <index_name>; IF NOT EXISTS : Detects whether the index that you want to drop exists. If it exists, it will be dropped. Example \u00b6 nebula> DROP TAG INDEX player_index_0;","title":"DROP INDEX"},{"location":"3.ngql-guide/14.native-index-statements/6.drop-native-index/#drop_index","text":"DROP INDEX removes an existing index from the current graph space.","title":"DROP INDEX"},{"location":"3.ngql-guide/14.native-index-statements/6.drop-native-index/#prerequisite","text":"Running the DROP INDEX statement requires some privileges of DROP TAG INDEX and DROP EDGE INDEX in the given graph space. Otherwise, Nebula Graph throws an error.","title":"Prerequisite"},{"location":"3.ngql-guide/14.native-index-statements/6.drop-native-index/#syntax","text":"DROP {TAG | EDGE} INDEX [IF EXISTS] <index_name>; IF NOT EXISTS : Detects whether the index that you want to drop exists. If it exists, it will be dropped.","title":"Syntax"},{"location":"3.ngql-guide/14.native-index-statements/6.drop-native-index/#example","text":"nebula> DROP TAG INDEX player_index_0;","title":"Example"},{"location":"3.ngql-guide/15.full-text-index-statements/1.search-with-text-based-index/","text":"Full-text indexes \u00b6 Full-text indexes are used to do prefix, wildcard, regexp, and fuzzy search on a string property. You can use the WHERE clause to specify the search strings in LOOKUP statements. Prerequisite \u00b6 Before using the full-text index, make sure that you have deployed a Elasticsearch cluster and a Listener cluster. For more information, see Deploy Elasticsearch and Deploy Listener . Precaution \u00b6 Before using the full-text index, make sure that you know the restrictions . Natural language full-text search \u00b6 A natural language search interprets the search string as a phrase in natural human language. The search is case-insensitive. By default, each substring (separated by spaces) will be searched separately. For example, there are three vertices with the tag player . The tag player contains the property name . The name of these three vertices are Kevin Durant , Tim Duncan , and David Beckham . Now that the full-text index of player.name is established, these three vertices will be queried when using the prefix search statement LOOKUP ON player WHERE PREFIX(player.name,\"d\"); . Syntax \u00b6 Create full-text indexes \u00b6 CREATE FULLTEXT {TAG | EDGE} INDEX <index_name> ON {<tag_name> | <edge_name>} ([<prop_name_list>]); Show full-text indexes \u00b6 SHOW FULLTEXT INDEXES; Rebuild full-text indexes \u00b6 REBUILD FULLTEXT INDEX; Drop full-text indexes \u00b6 DROP FULLTEXT INDEX <index_name>; Use query options \u00b6 LOOKUP ON {<tag> | <edge_type>} WHERE <expression> [YIELD <return_list>]; <expression> ::= PREFIX | WILDCARD | REGEXP | FUZZY <return_list> <prop_name> [AS <prop_alias>] [, <prop_name> [AS <prop_alias>] ...] PREFIX(schema_name.prop_name, prefix_string, row_limit, timeout) WILDCARD(schema_name.prop_name, wildcard_string, row_limit, timeout) REGEXP(schema_name.prop_name, regexp_string, row_limit, timeout) FUZZY(schema_name.prop_name, fuzzy_string, fuzziness, operator, row_limit, timeout) fuzziness (optional): Maximum edit distance allowed for matching. The default value is AUTO . For other valid values and more information, see Elasticsearch document . operator (optional): Boolean logic used to interpret the text. Valid values are OR (default) and AND . row_limit (optional): Specifies the number of rows to return. The default value is 100 . timeout (optional): Specifies the timeout time. The default value is 200ms . Examples \u00b6 // This example creates the graph space. nebula> CREATE SPACE IF NOT EXISTS basketballplayer (partition_num=3,replica_factor=1, vid_type=fixed_string(30)); // This example signs in the text service. nebula> SIGN IN TEXT SERVICE (127.0.0.1:9200, HTTP); // This example switches the graph space. nebula> USE basketballplayer; // This example adds the listener to the Nebula Graph cluster. nebula> ADD LISTENER ELASTICSEARCH 192.168.8.5:9789; // This example creates the tag. nebula> CREATE TAG IF NOT EXISTS player(name string, age int); // This example creates the native index. nebula> CREATE TAG INDEX IF NOT EXISTS name ON player(name(20)); // This example rebuilds the native index. nebula> REBUILD TAG INDEX; // This example creates the full-text index. The index name starts with \"nebula\". nebula> CREATE FULLTEXT TAG INDEX nebula_index_1 ON player(name); // This example rebuilds the full-text index. nebula> REBUILD FULLTEXT INDEX; // This example shows the full-text index. nebula> SHOW FULLTEXT INDEXES; +------------------+-------------+-------------+--------+ | Name | Schema Type | Schema Name | Fields | +------------------+-------------+-------------+--------+ | \"nebula_index_1\" | \"Tag\" | \"player\" | \"name\" | +------------------+-------------+-------------+--------+ // This example inserts the test data. nebula> INSERT VERTEX player(name, age) VALUES \\ \"Russell Westbrook\": (\"Russell Westbrook\", 30), \\ \"Chris Paul\": (\"Chris Paul\", 33),\\ \"Boris Diaw\": (\"Boris Diaw\", 36),\\ \"David West\": (\"David West\", 38),\\ \"Danny Green\": (\"Danny Green\", 31),\\ \"Tim Duncan\": (\"Tim Duncan\", 42),\\ \"James Harden\": (\"James Harden\", 29),\\ \"Tony Parker\": (\"Tony Parker\", 36),\\ \"Aron Baynes\": (\"Aron Baynes\", 32),\\ \"Ben Simmons\": (\"Ben Simmons\", 22),\\ \"Blake Griffin\": (\"Blake Griffin\", 30); // These examples run test queries. nebula> LOOKUP ON player WHERE PREFIX(player.name, \"B\") YIELD id(vertex); +-----------------+ | id(VERTEX) | +-----------------+ | \"Boris Diaw\" | | \"Ben Simmons\" | | \"Blake Griffin\" | +-----------------+ nebula> LOOKUP ON player WHERE WILDCARD(player.name, \"*ri*\") YIELD player.name, player.age; +-----------------+-----+ | name | age | +-----------------+-----+ | \"Chris Paul\" | 33 | | \"Boris Diaw\" | 36 | | \"Blake Griffin\" | 30 | +-----------------+-----+ nebula> LOOKUP ON player WHERE WILDCARD(player.name, \"*ri*\") | YIELD count(*); +----------+ | count(*) | +----------+ | 3 | +----------+ nebula> LOOKUP ON player WHERE REGEXP(player.name, \"R.*\") YIELD player.name, player.age; +---------------------+-----+ | name | age | +---------------------+-----+ | \"Russell Westbrook\" | 30 | +---------------------+-----+ nebula> LOOKUP ON player WHERE REGEXP(player.name, \".*\") YIELD id(vertex); +---------------------+ | id(VERTEX) | +---------------------+ | \"Danny Green\" | | \"David West\" | ... nebula> LOOKUP ON player WHERE FUZZY(player.name, \"Tim Dunncan\", AUTO, OR) YIELD player.name; +--------------+ | name | +--------------+ | \"Tim Duncan\" | +--------------+ // This example drops the full-text index. nebula> DROP FULLTEXT INDEX nebula_index_1;","title":"Search with full-text index"},{"location":"3.ngql-guide/15.full-text-index-statements/1.search-with-text-based-index/#full-text_indexes","text":"Full-text indexes are used to do prefix, wildcard, regexp, and fuzzy search on a string property. You can use the WHERE clause to specify the search strings in LOOKUP statements.","title":"Full-text indexes"},{"location":"3.ngql-guide/15.full-text-index-statements/1.search-with-text-based-index/#prerequisite","text":"Before using the full-text index, make sure that you have deployed a Elasticsearch cluster and a Listener cluster. For more information, see Deploy Elasticsearch and Deploy Listener .","title":"Prerequisite"},{"location":"3.ngql-guide/15.full-text-index-statements/1.search-with-text-based-index/#precaution","text":"Before using the full-text index, make sure that you know the restrictions .","title":"Precaution"},{"location":"3.ngql-guide/15.full-text-index-statements/1.search-with-text-based-index/#natural_language_full-text_search","text":"A natural language search interprets the search string as a phrase in natural human language. The search is case-insensitive. By default, each substring (separated by spaces) will be searched separately. For example, there are three vertices with the tag player . The tag player contains the property name . The name of these three vertices are Kevin Durant , Tim Duncan , and David Beckham . Now that the full-text index of player.name is established, these three vertices will be queried when using the prefix search statement LOOKUP ON player WHERE PREFIX(player.name,\"d\"); .","title":"Natural language full-text search"},{"location":"3.ngql-guide/15.full-text-index-statements/1.search-with-text-based-index/#syntax","text":"","title":"Syntax"},{"location":"3.ngql-guide/15.full-text-index-statements/1.search-with-text-based-index/#create_full-text_indexes","text":"CREATE FULLTEXT {TAG | EDGE} INDEX <index_name> ON {<tag_name> | <edge_name>} ([<prop_name_list>]);","title":"Create full-text indexes"},{"location":"3.ngql-guide/15.full-text-index-statements/1.search-with-text-based-index/#show_full-text_indexes","text":"SHOW FULLTEXT INDEXES;","title":"Show full-text indexes"},{"location":"3.ngql-guide/15.full-text-index-statements/1.search-with-text-based-index/#rebuild_full-text_indexes","text":"REBUILD FULLTEXT INDEX;","title":"Rebuild full-text indexes"},{"location":"3.ngql-guide/15.full-text-index-statements/1.search-with-text-based-index/#drop_full-text_indexes","text":"DROP FULLTEXT INDEX <index_name>;","title":"Drop full-text indexes"},{"location":"3.ngql-guide/15.full-text-index-statements/1.search-with-text-based-index/#use_query_options","text":"LOOKUP ON {<tag> | <edge_type>} WHERE <expression> [YIELD <return_list>]; <expression> ::= PREFIX | WILDCARD | REGEXP | FUZZY <return_list> <prop_name> [AS <prop_alias>] [, <prop_name> [AS <prop_alias>] ...] PREFIX(schema_name.prop_name, prefix_string, row_limit, timeout) WILDCARD(schema_name.prop_name, wildcard_string, row_limit, timeout) REGEXP(schema_name.prop_name, regexp_string, row_limit, timeout) FUZZY(schema_name.prop_name, fuzzy_string, fuzziness, operator, row_limit, timeout) fuzziness (optional): Maximum edit distance allowed for matching. The default value is AUTO . For other valid values and more information, see Elasticsearch document . operator (optional): Boolean logic used to interpret the text. Valid values are OR (default) and AND . row_limit (optional): Specifies the number of rows to return. The default value is 100 . timeout (optional): Specifies the timeout time. The default value is 200ms .","title":"Use query options"},{"location":"3.ngql-guide/15.full-text-index-statements/1.search-with-text-based-index/#examples","text":"// This example creates the graph space. nebula> CREATE SPACE IF NOT EXISTS basketballplayer (partition_num=3,replica_factor=1, vid_type=fixed_string(30)); // This example signs in the text service. nebula> SIGN IN TEXT SERVICE (127.0.0.1:9200, HTTP); // This example switches the graph space. nebula> USE basketballplayer; // This example adds the listener to the Nebula Graph cluster. nebula> ADD LISTENER ELASTICSEARCH 192.168.8.5:9789; // This example creates the tag. nebula> CREATE TAG IF NOT EXISTS player(name string, age int); // This example creates the native index. nebula> CREATE TAG INDEX IF NOT EXISTS name ON player(name(20)); // This example rebuilds the native index. nebula> REBUILD TAG INDEX; // This example creates the full-text index. The index name starts with \"nebula\". nebula> CREATE FULLTEXT TAG INDEX nebula_index_1 ON player(name); // This example rebuilds the full-text index. nebula> REBUILD FULLTEXT INDEX; // This example shows the full-text index. nebula> SHOW FULLTEXT INDEXES; +------------------+-------------+-------------+--------+ | Name | Schema Type | Schema Name | Fields | +------------------+-------------+-------------+--------+ | \"nebula_index_1\" | \"Tag\" | \"player\" | \"name\" | +------------------+-------------+-------------+--------+ // This example inserts the test data. nebula> INSERT VERTEX player(name, age) VALUES \\ \"Russell Westbrook\": (\"Russell Westbrook\", 30), \\ \"Chris Paul\": (\"Chris Paul\", 33),\\ \"Boris Diaw\": (\"Boris Diaw\", 36),\\ \"David West\": (\"David West\", 38),\\ \"Danny Green\": (\"Danny Green\", 31),\\ \"Tim Duncan\": (\"Tim Duncan\", 42),\\ \"James Harden\": (\"James Harden\", 29),\\ \"Tony Parker\": (\"Tony Parker\", 36),\\ \"Aron Baynes\": (\"Aron Baynes\", 32),\\ \"Ben Simmons\": (\"Ben Simmons\", 22),\\ \"Blake Griffin\": (\"Blake Griffin\", 30); // These examples run test queries. nebula> LOOKUP ON player WHERE PREFIX(player.name, \"B\") YIELD id(vertex); +-----------------+ | id(VERTEX) | +-----------------+ | \"Boris Diaw\" | | \"Ben Simmons\" | | \"Blake Griffin\" | +-----------------+ nebula> LOOKUP ON player WHERE WILDCARD(player.name, \"*ri*\") YIELD player.name, player.age; +-----------------+-----+ | name | age | +-----------------+-----+ | \"Chris Paul\" | 33 | | \"Boris Diaw\" | 36 | | \"Blake Griffin\" | 30 | +-----------------+-----+ nebula> LOOKUP ON player WHERE WILDCARD(player.name, \"*ri*\") | YIELD count(*); +----------+ | count(*) | +----------+ | 3 | +----------+ nebula> LOOKUP ON player WHERE REGEXP(player.name, \"R.*\") YIELD player.name, player.age; +---------------------+-----+ | name | age | +---------------------+-----+ | \"Russell Westbrook\" | 30 | +---------------------+-----+ nebula> LOOKUP ON player WHERE REGEXP(player.name, \".*\") YIELD id(vertex); +---------------------+ | id(VERTEX) | +---------------------+ | \"Danny Green\" | | \"David West\" | ... nebula> LOOKUP ON player WHERE FUZZY(player.name, \"Tim Dunncan\", AUTO, OR) YIELD player.name; +--------------+ | name | +--------------+ | \"Tim Duncan\" | +--------------+ // This example drops the full-text index. nebula> DROP FULLTEXT INDEX nebula_index_1;","title":"Examples"},{"location":"3.ngql-guide/16.subgraph-and-path/1.get-subgraph/","text":"GET SUBGRAPH \u00b6 The GET SUBGRAPH statement retrieves information of vertices and edges reachable from the source vertices of the specified edge types and returns information of the subgraph. Syntax \u00b6 GET SUBGRAPH [WITH PROP] [<step_count> STEPS] FROM {<vid>, <vid>...} [{IN | OUT | BOTH} <edge_type>, <edge_type>...] YIELD {[VERTICES AS <vertex_alias>] [,EDGES AS <edge_alias>]}; WITH PROP shows the properties. If not specified, the properties will be hidden. step_count specifies the number of hops from the source vertices and returns the subgraph from 0 to step_count hops. It must be a non-negative integer. Its default value is 1. vid specifies the vertex IDs. edge_type specifies the edge type. You can use IN , OUT , and BOTH to specify the traversal direction of the edge type. The default is BOTH . YIELD defines the output that needs to be returned. You can return only vertices or edges. A column alias must be set. Note The path type of GET SUBGRAPH is trail . Only vertices can be repeatedly visited in graph traversal. For more information, see Path . Examples \u00b6 The following graph is used as the sample. Insert the test data: nebula> CREATE SPACE IF NOT EXISTS subgraph(partition_num=15, replica_factor=1, vid_type=fixed_string(30)); nebula> USE subgraph; nebula> CREATE TAG IF NOT EXISTS player(name string, age int); nebula> CREATE TAG IF NOT EXISTS team(name string); nebula> CREATE EDGE IF NOT EXISTS follow(degree int); nebula> CREATE EDGE IF NOT EXISTS serve(start_year int, end_year int); nebula> INSERT VERTEX player(name, age) VALUES \"player100\":(\"Tim Duncan\", 42); nebula> INSERT VERTEX player(name, age) VALUES \"player101\":(\"Tony Parker\", 36); nebula> INSERT VERTEX player(name, age) VALUES \"player102\":(\"LaMarcus Aldridge\", 33); nebula> INSERT VERTEX team(name) VALUES \"team203\":(\"Trail Blazers\"), \"team204\":(\"Spurs\"); nebula> INSERT EDGE follow(degree) VALUES \"player101\" -> \"player100\":(95); nebula> INSERT EDGE follow(degree) VALUES \"player101\" -> \"player102\":(90); nebula> INSERT EDGE follow(degree) VALUES \"player102\" -> \"player100\":(75); nebula> INSERT EDGE serve(start_year, end_year) VALUES \"player101\" -> \"team204\":(1999, 2018),\"player102\" -> \"team203\":(2006, 2015); This example goes one step from the vertex player101 over all edge types and gets the subgraph. nebula> GET SUBGRAPH 1 STEPS FROM \"player101\" YIELD VERTICES AS nodes, EDGES AS relationships; +-------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------+ | nodes | relationships | +-------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------+ | [(\"player101\" :player{})] | [[:serve \"player101\"->\"team204\" @0 {}], [:follow \"player101\"->\"player100\" @0 {}], [:follow \"player101\"->\"player102\" @0 {}]] | | [(\"team204\" :team{}), (\"player100\" :player{}), (\"player102\" :player{})] | [[:follow \"player102\"->\"player100\" @0 {}]] | +-------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------+ The returned subgraph is as follows. This example goes one step from the vertex player101 over incoming follow edges and gets the subgraph. nebula> GET SUBGRAPH 1 STEPS FROM \"player101\" IN follow YIELD VERTICES AS nodes, EDGES AS relationships; +---------------------------+---------------+ | nodes | relationships | +---------------------------+---------------+ | [(\"player101\" :player{})] | [] | | [] | [] | +---------------------------+---------------+ There is no incoming follow edge to player101 , so only the vertex player101 is returned. This example goes one step from the vertex player101 over outgoing serve edges, gets the subgraph, and shows the property of the edge. nebula> GET SUBGRAPH WITH PROP 1 STEPS FROM \"player101\" OUT serve YIELD VERTICES AS nodes, EDGES AS relationships; +-------------------------------------------------------+-------------------------------------------------------------------------+ | nodes | relationships | +-------------------------------------------------------+-------------------------------------------------------------------------+ | [(\"player101\" :player{age: 36, name: \"Tony Parker\"})] | [[:serve \"player101\"->\"team204\" @0 {end_year: 2018, start_year: 1999}]] | | [(\"team204\" :team{name: \"Spurs\"})] | [] | +-------------------------------------------------------+-------------------------------------------------------------------------+ The returned subgraph is as follows. FAQ \u00b6 Why is the number of hops in the returned result greater than step_count ? \u00b6 To show the completeness of the subgraph, an additional hop is made on all vertices that meet the conditions. The following graph is used as the sample. The returned paths of GET SUBGRAPH 1 STEPS FROM \"A\"; are A->B , B->A , and A->C . To show the completeness of the subgraph, an additional hop is made on all vertices that meet the conditions, namely B->C . The returned path of GET SUBGRAPH 1 STEPS FROM \"A\" IN follow; is B->A . To show the completeness of the subgraph, an additional hop is made on all vertices that meet the conditions, namely A->B . If you only query paths or vertices that meet the conditions, we suggest you use MATCH or GO . The example is as follows. nebula> MATCH p= (v:player) -- (v2) WHERE id(v)==\"A\" RETURN p; nebula> GO 1 STEPS FROM \"A\" OVER follow YIELD id(vertex); Why is the number of hops in the returned result lower than step_count ? \u00b6 The query stops when there is not enough subgraph data and will not return the null value. nebula> GET SUBGRAPH 100 STEPS FROM \"player101\" OUT follow YIELD VERTICES AS nodes, EDGES AS relationships; +----------------------------------------------------+--------------------------------------------------------------------------------------+ | nodes | relationships | +----------------------------------------------------+--------------------------------------------------------------------------------------+ | [(\"player101\" :player{})] | [[:follow \"player101\"->\"player100\" @0 {}], [:follow \"player101\"->\"player102\" @0 {}]] | | [(\"player100\" :player{}), (\"player102\" :player{})] | [[:follow \"player102\"->\"player100\" @0 {}]] | +----------------------------------------------------+--------------------------------------------------------------------------------------+","title":"GET SUBGRAPH"},{"location":"3.ngql-guide/16.subgraph-and-path/1.get-subgraph/#get_subgraph","text":"The GET SUBGRAPH statement retrieves information of vertices and edges reachable from the source vertices of the specified edge types and returns information of the subgraph.","title":"GET SUBGRAPH"},{"location":"3.ngql-guide/16.subgraph-and-path/1.get-subgraph/#syntax","text":"GET SUBGRAPH [WITH PROP] [<step_count> STEPS] FROM {<vid>, <vid>...} [{IN | OUT | BOTH} <edge_type>, <edge_type>...] YIELD {[VERTICES AS <vertex_alias>] [,EDGES AS <edge_alias>]}; WITH PROP shows the properties. If not specified, the properties will be hidden. step_count specifies the number of hops from the source vertices and returns the subgraph from 0 to step_count hops. It must be a non-negative integer. Its default value is 1. vid specifies the vertex IDs. edge_type specifies the edge type. You can use IN , OUT , and BOTH to specify the traversal direction of the edge type. The default is BOTH . YIELD defines the output that needs to be returned. You can return only vertices or edges. A column alias must be set. Note The path type of GET SUBGRAPH is trail . Only vertices can be repeatedly visited in graph traversal. For more information, see Path .","title":"Syntax"},{"location":"3.ngql-guide/16.subgraph-and-path/1.get-subgraph/#examples","text":"The following graph is used as the sample. Insert the test data: nebula> CREATE SPACE IF NOT EXISTS subgraph(partition_num=15, replica_factor=1, vid_type=fixed_string(30)); nebula> USE subgraph; nebula> CREATE TAG IF NOT EXISTS player(name string, age int); nebula> CREATE TAG IF NOT EXISTS team(name string); nebula> CREATE EDGE IF NOT EXISTS follow(degree int); nebula> CREATE EDGE IF NOT EXISTS serve(start_year int, end_year int); nebula> INSERT VERTEX player(name, age) VALUES \"player100\":(\"Tim Duncan\", 42); nebula> INSERT VERTEX player(name, age) VALUES \"player101\":(\"Tony Parker\", 36); nebula> INSERT VERTEX player(name, age) VALUES \"player102\":(\"LaMarcus Aldridge\", 33); nebula> INSERT VERTEX team(name) VALUES \"team203\":(\"Trail Blazers\"), \"team204\":(\"Spurs\"); nebula> INSERT EDGE follow(degree) VALUES \"player101\" -> \"player100\":(95); nebula> INSERT EDGE follow(degree) VALUES \"player101\" -> \"player102\":(90); nebula> INSERT EDGE follow(degree) VALUES \"player102\" -> \"player100\":(75); nebula> INSERT EDGE serve(start_year, end_year) VALUES \"player101\" -> \"team204\":(1999, 2018),\"player102\" -> \"team203\":(2006, 2015); This example goes one step from the vertex player101 over all edge types and gets the subgraph. nebula> GET SUBGRAPH 1 STEPS FROM \"player101\" YIELD VERTICES AS nodes, EDGES AS relationships; +-------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------+ | nodes | relationships | +-------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------+ | [(\"player101\" :player{})] | [[:serve \"player101\"->\"team204\" @0 {}], [:follow \"player101\"->\"player100\" @0 {}], [:follow \"player101\"->\"player102\" @0 {}]] | | [(\"team204\" :team{}), (\"player100\" :player{}), (\"player102\" :player{})] | [[:follow \"player102\"->\"player100\" @0 {}]] | +-------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------+ The returned subgraph is as follows. This example goes one step from the vertex player101 over incoming follow edges and gets the subgraph. nebula> GET SUBGRAPH 1 STEPS FROM \"player101\" IN follow YIELD VERTICES AS nodes, EDGES AS relationships; +---------------------------+---------------+ | nodes | relationships | +---------------------------+---------------+ | [(\"player101\" :player{})] | [] | | [] | [] | +---------------------------+---------------+ There is no incoming follow edge to player101 , so only the vertex player101 is returned. This example goes one step from the vertex player101 over outgoing serve edges, gets the subgraph, and shows the property of the edge. nebula> GET SUBGRAPH WITH PROP 1 STEPS FROM \"player101\" OUT serve YIELD VERTICES AS nodes, EDGES AS relationships; +-------------------------------------------------------+-------------------------------------------------------------------------+ | nodes | relationships | +-------------------------------------------------------+-------------------------------------------------------------------------+ | [(\"player101\" :player{age: 36, name: \"Tony Parker\"})] | [[:serve \"player101\"->\"team204\" @0 {end_year: 2018, start_year: 1999}]] | | [(\"team204\" :team{name: \"Spurs\"})] | [] | +-------------------------------------------------------+-------------------------------------------------------------------------+ The returned subgraph is as follows.","title":"Examples"},{"location":"3.ngql-guide/16.subgraph-and-path/1.get-subgraph/#faq","text":"","title":"FAQ"},{"location":"3.ngql-guide/16.subgraph-and-path/1.get-subgraph/#why_is_the_number_of_hops_in_the_returned_result_greater_than_step_count","text":"To show the completeness of the subgraph, an additional hop is made on all vertices that meet the conditions. The following graph is used as the sample. The returned paths of GET SUBGRAPH 1 STEPS FROM \"A\"; are A->B , B->A , and A->C . To show the completeness of the subgraph, an additional hop is made on all vertices that meet the conditions, namely B->C . The returned path of GET SUBGRAPH 1 STEPS FROM \"A\" IN follow; is B->A . To show the completeness of the subgraph, an additional hop is made on all vertices that meet the conditions, namely A->B . If you only query paths or vertices that meet the conditions, we suggest you use MATCH or GO . The example is as follows. nebula> MATCH p= (v:player) -- (v2) WHERE id(v)==\"A\" RETURN p; nebula> GO 1 STEPS FROM \"A\" OVER follow YIELD id(vertex);","title":"Why is the number of hops in the returned result greater than step_count?"},{"location":"3.ngql-guide/16.subgraph-and-path/1.get-subgraph/#why_is_the_number_of_hops_in_the_returned_result_lower_than_step_count","text":"The query stops when there is not enough subgraph data and will not return the null value. nebula> GET SUBGRAPH 100 STEPS FROM \"player101\" OUT follow YIELD VERTICES AS nodes, EDGES AS relationships; +----------------------------------------------------+--------------------------------------------------------------------------------------+ | nodes | relationships | +----------------------------------------------------+--------------------------------------------------------------------------------------+ | [(\"player101\" :player{})] | [[:follow \"player101\"->\"player100\" @0 {}], [:follow \"player101\"->\"player102\" @0 {}]] | | [(\"player100\" :player{}), (\"player102\" :player{})] | [[:follow \"player102\"->\"player100\" @0 {}]] | +----------------------------------------------------+--------------------------------------------------------------------------------------+","title":"Why is the number of hops in the returned result lower than step_count?"},{"location":"3.ngql-guide/16.subgraph-and-path/2.find-path/","text":"FIND PATH \u00b6 The FIND PATH statement finds the paths between the selected source vertices and destination vertices. Syntax \u00b6 FIND { SHORTEST | ALL | NOLOOP } PATH [WITH PROP] FROM <vertex_id_list> TO <vertex_id_list> OVER <edge_type_list> [REVERSELY | BIDIRECT] [<WHERE clause>] [UPTO <N> STEPS] YIELD path as <alias> [| ORDER BY $-.path] [| LIMIT <M>]; <vertex_id_list> ::= [vertex_id [, vertex_id] ...] SHORTEST finds the shortest path. ALL finds all the paths. NOLOOP finds the paths without circles. WITH PROP shows properties of vertices and edges. If not specified, properties will be hidden. <vertex_id_list> is a list of vertex IDs separated with commas (,). It supports $- and $var . <edge_type_list> is a list of edge types separated with commas (,). * is all edge types. REVERSELY | BIDIRECT specifies the direction. REVERSELY is reverse graph traversal while BIDIRECT is bidirectional graph traversal. <WHERE clause> filters properties of edges. <N> is the maximum hop number of the path. The default value is 5 . <M> specifies the maximum number of rows to return. Note The path type of FIND PATH is trail . Only vertices can be repeatedly visited in graph traversal. For more information, see Path . Limitations \u00b6 When a list of source and/or destination vertex IDs are specified, the paths between any source vertices and the destination vertices will be returned. There can be cycles when searching all paths. FIND PATH only supports filtering properties of edges with WHERE clauses. Filtering properties of vertices and functions are not supported for now. FIND PATH is a single-thread procedure, so it uses much memory. Examples \u00b6 A returned path is like (<vertex_id>)-[:<edge_type_name>@<rank>]->(<vertex_id) . nebula> FIND SHORTEST PATH FROM \"player102\" TO \"team204\" OVER * YIELD path AS p; +--------------------------------------------+ | p | +--------------------------------------------+ | <(\"player102\")-[:serve@0 {}]->(\"team204\")> | +--------------------------------------------+ nebula> FIND SHORTEST PATH WITH PROP FROM \"team204\" TO \"player100\" OVER * REVERSELY YIELD path AS p; +--------------------------------------------------------------------------------------------------------------------------------------+ | p | +--------------------------------------------------------------------------------------------------------------------------------------+ | <(\"team204\" :team{name: \"Spurs\"})<-[:serve@0 {end_year: 2016, start_year: 1997}]-(\"player100\" :player{age: 42, name: \"Tim Duncan\"})> | +--------------------------------------------------------------------------------------------------------------------------------------+ nebula> FIND ALL PATH FROM \"player100\" TO \"team204\" OVER * WHERE follow.degree is EMPTY or follow.degree >=0 YIELD path AS p; +------------------------------------------------------------------------------+ | p | +------------------------------------------------------------------------------+ | <(\"player100\")-[:serve@0 {}]->(\"team204\")> | | <(\"player100\")-[:follow@0 {}]->(\"player125\")-[:serve@0 {}]->(\"team204\")> | | <(\"player100\")-[:follow@0 {}]->(\"player101\")-[:serve@0 {}]->(\"team204\")> | |... | +------------------------------------------------------------------------------+ nebula> FIND NOLOOP PATH FROM \"player100\" TO \"team204\" OVER * YIELD path AS p; +--------------------------------------------------------------------------------------------------------+ | p | +--------------------------------------------------------------------------------------------------------+ | <(\"player100\")-[:serve@0 {}]->(\"team204\")> | | <(\"player100\")-[:follow@0 {}]->(\"player125\")-[:serve@0 {}]->(\"team204\")> | | <(\"player100\")-[:follow@0 {}]->(\"player101\")-[:serve@0 {}]->(\"team204\")> | | <(\"player100\")-[:follow@0 {}]->(\"player101\")-[:follow@0 {}]->(\"player125\")-[:serve@0 {}]->(\"team204\")> | | <(\"player100\")-[:follow@0 {}]->(\"player101\")-[:follow@0 {}]->(\"player102\")-[:serve@0 {}]->(\"team204\")> | +--------------------------------------------------------------------------------------------------------+ FAQ \u00b6 Does it support the WHERE clause to achieve conditional filtering during graph traversal? \u00b6 FIND PATH only supports filtering properties of edges with WHERE clauses, such as FIND ALL PATH FROM \"player100\" TO \"team204\" OVER * WHERE follow.degree is EMPTY or follow.degree >=0; . Filtering properties of vertices is not supported for now.","title":"FIND PATH"},{"location":"3.ngql-guide/16.subgraph-and-path/2.find-path/#find_path","text":"The FIND PATH statement finds the paths between the selected source vertices and destination vertices.","title":"FIND PATH"},{"location":"3.ngql-guide/16.subgraph-and-path/2.find-path/#syntax","text":"FIND { SHORTEST | ALL | NOLOOP } PATH [WITH PROP] FROM <vertex_id_list> TO <vertex_id_list> OVER <edge_type_list> [REVERSELY | BIDIRECT] [<WHERE clause>] [UPTO <N> STEPS] YIELD path as <alias> [| ORDER BY $-.path] [| LIMIT <M>]; <vertex_id_list> ::= [vertex_id [, vertex_id] ...] SHORTEST finds the shortest path. ALL finds all the paths. NOLOOP finds the paths without circles. WITH PROP shows properties of vertices and edges. If not specified, properties will be hidden. <vertex_id_list> is a list of vertex IDs separated with commas (,). It supports $- and $var . <edge_type_list> is a list of edge types separated with commas (,). * is all edge types. REVERSELY | BIDIRECT specifies the direction. REVERSELY is reverse graph traversal while BIDIRECT is bidirectional graph traversal. <WHERE clause> filters properties of edges. <N> is the maximum hop number of the path. The default value is 5 . <M> specifies the maximum number of rows to return. Note The path type of FIND PATH is trail . Only vertices can be repeatedly visited in graph traversal. For more information, see Path .","title":"Syntax"},{"location":"3.ngql-guide/16.subgraph-and-path/2.find-path/#limitations","text":"When a list of source and/or destination vertex IDs are specified, the paths between any source vertices and the destination vertices will be returned. There can be cycles when searching all paths. FIND PATH only supports filtering properties of edges with WHERE clauses. Filtering properties of vertices and functions are not supported for now. FIND PATH is a single-thread procedure, so it uses much memory.","title":"Limitations"},{"location":"3.ngql-guide/16.subgraph-and-path/2.find-path/#examples","text":"A returned path is like (<vertex_id>)-[:<edge_type_name>@<rank>]->(<vertex_id) . nebula> FIND SHORTEST PATH FROM \"player102\" TO \"team204\" OVER * YIELD path AS p; +--------------------------------------------+ | p | +--------------------------------------------+ | <(\"player102\")-[:serve@0 {}]->(\"team204\")> | +--------------------------------------------+ nebula> FIND SHORTEST PATH WITH PROP FROM \"team204\" TO \"player100\" OVER * REVERSELY YIELD path AS p; +--------------------------------------------------------------------------------------------------------------------------------------+ | p | +--------------------------------------------------------------------------------------------------------------------------------------+ | <(\"team204\" :team{name: \"Spurs\"})<-[:serve@0 {end_year: 2016, start_year: 1997}]-(\"player100\" :player{age: 42, name: \"Tim Duncan\"})> | +--------------------------------------------------------------------------------------------------------------------------------------+ nebula> FIND ALL PATH FROM \"player100\" TO \"team204\" OVER * WHERE follow.degree is EMPTY or follow.degree >=0 YIELD path AS p; +------------------------------------------------------------------------------+ | p | +------------------------------------------------------------------------------+ | <(\"player100\")-[:serve@0 {}]->(\"team204\")> | | <(\"player100\")-[:follow@0 {}]->(\"player125\")-[:serve@0 {}]->(\"team204\")> | | <(\"player100\")-[:follow@0 {}]->(\"player101\")-[:serve@0 {}]->(\"team204\")> | |... | +------------------------------------------------------------------------------+ nebula> FIND NOLOOP PATH FROM \"player100\" TO \"team204\" OVER * YIELD path AS p; +--------------------------------------------------------------------------------------------------------+ | p | +--------------------------------------------------------------------------------------------------------+ | <(\"player100\")-[:serve@0 {}]->(\"team204\")> | | <(\"player100\")-[:follow@0 {}]->(\"player125\")-[:serve@0 {}]->(\"team204\")> | | <(\"player100\")-[:follow@0 {}]->(\"player101\")-[:serve@0 {}]->(\"team204\")> | | <(\"player100\")-[:follow@0 {}]->(\"player101\")-[:follow@0 {}]->(\"player125\")-[:serve@0 {}]->(\"team204\")> | | <(\"player100\")-[:follow@0 {}]->(\"player101\")-[:follow@0 {}]->(\"player102\")-[:serve@0 {}]->(\"team204\")> | +--------------------------------------------------------------------------------------------------------+","title":"Examples"},{"location":"3.ngql-guide/16.subgraph-and-path/2.find-path/#faq","text":"","title":"FAQ"},{"location":"3.ngql-guide/16.subgraph-and-path/2.find-path/#does_it_support_the_where_clause_to_achieve_conditional_filtering_during_graph_traversal","text":"FIND PATH only supports filtering properties of edges with WHERE clauses, such as FIND ALL PATH FROM \"player100\" TO \"team204\" OVER * WHERE follow.degree is EMPTY or follow.degree >=0; . Filtering properties of vertices is not supported for now.","title":"Does it support the WHERE clause to achieve conditional filtering during graph traversal?"},{"location":"3.ngql-guide/17.query-tuning-statements/1.explain-and-profile/","text":"EXPLAIN and PROFILE \u00b6 EXPLAIN helps output the execution plan of an nGQL statement without executing the statement. PROFILE executes the statement, then outputs the execution plan as well as the execution profile. You can optimize the queries for better performance according to the execution plan and profile. Execution Plan \u00b6 The execution plan is determined by the execution planner in the Nebula Graph query engine. The execution planner processes the parsed nGQL statements into actions . An action is the smallest unit that can be executed. A typical action fetches all neighbors of a given vertex, gets the properties of an edge, and filters vertices or edges based on the given conditions. Each action is assigned to an operator that performs the action. For example, a SHOW TAGS statement is processed into two actions and assigned to a Start operator and a ShowTags operator , while a more complex GO statement may be processed into more than 10 actions and assigned to 10 operators. Syntax \u00b6 EXPLAIN EXPLAIN [format=\"row\" | \"dot\"] <your_nGQL_statement>; PROFILE PROFILE [format=\"row\" | \"dot\"] <your_nGQL_statement>; Output formats \u00b6 The output of an EXPLAIN or a PROFILE statement has two formats, the default row format and the dot format. You can use the format option to modify the output format. Omitting the format option indicates using the default row format. The row format \u00b6 The row format outputs the return message in a table as follows. EXPLAIN nebula> EXPLAIN format=\"row\" SHOW TAGS; Execution succeeded (time spent 327/892 us) Execution Plan -----+----------+--------------+----------------+---------------------------------------------------------------------- | id | name | dependencies | profiling data | operator info | -----+----------+--------------+----------------+---------------------------------------------------------------------- | 1 | ShowTags | 0 | | outputVar: [{\"colNames\":[],\"name\":\"__ShowTags_1\",\"type\":\"DATASET\"}] | | | | | | inputVar: | -----+----------+--------------+----------------+---------------------------------------------------------------------- | 0 | Start | | | outputVar: [{\"colNames\":[],\"name\":\"__Start_0\",\"type\":\"DATASET\"}] | -----+----------+--------------+----------------+---------------------------------------------------------------------- PROFILE nebula> PROFILE format=\"row\" SHOW TAGS; +--------+ | Name | +--------+ | player | +--------+ | team | +--------+ Got 2 rows (time spent 2038/2728 us) Execution Plan -----+----------+--------------+----------------------------------------------------+---------------------------------------------------------------------- | id | name | dependencies | profiling data | operator info | -----+----------+--------------+----------------------------------------------------+---------------------------------------------------------------------- | 1 | ShowTags | 0 | ver: 0, rows: 1, execTime: 42us, totalTime: 1177us | outputVar: [{\"colNames\":[],\"name\":\"__ShowTags_1\",\"type\":\"DATASET\"}] | | | | | | inputVar: | -----+----------+--------------+----------------------------------------------------+---------------------------------------------------------------------- | 0 | Start | | ver: 0, rows: 0, execTime: 1us, totalTime: 57us | outputVar: [{\"colNames\":[],\"name\":\"__Start_0\",\"type\":\"DATASET\"}] | -----+----------+--------------+----------------------------------------------------+---------------------------------------------------------------------- The descriptions are as follows. Parameter Description id The ID of the operator . name The name of the operator . dependencies The ID of the operator that the current operator depends on. profiling data The content of the execution profile. ver is the version of the operator . rows shows the number of rows to be output by the operator . execTime shows the execution time of action . totalTime is the sum of the execution time, the system scheduling time, and the queueing time. operator info The detailed information of the operator . The dot format \u00b6 You can use the format=\"dot\" option to output the return message in the dot language, and then use Graphviz to generate a graph of the plan. Note Graphviz is open source graph visualization software. Graphviz provides an online tool for previewing DOT language files and exporting them to other formats such as SVG or JSON. For more information, see Graphviz Online . nebula> EXPLAIN format=\"dot\" SHOW TAGS; Execution succeeded (time spent 161/665 us) Execution Plan --------------------------------------------------------------------------------------------------------------------------------------------- ------------- plan --------------------------------------------------------------------------------------------------------------------------------------------- ------------- digraph exec_plan { rankdir=LR; \"ShowTags_0\"[label=\"ShowTags_0|outputVar: \\[\\{\\\"colNames\\\":\\[\\],\\\"name\\\":\\\"__ShowTags_0\\\",\\\"type\\\":\\\"DATASET\\\"\\}\\]\\l|inputVar:\\l\", shape=Mrecord]; \"Start_2\"->\"ShowTags_0\"; \"Start_2\"[label=\"Start_2|outputVar: \\[\\{\\\"colNames\\\":\\[\\],\\\"name\\\":\\\"__Start_2\\\",\\\"type\\\":\\\"DATASET\\\"\\}\\]\\l|inputVar: \\l\", shape=Mrecord]; } --------------------------------------------------------------------------------------------------------------------------------------------- ------------- The Graphviz graph transformed from the above DOT statement is as follows.","title":"EXPLAIN and PROFILE"},{"location":"3.ngql-guide/17.query-tuning-statements/1.explain-and-profile/#explain_and_profile","text":"EXPLAIN helps output the execution plan of an nGQL statement without executing the statement. PROFILE executes the statement, then outputs the execution plan as well as the execution profile. You can optimize the queries for better performance according to the execution plan and profile.","title":"EXPLAIN and PROFILE"},{"location":"3.ngql-guide/17.query-tuning-statements/1.explain-and-profile/#execution_plan","text":"The execution plan is determined by the execution planner in the Nebula Graph query engine. The execution planner processes the parsed nGQL statements into actions . An action is the smallest unit that can be executed. A typical action fetches all neighbors of a given vertex, gets the properties of an edge, and filters vertices or edges based on the given conditions. Each action is assigned to an operator that performs the action. For example, a SHOW TAGS statement is processed into two actions and assigned to a Start operator and a ShowTags operator , while a more complex GO statement may be processed into more than 10 actions and assigned to 10 operators.","title":"Execution Plan"},{"location":"3.ngql-guide/17.query-tuning-statements/1.explain-and-profile/#syntax","text":"EXPLAIN EXPLAIN [format=\"row\" | \"dot\"] <your_nGQL_statement>; PROFILE PROFILE [format=\"row\" | \"dot\"] <your_nGQL_statement>;","title":"Syntax"},{"location":"3.ngql-guide/17.query-tuning-statements/1.explain-and-profile/#output_formats","text":"The output of an EXPLAIN or a PROFILE statement has two formats, the default row format and the dot format. You can use the format option to modify the output format. Omitting the format option indicates using the default row format.","title":"Output formats"},{"location":"3.ngql-guide/17.query-tuning-statements/1.explain-and-profile/#the_row_format","text":"The row format outputs the return message in a table as follows. EXPLAIN nebula> EXPLAIN format=\"row\" SHOW TAGS; Execution succeeded (time spent 327/892 us) Execution Plan -----+----------+--------------+----------------+---------------------------------------------------------------------- | id | name | dependencies | profiling data | operator info | -----+----------+--------------+----------------+---------------------------------------------------------------------- | 1 | ShowTags | 0 | | outputVar: [{\"colNames\":[],\"name\":\"__ShowTags_1\",\"type\":\"DATASET\"}] | | | | | | inputVar: | -----+----------+--------------+----------------+---------------------------------------------------------------------- | 0 | Start | | | outputVar: [{\"colNames\":[],\"name\":\"__Start_0\",\"type\":\"DATASET\"}] | -----+----------+--------------+----------------+---------------------------------------------------------------------- PROFILE nebula> PROFILE format=\"row\" SHOW TAGS; +--------+ | Name | +--------+ | player | +--------+ | team | +--------+ Got 2 rows (time spent 2038/2728 us) Execution Plan -----+----------+--------------+----------------------------------------------------+---------------------------------------------------------------------- | id | name | dependencies | profiling data | operator info | -----+----------+--------------+----------------------------------------------------+---------------------------------------------------------------------- | 1 | ShowTags | 0 | ver: 0, rows: 1, execTime: 42us, totalTime: 1177us | outputVar: [{\"colNames\":[],\"name\":\"__ShowTags_1\",\"type\":\"DATASET\"}] | | | | | | inputVar: | -----+----------+--------------+----------------------------------------------------+---------------------------------------------------------------------- | 0 | Start | | ver: 0, rows: 0, execTime: 1us, totalTime: 57us | outputVar: [{\"colNames\":[],\"name\":\"__Start_0\",\"type\":\"DATASET\"}] | -----+----------+--------------+----------------------------------------------------+---------------------------------------------------------------------- The descriptions are as follows. Parameter Description id The ID of the operator . name The name of the operator . dependencies The ID of the operator that the current operator depends on. profiling data The content of the execution profile. ver is the version of the operator . rows shows the number of rows to be output by the operator . execTime shows the execution time of action . totalTime is the sum of the execution time, the system scheduling time, and the queueing time. operator info The detailed information of the operator .","title":"The row format"},{"location":"3.ngql-guide/17.query-tuning-statements/1.explain-and-profile/#the_dot_format","text":"You can use the format=\"dot\" option to output the return message in the dot language, and then use Graphviz to generate a graph of the plan. Note Graphviz is open source graph visualization software. Graphviz provides an online tool for previewing DOT language files and exporting them to other formats such as SVG or JSON. For more information, see Graphviz Online . nebula> EXPLAIN format=\"dot\" SHOW TAGS; Execution succeeded (time spent 161/665 us) Execution Plan --------------------------------------------------------------------------------------------------------------------------------------------- ------------- plan --------------------------------------------------------------------------------------------------------------------------------------------- ------------- digraph exec_plan { rankdir=LR; \"ShowTags_0\"[label=\"ShowTags_0|outputVar: \\[\\{\\\"colNames\\\":\\[\\],\\\"name\\\":\\\"__ShowTags_0\\\",\\\"type\\\":\\\"DATASET\\\"\\}\\]\\l|inputVar:\\l\", shape=Mrecord]; \"Start_2\"->\"ShowTags_0\"; \"Start_2\"[label=\"Start_2|outputVar: \\[\\{\\\"colNames\\\":\\[\\],\\\"name\\\":\\\"__Start_2\\\",\\\"type\\\":\\\"DATASET\\\"\\}\\]\\l|inputVar: \\l\", shape=Mrecord]; } --------------------------------------------------------------------------------------------------------------------------------------------- ------------- The Graphviz graph transformed from the above DOT statement is as follows.","title":"The dot format"},{"location":"3.ngql-guide/17.query-tuning-statements/6.kill-query/","text":"Kill queries \u00b6 KILL QUERY can terminate the query being executed, and is often used to terminate slow queries. Note Users with the God role can kill any query. Other roles can only kill their own queries. Syntax \u00b6 KILL QUERY (session=<session_id>, plan=<plan_id>); session_id : The ID of the session. plan_id : The ID of the execution plan. The ID of the session and the ID of the execution plan can uniquely determine a query. Both can be obtained through the SHOW QUERIES statement. Examples \u00b6 This example executes KILL QUERY in one session to terminate the query in another session. nebula> KILL QUERY(SESSION=1625553545984255,PLAN=163); The query will be terminated and the following information will be returned. [ERROR (-1005)]: Execution had been killed","title":"Kill queries"},{"location":"3.ngql-guide/17.query-tuning-statements/6.kill-query/#kill_queries","text":"KILL QUERY can terminate the query being executed, and is often used to terminate slow queries. Note Users with the God role can kill any query. Other roles can only kill their own queries.","title":"Kill queries"},{"location":"3.ngql-guide/17.query-tuning-statements/6.kill-query/#syntax","text":"KILL QUERY (session=<session_id>, plan=<plan_id>); session_id : The ID of the session. plan_id : The ID of the execution plan. The ID of the session and the ID of the execution plan can uniquely determine a query. Both can be obtained through the SHOW QUERIES statement.","title":"Syntax"},{"location":"3.ngql-guide/17.query-tuning-statements/6.kill-query/#examples","text":"This example executes KILL QUERY in one session to terminate the query in another session. nebula> KILL QUERY(SESSION=1625553545984255,PLAN=163); The query will be terminated and the following information will be returned. [ERROR (-1005)]: Execution had been killed","title":"Examples"},{"location":"3.ngql-guide/3.data-types/1.numeric/","text":"Numeric types \u00b6 nGQL supports both integer and floating-point number. Integer \u00b6 Signed 64-bit integer (INT64), 32-bit integer (INT32), 16-bit integer (INT16), and 8-bit integer (INT8) are supported. Type Declared keywords Range INT64 INT64 or INT -9,223,372,036,854,775,808 ~ 9,223,372,036,854,775,807 INT32 INT32 -2,147,483,648 ~ 2,147,483,647 INT16 INT16 -32,768 ~ 32,767 INT8 INT8 -128 ~ 127 Floating-point number \u00b6 Both single-precision floating-point format (FLOAT) and double-precision floating-point format (DOUBLE) are supported. Type Declared keywords Range Precision FLOAT FLOAT 3.4E +/- 38 6~7 bits DOUBLE DOUBLE 1.7E +/- 308 15~16 bits Scientific notation is also supported, such as 1e2 , 1.1e2 , .3e4 , 1.e4 , and -1234E-10 . Note The data type of DECIMAL in MySQL is not supported. Reading and writing of data values \u00b6 When writing and reading different types of data, nGQL complies with the following rules: Data type Set as VID Set as property Resulted data type INT64 Supported Supported INT64 INT32 Not supported Supported INT64 INT16 Not supported Supported INT64 INT8 Not supported Supported INT64 FLOAT Not supported Supported DOUBLE DOUBLE Not supported Supported DOUBLE For example, nGQL does not support setting VID as INT8, but supports setting a certain property type of TAG or Edge type as INT8. When using the nGQL statement to read the property of INT8, the resulted type is INT64. Multiple formats are supported: Decimal, such as 123456 . Hexadecimal, such as 0x1e240 . Octal, such as 0361100 . However, Nebula Graph will parse the written non-decimal value into a decimal value and save it. The value read is decimal. For example, the type of the property score is INT . The value of 0xb is assigned to it through the INSERT statement. If querying the property value with statements such as FETCH, you will get the result 11 , which is the decimal result of the hexadecimal 0xb . Round a FLOAT/DOUBLE value when inserting it to an INT column.","title":"Numeric"},{"location":"3.ngql-guide/3.data-types/1.numeric/#numeric_types","text":"nGQL supports both integer and floating-point number.","title":"Numeric types"},{"location":"3.ngql-guide/3.data-types/1.numeric/#integer","text":"Signed 64-bit integer (INT64), 32-bit integer (INT32), 16-bit integer (INT16), and 8-bit integer (INT8) are supported. Type Declared keywords Range INT64 INT64 or INT -9,223,372,036,854,775,808 ~ 9,223,372,036,854,775,807 INT32 INT32 -2,147,483,648 ~ 2,147,483,647 INT16 INT16 -32,768 ~ 32,767 INT8 INT8 -128 ~ 127","title":"Integer"},{"location":"3.ngql-guide/3.data-types/1.numeric/#floating-point_number","text":"Both single-precision floating-point format (FLOAT) and double-precision floating-point format (DOUBLE) are supported. Type Declared keywords Range Precision FLOAT FLOAT 3.4E +/- 38 6~7 bits DOUBLE DOUBLE 1.7E +/- 308 15~16 bits Scientific notation is also supported, such as 1e2 , 1.1e2 , .3e4 , 1.e4 , and -1234E-10 . Note The data type of DECIMAL in MySQL is not supported.","title":"Floating-point number"},{"location":"3.ngql-guide/3.data-types/1.numeric/#reading_and_writing_of_data_values","text":"When writing and reading different types of data, nGQL complies with the following rules: Data type Set as VID Set as property Resulted data type INT64 Supported Supported INT64 INT32 Not supported Supported INT64 INT16 Not supported Supported INT64 INT8 Not supported Supported INT64 FLOAT Not supported Supported DOUBLE DOUBLE Not supported Supported DOUBLE For example, nGQL does not support setting VID as INT8, but supports setting a certain property type of TAG or Edge type as INT8. When using the nGQL statement to read the property of INT8, the resulted type is INT64. Multiple formats are supported: Decimal, such as 123456 . Hexadecimal, such as 0x1e240 . Octal, such as 0361100 . However, Nebula Graph will parse the written non-decimal value into a decimal value and save it. The value read is decimal. For example, the type of the property score is INT . The value of 0xb is assigned to it through the INSERT statement. If querying the property value with statements such as FETCH, you will get the result 11 , which is the decimal result of the hexadecimal 0xb . Round a FLOAT/DOUBLE value when inserting it to an INT column.","title":"Reading and writing of data values"},{"location":"3.ngql-guide/3.data-types/10.geography/","text":"Geography \u00b6 Geography is a data type composed of latitude and longitude that represents geospatial information. Nebula Graph currently supports Point, LineString, and Polygon in Simple Features and some functions in SQL-MM 3 , such as part of the core geo parsing, construction, formatting, conversion, predicates, and dimensions. Type description \u00b6 A point is the basic data type of geography, which is determined by a latitude and a longitude. For example, \"POINT(3 8)\" means that the longitude is 3\u00b0 and the latitude is 8\u00b0 . Multiple points can form a linestring or a polygon. Shape Example Description Point \"POINT(3 8)\" Specifies the data type as a point. LineString \"LINESTRING(3 8, 4.7 73.23)\" Specifies the data type as a linestring. Polygon \"POLYGON((0 1, 1 2, 2 3, 0 1))\" Specifies the data type as a polygon. Examples \u00b6 For functions about the geography data type, see Geography functions . //Create a Tag to allow storing any geography data type. nebula> CREATE TAG IF NOT EXISTS any_shape(geo geography); //Create a Tag to allow storing a point only. nebula> CREATE TAG IF NOT EXISTS only_point(geo geography(point)); //Create a Tag to allow storing a linestring only. nebula> CREATE TAG IF NOT EXISTS only_linestring(geo geography(linestring)); //Create a Tag to allow storing a polygon only. nebula> CREATE TAG IF NOT EXISTS only_polygon(geo geography(polygon)); //Create an Edge type to allow storing any geography data type. nebula> CREATE EDGE IF NOT EXISTS any_shape_edge(geo geography); //Create a vertex to store the geography of a polygon. nebula> INSERT VERTEX any_shape(geo) VALUES \"103\":(ST_GeogFromText(\"POLYGON((0 1, 1 2, 2 3, 0 1))\")); //Create an edge to store the geography of a polygon. nebula> INSERT EDGE any_shape_edge(geo) VALUES \"201\"->\"302\":(ST_GeogFromText(\"POLYGON((0 1, 1 2, 2 3, 0 1))\")); //Query the geography of Vertex 103. nebula> FETCH PROP ON any_shape \"103\" YIELD ST_ASText(any_shape.geo); +----------+---------------------------------+ | VertexID | ST_ASText(any_shape.geo) | +----------+---------------------------------+ | \"103\" | \"POLYGON((0 1, 1 2, 2 3, 0 1))\" | +----------+---------------------------------+ //Query the geography of the edge which traverses from Vertex 201 to Vertex 302. nebula> FETCH PROP ON any_shape_edge \"201\"->\"302\" YIELD ST_ASText(any_shape_edge.geo); +---------------------+---------------------+----------------------+---------------------------------+ | any_shape_edge._src | any_shape_edge._dst | any_shape_edge._rank | ST_ASText(any_shape_edge.geo) | +---------------------+---------------------+----------------------+---------------------------------+ | \"201\" | \"302\" | 0 | \"POLYGON((0 1, 1 2, 2 3, 0 1))\" | +---------------------+---------------------+----------------------+---------------------------------+ //Create an index for the geography of the Tag any_shape and run LOOKUP. nebula> CREATE TAG INDEX IF NOT EXISTS any_shape_geo_index ON any_shape(geo); nebula> REBUILD TAG INDEX any_shape_geo_index; nebula> LOOKUP ON any_shape YIELD ST_ASText(any_shape.geo); +----------+-------------------------------------------------+ | VertexID | ST_ASText(any_shape.geo) | +----------+-------------------------------------------------+ | \"103\" | \"POLYGON((0 1, 1 2, 2 3, 0 1))\" | +----------+-------------------------------------------------+ When creating an index for geography properties, you can specify the parameters for the index. Parameter Default value Description s2_max_level 30 The maximum level of S2 cell used in the covering. Allowed values: 1 ~ 30 . Setting it to less than the default means that Nebula Graph will be forced to generate coverings using larger cells. s2_max_cells 8 The maximum number of S2 cells used in the covering. Provides a limit on how much work is done exploring the possible coverings. Allowed values: 1 ~ 30 . You may want to use higher values for odd-shaped regions such as skinny rectangles. Note Specifying the above two parameters does not affect the Point type of property. The s2_max_level value of the Point type is forced to be 30 . nebula> CREATE TAG INDEX IF NOT EXISTS any_shape_geo_index ON any_shape(geo) with (s2_max_level=30, s2_max_cells=8);","title":"Geography"},{"location":"3.ngql-guide/3.data-types/10.geography/#geography","text":"Geography is a data type composed of latitude and longitude that represents geospatial information. Nebula Graph currently supports Point, LineString, and Polygon in Simple Features and some functions in SQL-MM 3 , such as part of the core geo parsing, construction, formatting, conversion, predicates, and dimensions.","title":"Geography"},{"location":"3.ngql-guide/3.data-types/10.geography/#type_description","text":"A point is the basic data type of geography, which is determined by a latitude and a longitude. For example, \"POINT(3 8)\" means that the longitude is 3\u00b0 and the latitude is 8\u00b0 . Multiple points can form a linestring or a polygon. Shape Example Description Point \"POINT(3 8)\" Specifies the data type as a point. LineString \"LINESTRING(3 8, 4.7 73.23)\" Specifies the data type as a linestring. Polygon \"POLYGON((0 1, 1 2, 2 3, 0 1))\" Specifies the data type as a polygon.","title":"Type description"},{"location":"3.ngql-guide/3.data-types/10.geography/#examples","text":"For functions about the geography data type, see Geography functions . //Create a Tag to allow storing any geography data type. nebula> CREATE TAG IF NOT EXISTS any_shape(geo geography); //Create a Tag to allow storing a point only. nebula> CREATE TAG IF NOT EXISTS only_point(geo geography(point)); //Create a Tag to allow storing a linestring only. nebula> CREATE TAG IF NOT EXISTS only_linestring(geo geography(linestring)); //Create a Tag to allow storing a polygon only. nebula> CREATE TAG IF NOT EXISTS only_polygon(geo geography(polygon)); //Create an Edge type to allow storing any geography data type. nebula> CREATE EDGE IF NOT EXISTS any_shape_edge(geo geography); //Create a vertex to store the geography of a polygon. nebula> INSERT VERTEX any_shape(geo) VALUES \"103\":(ST_GeogFromText(\"POLYGON((0 1, 1 2, 2 3, 0 1))\")); //Create an edge to store the geography of a polygon. nebula> INSERT EDGE any_shape_edge(geo) VALUES \"201\"->\"302\":(ST_GeogFromText(\"POLYGON((0 1, 1 2, 2 3, 0 1))\")); //Query the geography of Vertex 103. nebula> FETCH PROP ON any_shape \"103\" YIELD ST_ASText(any_shape.geo); +----------+---------------------------------+ | VertexID | ST_ASText(any_shape.geo) | +----------+---------------------------------+ | \"103\" | \"POLYGON((0 1, 1 2, 2 3, 0 1))\" | +----------+---------------------------------+ //Query the geography of the edge which traverses from Vertex 201 to Vertex 302. nebula> FETCH PROP ON any_shape_edge \"201\"->\"302\" YIELD ST_ASText(any_shape_edge.geo); +---------------------+---------------------+----------------------+---------------------------------+ | any_shape_edge._src | any_shape_edge._dst | any_shape_edge._rank | ST_ASText(any_shape_edge.geo) | +---------------------+---------------------+----------------------+---------------------------------+ | \"201\" | \"302\" | 0 | \"POLYGON((0 1, 1 2, 2 3, 0 1))\" | +---------------------+---------------------+----------------------+---------------------------------+ //Create an index for the geography of the Tag any_shape and run LOOKUP. nebula> CREATE TAG INDEX IF NOT EXISTS any_shape_geo_index ON any_shape(geo); nebula> REBUILD TAG INDEX any_shape_geo_index; nebula> LOOKUP ON any_shape YIELD ST_ASText(any_shape.geo); +----------+-------------------------------------------------+ | VertexID | ST_ASText(any_shape.geo) | +----------+-------------------------------------------------+ | \"103\" | \"POLYGON((0 1, 1 2, 2 3, 0 1))\" | +----------+-------------------------------------------------+ When creating an index for geography properties, you can specify the parameters for the index. Parameter Default value Description s2_max_level 30 The maximum level of S2 cell used in the covering. Allowed values: 1 ~ 30 . Setting it to less than the default means that Nebula Graph will be forced to generate coverings using larger cells. s2_max_cells 8 The maximum number of S2 cells used in the covering. Provides a limit on how much work is done exploring the possible coverings. Allowed values: 1 ~ 30 . You may want to use higher values for odd-shaped regions such as skinny rectangles. Note Specifying the above two parameters does not affect the Point type of property. The s2_max_level value of the Point type is forced to be 30 . nebula> CREATE TAG INDEX IF NOT EXISTS any_shape_geo_index ON any_shape(geo) with (s2_max_level=30, s2_max_cells=8);","title":"Examples"},{"location":"3.ngql-guide/3.data-types/2.boolean/","text":"Boolean \u00b6 A boolean data type is declared with the bool keyword and can only take the values true or false . nGQL supports using boolean in the following ways: Define the data type of the property value as a boolean. Use boolean as judgment conditions in the WHERE clause.","title":"Boolean"},{"location":"3.ngql-guide/3.data-types/2.boolean/#boolean","text":"A boolean data type is declared with the bool keyword and can only take the values true or false . nGQL supports using boolean in the following ways: Define the data type of the property value as a boolean. Use boolean as judgment conditions in the WHERE clause.","title":"Boolean"},{"location":"3.ngql-guide/3.data-types/3.string/","text":"String \u00b6 Fixed-length strings and variable-length strings are supported. Declaration and literal representation \u00b6 The string type is declared with the keywords of: STRING : Variable-length strings. FIXED_STRING(<length>) : Fixed-length strings. <length> is the length of the string, such as FIXED_STRING(32) . A string type is used to store a sequence of characters (text). The literal constant is a sequence of characters of any length surrounded by double or single quotes. For example, \"Hello, Cooper\" or 'Hello, Cooper' . String reading and writing \u00b6 Nebula Graph supports using string types in the following ways: Define the data type of VID as a fixed-length string. Set the variable-length string as the Schema name, including the names of the graph space, tag, edge type, and property. Define the data type of the property as a fixed-length or variable-length string. For example: Define the data type of the property as a fixed-length string nebula> CREATE TAG IF NOT EXISTS t1 (p1 FIXED_STRING(10)); Define the data type of the property as a variable-length string nebula> CREATE TAG IF NOT EXISTS t2 (p2 STRING); When the fixed-length string you try to write exceeds the length limit: If the fixed-length string is a property, the writing will succeed, and Nebula Graph will truncate the string and only store the part that meets the length limit. If the fixed-length string is a VID, the writing will fail and Nebula Graph will return an error. Escape characters \u00b6 Line breaks are not allowed in a string. Escape characters are supported within strings, for example: \"\\n\\t\\r\\b\\f\" \"\\110ello world\" OpenCypher compatibility \u00b6 There are some tiny differences between openCypher and Cypher, as well as nGQL. The following is what openCypher requires. Single quotes cannot be converted to double quotes. # File: Literals.feature Feature: Literals Background: Given any graph Scenario: Return a single-quoted string When executing query: \"\"\" RETURN '' AS literal \"\"\" Then the result should be, in any order: | literal | | '' | # Note: it should return single-quotes as openCypher required. And no side effects While Cypher accepts both single quotes and double quotes as the return results. nGQL follows the Cypher way. nebula > YIELD '' AS quote1, \"\" AS quote2, \"'\" AS quote3, '\"' AS quote4 +--------+--------+--------+--------+ | quote1 | quote2 | quote3 | quote4 | +--------+--------+--------+--------+ | \"\" | \"\" | \"'\" | \"\"\" | +--------+--------+--------+--------+","title":"String"},{"location":"3.ngql-guide/3.data-types/3.string/#string","text":"Fixed-length strings and variable-length strings are supported.","title":"String"},{"location":"3.ngql-guide/3.data-types/3.string/#declaration_and_literal_representation","text":"The string type is declared with the keywords of: STRING : Variable-length strings. FIXED_STRING(<length>) : Fixed-length strings. <length> is the length of the string, such as FIXED_STRING(32) . A string type is used to store a sequence of characters (text). The literal constant is a sequence of characters of any length surrounded by double or single quotes. For example, \"Hello, Cooper\" or 'Hello, Cooper' .","title":"Declaration and literal representation"},{"location":"3.ngql-guide/3.data-types/3.string/#string_reading_and_writing","text":"Nebula Graph supports using string types in the following ways: Define the data type of VID as a fixed-length string. Set the variable-length string as the Schema name, including the names of the graph space, tag, edge type, and property. Define the data type of the property as a fixed-length or variable-length string. For example: Define the data type of the property as a fixed-length string nebula> CREATE TAG IF NOT EXISTS t1 (p1 FIXED_STRING(10)); Define the data type of the property as a variable-length string nebula> CREATE TAG IF NOT EXISTS t2 (p2 STRING); When the fixed-length string you try to write exceeds the length limit: If the fixed-length string is a property, the writing will succeed, and Nebula Graph will truncate the string and only store the part that meets the length limit. If the fixed-length string is a VID, the writing will fail and Nebula Graph will return an error.","title":"String reading and writing"},{"location":"3.ngql-guide/3.data-types/3.string/#escape_characters","text":"Line breaks are not allowed in a string. Escape characters are supported within strings, for example: \"\\n\\t\\r\\b\\f\" \"\\110ello world\"","title":"Escape characters"},{"location":"3.ngql-guide/3.data-types/3.string/#opencypher_compatibility","text":"There are some tiny differences between openCypher and Cypher, as well as nGQL. The following is what openCypher requires. Single quotes cannot be converted to double quotes. # File: Literals.feature Feature: Literals Background: Given any graph Scenario: Return a single-quoted string When executing query: \"\"\" RETURN '' AS literal \"\"\" Then the result should be, in any order: | literal | | '' | # Note: it should return single-quotes as openCypher required. And no side effects While Cypher accepts both single quotes and double quotes as the return results. nGQL follows the Cypher way. nebula > YIELD '' AS quote1, \"\" AS quote2, \"'\" AS quote3, '\"' AS quote4 +--------+--------+--------+--------+ | quote1 | quote2 | quote3 | quote4 | +--------+--------+--------+--------+ | \"\" | \"\" | \"'\" | \"\"\" | +--------+--------+--------+--------+","title":"OpenCypher compatibility"},{"location":"3.ngql-guide/3.data-types/4.date-and-time/","text":"Date and time types \u00b6 This topic will describe the DATE , TIME , DATETIME , TIMESTAMP , and DURATION types. Precautions \u00b6 While inserting time-type property values with DATE , TIME , and DATETIME , Nebula Graph transforms them to a UTC time according to the timezone specified with the timezone_name parameter in the configuration files . Note To change the timezone, modify the timezone_name value in the configuration files of all Nebula Graph services. date() , time() , and datetime() can convert a time-type property with a specified timezone. For example, datetime(\"2017-03-04 22:30:40.003000+08:00\") or datetime(\"2017-03-04T22:30:40.003000[Asia/Shanghai]\") . date() , time() , datetime() , and timestamp() all accept empty parameters to return the current date, time, and datetime. date() , time() , and datetime() all accept the property name to return a specific property value of itself. For example, date().month returns the current month, while time(\"02:59:40\").minute returns the minutes of the importing time. OpenCypher Compatibility \u00b6 In nGQL: Year, month, day, hour, minute, second, millisecond, and microsecond are supported, while the nanosecond is not supported. localdatetime() is not supported. Most string time formats are not supported. The exceptions are YYYY-MM-DDThh:mm:ss and YYYY-MM-DD hh:mm:ss . The single-digit string time format is supported. For example, time(\"1:1:1\") . DATE \u00b6 The DATE type is used for values with a date part but no time part. Nebula Graph retrieves and displays DATE values in the YYYY-MM-DD format. The supported range is -32768-01-01 to 32767-12-31 . The properties of date() include year , month , and day . TIME \u00b6 The TIME type is used for values with a time part but no date part. Nebula Graph retrieves and displays TIME values in hh:mm:ss.msmsmsususus format. The supported range is 00:00:00.000000 to 23:59:59.999999 . The properties of time() include hour , minute , and second . DATETIME \u00b6 The DATETIME type is used for values that contain both date and time parts. Nebula Graph retrieves and displays DATETIME values in YYYY-MM-DDThh:mm:ss.msmsmsususus format. The supported range is -32768-01-01T00:00:00.000000 to 32767-12-31T23:59:59.999999 . The properties of datetime() include year , month , day , hour , minute , and second . TIMESTAMP \u00b6 The TIMESTAMP data type is used for values that contain both date and time parts. It has a range of 1970-01-01T00:00:01 UTC to 2262-04-11T23:47:16 UTC. TIMESTAMP has the following features: Stored and displayed in the form of a timestamp, such as 1615974839 , which means 2021-03-17T17:53:59 . Supported TIMESTAMP querying methods: timestamp and timestamp() function. Supported TIMESTAMP inserting methods: timestamp, timestamp() function, and now() function. timestamp() function accepts empty parameters to get the timestamp of the current timezone and also accepts a string type parameter. # Return the current time. nebula> RETURN timestamp(); +-------------+ | timestamp() | +-------------+ | 1625469277 | +-------------+ nebula> RETURN timestamp(\"2022-01-05T06:18:43\"); +----------------------------------+ | timestamp(\"2022-01-05T06:18:43\") | +----------------------------------+ | 1641363523 | +----------------------------------+ Legacy version compatibility In Nebula Graph versions earlier than 3.0.0, the time string passed into the timestamp() function could include milliseconds and microseconds. As of version 3.0.0, the time string passed into the timestamp() function cannot include milliseconds and microseconds. The underlying storage data type is int64 . DURATION \u00b6 The DURATION data type is used to indicate a period of time. Map data that are freely combined by years , months , days , hours , minutes , and seconds indicates the DURATION . DURATION has the following features: Creating indexes for DURATION is not supported. DURATION can be used to calculate the specified time. Examples \u00b6 Create a tag named date1 with three properties: DATE , TIME , and DATETIME . nebula> CREATE TAG IF NOT EXISTS date1(p1 date, p2 time, p3 datetime); Insert a vertex named test1 . nebula> INSERT VERTEX date1(p1, p2, p3) VALUES \"test1\":(date(\"2021-03-17\"), time(\"17:53:59\"), datetime(\"2017-03-04T22:30:40.003000[Asia/Shanghai]\")); Return the content of the property p1 on test1 . nebula> CREATE TAG INDEX IF NOT EXISTS date1_index ON date1(p1); nebula> REBUILD TAG INDEX date1_index; nebula> MATCH (v:date1) RETURN v.date1.p1; +------------------+ | v.date1.p1.month | +------------------+ | 3 | +------------------+ Create a tag named school with the property of TIMESTAMP . nebula> CREATE TAG IF NOT EXISTS school(name string , found_time timestamp); Insert a vertex named DUT with a found-time timestamp of \"1988-03-01T08:00:00\" . # Insert as a timestamp. The corresponding timestamp of 1988-03-01T08:00:00 is 573177600, or 573206400 UTC. nebula> INSERT VERTEX school(name, found_time) VALUES \"DUT\":(\"DUT\", 573206400); # Insert in the form of date and time. nebula> INSERT VERTEX school(name, found_time) VALUES \"DUT\":(\"DUT\", timestamp(\"1988-03-01T08:00:00\")); Insert a vertex named dut and store time with now() or timestamp() functions. # Use now() function to store time nebula> INSERT VERTEX school(name, found_time) VALUES \"dut\":(\"dut\", now()); # Use timestamp() function to store time nebula> INSERT VERTEX school(name, found_time) VALUES \"dut\":(\"dut\", timestamp()); You can also use WITH statement to set a specific date and time, or to perform calculations. For example: nebula> WITH time({hour: 12, minute: 31, second: 14, millisecond:111, microsecond: 222}) AS d RETURN d; +-----------------+ | d | +-----------------+ | 12:31:14.111222 | +-----------------+ nebula> WITH date({year: 1984, month: 10, day: 11}) AS x RETURN x + 1; +------------+ | (x+1) | +------------+ | 1984-10-12 | +------------+ nebula> WITH date('1984-10-11') as x, duration({years: 12, days: 14, hours: 99, minutes: 12}) as d \\ RETURN x + d AS sum, x - d AS diff; +------------+------------+ | sum | diff | +------------+------------+ | 1996-10-29 | 1972-09-23 | +------------+------------+","title":"Date and time"},{"location":"3.ngql-guide/3.data-types/4.date-and-time/#date_and_time_types","text":"This topic will describe the DATE , TIME , DATETIME , TIMESTAMP , and DURATION types.","title":"Date and time types"},{"location":"3.ngql-guide/3.data-types/4.date-and-time/#precautions","text":"While inserting time-type property values with DATE , TIME , and DATETIME , Nebula Graph transforms them to a UTC time according to the timezone specified with the timezone_name parameter in the configuration files . Note To change the timezone, modify the timezone_name value in the configuration files of all Nebula Graph services. date() , time() , and datetime() can convert a time-type property with a specified timezone. For example, datetime(\"2017-03-04 22:30:40.003000+08:00\") or datetime(\"2017-03-04T22:30:40.003000[Asia/Shanghai]\") . date() , time() , datetime() , and timestamp() all accept empty parameters to return the current date, time, and datetime. date() , time() , and datetime() all accept the property name to return a specific property value of itself. For example, date().month returns the current month, while time(\"02:59:40\").minute returns the minutes of the importing time.","title":"Precautions"},{"location":"3.ngql-guide/3.data-types/4.date-and-time/#opencypher_compatibility","text":"In nGQL: Year, month, day, hour, minute, second, millisecond, and microsecond are supported, while the nanosecond is not supported. localdatetime() is not supported. Most string time formats are not supported. The exceptions are YYYY-MM-DDThh:mm:ss and YYYY-MM-DD hh:mm:ss . The single-digit string time format is supported. For example, time(\"1:1:1\") .","title":"OpenCypher Compatibility"},{"location":"3.ngql-guide/3.data-types/4.date-and-time/#date","text":"The DATE type is used for values with a date part but no time part. Nebula Graph retrieves and displays DATE values in the YYYY-MM-DD format. The supported range is -32768-01-01 to 32767-12-31 . The properties of date() include year , month , and day .","title":"DATE"},{"location":"3.ngql-guide/3.data-types/4.date-and-time/#time","text":"The TIME type is used for values with a time part but no date part. Nebula Graph retrieves and displays TIME values in hh:mm:ss.msmsmsususus format. The supported range is 00:00:00.000000 to 23:59:59.999999 . The properties of time() include hour , minute , and second .","title":"TIME"},{"location":"3.ngql-guide/3.data-types/4.date-and-time/#datetime","text":"The DATETIME type is used for values that contain both date and time parts. Nebula Graph retrieves and displays DATETIME values in YYYY-MM-DDThh:mm:ss.msmsmsususus format. The supported range is -32768-01-01T00:00:00.000000 to 32767-12-31T23:59:59.999999 . The properties of datetime() include year , month , day , hour , minute , and second .","title":"DATETIME"},{"location":"3.ngql-guide/3.data-types/4.date-and-time/#timestamp","text":"The TIMESTAMP data type is used for values that contain both date and time parts. It has a range of 1970-01-01T00:00:01 UTC to 2262-04-11T23:47:16 UTC. TIMESTAMP has the following features: Stored and displayed in the form of a timestamp, such as 1615974839 , which means 2021-03-17T17:53:59 . Supported TIMESTAMP querying methods: timestamp and timestamp() function. Supported TIMESTAMP inserting methods: timestamp, timestamp() function, and now() function. timestamp() function accepts empty parameters to get the timestamp of the current timezone and also accepts a string type parameter. # Return the current time. nebula> RETURN timestamp(); +-------------+ | timestamp() | +-------------+ | 1625469277 | +-------------+ nebula> RETURN timestamp(\"2022-01-05T06:18:43\"); +----------------------------------+ | timestamp(\"2022-01-05T06:18:43\") | +----------------------------------+ | 1641363523 | +----------------------------------+ Legacy version compatibility In Nebula Graph versions earlier than 3.0.0, the time string passed into the timestamp() function could include milliseconds and microseconds. As of version 3.0.0, the time string passed into the timestamp() function cannot include milliseconds and microseconds. The underlying storage data type is int64 .","title":"TIMESTAMP"},{"location":"3.ngql-guide/3.data-types/4.date-and-time/#duration","text":"The DURATION data type is used to indicate a period of time. Map data that are freely combined by years , months , days , hours , minutes , and seconds indicates the DURATION . DURATION has the following features: Creating indexes for DURATION is not supported. DURATION can be used to calculate the specified time.","title":"DURATION"},{"location":"3.ngql-guide/3.data-types/4.date-and-time/#examples","text":"Create a tag named date1 with three properties: DATE , TIME , and DATETIME . nebula> CREATE TAG IF NOT EXISTS date1(p1 date, p2 time, p3 datetime); Insert a vertex named test1 . nebula> INSERT VERTEX date1(p1, p2, p3) VALUES \"test1\":(date(\"2021-03-17\"), time(\"17:53:59\"), datetime(\"2017-03-04T22:30:40.003000[Asia/Shanghai]\")); Return the content of the property p1 on test1 . nebula> CREATE TAG INDEX IF NOT EXISTS date1_index ON date1(p1); nebula> REBUILD TAG INDEX date1_index; nebula> MATCH (v:date1) RETURN v.date1.p1; +------------------+ | v.date1.p1.month | +------------------+ | 3 | +------------------+ Create a tag named school with the property of TIMESTAMP . nebula> CREATE TAG IF NOT EXISTS school(name string , found_time timestamp); Insert a vertex named DUT with a found-time timestamp of \"1988-03-01T08:00:00\" . # Insert as a timestamp. The corresponding timestamp of 1988-03-01T08:00:00 is 573177600, or 573206400 UTC. nebula> INSERT VERTEX school(name, found_time) VALUES \"DUT\":(\"DUT\", 573206400); # Insert in the form of date and time. nebula> INSERT VERTEX school(name, found_time) VALUES \"DUT\":(\"DUT\", timestamp(\"1988-03-01T08:00:00\")); Insert a vertex named dut and store time with now() or timestamp() functions. # Use now() function to store time nebula> INSERT VERTEX school(name, found_time) VALUES \"dut\":(\"dut\", now()); # Use timestamp() function to store time nebula> INSERT VERTEX school(name, found_time) VALUES \"dut\":(\"dut\", timestamp()); You can also use WITH statement to set a specific date and time, or to perform calculations. For example: nebula> WITH time({hour: 12, minute: 31, second: 14, millisecond:111, microsecond: 222}) AS d RETURN d; +-----------------+ | d | +-----------------+ | 12:31:14.111222 | +-----------------+ nebula> WITH date({year: 1984, month: 10, day: 11}) AS x RETURN x + 1; +------------+ | (x+1) | +------------+ | 1984-10-12 | +------------+ nebula> WITH date('1984-10-11') as x, duration({years: 12, days: 14, hours: 99, minutes: 12}) as d \\ RETURN x + d AS sum, x - d AS diff; +------------+------------+ | sum | diff | +------------+------------+ | 1996-10-29 | 1972-09-23 | +------------+------------+","title":"Examples"},{"location":"3.ngql-guide/3.data-types/5.null/","text":"NULL \u00b6 You can set the properties for vertices or edges to NULL . Also, you can set the NOT NULL constraint to make sure that the property values are NOT NULL . If not specified, the property is set to NULL by default. Logical operations with NULL \u00b6 Here is the truth table for AND , OR , XOR , and NOT . a b a AND b a OR b a XOR b NOT a false false false false false true false null false null null true false true false true true true true false false true true false true null null true null false true true true true false false null false false null null null null null null null null null null true null true null null OpenCypher compatibility \u00b6 The comparisons and operations about NULL are different from openCypher. There may be changes later. Comparisons with NULL \u00b6 The comparison operations with NULL are incompatible with openCypher. Operations and RETURN with NULL \u00b6 The NULL operations and RETURN with NULL are incompatible with openCypher. Examples \u00b6 Use NOT NULL \u00b6 Create a tag named player . Specify the property name as NOT NULL . nebula> CREATE TAG IF NOT EXISTS player(name string NOT NULL, age int); Use SHOW to create tag statements. The property name is NOT NULL . The property age is NULL by default. nebula> SHOW CREATE TAG player; +-----------+-----------------------------------+ | Tag | Create Tag | +-----------+-----------------------------------+ | \"student\" | \"CREATE TAG `player` ( | | | `name` string NOT NULL, | | | `age` int64 NULL | | | ) ttl_duration = 0, ttl_col = \"\"\" | +-----------+-----------------------------------+ Insert the vertex Kobe . The property age can be NULL . nebula> INSERT VERTEX player(name, age) VALUES \"Kobe\":(\"Kobe\",null); Use NOT NULL and set the default \u00b6 Create a tag named player . Specify the property age as NOT NULL . The default value is 18 . nebula> CREATE TAG IF NOT EXISTS player(name string, age int NOT NULL DEFAULT 18); Insert the vertex Kobe . Specify the property name only. nebula> INSERT VERTEX player(name) VALUES \"Kobe\":(\"Kobe\"); Query the vertex Kobe . The property age is 18 by default. nebula> FETCH PROP ON player \"Kobe\" YIELD properties(vertex); +--------------------------+ | properties(VERTEX) | +--------------------------+ | {age: 18, name: \"Kobe\"} | +--------------------------+","title":"NULL"},{"location":"3.ngql-guide/3.data-types/5.null/#null","text":"You can set the properties for vertices or edges to NULL . Also, you can set the NOT NULL constraint to make sure that the property values are NOT NULL . If not specified, the property is set to NULL by default.","title":"NULL"},{"location":"3.ngql-guide/3.data-types/5.null/#logical_operations_with_null","text":"Here is the truth table for AND , OR , XOR , and NOT . a b a AND b a OR b a XOR b NOT a false false false false false true false null false null null true false true false true true true true false false true true false true null null true null false true true true true false false null false false null null null null null null null null null null true null true null null","title":"Logical operations with NULL"},{"location":"3.ngql-guide/3.data-types/5.null/#opencypher_compatibility","text":"The comparisons and operations about NULL are different from openCypher. There may be changes later.","title":"OpenCypher compatibility"},{"location":"3.ngql-guide/3.data-types/5.null/#comparisons_with_null","text":"The comparison operations with NULL are incompatible with openCypher.","title":"Comparisons with NULL"},{"location":"3.ngql-guide/3.data-types/5.null/#operations_and_return_with_null","text":"The NULL operations and RETURN with NULL are incompatible with openCypher.","title":"Operations and RETURN with NULL"},{"location":"3.ngql-guide/3.data-types/5.null/#examples","text":"","title":"Examples"},{"location":"3.ngql-guide/3.data-types/5.null/#use_not_null","text":"Create a tag named player . Specify the property name as NOT NULL . nebula> CREATE TAG IF NOT EXISTS player(name string NOT NULL, age int); Use SHOW to create tag statements. The property name is NOT NULL . The property age is NULL by default. nebula> SHOW CREATE TAG player; +-----------+-----------------------------------+ | Tag | Create Tag | +-----------+-----------------------------------+ | \"student\" | \"CREATE TAG `player` ( | | | `name` string NOT NULL, | | | `age` int64 NULL | | | ) ttl_duration = 0, ttl_col = \"\"\" | +-----------+-----------------------------------+ Insert the vertex Kobe . The property age can be NULL . nebula> INSERT VERTEX player(name, age) VALUES \"Kobe\":(\"Kobe\",null);","title":"Use NOT NULL"},{"location":"3.ngql-guide/3.data-types/5.null/#use_not_null_and_set_the_default","text":"Create a tag named player . Specify the property age as NOT NULL . The default value is 18 . nebula> CREATE TAG IF NOT EXISTS player(name string, age int NOT NULL DEFAULT 18); Insert the vertex Kobe . Specify the property name only. nebula> INSERT VERTEX player(name) VALUES \"Kobe\":(\"Kobe\"); Query the vertex Kobe . The property age is 18 by default. nebula> FETCH PROP ON player \"Kobe\" YIELD properties(vertex); +--------------------------+ | properties(VERTEX) | +--------------------------+ | {age: 18, name: \"Kobe\"} | +--------------------------+","title":"Use NOT NULL and set the default"},{"location":"3.ngql-guide/3.data-types/6.list/","text":"Lists \u00b6 The list is a composite data type. A list is a sequence of values. Individual elements in a list can be accessed by their positions. A list starts with a left square bracket [ and ends with a right square bracket ] . A list contains zero, one, or more expressions. List elements are separated from each other with commas ( , ). Whitespace around elements is ignored in the list, thus line breaks, tab stops, and blanks can be used for formatting. OpenCypher compatibility \u00b6 A composite data type (i.e. set, map, and list) CANNOT be stored as properties of vertices or edges. List operations \u00b6 You can use the preset list function to operate the list, or use the index to filter the elements in the list. Index syntax \u00b6 [M] [M..N] [M..] [..N] The index of nGQL supports queries from front to back, starting from 0. 0 means the first element, 1 means the second element, and so on. It also supports queries from back to front, starting from -1. -1 means the last element, -2 means the penultimate element, and so on. [M]: represents the element whose index is M. [M..N]: represents the elements whose indexes are greater or equal to M but smaller than N . Return empty when N is 0. [M..]: represents the elements whose indexes are greater or equal to M . [..N]: represents the elements whose indexes are smaller than N . Return empty when N is 0. Note Return empty if the index is out of bounds, while return normally if the index is within the bound. Return empty if M \u2265 N . When querying a single element, if M is null, return BAD_TYPE . When conducting a range query, if M or N is null, return null . Examples \u00b6 # The following query returns the list [1,2,3]. nebula> RETURN list[1, 2, 3] AS a; +-----------+ | a | +-----------+ | [1, 2, 3] | +-----------+ # The following query returns the element whose index is 3 in the list [1,2,3,4,5]. In a list, the index starts from 0, and thus the return element is 4. nebula> RETURN range(1,5)[3]; +---------------+ | range(1,5)[3] | +---------------+ | 4 | +---------------+ # The following query returns the element whose index is -2 in the list [1,2,3,4,5]. The index of the last element in a list is -1, and thus the return element is 4. nebula> RETURN range(1,5)[-2]; +------------------+ | range(1,5)[-(2)] | +------------------+ | 4 | +------------------+ # The following query returns the elements whose indexes are from 0 to 3 (not including 3) in the list [1,2,3,4,5]. nebula> RETURN range(1,5)[0..3]; +------------------+ | range(1,5)[0..3] | +------------------+ | [1, 2, 3] | +------------------+ # The following query returns the elements whose indexes are greater than 2 in the list [1,2,3,4,5]. nebula> RETURN range(1,5)[3..] AS a; +--------+ | a | +--------+ | [4, 5] | +--------+ # The following query returns the elements whose indexes are smaller than 3. nebula> WITH list[1, 2, 3, 4, 5] AS a \\ RETURN a[..3] AS r; +-----------+ | r | +-----------+ | [1, 2, 3] | +-----------+ # The following query filters the elements whose indexes are greater than 2 in the list [1,2,3,4,5], calculate them respectively, and returns them. nebula> RETURN [n IN range(1,5) WHERE n > 2 | n + 10] AS a; +--------------+ | a | +--------------+ | [13, 14, 15] | +--------------+ # The following query returns the elements from the first to the penultimate (inclusive) in the list [1, 2, 3]. nebula> YIELD list[1, 2, 3][0..-1] AS a; +--------+ | a | +--------+ | [1, 2] | +--------+ # The following query returns the elements from the first (exclusive) to the third backward in the list [1, 2, 3, 4, 5]. nebula> YIELD list[1, 2, 3, 4, 5][-3..-1] AS a; +--------+ | a | +--------+ | [3, 4] | +--------+ # The following query sets the variables and returns the elements whose indexes are 1 and 2. nebula> $var = YIELD 1 AS f, 3 AS t; \\ YIELD list[1, 2, 3][$var.f..$var.t] AS a; +--------+ | a | +--------+ | [2, 3] | +--------+ # The following query returns empty because the index is out of bound. It will return normally when the index is within the bound. nebula> RETURN list[1, 2, 3, 4, 5] [0..10] AS a; +-----------------+ | a | +-----------------+ | [1, 2, 3, 4, 5] | +-----------------+ nebula> RETURN list[1, 2, 3] [-5..5] AS a; +-----------+ | a | +-----------+ | [1, 2, 3] | +-----------+ # The following query returns empty because there is a [0..0]. nebula> RETURN list[1, 2, 3, 4, 5] [0..0] AS a; +----+ | a | +----+ | [] | +----+ # The following query returns empty because of M \u2265 N. nebula> RETURN list[1, 2, 3, 4, 5] [3..1] AS a; +----+ | a | +----+ | [] | +----+ # When conduct a range query, if `M` or `N` is null, return `null`. nebula> WITH list[1,2,3] AS a \\ RETURN a[0..null] as r; +----------+ | r | +----------+ | __NULL__ | +----------+ # The following query calculates the elements in the list [1,2,3,4,5] respectively and returns them without the list head. nebula> RETURN tail([n IN range(1, 5) | 2 * n - 10]) AS a; +-----------------+ | a | +-----------------+ | [-6, -4, -2, 0] | +-----------------+ # The following query takes the elements in the list [1,2,3] as true and return. nebula> RETURN [n IN range(1, 3) WHERE true | n] AS r; +-----------+ | r | +-----------+ | [1, 2, 3] | +-----------+ # The following query returns the length of the list [1,2,3]. nebula> RETURN size(list[1,2,3]); +-------------------+ | size(list[1,2,3]) | +-------------------+ | 3 | +-------------------+ # The following query calculates the elements in the list [92,90] and runs a conditional judgment in a where clause. nebula> GO FROM \"player100\" OVER follow WHERE properties(edge).degree NOT IN [x IN [92, 90] | x + $$.player.age] \\ YIELD dst(edge) AS id, properties(edge).degree AS degree; +-------------+--------+ | id | degree | +-------------+--------+ | \"player101\" | 95 | | \"player102\" | 90 | +-------------+--------+ # The following query takes the query result of the MATCH statement as the elements in a list. Then it calculates and returns them. nebula> MATCH p = (n:player{name:\"Tim Duncan\"})-[:follow]->(m) \\ RETURN [n IN nodes(p) | n.age + 100] AS r; +------------+ | r | +------------+ | [142, 136] | | [142, 133] | +------------+ OpenCypher compatibility \u00b6 In openCypher, return null when querying a single out-of-bound element. However, in nGQL, return OUT_OF_RANGE when querying a single out-of-bound element. nebula> RETURN range(0,5)[-12]; +-------------------+ | range(0,5)[-(12)] | +-------------------+ | OUT_OF_RANGE | +-------------------+ A composite data type (i.e., set, map, and list) CAN NOT be stored as properties for vertices or edges. It is recommended to modify the graph modeling method. The composite data type should be modeled as an adjacent edge of a vertex, rather than its property. Each adjacent edge can be dynamically added or deleted. The rank values of the adjacent edges can be used for sequencing. Patterns are not supported in the list. For example, [(src)-[]->(m) | m.name] .","title":"List"},{"location":"3.ngql-guide/3.data-types/6.list/#lists","text":"The list is a composite data type. A list is a sequence of values. Individual elements in a list can be accessed by their positions. A list starts with a left square bracket [ and ends with a right square bracket ] . A list contains zero, one, or more expressions. List elements are separated from each other with commas ( , ). Whitespace around elements is ignored in the list, thus line breaks, tab stops, and blanks can be used for formatting.","title":"Lists"},{"location":"3.ngql-guide/3.data-types/6.list/#opencypher_compatibility","text":"A composite data type (i.e. set, map, and list) CANNOT be stored as properties of vertices or edges.","title":"OpenCypher compatibility"},{"location":"3.ngql-guide/3.data-types/6.list/#list_operations","text":"You can use the preset list function to operate the list, or use the index to filter the elements in the list.","title":"List operations"},{"location":"3.ngql-guide/3.data-types/6.list/#index_syntax","text":"[M] [M..N] [M..] [..N] The index of nGQL supports queries from front to back, starting from 0. 0 means the first element, 1 means the second element, and so on. It also supports queries from back to front, starting from -1. -1 means the last element, -2 means the penultimate element, and so on. [M]: represents the element whose index is M. [M..N]: represents the elements whose indexes are greater or equal to M but smaller than N . Return empty when N is 0. [M..]: represents the elements whose indexes are greater or equal to M . [..N]: represents the elements whose indexes are smaller than N . Return empty when N is 0. Note Return empty if the index is out of bounds, while return normally if the index is within the bound. Return empty if M \u2265 N . When querying a single element, if M is null, return BAD_TYPE . When conducting a range query, if M or N is null, return null .","title":"Index syntax"},{"location":"3.ngql-guide/3.data-types/6.list/#examples","text":"# The following query returns the list [1,2,3]. nebula> RETURN list[1, 2, 3] AS a; +-----------+ | a | +-----------+ | [1, 2, 3] | +-----------+ # The following query returns the element whose index is 3 in the list [1,2,3,4,5]. In a list, the index starts from 0, and thus the return element is 4. nebula> RETURN range(1,5)[3]; +---------------+ | range(1,5)[3] | +---------------+ | 4 | +---------------+ # The following query returns the element whose index is -2 in the list [1,2,3,4,5]. The index of the last element in a list is -1, and thus the return element is 4. nebula> RETURN range(1,5)[-2]; +------------------+ | range(1,5)[-(2)] | +------------------+ | 4 | +------------------+ # The following query returns the elements whose indexes are from 0 to 3 (not including 3) in the list [1,2,3,4,5]. nebula> RETURN range(1,5)[0..3]; +------------------+ | range(1,5)[0..3] | +------------------+ | [1, 2, 3] | +------------------+ # The following query returns the elements whose indexes are greater than 2 in the list [1,2,3,4,5]. nebula> RETURN range(1,5)[3..] AS a; +--------+ | a | +--------+ | [4, 5] | +--------+ # The following query returns the elements whose indexes are smaller than 3. nebula> WITH list[1, 2, 3, 4, 5] AS a \\ RETURN a[..3] AS r; +-----------+ | r | +-----------+ | [1, 2, 3] | +-----------+ # The following query filters the elements whose indexes are greater than 2 in the list [1,2,3,4,5], calculate them respectively, and returns them. nebula> RETURN [n IN range(1,5) WHERE n > 2 | n + 10] AS a; +--------------+ | a | +--------------+ | [13, 14, 15] | +--------------+ # The following query returns the elements from the first to the penultimate (inclusive) in the list [1, 2, 3]. nebula> YIELD list[1, 2, 3][0..-1] AS a; +--------+ | a | +--------+ | [1, 2] | +--------+ # The following query returns the elements from the first (exclusive) to the third backward in the list [1, 2, 3, 4, 5]. nebula> YIELD list[1, 2, 3, 4, 5][-3..-1] AS a; +--------+ | a | +--------+ | [3, 4] | +--------+ # The following query sets the variables and returns the elements whose indexes are 1 and 2. nebula> $var = YIELD 1 AS f, 3 AS t; \\ YIELD list[1, 2, 3][$var.f..$var.t] AS a; +--------+ | a | +--------+ | [2, 3] | +--------+ # The following query returns empty because the index is out of bound. It will return normally when the index is within the bound. nebula> RETURN list[1, 2, 3, 4, 5] [0..10] AS a; +-----------------+ | a | +-----------------+ | [1, 2, 3, 4, 5] | +-----------------+ nebula> RETURN list[1, 2, 3] [-5..5] AS a; +-----------+ | a | +-----------+ | [1, 2, 3] | +-----------+ # The following query returns empty because there is a [0..0]. nebula> RETURN list[1, 2, 3, 4, 5] [0..0] AS a; +----+ | a | +----+ | [] | +----+ # The following query returns empty because of M \u2265 N. nebula> RETURN list[1, 2, 3, 4, 5] [3..1] AS a; +----+ | a | +----+ | [] | +----+ # When conduct a range query, if `M` or `N` is null, return `null`. nebula> WITH list[1,2,3] AS a \\ RETURN a[0..null] as r; +----------+ | r | +----------+ | __NULL__ | +----------+ # The following query calculates the elements in the list [1,2,3,4,5] respectively and returns them without the list head. nebula> RETURN tail([n IN range(1, 5) | 2 * n - 10]) AS a; +-----------------+ | a | +-----------------+ | [-6, -4, -2, 0] | +-----------------+ # The following query takes the elements in the list [1,2,3] as true and return. nebula> RETURN [n IN range(1, 3) WHERE true | n] AS r; +-----------+ | r | +-----------+ | [1, 2, 3] | +-----------+ # The following query returns the length of the list [1,2,3]. nebula> RETURN size(list[1,2,3]); +-------------------+ | size(list[1,2,3]) | +-------------------+ | 3 | +-------------------+ # The following query calculates the elements in the list [92,90] and runs a conditional judgment in a where clause. nebula> GO FROM \"player100\" OVER follow WHERE properties(edge).degree NOT IN [x IN [92, 90] | x + $$.player.age] \\ YIELD dst(edge) AS id, properties(edge).degree AS degree; +-------------+--------+ | id | degree | +-------------+--------+ | \"player101\" | 95 | | \"player102\" | 90 | +-------------+--------+ # The following query takes the query result of the MATCH statement as the elements in a list. Then it calculates and returns them. nebula> MATCH p = (n:player{name:\"Tim Duncan\"})-[:follow]->(m) \\ RETURN [n IN nodes(p) | n.age + 100] AS r; +------------+ | r | +------------+ | [142, 136] | | [142, 133] | +------------+","title":"Examples"},{"location":"3.ngql-guide/3.data-types/6.list/#opencypher_compatibility_1","text":"In openCypher, return null when querying a single out-of-bound element. However, in nGQL, return OUT_OF_RANGE when querying a single out-of-bound element. nebula> RETURN range(0,5)[-12]; +-------------------+ | range(0,5)[-(12)] | +-------------------+ | OUT_OF_RANGE | +-------------------+ A composite data type (i.e., set, map, and list) CAN NOT be stored as properties for vertices or edges. It is recommended to modify the graph modeling method. The composite data type should be modeled as an adjacent edge of a vertex, rather than its property. Each adjacent edge can be dynamically added or deleted. The rank values of the adjacent edges can be used for sequencing. Patterns are not supported in the list. For example, [(src)-[]->(m) | m.name] .","title":"OpenCypher compatibility"},{"location":"3.ngql-guide/3.data-types/7.set/","text":"Sets \u00b6 The set is a composite data type. A set is a set of values. Unlike a List, values in a set are unordered and each value must be unique. A set starts with a left curly bracket { and ends with a right curly bracket } . A set contains zero, one, or more expressions. Set elements are separated from each other with commas ( , ). Whitespace around elements is ignored in the set, thus line breaks, tab stops, and blanks can be used for formatting. OpenCypher compatibility \u00b6 A composite data type (i.e. set, map, and list) CANNOT be stored as properties of vertices or edges. A set is not a data type in openCypher, but in nGQL, users can use the set. Examples \u00b6 # The following query returns the set {1,2,3}. nebula> RETURN set{1, 2, 3} AS a; +-----------+ | a | +-----------+ | {3, 2, 1} | +-----------+ # The following query returns the set {1,2}, Because the set does not allow repeating elements, and the order is unordered. nebula> RETURN set{1, 2, 1} AS a; +--------+ | a | +--------+ | {2, 1} | +--------+ # The following query checks whether the set has the specified element 1. nebula> RETURN 1 IN set{1, 2} AS a; +------+ | a | +------+ | true | +------+ # The following query counts the number of elements in the set. nebula> YIELD size(set{1, 2, 1}) AS a; +---+ | a | +---+ | 2 | +---+ # The following query returns a set of target vertex property values. nebula> GO FROM \"player100\" OVER follow \\ YIELD set{properties($$).name,properties($$).age} as a; +-----------------------+ | a | +-----------------------+ | {36, \"Tony Parker\"} | | {41, \"Manu Ginobili\"} | +-----------------------+","title":"Set"},{"location":"3.ngql-guide/3.data-types/7.set/#sets","text":"The set is a composite data type. A set is a set of values. Unlike a List, values in a set are unordered and each value must be unique. A set starts with a left curly bracket { and ends with a right curly bracket } . A set contains zero, one, or more expressions. Set elements are separated from each other with commas ( , ). Whitespace around elements is ignored in the set, thus line breaks, tab stops, and blanks can be used for formatting.","title":"Sets"},{"location":"3.ngql-guide/3.data-types/7.set/#opencypher_compatibility","text":"A composite data type (i.e. set, map, and list) CANNOT be stored as properties of vertices or edges. A set is not a data type in openCypher, but in nGQL, users can use the set.","title":"OpenCypher compatibility"},{"location":"3.ngql-guide/3.data-types/7.set/#examples","text":"# The following query returns the set {1,2,3}. nebula> RETURN set{1, 2, 3} AS a; +-----------+ | a | +-----------+ | {3, 2, 1} | +-----------+ # The following query returns the set {1,2}, Because the set does not allow repeating elements, and the order is unordered. nebula> RETURN set{1, 2, 1} AS a; +--------+ | a | +--------+ | {2, 1} | +--------+ # The following query checks whether the set has the specified element 1. nebula> RETURN 1 IN set{1, 2} AS a; +------+ | a | +------+ | true | +------+ # The following query counts the number of elements in the set. nebula> YIELD size(set{1, 2, 1}) AS a; +---+ | a | +---+ | 2 | +---+ # The following query returns a set of target vertex property values. nebula> GO FROM \"player100\" OVER follow \\ YIELD set{properties($$).name,properties($$).age} as a; +-----------------------+ | a | +-----------------------+ | {36, \"Tony Parker\"} | | {41, \"Manu Ginobili\"} | +-----------------------+","title":"Examples"},{"location":"3.ngql-guide/3.data-types/8.map/","text":"Maps \u00b6 The map is a composite data type. Maps are unordered collections of key-value pairs. In maps, the key is a string. The value can have any data type. You can get the map element by using map['key'] . A map starts with a left curly bracket { and ends with a right curly bracket } . A map contains zero, one, or more key-value pairs. Map elements are separated from each other with commas ( , ). Whitespace around elements is ignored in the map, thus line breaks, tab stops, and blanks can be used for formatting. OpenCypher compatibility \u00b6 A composite data type (i.e. set, map, and list) CANNOT be stored as properties of vertices or edges. Map projection is not supported. Examples \u00b6 # The following query returns the simple map. nebula> YIELD map{key1: 'Value1', Key2: 'Value2'} as a; +----------------------------------+ | a | +----------------------------------+ | {Key2: \"Value2\", key1: \"Value1\"} | +----------------------------------+ # The following query returns the list type map. nebula> YIELD map{listKey: [{inner: 'Map1'}, {inner: 'Map2'}]} as a; +-----------------------------------------------+ | a | +-----------------------------------------------+ | {listKey: [{inner: \"Map1\"}, {inner: \"Map2\"}]} | +-----------------------------------------------+ # The following query returns the hybrid type map. nebula> RETURN map{a: LIST[1,2], b: SET{1,2,1}, c: \"hee\"} as a; +----------------------------------+ | a | +----------------------------------+ | {a: [1, 2], b: {2, 1}, c: \"hee\"} | +----------------------------------+ # The following query returns the specified element in a map. nebula> RETURN map{a: LIST[1,2], b: SET{1,2,1}, c: \"hee\"}[\"b\"] AS b; +--------+ | b | +--------+ | {2, 1} | +--------+ # The following query checks whether the map has the specified key, not support checks whether the map has the specified value yet. nebula> RETURN \"a\" IN MAP{a:1, b:2} AS a; +------+ | a | +------+ | true | +------+","title":"Map"},{"location":"3.ngql-guide/3.data-types/8.map/#maps","text":"The map is a composite data type. Maps are unordered collections of key-value pairs. In maps, the key is a string. The value can have any data type. You can get the map element by using map['key'] . A map starts with a left curly bracket { and ends with a right curly bracket } . A map contains zero, one, or more key-value pairs. Map elements are separated from each other with commas ( , ). Whitespace around elements is ignored in the map, thus line breaks, tab stops, and blanks can be used for formatting.","title":"Maps"},{"location":"3.ngql-guide/3.data-types/8.map/#opencypher_compatibility","text":"A composite data type (i.e. set, map, and list) CANNOT be stored as properties of vertices or edges. Map projection is not supported.","title":"OpenCypher compatibility"},{"location":"3.ngql-guide/3.data-types/8.map/#examples","text":"# The following query returns the simple map. nebula> YIELD map{key1: 'Value1', Key2: 'Value2'} as a; +----------------------------------+ | a | +----------------------------------+ | {Key2: \"Value2\", key1: \"Value1\"} | +----------------------------------+ # The following query returns the list type map. nebula> YIELD map{listKey: [{inner: 'Map1'}, {inner: 'Map2'}]} as a; +-----------------------------------------------+ | a | +-----------------------------------------------+ | {listKey: [{inner: \"Map1\"}, {inner: \"Map2\"}]} | +-----------------------------------------------+ # The following query returns the hybrid type map. nebula> RETURN map{a: LIST[1,2], b: SET{1,2,1}, c: \"hee\"} as a; +----------------------------------+ | a | +----------------------------------+ | {a: [1, 2], b: {2, 1}, c: \"hee\"} | +----------------------------------+ # The following query returns the specified element in a map. nebula> RETURN map{a: LIST[1,2], b: SET{1,2,1}, c: \"hee\"}[\"b\"] AS b; +--------+ | b | +--------+ | {2, 1} | +--------+ # The following query checks whether the map has the specified key, not support checks whether the map has the specified value yet. nebula> RETURN \"a\" IN MAP{a:1, b:2} AS a; +------+ | a | +------+ | true | +------+","title":"Examples"},{"location":"3.ngql-guide/3.data-types/9.type-conversion/","text":"Type Conversion/Type coercions \u00b6 Converting an expression of a given type to another type is known as type conversion. Type coercions functions \u00b6 Function Description toBoolean() Converts a string value to a boolean value. toFloat() Converts an integer or string value to a floating point number. toInteger() Converts a floating point or string value to an integer value. toSet() Converts a list or set value to a set value. type() Returns the string representation of the relationship type. Examples \u00b6 nebula> UNWIND [true, false, 'true', 'false', NULL] AS b \\ RETURN toBoolean(b) AS b; +----------+ | b | +----------+ | true | | false | | true | | false | | __NULL__ | +----------+ nebula> RETURN toFloat(1), toFloat('1.3'), toFloat('1e3'), toFloat('not a number'); +------------+----------------+----------------+-------------------------+ | toFloat(1) | toFloat(\"1.3\") | toFloat(\"1e3\") | toFloat(\"not a number\") | +------------+----------------+----------------+-------------------------+ | 1.0 | 1.3 | 1000.0 | __NULL__ | +------------+----------------+----------------+-------------------------+ nebula> RETURN toInteger(1), toInteger('1'), toInteger('1e3'), toInteger('not a number'); +--------------+----------------+------------------+---------------------------+ | toInteger(1) | toInteger(\"1\") | toInteger(\"1e3\") | toInteger(\"not a number\") | +--------------+----------------+------------------+---------------------------+ | 1 | 1 | 1000 | __NULL__ | +--------------+----------------+------------------+---------------------------+ nebula> MATCH (a:player)-[e]-() \\ RETURN type(e); +----------+ | type(e) | +----------+ | \"follow\" | | \"follow\" | +----------+ nebula> MATCH (a:player {name: \"Tim Duncan\"}) \\ WHERE toInteger(right(id(a),3)) == 100 \\ RETURN a; +----------------------------------------------------+ | a | +----------------------------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | +----------------------------------------------------+ nebula> MATCH (n:player) \\ WITH n LIMIT toInteger(ceil(1.8)) \\ RETURN count(*) AS count; +-------+ | count | +-------+ | 2 | +-------+ nebula> RETURN toSet(list[1,2,3,1,2]) AS list2set; +-----------+ | list2set | +-----------+ | {3, 1, 2} | +-----------+ nebula> RETURN toSet(set{1,2,3,1,2}) AS set2set; +-----------+ | set2set | +-----------+ | {3, 2, 1} | +-----------+","title":"Type conversion"},{"location":"3.ngql-guide/3.data-types/9.type-conversion/#type_conversiontype_coercions","text":"Converting an expression of a given type to another type is known as type conversion.","title":"Type Conversion/Type coercions"},{"location":"3.ngql-guide/3.data-types/9.type-conversion/#type_coercions_functions","text":"Function Description toBoolean() Converts a string value to a boolean value. toFloat() Converts an integer or string value to a floating point number. toInteger() Converts a floating point or string value to an integer value. toSet() Converts a list or set value to a set value. type() Returns the string representation of the relationship type.","title":"Type coercions functions"},{"location":"3.ngql-guide/3.data-types/9.type-conversion/#examples","text":"nebula> UNWIND [true, false, 'true', 'false', NULL] AS b \\ RETURN toBoolean(b) AS b; +----------+ | b | +----------+ | true | | false | | true | | false | | __NULL__ | +----------+ nebula> RETURN toFloat(1), toFloat('1.3'), toFloat('1e3'), toFloat('not a number'); +------------+----------------+----------------+-------------------------+ | toFloat(1) | toFloat(\"1.3\") | toFloat(\"1e3\") | toFloat(\"not a number\") | +------------+----------------+----------------+-------------------------+ | 1.0 | 1.3 | 1000.0 | __NULL__ | +------------+----------------+----------------+-------------------------+ nebula> RETURN toInteger(1), toInteger('1'), toInteger('1e3'), toInteger('not a number'); +--------------+----------------+------------------+---------------------------+ | toInteger(1) | toInteger(\"1\") | toInteger(\"1e3\") | toInteger(\"not a number\") | +--------------+----------------+------------------+---------------------------+ | 1 | 1 | 1000 | __NULL__ | +--------------+----------------+------------------+---------------------------+ nebula> MATCH (a:player)-[e]-() \\ RETURN type(e); +----------+ | type(e) | +----------+ | \"follow\" | | \"follow\" | +----------+ nebula> MATCH (a:player {name: \"Tim Duncan\"}) \\ WHERE toInteger(right(id(a),3)) == 100 \\ RETURN a; +----------------------------------------------------+ | a | +----------------------------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | +----------------------------------------------------+ nebula> MATCH (n:player) \\ WITH n LIMIT toInteger(ceil(1.8)) \\ RETURN count(*) AS count; +-------+ | count | +-------+ | 2 | +-------+ nebula> RETURN toSet(list[1,2,3,1,2]) AS list2set; +-----------+ | list2set | +-----------+ | {3, 1, 2} | +-----------+ nebula> RETURN toSet(set{1,2,3,1,2}) AS set2set; +-----------+ | set2set | +-----------+ | {3, 2, 1} | +-----------+","title":"Examples"},{"location":"3.ngql-guide/4.variable-and-composite-queries/1.composite-queries/","text":"Composite queries (clause structure) \u00b6 Composite queries put data from different queries together. They then use filters, group-bys, or sorting before returning the combined return results. Nebula Graph supports three methods to run composite queries (or sub-queries): (openCypher) Clauses are chained together, and they feed intermediate result sets between each other. (Native nGQL) More than one query can be batched together, separated by semicolons (;). The result of the last query is returned as the result of the batch. (Native nGQL) Queries can be piped together by using the pipe ( | ). The result of the previous query can be used as the input of the next query. OpenCypher compatibility \u00b6 In a composite query, do not put together openCypher and native nGQL clauses in one statement. For example, this statement is undefined: MATCH ... | GO ... | YIELD ... . If you are in the openCypher way ( MATCH , RETURN , WITH , etc), do not introduce any pipe or semicolons to combine the sub-clauses. If you are in the native nGQL way ( FETCH , GO , LOOKUP , etc), you must use pipe or semicolons to combine the sub-clauses. Undefined behavior Do not put together native nGQL and openCypher compatible sentences in one composite statement because this behavior is undefined. Composite queries are not transactional queries (as in SQL/Cypher) \u00b6 For example, a query is composed of three sub-queries: A B C , A | B | C or A; B; C . In that A is a read operation, B is a computation operation, and C is a write operation. If any part fails in the execution, the whole result will be undefined. There is no rollback. What is written depends on the query executor. Note OpenCypher has no requirement of transaction . Examples \u00b6 OpenCypher compatibility statement # Connect multiple queries with clauses. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})--() \\ WITH nodes(p) AS n \\ UNWIND n AS n1 \\ RETURN DISTINCT n1; Native nGQL (Semicolon queries) # Only return edges. nebula> SHOW TAGS; SHOW EDGES; # Insert multiple vertices. nebula> INSERT VERTEX player(name, age) VALUES \"player100\":(\"Tim Duncan\", 42); \\ INSERT VERTEX player(name, age) VALUES \"player101\":(\"Tony Parker\", 36); \\ INSERT VERTEX player(name, age) VALUES \"player102\":(\"LaMarcus Aldridge\", 33); Native nGQL (Pipe queries) # Connect multiple queries with pipes. nebula> GO FROM \"player100\" OVER follow YIELD dst(edge) AS id | \\ GO FROM $-.id OVER serve YIELD properties($$).name AS Team, \\ properties($^).name AS Player; +-----------+-----------------+ | Team | Player | +-----------+-----------------+ | \"Spurs\" | \"Tony Parker\" | | \"Hornets\" | \"Tony Parker\" | | \"Spurs\" | \"Manu Ginobili\" | +-----------+-----------------+","title":"Composite queries"},{"location":"3.ngql-guide/4.variable-and-composite-queries/1.composite-queries/#composite_queries_clause_structure","text":"Composite queries put data from different queries together. They then use filters, group-bys, or sorting before returning the combined return results. Nebula Graph supports three methods to run composite queries (or sub-queries): (openCypher) Clauses are chained together, and they feed intermediate result sets between each other. (Native nGQL) More than one query can be batched together, separated by semicolons (;). The result of the last query is returned as the result of the batch. (Native nGQL) Queries can be piped together by using the pipe ( | ). The result of the previous query can be used as the input of the next query.","title":"Composite queries (clause structure)"},{"location":"3.ngql-guide/4.variable-and-composite-queries/1.composite-queries/#opencypher_compatibility","text":"In a composite query, do not put together openCypher and native nGQL clauses in one statement. For example, this statement is undefined: MATCH ... | GO ... | YIELD ... . If you are in the openCypher way ( MATCH , RETURN , WITH , etc), do not introduce any pipe or semicolons to combine the sub-clauses. If you are in the native nGQL way ( FETCH , GO , LOOKUP , etc), you must use pipe or semicolons to combine the sub-clauses. Undefined behavior Do not put together native nGQL and openCypher compatible sentences in one composite statement because this behavior is undefined.","title":"OpenCypher compatibility"},{"location":"3.ngql-guide/4.variable-and-composite-queries/1.composite-queries/#composite_queries_are_not_transactional_queries_as_in_sqlcypher","text":"For example, a query is composed of three sub-queries: A B C , A | B | C or A; B; C . In that A is a read operation, B is a computation operation, and C is a write operation. If any part fails in the execution, the whole result will be undefined. There is no rollback. What is written depends on the query executor. Note OpenCypher has no requirement of transaction .","title":"Composite queries are not transactional queries (as in SQL/Cypher)"},{"location":"3.ngql-guide/4.variable-and-composite-queries/1.composite-queries/#examples","text":"OpenCypher compatibility statement # Connect multiple queries with clauses. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})--() \\ WITH nodes(p) AS n \\ UNWIND n AS n1 \\ RETURN DISTINCT n1; Native nGQL (Semicolon queries) # Only return edges. nebula> SHOW TAGS; SHOW EDGES; # Insert multiple vertices. nebula> INSERT VERTEX player(name, age) VALUES \"player100\":(\"Tim Duncan\", 42); \\ INSERT VERTEX player(name, age) VALUES \"player101\":(\"Tony Parker\", 36); \\ INSERT VERTEX player(name, age) VALUES \"player102\":(\"LaMarcus Aldridge\", 33); Native nGQL (Pipe queries) # Connect multiple queries with pipes. nebula> GO FROM \"player100\" OVER follow YIELD dst(edge) AS id | \\ GO FROM $-.id OVER serve YIELD properties($$).name AS Team, \\ properties($^).name AS Player; +-----------+-----------------+ | Team | Player | +-----------+-----------------+ | \"Spurs\" | \"Tony Parker\" | | \"Hornets\" | \"Tony Parker\" | | \"Spurs\" | \"Manu Ginobili\" | +-----------+-----------------+","title":"Examples"},{"location":"3.ngql-guide/4.variable-and-composite-queries/2.user-defined-variables/","text":"User-defined variables \u00b6 User-defined variables allow passing the result of one statement to another. OpenCypher compatibility \u00b6 In openCypher, when you refer to the vertex, edge, or path of a variable, you need to name it first. For example: nebula> MATCH (v:player{name:\"Tim Duncan\"}) RETURN v; +----------------------------------------------------+ | v | +----------------------------------------------------+ | (\"player100\" :player{name: \"Tim Duncan\", age: 42}) | +----------------------------------------------------+ The user-defined variable in the preceding query is v . Native nGQL \u00b6 User-defined variables are written as $var_name . The var_name consists of letters, numbers, or underline characters. Any other characters are not permitted. The user-defined variables are valid only at the current execution (namely, in this composite query). When the execution ends, the user-defined variables will be automatically expired. The user-defined variables in one statement CANNOT be used in any other clients, executions, or sessions. You can use user-defined variables in composite queries. Details about composite queries, see Composite queries . Note User-defined variables are case-sensitive. Example \u00b6 nebula> $var = GO FROM \"player100\" OVER follow YIELD dst(edge) AS id; \\ GO FROM $var.id OVER serve YIELD properties($$).name AS Team, \\ properties($^).name AS Player; +-----------+-----------------+ | Team | Player | +-----------+-----------------+ | \"Spurs\" | \"Tony Parker\" | | \"Hornets\" | \"Tony Parker\" | | \"Spurs\" | \"Manu Ginobili\" | +-----------+-----------------+","title":"User-defined variables"},{"location":"3.ngql-guide/4.variable-and-composite-queries/2.user-defined-variables/#user-defined_variables","text":"User-defined variables allow passing the result of one statement to another.","title":"User-defined variables"},{"location":"3.ngql-guide/4.variable-and-composite-queries/2.user-defined-variables/#opencypher_compatibility","text":"In openCypher, when you refer to the vertex, edge, or path of a variable, you need to name it first. For example: nebula> MATCH (v:player{name:\"Tim Duncan\"}) RETURN v; +----------------------------------------------------+ | v | +----------------------------------------------------+ | (\"player100\" :player{name: \"Tim Duncan\", age: 42}) | +----------------------------------------------------+ The user-defined variable in the preceding query is v .","title":"OpenCypher compatibility"},{"location":"3.ngql-guide/4.variable-and-composite-queries/2.user-defined-variables/#native_ngql","text":"User-defined variables are written as $var_name . The var_name consists of letters, numbers, or underline characters. Any other characters are not permitted. The user-defined variables are valid only at the current execution (namely, in this composite query). When the execution ends, the user-defined variables will be automatically expired. The user-defined variables in one statement CANNOT be used in any other clients, executions, or sessions. You can use user-defined variables in composite queries. Details about composite queries, see Composite queries . Note User-defined variables are case-sensitive.","title":"Native nGQL"},{"location":"3.ngql-guide/4.variable-and-composite-queries/2.user-defined-variables/#example","text":"nebula> $var = GO FROM \"player100\" OVER follow YIELD dst(edge) AS id; \\ GO FROM $var.id OVER serve YIELD properties($$).name AS Team, \\ properties($^).name AS Player; +-----------+-----------------+ | Team | Player | +-----------+-----------------+ | \"Spurs\" | \"Tony Parker\" | | \"Hornets\" | \"Tony Parker\" | | \"Spurs\" | \"Manu Ginobili\" | +-----------+-----------------+","title":"Example"},{"location":"3.ngql-guide/4.variable-and-composite-queries/3.property-reference/","text":"Property reference \u00b6 You can refer to the properties of a vertex or an edge in WHERE and YIELD syntax. Note This function applies to native nGQL only. Property reference for vertex \u00b6 For source vertex \u00b6 $^.<tag_name>.<prop_name> Parameter Description $^ is used to get the property of the source vertex. tag_name is the tag name of the vertex. prop_name specifies the property name. For destination vertex \u00b6 $$.<tag_name>.<prop_name> Parameter Description $$ is used to get the property of the destination vertex. tag_name is the tag name of the vertex. prop_name specifies the property name. Property reference for edge \u00b6 For user-defined edge property \u00b6 <edge_type>.<prop_name> Parameter Description edge_type is the edge type of the edge. prop_name specifies the property name of the edge type. For built-in properties \u00b6 Apart from the user-defined edge property, there are four built-in properties in each edge: Parameter Description _src source vertex ID of the edge _dst destination vertex ID of the edge _type edge type _rank the rank value for the edge Examples \u00b6 The following query returns the name property of the player tag on the source vertex and the age property of the player tag on the destination vertex. nebula> GO FROM \"player100\" OVER follow YIELD $^.player.name AS startName, $$.player.age AS endAge; +--------------+--------+ | startName | endAge | +--------------+--------+ | \"Tim Duncan\" | 36 | | \"Tim Duncan\" | 41 | +--------------+--------+ The following query returns the degree property of the edge type follow . nebula> GO FROM \"player100\" OVER follow YIELD follow.degree; +---------------+ | follow.degree | +---------------+ | 95 | +---------------+ The following query returns the source vertex, the destination vertex, the edge type, and the edge rank value of the edge type follow . nebula> GO FROM \"player100\" OVER follow YIELD follow._src, follow._dst, follow._type, follow._rank; +-------------+-------------+--------------+--------------+ | follow._src | follow._dst | follow._type | follow._rank | +-------------+-------------+--------------+--------------+ | \"player100\" | \"player101\" | 17 | 0 | | \"player100\" | \"player125\" | 17 | 0 | +-------------+-------------+--------------+--------------+ Legacy version compatibility Nebula Graph 2.6.0 and later versions support the new Schema function . The statements in the above examples are written as follows in 2.6.0. GO FROM \"player100\" OVER follow YIELD properties($^).name AS startName, properties($$).age AS endAge; GO FROM \"player100\" OVER follow YIELD properties(edge).degree; GO FROM \"player100\" OVER follow YIELD src(edge), dst(edge), type(edge), rank(edge); In 2.6.0, Nebula Graph is still compatible with the old syntax.","title":"Property reference"},{"location":"3.ngql-guide/4.variable-and-composite-queries/3.property-reference/#property_reference","text":"You can refer to the properties of a vertex or an edge in WHERE and YIELD syntax. Note This function applies to native nGQL only.","title":"Property reference"},{"location":"3.ngql-guide/4.variable-and-composite-queries/3.property-reference/#property_reference_for_vertex","text":"","title":"Property reference for vertex"},{"location":"3.ngql-guide/4.variable-and-composite-queries/3.property-reference/#for_source_vertex","text":"$^.<tag_name>.<prop_name> Parameter Description $^ is used to get the property of the source vertex. tag_name is the tag name of the vertex. prop_name specifies the property name.","title":"For source vertex"},{"location":"3.ngql-guide/4.variable-and-composite-queries/3.property-reference/#for_destination_vertex","text":"$$.<tag_name>.<prop_name> Parameter Description $$ is used to get the property of the destination vertex. tag_name is the tag name of the vertex. prop_name specifies the property name.","title":"For destination vertex"},{"location":"3.ngql-guide/4.variable-and-composite-queries/3.property-reference/#property_reference_for_edge","text":"","title":"Property reference for edge"},{"location":"3.ngql-guide/4.variable-and-composite-queries/3.property-reference/#for_user-defined_edge_property","text":"<edge_type>.<prop_name> Parameter Description edge_type is the edge type of the edge. prop_name specifies the property name of the edge type.","title":"For user-defined edge property"},{"location":"3.ngql-guide/4.variable-and-composite-queries/3.property-reference/#for_built-in_properties","text":"Apart from the user-defined edge property, there are four built-in properties in each edge: Parameter Description _src source vertex ID of the edge _dst destination vertex ID of the edge _type edge type _rank the rank value for the edge","title":"For built-in properties"},{"location":"3.ngql-guide/4.variable-and-composite-queries/3.property-reference/#examples","text":"The following query returns the name property of the player tag on the source vertex and the age property of the player tag on the destination vertex. nebula> GO FROM \"player100\" OVER follow YIELD $^.player.name AS startName, $$.player.age AS endAge; +--------------+--------+ | startName | endAge | +--------------+--------+ | \"Tim Duncan\" | 36 | | \"Tim Duncan\" | 41 | +--------------+--------+ The following query returns the degree property of the edge type follow . nebula> GO FROM \"player100\" OVER follow YIELD follow.degree; +---------------+ | follow.degree | +---------------+ | 95 | +---------------+ The following query returns the source vertex, the destination vertex, the edge type, and the edge rank value of the edge type follow . nebula> GO FROM \"player100\" OVER follow YIELD follow._src, follow._dst, follow._type, follow._rank; +-------------+-------------+--------------+--------------+ | follow._src | follow._dst | follow._type | follow._rank | +-------------+-------------+--------------+--------------+ | \"player100\" | \"player101\" | 17 | 0 | | \"player100\" | \"player125\" | 17 | 0 | +-------------+-------------+--------------+--------------+ Legacy version compatibility Nebula Graph 2.6.0 and later versions support the new Schema function . The statements in the above examples are written as follows in 2.6.0. GO FROM \"player100\" OVER follow YIELD properties($^).name AS startName, properties($$).age AS endAge; GO FROM \"player100\" OVER follow YIELD properties(edge).degree; GO FROM \"player100\" OVER follow YIELD src(edge), dst(edge), type(edge), rank(edge); In 2.6.0, Nebula Graph is still compatible with the old syntax.","title":"Examples"},{"location":"3.ngql-guide/5.operators/1.comparison/","text":"Comparison operators \u00b6 Nebula Graph supports the following comparison operators. Name Description = Assigns a value + Addition operator - Minus operator * Multiplication operator / Division operator == Equal operator != , <> Not equal operator > Greater than operator >= Greater than or equal operator < Less than operator <= Less than or equal operator % Modulo operator - Changes the sign of the argument IS NULL NULL check IS NOT NULL Not NULL check IS EMPTY EMPTY check IS NOT EMPTY Not EMPTY check The result of the comparison operation is true or false . Note Comparability between values of different types is often undefined. The result could be NULL or others. EMPTY is currently used only for checking, and does not support functions or operations such as GROUP BY , count() , sum() , max() , hash() , collect() , + or * . OpenCypher compatibility \u00b6 openCypher does not have EMPTY . Thus EMPTY is not supported in MATCH statements. Examples \u00b6 == \u00b6 String comparisons are case-sensitive. Values of different types are not equal. Note The equal operator is == in nGQL, while in openCypher it is = . nebula> RETURN 'A' == 'a', toUpper('A') == toUpper('a'), toLower('A') == toLower('a'); +------------+------------------------------+------------------------------+ | (\"A\"==\"a\") | (toUpper(\"A\")==toUpper(\"a\")) | (toLower(\"A\")==toLower(\"a\")) | +------------+------------------------------+------------------------------+ | false | true | true | +------------+------------------------------+------------------------------+ nebula> RETURN '2' == 2, toInteger('2') == 2; +----------+---------------------+ | (\"2\"==2) | (toInteger(\"2\")==2) | +----------+---------------------+ | false | true | +----------+---------------------+ > \u00b6 nebula> RETURN 3 > 2; +-------+ | (3>2) | +-------+ | true | +-------+ nebula> WITH 4 AS one, 3 AS two \\ RETURN one > two AS result; +--------+ | result | +--------+ | true | +--------+ >= \u00b6 nebula> RETURN 2 >= \"2\", 2 >= 2; +----------+--------+ | (2>=\"2\") | (2>=2) | +----------+--------+ | __NULL__ | true | +----------+--------+ < \u00b6 nebula> YIELD 2.0 < 1.9; +---------+ | (2<1.9) | +---------+ | false | +---------+ <= \u00b6 nebula> YIELD 0.11 <= 0.11; +--------------+ | (0.11<=0.11) | +--------------+ | true | +--------------+ != \u00b6 nebula> YIELD 1 != '1'; +----------+ | (1!=\"1\") | +----------+ | true | +----------+ IS [NOT] NULL \u00b6 nebula> RETURN null IS NULL AS value1, null == null AS value2, null != null AS value3; +--------+----------+----------+ | value1 | value2 | value3 | +--------+----------+----------+ | true | __NULL__ | __NULL__ | +--------+----------+----------+ nebula> RETURN length(NULL), size(NULL), count(NULL), NULL IS NULL, NULL IS NOT NULL, sin(NULL), NULL + NULL, [1, NULL] IS NULL; +--------------+------------+-------------+--------------+------------------+-----------+-------------+------------------+ | length(NULL) | size(NULL) | count(NULL) | NULL IS NULL | NULL IS NOT NULL | sin(NULL) | (NULL+NULL) | [1,NULL] IS NULL | +--------------+------------+-------------+--------------+------------------+-----------+-------------+------------------+ | __NULL__ | __NULL__ | 0 | true | false | __NULL__ | __NULL__ | false | +--------------+------------+-------------+--------------+------------------+-----------+-------------+------------------+ nebula> WITH {name: null} AS map \\ RETURN map.name IS NOT NULL; +----------------------+ | map.name IS NOT NULL | +----------------------+ | false | +----------------------+ nebula> WITH {name: 'Mats', name2: 'Pontus'} AS map1, \\ {name: null} AS map2, {notName: 0, notName2: null } AS map3 \\ RETURN map1.name IS NULL, map2.name IS NOT NULL, map3.name IS NULL; +-------------------+-----------------------+-------------------+ | map1.name IS NULL | map2.name IS NOT NULL | map3.name IS NULL | +-------------------+-----------------------+-------------------+ | false | false | true | +-------------------+-----------------------+-------------------+ nebula> MATCH (n:player) \\ RETURN n.player.age IS NULL, n.player.name IS NOT NULL, n.player.empty IS NULL; +----------------------+---------------------------+------------------------+ | n.player.age IS NULL | n.player.name IS NOT NULL | n.player.empty IS NULL | +----------------------+---------------------------+------------------------+ | false | true | true | | false | true | true | ... IS [NOT] EMPTY \u00b6 nebula> RETURN null IS EMPTY; +---------------+ | NULL IS EMPTY | +---------------+ | false | +---------------+ nebula> RETURN \"a\" IS NOT EMPTY; +------------------+ | \"a\" IS NOT EMPTY | +------------------+ | true | +------------------+ nebula> GO FROM \"player100\" OVER * WHERE properties($$).name IS NOT EMPTY YIELD dst(edge); +-------------+ | dst(EDGE) | +-------------+ | \"team204\" | | \"player101\" | | \"player125\" | +-------------+","title":"Comparison"},{"location":"3.ngql-guide/5.operators/1.comparison/#comparison_operators","text":"Nebula Graph supports the following comparison operators. Name Description = Assigns a value + Addition operator - Minus operator * Multiplication operator / Division operator == Equal operator != , <> Not equal operator > Greater than operator >= Greater than or equal operator < Less than operator <= Less than or equal operator % Modulo operator - Changes the sign of the argument IS NULL NULL check IS NOT NULL Not NULL check IS EMPTY EMPTY check IS NOT EMPTY Not EMPTY check The result of the comparison operation is true or false . Note Comparability between values of different types is often undefined. The result could be NULL or others. EMPTY is currently used only for checking, and does not support functions or operations such as GROUP BY , count() , sum() , max() , hash() , collect() , + or * .","title":"Comparison operators"},{"location":"3.ngql-guide/5.operators/1.comparison/#opencypher_compatibility","text":"openCypher does not have EMPTY . Thus EMPTY is not supported in MATCH statements.","title":"OpenCypher compatibility"},{"location":"3.ngql-guide/5.operators/1.comparison/#examples","text":"","title":"Examples"},{"location":"3.ngql-guide/5.operators/1.comparison/#_1","text":"String comparisons are case-sensitive. Values of different types are not equal. Note The equal operator is == in nGQL, while in openCypher it is = . nebula> RETURN 'A' == 'a', toUpper('A') == toUpper('a'), toLower('A') == toLower('a'); +------------+------------------------------+------------------------------+ | (\"A\"==\"a\") | (toUpper(\"A\")==toUpper(\"a\")) | (toLower(\"A\")==toLower(\"a\")) | +------------+------------------------------+------------------------------+ | false | true | true | +------------+------------------------------+------------------------------+ nebula> RETURN '2' == 2, toInteger('2') == 2; +----------+---------------------+ | (\"2\"==2) | (toInteger(\"2\")==2) | +----------+---------------------+ | false | true | +----------+---------------------+","title":"=="},{"location":"3.ngql-guide/5.operators/1.comparison/#_2","text":"nebula> RETURN 3 > 2; +-------+ | (3>2) | +-------+ | true | +-------+ nebula> WITH 4 AS one, 3 AS two \\ RETURN one > two AS result; +--------+ | result | +--------+ | true | +--------+","title":"&gt;"},{"location":"3.ngql-guide/5.operators/1.comparison/#_3","text":"nebula> RETURN 2 >= \"2\", 2 >= 2; +----------+--------+ | (2>=\"2\") | (2>=2) | +----------+--------+ | __NULL__ | true | +----------+--------+","title":"&gt;="},{"location":"3.ngql-guide/5.operators/1.comparison/#_4","text":"nebula> YIELD 2.0 < 1.9; +---------+ | (2<1.9) | +---------+ | false | +---------+","title":"&lt;"},{"location":"3.ngql-guide/5.operators/1.comparison/#_5","text":"nebula> YIELD 0.11 <= 0.11; +--------------+ | (0.11<=0.11) | +--------------+ | true | +--------------+","title":"&lt;="},{"location":"3.ngql-guide/5.operators/1.comparison/#_6","text":"nebula> YIELD 1 != '1'; +----------+ | (1!=\"1\") | +----------+ | true | +----------+","title":"!="},{"location":"3.ngql-guide/5.operators/1.comparison/#is_not_null","text":"nebula> RETURN null IS NULL AS value1, null == null AS value2, null != null AS value3; +--------+----------+----------+ | value1 | value2 | value3 | +--------+----------+----------+ | true | __NULL__ | __NULL__ | +--------+----------+----------+ nebula> RETURN length(NULL), size(NULL), count(NULL), NULL IS NULL, NULL IS NOT NULL, sin(NULL), NULL + NULL, [1, NULL] IS NULL; +--------------+------------+-------------+--------------+------------------+-----------+-------------+------------------+ | length(NULL) | size(NULL) | count(NULL) | NULL IS NULL | NULL IS NOT NULL | sin(NULL) | (NULL+NULL) | [1,NULL] IS NULL | +--------------+------------+-------------+--------------+------------------+-----------+-------------+------------------+ | __NULL__ | __NULL__ | 0 | true | false | __NULL__ | __NULL__ | false | +--------------+------------+-------------+--------------+------------------+-----------+-------------+------------------+ nebula> WITH {name: null} AS map \\ RETURN map.name IS NOT NULL; +----------------------+ | map.name IS NOT NULL | +----------------------+ | false | +----------------------+ nebula> WITH {name: 'Mats', name2: 'Pontus'} AS map1, \\ {name: null} AS map2, {notName: 0, notName2: null } AS map3 \\ RETURN map1.name IS NULL, map2.name IS NOT NULL, map3.name IS NULL; +-------------------+-----------------------+-------------------+ | map1.name IS NULL | map2.name IS NOT NULL | map3.name IS NULL | +-------------------+-----------------------+-------------------+ | false | false | true | +-------------------+-----------------------+-------------------+ nebula> MATCH (n:player) \\ RETURN n.player.age IS NULL, n.player.name IS NOT NULL, n.player.empty IS NULL; +----------------------+---------------------------+------------------------+ | n.player.age IS NULL | n.player.name IS NOT NULL | n.player.empty IS NULL | +----------------------+---------------------------+------------------------+ | false | true | true | | false | true | true | ...","title":"IS [NOT] NULL"},{"location":"3.ngql-guide/5.operators/1.comparison/#is_not_empty","text":"nebula> RETURN null IS EMPTY; +---------------+ | NULL IS EMPTY | +---------------+ | false | +---------------+ nebula> RETURN \"a\" IS NOT EMPTY; +------------------+ | \"a\" IS NOT EMPTY | +------------------+ | true | +------------------+ nebula> GO FROM \"player100\" OVER * WHERE properties($$).name IS NOT EMPTY YIELD dst(edge); +-------------+ | dst(EDGE) | +-------------+ | \"team204\" | | \"player101\" | | \"player125\" | +-------------+","title":"IS [NOT] EMPTY"},{"location":"3.ngql-guide/5.operators/2.boolean/","text":"Boolean operators \u00b6 Nebula Graph supports the following boolean operators. Name Description AND Logical AND NOT Logical NOT OR Logical OR XOR Logical XOR For the precedence of the operators, refer to Operator Precedence . For the logical operations with NULL , refer to NULL . Legacy version compatibility \u00b6 Non-zero numbers cannot be converted to boolean values.","title":"Boolean"},{"location":"3.ngql-guide/5.operators/2.boolean/#boolean_operators","text":"Nebula Graph supports the following boolean operators. Name Description AND Logical AND NOT Logical NOT OR Logical OR XOR Logical XOR For the precedence of the operators, refer to Operator Precedence . For the logical operations with NULL , refer to NULL .","title":"Boolean operators"},{"location":"3.ngql-guide/5.operators/2.boolean/#legacy_version_compatibility","text":"Non-zero numbers cannot be converted to boolean values.","title":"Legacy version compatibility"},{"location":"3.ngql-guide/5.operators/4.pipe/","text":"Pipe operators \u00b6 Multiple queries can be combined using pipe operators in nGQL. OpenCypher compatibility \u00b6 Pipe operators apply to native nGQL only. Syntax \u00b6 One major difference between nGQL and SQL is how sub-queries are composed. In SQL, sub-queries are nested in the query statements. In nGQL, the shell style PIPE (|) is introduced into the sub-queries. Examples \u00b6 nebula> GO FROM \"player100\" OVER follow \\ YIELD dst(edge) AS dstid, properties($$).name AS Name | \\ GO FROM $-.dstid OVER follow YIELD dst(edge); +-------------+ | dst(EDGE) | +-------------+ | \"player100\" | | \"player102\" | | \"player125\" | | \"player100\" | +-------------+ If there is no YIELD clause to define the output, the destination vertex ID is returned by default. If a YIELD clause is applied, the output is defined by the YIELD clause. Users must define aliases in the YIELD clause for the reference operator $- to use, just like $-.dstid in the preceding example. Performance tips \u00b6 In Nebula Graph, pipes will affect the performance. Take A | B as an example, the effects are as follows: Pipe operators operate synchronously. That is, the data can enter the pipe clause as a whole after the execution of clause A before the pipe operator is completed. Pipe operators need to be serialized and deserialized, which is executed in a single thread. If A sends a large amount of data to | , the entire query request may be very slow. You can try to split this statement. Send A from the application, Split the return results on the application, Send to multiple graphd processes concurrently, Every graphd process executes part of B. This is usually much faster than executing a complete A | B with a single graphd process.","title":"Pipe"},{"location":"3.ngql-guide/5.operators/4.pipe/#pipe_operators","text":"Multiple queries can be combined using pipe operators in nGQL.","title":"Pipe operators"},{"location":"3.ngql-guide/5.operators/4.pipe/#opencypher_compatibility","text":"Pipe operators apply to native nGQL only.","title":"OpenCypher compatibility"},{"location":"3.ngql-guide/5.operators/4.pipe/#syntax","text":"One major difference between nGQL and SQL is how sub-queries are composed. In SQL, sub-queries are nested in the query statements. In nGQL, the shell style PIPE (|) is introduced into the sub-queries.","title":"Syntax"},{"location":"3.ngql-guide/5.operators/4.pipe/#examples","text":"nebula> GO FROM \"player100\" OVER follow \\ YIELD dst(edge) AS dstid, properties($$).name AS Name | \\ GO FROM $-.dstid OVER follow YIELD dst(edge); +-------------+ | dst(EDGE) | +-------------+ | \"player100\" | | \"player102\" | | \"player125\" | | \"player100\" | +-------------+ If there is no YIELD clause to define the output, the destination vertex ID is returned by default. If a YIELD clause is applied, the output is defined by the YIELD clause. Users must define aliases in the YIELD clause for the reference operator $- to use, just like $-.dstid in the preceding example.","title":"Examples"},{"location":"3.ngql-guide/5.operators/4.pipe/#performance_tips","text":"In Nebula Graph, pipes will affect the performance. Take A | B as an example, the effects are as follows: Pipe operators operate synchronously. That is, the data can enter the pipe clause as a whole after the execution of clause A before the pipe operator is completed. Pipe operators need to be serialized and deserialized, which is executed in a single thread. If A sends a large amount of data to | , the entire query request may be very slow. You can try to split this statement. Send A from the application, Split the return results on the application, Send to multiple graphd processes concurrently, Every graphd process executes part of B. This is usually much faster than executing a complete A | B with a single graphd process.","title":"Performance tips"},{"location":"3.ngql-guide/5.operators/5.property-reference/","text":"Reference operators \u00b6 NGQL provides reference operators to represent a property in a WHERE or YIELD clause, or the output of the statement before the pipe operator in a composite query. OpenCypher compatibility \u00b6 Reference operators apply to native nGQL only. Reference operator List \u00b6 Reference operator Description $^ Refers to a source vertex property. For more information, see Property reference . $$ Refers to a destination vertex property. For more information, see Property reference . $- Refers to the output of the statement before the pipe operator in a composite query. For more information, see Pipe . Examples \u00b6 # The following example returns the age of the source vertex and the destination vertex. nebula> GO FROM \"player100\" OVER follow YIELD properties($^).age AS SrcAge, properties($$).age AS DestAge; +--------+---------+ | SrcAge | DestAge | +--------+---------+ | 42 | 36 | | 42 | 41 | +--------+---------+ # The following example returns the name and team of the players that player100 follows. nebula> GO FROM \"player100\" OVER follow \\ YIELD dst(edge) AS id | \\ GO FROM $-.id OVER serve \\ YIELD $^.player.name AS Player, properties($$).name AS Team; +-----------------+-----------+ | Player | Team | +-----------------+-----------+ | \"Tony Parker\" | \"Spurs\" | | \"Tony Parker\" | \"Hornets\" | | \"Manu Ginobili\" | \"Spurs\" | +-----------------+-----------+","title":"Property reference"},{"location":"3.ngql-guide/5.operators/5.property-reference/#reference_operators","text":"NGQL provides reference operators to represent a property in a WHERE or YIELD clause, or the output of the statement before the pipe operator in a composite query.","title":"Reference operators"},{"location":"3.ngql-guide/5.operators/5.property-reference/#opencypher_compatibility","text":"Reference operators apply to native nGQL only.","title":"OpenCypher compatibility"},{"location":"3.ngql-guide/5.operators/5.property-reference/#reference_operator_list","text":"Reference operator Description $^ Refers to a source vertex property. For more information, see Property reference . $$ Refers to a destination vertex property. For more information, see Property reference . $- Refers to the output of the statement before the pipe operator in a composite query. For more information, see Pipe .","title":"Reference operator List"},{"location":"3.ngql-guide/5.operators/5.property-reference/#examples","text":"# The following example returns the age of the source vertex and the destination vertex. nebula> GO FROM \"player100\" OVER follow YIELD properties($^).age AS SrcAge, properties($$).age AS DestAge; +--------+---------+ | SrcAge | DestAge | +--------+---------+ | 42 | 36 | | 42 | 41 | +--------+---------+ # The following example returns the name and team of the players that player100 follows. nebula> GO FROM \"player100\" OVER follow \\ YIELD dst(edge) AS id | \\ GO FROM $-.id OVER serve \\ YIELD $^.player.name AS Player, properties($$).name AS Team; +-----------------+-----------+ | Player | Team | +-----------------+-----------+ | \"Tony Parker\" | \"Spurs\" | | \"Tony Parker\" | \"Hornets\" | | \"Manu Ginobili\" | \"Spurs\" | +-----------------+-----------+","title":"Examples"},{"location":"3.ngql-guide/5.operators/6.set/","text":"Set operators \u00b6 This topic will describe the set operators, including UNION , UNION ALL , INTERSECT , and MINUS . To combine multiple queries, use these set operators. All set operators have equal precedence. If a nGQL statement contains multiple set operators, Nebula Graph will evaluate them from left to right unless parentheses explicitly specify another order. OpenCypher compatibility \u00b6 Set operators apply to native nGQL only. UNION, UNION DISTINCT, and UNION ALL \u00b6 <left> UNION [DISTINCT | ALL] <right> [ UNION [DISTINCT | ALL] <right> ...] Operator UNION DISTINCT (or by short UNION ) returns the union of two sets A and B without duplicated elements. Operator UNION ALL returns the union of two sets A and B with duplicated elements. The <left> and <right> must have the same number of columns and data types. Different data types are converted according to the Type Conversion . Examples \u00b6 # The following statement returns the union of two query results without duplicated elements. nebula> GO FROM \"player102\" OVER follow YIELD dst(edge) \\ UNION \\ GO FROM \"player100\" OVER follow YIELD dst(edge); +-------------+ | dst(EDGE) | +-------------+ | \"player100\" | | \"player101\" | | \"player125\" | +-------------+ # The following statement returns the union of two query results with duplicated elements. nebula> GO FROM \"player102\" OVER follow YIELD dst(edge) \\ UNION ALL \\ GO FROM \"player100\" OVER follow YIELD dst(edge); +-------------+ | dst(EDGE) | +-------------+ | \"player100\" | | \"player101\" | | \"player101\" | | \"player125\" | +-------------+ # UNION can also work with the YIELD statement. The DISTINCT keyword will check duplication by all the columns for every line, and remove duplicated lines if every column is the same. nebula> GO FROM \"player102\" OVER follow \\ YIELD dst(edge) AS id, properties(edge).degree AS Degree, properties($$).age AS Age \\ UNION /* DISTINCT */ \\ GO FROM \"player100\" OVER follow \\ YIELD dst(edge) AS id, properties(edge).degree AS Degree, properties($$).age AS Age; +-------------+--------+-----+ | id | Degree | Age | +-------------+--------+-----+ | \"player100\" | 75 | 42 | | \"player101\" | 75 | 36 | | \"player101\" | 95 | 36 | | \"player125\" | 95 | 41 | +-------------+--------+-----+ INTERSECT \u00b6 <left> INTERSECT <right> Operator INTERSECT returns the intersection of two sets A and B (denoted by A \u22c2 B). Similar to UNION , the left and right must have the same number of columns and data types. Different data types are converted according to the Type Conversion . Example \u00b6 nebula> GO FROM \"player102\" OVER follow \\ YIELD dst(edge) AS id, properties(edge).degree AS Degree, properties($$).age AS Age \\ INTERSECT \\ GO FROM \"player100\" OVER follow \\ YIELD dst(edge) AS id, properties(edge).degree AS Degree, properties($$).age AS Age; +----+--------+-----+ | id | Degree | Age | +----+--------+-----+ +----+--------+-----+ MINUS \u00b6 <left> MINUS <right> Operator MINUS returns the subtraction (or difference) of two sets A and B (denoted by A-B ). Always pay attention to the order of left and right . The set A-B consists of elements that are in A but not in B. Example \u00b6 nebula> GO FROM \"player100\" OVER follow YIELD dst(edge) \\ MINUS \\ GO FROM \"player102\" OVER follow YIELD dst(edge); +-------------+ | dst(EDGE) | +-------------+ | \"player125\" | +-------------+ nebula> GO FROM \"player102\" OVER follow YIELD dst(edge) \\ MINUS \\ GO FROM \"player100\" OVER follow YIELD dst(edge); +-------------+ | dst(EDGE) | +-------------+ | \"player100\" | +-------------+ Precedence of the set operators and pipe operators \u00b6 Please note that when a query contains a pipe | and a set operator, the pipe takes precedence. Refer to Pipe for details. The query GO FROM 1 UNION GO FROM 2 | GO FROM 3 is the same as the query GO FROM 1 UNION (GO FROM 2 | GO FROM 3) . Examples \u00b6 nebula> GO FROM \"player102\" OVER follow \\ YIELD dst(edge) AS play_dst \\ UNION \\ GO FROM \"team200\" OVER serve REVERSELY \\ YIELD src(edge) AS play_src \\ | GO FROM $-.play_src OVER follow YIELD dst(edge) AS play_dst; +-------------+ | play_dst | +-------------+ | \"player100\" | | \"player101\" | | \"player117\" | | \"player105\" | +-------------+ The above query executes the statements in the red bar first and then executes the statement in the green box. The parentheses can change the execution priority. For example: nebula> (GO FROM \"player102\" OVER follow \\ YIELD dst(edge) AS play_dst \\ UNION \\ GO FROM \"team200\" OVER serve REVERSELY \\ YIELD src(edge) AS play_dst) \\ | GO FROM $-.play_dst OVER follow YIELD dst(edge) AS play_dst; In the above query, the statements within the parentheses take precedence. That is, the UNION operation will be executed first, and its output will be executed as the input of the next operation with pipes.","title":"Set"},{"location":"3.ngql-guide/5.operators/6.set/#set_operators","text":"This topic will describe the set operators, including UNION , UNION ALL , INTERSECT , and MINUS . To combine multiple queries, use these set operators. All set operators have equal precedence. If a nGQL statement contains multiple set operators, Nebula Graph will evaluate them from left to right unless parentheses explicitly specify another order.","title":"Set operators"},{"location":"3.ngql-guide/5.operators/6.set/#opencypher_compatibility","text":"Set operators apply to native nGQL only.","title":"OpenCypher compatibility"},{"location":"3.ngql-guide/5.operators/6.set/#union_union_distinct_and_union_all","text":"<left> UNION [DISTINCT | ALL] <right> [ UNION [DISTINCT | ALL] <right> ...] Operator UNION DISTINCT (or by short UNION ) returns the union of two sets A and B without duplicated elements. Operator UNION ALL returns the union of two sets A and B with duplicated elements. The <left> and <right> must have the same number of columns and data types. Different data types are converted according to the Type Conversion .","title":"UNION, UNION DISTINCT, and UNION ALL"},{"location":"3.ngql-guide/5.operators/6.set/#examples","text":"# The following statement returns the union of two query results without duplicated elements. nebula> GO FROM \"player102\" OVER follow YIELD dst(edge) \\ UNION \\ GO FROM \"player100\" OVER follow YIELD dst(edge); +-------------+ | dst(EDGE) | +-------------+ | \"player100\" | | \"player101\" | | \"player125\" | +-------------+ # The following statement returns the union of two query results with duplicated elements. nebula> GO FROM \"player102\" OVER follow YIELD dst(edge) \\ UNION ALL \\ GO FROM \"player100\" OVER follow YIELD dst(edge); +-------------+ | dst(EDGE) | +-------------+ | \"player100\" | | \"player101\" | | \"player101\" | | \"player125\" | +-------------+ # UNION can also work with the YIELD statement. The DISTINCT keyword will check duplication by all the columns for every line, and remove duplicated lines if every column is the same. nebula> GO FROM \"player102\" OVER follow \\ YIELD dst(edge) AS id, properties(edge).degree AS Degree, properties($$).age AS Age \\ UNION /* DISTINCT */ \\ GO FROM \"player100\" OVER follow \\ YIELD dst(edge) AS id, properties(edge).degree AS Degree, properties($$).age AS Age; +-------------+--------+-----+ | id | Degree | Age | +-------------+--------+-----+ | \"player100\" | 75 | 42 | | \"player101\" | 75 | 36 | | \"player101\" | 95 | 36 | | \"player125\" | 95 | 41 | +-------------+--------+-----+","title":"Examples"},{"location":"3.ngql-guide/5.operators/6.set/#intersect","text":"<left> INTERSECT <right> Operator INTERSECT returns the intersection of two sets A and B (denoted by A \u22c2 B). Similar to UNION , the left and right must have the same number of columns and data types. Different data types are converted according to the Type Conversion .","title":"INTERSECT"},{"location":"3.ngql-guide/5.operators/6.set/#example","text":"nebula> GO FROM \"player102\" OVER follow \\ YIELD dst(edge) AS id, properties(edge).degree AS Degree, properties($$).age AS Age \\ INTERSECT \\ GO FROM \"player100\" OVER follow \\ YIELD dst(edge) AS id, properties(edge).degree AS Degree, properties($$).age AS Age; +----+--------+-----+ | id | Degree | Age | +----+--------+-----+ +----+--------+-----+","title":"Example"},{"location":"3.ngql-guide/5.operators/6.set/#minus","text":"<left> MINUS <right> Operator MINUS returns the subtraction (or difference) of two sets A and B (denoted by A-B ). Always pay attention to the order of left and right . The set A-B consists of elements that are in A but not in B.","title":"MINUS"},{"location":"3.ngql-guide/5.operators/6.set/#example_1","text":"nebula> GO FROM \"player100\" OVER follow YIELD dst(edge) \\ MINUS \\ GO FROM \"player102\" OVER follow YIELD dst(edge); +-------------+ | dst(EDGE) | +-------------+ | \"player125\" | +-------------+ nebula> GO FROM \"player102\" OVER follow YIELD dst(edge) \\ MINUS \\ GO FROM \"player100\" OVER follow YIELD dst(edge); +-------------+ | dst(EDGE) | +-------------+ | \"player100\" | +-------------+","title":"Example"},{"location":"3.ngql-guide/5.operators/6.set/#precedence_of_the_set_operators_and_pipe_operators","text":"Please note that when a query contains a pipe | and a set operator, the pipe takes precedence. Refer to Pipe for details. The query GO FROM 1 UNION GO FROM 2 | GO FROM 3 is the same as the query GO FROM 1 UNION (GO FROM 2 | GO FROM 3) .","title":"Precedence of the set operators and pipe operators"},{"location":"3.ngql-guide/5.operators/6.set/#examples_1","text":"nebula> GO FROM \"player102\" OVER follow \\ YIELD dst(edge) AS play_dst \\ UNION \\ GO FROM \"team200\" OVER serve REVERSELY \\ YIELD src(edge) AS play_src \\ | GO FROM $-.play_src OVER follow YIELD dst(edge) AS play_dst; +-------------+ | play_dst | +-------------+ | \"player100\" | | \"player101\" | | \"player117\" | | \"player105\" | +-------------+ The above query executes the statements in the red bar first and then executes the statement in the green box. The parentheses can change the execution priority. For example: nebula> (GO FROM \"player102\" OVER follow \\ YIELD dst(edge) AS play_dst \\ UNION \\ GO FROM \"team200\" OVER serve REVERSELY \\ YIELD src(edge) AS play_dst) \\ | GO FROM $-.play_dst OVER follow YIELD dst(edge) AS play_dst; In the above query, the statements within the parentheses take precedence. That is, the UNION operation will be executed first, and its output will be executed as the input of the next operation with pipes.","title":"Examples"},{"location":"3.ngql-guide/5.operators/7.string/","text":"String operators \u00b6 You can use the following string operators for concatenating, querying, and matching. Name Description + Concatenates strings. CONTAINS Performs searchings in strings. (NOT) IN Checks whether a value is within a set of values. (NOT) STARTS WITH Performs matchings at the beginning of a string. (NOT) ENDS WITH Performs matchings at the end of a string. Regular expressions Perform string matchings using regular expressions. Note All the string searchings or matchings are case-sensitive. Examples \u00b6 + \u00b6 nebula> RETURN 'a' + 'b'; +-----------+ | (\"a\"+\"b\") | +-----------+ | \"ab\" | +-----------+ nebula> UNWIND 'a' AS a UNWIND 'b' AS b RETURN a + b; +-------+ | (a+b) | +-------+ | \"ab\" | +-------+ CONTAINS \u00b6 The CONTAINS operator requires string types on both left and right sides. nebula> MATCH (s:player)-[e:serve]->(t:team) WHERE id(s) == \"player101\" \\ AND t.team.name CONTAINS \"ets\" RETURN s.player.name, e.start_year, e.end_year, t.team.name; +---------------+--------------+------------+-------------+ | s.player.name | e.start_year | e.end_year | t.team.name | +---------------+--------------+------------+-------------+ | \"Tony Parker\" | 2018 | 2019 | \"Hornets\" | +---------------+--------------+------------+-------------+ nebula> GO FROM \"player101\" OVER serve WHERE (STRING)properties(edge).start_year CONTAINS \"19\" AND \\ properties($^).name CONTAINS \"ny\" \\ YIELD properties($^).name, properties(edge).start_year, properties(edge).end_year, properties($$).name; +---------------------+-----------------------------+---------------------------+---------------------+ | properties($^).name | properties(EDGE).start_year | properties(EDGE).end_year | properties($$).name | +---------------------+-----------------------------+---------------------------+---------------------+ | \"Tony Parker\" | 1999 | 2018 | \"Spurs\" | +---------------------+-----------------------------+---------------------------+---------------------+ nebula> GO FROM \"player101\" OVER serve WHERE !(properties($$).name CONTAINS \"ets\") \\ YIELD properties($^).name, properties(edge).start_year, properties(edge).end_year, properties($$).name; +---------------------+-----------------------------+---------------------------+---------------------+ | properties($^).name | properties(EDGE).start_year | properties(EDGE).end_year | properties($$).name | +---------------------+-----------------------------+---------------------------+---------------------+ | \"Tony Parker\" | 1999 | 2018 | \"Spurs\" | +---------------------+-----------------------------+---------------------------+---------------------+ (NOT) IN \u00b6 nebula> RETURN 1 IN [1,2,3], \"Yao\" NOT IN [\"Yi\", \"Tim\", \"Kobe\"], NULL IN [\"Yi\", \"Tim\", \"Kobe\"]; +----------------+------------------------------------+-------------------------------+ | (1 IN [1,2,3]) | (\"Yao\" NOT IN [\"Yi\",\"Tim\",\"Kobe\"]) | (NULL IN [\"Yi\",\"Tim\",\"Kobe\"]) | +----------------+------------------------------------+-------------------------------+ | true | true | __NULL__ | +----------------+------------------------------------+-------------------------------+ (NOT) STARTS WITH \u00b6 nebula> RETURN 'apple' STARTS WITH 'app', 'apple' STARTS WITH 'a', 'apple' STARTS WITH toUpper('a'); +-----------------------------+---------------------------+------------------------------------+ | (\"apple\" STARTS WITH \"app\") | (\"apple\" STARTS WITH \"a\") | (\"apple\" STARTS WITH toUpper(\"a\")) | +-----------------------------+---------------------------+------------------------------------+ | true | true | false | +-----------------------------+---------------------------+------------------------------------+ nebula> RETURN 'apple' STARTS WITH 'b','apple' NOT STARTS WITH 'app'; +---------------------------+---------------------------------+ | (\"apple\" STARTS WITH \"b\") | (\"apple\" NOT STARTS WITH \"app\") | +---------------------------+---------------------------------+ | false | false | +---------------------------+---------------------------------+ (NOT) ENDS WITH \u00b6 nebula> RETURN 'apple' ENDS WITH 'app', 'apple' ENDS WITH 'e', 'apple' ENDS WITH 'E', 'apple' ENDS WITH 'b'; +---------------------------+-------------------------+-------------------------+-------------------------+ | (\"apple\" ENDS WITH \"app\") | (\"apple\" ENDS WITH \"e\") | (\"apple\" ENDS WITH \"E\") | (\"apple\" ENDS WITH \"b\") | +---------------------------+-------------------------+-------------------------+-------------------------+ | false | true | false | false | +---------------------------+-------------------------+-------------------------+-------------------------+ Regular expressions \u00b6 Note Regular expressions cannot work with native nGQL statements ( GO , FETCH , LOOKUP , etc.). Use it in openCypher only ( MATCH , WHERE , etc.). Nebula Graph supports filtering by using regular expressions. The regular expression syntax is inherited from std::regex . You can match on regular expressions by using =~ 'regexp' . For example: nebula> RETURN \"384748.39\" =~ \"\\\\d+(\\\\.\\\\d{2})?\"; +--------------------------------+ | (\"384748.39\"=~\"\\d+(\\.\\d{2})?\") | +--------------------------------+ | true | +--------------------------------+ nebula> MATCH (v:player) WHERE v.player.name =~ 'Tony.*' RETURN v.player.name; +---------------+ | v.player.name | +---------------+ | \"Tony Parker\" | +---------------+","title":"String"},{"location":"3.ngql-guide/5.operators/7.string/#string_operators","text":"You can use the following string operators for concatenating, querying, and matching. Name Description + Concatenates strings. CONTAINS Performs searchings in strings. (NOT) IN Checks whether a value is within a set of values. (NOT) STARTS WITH Performs matchings at the beginning of a string. (NOT) ENDS WITH Performs matchings at the end of a string. Regular expressions Perform string matchings using regular expressions. Note All the string searchings or matchings are case-sensitive.","title":"String operators"},{"location":"3.ngql-guide/5.operators/7.string/#examples","text":"","title":"Examples"},{"location":"3.ngql-guide/5.operators/7.string/#_1","text":"nebula> RETURN 'a' + 'b'; +-----------+ | (\"a\"+\"b\") | +-----------+ | \"ab\" | +-----------+ nebula> UNWIND 'a' AS a UNWIND 'b' AS b RETURN a + b; +-------+ | (a+b) | +-------+ | \"ab\" | +-------+","title":"+"},{"location":"3.ngql-guide/5.operators/7.string/#contains","text":"The CONTAINS operator requires string types on both left and right sides. nebula> MATCH (s:player)-[e:serve]->(t:team) WHERE id(s) == \"player101\" \\ AND t.team.name CONTAINS \"ets\" RETURN s.player.name, e.start_year, e.end_year, t.team.name; +---------------+--------------+------------+-------------+ | s.player.name | e.start_year | e.end_year | t.team.name | +---------------+--------------+------------+-------------+ | \"Tony Parker\" | 2018 | 2019 | \"Hornets\" | +---------------+--------------+------------+-------------+ nebula> GO FROM \"player101\" OVER serve WHERE (STRING)properties(edge).start_year CONTAINS \"19\" AND \\ properties($^).name CONTAINS \"ny\" \\ YIELD properties($^).name, properties(edge).start_year, properties(edge).end_year, properties($$).name; +---------------------+-----------------------------+---------------------------+---------------------+ | properties($^).name | properties(EDGE).start_year | properties(EDGE).end_year | properties($$).name | +---------------------+-----------------------------+---------------------------+---------------------+ | \"Tony Parker\" | 1999 | 2018 | \"Spurs\" | +---------------------+-----------------------------+---------------------------+---------------------+ nebula> GO FROM \"player101\" OVER serve WHERE !(properties($$).name CONTAINS \"ets\") \\ YIELD properties($^).name, properties(edge).start_year, properties(edge).end_year, properties($$).name; +---------------------+-----------------------------+---------------------------+---------------------+ | properties($^).name | properties(EDGE).start_year | properties(EDGE).end_year | properties($$).name | +---------------------+-----------------------------+---------------------------+---------------------+ | \"Tony Parker\" | 1999 | 2018 | \"Spurs\" | +---------------------+-----------------------------+---------------------------+---------------------+","title":"CONTAINS"},{"location":"3.ngql-guide/5.operators/7.string/#not_in","text":"nebula> RETURN 1 IN [1,2,3], \"Yao\" NOT IN [\"Yi\", \"Tim\", \"Kobe\"], NULL IN [\"Yi\", \"Tim\", \"Kobe\"]; +----------------+------------------------------------+-------------------------------+ | (1 IN [1,2,3]) | (\"Yao\" NOT IN [\"Yi\",\"Tim\",\"Kobe\"]) | (NULL IN [\"Yi\",\"Tim\",\"Kobe\"]) | +----------------+------------------------------------+-------------------------------+ | true | true | __NULL__ | +----------------+------------------------------------+-------------------------------+","title":"(NOT) IN"},{"location":"3.ngql-guide/5.operators/7.string/#not_starts_with","text":"nebula> RETURN 'apple' STARTS WITH 'app', 'apple' STARTS WITH 'a', 'apple' STARTS WITH toUpper('a'); +-----------------------------+---------------------------+------------------------------------+ | (\"apple\" STARTS WITH \"app\") | (\"apple\" STARTS WITH \"a\") | (\"apple\" STARTS WITH toUpper(\"a\")) | +-----------------------------+---------------------------+------------------------------------+ | true | true | false | +-----------------------------+---------------------------+------------------------------------+ nebula> RETURN 'apple' STARTS WITH 'b','apple' NOT STARTS WITH 'app'; +---------------------------+---------------------------------+ | (\"apple\" STARTS WITH \"b\") | (\"apple\" NOT STARTS WITH \"app\") | +---------------------------+---------------------------------+ | false | false | +---------------------------+---------------------------------+","title":"(NOT) STARTS WITH"},{"location":"3.ngql-guide/5.operators/7.string/#not_ends_with","text":"nebula> RETURN 'apple' ENDS WITH 'app', 'apple' ENDS WITH 'e', 'apple' ENDS WITH 'E', 'apple' ENDS WITH 'b'; +---------------------------+-------------------------+-------------------------+-------------------------+ | (\"apple\" ENDS WITH \"app\") | (\"apple\" ENDS WITH \"e\") | (\"apple\" ENDS WITH \"E\") | (\"apple\" ENDS WITH \"b\") | +---------------------------+-------------------------+-------------------------+-------------------------+ | false | true | false | false | +---------------------------+-------------------------+-------------------------+-------------------------+","title":"(NOT) ENDS WITH"},{"location":"3.ngql-guide/5.operators/7.string/#regular_expressions","text":"Note Regular expressions cannot work with native nGQL statements ( GO , FETCH , LOOKUP , etc.). Use it in openCypher only ( MATCH , WHERE , etc.). Nebula Graph supports filtering by using regular expressions. The regular expression syntax is inherited from std::regex . You can match on regular expressions by using =~ 'regexp' . For example: nebula> RETURN \"384748.39\" =~ \"\\\\d+(\\\\.\\\\d{2})?\"; +--------------------------------+ | (\"384748.39\"=~\"\\d+(\\.\\d{2})?\") | +--------------------------------+ | true | +--------------------------------+ nebula> MATCH (v:player) WHERE v.player.name =~ 'Tony.*' RETURN v.player.name; +---------------+ | v.player.name | +---------------+ | \"Tony Parker\" | +---------------+","title":"Regular expressions"},{"location":"3.ngql-guide/5.operators/8.list/","text":"List operators \u00b6 Nebula Graph supports the following list operators: List operator Description + Concatenates lists. IN Checks if an element exists in a list. [] Accesses an element(s) in a list using the index operator. Examples \u00b6 nebula> YIELD [1,2,3,4,5]+[6,7] AS myList; +-----------------------+ | myList | +-----------------------+ | [1, 2, 3, 4, 5, 6, 7] | +-----------------------+ nebula> RETURN size([NULL, 1, 2]); +------------------+ | size([NULL,1,2]) | +------------------+ | 3 | +------------------+ nebula> RETURN NULL IN [NULL, 1]; +--------------------+ | (NULL IN [NULL,1]) | +--------------------+ | __NULL__ | +--------------------+ nebula> WITH [2, 3, 4, 5] AS numberlist \\ UNWIND numberlist AS number \\ WITH number \\ WHERE number IN [2, 3, 8] \\ RETURN number; +--------+ | number | +--------+ | 2 | | 3 | +--------+ nebula> WITH ['Anne', 'John', 'Bill', 'Diane', 'Eve'] AS names RETURN names[1] AS result; +--------+ | result | +--------+ | \"John\" | +--------+","title":"List"},{"location":"3.ngql-guide/5.operators/8.list/#list_operators","text":"Nebula Graph supports the following list operators: List operator Description + Concatenates lists. IN Checks if an element exists in a list. [] Accesses an element(s) in a list using the index operator.","title":"List operators"},{"location":"3.ngql-guide/5.operators/8.list/#examples","text":"nebula> YIELD [1,2,3,4,5]+[6,7] AS myList; +-----------------------+ | myList | +-----------------------+ | [1, 2, 3, 4, 5, 6, 7] | +-----------------------+ nebula> RETURN size([NULL, 1, 2]); +------------------+ | size([NULL,1,2]) | +------------------+ | 3 | +------------------+ nebula> RETURN NULL IN [NULL, 1]; +--------------------+ | (NULL IN [NULL,1]) | +--------------------+ | __NULL__ | +--------------------+ nebula> WITH [2, 3, 4, 5] AS numberlist \\ UNWIND numberlist AS number \\ WITH number \\ WHERE number IN [2, 3, 8] \\ RETURN number; +--------+ | number | +--------+ | 2 | | 3 | +--------+ nebula> WITH ['Anne', 'John', 'Bill', 'Diane', 'Eve'] AS names RETURN names[1] AS result; +--------+ | result | +--------+ | \"John\" | +--------+","title":"Examples"},{"location":"3.ngql-guide/5.operators/9.precedence/","text":"Operator precedence \u00b6 The following list shows the precedence of nGQL operators in descending order. Operators that are shown together on a line have the same precedence. - (negative number) ! , NOT * , / , % - , + == , >= , > , <= , < , <> , != AND OR , XOR = (assignment) For operators that occur at the same precedence level within an expression, evaluation proceeds left to right, with the exception that assignments evaluate right to left. The precedence of operators determines the order of evaluation of terms in an expression. To modify this order and group terms explicitly, use parentheses. Examples \u00b6 nebula> RETURN 2+3*5; +-----------+ | (2+(3*5)) | +-----------+ | 17 | +-----------+ nebula> RETURN (2+3)*5; +-----------+ | ((2+3)*5) | +-----------+ | 25 | +-----------+ OpenCypher compatibility \u00b6 In openCypher, comparisons can be chained arbitrarily, e.g., x < y <= z is equivalent to x < y AND y <= z in openCypher. But in nGQL, x < y <= z is equivalent to (x < y) <= z . The result of (x < y) is a boolean. Compare it with an integer z , and you will get the final result NULL .","title":"Precedence"},{"location":"3.ngql-guide/5.operators/9.precedence/#operator_precedence","text":"The following list shows the precedence of nGQL operators in descending order. Operators that are shown together on a line have the same precedence. - (negative number) ! , NOT * , / , % - , + == , >= , > , <= , < , <> , != AND OR , XOR = (assignment) For operators that occur at the same precedence level within an expression, evaluation proceeds left to right, with the exception that assignments evaluate right to left. The precedence of operators determines the order of evaluation of terms in an expression. To modify this order and group terms explicitly, use parentheses.","title":"Operator precedence"},{"location":"3.ngql-guide/5.operators/9.precedence/#examples","text":"nebula> RETURN 2+3*5; +-----------+ | (2+(3*5)) | +-----------+ | 17 | +-----------+ nebula> RETURN (2+3)*5; +-----------+ | ((2+3)*5) | +-----------+ | 25 | +-----------+","title":"Examples"},{"location":"3.ngql-guide/5.operators/9.precedence/#opencypher_compatibility","text":"In openCypher, comparisons can be chained arbitrarily, e.g., x < y <= z is equivalent to x < y AND y <= z in openCypher. But in nGQL, x < y <= z is equivalent to (x < y) <= z . The result of (x < y) is a boolean. Compare it with an integer z , and you will get the final result NULL .","title":"OpenCypher compatibility"},{"location":"3.ngql-guide/6.functions-and-expressions/1.math/","text":"Built-in math functions \u00b6 Function descriptions \u00b6 Nebula Graph supports the following built-in math functions: Function Description double abs(double x) Returns the absolute value of the argument. double floor(double x) Returns the largest integer value smaller than or equal to the argument. (Rounds down) double ceil(double x) Returns the smallest integer greater than or equal to the argument. (Rounds up) double round(double x, int y) Returns the rounded value of x. y specifies the rounding index (position). If y is greater than 0, round at the yth position to the right of the decimal point. If y is less than 0, round at the yth position to the left of the decimal point. Pay attention to the floating-point precision when using this function. double sqrt(double x) Returns the square root of the argument. double cbrt(double x) Returns the cubic root of the argument. double hypot(double x, double y) Returns the hypotenuse of a right-angled triangle. double pow(double x, double y) Returns the result of \\(x^y\\) . double exp(double x) Returns the result of \\(e^x\\) . double exp2(double x) Returns the result of \\(2^x\\) . double log(double x) Returns the base-e logarithm of the argument. double log2(double x) Returns the base-2 logarithm of the argument. double log10(double x) Returns the base-10 logarithm of the argument. double sin(double x) Returns the sine of the argument. double asin(double x) Returns the inverse sine of the argument. double cos(double x) Returns the cosine of the argument. double acos(double x) Returns the inverse cosine of the argument. double tan(double x) Returns the tangent of the argument. double atan(double x) Returns the inverse tangent of the argument. double rand() Returns a random floating point number in the range from 0 (inclusive) to 1 (exclusive); i.e.[0,1). int rand32(int min, int max) Returns a random 32-bit integer in [min, max) . If you set only one argument, it is parsed as max and min is 0 by default. If you set no argument, the system returns a random signed 32-bit integer. int rand64(int min, int max) Returns a random 64-bit integer in [min, max) . If you set only one argument, it is parsed as max and min is 0 by default. If you set no argument, the system returns a random signed 64-bit integer. collect() Puts all the collected values into a list. avg() Returns the average value of the argument. count() Returns the number of records. max() Returns the maximum value. min() Returns the minimum value. std() Returns the population standard deviation. sum() Returns the sum value. bit_and() Bitwise AND. bit_or() Bitwise OR. bit_xor() Bitwise XOR. int size() Returns the number of elements in a list or a map. int range(int start, int end, int step) Returns a list of integers from [start,end] in the specified steps. step is 1 by default. int sign(double x) Returns the signum of the given number. If the number is 0, the system returns 0. If the number is negative, the system returns -1. If the number is positive, the system returns 1. double e() Returns the base of the natural logarithm, e (2.718281828459045). double pi() Returns the mathematical constant pi (3.141592653589793). double radians() Converts degrees to radians. radians(180) returns 3.141592653589793 . Note If the argument is NULL , the output is undefined. Example \u00b6 # The following statement supports aggregate functions. nebula> GO FROM \"player100\" OVER follow YIELD dst(edge) AS dst, properties($$).age AS age \\ | GROUP BY $-.dst \\ YIELD \\ $-.dst AS dst, \\ toInteger((sum($-.age)/count($-.age)))+avg(distinct $-.age+1)+1 AS statistics; +-------------+------------+ | dst | statistics | +-------------+------------+ | \"player125\" | 84.0 | | \"player101\" | 74.0 | +-------------+------------+ Got 2 rows (time spent 4739/5064 us)","title":"Math functions"},{"location":"3.ngql-guide/6.functions-and-expressions/1.math/#built-in_math_functions","text":"","title":"Built-in math functions"},{"location":"3.ngql-guide/6.functions-and-expressions/1.math/#function_descriptions","text":"Nebula Graph supports the following built-in math functions: Function Description double abs(double x) Returns the absolute value of the argument. double floor(double x) Returns the largest integer value smaller than or equal to the argument. (Rounds down) double ceil(double x) Returns the smallest integer greater than or equal to the argument. (Rounds up) double round(double x, int y) Returns the rounded value of x. y specifies the rounding index (position). If y is greater than 0, round at the yth position to the right of the decimal point. If y is less than 0, round at the yth position to the left of the decimal point. Pay attention to the floating-point precision when using this function. double sqrt(double x) Returns the square root of the argument. double cbrt(double x) Returns the cubic root of the argument. double hypot(double x, double y) Returns the hypotenuse of a right-angled triangle. double pow(double x, double y) Returns the result of \\(x^y\\) . double exp(double x) Returns the result of \\(e^x\\) . double exp2(double x) Returns the result of \\(2^x\\) . double log(double x) Returns the base-e logarithm of the argument. double log2(double x) Returns the base-2 logarithm of the argument. double log10(double x) Returns the base-10 logarithm of the argument. double sin(double x) Returns the sine of the argument. double asin(double x) Returns the inverse sine of the argument. double cos(double x) Returns the cosine of the argument. double acos(double x) Returns the inverse cosine of the argument. double tan(double x) Returns the tangent of the argument. double atan(double x) Returns the inverse tangent of the argument. double rand() Returns a random floating point number in the range from 0 (inclusive) to 1 (exclusive); i.e.[0,1). int rand32(int min, int max) Returns a random 32-bit integer in [min, max) . If you set only one argument, it is parsed as max and min is 0 by default. If you set no argument, the system returns a random signed 32-bit integer. int rand64(int min, int max) Returns a random 64-bit integer in [min, max) . If you set only one argument, it is parsed as max and min is 0 by default. If you set no argument, the system returns a random signed 64-bit integer. collect() Puts all the collected values into a list. avg() Returns the average value of the argument. count() Returns the number of records. max() Returns the maximum value. min() Returns the minimum value. std() Returns the population standard deviation. sum() Returns the sum value. bit_and() Bitwise AND. bit_or() Bitwise OR. bit_xor() Bitwise XOR. int size() Returns the number of elements in a list or a map. int range(int start, int end, int step) Returns a list of integers from [start,end] in the specified steps. step is 1 by default. int sign(double x) Returns the signum of the given number. If the number is 0, the system returns 0. If the number is negative, the system returns -1. If the number is positive, the system returns 1. double e() Returns the base of the natural logarithm, e (2.718281828459045). double pi() Returns the mathematical constant pi (3.141592653589793). double radians() Converts degrees to radians. radians(180) returns 3.141592653589793 . Note If the argument is NULL , the output is undefined.","title":"Function descriptions"},{"location":"3.ngql-guide/6.functions-and-expressions/1.math/#example","text":"# The following statement supports aggregate functions. nebula> GO FROM \"player100\" OVER follow YIELD dst(edge) AS dst, properties($$).age AS age \\ | GROUP BY $-.dst \\ YIELD \\ $-.dst AS dst, \\ toInteger((sum($-.age)/count($-.age)))+avg(distinct $-.age+1)+1 AS statistics; +-------------+------------+ | dst | statistics | +-------------+------------+ | \"player125\" | 84.0 | | \"player101\" | 74.0 | +-------------+------------+ Got 2 rows (time spent 4739/5064 us)","title":"Example"},{"location":"3.ngql-guide/6.functions-and-expressions/10.collect/","text":"collect() \u00b6 The collect() function returns a list containing the values returned by an expression. Using this function aggregates data by merging multiple records or values into a single list. The aggregate function collect() works like GROUP BY in SQL. Examples \u00b6 nebula> UNWIND [1, 2, 1] AS a \\ RETURN a; +---+ | a | +---+ | 1 | | 2 | | 1 | +---+ nebula> UNWIND [1, 2, 1] AS a \\ RETURN collect(a); +------------+ | collect(a) | +------------+ | [1, 2, 1] | +------------+ nebula> UNWIND [1, 2, 1] AS a \\ RETURN a, collect(a), size(collect(a)); +---+------------+------------------+ | a | collect(a) | size(COLLECT(a)) | +---+------------+------------------+ | 2 | [2] | 1 | | 1 | [1, 1] | 2 | +---+------------+------------------+ # The following examples sort the results in descending order, limit output rows to 3, and collect the output into a list.\u0153 nebula> UNWIND [\"c\", \"b\", \"a\", \"d\" ] AS p \\ WITH p AS q \\ ORDER BY q DESC LIMIT 3 \\ RETURN collect(q); +-----------------+ | collect(q) | +-----------------+ | [\"d\", \"c\", \"b\"] | +-----------------+ nebula> WITH [1, 1, 2, 2] AS coll \\ UNWIND coll AS x \\ WITH DISTINCT x \\ RETURN collect(x) AS ss; +--------+ | ss | +--------+ | [1, 2] | +--------+ nebula> MATCH (n:player) \\ RETURN collect(n.player.age); +---------------------------------------------------------------+ | collect(n.player.age) | +---------------------------------------------------------------+ | [32, 32, 34, 29, 41, 40, 33, 25, 40, 37, ... ... # The following example aggregates all the players' names by their ages. nebula> MATCH (n:player) \\ RETURN n.player.age AS age, collect(n.player.name); +-----+--------------------------------------------------------------------------+ | age | collect(n.player.name) | +-----+--------------------------------------------------------------------------+ | 24 | [\"Giannis Antetokounmpo\"] | | 20 | [\"Luka Doncic\"] | | 25 | [\"Joel Embiid\", \"Kyle Anderson\"] | +-----+--------------------------------------------------------------------------+ ...","title":"collect()"},{"location":"3.ngql-guide/6.functions-and-expressions/10.collect/#collect","text":"The collect() function returns a list containing the values returned by an expression. Using this function aggregates data by merging multiple records or values into a single list. The aggregate function collect() works like GROUP BY in SQL.","title":"collect()"},{"location":"3.ngql-guide/6.functions-and-expressions/10.collect/#examples","text":"nebula> UNWIND [1, 2, 1] AS a \\ RETURN a; +---+ | a | +---+ | 1 | | 2 | | 1 | +---+ nebula> UNWIND [1, 2, 1] AS a \\ RETURN collect(a); +------------+ | collect(a) | +------------+ | [1, 2, 1] | +------------+ nebula> UNWIND [1, 2, 1] AS a \\ RETURN a, collect(a), size(collect(a)); +---+------------+------------------+ | a | collect(a) | size(COLLECT(a)) | +---+------------+------------------+ | 2 | [2] | 1 | | 1 | [1, 1] | 2 | +---+------------+------------------+ # The following examples sort the results in descending order, limit output rows to 3, and collect the output into a list.\u0153 nebula> UNWIND [\"c\", \"b\", \"a\", \"d\" ] AS p \\ WITH p AS q \\ ORDER BY q DESC LIMIT 3 \\ RETURN collect(q); +-----------------+ | collect(q) | +-----------------+ | [\"d\", \"c\", \"b\"] | +-----------------+ nebula> WITH [1, 1, 2, 2] AS coll \\ UNWIND coll AS x \\ WITH DISTINCT x \\ RETURN collect(x) AS ss; +--------+ | ss | +--------+ | [1, 2] | +--------+ nebula> MATCH (n:player) \\ RETURN collect(n.player.age); +---------------------------------------------------------------+ | collect(n.player.age) | +---------------------------------------------------------------+ | [32, 32, 34, 29, 41, 40, 33, 25, 40, 37, ... ... # The following example aggregates all the players' names by their ages. nebula> MATCH (n:player) \\ RETURN n.player.age AS age, collect(n.player.name); +-----+--------------------------------------------------------------------------+ | age | collect(n.player.name) | +-----+--------------------------------------------------------------------------+ | 24 | [\"Giannis Antetokounmpo\"] | | 20 | [\"Luka Doncic\"] | | 25 | [\"Joel Embiid\", \"Kyle Anderson\"] | +-----+--------------------------------------------------------------------------+ ...","title":"Examples"},{"location":"3.ngql-guide/6.functions-and-expressions/11.reduce/","text":"reduce() function \u00b6 This topic will describe the reduce function. OpenCypher Compatibility \u00b6 In openCypher, the reduce() function is not defined. nGQL will implement the reduce() function in the Cypher way. Syntax \u00b6 The reduce() function applies an expression to each element in a list one by one, chains the result to the next iteration by taking it as the initial value, and returns the final result. This function iterates each element e in the given list, runs the expression on e , accumulates the result with the initial value, and store the new result in the accumulator as the initial value of the next iteration. It works like the fold or reduce method in functional languages such as Lisp and Scala. reduce(<accumulator> = <initial>, <variable> IN <list> | <expression>) Parameter Description accumulator A variable that will hold the accumulated results as the list is iterated. initial An expression that runs once to give an initial value to the accumulator . variable A variable in the list that will be applied to the expression successively. list A list or a list of expressions. expression This expression will be run on each element in the list once and store the result value in the accumulator . Note The type of the value returned depends on the parameters provided, along with the semantics of the expression. Examples \u00b6 nebula> RETURN reduce(totalNum = 10, n IN range(1, 3) | totalNum + n) AS r; +----+ | r | +----+ | 16 | +----+ nebula> RETURN reduce(totalNum = -4 * 5, n IN [1, 2] | totalNum + n * 2) AS r; +-----+ | r | +-----+ | -14 | +-----+ nebula> MATCH p = (n:player{name:\"LeBron James\"})<-[:follow]-(m) \\ RETURN nodes(p)[0].age AS src1, nodes(p)[1].age AS dst2, \\ reduce(totalAge = 100, n IN nodes(p) | totalAge + n.age) AS sum; +------+------+-----+ | src1 | dst2 | sum | +------+------+-----+ | 34 | 31 | 165 | | 34 | 29 | 163 | | 34 | 33 | 167 | | 34 | 26 | 160 | | 34 | 34 | 168 | | 34 | 37 | 171 | +------+------+-----+ nebula> LOOKUP ON player WHERE player.name == \"Tony Parker\" YIELD id(vertex) AS VertexID \\ | GO FROM $-.VertexID over follow \\ WHERE properties(edge).degree != reduce(totalNum = 5, n IN range(1, 3) | properties($$).age + totalNum + n) \\ YIELD properties($$).name AS id, properties($$).age AS age, properties(edge).degree AS degree; +---------------------+-----+--------+ | id | age | degree | +---------------------+-----+--------+ | \"Tim Duncan\" | 42 | 95 | | \"LaMarcus Aldridge\" | 33 | 90 | | \"Manu Ginobili\" | 41 | 95 | +---------------------+-----+--------+","title":"reduce()"},{"location":"3.ngql-guide/6.functions-and-expressions/11.reduce/#reduce_function","text":"This topic will describe the reduce function.","title":"reduce() function"},{"location":"3.ngql-guide/6.functions-and-expressions/11.reduce/#opencypher_compatibility","text":"In openCypher, the reduce() function is not defined. nGQL will implement the reduce() function in the Cypher way.","title":"OpenCypher Compatibility"},{"location":"3.ngql-guide/6.functions-and-expressions/11.reduce/#syntax","text":"The reduce() function applies an expression to each element in a list one by one, chains the result to the next iteration by taking it as the initial value, and returns the final result. This function iterates each element e in the given list, runs the expression on e , accumulates the result with the initial value, and store the new result in the accumulator as the initial value of the next iteration. It works like the fold or reduce method in functional languages such as Lisp and Scala. reduce(<accumulator> = <initial>, <variable> IN <list> | <expression>) Parameter Description accumulator A variable that will hold the accumulated results as the list is iterated. initial An expression that runs once to give an initial value to the accumulator . variable A variable in the list that will be applied to the expression successively. list A list or a list of expressions. expression This expression will be run on each element in the list once and store the result value in the accumulator . Note The type of the value returned depends on the parameters provided, along with the semantics of the expression.","title":"Syntax"},{"location":"3.ngql-guide/6.functions-and-expressions/11.reduce/#examples","text":"nebula> RETURN reduce(totalNum = 10, n IN range(1, 3) | totalNum + n) AS r; +----+ | r | +----+ | 16 | +----+ nebula> RETURN reduce(totalNum = -4 * 5, n IN [1, 2] | totalNum + n * 2) AS r; +-----+ | r | +-----+ | -14 | +-----+ nebula> MATCH p = (n:player{name:\"LeBron James\"})<-[:follow]-(m) \\ RETURN nodes(p)[0].age AS src1, nodes(p)[1].age AS dst2, \\ reduce(totalAge = 100, n IN nodes(p) | totalAge + n.age) AS sum; +------+------+-----+ | src1 | dst2 | sum | +------+------+-----+ | 34 | 31 | 165 | | 34 | 29 | 163 | | 34 | 33 | 167 | | 34 | 26 | 160 | | 34 | 34 | 168 | | 34 | 37 | 171 | +------+------+-----+ nebula> LOOKUP ON player WHERE player.name == \"Tony Parker\" YIELD id(vertex) AS VertexID \\ | GO FROM $-.VertexID over follow \\ WHERE properties(edge).degree != reduce(totalNum = 5, n IN range(1, 3) | properties($$).age + totalNum + n) \\ YIELD properties($$).name AS id, properties($$).age AS age, properties(edge).degree AS degree; +---------------------+-----+--------+ | id | age | degree | +---------------------+-----+--------+ | \"Tim Duncan\" | 42 | 95 | | \"LaMarcus Aldridge\" | 33 | 90 | | \"Manu Ginobili\" | 41 | 95 | +---------------------+-----+--------+","title":"Examples"},{"location":"3.ngql-guide/6.functions-and-expressions/12.hash/","text":"hash function \u00b6 The hash() function returns the hash value of the argument. The argument can be a number, a string, a list, a boolean, null, or an expression that evaluates to a value of the preceding data types. The source code of the hash() function (MurmurHash2), seed ( 0xc70f6907UL ), and other parameters can be found in MurmurHash2.h . For Java, the hash function operates as follows. MurmurHash2 . hash64 ( \"to_be_hashed\" . getBytes (), \"to_be_hashed\" . getBytes (). length , 0xc70f6907 ) Legacy version compatibility \u00b6 In nGQL 1.0, when nGQL does not support string VIDs, a common practice is to hash the strings first and then use the values as VIDs. But in nGQL 2.0, both string VIDs and integer VIDs are supported, so there is no need to use hash() to set VIDs. Hash a number \u00b6 nebula> YIELD hash(-123); +--------------+ | hash(-(123)) | +--------------+ | -123 | +--------------+ Hash a string \u00b6 nebula> YIELD hash(\"to_be_hashed\"); +----------------------+ | hash(to_be_hashed) | +----------------------+ | -1098333533029391540 | +----------------------+ Hash a list \u00b6 nebula> YIELD hash([1,2,3]); +----------------+ | hash([1,2,3]) | +----------------+ | 11093822460243 | +----------------+ Hash a boolean \u00b6 nebula> YIELD hash(true); +------------+ | hash(true) | +------------+ | 1 | +------------+ nebula> YIELD hash(false); +-------------+ | hash(false) | +-------------+ | 0 | +-------------+ Hash NULL \u00b6 nebula> YIELD hash(NULL); +------------+ | hash(NULL) | +------------+ | -1 | +------------+ Hash an expression \u00b6 nebula> YIELD hash(toLower(\"HELLO NEBULA\")); +-------------------------------+ | hash(toLower(\"HELLO NEBULA\")) | +-------------------------------+ | -8481157362655072082 | +-------------------------------+","title":"hash()"},{"location":"3.ngql-guide/6.functions-and-expressions/12.hash/#hash_function","text":"The hash() function returns the hash value of the argument. The argument can be a number, a string, a list, a boolean, null, or an expression that evaluates to a value of the preceding data types. The source code of the hash() function (MurmurHash2), seed ( 0xc70f6907UL ), and other parameters can be found in MurmurHash2.h . For Java, the hash function operates as follows. MurmurHash2 . hash64 ( \"to_be_hashed\" . getBytes (), \"to_be_hashed\" . getBytes (). length , 0xc70f6907 )","title":"hash function"},{"location":"3.ngql-guide/6.functions-and-expressions/12.hash/#legacy_version_compatibility","text":"In nGQL 1.0, when nGQL does not support string VIDs, a common practice is to hash the strings first and then use the values as VIDs. But in nGQL 2.0, both string VIDs and integer VIDs are supported, so there is no need to use hash() to set VIDs.","title":"Legacy version compatibility"},{"location":"3.ngql-guide/6.functions-and-expressions/12.hash/#hash_a_number","text":"nebula> YIELD hash(-123); +--------------+ | hash(-(123)) | +--------------+ | -123 | +--------------+","title":"Hash a number"},{"location":"3.ngql-guide/6.functions-and-expressions/12.hash/#hash_a_string","text":"nebula> YIELD hash(\"to_be_hashed\"); +----------------------+ | hash(to_be_hashed) | +----------------------+ | -1098333533029391540 | +----------------------+","title":"Hash a string"},{"location":"3.ngql-guide/6.functions-and-expressions/12.hash/#hash_a_list","text":"nebula> YIELD hash([1,2,3]); +----------------+ | hash([1,2,3]) | +----------------+ | 11093822460243 | +----------------+","title":"Hash a list"},{"location":"3.ngql-guide/6.functions-and-expressions/12.hash/#hash_a_boolean","text":"nebula> YIELD hash(true); +------------+ | hash(true) | +------------+ | 1 | +------------+ nebula> YIELD hash(false); +-------------+ | hash(false) | +-------------+ | 0 | +-------------+","title":"Hash a boolean"},{"location":"3.ngql-guide/6.functions-and-expressions/12.hash/#hash_null","text":"nebula> YIELD hash(NULL); +------------+ | hash(NULL) | +------------+ | -1 | +------------+","title":"Hash NULL"},{"location":"3.ngql-guide/6.functions-and-expressions/12.hash/#hash_an_expression","text":"nebula> YIELD hash(toLower(\"HELLO NEBULA\")); +-------------------------------+ | hash(toLower(\"HELLO NEBULA\")) | +-------------------------------+ | -8481157362655072082 | +-------------------------------+","title":"Hash an expression"},{"location":"3.ngql-guide/6.functions-and-expressions/13.concat/","text":"concat function \u00b6 The concat() and concat_ws() functions return strings concatenated by one or more strings. concat() function \u00b6 The concat() function requires at least two or more strings. All the parameters are concatenated into one string. If there is only one string, the string itself is returned. If any one of the strings is NULL , NULL is returned. Syntax \u00b6 concat ( string1,string2,... ) Examples \u00b6 //This example concatenates 1 , 2 , and 3 . nebula> RETURN concat ( \"1\" , \"2\" , \"3\" ) AS r ; +-------+ | r | +-------+ | \"123\" | +-------+ //In this example, one of the string is NULL. nebula> RETURN concat ( \"1\" , \"2\" ,NULL ) AS r ; +----------+ | r | +----------+ | __NULL__ | +----------+ nebula> GO FROM \"player100\" over follow \\ YIELD concat ( src ( edge ) , properties ( $^ ) .age, properties ( $$ ) .name, properties ( edge ) .degree ) AS A ; +------------------------------+ | A | +------------------------------+ | \"player10042Tony Parker95\" | | \"player10042Manu Ginobili95\" | +------------------------------+ concat_ws() function \u00b6 The concat_ws() function connects two or more strings with a predefined separator. If the separator is NULL , the concat_ws() function returns NULL . If the separator is not NULL and there is only one string, the string itself is returned. If the separator is not NULL and there is a NULL in the strings, NULL is ignored during the concatenation. Syntax \u00b6 concat_ws ( separator,string1,string2,... ) Examples \u00b6 //This example concatenates a, b, and c with the separator +. nebula> RETURN concat_ws ( \"+\" , \"a\" , \"b\" , \"c\" ) AS r ; +---------+ | r | +---------+ | \"a+b+c\" | +---------+ //In this example, the separator is NULL. neubla> RETURN concat_ws ( NULL, \"a\" , \"b\" , \"c\" ) AS r ; +----------+ | r | +----------+ | __NULL__ | +----------+ //In this example, the separator is + and there is a NULL in the strings. nebula> RETURN concat_ws ( \"+\" , \"a\" ,NULL, \"b\" , \"c\" ) AS r ; +---------+ | r | +---------+ | \"a+b+c\" | +---------+ //In this example, the separator is + and there is only one string. nebula> RETURN concat_ws ( \"+\" , \"a\" ) AS r ; +-----+ | r | +-----+ | \"a\" | +-----+ nebula> GO FROM \"player100\" over follow \\ YIELD concat_ws ( \" \" ,src ( edge ) , properties ( $^ ) .age, properties ( $$ ) .name, properties ( edge ) .degree ) AS A ; +---------------------------------+ | A | +---------------------------------+ | \"player100 42 Tony Parker 95\" | | \"player100 42 Manu Ginobili 95\" | +---------------------------------+","title":"concat()"},{"location":"3.ngql-guide/6.functions-and-expressions/13.concat/#concat_function","text":"The concat() and concat_ws() functions return strings concatenated by one or more strings.","title":"concat function"},{"location":"3.ngql-guide/6.functions-and-expressions/13.concat/#concat_function_1","text":"The concat() function requires at least two or more strings. All the parameters are concatenated into one string. If there is only one string, the string itself is returned. If any one of the strings is NULL , NULL is returned.","title":"concat() function"},{"location":"3.ngql-guide/6.functions-and-expressions/13.concat/#syntax","text":"concat ( string1,string2,... )","title":"Syntax"},{"location":"3.ngql-guide/6.functions-and-expressions/13.concat/#examples","text":"//This example concatenates 1 , 2 , and 3 . nebula> RETURN concat ( \"1\" , \"2\" , \"3\" ) AS r ; +-------+ | r | +-------+ | \"123\" | +-------+ //In this example, one of the string is NULL. nebula> RETURN concat ( \"1\" , \"2\" ,NULL ) AS r ; +----------+ | r | +----------+ | __NULL__ | +----------+ nebula> GO FROM \"player100\" over follow \\ YIELD concat ( src ( edge ) , properties ( $^ ) .age, properties ( $$ ) .name, properties ( edge ) .degree ) AS A ; +------------------------------+ | A | +------------------------------+ | \"player10042Tony Parker95\" | | \"player10042Manu Ginobili95\" | +------------------------------+","title":"Examples"},{"location":"3.ngql-guide/6.functions-and-expressions/13.concat/#concat_ws_function","text":"The concat_ws() function connects two or more strings with a predefined separator. If the separator is NULL , the concat_ws() function returns NULL . If the separator is not NULL and there is only one string, the string itself is returned. If the separator is not NULL and there is a NULL in the strings, NULL is ignored during the concatenation.","title":"concat_ws() function"},{"location":"3.ngql-guide/6.functions-and-expressions/13.concat/#syntax_1","text":"concat_ws ( separator,string1,string2,... )","title":"Syntax"},{"location":"3.ngql-guide/6.functions-and-expressions/13.concat/#examples_1","text":"//This example concatenates a, b, and c with the separator +. nebula> RETURN concat_ws ( \"+\" , \"a\" , \"b\" , \"c\" ) AS r ; +---------+ | r | +---------+ | \"a+b+c\" | +---------+ //In this example, the separator is NULL. neubla> RETURN concat_ws ( NULL, \"a\" , \"b\" , \"c\" ) AS r ; +----------+ | r | +----------+ | __NULL__ | +----------+ //In this example, the separator is + and there is a NULL in the strings. nebula> RETURN concat_ws ( \"+\" , \"a\" ,NULL, \"b\" , \"c\" ) AS r ; +---------+ | r | +---------+ | \"a+b+c\" | +---------+ //In this example, the separator is + and there is only one string. nebula> RETURN concat_ws ( \"+\" , \"a\" ) AS r ; +-----+ | r | +-----+ | \"a\" | +-----+ nebula> GO FROM \"player100\" over follow \\ YIELD concat_ws ( \" \" ,src ( edge ) , properties ( $^ ) .age, properties ( $$ ) .name, properties ( edge ) .degree ) AS A ; +---------------------------------+ | A | +---------------------------------+ | \"player100 42 Tony Parker 95\" | | \"player100 42 Manu Ginobili 95\" | +---------------------------------+","title":"Examples"},{"location":"3.ngql-guide/6.functions-and-expressions/14.geo/","text":"Geography functions \u00b6 Geography functions are used to generate or perform operations on the value of the geography data type. For descriptions of the geography data types, see Geography . Descriptions \u00b6 Function Return Type Description ST_Point(longitude, latitude) GEOGRAPHY Creates the geography that contains a point. ST_GeogFromText(wkt_string) GEOGRAPHY Returns the geography corresponding to the input WKT string. ST_ASText(geography) STRING Returns the WKT string of the input geography. ST_Centroid(geography) GEOGRAPHY Returns the centroid of the input geography in the form of the single point geography. ST_ISValid(geography) BOOL Returns whether the input geography is valid. ST_Intersects(geography_1, geography_2) BOOL Returns whether geography_1 and geography_2 have intersections. ST_Covers(geography_1, geography_2) BOOL Returns whether geography_1 completely contains geography_2. If there is no point outside geography_1 in geography_2, return True. ST_CoveredBy(geography_1, geography_2) BOOL Returns whether geography_2 completely contains geography_1.If there is no point outside geography_2 in geography_1, return True. ST_DWithin(geography_1, geography_2, distance) BOOL If the distance between one point (at least) in geography_1 and one point in geography_2 is less than or equal to the distance specified by the distance parameter (measured by meters), return True. ST_Distance(geography_1, geography_2) FLOAT Returns the smallest possible distance (measured by meters) between two non-empty geographies. S2_CellIdFromPoint(point_geography) INT Returns the S2 Cell ID that covers the point geography. S2_CoveringCellIds(geography) ARRAY<INT64> Returns an array of S2 Cell IDs that cover the input geography. Examples \u00b6 nebula> RETURN ST_ASText(ST_Point(1,1)); +--------------------------+ | ST_ASText(ST_Point(1,1)) | +--------------------------+ | \"POINT(1 1)\" | +--------------------------+ nebula> RETURN ST_ASText(ST_GeogFromText(\"POINT(3 8)\")); +------------------------------------------+ | ST_ASText(ST_GeogFromText(\"POINT(3 8)\")) | +------------------------------------------+ | \"POINT(3 8)\" | +------------------------------------------+ nebula> RETURN ST_ASTEXT(ST_Centroid(ST_GeogFromText(\"LineString(0 1,1 0)\"))); +----------------------------------------------------------------+ | ST_ASTEXT(ST_Centroid(ST_GeogFromText(\"LineString(0 1,1 0)\"))) | +----------------------------------------------------------------+ | \"POINT(0.5000380800773782 0.5000190382261059)\" | +----------------------------------------------------------------+ nebula> RETURN ST_ISValid(ST_GeogFromText(\"POINT(3 8)\")); +-------------------------------------------+ | ST_ISValid(ST_GeogFromText(\"POINT(3 8)\")) | +-------------------------------------------+ | true | +-------------------------------------------+ nebula> RETURN ST_Intersects(ST_GeogFromText(\"LineString(0 1,1 0)\"),ST_GeogFromText(\"LineString(0 0,1 1)\")); +----------------------------------------------------------------------------------------------+ | ST_Intersects(ST_GeogFromText(\"LineString(0 1,1 0)\"),ST_GeogFromText(\"LineString(0 0,1 1)\")) | +----------------------------------------------------------------------------------------------+ | true | +----------------------------------------------------------------------------------------------+ nebula> RETURN ST_Covers(ST_GeogFromText(\"POLYGON((0 0,10 0,10 10,0 10,0 0))\"),ST_Point(1,2)); +--------------------------------------------------------------------------------+ | ST_Covers(ST_GeogFromText(\"POLYGON((0 0,10 0,10 10,0 10,0 0))\"),ST_Point(1,2)) | +--------------------------------------------------------------------------------+ | true | +--------------------------------------------------------------------------------+ nebula> RETURN ST_CoveredBy(ST_Point(1,2),ST_GeogFromText(\"POLYGON((0 0,10 0,10 10,0 10,0 0))\")); +-----------------------------------------------------------------------------------+ | ST_CoveredBy(ST_Point(1,2),ST_GeogFromText(\"POLYGON((0 0,10 0,10 10,0 10,0 0))\")) | +-----------------------------------------------------------------------------------+ | true | +-----------------------------------------------------------------------------------+ nebula> RETURN ST_dwithin(ST_GeogFromText(\"Point(0 0)\"),ST_GeogFromText(\"Point(10 10)\"),20000000000.0); +---------------------------------------------------------------------------------------+ | ST_dwithin(ST_GeogFromText(\"Point(0 0)\"),ST_GeogFromText(\"Point(10 10)\"),20000000000) | +---------------------------------------------------------------------------------------+ | true | +---------------------------------------------------------------------------------------+ nebula> RETURN ST_Distance(ST_GeogFromText(\"Point(0 0)\"),ST_GeogFromText(\"Point(10 10)\")); +----------------------------------------------------------------------------+ | ST_Distance(ST_GeogFromText(\"Point(0 0)\"),ST_GeogFromText(\"Point(10 10)\")) | +----------------------------------------------------------------------------+ | 1568523.0187677438 | +----------------------------------------------------------------------------+ nebula> RETURN S2_CellIdFromPoint(ST_GeogFromText(\"Point(1 1)\")); +---------------------------------------------------+ | S2_CellIdFromPoint(ST_GeogFromText(\"Point(1 1)\")) | +---------------------------------------------------+ | 1153277837650709461 | +---------------------------------------------------+ nebula> RETURN S2_CoveringCellIds(ST_GeogFromText(\"POLYGON((0 1, 1 2, 2 3, 0 1))\")); +--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | S2_CoveringCellIds(ST_GeogFromText(\"POLYGON((0 1, 1 2, 2 3, 0 1))\")) | +--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | [1152391494368201343, 1153466862374223872, 1153554823304445952, 1153836298281156608, 1153959443583467520, 1154240918560178176, 1160503736791990272, 1160591697722212352] | +--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+","title":"Geography functions"},{"location":"3.ngql-guide/6.functions-and-expressions/14.geo/#geography_functions","text":"Geography functions are used to generate or perform operations on the value of the geography data type. For descriptions of the geography data types, see Geography .","title":"Geography functions"},{"location":"3.ngql-guide/6.functions-and-expressions/14.geo/#descriptions","text":"Function Return Type Description ST_Point(longitude, latitude) GEOGRAPHY Creates the geography that contains a point. ST_GeogFromText(wkt_string) GEOGRAPHY Returns the geography corresponding to the input WKT string. ST_ASText(geography) STRING Returns the WKT string of the input geography. ST_Centroid(geography) GEOGRAPHY Returns the centroid of the input geography in the form of the single point geography. ST_ISValid(geography) BOOL Returns whether the input geography is valid. ST_Intersects(geography_1, geography_2) BOOL Returns whether geography_1 and geography_2 have intersections. ST_Covers(geography_1, geography_2) BOOL Returns whether geography_1 completely contains geography_2. If there is no point outside geography_1 in geography_2, return True. ST_CoveredBy(geography_1, geography_2) BOOL Returns whether geography_2 completely contains geography_1.If there is no point outside geography_2 in geography_1, return True. ST_DWithin(geography_1, geography_2, distance) BOOL If the distance between one point (at least) in geography_1 and one point in geography_2 is less than or equal to the distance specified by the distance parameter (measured by meters), return True. ST_Distance(geography_1, geography_2) FLOAT Returns the smallest possible distance (measured by meters) between two non-empty geographies. S2_CellIdFromPoint(point_geography) INT Returns the S2 Cell ID that covers the point geography. S2_CoveringCellIds(geography) ARRAY<INT64> Returns an array of S2 Cell IDs that cover the input geography.","title":"Descriptions"},{"location":"3.ngql-guide/6.functions-and-expressions/14.geo/#examples","text":"nebula> RETURN ST_ASText(ST_Point(1,1)); +--------------------------+ | ST_ASText(ST_Point(1,1)) | +--------------------------+ | \"POINT(1 1)\" | +--------------------------+ nebula> RETURN ST_ASText(ST_GeogFromText(\"POINT(3 8)\")); +------------------------------------------+ | ST_ASText(ST_GeogFromText(\"POINT(3 8)\")) | +------------------------------------------+ | \"POINT(3 8)\" | +------------------------------------------+ nebula> RETURN ST_ASTEXT(ST_Centroid(ST_GeogFromText(\"LineString(0 1,1 0)\"))); +----------------------------------------------------------------+ | ST_ASTEXT(ST_Centroid(ST_GeogFromText(\"LineString(0 1,1 0)\"))) | +----------------------------------------------------------------+ | \"POINT(0.5000380800773782 0.5000190382261059)\" | +----------------------------------------------------------------+ nebula> RETURN ST_ISValid(ST_GeogFromText(\"POINT(3 8)\")); +-------------------------------------------+ | ST_ISValid(ST_GeogFromText(\"POINT(3 8)\")) | +-------------------------------------------+ | true | +-------------------------------------------+ nebula> RETURN ST_Intersects(ST_GeogFromText(\"LineString(0 1,1 0)\"),ST_GeogFromText(\"LineString(0 0,1 1)\")); +----------------------------------------------------------------------------------------------+ | ST_Intersects(ST_GeogFromText(\"LineString(0 1,1 0)\"),ST_GeogFromText(\"LineString(0 0,1 1)\")) | +----------------------------------------------------------------------------------------------+ | true | +----------------------------------------------------------------------------------------------+ nebula> RETURN ST_Covers(ST_GeogFromText(\"POLYGON((0 0,10 0,10 10,0 10,0 0))\"),ST_Point(1,2)); +--------------------------------------------------------------------------------+ | ST_Covers(ST_GeogFromText(\"POLYGON((0 0,10 0,10 10,0 10,0 0))\"),ST_Point(1,2)) | +--------------------------------------------------------------------------------+ | true | +--------------------------------------------------------------------------------+ nebula> RETURN ST_CoveredBy(ST_Point(1,2),ST_GeogFromText(\"POLYGON((0 0,10 0,10 10,0 10,0 0))\")); +-----------------------------------------------------------------------------------+ | ST_CoveredBy(ST_Point(1,2),ST_GeogFromText(\"POLYGON((0 0,10 0,10 10,0 10,0 0))\")) | +-----------------------------------------------------------------------------------+ | true | +-----------------------------------------------------------------------------------+ nebula> RETURN ST_dwithin(ST_GeogFromText(\"Point(0 0)\"),ST_GeogFromText(\"Point(10 10)\"),20000000000.0); +---------------------------------------------------------------------------------------+ | ST_dwithin(ST_GeogFromText(\"Point(0 0)\"),ST_GeogFromText(\"Point(10 10)\"),20000000000) | +---------------------------------------------------------------------------------------+ | true | +---------------------------------------------------------------------------------------+ nebula> RETURN ST_Distance(ST_GeogFromText(\"Point(0 0)\"),ST_GeogFromText(\"Point(10 10)\")); +----------------------------------------------------------------------------+ | ST_Distance(ST_GeogFromText(\"Point(0 0)\"),ST_GeogFromText(\"Point(10 10)\")) | +----------------------------------------------------------------------------+ | 1568523.0187677438 | +----------------------------------------------------------------------------+ nebula> RETURN S2_CellIdFromPoint(ST_GeogFromText(\"Point(1 1)\")); +---------------------------------------------------+ | S2_CellIdFromPoint(ST_GeogFromText(\"Point(1 1)\")) | +---------------------------------------------------+ | 1153277837650709461 | +---------------------------------------------------+ nebula> RETURN S2_CoveringCellIds(ST_GeogFromText(\"POLYGON((0 1, 1 2, 2 3, 0 1))\")); +--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | S2_CoveringCellIds(ST_GeogFromText(\"POLYGON((0 1, 1 2, 2 3, 0 1))\")) | +--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | [1152391494368201343, 1153466862374223872, 1153554823304445952, 1153836298281156608, 1153959443583467520, 1154240918560178176, 1160503736791990272, 1160591697722212352] | +--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+","title":"Examples"},{"location":"3.ngql-guide/6.functions-and-expressions/2.string/","text":"Built-in string functions \u00b6 Nebula Graph supports the following built-in string functions: Note Like SQL, the position index of nGQL starts from 1 , while in C language it starts from 0 . Function Description int strcasecmp(string a, string b) Compares string a and b without case sensitivity. When a = b, the return value is 0. When a > b, the return value is greater than 0. When a < b, the return value is less than 0. string lower(string a) Returns the argument in lowercase. string toLower(string a) The same as lower() . string upper(string a) Returns the argument in uppercase. string toUpper(string a) The same as upper() . int length(string a) Returns the length of the given string in bytes. string trim(string a) Removes leading and trailing spaces. string ltrim(string a) Removes leading spaces. string rtrim(string a) Removes trailing spaces. string left(string a, int count) Returns a substring consisting of count characters from the left side of string a. If string a is shorter than count , the system returns string a. string right(string a, int count) Returns a substring consisting of count characters from the right side of string a. If string a is shorter than count , the system returns string a. string lpad(string a, int size, string letters) Left-pads string a with string letters and returns a substring with the length of size . string rpad(string a, int size, string letters) Right-pads string a with string letters and returns a substring with the length of size . string substr(string a, int pos, int count) Returns a substring extracting count characters starting from the specified position pos of string a. string substring(string a, int pos, int count) The same as substr() . string reverse(string) Returns a string in reverse order. string replace(string a, string b, string c) Replaces string b in string a with string c. list split(string a, string b) Splits string a at string b and returns a list of strings. string toString() Takes in any data type and converts it into a string. int hash() Takes in any data type and encodes it into a hash value. Note If the argument is NULL , the return is undefined. Explanations for the return of substr() and substring() \u00b6 The position index starts from 0 . If pos is 0, the whole string is returned. If pos is greater than the maximum string index, an empty string is returned. If pos is a negative number, BAD_DATA is returned. If count is omitted, the function returns the substring starting at the position given by pos and extending to the end of the string. If count is 0, an empty string is returned. Using NULL as any of the argument of substr() will cause an issue . OpenCypher compatibility In openCypher, if a is null , null is returned. In openCypher, if pos is 0, the returned substring starts from the first character, and extend to count characters. In openCypher, if either pos or count is null or a negative integer, an issue is raised.","title":"String functions"},{"location":"3.ngql-guide/6.functions-and-expressions/2.string/#built-in_string_functions","text":"Nebula Graph supports the following built-in string functions: Note Like SQL, the position index of nGQL starts from 1 , while in C language it starts from 0 . Function Description int strcasecmp(string a, string b) Compares string a and b without case sensitivity. When a = b, the return value is 0. When a > b, the return value is greater than 0. When a < b, the return value is less than 0. string lower(string a) Returns the argument in lowercase. string toLower(string a) The same as lower() . string upper(string a) Returns the argument in uppercase. string toUpper(string a) The same as upper() . int length(string a) Returns the length of the given string in bytes. string trim(string a) Removes leading and trailing spaces. string ltrim(string a) Removes leading spaces. string rtrim(string a) Removes trailing spaces. string left(string a, int count) Returns a substring consisting of count characters from the left side of string a. If string a is shorter than count , the system returns string a. string right(string a, int count) Returns a substring consisting of count characters from the right side of string a. If string a is shorter than count , the system returns string a. string lpad(string a, int size, string letters) Left-pads string a with string letters and returns a substring with the length of size . string rpad(string a, int size, string letters) Right-pads string a with string letters and returns a substring with the length of size . string substr(string a, int pos, int count) Returns a substring extracting count characters starting from the specified position pos of string a. string substring(string a, int pos, int count) The same as substr() . string reverse(string) Returns a string in reverse order. string replace(string a, string b, string c) Replaces string b in string a with string c. list split(string a, string b) Splits string a at string b and returns a list of strings. string toString() Takes in any data type and converts it into a string. int hash() Takes in any data type and encodes it into a hash value. Note If the argument is NULL , the return is undefined.","title":"Built-in string functions"},{"location":"3.ngql-guide/6.functions-and-expressions/2.string/#explanations_for_the_return_of_substr_and_substring","text":"The position index starts from 0 . If pos is 0, the whole string is returned. If pos is greater than the maximum string index, an empty string is returned. If pos is a negative number, BAD_DATA is returned. If count is omitted, the function returns the substring starting at the position given by pos and extending to the end of the string. If count is 0, an empty string is returned. Using NULL as any of the argument of substr() will cause an issue . OpenCypher compatibility In openCypher, if a is null , null is returned. In openCypher, if pos is 0, the returned substring starts from the first character, and extend to count characters. In openCypher, if either pos or count is null or a negative integer, an issue is raised.","title":"Explanations for the return of substr() and substring()"},{"location":"3.ngql-guide/6.functions-and-expressions/3.date-and-time/","text":"Built-in date and time functions \u00b6 Nebula Graph supports the following built-in date and time functions: Function Description int now() Returns the current date and time of the system time zone. timestamp timestamp() Returns the current date and time of the system time zone. date date() Returns the current UTC date based on the current system. time time() Returns the current UTC time based on the current system. datetime datetime() Returns the current UTC date and time based on the current system. map duration() Returns the period of time. It can be used to calculate the specified time. For more information, see Date and time types . Examples \u00b6 nebula> RETURN now(), timestamp(), date(), time(), datetime(); +------------+-------------+------------+-----------------+----------------------------+ | now() | timestamp() | date() | time() | datetime() | +------------+-------------+------------+-----------------+----------------------------+ | 1640057560 | 1640057560 | 2021-12-21 | 03:32:40.351000 | 2021-12-21T03:32:40.351000 | +------------+-------------+------------+-----------------+----------------------------+","title":"Date and time functions"},{"location":"3.ngql-guide/6.functions-and-expressions/3.date-and-time/#built-in_date_and_time_functions","text":"Nebula Graph supports the following built-in date and time functions: Function Description int now() Returns the current date and time of the system time zone. timestamp timestamp() Returns the current date and time of the system time zone. date date() Returns the current UTC date based on the current system. time time() Returns the current UTC time based on the current system. datetime datetime() Returns the current UTC date and time based on the current system. map duration() Returns the period of time. It can be used to calculate the specified time. For more information, see Date and time types .","title":"Built-in date and time functions"},{"location":"3.ngql-guide/6.functions-and-expressions/3.date-and-time/#examples","text":"nebula> RETURN now(), timestamp(), date(), time(), datetime(); +------------+-------------+------------+-----------------+----------------------------+ | now() | timestamp() | date() | time() | datetime() | +------------+-------------+------------+-----------------+----------------------------+ | 1640057560 | 1640057560 | 2021-12-21 | 03:32:40.351000 | 2021-12-21T03:32:40.351000 | +------------+-------------+------------+-----------------+----------------------------+","title":"Examples"},{"location":"3.ngql-guide/6.functions-and-expressions/4.schema/","text":"Schema functions \u00b6 Nebula Graph supports the following schema functions. For nGQL statements \u00b6 Note The following functions are available in YIELD and WHERE clauses. Function Description id(vertex) Returns the ID of a vertex. The data type of the result is the same as the vertex ID. map properties(vertex) Returns the properties of a vertex. map properties(edge) Returns the properties of an edge. string type(edge) Returns the edge type of an edge. src(edge) Returns the source vertex ID of an edge. The data type of the result is the same as the vertex ID. dst(edge) Returns the destination vertex ID of an edge. The data type of the result is the same as the vertex ID. int rank(edge) Returns the rank value of an edge. vertex Returns the information of vertices, including VIDs, tags, properties, and values. edge Returns the information of edges, including edge types, source vertices, destination vertices, ranks, properties, and values. vertices Returns the information of vertices in a subgraph. For more information, see GET SUBGRAPH \u3002 edges Returns the information of edges in a subgraph. For more information, see GET SUBGRAPH \u3002 path Returns the information of a path. For more information, see FIND PATH \u3002 Note Since vertex, edge, vertices, edges, and path are keywords, you need to use AS <alias> to set the alias, such as GO FROM \"player100\" OVER follow YIELD edge AS e; . For statements compatible with openCypher \u00b6 Function Description id(<vertex>) Returns the ID of a vertex. The data type of the result is the same as the vertex ID. list tags(<vertex>) Returns the Tag of a vertex, which serves the same purpose as labels(). list labels(<vertex>) Returns the Tag of a vertex, which serves the same purpose as tags(). This function is used for compatibility with openCypher syntax. map properties(<vertex_or_edge>) Returns the properties of a vertex or an edge. string type(<edge>) Returns the edge type of an edge. src(<edge>) Returns the source vertex ID of an edge. The data type of the result is the same as the vertex ID. dst(<edge>) Returns the destination vertex ID of an edge. The data type of the result is the same as the vertex ID. vertex startNode(<path>) Visits an edge or a path and returns its source vertex ID. string endNode(<path>) Visits an edge or a path and returns its destination vertex ID. int rank(<edge>) Returns the rank value of an edge. Examples \u00b6 nebula> GO FROM \"player100\" OVER follow REVERSELY \\ YIELD src(edge) AS destination; +-------------+ | destination | +-------------+ | \"player101\" | | \"player102\" | ... nebula> LOOKUP ON player WHERE player.age > 45 YIELD id(vertex); +-------------+ | id(VERTEX) | +-------------+ | \"player144\" | | \"player140\" | +-------------+ nebula> MATCH (a:player) WHERE id(a) == \"player100\" \\ RETURN tags(a), labels(a), properties(a); +------------+------------+-------------------------------+ | tags(a) | labels(a) | properties(a) | +------------+------------+-------------------------------+ | [\"player\"] | [\"player\"] | {age: 42, name: \"Tim Duncan\"} | +------------+------------+-------------------------------+ nebula> MATCH p = (a :player {name : \"Tim Duncan\"})-[r:serve]-(t) \\ RETURN type(r), rank(r); +---------+---------+ | type(r) | rank(r) | +---------+---------+ | \"serve\" | 0 | +---------+---------+ nebula> MATCH p = (a :player {name : \"Tim Duncan\"})-[r:serve]-(t) \\ RETURN startNode(p), endNode(p); +----------------------------------------------------+----------------------------------+ | startNode(p) | endNode(p) | +----------------------------------------------------+----------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | (\"team204\" :team{name: \"Spurs\"}) | +----------------------------------------------------+----------------------------------+","title":"Schema functions"},{"location":"3.ngql-guide/6.functions-and-expressions/4.schema/#schema_functions","text":"Nebula Graph supports the following schema functions.","title":"Schema functions"},{"location":"3.ngql-guide/6.functions-and-expressions/4.schema/#for_ngql_statements","text":"Note The following functions are available in YIELD and WHERE clauses. Function Description id(vertex) Returns the ID of a vertex. The data type of the result is the same as the vertex ID. map properties(vertex) Returns the properties of a vertex. map properties(edge) Returns the properties of an edge. string type(edge) Returns the edge type of an edge. src(edge) Returns the source vertex ID of an edge. The data type of the result is the same as the vertex ID. dst(edge) Returns the destination vertex ID of an edge. The data type of the result is the same as the vertex ID. int rank(edge) Returns the rank value of an edge. vertex Returns the information of vertices, including VIDs, tags, properties, and values. edge Returns the information of edges, including edge types, source vertices, destination vertices, ranks, properties, and values. vertices Returns the information of vertices in a subgraph. For more information, see GET SUBGRAPH \u3002 edges Returns the information of edges in a subgraph. For more information, see GET SUBGRAPH \u3002 path Returns the information of a path. For more information, see FIND PATH \u3002 Note Since vertex, edge, vertices, edges, and path are keywords, you need to use AS <alias> to set the alias, such as GO FROM \"player100\" OVER follow YIELD edge AS e; .","title":"For nGQL statements"},{"location":"3.ngql-guide/6.functions-and-expressions/4.schema/#for_statements_compatible_with_opencypher","text":"Function Description id(<vertex>) Returns the ID of a vertex. The data type of the result is the same as the vertex ID. list tags(<vertex>) Returns the Tag of a vertex, which serves the same purpose as labels(). list labels(<vertex>) Returns the Tag of a vertex, which serves the same purpose as tags(). This function is used for compatibility with openCypher syntax. map properties(<vertex_or_edge>) Returns the properties of a vertex or an edge. string type(<edge>) Returns the edge type of an edge. src(<edge>) Returns the source vertex ID of an edge. The data type of the result is the same as the vertex ID. dst(<edge>) Returns the destination vertex ID of an edge. The data type of the result is the same as the vertex ID. vertex startNode(<path>) Visits an edge or a path and returns its source vertex ID. string endNode(<path>) Visits an edge or a path and returns its destination vertex ID. int rank(<edge>) Returns the rank value of an edge.","title":"For statements compatible with openCypher"},{"location":"3.ngql-guide/6.functions-and-expressions/4.schema/#examples","text":"nebula> GO FROM \"player100\" OVER follow REVERSELY \\ YIELD src(edge) AS destination; +-------------+ | destination | +-------------+ | \"player101\" | | \"player102\" | ... nebula> LOOKUP ON player WHERE player.age > 45 YIELD id(vertex); +-------------+ | id(VERTEX) | +-------------+ | \"player144\" | | \"player140\" | +-------------+ nebula> MATCH (a:player) WHERE id(a) == \"player100\" \\ RETURN tags(a), labels(a), properties(a); +------------+------------+-------------------------------+ | tags(a) | labels(a) | properties(a) | +------------+------------+-------------------------------+ | [\"player\"] | [\"player\"] | {age: 42, name: \"Tim Duncan\"} | +------------+------------+-------------------------------+ nebula> MATCH p = (a :player {name : \"Tim Duncan\"})-[r:serve]-(t) \\ RETURN type(r), rank(r); +---------+---------+ | type(r) | rank(r) | +---------+---------+ | \"serve\" | 0 | +---------+---------+ nebula> MATCH p = (a :player {name : \"Tim Duncan\"})-[r:serve]-(t) \\ RETURN startNode(p), endNode(p); +----------------------------------------------------+----------------------------------+ | startNode(p) | endNode(p) | +----------------------------------------------------+----------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | (\"team204\" :team{name: \"Spurs\"}) | +----------------------------------------------------+----------------------------------+","title":"Examples"},{"location":"3.ngql-guide/6.functions-and-expressions/5.case-expressions/","text":"CASE expressions \u00b6 The CASE expression uses conditions to filter the result of an nGQL query statement. It is usually used in the YIELD and RETURN clauses. nGQL provides two forms of CASE expressions just like openCypher: the simple form and the generic form. The CASE expression will traverse all the conditions. When the first condition is met, the CASE expression stops reading the conditions and returns the result. If no conditions are met, it returns the result in the ELSE clause. If there is no ELSE clause and no conditions are met, it returns NULL . The simple form of CASE expressions \u00b6 Syntax \u00b6 CASE <comparer> WHEN <value> THEN <result> [WHEN ...] [ELSE <default>] END Caution Always remember to end the CASE expression with an END . Parameter Description comparer A value or a valid expression that outputs a value. This value is used to compare with the value . value It will be compared with the comparer . If the value matches the comparer , then this condition is met. result The result is returned by the CASE expression if the value matches the comparer . default The default is returned by the CASE expression if no conditions are met. Examples \u00b6 nebula> RETURN \\ CASE 2+3 \\ WHEN 4 THEN 0 \\ WHEN 5 THEN 1 \\ ELSE -1 \\ END \\ AS result; +--------+ | result | +--------+ | 1 | +--------+ nebula> GO FROM \"player100\" OVER follow \\ YIELD properties($$).name AS Name, \\ CASE properties($$).age > 35 \\ WHEN true THEN \"Yes\" \\ WHEN false THEN \"No\" \\ ELSE \"Nah\" \\ END \\ AS Age_above_35; +-----------------+--------------+ | Name | Age_above_35 | +-----------------+--------------+ | \"Tony Parker\" | \"Yes\" | | \"Manu Ginobili\" | \"Yes\" | +-----------------+--------------+ The generic form of CASE expressions \u00b6 Syntax \u00b6 CASE WHEN <condition> THEN <result> [WHEN ...] [ELSE <default>] END Parameter Description condition If the condition is evaluated as true, the result is returned by the CASE expression. result The result is returned by the CASE expression if the condition is evaluated as true. default The default is returned by the CASE expression if no conditions are met. Examples \u00b6 nebula> YIELD \\ CASE WHEN 4 > 5 THEN 0 \\ WHEN 3+4==7 THEN 1 \\ ELSE 2 \\ END \\ AS result; +--------+ | result | +--------+ | 1 | +--------+ nebula> MATCH (v:player) WHERE v.player.age > 30 \\ RETURN v.player.name AS Name, \\ CASE \\ WHEN v.player.name STARTS WITH \"T\" THEN \"Yes\" \\ ELSE \"No\" \\ END \\ AS Starts_with_T; +---------------------+---------------+ | Name | Starts_with_T | +---------------------+---------------+ | \"Tim\" | \"Yes\" | | \"LaMarcus Aldridge\" | \"No\" | | \"Tony Parker\" | \"Yes\" | +---------------------+---------------+ Differences between the simple form and the generic form \u00b6 To avoid the misuse of the simple form and the generic form, it is important to understand their differences. The following example can help explain them. nebula> GO FROM \"player100\" OVER follow \\ YIELD properties($$).name AS Name, properties($$).age AS Age, \\ CASE properties($$).age \\ WHEN properties($$).age > 35 THEN \"Yes\" \\ ELSE \"No\" \\ END \\ AS Age_above_35; +-----------------+-----+--------------+ | Name | Age | Age_above_35 | +-----------------+-----+--------------+ | \"Tony Parker\" | 36 | \"No\" | | \"Manu Ginobili\" | 41 | \"No\" | +-----------------+-----+--------------+ The preceding GO query is intended to output Yes when the player's age is above 35. However, in this example, when the player's age is 36, the actual output is not as expected: It is No instead of Yes . This is because the query uses the CASE expression in the simple form, and a comparison between the values of $$.player.age and $$.player.age > 35 is made. When the player age is 36: The value of $$.player.age is 36 . It is an integer. $$.player.age > 35 is evaluated to be true . It is a boolean. The values of $$.player.age and $$.player.age > 35 do not match. Therefore, the condition is not met and No is returned.","title":"Case expressions"},{"location":"3.ngql-guide/6.functions-and-expressions/5.case-expressions/#case_expressions","text":"The CASE expression uses conditions to filter the result of an nGQL query statement. It is usually used in the YIELD and RETURN clauses. nGQL provides two forms of CASE expressions just like openCypher: the simple form and the generic form. The CASE expression will traverse all the conditions. When the first condition is met, the CASE expression stops reading the conditions and returns the result. If no conditions are met, it returns the result in the ELSE clause. If there is no ELSE clause and no conditions are met, it returns NULL .","title":"CASE expressions"},{"location":"3.ngql-guide/6.functions-and-expressions/5.case-expressions/#the_simple_form_of_case_expressions","text":"","title":"The simple form of CASE expressions"},{"location":"3.ngql-guide/6.functions-and-expressions/5.case-expressions/#syntax","text":"CASE <comparer> WHEN <value> THEN <result> [WHEN ...] [ELSE <default>] END Caution Always remember to end the CASE expression with an END . Parameter Description comparer A value or a valid expression that outputs a value. This value is used to compare with the value . value It will be compared with the comparer . If the value matches the comparer , then this condition is met. result The result is returned by the CASE expression if the value matches the comparer . default The default is returned by the CASE expression if no conditions are met.","title":"Syntax"},{"location":"3.ngql-guide/6.functions-and-expressions/5.case-expressions/#examples","text":"nebula> RETURN \\ CASE 2+3 \\ WHEN 4 THEN 0 \\ WHEN 5 THEN 1 \\ ELSE -1 \\ END \\ AS result; +--------+ | result | +--------+ | 1 | +--------+ nebula> GO FROM \"player100\" OVER follow \\ YIELD properties($$).name AS Name, \\ CASE properties($$).age > 35 \\ WHEN true THEN \"Yes\" \\ WHEN false THEN \"No\" \\ ELSE \"Nah\" \\ END \\ AS Age_above_35; +-----------------+--------------+ | Name | Age_above_35 | +-----------------+--------------+ | \"Tony Parker\" | \"Yes\" | | \"Manu Ginobili\" | \"Yes\" | +-----------------+--------------+","title":"Examples"},{"location":"3.ngql-guide/6.functions-and-expressions/5.case-expressions/#the_generic_form_of_case_expressions","text":"","title":"The generic form of CASE expressions"},{"location":"3.ngql-guide/6.functions-and-expressions/5.case-expressions/#syntax_1","text":"CASE WHEN <condition> THEN <result> [WHEN ...] [ELSE <default>] END Parameter Description condition If the condition is evaluated as true, the result is returned by the CASE expression. result The result is returned by the CASE expression if the condition is evaluated as true. default The default is returned by the CASE expression if no conditions are met.","title":"Syntax"},{"location":"3.ngql-guide/6.functions-and-expressions/5.case-expressions/#examples_1","text":"nebula> YIELD \\ CASE WHEN 4 > 5 THEN 0 \\ WHEN 3+4==7 THEN 1 \\ ELSE 2 \\ END \\ AS result; +--------+ | result | +--------+ | 1 | +--------+ nebula> MATCH (v:player) WHERE v.player.age > 30 \\ RETURN v.player.name AS Name, \\ CASE \\ WHEN v.player.name STARTS WITH \"T\" THEN \"Yes\" \\ ELSE \"No\" \\ END \\ AS Starts_with_T; +---------------------+---------------+ | Name | Starts_with_T | +---------------------+---------------+ | \"Tim\" | \"Yes\" | | \"LaMarcus Aldridge\" | \"No\" | | \"Tony Parker\" | \"Yes\" | +---------------------+---------------+","title":"Examples"},{"location":"3.ngql-guide/6.functions-and-expressions/5.case-expressions/#differences_between_the_simple_form_and_the_generic_form","text":"To avoid the misuse of the simple form and the generic form, it is important to understand their differences. The following example can help explain them. nebula> GO FROM \"player100\" OVER follow \\ YIELD properties($$).name AS Name, properties($$).age AS Age, \\ CASE properties($$).age \\ WHEN properties($$).age > 35 THEN \"Yes\" \\ ELSE \"No\" \\ END \\ AS Age_above_35; +-----------------+-----+--------------+ | Name | Age | Age_above_35 | +-----------------+-----+--------------+ | \"Tony Parker\" | 36 | \"No\" | | \"Manu Ginobili\" | 41 | \"No\" | +-----------------+-----+--------------+ The preceding GO query is intended to output Yes when the player's age is above 35. However, in this example, when the player's age is 36, the actual output is not as expected: It is No instead of Yes . This is because the query uses the CASE expression in the simple form, and a comparison between the values of $$.player.age and $$.player.age > 35 is made. When the player age is 36: The value of $$.player.age is 36 . It is an integer. $$.player.age > 35 is evaluated to be true . It is a boolean. The values of $$.player.age and $$.player.age > 35 do not match. Therefore, the condition is not met and No is returned.","title":"Differences between the simple form and the generic form"},{"location":"3.ngql-guide/6.functions-and-expressions/6.list/","text":"List functions \u00b6 Nebula Graph supports the following list functions: Function Description keys(expr) Returns a list containing the string representations for all the property names of vertices, edges, or maps. labels(vertex) Returns the list containing all the tags of a vertex. nodes(path) Returns the list containing all the vertices in a path. range(start, end [, step]) Returns the list containing all the fixed-length steps in [start,end] . step is 1 by default. relationships(path) Returns the list containing all the relationships in a path. reverse(list) Returns the list reversing the order of all elements in the original list. tail(list) Returns all the elements of the original list, excluding the first one. head(list) Returns the first element of a list. last(list) Returns the last element of a list. coalesce(list) Returns the first not null value in a list. reduce() See reduce() function . Note If the argument is NULL , the output is undefined. Examples \u00b6 nebula> WITH [NULL, 4923, 'abc', 521, 487] AS ids \\ RETURN reverse(ids), tail(ids), head(ids), last(ids), coalesce(ids); +-----------------------------------+-------------------------+-----------+-----------+---------------+ | reverse(ids) | tail(ids) | head(ids) | last(ids) | coalesce(ids) | +-----------------------------------+-------------------------+-----------+-----------+---------------+ | [487, 521, \"abc\", 4923, __NULL__] | [4923, \"abc\", 521, 487] | __NULL__ | 487 | 4923 | +-----------------------------------+-------------------------+-----------+-----------+---------------+ nebula> MATCH (a:player)-[r]->() \\ WHERE id(a) == \"player100\" \\ RETURN labels(a), keys(r); +------------+----------------------------+ | labels(a) | keys(r) | +------------+----------------------------+ | [\"player\"] | [\"degree\"] | | [\"player\"] | [\"degree\"] | | [\"player\"] | [\"end_year\", \"start_year\"] | +------------+----------------------------+ nebula> MATCH p = (a:player)-[]->(b)-[]->(c:team) \\ WHERE a.player.name == \"Tim Duncan\" AND c.team.name == \"Spurs\" \\ RETURN nodes(p); +-----------------------------------------------------------------------------------------------------------------------------------------------+ | nodes(p) | +-----------------------------------------------------------------------------------------------------------------------------------------------+ | [(\"player100\" :player{age: 42, name: \"Tim Duncan\"}), (\"player101\" :player{age: 36, name: \"Tony Parker\"}), (\"team204\" :team{name: \"Spurs\"})] | | [(\"player100\" :player{age: 42, name: \"Tim Duncan\"}), (\"player125\" :player{age: 41, name: \"Manu Ginobili\"}), (\"team204\" :team{name: \"Spurs\"})] | +-----------------------------------------------------------------------------------------------------------------------------------------------+ nebula> MATCH p = (a:player)-[]->(b)-[]->(c:team) WHERE a.player.name == \"Tim Duncan\" AND c.team.name == \"Spurs\" RETURN relationships(p); +-----------------------------------------------------------------------------------------------------------------------------+ | relationships(p) | +-----------------------------------------------------------------------------------------------------------------------------+ | [[:follow \"player100\"->\"player101\" @0 {degree: 95}], [:serve \"player101\"->\"team204\" @0 {end_year: 2018, start_year: 1999}]] | | [[:follow \"player100\"->\"player125\" @0 {degree: 95}], [:serve \"player125\"->\"team204\" @0 {end_year: 2018, start_year: 2002}]] | +-----------------------------------------------------------------------------------------------------------------------------+","title":"List functions"},{"location":"3.ngql-guide/6.functions-and-expressions/6.list/#list_functions","text":"Nebula Graph supports the following list functions: Function Description keys(expr) Returns a list containing the string representations for all the property names of vertices, edges, or maps. labels(vertex) Returns the list containing all the tags of a vertex. nodes(path) Returns the list containing all the vertices in a path. range(start, end [, step]) Returns the list containing all the fixed-length steps in [start,end] . step is 1 by default. relationships(path) Returns the list containing all the relationships in a path. reverse(list) Returns the list reversing the order of all elements in the original list. tail(list) Returns all the elements of the original list, excluding the first one. head(list) Returns the first element of a list. last(list) Returns the last element of a list. coalesce(list) Returns the first not null value in a list. reduce() See reduce() function . Note If the argument is NULL , the output is undefined.","title":"List functions"},{"location":"3.ngql-guide/6.functions-and-expressions/6.list/#examples","text":"nebula> WITH [NULL, 4923, 'abc', 521, 487] AS ids \\ RETURN reverse(ids), tail(ids), head(ids), last(ids), coalesce(ids); +-----------------------------------+-------------------------+-----------+-----------+---------------+ | reverse(ids) | tail(ids) | head(ids) | last(ids) | coalesce(ids) | +-----------------------------------+-------------------------+-----------+-----------+---------------+ | [487, 521, \"abc\", 4923, __NULL__] | [4923, \"abc\", 521, 487] | __NULL__ | 487 | 4923 | +-----------------------------------+-------------------------+-----------+-----------+---------------+ nebula> MATCH (a:player)-[r]->() \\ WHERE id(a) == \"player100\" \\ RETURN labels(a), keys(r); +------------+----------------------------+ | labels(a) | keys(r) | +------------+----------------------------+ | [\"player\"] | [\"degree\"] | | [\"player\"] | [\"degree\"] | | [\"player\"] | [\"end_year\", \"start_year\"] | +------------+----------------------------+ nebula> MATCH p = (a:player)-[]->(b)-[]->(c:team) \\ WHERE a.player.name == \"Tim Duncan\" AND c.team.name == \"Spurs\" \\ RETURN nodes(p); +-----------------------------------------------------------------------------------------------------------------------------------------------+ | nodes(p) | +-----------------------------------------------------------------------------------------------------------------------------------------------+ | [(\"player100\" :player{age: 42, name: \"Tim Duncan\"}), (\"player101\" :player{age: 36, name: \"Tony Parker\"}), (\"team204\" :team{name: \"Spurs\"})] | | [(\"player100\" :player{age: 42, name: \"Tim Duncan\"}), (\"player125\" :player{age: 41, name: \"Manu Ginobili\"}), (\"team204\" :team{name: \"Spurs\"})] | +-----------------------------------------------------------------------------------------------------------------------------------------------+ nebula> MATCH p = (a:player)-[]->(b)-[]->(c:team) WHERE a.player.name == \"Tim Duncan\" AND c.team.name == \"Spurs\" RETURN relationships(p); +-----------------------------------------------------------------------------------------------------------------------------+ | relationships(p) | +-----------------------------------------------------------------------------------------------------------------------------+ | [[:follow \"player100\"->\"player101\" @0 {degree: 95}], [:serve \"player101\"->\"team204\" @0 {end_year: 2018, start_year: 1999}]] | | [[:follow \"player100\"->\"player125\" @0 {degree: 95}], [:serve \"player125\"->\"team204\" @0 {end_year: 2018, start_year: 2002}]] | +-----------------------------------------------------------------------------------------------------------------------------+","title":"Examples"},{"location":"3.ngql-guide/6.functions-and-expressions/7.count/","text":"count() function \u00b6 The count() function counts the number of the specified values or rows. (Native nGQL) You can use count() and GROUP BY together to group and count the number of specific values. Use YIELD to return. (OpenCypher style) You can use count() and RETURN . GROUP BY is not necessary. Syntax \u00b6 count({expr | *}) count(*) returns the number of rows (including NULL). count(expr) returns the number of non-NULL values that meet the expression. count() and size() are different. Examples \u00b6 nebula> WITH [NULL, 1, 1, 2, 2] As a UNWIND a AS b \\ RETURN count(b), count(*), count(DISTINCT b); +----------+----------+-------------------+ | count(b) | count(*) | count(distinct b) | +----------+----------+-------------------+ | 4 | 5 | 2 | +----------+----------+-------------------+ # The statement in the following example searches for the people whom `player101` follows and people who follow `player101`, i.e. a bidirectional query. nebula> GO FROM \"player101\" OVER follow BIDIRECT \\ YIELD properties($$).name AS Name \\ | GROUP BY $-.Name YIELD $-.Name, count(*); +---------------------+----------+ | $-.Name | count(*) | +---------------------+----------+ | \"LaMarcus Aldridge\" | 2 | | \"Tim Duncan\" | 2 | | \"Marco Belinelli\" | 1 | | \"Manu Ginobili\" | 1 | | \"Boris Diaw\" | 1 | | \"Dejounte Murray\" | 1 | +---------------------+----------+ The preceding example retrieves two columns: $-.Name : the names of the people. count(*) : how many times the names show up. Because there are no duplicate names in the basketballplayer dataset, the number 2 in the column count(*) shows that the person in that row and player101 have followed each other. # a: The statement in the following example retrieves the age distribution of the players in the dataset. nebula> LOOKUP ON player \\ YIELD player.age As playerage \\ | GROUP BY $-.playerage \\ YIELD $-.playerage as age, count(*) AS number \\ | ORDER BY $-.number DESC, $-.age DESC; +-----+--------+ | age | number | +-----+--------+ | 34 | 4 | | 33 | 4 | | 30 | 4 | | 29 | 4 | | 38 | 3 | +-----+--------+ ... # b: The statement in the following example retrieves the age distribution of the players in the dataset. nebula> MATCH (n:player) \\ RETURN n.player.age as age, count(*) as number \\ ORDER BY number DESC, age DESC; +-----+--------+ | age | number | +-----+--------+ | 34 | 4 | | 33 | 4 | | 30 | 4 | | 29 | 4 | | 38 | 3 | +-----+--------+ ... # The statement in the following example counts the number of edges that Tim Duncan relates. nebula> MATCH (v:player{name:\"Tim Duncan\"}) -- (v2) \\ RETURN count(DISTINCT v2); +--------------------+ | count(distinct v2) | +--------------------+ | 11 | +--------------------+ # The statement in the following example counts the number of edges that Tim Duncan relates and returns two columns (no DISTINCT and DISTINCT) in multi-hop queries. nebula> MATCH (n:player {name : \"Tim Duncan\"})-[]->(friend:player)-[]->(fof:player) \\ RETURN count(fof), count(DISTINCT fof); +------------+---------------------+ | count(fof) | count(distinct fof) | +------------+---------------------+ | 4 | 3 | +------------+---------------------+","title":"count()"},{"location":"3.ngql-guide/6.functions-and-expressions/7.count/#count_function","text":"The count() function counts the number of the specified values or rows. (Native nGQL) You can use count() and GROUP BY together to group and count the number of specific values. Use YIELD to return. (OpenCypher style) You can use count() and RETURN . GROUP BY is not necessary.","title":"count() function"},{"location":"3.ngql-guide/6.functions-and-expressions/7.count/#syntax","text":"count({expr | *}) count(*) returns the number of rows (including NULL). count(expr) returns the number of non-NULL values that meet the expression. count() and size() are different.","title":"Syntax"},{"location":"3.ngql-guide/6.functions-and-expressions/7.count/#examples","text":"nebula> WITH [NULL, 1, 1, 2, 2] As a UNWIND a AS b \\ RETURN count(b), count(*), count(DISTINCT b); +----------+----------+-------------------+ | count(b) | count(*) | count(distinct b) | +----------+----------+-------------------+ | 4 | 5 | 2 | +----------+----------+-------------------+ # The statement in the following example searches for the people whom `player101` follows and people who follow `player101`, i.e. a bidirectional query. nebula> GO FROM \"player101\" OVER follow BIDIRECT \\ YIELD properties($$).name AS Name \\ | GROUP BY $-.Name YIELD $-.Name, count(*); +---------------------+----------+ | $-.Name | count(*) | +---------------------+----------+ | \"LaMarcus Aldridge\" | 2 | | \"Tim Duncan\" | 2 | | \"Marco Belinelli\" | 1 | | \"Manu Ginobili\" | 1 | | \"Boris Diaw\" | 1 | | \"Dejounte Murray\" | 1 | +---------------------+----------+ The preceding example retrieves two columns: $-.Name : the names of the people. count(*) : how many times the names show up. Because there are no duplicate names in the basketballplayer dataset, the number 2 in the column count(*) shows that the person in that row and player101 have followed each other. # a: The statement in the following example retrieves the age distribution of the players in the dataset. nebula> LOOKUP ON player \\ YIELD player.age As playerage \\ | GROUP BY $-.playerage \\ YIELD $-.playerage as age, count(*) AS number \\ | ORDER BY $-.number DESC, $-.age DESC; +-----+--------+ | age | number | +-----+--------+ | 34 | 4 | | 33 | 4 | | 30 | 4 | | 29 | 4 | | 38 | 3 | +-----+--------+ ... # b: The statement in the following example retrieves the age distribution of the players in the dataset. nebula> MATCH (n:player) \\ RETURN n.player.age as age, count(*) as number \\ ORDER BY number DESC, age DESC; +-----+--------+ | age | number | +-----+--------+ | 34 | 4 | | 33 | 4 | | 30 | 4 | | 29 | 4 | | 38 | 3 | +-----+--------+ ... # The statement in the following example counts the number of edges that Tim Duncan relates. nebula> MATCH (v:player{name:\"Tim Duncan\"}) -- (v2) \\ RETURN count(DISTINCT v2); +--------------------+ | count(distinct v2) | +--------------------+ | 11 | +--------------------+ # The statement in the following example counts the number of edges that Tim Duncan relates and returns two columns (no DISTINCT and DISTINCT) in multi-hop queries. nebula> MATCH (n:player {name : \"Tim Duncan\"})-[]->(friend:player)-[]->(fof:player) \\ RETURN count(fof), count(DISTINCT fof); +------------+---------------------+ | count(fof) | count(distinct fof) | +------------+---------------------+ | 4 | 3 | +------------+---------------------+","title":"Examples"},{"location":"3.ngql-guide/6.functions-and-expressions/8.predicate/","text":"Predicate functions \u00b6 Predicate functions return true or false . They are most commonly used in WHERE clauses. Nebula Graph supports the following predicate functions: Functions Description exists() Returns true if the specified property exists in the vertex, edge or map. Otherwise, returns false . any() Returns true if the specified predicate holds for at least one element in the given list. Otherwise, returns false . all() Returns true if the specified predicate holds for all elements in the given list. Otherwise, returns false . none() Returns true if the specified predicate holds for no element in the given list. Otherwise, returns false . single() Returns true if the specified predicate holds for exactly one of the elements in the given list. Otherwise, returns false . Note NULL is returned if the list is NULL or all of its elements are NULL. Compatibility In openCypher, only function exists() is defined and specified. The other functions are implement-dependent. Syntax \u00b6 <predicate>(<variable> IN <list> WHERE <condition>) Examples \u00b6 nebula> RETURN any(n IN [1, 2, 3, 4, 5, NULL] \\ WHERE n > 2) AS r; +------+ | r | +------+ | true | +------+ nebula> RETURN single(n IN range(1, 5) \\ WHERE n == 3) AS r; +------+ | r | +------+ | true | +------+ nebula> RETURN none(n IN range(1, 3) \\ WHERE n == 0) AS r; +------+ | r | +------+ | true | +------+ nebula> WITH [1, 2, 3, 4, 5, NULL] AS a \\ RETURN any(n IN a WHERE n > 2); +-------------------------+ | any(n IN a WHERE (n>2)) | +-------------------------+ | true | +-------------------------+ nebula> MATCH p = (n:player{name:\"LeBron James\"})<-[:follow]-(m) \\ RETURN nodes(p)[0].name AS n1, nodes(p)[1].name AS n2, \\ all(n IN nodes(p) WHERE n.name NOT STARTS WITH \"D\") AS b; +----------------+-------------------+-------+ | n1 | n2 | b | +----------------+-------------------+-------+ | \"LeBron James\" | \"Danny Green\" | false | | \"LeBron James\" | \"Dejounte Murray\" | false | | \"LeBron James\" | \"Chris Paul\" | true | | \"LeBron James\" | \"Kyrie Irving\" | true | | \"LeBron James\" | \"Carmelo Anthony\" | true | | \"LeBron James\" | \"Dwyane Wade\" | false | +----------------+-------------------+-------+ nebula> MATCH p = (n:player{name:\"LeBron James\"})-[:follow]->(m) \\ RETURN single(n IN nodes(p) WHERE n.age > 40) AS b; +------+ | b | +------+ | true | +------+ nebula> MATCH (n:player) \\ RETURN exists(n.player.id), n IS NOT NULL; +--------------+---------------+ | exists(n.id) | n IS NOT NULL | +--------------+---------------+ | false | true | ... nebula> MATCH (n:player) \\ WHERE exists(n['name']) RETURN n; +-------------------------------------------------------------------------------------------------------------+ | n | +-------------------------------------------------------------------------------------------------------------+ | (\"Grant Hill\" :player{age: 46, name: \"Grant Hill\"}) | | (\"Marc Gasol\" :player{age: 34, name: \"Marc Gasol\"}) | +-------------------------------------------------------------------------------------------------------------+ ...","title":"Predicate functions"},{"location":"3.ngql-guide/6.functions-and-expressions/8.predicate/#predicate_functions","text":"Predicate functions return true or false . They are most commonly used in WHERE clauses. Nebula Graph supports the following predicate functions: Functions Description exists() Returns true if the specified property exists in the vertex, edge or map. Otherwise, returns false . any() Returns true if the specified predicate holds for at least one element in the given list. Otherwise, returns false . all() Returns true if the specified predicate holds for all elements in the given list. Otherwise, returns false . none() Returns true if the specified predicate holds for no element in the given list. Otherwise, returns false . single() Returns true if the specified predicate holds for exactly one of the elements in the given list. Otherwise, returns false . Note NULL is returned if the list is NULL or all of its elements are NULL. Compatibility In openCypher, only function exists() is defined and specified. The other functions are implement-dependent.","title":"Predicate functions"},{"location":"3.ngql-guide/6.functions-and-expressions/8.predicate/#syntax","text":"<predicate>(<variable> IN <list> WHERE <condition>)","title":"Syntax"},{"location":"3.ngql-guide/6.functions-and-expressions/8.predicate/#examples","text":"nebula> RETURN any(n IN [1, 2, 3, 4, 5, NULL] \\ WHERE n > 2) AS r; +------+ | r | +------+ | true | +------+ nebula> RETURN single(n IN range(1, 5) \\ WHERE n == 3) AS r; +------+ | r | +------+ | true | +------+ nebula> RETURN none(n IN range(1, 3) \\ WHERE n == 0) AS r; +------+ | r | +------+ | true | +------+ nebula> WITH [1, 2, 3, 4, 5, NULL] AS a \\ RETURN any(n IN a WHERE n > 2); +-------------------------+ | any(n IN a WHERE (n>2)) | +-------------------------+ | true | +-------------------------+ nebula> MATCH p = (n:player{name:\"LeBron James\"})<-[:follow]-(m) \\ RETURN nodes(p)[0].name AS n1, nodes(p)[1].name AS n2, \\ all(n IN nodes(p) WHERE n.name NOT STARTS WITH \"D\") AS b; +----------------+-------------------+-------+ | n1 | n2 | b | +----------------+-------------------+-------+ | \"LeBron James\" | \"Danny Green\" | false | | \"LeBron James\" | \"Dejounte Murray\" | false | | \"LeBron James\" | \"Chris Paul\" | true | | \"LeBron James\" | \"Kyrie Irving\" | true | | \"LeBron James\" | \"Carmelo Anthony\" | true | | \"LeBron James\" | \"Dwyane Wade\" | false | +----------------+-------------------+-------+ nebula> MATCH p = (n:player{name:\"LeBron James\"})-[:follow]->(m) \\ RETURN single(n IN nodes(p) WHERE n.age > 40) AS b; +------+ | b | +------+ | true | +------+ nebula> MATCH (n:player) \\ RETURN exists(n.player.id), n IS NOT NULL; +--------------+---------------+ | exists(n.id) | n IS NOT NULL | +--------------+---------------+ | false | true | ... nebula> MATCH (n:player) \\ WHERE exists(n['name']) RETURN n; +-------------------------------------------------------------------------------------------------------------+ | n | +-------------------------------------------------------------------------------------------------------------+ | (\"Grant Hill\" :player{age: 46, name: \"Grant Hill\"}) | | (\"Marc Gasol\" :player{age: 34, name: \"Marc Gasol\"}) | +-------------------------------------------------------------------------------------------------------------+ ...","title":"Examples"},{"location":"3.ngql-guide/6.functions-and-expressions/9.user-defined-functions/","text":"User-defined functions \u00b6 OpenCypher compatibility \u00b6 User-defined functions (UDF) and storage processes are not yet supported nor designed in Nebula Graph 3.1.0.","title":"User-defined functions"},{"location":"3.ngql-guide/6.functions-and-expressions/9.user-defined-functions/#user-defined_functions","text":"","title":"User-defined functions"},{"location":"3.ngql-guide/6.functions-and-expressions/9.user-defined-functions/#opencypher_compatibility","text":"User-defined functions (UDF) and storage processes are not yet supported nor designed in Nebula Graph 3.1.0.","title":"OpenCypher compatibility"},{"location":"3.ngql-guide/7.general-query-statements/2.match/","text":"MATCH \u00b6 The MATCH statement supports searching based on pattern matching. A MATCH statement defines a search pattern and uses it to match data stored in Nebula Graph and to retrieve them in the form defined in the RETURN clause. The examples in this topic use the basketballplayer dataset as the sample dataset. Syntax \u00b6 The syntax of MATCH is relatively more flexible compared with that of other query statements such as GO or LOOKUP . But generally, it can be summarized as follows. MATCH <pattern> [<clause_1>] RETURN <output> [<clause_2>]; pattern : For the detailed description of patterns, see Patterns . The MATCH statement supports matching one or multiple patterns. Multiple patterns are separated by commas (,). For example: (a)-[]->(b),(c)-[]->(d) . clause_1 : The WHERE , WITH , UNWIND , and OPTIONAL MATCH clauses are supported, and the MATCH clause can also be used. output : Define the output to be returned. You can rename the output column by using AS . clause_2 : The ORDER BY and LIMIT clauses are supported. Precautions \u00b6 Legacy version compatibility Starting from Nebula Graph version 3.0.0, in order to distinguish the properties of different tags, you need to specify a tag name when querying properties. The original statement RETURN variable_name.property_name is changed to RETURN variable_name.<tag_name>.property_name . Note Currently the match statement cannot find dangling edges. The MATCH statement retrieves data according to the RETURN clause. The path type of the MATCH statement is trail . That is, only vertices can be repeatedly visited in the graph traversal. Edges cannot be repeatedly visited. For details, see path . In a valid MATCH statement, the VID of a specific vertex must be specified with the id() function in the WHERE clause. There is no need to create an index. When traversing all vertices and edges with MATCH , such as MATCH (v) RETURN v LIMIT N , there is no need to create an index, but you need to use LIMIT to limit the number of output results. When traversing all vertices of the specified Tag or edge of the specified Edge Type, such as MATCH (v:player) RETURN v LIMIT N , there is no need to create an index, but you need to use LIMIT to limit the number of output results. In addition to the foregoing, make sure there is at least one index in the MATCH statement. How to create native indexes, see CREATE INDEX . Using patterns in MATCH statements \u00b6 Create indexes \u00b6 # The following example creates an index on both the name property of the tag player and the edge type follow. nebula> CREATE TAG INDEX IF NOT EXISTS name ON player(name(20)); nebula> CREATE EDGE INDEX IF NOT EXISTS follow_index on follow(); # The following example rebuilds the index. nebula> REBUILD TAG INDEX name; +------------+ | New Job Id | +------------+ | 121 | +------------+ nebula> REBUILD EDGE INDEX follow_index; +------------+ | New Job Id | +------------+ | 122 | +------------+ # The following example makes sure the index is rebuilt successfully. nebula> SHOW JOB 121; +----------------+---------------------+------------+----------------------------+----------------------------+-------------+ | Job Id(TaskId) | Command(Dest) | Status | Start Time | Stop Time | Error Code | +----------------+---------------------+------------+----------------------------+----------------------------+-------------+ | 121 | \"REBUILD_TAG_INDEX\" | \"FINISHED\" | 2021-05-27T02:18:02.000000 | 2021-05-27T02:18:02.000000 | \"SUCCEEDED\" | | 0 | \"storaged1\" | \"FINISHED\" | 2021-05-27T02:18:02.000000 | 2021-05-27T02:18:02.000000 | \"SUCCEEDED\" | | 1 | \"storaged0\" | \"FINISHED\" | 2021-05-27T02:18:02.000000 | 2021-05-27T02:18:02.000000 | \"SUCCEEDED\" | | 2 | \"storaged2\" | \"FINISHED\" | 2021-05-27T02:18:02.000000 | 2021-05-27T02:18:02.000000 | \"SUCCEEDED\" | +----------------+---------------------+------------+----------------------------+----------------------------+-------------+ nebula> SHOW JOB 122; +----------------+----------------------+------------+----------------------------+----------------------------+-------------+ | Job Id(TaskId) | Command(Dest) | Status | Start Time | Stop Time | Error Code | +----------------+----------------------+------------+----------------------------+----------------------------+-------------+ | 122 | \"REBUILD_EDGE_INDEX\" | \"FINISHED\" | 2021-05-27T02:18:11.000000 | 2021-05-27T02:18:11.000000 | \"SUCCEEDED\" | | 0 | \"storaged1\" | \"FINISHED\" | 2021-05-27T02:18:11.000000 | 2021-05-27T02:18:21.000000 | \"SUCCEEDED\" | | 1 | \"storaged0\" | \"FINISHED\" | 2021-05-27T02:18:11.000000 | 2021-05-27T02:18:21.000000 | \"SUCCEEDED\" | | 2 | \"storaged2\" | \"FINISHED\" | 2021-05-27T02:18:11.000000 | 2021-05-27T02:18:21.000000 | \"SUCCEEDED\" | +----------------+----------------------+------------+----------------------------+----------------------------+-------------+ Match vertices \u00b6 Legacy version compatibility In Nebula Graph versions earlier than 3.0.0, nGQL does not support MATCH (v) RETURN v . As of version 3.0.0, nGQL support MATCH (v) RETURN v LIMIT n , there is no need to create an index, but you must use LIMIT to limit the number of output results. nGQL still does not support MATCH (v) RETURN v . You can use a user-defined variable in a pair of parentheses to represent a vertex in a pattern. For example: (v) . nebula> MATCH (v) \\ RETURN v \\ LIMIT 3; +-----------------------------------------------------------+ | v | +-----------------------------------------------------------+ | (\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"}) | | (\"player106\" :player{age: 25, name: \"Kyle Anderson\"}) | | (\"player115\" :player{age: 40, name: \"Kobe Bryant\"}) | +-----------------------------------------------------------+ Match tags \u00b6 Note In Nebula Graph versions earlier than 3.0.0, the prerequisite for matching a tag is that the tag itself has an index or a certain property of the tag has an index. As of version 3.0.0, there is no need to create an index for matching a tag, but you need to use LIMIT to limit the number of output results. You can specify a tag with :<tag_name> after the vertex in a pattern. nebula> MATCH (v:player) \\ RETURN v; +---------------------------------------------------------------+ | v | +---------------------------------------------------------------+ | (\"player105\" :player{age: 31, name: \"Danny Green\"}) | | (\"player109\" :player{age: 34, name: \"Tiago Splitter\"}) | | (\"player111\" :player{age: 38, name: \"David West\"}) | ... To match vertices with multiple tags, use colons (:). nebula> CREATE TAG actor (name string, age int); nebula> INSERT VERTEX actor(name, age) VALUES \"player100\":(\"Tim Duncan\", 42); nebula> MATCH (v:player:actor) \\ RETURN v \\ LIMIT 10; +----------------------------------------------------------------------------------------+ | v | +----------------------------------------------------------------------------------------+ | (\"player100\" :actor{age: 42, name: \"Tim Duncan\"} :player{age: 42, name: \"Tim Duncan\"}) | +----------------------------------------------------------------------------------------+ Match vertex properties \u00b6 Note The prerequisite for matching a vertex property is that the tag itself has an index of the corresponding property. Otherwise, you cannot execute the MATCH statement to match the property. You can specify a vertex property with {<prop_name>: <prop_value>} after the tag in a pattern. # The following example uses the name property to match a vertex. nebula> MATCH (v:player{name:\"Tim Duncan\"}) \\ RETURN v; +----------------------------------------------------+ | v | +----------------------------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | +----------------------------------------------------+ The WHERE clause can do the same thing: nebula> MATCH (v:player) \\ WHERE v.player.name == \"Tim Duncan\" \\ RETURN v; +----------------------------------------------------+ | v | +----------------------------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | +----------------------------------------------------+ OpenCypher compatibility In openCypher 9, = is the equality operator. However, in nGQL, == is the equality operator and = is the assignment operator (as in C++ or Java). Match VIDs \u00b6 You can use the VID to match a vertex. The id() function can retrieve the VID of a vertex. nebula> MATCH (v) \\ WHERE id(v) == 'player101' \\ RETURN v; +-----------------------------------------------------+ | v | +-----------------------------------------------------+ | (\"player101\" :player{age: 36, name: \"Tony Parker\"}) | +-----------------------------------------------------+ To match multiple VIDs, use WHERE id(v) IN [vid_list] . nebula> MATCH (v:player { name: 'Tim Duncan' })--(v2) \\ WHERE id(v2) IN [\"player101\", \"player102\"] \\ RETURN v2; +-----------------------------------------------------------+ | v2 | +-----------------------------------------------------------+ | (\"player101\" :player{age: 36, name: \"Tony Parker\"}) | | (\"player101\" :player{age: 36, name: \"Tony Parker\"}) | | (\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"}) | +-----------------------------------------------------------+ Match connected vertices \u00b6 You can use the -- symbol to represent edges of both directions and match vertices connected by these edges. Legacy version compatibility In nGQL 1.x, the -- symbol is used for inline comments. Starting from nGQL 2.x, the -- symbol represents an incoming or outgoing edge. nebula> MATCH (v:player{name:\"Tim Duncan\"})--(v2) \\ RETURN v2.player.name AS Name; +---------------------+ | Name | +---------------------+ | \"Manu Ginobili\" | | \"Manu Ginobili\" | | \"Tiago Splitter\" | ... You can add a > or < to the -- symbol to specify the direction of an edge. In the following example, --> represents an edge that starts from v and points to v2 . To v , this is an outgoing edge, and to v2 this is an incoming edge. nebula> MATCH (v:player{name:\"Tim Duncan\"})-->(v2:player) \\ RETURN v2.player.name AS Name; +-----------------+ | Name | +-----------------+ | \"Manu Ginobili\" | | \"Tony Parker\" | +-----------------+ To query the properties of the target vertices, use the CASE expression. nebula> MATCH (v:player{name:\"Tim Duncan\"})--(v2) \\ RETURN \\ CASE WHEN v2.team.name IS NOT NULL \\ THEN v2.team.name \\ WHEN v2.player.name IS NOT NULL \\ THEN v2.player.name END AS Name; +---------------------+ | Name | +---------------------+ | \"Manu Ginobili\" | | \"Manu Ginobili\" | | \"Spurs\" | | \"Dejounte Murray\" | ... To extend the pattern, you can add more vertices and edges. nebula> MATCH (v:player{name:\"Tim Duncan\"})-->(v2)<--(v3) \\ RETURN v3.player.name AS Name; +---------------------+ | Name | +---------------------+ | \"Dejounte Murray\" | | \"LaMarcus Aldridge\" | | \"Marco Belinelli\" | ... If you do not need to refer to a vertex, you can omit the variable representing it in the parentheses. nebula> MATCH (v:player{name:\"Tim Duncan\"})-->()<--(v3) \\ RETURN v3.player.name AS Name; +---------------------+ | Name | +---------------------+ | \"Dejounte Murray\" | | \"LaMarcus Aldridge\" | | \"Marco Belinelli\" | ... Match paths \u00b6 Connected vertices and edges form a path. You can use a user-defined variable to name a path as follows. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-->(v2) \\ RETURN p; +--------------------------------------------------------------------------------------------------------------------------------------+ | p | +--------------------------------------------------------------------------------------------------------------------------------------+ | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:serve@0 {end_year: 2016, start_year: 1997}]->(\"team204\" :team{name: \"Spurs\"})> | | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player101\" :player{age: 36, name: \"Tony Parker\"})> | | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player125\" :player{age: 41, name: \"Manu Ginobili\"})> | +--------------------------------------------------------------------------------------------------------------------------------------+ OpenCypher compatibility In nGQL, the @ symbol represents the rank of an edge, but openCypher has no such concept. Match edges \u00b6 OpenCypher compatibility In Nebula Graph versions earlier than 3.0.0, the prerequisite for matching a edge is that the edge itself has an index or a certain property of the edge has an index. As of version 3.0.0, there is no need to create an index for matching a edge, but you need to use LIMIT to limit the number of output results and you must specify the direction of the edge. nebula> MATCH ()<-[e]-() \\ RETURN e \\ LIMIT 3; +----------------------------------------------------+ | e | +----------------------------------------------------+ | [:follow \"player101\"->\"player102\" @0 {degree: 90}] | | [:follow \"player103\"->\"player102\" @0 {degree: 70}] | | [:follow \"player135\"->\"player102\" @0 {degree: 80}] | +----------------------------------------------------+ Match edge types \u00b6 Just like vertices, you can specify edge types with :<edge_type> in a pattern. For example: -[e:follow]- . OpenCypher compatibility In Nebula Graph versions earlier than 3.0.0, the prerequisite for matching a edge type is that the edge type itself has an index or a certain property of the edge type has an index. As of version 3.0.0, there is no need to create an index for matching a edge type, but you need to use LIMIT to limit the number of output results and you must specify the direction of the edge. nebula> MATCH ()-[e:follow]->() \\ RETURN e \\ limit 3; +----------------------------------------------------+ | e | +----------------------------------------------------+ | [:follow \"player102\"->\"player100\" @0 {degree: 75}] | | [:follow \"player102\"->\"player101\" @0 {degree: 75}] | | [:follow \"player129\"->\"player116\" @0 {degree: 90}] | +----------------------------------------------------+ # Before you execute the following statement, you must create an index on the edge type itself or a certain property of the edge type nebula> MATCH ()-[e:follow]-() \\ RETURN e; +-----------------------------------------------------+ | e | +-----------------------------------------------------+ | [:follow \"player126\"->\"player116\" @0 {degree: 13}] | | [:follow \"player142\"->\"player117\" @0 {degree: 90}] | | [:follow \"player136\"->\"player117\" @0 {degree: 90}] | | [:follow \"player136\"->\"player148\" @0 {degree: 85}] | \u00b7\u00b7\u00b7 Match edge type properties \u00b6 Note The prerequisite for matching an edge type property is that the edge type itself has an index of the corresponding property. Otherwise, you cannot execute the MATCH statement to match the property. You can specify edge type properties with {<prop_name>: <prop_value>} in a pattern. For example: [e:follow{likeness:95}] . nebula> MATCH (v:player{name:\"Tim Duncan\"})-[e:follow{degree:95}]->(v2) \\ RETURN e; +--------------------------------------------------------+ | e | +--------------------------------------------------------+ | [:follow \"player100\"->\"player101\" @0 {degree: 95}] | | [:follow \"player100\"->\"player125\" @0 {degree: 95}] | +--------------------------------------------------------+ Match multiple edge types \u00b6 The | symbol can help matching multiple edge types. For example: [e:follow|:serve] . The English colon (:) before the first edge type cannot be omitted, but the English colon before the subsequent edge type can be omitted, such as [e:follow|serve] . nebula> MATCH (v:player{name:\"Tim Duncan\"})-[e:follow|:serve]->(v2) \\ RETURN e; +---------------------------------------------------------------------------+ | e | +---------------------------------------------------------------------------+ | [:follow \"player100\"->\"player101\" @0 {degree: 95}] | | [:follow \"player100\"->\"player125\" @0 {degree: 95}] | | [:serve \"player100\"->\"team204\" @0 {end_year: 2016, start_year: 1997}] | +---------------------------------------------------------------------------+ Match multiple edges \u00b6 You can extend a pattern to match multiple edges in a path. nebula> MATCH (v:player{name:\"Tim Duncan\"})-[]->(v2)<-[e:serve]-(v3) \\ RETURN v2, v3; +----------------------------------+-----------------------------------------------------------+ | v2 | v3 | +----------------------------------+-----------------------------------------------------------+ | (\"team204\" :team{name: \"Spurs\"}) | (\"player104\" :player{age: 32, name: \"Marco Belinelli\"}) | | (\"team204\" :team{name: \"Spurs\"}) | (\"player101\" :player{age: 36, name: \"Tony Parker\"}) | | (\"team204\" :team{name: \"Spurs\"}) | (\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"}) | ... Match fixed-length paths \u00b6 You can use the :<edge_type>*<hop> pattern to match a fixed-length path. hop must be a non-negative integer. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[e:follow*2]->(v2) \\ RETURN DISTINCT v2 AS Friends; +-----------------------------------------------------------+ | Friends | +-----------------------------------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | | (\"player125\" :player{age: 41, name: \"Manu Ginobili\"}) | | (\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"}) | +-----------------------------------------------------------+ If hop is 0, the pattern will match the source vertex of the path. nebula> MATCH (v:player{name:\"Tim Duncan\"}) -[*0]-> (v2) \\ RETURN v2; +----------------------------------------------------+ | v2 | +----------------------------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | +----------------------------------------------------+ Note When you conditionally filter on multi-hop edges, such as -[e:follow*2]-> , note that the e is a list of edges instead of a single edge. For example, the following statement is correct from the syntax point of view which may not get your expected query result, because the e is a list without the .degree property. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[e:follow*2]->(v2) \\ WHERE e.degree > 1 \\ RETURN DISTINCT v2 AS Friends; The correct statement is as follows: nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[e:follow*2]->(v2) \\ WHERE ALL(e_ in e WHERE e_.degree > 0) \\ RETURN DISTINCT v2 AS Friends; Further, the following statement is for filtering the properties of the first-hop edge in multi-hop edges: nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[e:follow*2]->(v2) \\ WHERE e[0].degree > 98 \\ RETURN DISTINCT v2 AS Friends; Match variable-length paths \u00b6 You can use the :<edge_type>*[minHop..maxHop] pattern to match variable-length paths. minHop and maxHop are optional and default to 1 and infinity respectively. Note When setting bounds, at least one of minHop and maxHop exists. Caution If maxHop is not set, it may cause the Graph service to OOM, execute this command with caution. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[e:follow*]->(v2) \\ RETURN v2 AS Friends; +-----------------------------------------------------------+ | Friends | +-----------------------------------------------------------+ | (\"player125\" :player{age: 41, name: \"Manu Ginobili\"}) | | (\"player101\" :player{age: 36, name: \"Tony Parker\"}) | ... nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[e:follow*1..3]->(v2) \\ RETURN v2 AS Friends; +-----------------------------------------------------------+ | Friends | +-----------------------------------------------------------+ | (\"player101\" :player{age: 36, name: \"Tony Parker\"}) | | (\"player125\" :player{age: 41, name: \"Manu Ginobili\"}) | | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | ... nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[e:follow*1..]->(v2) \\ RETURN v2 AS Friends; +-----------------------------------------------------------+ | Friends | +-----------------------------------------------------------+ | (\"player125\" :player{age: 41, name: \"Manu Ginobili\"}) | | (\"player101\" :player{age: 36, name: \"Tony Parker\"}) | | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | ... You can use the DISTINCT keyword to aggregate duplicate results. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[e:follow*1..3]->(v2:player) \\ RETURN DISTINCT v2 AS Friends, count(v2); +-----------------------------------------------------------+-----------+ | Friends | count(v2) | +-----------------------------------------------------------+-----------+ | (\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"}) | 1 | | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | 4 | | (\"player101\" :player{age: 36, name: \"Tony Parker\"}) | 3 | | (\"player125\" :player{age: 41, name: \"Manu Ginobili\"}) | 3 | +-----------------------------------------------------------+-----------+ If minHop is 0 , the pattern will match the source vertex of the path. Compared to the preceding statement, the following example uses 0 as the minHop . So in the following result set, \"Tim Duncan\" is counted one more time than it is in the preceding result set because it is the source vertex. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[e:follow*0..3]->(v2:player) \\ RETURN DISTINCT v2 AS Friends, count(v2); +-----------------------------------------------------------+-----------+ | Friends | count(v2) | +-----------------------------------------------------------+-----------+ | (\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"}) | 1 | | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | 5 | | (\"player125\" :player{age: 41, name: \"Manu Ginobili\"}) | 3 | | (\"player101\" :player{age: 36, name: \"Tony Parker\"}) | 3 | +-----------------------------------------------------------+-----------+ Match variable-length paths with multiple edge types \u00b6 You can specify multiple edge types in a fixed-length or variable-length pattern. In this case, hop , minHop , and maxHop take effect on all edge types. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[e:follow|serve*2]->(v2) \\ RETURN DISTINCT v2; +-----------------------------------------------------------+ | v2 | +-----------------------------------------------------------+ | (\"team204\" :team{name: \"Spurs\"}) | | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | | (\"team215\" :team{name: \"Hornets\"}) | | (\"player125\" :player{age: 41, name: \"Manu Ginobili\"}) | | (\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"}) | +-----------------------------------------------------------+ Match multiple patterns \u00b6 You can separate multiple patterns with commas (,). nebula> MATCH (v1:player{name:\"Tim Duncan\"}), (v2:team{name:\"Spurs\"}) \\ RETURN v1,v2; +----------------------------------------------------+----------------------------------+ | v1 | v2 | +----------------------------------------------------+----------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | (\"team204\" :team{name: \"Spurs\"}) | +----------------------------------------------------+----------------------------------+ Retrieve with multiple match \u00b6 Multiple MATCH can be used when different patterns have different filtering criteria and return the rows that exactly match the pattern. nebula> MATCH (m)-[]->(n) WHERE id(m)==\"player100\" \\ MATCH (n)-[]->(l) WHERE id(n)==\"player125\" \\ RETURN id(m),id(n),id(l); +-------------+-------------+-------------+ | id(m) | id(n) | id(l) | +-------------+-------------+-------------+ | \"player100\" | \"player125\" | \"team204\" | | \"player100\" | \"player125\" | \"player100\" | +-------------+-------------+-------------+ Retrieve with optional match \u00b6 See OPTIONAL MATCH \u3002 Performance In Nebula Graph, the performance and resource usage of the MATCH statement have been optimized. But we still recommend to use GO , LOOKUP , | , and FETCH instead of MATCH when high performance is required.","title":"MATCH"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match","text":"The MATCH statement supports searching based on pattern matching. A MATCH statement defines a search pattern and uses it to match data stored in Nebula Graph and to retrieve them in the form defined in the RETURN clause. The examples in this topic use the basketballplayer dataset as the sample dataset.","title":"MATCH"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#syntax","text":"The syntax of MATCH is relatively more flexible compared with that of other query statements such as GO or LOOKUP . But generally, it can be summarized as follows. MATCH <pattern> [<clause_1>] RETURN <output> [<clause_2>]; pattern : For the detailed description of patterns, see Patterns . The MATCH statement supports matching one or multiple patterns. Multiple patterns are separated by commas (,). For example: (a)-[]->(b),(c)-[]->(d) . clause_1 : The WHERE , WITH , UNWIND , and OPTIONAL MATCH clauses are supported, and the MATCH clause can also be used. output : Define the output to be returned. You can rename the output column by using AS . clause_2 : The ORDER BY and LIMIT clauses are supported.","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#precautions","text":"Legacy version compatibility Starting from Nebula Graph version 3.0.0, in order to distinguish the properties of different tags, you need to specify a tag name when querying properties. The original statement RETURN variable_name.property_name is changed to RETURN variable_name.<tag_name>.property_name . Note Currently the match statement cannot find dangling edges. The MATCH statement retrieves data according to the RETURN clause. The path type of the MATCH statement is trail . That is, only vertices can be repeatedly visited in the graph traversal. Edges cannot be repeatedly visited. For details, see path . In a valid MATCH statement, the VID of a specific vertex must be specified with the id() function in the WHERE clause. There is no need to create an index. When traversing all vertices and edges with MATCH , such as MATCH (v) RETURN v LIMIT N , there is no need to create an index, but you need to use LIMIT to limit the number of output results. When traversing all vertices of the specified Tag or edge of the specified Edge Type, such as MATCH (v:player) RETURN v LIMIT N , there is no need to create an index, but you need to use LIMIT to limit the number of output results. In addition to the foregoing, make sure there is at least one index in the MATCH statement. How to create native indexes, see CREATE INDEX .","title":"Precautions"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#using_patterns_in_match_statements","text":"","title":"Using patterns in MATCH statements"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#create_indexes","text":"# The following example creates an index on both the name property of the tag player and the edge type follow. nebula> CREATE TAG INDEX IF NOT EXISTS name ON player(name(20)); nebula> CREATE EDGE INDEX IF NOT EXISTS follow_index on follow(); # The following example rebuilds the index. nebula> REBUILD TAG INDEX name; +------------+ | New Job Id | +------------+ | 121 | +------------+ nebula> REBUILD EDGE INDEX follow_index; +------------+ | New Job Id | +------------+ | 122 | +------------+ # The following example makes sure the index is rebuilt successfully. nebula> SHOW JOB 121; +----------------+---------------------+------------+----------------------------+----------------------------+-------------+ | Job Id(TaskId) | Command(Dest) | Status | Start Time | Stop Time | Error Code | +----------------+---------------------+------------+----------------------------+----------------------------+-------------+ | 121 | \"REBUILD_TAG_INDEX\" | \"FINISHED\" | 2021-05-27T02:18:02.000000 | 2021-05-27T02:18:02.000000 | \"SUCCEEDED\" | | 0 | \"storaged1\" | \"FINISHED\" | 2021-05-27T02:18:02.000000 | 2021-05-27T02:18:02.000000 | \"SUCCEEDED\" | | 1 | \"storaged0\" | \"FINISHED\" | 2021-05-27T02:18:02.000000 | 2021-05-27T02:18:02.000000 | \"SUCCEEDED\" | | 2 | \"storaged2\" | \"FINISHED\" | 2021-05-27T02:18:02.000000 | 2021-05-27T02:18:02.000000 | \"SUCCEEDED\" | +----------------+---------------------+------------+----------------------------+----------------------------+-------------+ nebula> SHOW JOB 122; +----------------+----------------------+------------+----------------------------+----------------------------+-------------+ | Job Id(TaskId) | Command(Dest) | Status | Start Time | Stop Time | Error Code | +----------------+----------------------+------------+----------------------------+----------------------------+-------------+ | 122 | \"REBUILD_EDGE_INDEX\" | \"FINISHED\" | 2021-05-27T02:18:11.000000 | 2021-05-27T02:18:11.000000 | \"SUCCEEDED\" | | 0 | \"storaged1\" | \"FINISHED\" | 2021-05-27T02:18:11.000000 | 2021-05-27T02:18:21.000000 | \"SUCCEEDED\" | | 1 | \"storaged0\" | \"FINISHED\" | 2021-05-27T02:18:11.000000 | 2021-05-27T02:18:21.000000 | \"SUCCEEDED\" | | 2 | \"storaged2\" | \"FINISHED\" | 2021-05-27T02:18:11.000000 | 2021-05-27T02:18:21.000000 | \"SUCCEEDED\" | +----------------+----------------------+------------+----------------------------+----------------------------+-------------+","title":"Create indexes"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_vertices","text":"Legacy version compatibility In Nebula Graph versions earlier than 3.0.0, nGQL does not support MATCH (v) RETURN v . As of version 3.0.0, nGQL support MATCH (v) RETURN v LIMIT n , there is no need to create an index, but you must use LIMIT to limit the number of output results. nGQL still does not support MATCH (v) RETURN v . You can use a user-defined variable in a pair of parentheses to represent a vertex in a pattern. For example: (v) . nebula> MATCH (v) \\ RETURN v \\ LIMIT 3; +-----------------------------------------------------------+ | v | +-----------------------------------------------------------+ | (\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"}) | | (\"player106\" :player{age: 25, name: \"Kyle Anderson\"}) | | (\"player115\" :player{age: 40, name: \"Kobe Bryant\"}) | +-----------------------------------------------------------+","title":"Match vertices"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_tags","text":"Note In Nebula Graph versions earlier than 3.0.0, the prerequisite for matching a tag is that the tag itself has an index or a certain property of the tag has an index. As of version 3.0.0, there is no need to create an index for matching a tag, but you need to use LIMIT to limit the number of output results. You can specify a tag with :<tag_name> after the vertex in a pattern. nebula> MATCH (v:player) \\ RETURN v; +---------------------------------------------------------------+ | v | +---------------------------------------------------------------+ | (\"player105\" :player{age: 31, name: \"Danny Green\"}) | | (\"player109\" :player{age: 34, name: \"Tiago Splitter\"}) | | (\"player111\" :player{age: 38, name: \"David West\"}) | ... To match vertices with multiple tags, use colons (:). nebula> CREATE TAG actor (name string, age int); nebula> INSERT VERTEX actor(name, age) VALUES \"player100\":(\"Tim Duncan\", 42); nebula> MATCH (v:player:actor) \\ RETURN v \\ LIMIT 10; +----------------------------------------------------------------------------------------+ | v | +----------------------------------------------------------------------------------------+ | (\"player100\" :actor{age: 42, name: \"Tim Duncan\"} :player{age: 42, name: \"Tim Duncan\"}) | +----------------------------------------------------------------------------------------+","title":"Match tags"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_vertex_properties","text":"Note The prerequisite for matching a vertex property is that the tag itself has an index of the corresponding property. Otherwise, you cannot execute the MATCH statement to match the property. You can specify a vertex property with {<prop_name>: <prop_value>} after the tag in a pattern. # The following example uses the name property to match a vertex. nebula> MATCH (v:player{name:\"Tim Duncan\"}) \\ RETURN v; +----------------------------------------------------+ | v | +----------------------------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | +----------------------------------------------------+ The WHERE clause can do the same thing: nebula> MATCH (v:player) \\ WHERE v.player.name == \"Tim Duncan\" \\ RETURN v; +----------------------------------------------------+ | v | +----------------------------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | +----------------------------------------------------+ OpenCypher compatibility In openCypher 9, = is the equality operator. However, in nGQL, == is the equality operator and = is the assignment operator (as in C++ or Java).","title":"Match vertex properties"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_vids","text":"You can use the VID to match a vertex. The id() function can retrieve the VID of a vertex. nebula> MATCH (v) \\ WHERE id(v) == 'player101' \\ RETURN v; +-----------------------------------------------------+ | v | +-----------------------------------------------------+ | (\"player101\" :player{age: 36, name: \"Tony Parker\"}) | +-----------------------------------------------------+ To match multiple VIDs, use WHERE id(v) IN [vid_list] . nebula> MATCH (v:player { name: 'Tim Duncan' })--(v2) \\ WHERE id(v2) IN [\"player101\", \"player102\"] \\ RETURN v2; +-----------------------------------------------------------+ | v2 | +-----------------------------------------------------------+ | (\"player101\" :player{age: 36, name: \"Tony Parker\"}) | | (\"player101\" :player{age: 36, name: \"Tony Parker\"}) | | (\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"}) | +-----------------------------------------------------------+","title":"Match VIDs"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_connected_vertices","text":"You can use the -- symbol to represent edges of both directions and match vertices connected by these edges. Legacy version compatibility In nGQL 1.x, the -- symbol is used for inline comments. Starting from nGQL 2.x, the -- symbol represents an incoming or outgoing edge. nebula> MATCH (v:player{name:\"Tim Duncan\"})--(v2) \\ RETURN v2.player.name AS Name; +---------------------+ | Name | +---------------------+ | \"Manu Ginobili\" | | \"Manu Ginobili\" | | \"Tiago Splitter\" | ... You can add a > or < to the -- symbol to specify the direction of an edge. In the following example, --> represents an edge that starts from v and points to v2 . To v , this is an outgoing edge, and to v2 this is an incoming edge. nebula> MATCH (v:player{name:\"Tim Duncan\"})-->(v2:player) \\ RETURN v2.player.name AS Name; +-----------------+ | Name | +-----------------+ | \"Manu Ginobili\" | | \"Tony Parker\" | +-----------------+ To query the properties of the target vertices, use the CASE expression. nebula> MATCH (v:player{name:\"Tim Duncan\"})--(v2) \\ RETURN \\ CASE WHEN v2.team.name IS NOT NULL \\ THEN v2.team.name \\ WHEN v2.player.name IS NOT NULL \\ THEN v2.player.name END AS Name; +---------------------+ | Name | +---------------------+ | \"Manu Ginobili\" | | \"Manu Ginobili\" | | \"Spurs\" | | \"Dejounte Murray\" | ... To extend the pattern, you can add more vertices and edges. nebula> MATCH (v:player{name:\"Tim Duncan\"})-->(v2)<--(v3) \\ RETURN v3.player.name AS Name; +---------------------+ | Name | +---------------------+ | \"Dejounte Murray\" | | \"LaMarcus Aldridge\" | | \"Marco Belinelli\" | ... If you do not need to refer to a vertex, you can omit the variable representing it in the parentheses. nebula> MATCH (v:player{name:\"Tim Duncan\"})-->()<--(v3) \\ RETURN v3.player.name AS Name; +---------------------+ | Name | +---------------------+ | \"Dejounte Murray\" | | \"LaMarcus Aldridge\" | | \"Marco Belinelli\" | ...","title":"Match connected vertices"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_paths","text":"Connected vertices and edges form a path. You can use a user-defined variable to name a path as follows. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-->(v2) \\ RETURN p; +--------------------------------------------------------------------------------------------------------------------------------------+ | p | +--------------------------------------------------------------------------------------------------------------------------------------+ | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:serve@0 {end_year: 2016, start_year: 1997}]->(\"team204\" :team{name: \"Spurs\"})> | | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player101\" :player{age: 36, name: \"Tony Parker\"})> | | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player125\" :player{age: 41, name: \"Manu Ginobili\"})> | +--------------------------------------------------------------------------------------------------------------------------------------+ OpenCypher compatibility In nGQL, the @ symbol represents the rank of an edge, but openCypher has no such concept.","title":"Match paths"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_edges","text":"OpenCypher compatibility In Nebula Graph versions earlier than 3.0.0, the prerequisite for matching a edge is that the edge itself has an index or a certain property of the edge has an index. As of version 3.0.0, there is no need to create an index for matching a edge, but you need to use LIMIT to limit the number of output results and you must specify the direction of the edge. nebula> MATCH ()<-[e]-() \\ RETURN e \\ LIMIT 3; +----------------------------------------------------+ | e | +----------------------------------------------------+ | [:follow \"player101\"->\"player102\" @0 {degree: 90}] | | [:follow \"player103\"->\"player102\" @0 {degree: 70}] | | [:follow \"player135\"->\"player102\" @0 {degree: 80}] | +----------------------------------------------------+","title":"Match edges"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_edge_types","text":"Just like vertices, you can specify edge types with :<edge_type> in a pattern. For example: -[e:follow]- . OpenCypher compatibility In Nebula Graph versions earlier than 3.0.0, the prerequisite for matching a edge type is that the edge type itself has an index or a certain property of the edge type has an index. As of version 3.0.0, there is no need to create an index for matching a edge type, but you need to use LIMIT to limit the number of output results and you must specify the direction of the edge. nebula> MATCH ()-[e:follow]->() \\ RETURN e \\ limit 3; +----------------------------------------------------+ | e | +----------------------------------------------------+ | [:follow \"player102\"->\"player100\" @0 {degree: 75}] | | [:follow \"player102\"->\"player101\" @0 {degree: 75}] | | [:follow \"player129\"->\"player116\" @0 {degree: 90}] | +----------------------------------------------------+ # Before you execute the following statement, you must create an index on the edge type itself or a certain property of the edge type nebula> MATCH ()-[e:follow]-() \\ RETURN e; +-----------------------------------------------------+ | e | +-----------------------------------------------------+ | [:follow \"player126\"->\"player116\" @0 {degree: 13}] | | [:follow \"player142\"->\"player117\" @0 {degree: 90}] | | [:follow \"player136\"->\"player117\" @0 {degree: 90}] | | [:follow \"player136\"->\"player148\" @0 {degree: 85}] | \u00b7\u00b7\u00b7","title":"Match edge types"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_edge_type_properties","text":"Note The prerequisite for matching an edge type property is that the edge type itself has an index of the corresponding property. Otherwise, you cannot execute the MATCH statement to match the property. You can specify edge type properties with {<prop_name>: <prop_value>} in a pattern. For example: [e:follow{likeness:95}] . nebula> MATCH (v:player{name:\"Tim Duncan\"})-[e:follow{degree:95}]->(v2) \\ RETURN e; +--------------------------------------------------------+ | e | +--------------------------------------------------------+ | [:follow \"player100\"->\"player101\" @0 {degree: 95}] | | [:follow \"player100\"->\"player125\" @0 {degree: 95}] | +--------------------------------------------------------+","title":"Match edge type properties"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_multiple_edge_types","text":"The | symbol can help matching multiple edge types. For example: [e:follow|:serve] . The English colon (:) before the first edge type cannot be omitted, but the English colon before the subsequent edge type can be omitted, such as [e:follow|serve] . nebula> MATCH (v:player{name:\"Tim Duncan\"})-[e:follow|:serve]->(v2) \\ RETURN e; +---------------------------------------------------------------------------+ | e | +---------------------------------------------------------------------------+ | [:follow \"player100\"->\"player101\" @0 {degree: 95}] | | [:follow \"player100\"->\"player125\" @0 {degree: 95}] | | [:serve \"player100\"->\"team204\" @0 {end_year: 2016, start_year: 1997}] | +---------------------------------------------------------------------------+","title":"Match multiple edge types"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_multiple_edges","text":"You can extend a pattern to match multiple edges in a path. nebula> MATCH (v:player{name:\"Tim Duncan\"})-[]->(v2)<-[e:serve]-(v3) \\ RETURN v2, v3; +----------------------------------+-----------------------------------------------------------+ | v2 | v3 | +----------------------------------+-----------------------------------------------------------+ | (\"team204\" :team{name: \"Spurs\"}) | (\"player104\" :player{age: 32, name: \"Marco Belinelli\"}) | | (\"team204\" :team{name: \"Spurs\"}) | (\"player101\" :player{age: 36, name: \"Tony Parker\"}) | | (\"team204\" :team{name: \"Spurs\"}) | (\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"}) | ...","title":"Match multiple edges"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_fixed-length_paths","text":"You can use the :<edge_type>*<hop> pattern to match a fixed-length path. hop must be a non-negative integer. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[e:follow*2]->(v2) \\ RETURN DISTINCT v2 AS Friends; +-----------------------------------------------------------+ | Friends | +-----------------------------------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | | (\"player125\" :player{age: 41, name: \"Manu Ginobili\"}) | | (\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"}) | +-----------------------------------------------------------+ If hop is 0, the pattern will match the source vertex of the path. nebula> MATCH (v:player{name:\"Tim Duncan\"}) -[*0]-> (v2) \\ RETURN v2; +----------------------------------------------------+ | v2 | +----------------------------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | +----------------------------------------------------+ Note When you conditionally filter on multi-hop edges, such as -[e:follow*2]-> , note that the e is a list of edges instead of a single edge. For example, the following statement is correct from the syntax point of view which may not get your expected query result, because the e is a list without the .degree property. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[e:follow*2]->(v2) \\ WHERE e.degree > 1 \\ RETURN DISTINCT v2 AS Friends; The correct statement is as follows: nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[e:follow*2]->(v2) \\ WHERE ALL(e_ in e WHERE e_.degree > 0) \\ RETURN DISTINCT v2 AS Friends; Further, the following statement is for filtering the properties of the first-hop edge in multi-hop edges: nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[e:follow*2]->(v2) \\ WHERE e[0].degree > 98 \\ RETURN DISTINCT v2 AS Friends;","title":"Match fixed-length paths"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_variable-length_paths","text":"You can use the :<edge_type>*[minHop..maxHop] pattern to match variable-length paths. minHop and maxHop are optional and default to 1 and infinity respectively. Note When setting bounds, at least one of minHop and maxHop exists. Caution If maxHop is not set, it may cause the Graph service to OOM, execute this command with caution. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[e:follow*]->(v2) \\ RETURN v2 AS Friends; +-----------------------------------------------------------+ | Friends | +-----------------------------------------------------------+ | (\"player125\" :player{age: 41, name: \"Manu Ginobili\"}) | | (\"player101\" :player{age: 36, name: \"Tony Parker\"}) | ... nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[e:follow*1..3]->(v2) \\ RETURN v2 AS Friends; +-----------------------------------------------------------+ | Friends | +-----------------------------------------------------------+ | (\"player101\" :player{age: 36, name: \"Tony Parker\"}) | | (\"player125\" :player{age: 41, name: \"Manu Ginobili\"}) | | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | ... nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[e:follow*1..]->(v2) \\ RETURN v2 AS Friends; +-----------------------------------------------------------+ | Friends | +-----------------------------------------------------------+ | (\"player125\" :player{age: 41, name: \"Manu Ginobili\"}) | | (\"player101\" :player{age: 36, name: \"Tony Parker\"}) | | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | ... You can use the DISTINCT keyword to aggregate duplicate results. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[e:follow*1..3]->(v2:player) \\ RETURN DISTINCT v2 AS Friends, count(v2); +-----------------------------------------------------------+-----------+ | Friends | count(v2) | +-----------------------------------------------------------+-----------+ | (\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"}) | 1 | | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | 4 | | (\"player101\" :player{age: 36, name: \"Tony Parker\"}) | 3 | | (\"player125\" :player{age: 41, name: \"Manu Ginobili\"}) | 3 | +-----------------------------------------------------------+-----------+ If minHop is 0 , the pattern will match the source vertex of the path. Compared to the preceding statement, the following example uses 0 as the minHop . So in the following result set, \"Tim Duncan\" is counted one more time than it is in the preceding result set because it is the source vertex. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[e:follow*0..3]->(v2:player) \\ RETURN DISTINCT v2 AS Friends, count(v2); +-----------------------------------------------------------+-----------+ | Friends | count(v2) | +-----------------------------------------------------------+-----------+ | (\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"}) | 1 | | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | 5 | | (\"player125\" :player{age: 41, name: \"Manu Ginobili\"}) | 3 | | (\"player101\" :player{age: 36, name: \"Tony Parker\"}) | 3 | +-----------------------------------------------------------+-----------+","title":"Match variable-length paths"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_variable-length_paths_with_multiple_edge_types","text":"You can specify multiple edge types in a fixed-length or variable-length pattern. In this case, hop , minHop , and maxHop take effect on all edge types. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[e:follow|serve*2]->(v2) \\ RETURN DISTINCT v2; +-----------------------------------------------------------+ | v2 | +-----------------------------------------------------------+ | (\"team204\" :team{name: \"Spurs\"}) | | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | | (\"team215\" :team{name: \"Hornets\"}) | | (\"player125\" :player{age: 41, name: \"Manu Ginobili\"}) | | (\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"}) | +-----------------------------------------------------------+","title":"Match variable-length paths with multiple edge types"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_multiple_patterns","text":"You can separate multiple patterns with commas (,). nebula> MATCH (v1:player{name:\"Tim Duncan\"}), (v2:team{name:\"Spurs\"}) \\ RETURN v1,v2; +----------------------------------------------------+----------------------------------+ | v1 | v2 | +----------------------------------------------------+----------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | (\"team204\" :team{name: \"Spurs\"}) | +----------------------------------------------------+----------------------------------+","title":"Match multiple patterns"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#retrieve_with_multiple_match","text":"Multiple MATCH can be used when different patterns have different filtering criteria and return the rows that exactly match the pattern. nebula> MATCH (m)-[]->(n) WHERE id(m)==\"player100\" \\ MATCH (n)-[]->(l) WHERE id(n)==\"player125\" \\ RETURN id(m),id(n),id(l); +-------------+-------------+-------------+ | id(m) | id(n) | id(l) | +-------------+-------------+-------------+ | \"player100\" | \"player125\" | \"team204\" | | \"player100\" | \"player125\" | \"player100\" | +-------------+-------------+-------------+","title":"Retrieve with multiple match"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#retrieve_with_optional_match","text":"See OPTIONAL MATCH \u3002 Performance In Nebula Graph, the performance and resource usage of the MATCH statement have been optimized. But we still recommend to use GO , LOOKUP , | , and FETCH instead of MATCH when high performance is required.","title":"Retrieve with optional match"},{"location":"3.ngql-guide/7.general-query-statements/3.go/","text":"GO \u00b6 GO traverses in a graph with specified filters and returns results. OpenCypher compatibility \u00b6 This topic applies to native nGQL only. Syntax \u00b6 GO [[<M> TO] <N> STEPS ] FROM <vertex_list> OVER <edge_type_list> [{REVERSELY | BIDIRECT}] [ WHERE <conditions> ] YIELD [DISTINCT] <return_list> [{SAMPLE <sample_list> | <limit_by_list_clause>}] [| GROUP BY {col_name | expr | position} YIELD <col_name>] [| ORDER BY <expression> [{ASC | DESC}]] [| LIMIT [<offset>,] <number_rows>]; <vertex_list> ::= <vid> [, <vid> ...] <edge_type_list> ::= edge_type [, edge_type ...] | * <return_list> ::= <col_name> [AS <col_alias>] [, <col_name> [AS <col_alias>] ...] <N> STEPS : specifies the hop number. If not specified, the default value for N is one . When N is zero , Nebula Graph does not traverse any edges and returns nothing. Note The path type of the GO statement is walk , which means both vertices and edges can be repeatedly visited in graph traversal. For more information, see Path . M TO N STEPS : traverses from M to N hops. When M is zero , the output is the same as that of M is one . That is, the output of GO 0 TO 2 and GO 1 TO 2 are the same. <vertex_list> : represents a list of vertex IDs separated by commas, or a special place holder $-.id . For more information, see Pipe . <edge_type_list> : represents a list of edge types which the traversal can go through. REVERSELY | BIDIRECT : defines the direction of the query. By default, the GO statement searches for outgoing edges of <vertex_list> . If REVERSELY is set, GO searches for incoming edges. If BIDIRECT is set, GO searches for edges of both directions. WHERE <expression> : specifies the traversal filters. You can use the WHERE clause for the source vertices, the edges, and the destination vertices. You can use it together with AND , OR , NOT , and XOR . For more information, see WHERE . Note There are some restrictions for the WHERE clause when you traverse along with multiple edge types. For example, WHERE edge1.prop1 > edge2.prop2 is not supported. YIELD [DISTINCT] <return_list> : defines the output to be returned. It is recommended to use the Schema function to fill in <return_list> . src(edge) , dst(edge) , type(edge) ) , rank(edge) , etc., are currently supported, while nested functions are not. For more information, see YIELD . SAMPLE <sample_list> : takes samples from the result set. For more information, see SAMPLE . <limit_by_list_clause> : limits the number of outputs during the traversal process. For more information, see LIMIT . GROUP BY : groups the output into subgroups based on the value of the specified property. For more information, see GROUP BY . After grouping, you need to use YIELD again to define the output that needs to be returned. ORDER BY : sorts outputs with specified orders. For more information, see ORDER BY . Note When the sorting method is not specified, the output orders can be different for the same query. LIMIT [<offset>,] <number_rows>] : limits the number of rows of the output. For more information, see LIMIT . Examples \u00b6 # The following example returns the teams that player 102 serves. nebula> GO FROM \"player102\" OVER serve YIELD dst(edge); +-----------+ | dst(EDGE) | +-----------+ | \"team203\" | | \"team204\" | +-----------+ # The following example returns the friends of player 102 with 2 hops. nebula> GO 2 STEPS FROM \"player102\" OVER follow YIELD dst(edge); +-------------+ | dst(EDGE) | +-------------+ | \"player101\" | | \"player125\" | | \"player100\" | | \"player102\" | | \"player125\" | +-------------+ # The following example adds a filter for the traversal. nebula> GO FROM \"player100\", \"player102\" OVER serve \\ WHERE properties(edge).start_year > 1995 \\ YIELD DISTINCT properties($$).name AS team_name, properties(edge).start_year AS start_year, properties($^).name AS player_name; +-----------------+------------+---------------------+ | team_name | start_year | player_name | +-----------------+------------+---------------------+ | \"Spurs\" | 1997 | \"Tim Duncan\" | | \"Trail Blazers\" | 2006 | \"LaMarcus Aldridge\" | | \"Spurs\" | 2015 | \"LaMarcus Aldridge\" | +-----------------+------------+---------------------+ # The following example traverses along with multiple edge types. If there is no value for a property, the output is UNKNOWN_PROP. nebula> GO FROM \"player100\" OVER follow, serve \\ YIELD properties(edge).degree, properties(edge).start_year; +-------------------------+-----------------------------+ | properties(EDGE).degree | properties(EDGE).start_year | +-------------------------+-----------------------------+ | 95 | UNKNOWN_PROP | | 95 | UNKNOWN_PROP | | UNKNOWN_PROP | 1997 | +-------------------------+-----------------------------+ # The following example returns the neighbor vertices in the incoming direction of player 100. nebula> GO FROM \"player100\" OVER follow REVERSELY \\ YIELD src(edge) AS destination; +-------------+ | destination | +-------------+ | \"player101\" | | \"player102\" | ... # This MATCH query shares the same semantics with the preceding GO query. nebula> MATCH (v)<-[e:follow]- (v2) WHERE id(v) == 'player100' \\ RETURN id(v2) AS destination; +-------------+ | destination | +-------------+ | \"player101\" | | \"player102\" | +-------------+ ... # The following example retrieves the friends of player 100 and the teams that they serve. nebula> GO FROM \"player100\" OVER follow REVERSELY \\ YIELD src(edge) AS id | \\ GO FROM $-.id OVER serve \\ WHERE properties($^).age > 20 \\ YIELD properties($^).name AS FriendOf, properties($$).name AS Team; +---------------------+-----------------+ | FriendOf | Team | +---------------------+-----------------+ | \"Boris Diaw\" | \"Spurs\" | | \"Boris Diaw\" | \"Jazz\" | | \"Boris Diaw\" | \"Suns\" | ... # This MATCH query shares the same semantics with the preceding GO query. nebula> MATCH (v)<-[e:follow]- (v2)-[e2:serve]->(v3) \\ WHERE id(v) == 'player100' \\ RETURN v2.player.name AS FriendOf, v3.team.name AS Team; +---------------------+-----------------+ | FriendOf | Team | +---------------------+-----------------+ | \"Boris Diaw\" | \"Spurs\" | | \"Boris Diaw\" | \"Jazz\" | | \"Boris Diaw\" | \"Suns\" | ... # The following example retrieves the friends of player 100 within 1 or 2 hops. nebula> GO 1 TO 2 STEPS FROM \"player100\" OVER follow \\ YIELD dst(edge) AS destination; +-------------+ | destination | +-------------+ | \"player101\" | | \"player125\" | ... # This MATCH query shares the same semantics with the preceding GO query. nebula> MATCH (v) -[e:follow*1..2]->(v2) \\ WHERE id(v) == \"player100\" \\ RETURN id(v2) AS destination; +-------------+ | destination | +-------------+ | \"player100\" | | \"player102\" | ... # The following example the outputs according to age. nebula> GO 2 STEPS FROM \"player100\" OVER follow \\ YIELD src(edge) AS src, dst(edge) AS dst, properties($$).age AS age \\ | GROUP BY $-.dst \\ YIELD $-.dst AS dst, collect_set($-.src) AS src, collect($-.age) AS age; +-------------+----------------------------+----------+ | dst | src | age | +-------------+----------------------------+----------+ | \"player125\" | [\"player101\"] | [41] | | \"player100\" | [\"player125\", \"player101\"] | [42, 42] | | \"player102\" | [\"player101\"] | [33] | +-------------+----------------------------+----------+ # The following example groups the outputs and restricts the number of rows of the outputs. nebula> $a = GO FROM \"player100\" OVER follow YIELD src(edge) AS src, dst(edge) AS dst; \\ GO 2 STEPS FROM $a.dst OVER follow \\ YIELD $a.src AS src, $a.dst, src(edge), dst(edge) \\ | ORDER BY $-.src | OFFSET 1 LIMIT 2; +-------------+-------------+-------------+-------------+ | src | $a.dst | follow._src | follow._dst | +-------------+-------------+-------------+-------------+ | \"player100\" | \"player125\" | \"player100\" | \"player101\" | | \"player100\" | \"player101\" | \"player100\" | \"player125\" | +-------------+-------------+-------------+-------------+ # The following example determines if $$.player.name IS NOT EMPTY. nebula> GO FROM \"player100\" OVER follow WHERE properties($$).name IS NOT EMPTY YIELD dst(edge); +-------------+ | follow._dst | +-------------+ | \"player125\" | | \"player101\" | +-------------+","title":"GO"},{"location":"3.ngql-guide/7.general-query-statements/3.go/#go","text":"GO traverses in a graph with specified filters and returns results.","title":"GO"},{"location":"3.ngql-guide/7.general-query-statements/3.go/#opencypher_compatibility","text":"This topic applies to native nGQL only.","title":"OpenCypher compatibility"},{"location":"3.ngql-guide/7.general-query-statements/3.go/#syntax","text":"GO [[<M> TO] <N> STEPS ] FROM <vertex_list> OVER <edge_type_list> [{REVERSELY | BIDIRECT}] [ WHERE <conditions> ] YIELD [DISTINCT] <return_list> [{SAMPLE <sample_list> | <limit_by_list_clause>}] [| GROUP BY {col_name | expr | position} YIELD <col_name>] [| ORDER BY <expression> [{ASC | DESC}]] [| LIMIT [<offset>,] <number_rows>]; <vertex_list> ::= <vid> [, <vid> ...] <edge_type_list> ::= edge_type [, edge_type ...] | * <return_list> ::= <col_name> [AS <col_alias>] [, <col_name> [AS <col_alias>] ...] <N> STEPS : specifies the hop number. If not specified, the default value for N is one . When N is zero , Nebula Graph does not traverse any edges and returns nothing. Note The path type of the GO statement is walk , which means both vertices and edges can be repeatedly visited in graph traversal. For more information, see Path . M TO N STEPS : traverses from M to N hops. When M is zero , the output is the same as that of M is one . That is, the output of GO 0 TO 2 and GO 1 TO 2 are the same. <vertex_list> : represents a list of vertex IDs separated by commas, or a special place holder $-.id . For more information, see Pipe . <edge_type_list> : represents a list of edge types which the traversal can go through. REVERSELY | BIDIRECT : defines the direction of the query. By default, the GO statement searches for outgoing edges of <vertex_list> . If REVERSELY is set, GO searches for incoming edges. If BIDIRECT is set, GO searches for edges of both directions. WHERE <expression> : specifies the traversal filters. You can use the WHERE clause for the source vertices, the edges, and the destination vertices. You can use it together with AND , OR , NOT , and XOR . For more information, see WHERE . Note There are some restrictions for the WHERE clause when you traverse along with multiple edge types. For example, WHERE edge1.prop1 > edge2.prop2 is not supported. YIELD [DISTINCT] <return_list> : defines the output to be returned. It is recommended to use the Schema function to fill in <return_list> . src(edge) , dst(edge) , type(edge) ) , rank(edge) , etc., are currently supported, while nested functions are not. For more information, see YIELD . SAMPLE <sample_list> : takes samples from the result set. For more information, see SAMPLE . <limit_by_list_clause> : limits the number of outputs during the traversal process. For more information, see LIMIT . GROUP BY : groups the output into subgroups based on the value of the specified property. For more information, see GROUP BY . After grouping, you need to use YIELD again to define the output that needs to be returned. ORDER BY : sorts outputs with specified orders. For more information, see ORDER BY . Note When the sorting method is not specified, the output orders can be different for the same query. LIMIT [<offset>,] <number_rows>] : limits the number of rows of the output. For more information, see LIMIT .","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/3.go/#examples","text":"# The following example returns the teams that player 102 serves. nebula> GO FROM \"player102\" OVER serve YIELD dst(edge); +-----------+ | dst(EDGE) | +-----------+ | \"team203\" | | \"team204\" | +-----------+ # The following example returns the friends of player 102 with 2 hops. nebula> GO 2 STEPS FROM \"player102\" OVER follow YIELD dst(edge); +-------------+ | dst(EDGE) | +-------------+ | \"player101\" | | \"player125\" | | \"player100\" | | \"player102\" | | \"player125\" | +-------------+ # The following example adds a filter for the traversal. nebula> GO FROM \"player100\", \"player102\" OVER serve \\ WHERE properties(edge).start_year > 1995 \\ YIELD DISTINCT properties($$).name AS team_name, properties(edge).start_year AS start_year, properties($^).name AS player_name; +-----------------+------------+---------------------+ | team_name | start_year | player_name | +-----------------+------------+---------------------+ | \"Spurs\" | 1997 | \"Tim Duncan\" | | \"Trail Blazers\" | 2006 | \"LaMarcus Aldridge\" | | \"Spurs\" | 2015 | \"LaMarcus Aldridge\" | +-----------------+------------+---------------------+ # The following example traverses along with multiple edge types. If there is no value for a property, the output is UNKNOWN_PROP. nebula> GO FROM \"player100\" OVER follow, serve \\ YIELD properties(edge).degree, properties(edge).start_year; +-------------------------+-----------------------------+ | properties(EDGE).degree | properties(EDGE).start_year | +-------------------------+-----------------------------+ | 95 | UNKNOWN_PROP | | 95 | UNKNOWN_PROP | | UNKNOWN_PROP | 1997 | +-------------------------+-----------------------------+ # The following example returns the neighbor vertices in the incoming direction of player 100. nebula> GO FROM \"player100\" OVER follow REVERSELY \\ YIELD src(edge) AS destination; +-------------+ | destination | +-------------+ | \"player101\" | | \"player102\" | ... # This MATCH query shares the same semantics with the preceding GO query. nebula> MATCH (v)<-[e:follow]- (v2) WHERE id(v) == 'player100' \\ RETURN id(v2) AS destination; +-------------+ | destination | +-------------+ | \"player101\" | | \"player102\" | +-------------+ ... # The following example retrieves the friends of player 100 and the teams that they serve. nebula> GO FROM \"player100\" OVER follow REVERSELY \\ YIELD src(edge) AS id | \\ GO FROM $-.id OVER serve \\ WHERE properties($^).age > 20 \\ YIELD properties($^).name AS FriendOf, properties($$).name AS Team; +---------------------+-----------------+ | FriendOf | Team | +---------------------+-----------------+ | \"Boris Diaw\" | \"Spurs\" | | \"Boris Diaw\" | \"Jazz\" | | \"Boris Diaw\" | \"Suns\" | ... # This MATCH query shares the same semantics with the preceding GO query. nebula> MATCH (v)<-[e:follow]- (v2)-[e2:serve]->(v3) \\ WHERE id(v) == 'player100' \\ RETURN v2.player.name AS FriendOf, v3.team.name AS Team; +---------------------+-----------------+ | FriendOf | Team | +---------------------+-----------------+ | \"Boris Diaw\" | \"Spurs\" | | \"Boris Diaw\" | \"Jazz\" | | \"Boris Diaw\" | \"Suns\" | ... # The following example retrieves the friends of player 100 within 1 or 2 hops. nebula> GO 1 TO 2 STEPS FROM \"player100\" OVER follow \\ YIELD dst(edge) AS destination; +-------------+ | destination | +-------------+ | \"player101\" | | \"player125\" | ... # This MATCH query shares the same semantics with the preceding GO query. nebula> MATCH (v) -[e:follow*1..2]->(v2) \\ WHERE id(v) == \"player100\" \\ RETURN id(v2) AS destination; +-------------+ | destination | +-------------+ | \"player100\" | | \"player102\" | ... # The following example the outputs according to age. nebula> GO 2 STEPS FROM \"player100\" OVER follow \\ YIELD src(edge) AS src, dst(edge) AS dst, properties($$).age AS age \\ | GROUP BY $-.dst \\ YIELD $-.dst AS dst, collect_set($-.src) AS src, collect($-.age) AS age; +-------------+----------------------------+----------+ | dst | src | age | +-------------+----------------------------+----------+ | \"player125\" | [\"player101\"] | [41] | | \"player100\" | [\"player125\", \"player101\"] | [42, 42] | | \"player102\" | [\"player101\"] | [33] | +-------------+----------------------------+----------+ # The following example groups the outputs and restricts the number of rows of the outputs. nebula> $a = GO FROM \"player100\" OVER follow YIELD src(edge) AS src, dst(edge) AS dst; \\ GO 2 STEPS FROM $a.dst OVER follow \\ YIELD $a.src AS src, $a.dst, src(edge), dst(edge) \\ | ORDER BY $-.src | OFFSET 1 LIMIT 2; +-------------+-------------+-------------+-------------+ | src | $a.dst | follow._src | follow._dst | +-------------+-------------+-------------+-------------+ | \"player100\" | \"player125\" | \"player100\" | \"player101\" | | \"player100\" | \"player101\" | \"player100\" | \"player125\" | +-------------+-------------+-------------+-------------+ # The following example determines if $$.player.name IS NOT EMPTY. nebula> GO FROM \"player100\" OVER follow WHERE properties($$).name IS NOT EMPTY YIELD dst(edge); +-------------+ | follow._dst | +-------------+ | \"player125\" | | \"player101\" | +-------------+","title":"Examples"},{"location":"3.ngql-guide/7.general-query-statements/4.fetch/","text":"FETCH \u00b6 The FETCH statement retrieves the properties of the specified vertices or edges. OpenCypher Compatibility \u00b6 This topic applies to native nGQL only. Fetch vertex properties \u00b6 Syntax \u00b6 FETCH PROP ON {<tag_name>[, tag_name ...] | *} <vid> [, vid ...] YIELD <return_list> [AS <alias>]; Parameter Description tag_name The name of the tag. * Represents all the tags in the current graph space. vid The vertex ID. YIELD Define the output to be returned. For details, see YIELD . AS Set an alias. Fetch vertex properties by one tag \u00b6 Specify a tag in the FETCH statement to fetch the vertex properties by that tag. nebula> FETCH PROP ON player \"player100\" YIELD properties(vertex); +-------------------------------+ | properties(VERTEX) | +-------------------------------+ | {age: 42, name: \"Tim Duncan\"} | +-------------------------------+ Fetch specific properties of a vertex \u00b6 Use a YIELD clause to specify the properties to be returned. nebula> FETCH PROP ON player \"player100\" \\ YIELD properties(vertex).name AS name; +--------------+ | name | +--------------+ | \"Tim Duncan\" | +--------------+ Fetch properties of multiple vertices \u00b6 Specify multiple VIDs (vertex IDs) to fetch properties of multiple vertices. Separate the VIDs with commas. nebula> FETCH PROP ON player \"player101\", \"player102\", \"player103\" YIELD properties(vertex); +--------------------------------------+ | properties(VERTEX) | +--------------------------------------+ | {age: 33, name: \"LaMarcus Aldridge\"} | | {age: 40, name: \"Tony Parker\"} | | {age: 32, name: \"Rudy Gay\"} | +--------------------------------------+ Fetch vertex properties by multiple tags \u00b6 Specify multiple tags in the FETCH statement to fetch the vertex properties by the tags. Separate the tags with commas. # The following example creates a new tag t1. nebula> CREATE TAG IF NOT EXISTS t1(a string, b int); # The following example attaches t1 to the vertex \"player100\". nebula> INSERT VERTEX t1(a, b) VALUES \"player100\":(\"Hello\", 100); # The following example fetches the properties of vertex \"player100\" by the tags player and t1. nebula> FETCH PROP ON player, t1 \"player100\" YIELD vertex AS v; +----------------------------------------------------------------------------+ | v | +----------------------------------------------------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"} :t1{a: \"Hello\", b: 100}) | +----------------------------------------------------------------------------+ You can combine multiple tags with multiple VIDs in a FETCH statement. nebula> FETCH PROP ON player, t1 \"player100\", \"player103\" YIELD vertex AS v; +----------------------------------------------------------------------------+ | v | +----------------------------------------------------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"} :t1{a: \"Hello\", b: 100}) | | (\"player103\" :player{age: 32, name: \"Rudy Gay\"}) | +----------------------------------------------------------------------------+ Fetch vertex properties by all tags \u00b6 Set an asterisk symbol * to fetch properties by all tags in the current graph space. nebula> FETCH PROP ON * \"player100\", \"player106\", \"team200\" YIELD vertex AS v; +----------------------------------------------------------------------------+ | v | +----------------------------------------------------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"} :t1{a: \"Hello\", b: 100}) | | (\"player106\" :player{age: 25, name: \"Kyle Anderson\"}) | | (\"team200\" :team{name: \"Warriors\"}) | +----------------------------------------------------------------------------+ Fetch edge properties \u00b6 Syntax \u00b6 FETCH PROP ON <edge_type> <src_vid> -> <dst_vid>[@<rank>] [, <src_vid> -> <dst_vid> ...] YIELD <output>; Parameter Description edge_type The name of the edge type. src_vid The VID of the source vertex. It specifies the start of an edge. dst_vid The VID of the destination vertex. It specifies the end of an edge. rank The rank of the edge. It is optional and defaults to 0 . It distinguishes an edge from other edges with the same edge type, source vertex, destination vertex, and rank. YIELD Define the output to be returned. For details, see YIELD . Fetch all properties of an edge \u00b6 The following statement fetches all the properties of the serve edge that connects vertex \"player100\" and vertex \"team204\" . nebula> FETCH PROP ON serve \"player100\" -> \"team204\" YIELD properties(edge); +------------------------------------+ | properties(EDGE) | +------------------------------------+ | {end_year: 2016, start_year: 1997} | +------------------------------------+ Fetch specific properties of an edge \u00b6 Use a YIELD clause to fetch specific properties of an edge. nebula> FETCH PROP ON serve \"player100\" -> \"team204\" \\ YIELD properties(edge).start_year; +-----------------------------+ | properties(EDGE).start_year | +-----------------------------+ | 1997 | +-----------------------------+ Fetch properties of multiple edges \u00b6 Specify multiple edge patterns ( <src_vid> -> <dst_vid>[@<rank>] ) to fetch properties of multiple edges. Separate the edge patterns with commas. nebula> FETCH PROP ON serve \"player100\" -> \"team204\", \"player133\" -> \"team202\" YIELD edge AS e; +-----------------------------------------------------------------------+ | e | +-----------------------------------------------------------------------+ | [:serve \"player100\"->\"team204\" @0 {end_year: 2016, start_year: 1997}] | | [:serve \"player133\"->\"team202\" @0 {end_year: 2011, start_year: 2002}] | +-----------------------------------------------------------------------+ Fetch properties based on edge rank \u00b6 If there are multiple edges with the same edge type, source vertex, and destination vertex, you can specify the rank to fetch the properties on the correct edge. # The following example inserts edges with different ranks and property values. nebula> insert edge serve(start_year,end_year) \\ values \"player100\"->\"team204\"@1:(1998, 2017); nebula> insert edge serve(start_year,end_year) \\ values \"player100\"->\"team204\"@2:(1990, 2018); # By default, the FETCH statement returns the edge whose rank is 0. nebula> FETCH PROP ON serve \"player100\" -> \"team204\" YIELD edge AS e; +-----------------------------------------------------------------------+ | e | +-----------------------------------------------------------------------+ | [:serve \"player100\"->\"team204\" @0 {end_year: 2016, start_year: 1997}] | +-----------------------------------------------------------------------+ # To fetch on an edge whose rank is not 0, set its rank in the FETCH statement. nebula> FETCH PROP ON serve \"player100\" -> \"team204\"@1 YIELD edge AS e; +-----------------------------------------------------------------------+ | e | +-----------------------------------------------------------------------+ | [:serve \"player100\"->\"team204\" @1 {end_year: 2017, start_year: 1998}] | +-----------------------------------------------------------------------+ Use FETCH in composite queries \u00b6 A common way to use FETCH is to combine it with native nGQL such as GO . The following statement returns the degree values of the follow edges that start from vertex \"player101\" . nebula> GO FROM \"player101\" OVER follow \\ YIELD src(edge) AS s, dst(edge) AS d \\ | FETCH PROP ON follow $-.s -> $-.d \\ YIELD properties(edge).degree; +-------------------------+ | properties(EDGE).degree | +-------------------------+ | 95 | | 90 | | 95 | +-------------------------+ Or you can use user-defined variables to construct similar queries. nebula> $var = GO FROM \"player101\" OVER follow \\ YIELD src(edge) AS s, dst(edge) AS d; \\ FETCH PROP ON follow $var.s -> $var.d \\ YIELD properties(edge).degree; +-------------------------+ | properties(EDGE).degree | +-------------------------+ | 95 | | 90 | | 95 | +-------------------------+ For more information about composite queries, see Composite queries (clause structure) .","title":"FETCH"},{"location":"3.ngql-guide/7.general-query-statements/4.fetch/#fetch","text":"The FETCH statement retrieves the properties of the specified vertices or edges.","title":"FETCH"},{"location":"3.ngql-guide/7.general-query-statements/4.fetch/#opencypher_compatibility","text":"This topic applies to native nGQL only.","title":"OpenCypher Compatibility"},{"location":"3.ngql-guide/7.general-query-statements/4.fetch/#fetch_vertex_properties","text":"","title":"Fetch vertex properties"},{"location":"3.ngql-guide/7.general-query-statements/4.fetch/#syntax","text":"FETCH PROP ON {<tag_name>[, tag_name ...] | *} <vid> [, vid ...] YIELD <return_list> [AS <alias>]; Parameter Description tag_name The name of the tag. * Represents all the tags in the current graph space. vid The vertex ID. YIELD Define the output to be returned. For details, see YIELD . AS Set an alias.","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/4.fetch/#fetch_vertex_properties_by_one_tag","text":"Specify a tag in the FETCH statement to fetch the vertex properties by that tag. nebula> FETCH PROP ON player \"player100\" YIELD properties(vertex); +-------------------------------+ | properties(VERTEX) | +-------------------------------+ | {age: 42, name: \"Tim Duncan\"} | +-------------------------------+","title":"Fetch vertex properties by one tag"},{"location":"3.ngql-guide/7.general-query-statements/4.fetch/#fetch_specific_properties_of_a_vertex","text":"Use a YIELD clause to specify the properties to be returned. nebula> FETCH PROP ON player \"player100\" \\ YIELD properties(vertex).name AS name; +--------------+ | name | +--------------+ | \"Tim Duncan\" | +--------------+","title":"Fetch specific properties of a vertex"},{"location":"3.ngql-guide/7.general-query-statements/4.fetch/#fetch_properties_of_multiple_vertices","text":"Specify multiple VIDs (vertex IDs) to fetch properties of multiple vertices. Separate the VIDs with commas. nebula> FETCH PROP ON player \"player101\", \"player102\", \"player103\" YIELD properties(vertex); +--------------------------------------+ | properties(VERTEX) | +--------------------------------------+ | {age: 33, name: \"LaMarcus Aldridge\"} | | {age: 40, name: \"Tony Parker\"} | | {age: 32, name: \"Rudy Gay\"} | +--------------------------------------+","title":"Fetch properties of multiple vertices"},{"location":"3.ngql-guide/7.general-query-statements/4.fetch/#fetch_vertex_properties_by_multiple_tags","text":"Specify multiple tags in the FETCH statement to fetch the vertex properties by the tags. Separate the tags with commas. # The following example creates a new tag t1. nebula> CREATE TAG IF NOT EXISTS t1(a string, b int); # The following example attaches t1 to the vertex \"player100\". nebula> INSERT VERTEX t1(a, b) VALUES \"player100\":(\"Hello\", 100); # The following example fetches the properties of vertex \"player100\" by the tags player and t1. nebula> FETCH PROP ON player, t1 \"player100\" YIELD vertex AS v; +----------------------------------------------------------------------------+ | v | +----------------------------------------------------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"} :t1{a: \"Hello\", b: 100}) | +----------------------------------------------------------------------------+ You can combine multiple tags with multiple VIDs in a FETCH statement. nebula> FETCH PROP ON player, t1 \"player100\", \"player103\" YIELD vertex AS v; +----------------------------------------------------------------------------+ | v | +----------------------------------------------------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"} :t1{a: \"Hello\", b: 100}) | | (\"player103\" :player{age: 32, name: \"Rudy Gay\"}) | +----------------------------------------------------------------------------+","title":"Fetch vertex properties by multiple tags"},{"location":"3.ngql-guide/7.general-query-statements/4.fetch/#fetch_vertex_properties_by_all_tags","text":"Set an asterisk symbol * to fetch properties by all tags in the current graph space. nebula> FETCH PROP ON * \"player100\", \"player106\", \"team200\" YIELD vertex AS v; +----------------------------------------------------------------------------+ | v | +----------------------------------------------------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"} :t1{a: \"Hello\", b: 100}) | | (\"player106\" :player{age: 25, name: \"Kyle Anderson\"}) | | (\"team200\" :team{name: \"Warriors\"}) | +----------------------------------------------------------------------------+","title":"Fetch vertex properties by all tags"},{"location":"3.ngql-guide/7.general-query-statements/4.fetch/#fetch_edge_properties","text":"","title":"Fetch edge properties"},{"location":"3.ngql-guide/7.general-query-statements/4.fetch/#syntax_1","text":"FETCH PROP ON <edge_type> <src_vid> -> <dst_vid>[@<rank>] [, <src_vid> -> <dst_vid> ...] YIELD <output>; Parameter Description edge_type The name of the edge type. src_vid The VID of the source vertex. It specifies the start of an edge. dst_vid The VID of the destination vertex. It specifies the end of an edge. rank The rank of the edge. It is optional and defaults to 0 . It distinguishes an edge from other edges with the same edge type, source vertex, destination vertex, and rank. YIELD Define the output to be returned. For details, see YIELD .","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/4.fetch/#fetch_all_properties_of_an_edge","text":"The following statement fetches all the properties of the serve edge that connects vertex \"player100\" and vertex \"team204\" . nebula> FETCH PROP ON serve \"player100\" -> \"team204\" YIELD properties(edge); +------------------------------------+ | properties(EDGE) | +------------------------------------+ | {end_year: 2016, start_year: 1997} | +------------------------------------+","title":"Fetch all properties of an edge"},{"location":"3.ngql-guide/7.general-query-statements/4.fetch/#fetch_specific_properties_of_an_edge","text":"Use a YIELD clause to fetch specific properties of an edge. nebula> FETCH PROP ON serve \"player100\" -> \"team204\" \\ YIELD properties(edge).start_year; +-----------------------------+ | properties(EDGE).start_year | +-----------------------------+ | 1997 | +-----------------------------+","title":"Fetch specific properties of an edge"},{"location":"3.ngql-guide/7.general-query-statements/4.fetch/#fetch_properties_of_multiple_edges","text":"Specify multiple edge patterns ( <src_vid> -> <dst_vid>[@<rank>] ) to fetch properties of multiple edges. Separate the edge patterns with commas. nebula> FETCH PROP ON serve \"player100\" -> \"team204\", \"player133\" -> \"team202\" YIELD edge AS e; +-----------------------------------------------------------------------+ | e | +-----------------------------------------------------------------------+ | [:serve \"player100\"->\"team204\" @0 {end_year: 2016, start_year: 1997}] | | [:serve \"player133\"->\"team202\" @0 {end_year: 2011, start_year: 2002}] | +-----------------------------------------------------------------------+","title":"Fetch properties of multiple edges"},{"location":"3.ngql-guide/7.general-query-statements/4.fetch/#fetch_properties_based_on_edge_rank","text":"If there are multiple edges with the same edge type, source vertex, and destination vertex, you can specify the rank to fetch the properties on the correct edge. # The following example inserts edges with different ranks and property values. nebula> insert edge serve(start_year,end_year) \\ values \"player100\"->\"team204\"@1:(1998, 2017); nebula> insert edge serve(start_year,end_year) \\ values \"player100\"->\"team204\"@2:(1990, 2018); # By default, the FETCH statement returns the edge whose rank is 0. nebula> FETCH PROP ON serve \"player100\" -> \"team204\" YIELD edge AS e; +-----------------------------------------------------------------------+ | e | +-----------------------------------------------------------------------+ | [:serve \"player100\"->\"team204\" @0 {end_year: 2016, start_year: 1997}] | +-----------------------------------------------------------------------+ # To fetch on an edge whose rank is not 0, set its rank in the FETCH statement. nebula> FETCH PROP ON serve \"player100\" -> \"team204\"@1 YIELD edge AS e; +-----------------------------------------------------------------------+ | e | +-----------------------------------------------------------------------+ | [:serve \"player100\"->\"team204\" @1 {end_year: 2017, start_year: 1998}] | +-----------------------------------------------------------------------+","title":"Fetch properties based on edge rank"},{"location":"3.ngql-guide/7.general-query-statements/4.fetch/#use_fetch_in_composite_queries","text":"A common way to use FETCH is to combine it with native nGQL such as GO . The following statement returns the degree values of the follow edges that start from vertex \"player101\" . nebula> GO FROM \"player101\" OVER follow \\ YIELD src(edge) AS s, dst(edge) AS d \\ | FETCH PROP ON follow $-.s -> $-.d \\ YIELD properties(edge).degree; +-------------------------+ | properties(EDGE).degree | +-------------------------+ | 95 | | 90 | | 95 | +-------------------------+ Or you can use user-defined variables to construct similar queries. nebula> $var = GO FROM \"player101\" OVER follow \\ YIELD src(edge) AS s, dst(edge) AS d; \\ FETCH PROP ON follow $var.s -> $var.d \\ YIELD properties(edge).degree; +-------------------------+ | properties(EDGE).degree | +-------------------------+ | 95 | | 90 | | 95 | +-------------------------+ For more information about composite queries, see Composite queries (clause structure) .","title":"Use FETCH in composite queries"},{"location":"3.ngql-guide/7.general-query-statements/5.lookup/","text":"LOOKUP \u00b6 The LOOKUP statement traverses data based on indexes. You can use LOOKUP for the following purposes: Search for the specific data based on conditions defined by the WHERE clause. List vertices with a tag: retrieve the VID of all vertices with a tag. List edges with an edge type: retrieve the source vertex IDs, destination vertex IDs, and ranks of all edges with an edge type. Count the number of vertices or edges with a tag or an edge type. OpenCypher compatibility \u00b6 This topic applies to native nGQL only. Precautions \u00b6 Correct use of indexes can speed up queries, but indexes can dramatically reduce the write performance. The performance reduction can be 90% or even more. DO NOT use indexes in production environments unless you are fully aware of their influences on your service. If the specified property is not indexed when using the LOOKUP statement, Nebula Graph randomly selects one of the available indexes. For example, the tag player has two properties, name and age . Both the tag player itself and the property name have indexes, but the property age has no indexes. When running LOOKUP ON player WHERE player.age == 36 YIELD player.name; , Nebula Graph randomly uses one of the indexes of the tag player and the property name . Legacy version compatibility Before the release 2.5.0, if the specified property is not indexed when using the LOOKUP statement, Nebula Graph reports an error and does not use other indexes. Prerequisites \u00b6 Before using the LOOKUP statement, make sure that at least one index is created. If there are already related vertices, edges, or properties before an index is created, the user must rebuild the index after creating the index to make it valid. Syntax \u00b6 LOOKUP ON {<vertex_tag> | <edge_type>} [WHERE <expression> [AND <expression> ...]] YIELD <return_list> [AS <alias>]; <return_list> <prop_name> [AS <col_alias>] [, <prop_name> [AS <prop_alias>] ...]; WHERE <expression> : filters data with specified conditions. Both AND and OR are supported between different expressions. For more information, see WHERE . YIELD : Define the output to be returned. For details, see YIELD . AS : Set an alias. Limitations of using WHERE in LOOKUP \u00b6 The WHERE clause in a LOOKUP statement does not support the following operations: $- and $^ . In relational expressions, operators are not supported to have field names on both sides, such as tagName.prop1> tagName.prop2 . Nested AliasProp expressions in operation expressions and function expressions are not supported. The XOR operation is not supported. Retrieve vertices \u00b6 The following example returns vertices whose name is Tony Parker and the tag is player . nebula> CREATE TAG INDEX IF NOT EXISTS index_player ON player(name(30), age); nebula> REBUILD TAG INDEX index_player; +------------+ | New Job Id | +------------+ | 15 | +------------+ nebula> LOOKUP ON player \\ WHERE player.name == \"Tony Parker\" \\ YIELD id(vertex); +---------------+ | id(VERTEX) | +---------------+ | \"player101\" | +---------------+ nebula> LOOKUP ON player \\ WHERE player.name == \"Tony Parker\" \\ YIELD properties(vertex).name AS name, properties(vertex).age AS age; +---------------+-----+ | name | age | +---------------+-----+ | \"Tony Parker\" | 36 | +---------------+-----+ nebula> LOOKUP ON player \\ WHERE player.age > 45 \\ YIELD id(vertex); +-------------+ | id(VERTEX) | +-------------+ | \"player144\" | | \"player140\" | +-------------+ nebula> LOOKUP ON player \\ WHERE player.name STARTS WITH \"B\" \\ AND player.age IN [22,30] \\ YIELD properties(vertex).name, properties(vertex).age; +-------------------------+------------------------+ | properties(VERTEX).name | properties(VERTEX).age | +-------------------------+------------------------+ | \"Ben Simmons\" | 22 | | \"Blake Griffin\" | 30 | +-------------------------+------------------------+ nebula> LOOKUP ON player \\ WHERE player.name == \"Kobe Bryant\"\\ YIELD id(vertex) AS VertexID, properties(vertex).name AS name |\\ GO FROM $-.VertexID OVER serve \\ YIELD $-.name, properties(edge).start_year, properties(edge).end_year, properties($$).name; +---------------+-----------------------------+---------------------------+---------------------+ | $-.name | properties(EDGE).start_year | properties(EDGE).end_year | properties($$).name | +---------------+-----------------------------+---------------------------+---------------------+ | \"Kobe Bryant\" | 1996 | 2016 | \"Lakers\" | +---------------+-----------------------------+---------------------------+---------------------+ Retrieve edges \u00b6 The following example returns edges whose degree is 90 and the edge type is follow . nebula> CREATE EDGE INDEX IF NOT EXISTS index_follow ON follow(degree); nebula> REBUILD EDGE INDEX index_follow; +------------+ | New Job Id | +------------+ | 62 | +------------+ nebula> LOOKUP ON follow \\ WHERE follow.degree == 90 YIELD edge AS e; +----------------------------------------------------+ | e | +----------------------------------------------------+ | [:follow \"player109\"->\"player125\" @0 {degree: 90}] | | [:follow \"player118\"->\"player120\" @0 {degree: 90}] | | [:follow \"player118\"->\"player131\" @0 {degree: 90}] | ... nebula> LOOKUP ON follow \\ WHERE follow.degree == 90 \\ YIELD properties(edge).degree; +-------------+-------------+---------+-------------------------+ | SrcVID | DstVID | Ranking | properties(EDGE).degree | +-------------+-------------+---------+-------------------------+ | \"player150\" | \"player143\" | 0 | 90 | | \"player150\" | \"player137\" | 0 | 90 | | \"player148\" | \"player136\" | 0 | 90 | ... nebula> LOOKUP ON follow \\ WHERE follow.degree == 60 \\ YIELD dst(edge) AS DstVID, properties(edge).degree AS Degree |\\ GO FROM $-.DstVID OVER serve \\ YIELD $-.DstVID, properties(edge).start_year, properties(edge).end_year, properties($$).name; +-------------+-----------------------------+---------------------------+---------------------+ | $-.DstVID | properties(EDGE).start_year | properties(EDGE).end_year | properties($$).name | +-------------+-----------------------------+---------------------------+---------------------+ | \"player105\" | 2010 | 2018 | \"Spurs\" | | \"player105\" | 2009 | 2010 | \"Cavaliers\" | | \"player105\" | 2018 | 2019 | \"Raptors\" | +-------------+-----------------------------+---------------------------+---------------------+ List vertices or edges with a tag or an edge type \u00b6 To list vertices or edges with a tag or an edge type, at least one index must exist on the tag, the edge type, or its property. For example, if there is a player tag with a name property and an age property, to retrieve the VID of all vertices tagged with player , there has to be an index on the player tag itself, the name property, or the age property. The following example shows how to retrieve the VID of all vertices tagged with player . nebula> CREATE TAG IF NOT EXISTS player(name string,age int); nebula> CREATE TAG INDEX IF NOT EXISTS player_index on player(); nebula> REBUILD TAG INDEX player_index; +------------+ | New Job Id | +------------+ | 66 | +------------+ nebula> INSERT VERTEX player(name,age) \\ VALUES \"player100\":(\"Tim Duncan\", 42), \"player101\":(\"Tony Parker\", 36); The following statement retrieves the VID of all vertices with the tag `player`. It is similar to `MATCH (n:player) RETURN id(n) /*, n */`. nebula> LOOKUP ON player YIELD id(vertex); +-------------+ | id(VERTEX) | +-------------+ | \"player100\" | | \"player101\" | ... The following example shows how to retrieve the source Vertex IDs, destination vertex IDs, and ranks of all edges of the follow edge type. nebula> CREATE EDGE IF NOT EXISTS follow(degree int); nebula> CREATE EDGE INDEX IF NOT EXISTS follow_index on follow(); nebula> REBUILD EDGE INDEX follow_index; +------------+ | New Job Id | +------------+ | 88 | +------------+ nebula> INSERT EDGE follow(degree) \\ VALUES \"player100\"->\"player101\":(95); The following statement retrieves all edges with the edge type `follow`. It is similar to `MATCH (s)-[e:follow]->(d) RETURN id(s), rank(e), id(d) /*, type(e) */`. nebula)> LOOKUP ON follow YIELD edge AS e; +-----------------------------------------------------+ | e | +-----------------------------------------------------+ | [:follow \"player105\"->\"player100\" @0 {degree: 70}] | | [:follow \"player105\"->\"player116\" @0 {degree: 80}] | | [:follow \"player109\"->\"player100\" @0 {degree: 80}] | ... Count the numbers of vertices or edges \u00b6 The following example shows how to count the number of vertices tagged with player and edges of the follow edge type. nebula> LOOKUP ON player YIELD id(vertex)|\\ YIELD COUNT(*) AS Player_Number; +---------------+ | Player_Number | +---------------+ | 51 | +---------------+ nebula> LOOKUP ON follow YIELD edge AS e| \\ YIELD COUNT(*) AS Follow_Number; +---------------+ | Follow_Number | +---------------+ | 81 | +---------------+ Note You can also use SHOW STATS to count the numbers of vertices or edges.","title":"LOOKUP"},{"location":"3.ngql-guide/7.general-query-statements/5.lookup/#lookup","text":"The LOOKUP statement traverses data based on indexes. You can use LOOKUP for the following purposes: Search for the specific data based on conditions defined by the WHERE clause. List vertices with a tag: retrieve the VID of all vertices with a tag. List edges with an edge type: retrieve the source vertex IDs, destination vertex IDs, and ranks of all edges with an edge type. Count the number of vertices or edges with a tag or an edge type.","title":"LOOKUP"},{"location":"3.ngql-guide/7.general-query-statements/5.lookup/#opencypher_compatibility","text":"This topic applies to native nGQL only.","title":"OpenCypher compatibility"},{"location":"3.ngql-guide/7.general-query-statements/5.lookup/#precautions","text":"Correct use of indexes can speed up queries, but indexes can dramatically reduce the write performance. The performance reduction can be 90% or even more. DO NOT use indexes in production environments unless you are fully aware of their influences on your service. If the specified property is not indexed when using the LOOKUP statement, Nebula Graph randomly selects one of the available indexes. For example, the tag player has two properties, name and age . Both the tag player itself and the property name have indexes, but the property age has no indexes. When running LOOKUP ON player WHERE player.age == 36 YIELD player.name; , Nebula Graph randomly uses one of the indexes of the tag player and the property name . Legacy version compatibility Before the release 2.5.0, if the specified property is not indexed when using the LOOKUP statement, Nebula Graph reports an error and does not use other indexes.","title":"Precautions"},{"location":"3.ngql-guide/7.general-query-statements/5.lookup/#prerequisites","text":"Before using the LOOKUP statement, make sure that at least one index is created. If there are already related vertices, edges, or properties before an index is created, the user must rebuild the index after creating the index to make it valid.","title":"Prerequisites"},{"location":"3.ngql-guide/7.general-query-statements/5.lookup/#syntax","text":"LOOKUP ON {<vertex_tag> | <edge_type>} [WHERE <expression> [AND <expression> ...]] YIELD <return_list> [AS <alias>]; <return_list> <prop_name> [AS <col_alias>] [, <prop_name> [AS <prop_alias>] ...]; WHERE <expression> : filters data with specified conditions. Both AND and OR are supported between different expressions. For more information, see WHERE . YIELD : Define the output to be returned. For details, see YIELD . AS : Set an alias.","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/5.lookup/#limitations_of_using_where_in_lookup","text":"The WHERE clause in a LOOKUP statement does not support the following operations: $- and $^ . In relational expressions, operators are not supported to have field names on both sides, such as tagName.prop1> tagName.prop2 . Nested AliasProp expressions in operation expressions and function expressions are not supported. The XOR operation is not supported.","title":"Limitations of using WHERE in LOOKUP"},{"location":"3.ngql-guide/7.general-query-statements/5.lookup/#retrieve_vertices","text":"The following example returns vertices whose name is Tony Parker and the tag is player . nebula> CREATE TAG INDEX IF NOT EXISTS index_player ON player(name(30), age); nebula> REBUILD TAG INDEX index_player; +------------+ | New Job Id | +------------+ | 15 | +------------+ nebula> LOOKUP ON player \\ WHERE player.name == \"Tony Parker\" \\ YIELD id(vertex); +---------------+ | id(VERTEX) | +---------------+ | \"player101\" | +---------------+ nebula> LOOKUP ON player \\ WHERE player.name == \"Tony Parker\" \\ YIELD properties(vertex).name AS name, properties(vertex).age AS age; +---------------+-----+ | name | age | +---------------+-----+ | \"Tony Parker\" | 36 | +---------------+-----+ nebula> LOOKUP ON player \\ WHERE player.age > 45 \\ YIELD id(vertex); +-------------+ | id(VERTEX) | +-------------+ | \"player144\" | | \"player140\" | +-------------+ nebula> LOOKUP ON player \\ WHERE player.name STARTS WITH \"B\" \\ AND player.age IN [22,30] \\ YIELD properties(vertex).name, properties(vertex).age; +-------------------------+------------------------+ | properties(VERTEX).name | properties(VERTEX).age | +-------------------------+------------------------+ | \"Ben Simmons\" | 22 | | \"Blake Griffin\" | 30 | +-------------------------+------------------------+ nebula> LOOKUP ON player \\ WHERE player.name == \"Kobe Bryant\"\\ YIELD id(vertex) AS VertexID, properties(vertex).name AS name |\\ GO FROM $-.VertexID OVER serve \\ YIELD $-.name, properties(edge).start_year, properties(edge).end_year, properties($$).name; +---------------+-----------------------------+---------------------------+---------------------+ | $-.name | properties(EDGE).start_year | properties(EDGE).end_year | properties($$).name | +---------------+-----------------------------+---------------------------+---------------------+ | \"Kobe Bryant\" | 1996 | 2016 | \"Lakers\" | +---------------+-----------------------------+---------------------------+---------------------+","title":"Retrieve vertices"},{"location":"3.ngql-guide/7.general-query-statements/5.lookup/#retrieve_edges","text":"The following example returns edges whose degree is 90 and the edge type is follow . nebula> CREATE EDGE INDEX IF NOT EXISTS index_follow ON follow(degree); nebula> REBUILD EDGE INDEX index_follow; +------------+ | New Job Id | +------------+ | 62 | +------------+ nebula> LOOKUP ON follow \\ WHERE follow.degree == 90 YIELD edge AS e; +----------------------------------------------------+ | e | +----------------------------------------------------+ | [:follow \"player109\"->\"player125\" @0 {degree: 90}] | | [:follow \"player118\"->\"player120\" @0 {degree: 90}] | | [:follow \"player118\"->\"player131\" @0 {degree: 90}] | ... nebula> LOOKUP ON follow \\ WHERE follow.degree == 90 \\ YIELD properties(edge).degree; +-------------+-------------+---------+-------------------------+ | SrcVID | DstVID | Ranking | properties(EDGE).degree | +-------------+-------------+---------+-------------------------+ | \"player150\" | \"player143\" | 0 | 90 | | \"player150\" | \"player137\" | 0 | 90 | | \"player148\" | \"player136\" | 0 | 90 | ... nebula> LOOKUP ON follow \\ WHERE follow.degree == 60 \\ YIELD dst(edge) AS DstVID, properties(edge).degree AS Degree |\\ GO FROM $-.DstVID OVER serve \\ YIELD $-.DstVID, properties(edge).start_year, properties(edge).end_year, properties($$).name; +-------------+-----------------------------+---------------------------+---------------------+ | $-.DstVID | properties(EDGE).start_year | properties(EDGE).end_year | properties($$).name | +-------------+-----------------------------+---------------------------+---------------------+ | \"player105\" | 2010 | 2018 | \"Spurs\" | | \"player105\" | 2009 | 2010 | \"Cavaliers\" | | \"player105\" | 2018 | 2019 | \"Raptors\" | +-------------+-----------------------------+---------------------------+---------------------+","title":"Retrieve edges"},{"location":"3.ngql-guide/7.general-query-statements/5.lookup/#list_vertices_or_edges_with_a_tag_or_an_edge_type","text":"To list vertices or edges with a tag or an edge type, at least one index must exist on the tag, the edge type, or its property. For example, if there is a player tag with a name property and an age property, to retrieve the VID of all vertices tagged with player , there has to be an index on the player tag itself, the name property, or the age property. The following example shows how to retrieve the VID of all vertices tagged with player . nebula> CREATE TAG IF NOT EXISTS player(name string,age int); nebula> CREATE TAG INDEX IF NOT EXISTS player_index on player(); nebula> REBUILD TAG INDEX player_index; +------------+ | New Job Id | +------------+ | 66 | +------------+ nebula> INSERT VERTEX player(name,age) \\ VALUES \"player100\":(\"Tim Duncan\", 42), \"player101\":(\"Tony Parker\", 36); The following statement retrieves the VID of all vertices with the tag `player`. It is similar to `MATCH (n:player) RETURN id(n) /*, n */`. nebula> LOOKUP ON player YIELD id(vertex); +-------------+ | id(VERTEX) | +-------------+ | \"player100\" | | \"player101\" | ... The following example shows how to retrieve the source Vertex IDs, destination vertex IDs, and ranks of all edges of the follow edge type. nebula> CREATE EDGE IF NOT EXISTS follow(degree int); nebula> CREATE EDGE INDEX IF NOT EXISTS follow_index on follow(); nebula> REBUILD EDGE INDEX follow_index; +------------+ | New Job Id | +------------+ | 88 | +------------+ nebula> INSERT EDGE follow(degree) \\ VALUES \"player100\"->\"player101\":(95); The following statement retrieves all edges with the edge type `follow`. It is similar to `MATCH (s)-[e:follow]->(d) RETURN id(s), rank(e), id(d) /*, type(e) */`. nebula)> LOOKUP ON follow YIELD edge AS e; +-----------------------------------------------------+ | e | +-----------------------------------------------------+ | [:follow \"player105\"->\"player100\" @0 {degree: 70}] | | [:follow \"player105\"->\"player116\" @0 {degree: 80}] | | [:follow \"player109\"->\"player100\" @0 {degree: 80}] | ...","title":"List vertices or edges with a tag or an edge type"},{"location":"3.ngql-guide/7.general-query-statements/5.lookup/#count_the_numbers_of_vertices_or_edges","text":"The following example shows how to count the number of vertices tagged with player and edges of the follow edge type. nebula> LOOKUP ON player YIELD id(vertex)|\\ YIELD COUNT(*) AS Player_Number; +---------------+ | Player_Number | +---------------+ | 51 | +---------------+ nebula> LOOKUP ON follow YIELD edge AS e| \\ YIELD COUNT(*) AS Follow_Number; +---------------+ | Follow_Number | +---------------+ | 81 | +---------------+ Note You can also use SHOW STATS to count the numbers of vertices or edges.","title":"Count the numbers of vertices or edges"},{"location":"3.ngql-guide/7.general-query-statements/optional-match/","text":"OPTIONAL MATCH \u00b6 The OPTIONAL MATCH clause is used to search for the pattern described in it. OPTIONAL MATCH matches patterns against your graph database, just like MATCH does. The difference is that if no matches are found, OPTIONAL MATCH will use a null for missing parts of the pattern. OpenCypher Compatibility \u00b6 This topic applies to the openCypher syntax in nGQL only. Example \u00b6 The example of the use of OPTIONAL MATCH in the MATCH statement is as follows: nebula> MATCH (m)-[]->(n) WHERE id(m)==\"player100\" \\ OPTIONAL MATCH (n)-[]->(l) WHERE id(n)==\"player125\" \\ RETURN id(m),id(n),id(l); +-------------+-------------+-------------+ | id(m) | id(n) | id(l) | +-------------+-------------+-------------+ | \"player100\" | \"team204\" | __NULL__ | | \"player100\" | \"player101\" | __NULL__ | | \"player100\" | \"player125\" | \"team204\" | | \"player100\" | \"player125\" | \"player100\" | +-------------+-------------+-------------+ Using multiple MATCH instead of OPTIONAL MATCH returns rows that match the pattern exactly. The example is as follows: nebula> MATCH (m)-[]->(n) WHERE id(m)==\"player100\" \\ MATCH (n)-[]->(l) WHERE id(n)==\"player125\" \\ RETURN id(m),id(n),id(l); +-------------+-------------+-------------+ | id(m) | id(n) | id(l) | +-------------+-------------+-------------+ | \"player100\" | \"player125\" | \"team204\" | | \"player100\" | \"player125\" | \"player100\" | +-------------+-------------+-------------+","title":"OPTIONAL MATCH"},{"location":"3.ngql-guide/7.general-query-statements/optional-match/#optional_match","text":"The OPTIONAL MATCH clause is used to search for the pattern described in it. OPTIONAL MATCH matches patterns against your graph database, just like MATCH does. The difference is that if no matches are found, OPTIONAL MATCH will use a null for missing parts of the pattern.","title":"OPTIONAL MATCH"},{"location":"3.ngql-guide/7.general-query-statements/optional-match/#opencypher_compatibility","text":"This topic applies to the openCypher syntax in nGQL only.","title":"OpenCypher Compatibility"},{"location":"3.ngql-guide/7.general-query-statements/optional-match/#example","text":"The example of the use of OPTIONAL MATCH in the MATCH statement is as follows: nebula> MATCH (m)-[]->(n) WHERE id(m)==\"player100\" \\ OPTIONAL MATCH (n)-[]->(l) WHERE id(n)==\"player125\" \\ RETURN id(m),id(n),id(l); +-------------+-------------+-------------+ | id(m) | id(n) | id(l) | +-------------+-------------+-------------+ | \"player100\" | \"team204\" | __NULL__ | | \"player100\" | \"player101\" | __NULL__ | | \"player100\" | \"player125\" | \"team204\" | | \"player100\" | \"player125\" | \"player100\" | +-------------+-------------+-------------+ Using multiple MATCH instead of OPTIONAL MATCH returns rows that match the pattern exactly. The example is as follows: nebula> MATCH (m)-[]->(n) WHERE id(m)==\"player100\" \\ MATCH (n)-[]->(l) WHERE id(n)==\"player125\" \\ RETURN id(m),id(n),id(l); +-------------+-------------+-------------+ | id(m) | id(n) | id(l) | +-------------+-------------+-------------+ | \"player100\" | \"player125\" | \"team204\" | | \"player100\" | \"player125\" | \"player100\" | +-------------+-------------+-------------+","title":"Example"},{"location":"3.ngql-guide/7.general-query-statements/6.show/1.show-charset/","text":"SHOW CHARSET \u00b6 The SHOW CHARSET statement shows the available character sets. Currently available types are utf8 and utf8mb4 . The default charset type is utf8 . Nebula Graph extends the uft8 to support four-byte characters. Therefore utf8 and utf8mb4 are equivalent. Syntax \u00b6 SHOW CHARSET; Example \u00b6 nebula> SHOW CHARSET; +---------+-----------------+-------------------+--------+ | Charset | Description | Default collation | Maxlen | +---------+-----------------+-------------------+--------+ | \"utf8\" | \"UTF-8 Unicode\" | \"utf8_bin\" | 4 | +---------+-----------------+-------------------+--------+ Parameter Description Charset The name of the character set. Description The description of the character set. Default collation The default collation of the character set. Maxlen The maximum number of bytes required to store one character.","title":"SHOW CHARSET"},{"location":"3.ngql-guide/7.general-query-statements/6.show/1.show-charset/#show_charset","text":"The SHOW CHARSET statement shows the available character sets. Currently available types are utf8 and utf8mb4 . The default charset type is utf8 . Nebula Graph extends the uft8 to support four-byte characters. Therefore utf8 and utf8mb4 are equivalent.","title":"SHOW CHARSET"},{"location":"3.ngql-guide/7.general-query-statements/6.show/1.show-charset/#syntax","text":"SHOW CHARSET;","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/6.show/1.show-charset/#example","text":"nebula> SHOW CHARSET; +---------+-----------------+-------------------+--------+ | Charset | Description | Default collation | Maxlen | +---------+-----------------+-------------------+--------+ | \"utf8\" | \"UTF-8 Unicode\" | \"utf8_bin\" | 4 | +---------+-----------------+-------------------+--------+ Parameter Description Charset The name of the character set. Description The description of the character set. Default collation The default collation of the character set. Maxlen The maximum number of bytes required to store one character.","title":"Example"},{"location":"3.ngql-guide/7.general-query-statements/6.show/10.show-roles/","text":"SHOW ROLES \u00b6 The SHOW ROLES statement shows the roles that are assigned to a user account. The return message differs according to the role of the user who is running this statement: If the user is a GOD or ADMIN and is granted access to the specified graph space, Nebula Graph shows all roles in this graph space except for GOD . If the user is a DBA , USER , or GUEST and is granted access to the specified graph space, Nebula Graph shows the user's own role in this graph space. If the user does not have access to the specified graph space, Nebula Graph returns PermissionError . For more information about roles, see Roles and privileges . Syntax \u00b6 SHOW ROLES IN <space_name>; Example \u00b6 nebula> SHOW ROLES in basketballplayer; +---------+-----------+ | Account | Role Type | +---------+-----------+ | \"user1\" | \"ADMIN\" | +---------+-----------+","title":"SHOW ROLES"},{"location":"3.ngql-guide/7.general-query-statements/6.show/10.show-roles/#show_roles","text":"The SHOW ROLES statement shows the roles that are assigned to a user account. The return message differs according to the role of the user who is running this statement: If the user is a GOD or ADMIN and is granted access to the specified graph space, Nebula Graph shows all roles in this graph space except for GOD . If the user is a DBA , USER , or GUEST and is granted access to the specified graph space, Nebula Graph shows the user's own role in this graph space. If the user does not have access to the specified graph space, Nebula Graph returns PermissionError . For more information about roles, see Roles and privileges .","title":"SHOW ROLES"},{"location":"3.ngql-guide/7.general-query-statements/6.show/10.show-roles/#syntax","text":"SHOW ROLES IN <space_name>;","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/6.show/10.show-roles/#example","text":"nebula> SHOW ROLES in basketballplayer; +---------+-----------+ | Account | Role Type | +---------+-----------+ | \"user1\" | \"ADMIN\" | +---------+-----------+","title":"Example"},{"location":"3.ngql-guide/7.general-query-statements/6.show/11.show-snapshots/","text":"SHOW SNAPSHOTS \u00b6 The SHOW SNAPSHOTS statement shows the information of all the snapshots. For how to create a snapshot and backup data, see Snapshot . Role requirement \u00b6 Only the root user who has the GOD role can use the SHOW SNAPSHOTS statement. Syntax \u00b6 SHOW SNAPSHOTS; Example \u00b6 nebula> SHOW SNAPSHOTS; +--------------------------------+---------+-----------------------------------------------------+ | Name | Status | Hosts | +--------------------------------+---------+-----------------------------------------------------+ | \"SNAPSHOT_2020_12_16_11_13_55\" | \"VALID\" | \"storaged0:9779, storaged1:9779, storaged2:9779\" | | \"SNAPSHOT_2020_12_16_11_14_10\" | \"VALID\" | \"storaged0:9779, storaged1:9779, storaged2:9779\" | +--------------------------------+---------+-----------------------------------------------------+","title":"SHOW SNAPSHOTS"},{"location":"3.ngql-guide/7.general-query-statements/6.show/11.show-snapshots/#show_snapshots","text":"The SHOW SNAPSHOTS statement shows the information of all the snapshots. For how to create a snapshot and backup data, see Snapshot .","title":"SHOW SNAPSHOTS"},{"location":"3.ngql-guide/7.general-query-statements/6.show/11.show-snapshots/#role_requirement","text":"Only the root user who has the GOD role can use the SHOW SNAPSHOTS statement.","title":"Role requirement"},{"location":"3.ngql-guide/7.general-query-statements/6.show/11.show-snapshots/#syntax","text":"SHOW SNAPSHOTS;","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/6.show/11.show-snapshots/#example","text":"nebula> SHOW SNAPSHOTS; +--------------------------------+---------+-----------------------------------------------------+ | Name | Status | Hosts | +--------------------------------+---------+-----------------------------------------------------+ | \"SNAPSHOT_2020_12_16_11_13_55\" | \"VALID\" | \"storaged0:9779, storaged1:9779, storaged2:9779\" | | \"SNAPSHOT_2020_12_16_11_14_10\" | \"VALID\" | \"storaged0:9779, storaged1:9779, storaged2:9779\" | +--------------------------------+---------+-----------------------------------------------------+","title":"Example"},{"location":"3.ngql-guide/7.general-query-statements/6.show/12.show-spaces/","text":"SHOW SPACES \u00b6 The SHOW SPACES statement shows existing graph spaces in Nebula Graph. For how to create a graph space, see CREATE SPACE . Syntax \u00b6 SHOW SPACES; Example \u00b6 nebula> SHOW SPACES; +---------------------+ | Name | +---------------------+ | \"docs\" | | \"basketballplayer\" | +---------------------+","title":"SHOW SPACES"},{"location":"3.ngql-guide/7.general-query-statements/6.show/12.show-spaces/#show_spaces","text":"The SHOW SPACES statement shows existing graph spaces in Nebula Graph. For how to create a graph space, see CREATE SPACE .","title":"SHOW SPACES"},{"location":"3.ngql-guide/7.general-query-statements/6.show/12.show-spaces/#syntax","text":"SHOW SPACES;","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/6.show/12.show-spaces/#example","text":"nebula> SHOW SPACES; +---------------------+ | Name | +---------------------+ | \"docs\" | | \"basketballplayer\" | +---------------------+","title":"Example"},{"location":"3.ngql-guide/7.general-query-statements/6.show/14.show-stats/","text":"SHOW STATS \u00b6 The SHOW STATS statement shows the statistics of the graph space collected by the latest STATS job. The statistics include the following information: The number of vertices in the graph space The number of edges in the graph space The number of vertices of each tag The number of edges of each edge type Prerequisites \u00b6 You have to run the SUBMIT JOB STATS statement in the graph space where you want to collect statistics. For more information, see SUBMIT JOB STATS . Caution The result of the SHOW STATS statement is based on the last executed SUBMIT JOB STATS statement. If you want to update the result, run SUBMIT JOB STATS again. Otherwise the statistics will be wrong. Syntax \u00b6 SHOW STATS; Examples \u00b6 # Choose a graph space. nebula> USE basketballplayer; # Start SUBMIT JOB STATS. nebula> SUBMIT JOB STATS; +------------+ | New Job Id | +------------+ | 98 | +------------+ # Make sure the job executes successfully. nebula> SHOW JOB 98; +----------------+---------------+------------+----------------------------+----------------------------+-------------+ | Job Id(TaskId) | Command(Dest) | Status | Start Time | Stop Time | Error Code | +----------------+---------------+------------+----------------------------+----------------------------+-------------+ | 98 | \"STATS\" | \"FINISHED\" | 2021-11-01T09:33:21.000000 | 2021-11-01T09:33:21.000000 | \"SUCCEEDED\" | | 0 | \"storaged2\" | \"FINISHED\" | 2021-11-01T09:33:21.000000 | 2021-11-01T09:33:21.000000 | \"SUCCEEDED\" | | 1 | \"storaged0\" | \"FINISHED\" | 2021-11-01T09:33:21.000000 | 2021-11-01T09:33:21.000000 | \"SUCCEEDED\" | | 2 | \"storaged1\" | \"FINISHED\" | 2021-11-01T09:33:21.000000 | 2021-11-01T09:33:21.000000 | \"SUCCEEDED\" | +----------------+---------------+------------+----------------------------+----------------------------+-------------+ # Show the statistics of the graph space. nebula> SHOW STATS; +---------+------------+-------+ | Type | Name | Count | +---------+------------+-------+ | \"Tag\" | \"player\" | 51 | | \"Tag\" | \"team\" | 30 | | \"Edge\" | \"follow\" | 81 | | \"Edge\" | \"serve\" | 152 | | \"Space\" | \"vertices\" | 81 | | \"Space\" | \"edges\" | 233 | +---------+------------+-------+","title":"SHOW STATS"},{"location":"3.ngql-guide/7.general-query-statements/6.show/14.show-stats/#show_stats","text":"The SHOW STATS statement shows the statistics of the graph space collected by the latest STATS job. The statistics include the following information: The number of vertices in the graph space The number of edges in the graph space The number of vertices of each tag The number of edges of each edge type","title":"SHOW STATS"},{"location":"3.ngql-guide/7.general-query-statements/6.show/14.show-stats/#prerequisites","text":"You have to run the SUBMIT JOB STATS statement in the graph space where you want to collect statistics. For more information, see SUBMIT JOB STATS . Caution The result of the SHOW STATS statement is based on the last executed SUBMIT JOB STATS statement. If you want to update the result, run SUBMIT JOB STATS again. Otherwise the statistics will be wrong.","title":"Prerequisites"},{"location":"3.ngql-guide/7.general-query-statements/6.show/14.show-stats/#syntax","text":"SHOW STATS;","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/6.show/14.show-stats/#examples","text":"# Choose a graph space. nebula> USE basketballplayer; # Start SUBMIT JOB STATS. nebula> SUBMIT JOB STATS; +------------+ | New Job Id | +------------+ | 98 | +------------+ # Make sure the job executes successfully. nebula> SHOW JOB 98; +----------------+---------------+------------+----------------------------+----------------------------+-------------+ | Job Id(TaskId) | Command(Dest) | Status | Start Time | Stop Time | Error Code | +----------------+---------------+------------+----------------------------+----------------------------+-------------+ | 98 | \"STATS\" | \"FINISHED\" | 2021-11-01T09:33:21.000000 | 2021-11-01T09:33:21.000000 | \"SUCCEEDED\" | | 0 | \"storaged2\" | \"FINISHED\" | 2021-11-01T09:33:21.000000 | 2021-11-01T09:33:21.000000 | \"SUCCEEDED\" | | 1 | \"storaged0\" | \"FINISHED\" | 2021-11-01T09:33:21.000000 | 2021-11-01T09:33:21.000000 | \"SUCCEEDED\" | | 2 | \"storaged1\" | \"FINISHED\" | 2021-11-01T09:33:21.000000 | 2021-11-01T09:33:21.000000 | \"SUCCEEDED\" | +----------------+---------------+------------+----------------------------+----------------------------+-------------+ # Show the statistics of the graph space. nebula> SHOW STATS; +---------+------------+-------+ | Type | Name | Count | +---------+------------+-------+ | \"Tag\" | \"player\" | 51 | | \"Tag\" | \"team\" | 30 | | \"Edge\" | \"follow\" | 81 | | \"Edge\" | \"serve\" | 152 | | \"Space\" | \"vertices\" | 81 | | \"Space\" | \"edges\" | 233 | +---------+------------+-------+","title":"Examples"},{"location":"3.ngql-guide/7.general-query-statements/6.show/15.show-tags-edges/","text":"SHOW TAGS/EDGES \u00b6 The SHOW TAGS statement shows all the tags in the current graph space. The SHOW EDGES statement shows all the edge types in the current graph space. Syntax \u00b6 SHOW {TAGS | EDGES}; Examples \u00b6 nebula> SHOW TAGS; +----------+ | Name | +----------+ | \"player\" | | \"star\" | | \"team\" | +----------+ nebula> SHOW EDGES; +----------+ | Name | +----------+ | \"follow\" | | \"serve\" | +----------+","title":"SHOW TAGS/EDGES"},{"location":"3.ngql-guide/7.general-query-statements/6.show/15.show-tags-edges/#show_tagsedges","text":"The SHOW TAGS statement shows all the tags in the current graph space. The SHOW EDGES statement shows all the edge types in the current graph space.","title":"SHOW TAGS/EDGES"},{"location":"3.ngql-guide/7.general-query-statements/6.show/15.show-tags-edges/#syntax","text":"SHOW {TAGS | EDGES};","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/6.show/15.show-tags-edges/#examples","text":"nebula> SHOW TAGS; +----------+ | Name | +----------+ | \"player\" | | \"star\" | | \"team\" | +----------+ nebula> SHOW EDGES; +----------+ | Name | +----------+ | \"follow\" | | \"serve\" | +----------+","title":"Examples"},{"location":"3.ngql-guide/7.general-query-statements/6.show/16.show-users/","text":"SHOW USERS \u00b6 The SHOW USERS statement shows the user information. Role requirement \u00b6 Only the root user who has the GOD role can use the SHOW USERS statement. Syntax \u00b6 SHOW USERS; Example \u00b6 nebula> SHOW USERS; +---------+-----------------+ | Account | IP Whitelist | +---------+-----------------+ | \"root\" | \"\" | | \"user1\" | \"\" | | \"user2\" | \"192.168.10.10\" | +---------+-----------------+","title":"SHOW USERS"},{"location":"3.ngql-guide/7.general-query-statements/6.show/16.show-users/#show_users","text":"The SHOW USERS statement shows the user information.","title":"SHOW USERS"},{"location":"3.ngql-guide/7.general-query-statements/6.show/16.show-users/#role_requirement","text":"Only the root user who has the GOD role can use the SHOW USERS statement.","title":"Role requirement"},{"location":"3.ngql-guide/7.general-query-statements/6.show/16.show-users/#syntax","text":"SHOW USERS;","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/6.show/16.show-users/#example","text":"nebula> SHOW USERS; +---------+-----------------+ | Account | IP Whitelist | +---------+-----------------+ | \"root\" | \"\" | | \"user1\" | \"\" | | \"user2\" | \"192.168.10.10\" | +---------+-----------------+","title":"Example"},{"location":"3.ngql-guide/7.general-query-statements/6.show/17.show-sessions/","text":"SHOW SESSIONS \u00b6 When a user logs in to the database, a corresponding session will be created and users can query for session information. The SHOW SESSIONS statement shows the information of all the sessions. It can also show a specified session with its ID. Precautions \u00b6 The client will call the API release to release the session and clear the session information when you run exit after the operation ends. If you exit the database in an unexpected way and the session timeout duration is not set via session_idle_timeout_secs in nebula-graphd.conf , the session will not be released automatically. For those sessions that are not automatically released, you need to delete them manually (TODO: coding). SHOW SESSIONS queries the session information of all the Graph services. SHOW LOCAL SESSIONS queries the session information of the currently connected Graph service and does not query the session information of other Graph services. SHOW SESSION <Session_Id> queries the session information with a specific session id. Syntax \u00b6 SHOW [LOCAL] SESSIONS; SHOW SESSION <Session_Id>; Examples \u00b6 nebula> SHOW SESSIONS; +------------------+----------+--------------------+----------------------------+----------------------------+------------------+----------+--------------------+ | SessionId | UserName | SpaceName | CreateTime | UpdateTime | GraphAddr | Timezone | ClientIp | +------------------+----------+--------------------+----------------------------+----------------------------+------------------+----------+--------------------+ | 1635128818397714 | \"root\" | \"test\" | 2021-10-25T02:26:58.397714 | 2021-10-25T08:31:31.846846 | \"127.0.0.1:9669\" | 0 | \"::ffff:127.0.0.1\" | | 1635254859271703 | \"root\" | \"basketballplayer\" | 2021-10-26T13:27:39.271703 | 2021-10-26T13:51:38.277704 | \"127.0.0.1:9669\" | 0 | \"::ffff:127.0.0.1\" | | 1634871229727322 | \"root\" | \"basketballplayer\" | 2021-10-22T02:53:49.727322 | 2021-10-22T02:53:56.564001 | \"127.0.0.1:9669\" | 0 | \"::ffff:127.0.0.1\" | | 1635750725840229 | \"root\" | \"basketballplayer\" | 2021-11-01T07:12:05.840229 | 2021-11-01T09:42:36.883617 | \"127.0.0.1:9669\" | 0 | \"::ffff:127.0.0.1\" | | 1635299224732060 | \"root\" | \"basketballplayer\" | 2021-10-27T01:47:04.732060 | 2021-10-27T09:04:31.741126 | \"127.0.0.1:9669\" | 0 | \"::ffff:127.0.0.1\" | | 1634628999765689 | \"root\" | \"\" | 2021-10-19T07:36:39.765689 | 2021-10-19T07:36:39.768064 | \"127.0.0.1:9669\" | 0 | \"::ffff:127.0.0.1\" | | 1634886296595136 | \"root\" | \"basketballplayer\" | 2021-10-22T07:04:56.595136 | 2021-10-22T09:48:20.299364 | \"127.0.0.1:9669\" | 0 | \"::ffff:127.0.0.1\" | | 1634629179882439 | \"root\" | \"basketballplayer\" | 2021-10-19T07:39:39.882439 | 2021-10-19T09:34:52.153145 | \"127.0.0.1:9669\" | 0 | \"::ffff:127.0.0.1\" | | 1635246158961634 | \"root\" | \"basketballplayer\" | 2021-10-26T11:02:38.961634 | 2021-10-26T11:02:51.250897 | \"127.0.0.1:9669\" | 0 | \"::ffff:127.0.0.1\" | | 1634785346839017 | \"root\" | \"basketballplayer\" | 2021-10-21T03:02:26.839017 | 2021-10-21T11:07:40.911329 | \"127.0.0.1:9669\" | 0 | \"::ffff:127.0.0.1\" | +------------------+----------+--------------------+----------------------------+----------------------------+------------------+----------+--------------------+ nebula> SHOW SESSION 1635254859271703; +--------------+----------------------------+ | VariableName | Value | +--------------+----------------------------+ | \"SessionID\" | 1635254859271703 | | \"UserName\" | \"root\" | | \"SpaceName\" | \"basketballplayer\" | | \"CreateTime\" | 2021-10-26T13:27:39.271703 | | \"UpdateTime\" | 2021-10-26T13:51:38.277704 | | \"GraphAddr\" | \"127.0.0.1:9669\" | | \"Timezone\" | 0 | | \"ClientIp\" | \"::ffff:127.0.0.1\" | +--------------+----------------------------+ Parameter Description SessionId The session ID, namely the identifier of a session. UserName The username in a session. SpaceName The name of the graph space that the user uses currently. It is null ( \"\" ) when you first log in because there is no specified graph space. CreateTime The time when the session is created, namely the time when the user logs in. The time zone is specified by timezone_name in the configuration file. UpdateTime The system will update the time when there is an operation. The time zone is specified by timezone_name in the configuration file. GraphAddr The IP address and port of the Graph server that hosts the session. Timezone A reserved parameter that has no specified meaning for now. ClientIp The IP address of the client.","title":"SHOW SESSIONS"},{"location":"3.ngql-guide/7.general-query-statements/6.show/17.show-sessions/#show_sessions","text":"When a user logs in to the database, a corresponding session will be created and users can query for session information. The SHOW SESSIONS statement shows the information of all the sessions. It can also show a specified session with its ID.","title":"SHOW SESSIONS"},{"location":"3.ngql-guide/7.general-query-statements/6.show/17.show-sessions/#precautions","text":"The client will call the API release to release the session and clear the session information when you run exit after the operation ends. If you exit the database in an unexpected way and the session timeout duration is not set via session_idle_timeout_secs in nebula-graphd.conf , the session will not be released automatically. For those sessions that are not automatically released, you need to delete them manually (TODO: coding). SHOW SESSIONS queries the session information of all the Graph services. SHOW LOCAL SESSIONS queries the session information of the currently connected Graph service and does not query the session information of other Graph services. SHOW SESSION <Session_Id> queries the session information with a specific session id.","title":"Precautions"},{"location":"3.ngql-guide/7.general-query-statements/6.show/17.show-sessions/#syntax","text":"SHOW [LOCAL] SESSIONS; SHOW SESSION <Session_Id>;","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/6.show/17.show-sessions/#examples","text":"nebula> SHOW SESSIONS; +------------------+----------+--------------------+----------------------------+----------------------------+------------------+----------+--------------------+ | SessionId | UserName | SpaceName | CreateTime | UpdateTime | GraphAddr | Timezone | ClientIp | +------------------+----------+--------------------+----------------------------+----------------------------+------------------+----------+--------------------+ | 1635128818397714 | \"root\" | \"test\" | 2021-10-25T02:26:58.397714 | 2021-10-25T08:31:31.846846 | \"127.0.0.1:9669\" | 0 | \"::ffff:127.0.0.1\" | | 1635254859271703 | \"root\" | \"basketballplayer\" | 2021-10-26T13:27:39.271703 | 2021-10-26T13:51:38.277704 | \"127.0.0.1:9669\" | 0 | \"::ffff:127.0.0.1\" | | 1634871229727322 | \"root\" | \"basketballplayer\" | 2021-10-22T02:53:49.727322 | 2021-10-22T02:53:56.564001 | \"127.0.0.1:9669\" | 0 | \"::ffff:127.0.0.1\" | | 1635750725840229 | \"root\" | \"basketballplayer\" | 2021-11-01T07:12:05.840229 | 2021-11-01T09:42:36.883617 | \"127.0.0.1:9669\" | 0 | \"::ffff:127.0.0.1\" | | 1635299224732060 | \"root\" | \"basketballplayer\" | 2021-10-27T01:47:04.732060 | 2021-10-27T09:04:31.741126 | \"127.0.0.1:9669\" | 0 | \"::ffff:127.0.0.1\" | | 1634628999765689 | \"root\" | \"\" | 2021-10-19T07:36:39.765689 | 2021-10-19T07:36:39.768064 | \"127.0.0.1:9669\" | 0 | \"::ffff:127.0.0.1\" | | 1634886296595136 | \"root\" | \"basketballplayer\" | 2021-10-22T07:04:56.595136 | 2021-10-22T09:48:20.299364 | \"127.0.0.1:9669\" | 0 | \"::ffff:127.0.0.1\" | | 1634629179882439 | \"root\" | \"basketballplayer\" | 2021-10-19T07:39:39.882439 | 2021-10-19T09:34:52.153145 | \"127.0.0.1:9669\" | 0 | \"::ffff:127.0.0.1\" | | 1635246158961634 | \"root\" | \"basketballplayer\" | 2021-10-26T11:02:38.961634 | 2021-10-26T11:02:51.250897 | \"127.0.0.1:9669\" | 0 | \"::ffff:127.0.0.1\" | | 1634785346839017 | \"root\" | \"basketballplayer\" | 2021-10-21T03:02:26.839017 | 2021-10-21T11:07:40.911329 | \"127.0.0.1:9669\" | 0 | \"::ffff:127.0.0.1\" | +------------------+----------+--------------------+----------------------------+----------------------------+------------------+----------+--------------------+ nebula> SHOW SESSION 1635254859271703; +--------------+----------------------------+ | VariableName | Value | +--------------+----------------------------+ | \"SessionID\" | 1635254859271703 | | \"UserName\" | \"root\" | | \"SpaceName\" | \"basketballplayer\" | | \"CreateTime\" | 2021-10-26T13:27:39.271703 | | \"UpdateTime\" | 2021-10-26T13:51:38.277704 | | \"GraphAddr\" | \"127.0.0.1:9669\" | | \"Timezone\" | 0 | | \"ClientIp\" | \"::ffff:127.0.0.1\" | +--------------+----------------------------+ Parameter Description SessionId The session ID, namely the identifier of a session. UserName The username in a session. SpaceName The name of the graph space that the user uses currently. It is null ( \"\" ) when you first log in because there is no specified graph space. CreateTime The time when the session is created, namely the time when the user logs in. The time zone is specified by timezone_name in the configuration file. UpdateTime The system will update the time when there is an operation. The time zone is specified by timezone_name in the configuration file. GraphAddr The IP address and port of the Graph server that hosts the session. Timezone A reserved parameter that has no specified meaning for now. ClientIp The IP address of the client.","title":"Examples"},{"location":"3.ngql-guide/7.general-query-statements/6.show/18.show-queries/","text":"SHOW QUERIES \u00b6 The SHOW QUERIES statement shows the information of working queries in the current session. Note To terminate queries, see Kill Query . Precautions \u00b6 The SHOW LOCAL QUERIES statement gets the status of queries in the current session from the local cache with almost no latency. The SHOW QUERIES statement gets the information of queries in all the sessions from the Meta Service. The information will be synchronized to the Meta Service according to the interval defined by session_reclaim_interval_secs . Therefore the information that you get from the client may belong to the last synchronization interval. Syntax \u00b6 SHOW [LOCAL] QUERIES; Examples \u00b6 nebula> SHOW LOCAL QUERIES; +------------------+-----------------+--------+----------------------+----------------------------+----------------+-----------+-----------------+ | SessionID | ExecutionPlanID | User | Host | StartTime | DurationInUSec | Status | Query | +------------------+-----------------+--------+----------------------+----------------------------+----------------+-----------+-----------------+ | 1625463842921750 | 46 | \"root\" | \"\"192.168.x.x\":9669\" | 2021-07-05T05:44:19.502903 | 0 | \"RUNNING\" | \"SHOW QUERIES;\" | +------------------+-----------------+--------+----------------------+----------------------------+----------------+-----------+-----------------+ nebula> SHOW QUERIES; +------------------+-----------------+---------+----------------------+----------------------------+----------------+-----------+---------------------------------------------------------+ | SessionID | ExecutionPlanID | User | Host | StartTime | DurationInUSec | Status | Query | +------------------+-----------------+---------+----------------------+----------------------------+----------------+-----------+---------------------------------------------------------+ | 1625456037718757 | 54 | \"user1\" | \"\"192.168.x.x\":9669\" | 2021-07-05T05:51:08.691318 | 1504502 | \"RUNNING\" | \"MATCH p=(v:player)-[*1..4]-(v2) RETURN v2 AS Friends;\" | +------------------+-----------------+---------+----------------------+----------------------------+----------------+-----------+---------------------------------------------------------+ # The following statement returns the top 10 queries that have the longest duration. nebula> SHOW QUERIES | ORDER BY $-.DurationInUSec DESC | LIMIT 10; +------------------+-----------------+---------+----------------------+----------------------------+----------------+-----------+-------------------------------------------------------+ | SessionID | ExecutionPlanID | User | Host | StartTime | DurationInUSec | Status | Query | +------------------+-----------------+---------+----------------------+----------------------------+----------------+-----------+-------------------------------------------------------+ | 1625471375320831 | 98 | \"user2\" | \"\"192.168.x.x\":9669\" | 2021-07-05T07:50:24.461779 | 2608176 | \"RUNNING\" | \"MATCH (v:player)-[*1..4]-(v2) RETURN v2 AS Friends;\" | | 1625456037718757 | 99 | \"user1\" | \"\"192.168.x.x\":9669\" | 2021-07-05T07:50:24.910616 | 2159333 | \"RUNNING\" | \"MATCH (v:player)-[*1..4]-(v2) RETURN v2 AS Friends;\" | +------------------+-----------------+---------+----------------------+----------------------------+----------------+-----------+-------------------------------------------------------+ The descriptions are as follows. Parameter Description SessionID The session ID. ExecutionPlanID The ID of the execution plan. User The username that executes the query. Host The IP address and port of the Graph server that hosts the session. StartTime The time when the query starts. DurationInUSec The duration of the query. The unit is microsecond. Status The current status of the query. Query The query statement.","title":"SHOW QUERIES"},{"location":"3.ngql-guide/7.general-query-statements/6.show/18.show-queries/#show_queries","text":"The SHOW QUERIES statement shows the information of working queries in the current session. Note To terminate queries, see Kill Query .","title":"SHOW QUERIES"},{"location":"3.ngql-guide/7.general-query-statements/6.show/18.show-queries/#precautions","text":"The SHOW LOCAL QUERIES statement gets the status of queries in the current session from the local cache with almost no latency. The SHOW QUERIES statement gets the information of queries in all the sessions from the Meta Service. The information will be synchronized to the Meta Service according to the interval defined by session_reclaim_interval_secs . Therefore the information that you get from the client may belong to the last synchronization interval.","title":"Precautions"},{"location":"3.ngql-guide/7.general-query-statements/6.show/18.show-queries/#syntax","text":"SHOW [LOCAL] QUERIES;","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/6.show/18.show-queries/#examples","text":"nebula> SHOW LOCAL QUERIES; +------------------+-----------------+--------+----------------------+----------------------------+----------------+-----------+-----------------+ | SessionID | ExecutionPlanID | User | Host | StartTime | DurationInUSec | Status | Query | +------------------+-----------------+--------+----------------------+----------------------------+----------------+-----------+-----------------+ | 1625463842921750 | 46 | \"root\" | \"\"192.168.x.x\":9669\" | 2021-07-05T05:44:19.502903 | 0 | \"RUNNING\" | \"SHOW QUERIES;\" | +------------------+-----------------+--------+----------------------+----------------------------+----------------+-----------+-----------------+ nebula> SHOW QUERIES; +------------------+-----------------+---------+----------------------+----------------------------+----------------+-----------+---------------------------------------------------------+ | SessionID | ExecutionPlanID | User | Host | StartTime | DurationInUSec | Status | Query | +------------------+-----------------+---------+----------------------+----------------------------+----------------+-----------+---------------------------------------------------------+ | 1625456037718757 | 54 | \"user1\" | \"\"192.168.x.x\":9669\" | 2021-07-05T05:51:08.691318 | 1504502 | \"RUNNING\" | \"MATCH p=(v:player)-[*1..4]-(v2) RETURN v2 AS Friends;\" | +------------------+-----------------+---------+----------------------+----------------------------+----------------+-----------+---------------------------------------------------------+ # The following statement returns the top 10 queries that have the longest duration. nebula> SHOW QUERIES | ORDER BY $-.DurationInUSec DESC | LIMIT 10; +------------------+-----------------+---------+----------------------+----------------------------+----------------+-----------+-------------------------------------------------------+ | SessionID | ExecutionPlanID | User | Host | StartTime | DurationInUSec | Status | Query | +------------------+-----------------+---------+----------------------+----------------------------+----------------+-----------+-------------------------------------------------------+ | 1625471375320831 | 98 | \"user2\" | \"\"192.168.x.x\":9669\" | 2021-07-05T07:50:24.461779 | 2608176 | \"RUNNING\" | \"MATCH (v:player)-[*1..4]-(v2) RETURN v2 AS Friends;\" | | 1625456037718757 | 99 | \"user1\" | \"\"192.168.x.x\":9669\" | 2021-07-05T07:50:24.910616 | 2159333 | \"RUNNING\" | \"MATCH (v:player)-[*1..4]-(v2) RETURN v2 AS Friends;\" | +------------------+-----------------+---------+----------------------+----------------------------+----------------+-----------+-------------------------------------------------------+ The descriptions are as follows. Parameter Description SessionID The session ID. ExecutionPlanID The ID of the execution plan. User The username that executes the query. Host The IP address and port of the Graph server that hosts the session. StartTime The time when the query starts. DurationInUSec The duration of the query. The unit is microsecond. Status The current status of the query. Query The query statement.","title":"Examples"},{"location":"3.ngql-guide/7.general-query-statements/6.show/19.show-meta-leader/","text":"SHOW META LEADER \u00b6 The SHOW META LEADER statement shows the information of the leader in the current Meta cluster. For more information about the Meta service, see Meta service . Syntax \u00b6 SHOW META LEADER; Example \u00b6 nebula> SHOW META LEADER; +------------------+---------------------------+ | Meta Leader | secs from last heart beat | +------------------+---------------------------+ | \"127.0.0.1:9559\" | 3 | +------------------+---------------------------+ Parameter Description Meta Leader Shows the information of the leader in the Meta cluster, including the IP address and port of the server where the leader is located. secs from last heart beat Indicates the time interval since the last heartbeat. This parameter is measured in seconds.","title":"SHOW META LEADER"},{"location":"3.ngql-guide/7.general-query-statements/6.show/19.show-meta-leader/#show_meta_leader","text":"The SHOW META LEADER statement shows the information of the leader in the current Meta cluster. For more information about the Meta service, see Meta service .","title":"SHOW META LEADER"},{"location":"3.ngql-guide/7.general-query-statements/6.show/19.show-meta-leader/#syntax","text":"SHOW META LEADER;","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/6.show/19.show-meta-leader/#example","text":"nebula> SHOW META LEADER; +------------------+---------------------------+ | Meta Leader | secs from last heart beat | +------------------+---------------------------+ | \"127.0.0.1:9559\" | 3 | +------------------+---------------------------+ Parameter Description Meta Leader Shows the information of the leader in the Meta cluster, including the IP address and port of the server where the leader is located. secs from last heart beat Indicates the time interval since the last heartbeat. This parameter is measured in seconds.","title":"Example"},{"location":"3.ngql-guide/7.general-query-statements/6.show/2.show-collation/","text":"SHOW COLLATION \u00b6 The SHOW COLLATION statement shows the collations supported by Nebula Graph. Currently available types are: utf8_bin , utf8_general_ci , utf8mb4_bin , and utf8mb4_general_ci . When the character set is utf8 , the default collate is utf8_bin . When the character set is utf8mb4 , the default collate is utf8mb4_bin . Both utf8mb4_bin and utf8mb4_general_ci are case-insensitive. Syntax \u00b6 SHOW COLLATION; Example \u00b6 nebula> SHOW COLLATION; +------------+---------+ | Collation | Charset | +------------+---------+ | \"utf8_bin\" | \"utf8\" | +------------+---------+ Parameter Description Collation The name of the collation. Charset The name of the character set with which the collation is associated.","title":"SHOW COLLATION"},{"location":"3.ngql-guide/7.general-query-statements/6.show/2.show-collation/#show_collation","text":"The SHOW COLLATION statement shows the collations supported by Nebula Graph. Currently available types are: utf8_bin , utf8_general_ci , utf8mb4_bin , and utf8mb4_general_ci . When the character set is utf8 , the default collate is utf8_bin . When the character set is utf8mb4 , the default collate is utf8mb4_bin . Both utf8mb4_bin and utf8mb4_general_ci are case-insensitive.","title":"SHOW COLLATION"},{"location":"3.ngql-guide/7.general-query-statements/6.show/2.show-collation/#syntax","text":"SHOW COLLATION;","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/6.show/2.show-collation/#example","text":"nebula> SHOW COLLATION; +------------+---------+ | Collation | Charset | +------------+---------+ | \"utf8_bin\" | \"utf8\" | +------------+---------+ Parameter Description Collation The name of the collation. Charset The name of the character set with which the collation is associated.","title":"Example"},{"location":"3.ngql-guide/7.general-query-statements/6.show/4.show-create-space/","text":"SHOW CREATE SPACE \u00b6 The SHOW CREATE SPACE statement shows the creating statement of the specified graph space. For details about the graph space information, see CREATE SPACE . Syntax \u00b6 SHOW CREATE SPACE <space_name>; Example \u00b6 nebula> SHOW CREATE SPACE basketballplayer; +--------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+ | Space | Create Space | +--------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+ | \"basketballplayer\" | \"CREATE SPACE `basketballplayer` (partition_num = 10, replica_factor = 1, charset = utf8, collate = utf8_bin, vid_type = FIXED_STRING(32)) ON default\" | +--------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+","title":"SHOW CREATE SPACE"},{"location":"3.ngql-guide/7.general-query-statements/6.show/4.show-create-space/#show_create_space","text":"The SHOW CREATE SPACE statement shows the creating statement of the specified graph space. For details about the graph space information, see CREATE SPACE .","title":"SHOW CREATE SPACE"},{"location":"3.ngql-guide/7.general-query-statements/6.show/4.show-create-space/#syntax","text":"SHOW CREATE SPACE <space_name>;","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/6.show/4.show-create-space/#example","text":"nebula> SHOW CREATE SPACE basketballplayer; +--------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+ | Space | Create Space | +--------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+ | \"basketballplayer\" | \"CREATE SPACE `basketballplayer` (partition_num = 10, replica_factor = 1, charset = utf8, collate = utf8_bin, vid_type = FIXED_STRING(32)) ON default\" | +--------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+","title":"Example"},{"location":"3.ngql-guide/7.general-query-statements/6.show/5.show-create-tag-edge/","text":"SHOW CREATE TAG/EDGE \u00b6 The SHOW CREATE TAG statement shows the basic information of the specified tag. For details about the tag, see CREATE TAG . The SHOW CREATE EDGE statement shows the basic information of the specified edge type. For details about the edge type, see CREATE EDGE . Syntax \u00b6 SHOW CREATE {TAG <tag_name> | EDGE <edge_name>}; Examples \u00b6 nebula> SHOW CREATE TAG player; +----------+-----------------------------------+ | Tag | Create Tag | +----------+-----------------------------------+ | \"player\" | \"CREATE TAG `player` ( | | | `name` string NULL, | | | `age` int64 NULL | | | ) ttl_duration = 0, ttl_col = \"\"\" | +----------+-----------------------------------+ nebula> SHOW CREATE EDGE follow; +----------+-----------------------------------+ | Edge | Create Edge | +----------+-----------------------------------+ | \"follow\" | \"CREATE EDGE `follow` ( | | | `degree` int64 NULL | | | ) ttl_duration = 0, ttl_col = \"\"\" | +----------+-----------------------------------+","title":"SHOW CREATE TAG/EDGE"},{"location":"3.ngql-guide/7.general-query-statements/6.show/5.show-create-tag-edge/#show_create_tagedge","text":"The SHOW CREATE TAG statement shows the basic information of the specified tag. For details about the tag, see CREATE TAG . The SHOW CREATE EDGE statement shows the basic information of the specified edge type. For details about the edge type, see CREATE EDGE .","title":"SHOW CREATE TAG/EDGE"},{"location":"3.ngql-guide/7.general-query-statements/6.show/5.show-create-tag-edge/#syntax","text":"SHOW CREATE {TAG <tag_name> | EDGE <edge_name>};","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/6.show/5.show-create-tag-edge/#examples","text":"nebula> SHOW CREATE TAG player; +----------+-----------------------------------+ | Tag | Create Tag | +----------+-----------------------------------+ | \"player\" | \"CREATE TAG `player` ( | | | `name` string NULL, | | | `age` int64 NULL | | | ) ttl_duration = 0, ttl_col = \"\"\" | +----------+-----------------------------------+ nebula> SHOW CREATE EDGE follow; +----------+-----------------------------------+ | Edge | Create Edge | +----------+-----------------------------------+ | \"follow\" | \"CREATE EDGE `follow` ( | | | `degree` int64 NULL | | | ) ttl_duration = 0, ttl_col = \"\"\" | +----------+-----------------------------------+","title":"Examples"},{"location":"3.ngql-guide/7.general-query-statements/6.show/6.show-hosts/","text":"SHOW HOSTS \u00b6 The SHOW HOSTS statement shows the cluster information, including the port, status, leader, partition, and version information. You can also add the service type in the statement to view the information of the specific service. Syntax \u00b6 SHOW HOSTS [GRAPH | STORAGE | META]; Note For a Nebula Graph cluster installed with the source code, the version of the cluster will not be displayed in the output after executing the command SHOW HOSTS (GRAPH | STORAGE | META) with the service name. Examples \u00b6 nebula> SHOW HOSTS; +-------------+-------+-----------+----------+--------------+----------------------------------+------------------------------+---------+ | Host | Port | HTTP port | Status | Leader count | Leader distribution | Partition distribution | Version | +-------------+-------+-----------+----------+--------------+----------------------------------+------------------------------+---------+ | \"storaged0\" | 9779 | 19669 | \"ONLINE\" | 8 | \"docs:5, basketballplayer:3\" | \"docs:5, basketballplayer:3\" | \"3.1.0\" | | \"storaged1\" | 9779 | 19669 | \"ONLINE\" | 9 | \"basketballplayer:4, docs:5\" | \"docs:5, basketballplayer:4\" | \"3.1.0\" | | \"storaged2\" | 9779 | 19669 | \"ONLINE\" | 8 | \"basketballplayer:3, docs:5\" | \"docs:5, basketballplayer:3\" | \"3.1.0\" | +-------------+-------+-----------+----------+--------------+----------------------------------+------------------------------+---------+ nebula> SHOW HOSTS GRAPH; +-----------+------+----------+---------+--------------+---------+ | Host | Port | Status | Role | Git Info Sha | Version | +-----------+------+----------+---------+--------------+---------+ | \"graphd\" | 9669 | \"ONLINE\" | \"GRAPH\" | \"3ba41bd\" | \"3.1.0\" | | \"graphd1\" | 9669 | \"ONLINE\" | \"GRAPH\" | \"3ba41bd\" | \"3.1.0\" | | \"graphd2\" | 9669 | \"ONLINE\" | \"GRAPH\" | \"3ba41bd\" | \"3.1.0\" | +-----------+------+----------+---------+--------------+---------+ nebula> SHOW HOSTS STORAGE; +-------------+------+----------+-----------+--------------+---------+ | Host | Port | Status | Role | Git Info Sha | Version | +-------------+------+----------+-----------+--------------+---------+ | \"storaged0\" | 9779 | \"ONLINE\" | \"STORAGE\" | \"3ba41bd\" | \"3.1.0\" | | \"storaged1\" | 9779 | \"ONLINE\" | \"STORAGE\" | \"3ba41bd\" | \"3.1.0\" | | \"storaged2\" | 9779 | \"ONLINE\" | \"STORAGE\" | \"3ba41bd\" | \"3.1.0\" | +-------------+------+----------+-----------+--------------+---------+ nebula> SHOW HOSTS META; +----------+------+----------+--------+--------------+---------+ | Host | Port | Status | Role | Git Info Sha | Version | +----------+------+----------+--------+--------------+---------+ | \"metad2\" | 9559 | \"ONLINE\" | \"META\" | \"3ba41bd\" | \"3.1.0\" | | \"metad0\" | 9559 | \"ONLINE\" | \"META\" | \"3ba41bd\" | \"3.1.0\" | | \"metad1\" | 9559 | \"ONLINE\" | \"META\" | \"3ba41bd\" | \"3.1.0\" | +----------+------+----------+--------+--------------+---------+","title":"SHOW HOSTS"},{"location":"3.ngql-guide/7.general-query-statements/6.show/6.show-hosts/#show_hosts","text":"The SHOW HOSTS statement shows the cluster information, including the port, status, leader, partition, and version information. You can also add the service type in the statement to view the information of the specific service.","title":"SHOW HOSTS"},{"location":"3.ngql-guide/7.general-query-statements/6.show/6.show-hosts/#syntax","text":"SHOW HOSTS [GRAPH | STORAGE | META]; Note For a Nebula Graph cluster installed with the source code, the version of the cluster will not be displayed in the output after executing the command SHOW HOSTS (GRAPH | STORAGE | META) with the service name.","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/6.show/6.show-hosts/#examples","text":"nebula> SHOW HOSTS; +-------------+-------+-----------+----------+--------------+----------------------------------+------------------------------+---------+ | Host | Port | HTTP port | Status | Leader count | Leader distribution | Partition distribution | Version | +-------------+-------+-----------+----------+--------------+----------------------------------+------------------------------+---------+ | \"storaged0\" | 9779 | 19669 | \"ONLINE\" | 8 | \"docs:5, basketballplayer:3\" | \"docs:5, basketballplayer:3\" | \"3.1.0\" | | \"storaged1\" | 9779 | 19669 | \"ONLINE\" | 9 | \"basketballplayer:4, docs:5\" | \"docs:5, basketballplayer:4\" | \"3.1.0\" | | \"storaged2\" | 9779 | 19669 | \"ONLINE\" | 8 | \"basketballplayer:3, docs:5\" | \"docs:5, basketballplayer:3\" | \"3.1.0\" | +-------------+-------+-----------+----------+--------------+----------------------------------+------------------------------+---------+ nebula> SHOW HOSTS GRAPH; +-----------+------+----------+---------+--------------+---------+ | Host | Port | Status | Role | Git Info Sha | Version | +-----------+------+----------+---------+--------------+---------+ | \"graphd\" | 9669 | \"ONLINE\" | \"GRAPH\" | \"3ba41bd\" | \"3.1.0\" | | \"graphd1\" | 9669 | \"ONLINE\" | \"GRAPH\" | \"3ba41bd\" | \"3.1.0\" | | \"graphd2\" | 9669 | \"ONLINE\" | \"GRAPH\" | \"3ba41bd\" | \"3.1.0\" | +-----------+------+----------+---------+--------------+---------+ nebula> SHOW HOSTS STORAGE; +-------------+------+----------+-----------+--------------+---------+ | Host | Port | Status | Role | Git Info Sha | Version | +-------------+------+----------+-----------+--------------+---------+ | \"storaged0\" | 9779 | \"ONLINE\" | \"STORAGE\" | \"3ba41bd\" | \"3.1.0\" | | \"storaged1\" | 9779 | \"ONLINE\" | \"STORAGE\" | \"3ba41bd\" | \"3.1.0\" | | \"storaged2\" | 9779 | \"ONLINE\" | \"STORAGE\" | \"3ba41bd\" | \"3.1.0\" | +-------------+------+----------+-----------+--------------+---------+ nebula> SHOW HOSTS META; +----------+------+----------+--------+--------------+---------+ | Host | Port | Status | Role | Git Info Sha | Version | +----------+------+----------+--------+--------------+---------+ | \"metad2\" | 9559 | \"ONLINE\" | \"META\" | \"3ba41bd\" | \"3.1.0\" | | \"metad0\" | 9559 | \"ONLINE\" | \"META\" | \"3ba41bd\" | \"3.1.0\" | | \"metad1\" | 9559 | \"ONLINE\" | \"META\" | \"3ba41bd\" | \"3.1.0\" | +----------+------+----------+--------+--------------+---------+","title":"Examples"},{"location":"3.ngql-guide/7.general-query-statements/6.show/7.show-index-status/","text":"SHOW INDEX STATUS \u00b6 The SHOW INDEX STATUS statement shows the status of jobs that rebuild native indexes, which helps check whether a native index is successfully rebuilt or not. Syntax \u00b6 SHOW {TAG | EDGE} INDEX STATUS; Examples \u00b6 nebula> SHOW TAG INDEX STATUS; +------------------------------------+--------------+ | Name | Index Status | +------------------------------------+--------------+ | \"date1_index\" | \"FINISHED\" | | \"basketballplayer_all_tag_indexes\" | \"FINISHED\" | | \"any_shape_geo_index\" | \"FINISHED\" | +------------------------------------+--------------+ nebula> SHOW EDGE INDEX STATUS; +----------------+--------------+ | Name | Index Status | +----------------+--------------+ | \"follow_index\" | \"FINISHED\" | +----------------+--------------+ Related topics \u00b6 Job manager and the JOB statements REBUILD NATIVE INDEX","title":"SHOW INDEX STATUS"},{"location":"3.ngql-guide/7.general-query-statements/6.show/7.show-index-status/#show_index_status","text":"The SHOW INDEX STATUS statement shows the status of jobs that rebuild native indexes, which helps check whether a native index is successfully rebuilt or not.","title":"SHOW INDEX STATUS"},{"location":"3.ngql-guide/7.general-query-statements/6.show/7.show-index-status/#syntax","text":"SHOW {TAG | EDGE} INDEX STATUS;","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/6.show/7.show-index-status/#examples","text":"nebula> SHOW TAG INDEX STATUS; +------------------------------------+--------------+ | Name | Index Status | +------------------------------------+--------------+ | \"date1_index\" | \"FINISHED\" | | \"basketballplayer_all_tag_indexes\" | \"FINISHED\" | | \"any_shape_geo_index\" | \"FINISHED\" | +------------------------------------+--------------+ nebula> SHOW EDGE INDEX STATUS; +----------------+--------------+ | Name | Index Status | +----------------+--------------+ | \"follow_index\" | \"FINISHED\" | +----------------+--------------+","title":"Examples"},{"location":"3.ngql-guide/7.general-query-statements/6.show/7.show-index-status/#related_topics","text":"Job manager and the JOB statements REBUILD NATIVE INDEX","title":"Related topics"},{"location":"3.ngql-guide/7.general-query-statements/6.show/8.show-indexes/","text":"SHOW INDEXES \u00b6 The SHOW INDEXES statement shows the names of existing native indexes. Syntax \u00b6 SHOW {TAG | EDGE} INDEXES; Examples \u00b6 nebula> SHOW TAG INDEXES; +------------------+--------------+-----------------+ | Index Name | By Tag | Columns | +------------------+--------------+-----------------+ | \"fix\" | \"fix_string\" | [\"p1\"] | | \"player_index_0\" | \"player\" | [\"name\"] | | \"player_index_1\" | \"player\" | [\"name\", \"age\"] | | \"var\" | \"var_string\" | [\"p1\"] | +------------------+--------------+-----------------+ nebula> SHOW EDGE INDEXES; +----------------+----------+---------+ | Index Name | By Edge | Columns | +----------------+----------+---------+ | \"follow_index\" | \"follow\" | [] | +----------------+----------+---------+ Legacy version compatibility In Nebula Graph 2.x, SHOW TAG/EDGE INDEXES only returns Names .","title":"SHOW INDEXES"},{"location":"3.ngql-guide/7.general-query-statements/6.show/8.show-indexes/#show_indexes","text":"The SHOW INDEXES statement shows the names of existing native indexes.","title":"SHOW INDEXES"},{"location":"3.ngql-guide/7.general-query-statements/6.show/8.show-indexes/#syntax","text":"SHOW {TAG | EDGE} INDEXES;","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/6.show/8.show-indexes/#examples","text":"nebula> SHOW TAG INDEXES; +------------------+--------------+-----------------+ | Index Name | By Tag | Columns | +------------------+--------------+-----------------+ | \"fix\" | \"fix_string\" | [\"p1\"] | | \"player_index_0\" | \"player\" | [\"name\"] | | \"player_index_1\" | \"player\" | [\"name\", \"age\"] | | \"var\" | \"var_string\" | [\"p1\"] | +------------------+--------------+-----------------+ nebula> SHOW EDGE INDEXES; +----------------+----------+---------+ | Index Name | By Edge | Columns | +----------------+----------+---------+ | \"follow_index\" | \"follow\" | [] | +----------------+----------+---------+ Legacy version compatibility In Nebula Graph 2.x, SHOW TAG/EDGE INDEXES only returns Names .","title":"Examples"},{"location":"3.ngql-guide/7.general-query-statements/6.show/9.show-parts/","text":"SHOW PARTS \u00b6 The SHOW PARTS statement shows the information of a specified partition or all partitions in a graph space. Syntax \u00b6 SHOW PARTS [<part_id>]; Examples \u00b6 nebula> SHOW PARTS; +--------------+--------------------+--------------------+-------+ | Partition ID | Leader | Peers | Losts | +--------------+--------------------+--------------------+-------+ | 1 | \"192.168.2.1:9779\" | \"192.168.2.1:9779\" | \"\" | | 2 | \"192.168.2.2:9779\" | \"192.168.2.2:9779\" | \"\" | | 3 | \"192.168.2.3:9779\" | \"192.168.2.3:9779\" | \"\" | | 4 | \"192.168.2.1:9779\" | \"192.168.2.1:9779\" | \"\" | | 5 | \"192.168.2.2:9779\" | \"192.168.2.2:9779\" | \"\" | | 6 | \"192.168.2.3:9779\" | \"192.168.2.3:9779\" | \"\" | | 7 | \"192.168.2.1:9779\" | \"192.168.2.1:9779\" | \"\" | | 8 | \"192.168.2.2:9779\" | \"192.168.2.2:9779\" | \"\" | | 9 | \"192.168.2.3:9779\" | \"192.168.2.3:9779\" | \"\" | | 10 | \"192.168.2.1:9779\" | \"192.168.2.1:9779\" | \"\" | +--------------+--------------------+--------------------+-------+ nebula> SHOW PARTS 1; +--------------+--------------------+--------------------+-------+ | Partition ID | Leader | Peers | Losts | +--------------+--------------------+--------------------+-------+ | 1 | \"192.168.2.1:9779\" | \"192.168.2.1:9779\" | \"\" | +--------------+--------------------+--------------------+-------+ The descriptions are as follows. Parameter Description Partition ID The ID of the partition. Leader The IP address and the port of the leader. Peers The IP addresses and the ports of all the replicas. Losts The IP addresses and the ports of replicas at fault.","title":"SHOW PARTS"},{"location":"3.ngql-guide/7.general-query-statements/6.show/9.show-parts/#show_parts","text":"The SHOW PARTS statement shows the information of a specified partition or all partitions in a graph space.","title":"SHOW PARTS"},{"location":"3.ngql-guide/7.general-query-statements/6.show/9.show-parts/#syntax","text":"SHOW PARTS [<part_id>];","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/6.show/9.show-parts/#examples","text":"nebula> SHOW PARTS; +--------------+--------------------+--------------------+-------+ | Partition ID | Leader | Peers | Losts | +--------------+--------------------+--------------------+-------+ | 1 | \"192.168.2.1:9779\" | \"192.168.2.1:9779\" | \"\" | | 2 | \"192.168.2.2:9779\" | \"192.168.2.2:9779\" | \"\" | | 3 | \"192.168.2.3:9779\" | \"192.168.2.3:9779\" | \"\" | | 4 | \"192.168.2.1:9779\" | \"192.168.2.1:9779\" | \"\" | | 5 | \"192.168.2.2:9779\" | \"192.168.2.2:9779\" | \"\" | | 6 | \"192.168.2.3:9779\" | \"192.168.2.3:9779\" | \"\" | | 7 | \"192.168.2.1:9779\" | \"192.168.2.1:9779\" | \"\" | | 8 | \"192.168.2.2:9779\" | \"192.168.2.2:9779\" | \"\" | | 9 | \"192.168.2.3:9779\" | \"192.168.2.3:9779\" | \"\" | | 10 | \"192.168.2.1:9779\" | \"192.168.2.1:9779\" | \"\" | +--------------+--------------------+--------------------+-------+ nebula> SHOW PARTS 1; +--------------+--------------------+--------------------+-------+ | Partition ID | Leader | Peers | Losts | +--------------+--------------------+--------------------+-------+ | 1 | \"192.168.2.1:9779\" | \"192.168.2.1:9779\" | \"\" | +--------------+--------------------+--------------------+-------+ The descriptions are as follows. Parameter Description Partition ID The ID of the partition. Leader The IP address and the port of the leader. Peers The IP addresses and the ports of all the replicas. Losts The IP addresses and the ports of replicas at fault.","title":"Examples"},{"location":"3.ngql-guide/8.clauses-and-options/group-by/","text":"GROUP BY \u00b6 The GROUP BY clause can be used to aggregate data. OpenCypher Compatibility \u00b6 This topic applies to native nGQL only. You can also use the count() function to aggregate data. nebula> MATCH (v:player)<-[:follow]-(:player) RETURN v.player.name AS Name, count(*) as cnt ORDER BY cnt DESC; +----------------------+-----+ | Name | cnt | +----------------------+-----+ | \"Tim Duncan\" | 10 | | \"LeBron James\" | 6 | | \"Tony Parker\" | 5 | | \"Chris Paul\" | 4 | | \"Manu Ginobili\" | 4 | +----------------------+-----+ ... Syntax \u00b6 The GROUP BY clause groups the rows with the same value. Then operations such as counting, sorting, and calculation can be applied. The GROUP BY clause works after the pipe symbol (|) and before a YIELD clause. | GROUP BY <var> YIELD <var>, <aggregation_function(var)> The aggregation_function() function supports avg() , sum() , max() , min() , count() , collect() , and std() . Examples \u00b6 The following statement finds all the vertices connected directly to vertex \"player100\" , groups the result set by player names, and counts how many times the name shows up in the result set. nebula> GO FROM \"player100\" OVER follow BIDIRECT \\ YIELD properties($$).name as Name \\ | GROUP BY $-.Name \\ YIELD $-.Name as Player, count(*) AS Name_Count; +---------------------+------------+ | Player | Name_Count | +---------------------+------------+ | \"Shaquille O'Neal\" | 1 | | \"Tiago Splitter\" | 1 | | \"Manu Ginobili\" | 2 | | \"Boris Diaw\" | 1 | | \"LaMarcus Aldridge\" | 1 | | \"Tony Parker\" | 2 | | \"Marco Belinelli\" | 1 | | \"Dejounte Murray\" | 1 | | \"Danny Green\" | 1 | | \"Aron Baynes\" | 1 | +---------------------+------------+ Group and calculate with functions \u00b6 The following statement finds all the vertices connected directly to vertex \"player100\" , groups the result set by source vertices, and returns the sum of degree values. nebula> GO FROM \"player100\" OVER follow \\ YIELD src(edge) AS player, properties(edge).degree AS degree \\ | GROUP BY $-.player \\ YIELD sum($-.degree); +----------------+ | sum($-.degree) | +----------------+ | 190 | +----------------+ For more information about the sum() function, see Built-in math functions .","title":"GROUP BY"},{"location":"3.ngql-guide/8.clauses-and-options/group-by/#group_by","text":"The GROUP BY clause can be used to aggregate data.","title":"GROUP BY"},{"location":"3.ngql-guide/8.clauses-and-options/group-by/#opencypher_compatibility","text":"This topic applies to native nGQL only. You can also use the count() function to aggregate data. nebula> MATCH (v:player)<-[:follow]-(:player) RETURN v.player.name AS Name, count(*) as cnt ORDER BY cnt DESC; +----------------------+-----+ | Name | cnt | +----------------------+-----+ | \"Tim Duncan\" | 10 | | \"LeBron James\" | 6 | | \"Tony Parker\" | 5 | | \"Chris Paul\" | 4 | | \"Manu Ginobili\" | 4 | +----------------------+-----+ ...","title":"OpenCypher Compatibility"},{"location":"3.ngql-guide/8.clauses-and-options/group-by/#syntax","text":"The GROUP BY clause groups the rows with the same value. Then operations such as counting, sorting, and calculation can be applied. The GROUP BY clause works after the pipe symbol (|) and before a YIELD clause. | GROUP BY <var> YIELD <var>, <aggregation_function(var)> The aggregation_function() function supports avg() , sum() , max() , min() , count() , collect() , and std() .","title":"Syntax"},{"location":"3.ngql-guide/8.clauses-and-options/group-by/#examples","text":"The following statement finds all the vertices connected directly to vertex \"player100\" , groups the result set by player names, and counts how many times the name shows up in the result set. nebula> GO FROM \"player100\" OVER follow BIDIRECT \\ YIELD properties($$).name as Name \\ | GROUP BY $-.Name \\ YIELD $-.Name as Player, count(*) AS Name_Count; +---------------------+------------+ | Player | Name_Count | +---------------------+------------+ | \"Shaquille O'Neal\" | 1 | | \"Tiago Splitter\" | 1 | | \"Manu Ginobili\" | 2 | | \"Boris Diaw\" | 1 | | \"LaMarcus Aldridge\" | 1 | | \"Tony Parker\" | 2 | | \"Marco Belinelli\" | 1 | | \"Dejounte Murray\" | 1 | | \"Danny Green\" | 1 | | \"Aron Baynes\" | 1 | +---------------------+------------+","title":"Examples"},{"location":"3.ngql-guide/8.clauses-and-options/group-by/#group_and_calculate_with_functions","text":"The following statement finds all the vertices connected directly to vertex \"player100\" , groups the result set by source vertices, and returns the sum of degree values. nebula> GO FROM \"player100\" OVER follow \\ YIELD src(edge) AS player, properties(edge).degree AS degree \\ | GROUP BY $-.player \\ YIELD sum($-.degree); +----------------+ | sum($-.degree) | +----------------+ | 190 | +----------------+ For more information about the sum() function, see Built-in math functions .","title":"Group and calculate with functions"},{"location":"3.ngql-guide/8.clauses-and-options/limit/","text":"LIMIT AND SKIP \u00b6 The LIMIT clause constrains the number of rows in the output. The usage of LIMIT in native nGQL statements and openCypher compatible statements is different. Native nGQL: Generally, a pipe | needs to be used before the LIMIT clause. The offset parameter can be set or omitted directly after the LIMIT statement. OpenCypher compatible statements: No pipes are permitted before the LIMIT clause. And you can use SKIP to indicate an offset. Note When using LIMIT in either syntax above, it is important to use an ORDER BY clause that constrains the output into a unique order. Otherwise, you will get an unpredictable subset of the output. Legacy version compatibility In Nebula Graph 2.6.0, GO statements support the new LIMIT syntax. Some operators related to LIMIT support computing pushdown. LIMIT in native nGQL statements \u00b6 In native nGQL, LIMIT has general syntax and exclusive syntax in GO statements. General LIMIT syntax in native nGQL statements \u00b6 In native nGQL, the general LIMIT syntax works the same as in SQL . The LIMIT clause accepts one or two parameters. The values of both parameters must be non-negative integers and be used after a pipe. The syntax and description are as follows: ... | LIMIT [<offset>,] <number_rows>; Parameter Description offset The offset value. It defines the row from which to start returning. The offset starts from 0 . The default value is 0 , which returns from the first row. number_rows It constrains the total number of returned rows. For example: # The following example returns the top 3 rows of data from the result. nebula> LOOKUP ON player YIELD id(vertex)|\\ LIMIT 3; +-------------+ | id(VERTEX) | +-------------+ | \"player100\" | | \"player101\" | | \"player102\" | +-------------+ # The following example returns the 3 rows of data starting from the second row of the sorted output. nebula> GO FROM \"player100\" OVER follow REVERSELY \\ YIELD properties($$).name AS Friend, properties($$).age AS Age \\ | ORDER BY $-.Age, $-.Friend \\ | LIMIT 1, 3; +-------------------+-----+ | Friend | Age | +-------------------+-----+ | \"Danny Green\" | 31 | | \"Aron Baynes\" | 32 | | \"Marco Belinelli\" | 32 | +-------------------+-----+ LIMIT in GO statements \u00b6 In addition to the general syntax in the native nGQL, the LIMIT in the GO statement also supports limiting the number of output results based on edges. Syntax: <go_statement> LIMIT <limit_list>; limit_list is a list. Elements in the list must be natural numbers, and the number of elements must be the same as the maximum number of STEPS in the GO statement. The following takes GO 1 TO 3 STEPS FROM \"A\" OVER * LIMIT <limit_list> as an example to introduce this usage of LIMIT in detail. The list limit_list must contain 3 natural numbers, such as GO 1 TO 3 STEPS FROM \"A\" OVER * LIMIT [1,2,4] . 1 in LIMIT [1,2,4] means that the system automatically selects 1 edge to continue traversal in the first step. 2 means to select 2 edges to continue traversal in the second step. 4 indicates that 4 edges are selected to continue traversal in the third step. Because GO 1 TO 3 STEPS means to return all the traversal results from the first to third steps, all the red edges and their source and destination vertices in the figure below will be matched by this GO statement. And the yellow edges represent there is no path selected when the GO statement traverses. If it is not GO 1 TO 3 STEPS but GO 3 STEPS , it will only match the red edges of the third step and the vertices at both ends. In the basketballplayer dataset, the example is as follows: nebula> GO 3 STEPS FROM \"player100\" \\ OVER * \\ YIELD properties($$).name AS NAME, properties($$).age AS Age \\ LIMIT [3,3,3]; +-----------------+--------------+ | NAME | Age | +-----------------+--------------+ | \"Spurs\" | UNKNOWN_PROP | | \"Tony Parker\" | 36 | | \"Manu Ginobili\" | 41 | +-----------------+--------------+ nebula> GO 3 STEPS FROM \"player102\" OVER * \\ YIELD dst(edge) \\ LIMIT [rand32(5),rand32(5),rand32(5)]; +-------------+ | dst(EDGE) | +-------------+ | \"team204\" | | \"team215\" | | \"player100\" | | \"player102\" | +-------------+ LIMIT in openCypher compatible statements \u00b6 In openCypher compatible statements such as MATCH , there is no need to use a pipe when LIMIT is used. The syntax and description are as follows: ... [SKIP <offset>] [LIMIT <number_rows>]; Parameter Description offset The offset value. It defines the row from which to start returning. The offset starts from 0 . The default value is 0 , which returns from the first row. number_rows It constrains the total number of returned rows. Both offset and number_rows accept expressions, but the result of the expression must be a non-negative integer. Note Fraction expressions composed of two integers are automatically floored to integers. For example, 8/6 is floored to 1. Examples of LIMIT \u00b6 LIMIT can be used alone to return a specified number of results. nebula> MATCH (v:player) RETURN v.player.name AS Name, v.player.age AS Age \\ ORDER BY Age LIMIT 5; +-------------------------+-----+ | Name | Age | +-------------------------+-----+ | \"Luka Doncic\" | 20 | | \"Ben Simmons\" | 22 | | \"Kristaps Porzingis\" | 23 | | \"Giannis Antetokounmpo\" | 24 | | \"Kyle Anderson\" | 25 | +-------------------------+-----+ nebula> MATCH (v:player) RETURN v.player.name AS Name, v.player.age AS Age \\ ORDER BY Age LIMIT rand32(5); +-------------------------+-----+ | Name | Age | +-------------------------+-----+ | \"Luka Doncic\" | 20 | | \"Ben Simmons\" | 22 | | \"Kristaps Porzingis\" | 23 | | \"Giannis Antetokounmpo\" | 24 | +-------------------------+-----+ Examples of SKIP \u00b6 SKIP can be used alone to set the offset and return the data after the specified position. nebula> MATCH (v:player{name:\"Tim Duncan\"}) --> (v2) \\ RETURN v2.player.name AS Name, v2.player.age AS Age \\ ORDER BY Age DESC SKIP 1; +-----------------+-----+ | Name | Age | +-----------------+-----+ | \"Manu Ginobili\" | 41 | | \"Tony Parker\" | 36 | +-----------------+-----+ nebula> MATCH (v:player{name:\"Tim Duncan\"}) --> (v2) \\ RETURN v2.player.name AS Name, v2.player.age AS Age \\ ORDER BY Age DESC SKIP 1+1; +---------------+-----+ | Name | Age | +---------------+-----+ | \"Tony Parker\" | 36 | +---------------+-----+ Example of SKIP and LIMIT \u00b6 SKIP and LIMIT can be used together to return the specified amount of data starting from the specified position. nebula> MATCH (v:player{name:\"Tim Duncan\"}) --> (v2) \\ RETURN v2.player.name AS Name, v2.player.age AS Age \\ ORDER BY Age DESC SKIP 1 LIMIT 1; +-----------------+-----+ | Name | Age | +-----------------+-----+ | \"Manu Ginobili\" | 41 | +-----------------+-----+","title":"LIMIT and SKIP"},{"location":"3.ngql-guide/8.clauses-and-options/limit/#limit_and_skip","text":"The LIMIT clause constrains the number of rows in the output. The usage of LIMIT in native nGQL statements and openCypher compatible statements is different. Native nGQL: Generally, a pipe | needs to be used before the LIMIT clause. The offset parameter can be set or omitted directly after the LIMIT statement. OpenCypher compatible statements: No pipes are permitted before the LIMIT clause. And you can use SKIP to indicate an offset. Note When using LIMIT in either syntax above, it is important to use an ORDER BY clause that constrains the output into a unique order. Otherwise, you will get an unpredictable subset of the output. Legacy version compatibility In Nebula Graph 2.6.0, GO statements support the new LIMIT syntax. Some operators related to LIMIT support computing pushdown.","title":"LIMIT AND SKIP"},{"location":"3.ngql-guide/8.clauses-and-options/limit/#limit_in_native_ngql_statements","text":"In native nGQL, LIMIT has general syntax and exclusive syntax in GO statements.","title":"LIMIT in native nGQL statements"},{"location":"3.ngql-guide/8.clauses-and-options/limit/#general_limit_syntax_in_native_ngql_statements","text":"In native nGQL, the general LIMIT syntax works the same as in SQL . The LIMIT clause accepts one or two parameters. The values of both parameters must be non-negative integers and be used after a pipe. The syntax and description are as follows: ... | LIMIT [<offset>,] <number_rows>; Parameter Description offset The offset value. It defines the row from which to start returning. The offset starts from 0 . The default value is 0 , which returns from the first row. number_rows It constrains the total number of returned rows. For example: # The following example returns the top 3 rows of data from the result. nebula> LOOKUP ON player YIELD id(vertex)|\\ LIMIT 3; +-------------+ | id(VERTEX) | +-------------+ | \"player100\" | | \"player101\" | | \"player102\" | +-------------+ # The following example returns the 3 rows of data starting from the second row of the sorted output. nebula> GO FROM \"player100\" OVER follow REVERSELY \\ YIELD properties($$).name AS Friend, properties($$).age AS Age \\ | ORDER BY $-.Age, $-.Friend \\ | LIMIT 1, 3; +-------------------+-----+ | Friend | Age | +-------------------+-----+ | \"Danny Green\" | 31 | | \"Aron Baynes\" | 32 | | \"Marco Belinelli\" | 32 | +-------------------+-----+","title":"General LIMIT syntax in native nGQL statements"},{"location":"3.ngql-guide/8.clauses-and-options/limit/#limit_in_go_statements","text":"In addition to the general syntax in the native nGQL, the LIMIT in the GO statement also supports limiting the number of output results based on edges. Syntax: <go_statement> LIMIT <limit_list>; limit_list is a list. Elements in the list must be natural numbers, and the number of elements must be the same as the maximum number of STEPS in the GO statement. The following takes GO 1 TO 3 STEPS FROM \"A\" OVER * LIMIT <limit_list> as an example to introduce this usage of LIMIT in detail. The list limit_list must contain 3 natural numbers, such as GO 1 TO 3 STEPS FROM \"A\" OVER * LIMIT [1,2,4] . 1 in LIMIT [1,2,4] means that the system automatically selects 1 edge to continue traversal in the first step. 2 means to select 2 edges to continue traversal in the second step. 4 indicates that 4 edges are selected to continue traversal in the third step. Because GO 1 TO 3 STEPS means to return all the traversal results from the first to third steps, all the red edges and their source and destination vertices in the figure below will be matched by this GO statement. And the yellow edges represent there is no path selected when the GO statement traverses. If it is not GO 1 TO 3 STEPS but GO 3 STEPS , it will only match the red edges of the third step and the vertices at both ends. In the basketballplayer dataset, the example is as follows: nebula> GO 3 STEPS FROM \"player100\" \\ OVER * \\ YIELD properties($$).name AS NAME, properties($$).age AS Age \\ LIMIT [3,3,3]; +-----------------+--------------+ | NAME | Age | +-----------------+--------------+ | \"Spurs\" | UNKNOWN_PROP | | \"Tony Parker\" | 36 | | \"Manu Ginobili\" | 41 | +-----------------+--------------+ nebula> GO 3 STEPS FROM \"player102\" OVER * \\ YIELD dst(edge) \\ LIMIT [rand32(5),rand32(5),rand32(5)]; +-------------+ | dst(EDGE) | +-------------+ | \"team204\" | | \"team215\" | | \"player100\" | | \"player102\" | +-------------+","title":"LIMIT in GO statements"},{"location":"3.ngql-guide/8.clauses-and-options/limit/#limit_in_opencypher_compatible_statements","text":"In openCypher compatible statements such as MATCH , there is no need to use a pipe when LIMIT is used. The syntax and description are as follows: ... [SKIP <offset>] [LIMIT <number_rows>]; Parameter Description offset The offset value. It defines the row from which to start returning. The offset starts from 0 . The default value is 0 , which returns from the first row. number_rows It constrains the total number of returned rows. Both offset and number_rows accept expressions, but the result of the expression must be a non-negative integer. Note Fraction expressions composed of two integers are automatically floored to integers. For example, 8/6 is floored to 1.","title":"LIMIT in openCypher compatible statements"},{"location":"3.ngql-guide/8.clauses-and-options/limit/#examples_of_limit","text":"LIMIT can be used alone to return a specified number of results. nebula> MATCH (v:player) RETURN v.player.name AS Name, v.player.age AS Age \\ ORDER BY Age LIMIT 5; +-------------------------+-----+ | Name | Age | +-------------------------+-----+ | \"Luka Doncic\" | 20 | | \"Ben Simmons\" | 22 | | \"Kristaps Porzingis\" | 23 | | \"Giannis Antetokounmpo\" | 24 | | \"Kyle Anderson\" | 25 | +-------------------------+-----+ nebula> MATCH (v:player) RETURN v.player.name AS Name, v.player.age AS Age \\ ORDER BY Age LIMIT rand32(5); +-------------------------+-----+ | Name | Age | +-------------------------+-----+ | \"Luka Doncic\" | 20 | | \"Ben Simmons\" | 22 | | \"Kristaps Porzingis\" | 23 | | \"Giannis Antetokounmpo\" | 24 | +-------------------------+-----+","title":"Examples of LIMIT"},{"location":"3.ngql-guide/8.clauses-and-options/limit/#examples_of_skip","text":"SKIP can be used alone to set the offset and return the data after the specified position. nebula> MATCH (v:player{name:\"Tim Duncan\"}) --> (v2) \\ RETURN v2.player.name AS Name, v2.player.age AS Age \\ ORDER BY Age DESC SKIP 1; +-----------------+-----+ | Name | Age | +-----------------+-----+ | \"Manu Ginobili\" | 41 | | \"Tony Parker\" | 36 | +-----------------+-----+ nebula> MATCH (v:player{name:\"Tim Duncan\"}) --> (v2) \\ RETURN v2.player.name AS Name, v2.player.age AS Age \\ ORDER BY Age DESC SKIP 1+1; +---------------+-----+ | Name | Age | +---------------+-----+ | \"Tony Parker\" | 36 | +---------------+-----+","title":"Examples of SKIP"},{"location":"3.ngql-guide/8.clauses-and-options/limit/#example_of_skip_and_limit","text":"SKIP and LIMIT can be used together to return the specified amount of data starting from the specified position. nebula> MATCH (v:player{name:\"Tim Duncan\"}) --> (v2) \\ RETURN v2.player.name AS Name, v2.player.age AS Age \\ ORDER BY Age DESC SKIP 1 LIMIT 1; +-----------------+-----+ | Name | Age | +-----------------+-----+ | \"Manu Ginobili\" | 41 | +-----------------+-----+","title":"Example of SKIP and LIMIT"},{"location":"3.ngql-guide/8.clauses-and-options/order-by/","text":"ORDER BY \u00b6 The ORDER BY clause specifies the order of the rows in the output. Native nGQL: You must use a pipe ( | ) and an ORDER BY clause after YIELD clause. OpenCypher style: No pipes are permitted. The ORDER BY clause follows a RETURN clause. There are two order options: ASC : Ascending. ASC is the default order. DESC : Descending. Native nGQL Syntax \u00b6 <YIELD clause> ORDER BY <expression> [ASC | DESC] [, <expression> [ASC | DESC] ...]; Compatibility In the native nGQL syntax, $-. must be used after ORDER BY . But it is not required in releases prior to 2.5.0. Examples \u00b6 nebula> FETCH PROP ON player \"player100\", \"player101\", \"player102\", \"player103\" \\ YIELD player.age AS age, player.name AS name \\ | ORDER BY $-.age ASC, $-.name DESC; +-----+---------------------+ | age | name | +-----+---------------------+ | 32 | \"Rudy Gay\" | | 33 | \"LaMarcus Aldridge\" | | 36 | \"Tony Parker\" | | 42 | \"Tim Duncan\" | +-----+---------------------+ nebula> $var = GO FROM \"player100\" OVER follow \\ YIELD dst(edge) AS dst; \\ ORDER BY $var.dst DESC; +-------------+ | dst | +-------------+ | \"player125\" | | \"player101\" | +-------------+ OpenCypher Syntax \u00b6 <RETURN clause> ORDER BY <expression> [ASC | DESC] [, <expression> [ASC | DESC] ...]; Examples \u00b6 nebula> MATCH (v:player) RETURN v.player.name AS Name, v.player.age AS Age \\ ORDER BY Name DESC; +-----------------+-----+ | Name | Age | +-----------------+-----+ | \"Yao Ming\" | 38 | | \"Vince Carter\" | 42 | | \"Tracy McGrady\" | 39 | | \"Tony Parker\" | 36 | | \"Tim Duncan\" | 42 | +-----------------+-----+ ... # In the following example, nGQL sorts the rows by age first. If multiple people are of the same age, nGQL will then sort them by name. nebula> MATCH (v:player) RETURN v.player.age AS Age, v.player.name AS Name \\ ORDER BY Age DESC, Name ASC; +-----+-------------------+ | Age | Name | +-----+-------------------+ | 47 | \"Shaquille O'Neal\" | | 46 | \"Grant Hill\" | | 45 | \"Jason Kidd\" | | 45 | \"Steve Nash\" | +-----+-------------------+ ... Order of NULL values \u00b6 nGQL lists NULL values at the end of the output for ascending sorting, and at the start for descending sorting. nebula> MATCH (v:player{name:\"Tim Duncan\"}) --> (v2) \\ RETURN v2.player.name AS Name, v2.player.age AS Age \\ ORDER BY Age; +-----------------+--------------+ | Name | Age | +-----------------+--------------+ | \"Tony Parker\" | 36 | | \"Manu Ginobili\" | 41 | | \"Spurs\" | UNKNOWN_PROP | +-----------------+--------------+ nebula> MATCH (v:player{name:\"Tim Duncan\"}) --> (v2) \\ RETURN v2.player.name AS Name, v2.player.age AS Age \\ ORDER BY Age DESC; +-----------------+--------------+ | Name | Age | +-----------------+--------------+ | \"Spurs\" | UNKNOWN_PROP | | \"Manu Ginobili\" | 41 | | \"Tony Parker\" | 36 | +-----------------+--------------+","title":"ORDER BY"},{"location":"3.ngql-guide/8.clauses-and-options/order-by/#order_by","text":"The ORDER BY clause specifies the order of the rows in the output. Native nGQL: You must use a pipe ( | ) and an ORDER BY clause after YIELD clause. OpenCypher style: No pipes are permitted. The ORDER BY clause follows a RETURN clause. There are two order options: ASC : Ascending. ASC is the default order. DESC : Descending.","title":"ORDER BY"},{"location":"3.ngql-guide/8.clauses-and-options/order-by/#native_ngql_syntax","text":"<YIELD clause> ORDER BY <expression> [ASC | DESC] [, <expression> [ASC | DESC] ...]; Compatibility In the native nGQL syntax, $-. must be used after ORDER BY . But it is not required in releases prior to 2.5.0.","title":"Native nGQL Syntax"},{"location":"3.ngql-guide/8.clauses-and-options/order-by/#examples","text":"nebula> FETCH PROP ON player \"player100\", \"player101\", \"player102\", \"player103\" \\ YIELD player.age AS age, player.name AS name \\ | ORDER BY $-.age ASC, $-.name DESC; +-----+---------------------+ | age | name | +-----+---------------------+ | 32 | \"Rudy Gay\" | | 33 | \"LaMarcus Aldridge\" | | 36 | \"Tony Parker\" | | 42 | \"Tim Duncan\" | +-----+---------------------+ nebula> $var = GO FROM \"player100\" OVER follow \\ YIELD dst(edge) AS dst; \\ ORDER BY $var.dst DESC; +-------------+ | dst | +-------------+ | \"player125\" | | \"player101\" | +-------------+","title":"Examples"},{"location":"3.ngql-guide/8.clauses-and-options/order-by/#opencypher_syntax","text":"<RETURN clause> ORDER BY <expression> [ASC | DESC] [, <expression> [ASC | DESC] ...];","title":"OpenCypher Syntax"},{"location":"3.ngql-guide/8.clauses-and-options/order-by/#examples_1","text":"nebula> MATCH (v:player) RETURN v.player.name AS Name, v.player.age AS Age \\ ORDER BY Name DESC; +-----------------+-----+ | Name | Age | +-----------------+-----+ | \"Yao Ming\" | 38 | | \"Vince Carter\" | 42 | | \"Tracy McGrady\" | 39 | | \"Tony Parker\" | 36 | | \"Tim Duncan\" | 42 | +-----------------+-----+ ... # In the following example, nGQL sorts the rows by age first. If multiple people are of the same age, nGQL will then sort them by name. nebula> MATCH (v:player) RETURN v.player.age AS Age, v.player.name AS Name \\ ORDER BY Age DESC, Name ASC; +-----+-------------------+ | Age | Name | +-----+-------------------+ | 47 | \"Shaquille O'Neal\" | | 46 | \"Grant Hill\" | | 45 | \"Jason Kidd\" | | 45 | \"Steve Nash\" | +-----+-------------------+ ...","title":"Examples"},{"location":"3.ngql-guide/8.clauses-and-options/order-by/#order_of_null_values","text":"nGQL lists NULL values at the end of the output for ascending sorting, and at the start for descending sorting. nebula> MATCH (v:player{name:\"Tim Duncan\"}) --> (v2) \\ RETURN v2.player.name AS Name, v2.player.age AS Age \\ ORDER BY Age; +-----------------+--------------+ | Name | Age | +-----------------+--------------+ | \"Tony Parker\" | 36 | | \"Manu Ginobili\" | 41 | | \"Spurs\" | UNKNOWN_PROP | +-----------------+--------------+ nebula> MATCH (v:player{name:\"Tim Duncan\"}) --> (v2) \\ RETURN v2.player.name AS Name, v2.player.age AS Age \\ ORDER BY Age DESC; +-----------------+--------------+ | Name | Age | +-----------------+--------------+ | \"Spurs\" | UNKNOWN_PROP | | \"Manu Ginobili\" | 41 | | \"Tony Parker\" | 36 | +-----------------+--------------+","title":"Order of NULL values"},{"location":"3.ngql-guide/8.clauses-and-options/return/","text":"RETURN \u00b6 The RETURN clause defines the output of an nGQL query. To return multiple fields, separate them with commas. RETURN can lead a clause or a statement: A RETURN clause can work in openCypher statements in nGQL, such as MATCH or UNWIND . A RETURN statement can work independently to output the result of an expression. OpenCypher compatibility \u00b6 This topic applies to the openCypher syntax in nGQL only. For native nGQL, use YIELD . RETURN does not support the following openCypher features yet. Return variables with uncommon characters, for example: MATCH (`non-english_characters`:player) \\ RETURN `non-english_characters`; Set a pattern in the RETURN clause and return all elements that this pattern matches, for example: MATCH (v:player) \\ RETURN (v)-[e]->(v2); Map order description \u00b6 When RETURN returns the map data structure, the order of key-value pairs is undefined. nebula> RETURN {age: 32, name: \"Marco Belinelli\"}; +------------------------------------+ | {age:32,name:\"Marco Belinelli\"} | +------------------------------------+ | {age: 32, name: \"Marco Belinelli\"} | +------------------------------------+ nebula> RETURN {zage: 32, name: \"Marco Belinelli\"}; +-------------------------------------+ | {zage:32,name:\"Marco Belinelli\"} | +-------------------------------------+ | {name: \"Marco Belinelli\", zage: 32} | +-------------------------------------+ Return vertices \u00b6 nebula> MATCH (v:player) \\ RETURN v; +---------------------------------------------------------------+ | v | +---------------------------------------------------------------+ | (\"player104\" :player{age: 32, name: \"Marco Belinelli\"}) | | (\"player107\" :player{age: 32, name: \"Aron Baynes\"}) | | (\"player116\" :player{age: 34, name: \"LeBron James\"}) | | (\"player120\" :player{age: 29, name: \"James Harden\"}) | | (\"player125\" :player{age: 41, name: \"Manu Ginobili\"}) | +---------------------------------------------------------------+ ... Return edges \u00b6 nebula> MATCH (v:player)-[e]->() \\ RETURN e; +------------------------------------------------------------------------------+ | e | +------------------------------------------------------------------------------+ | [:follow \"player104\"->\"player100\" @0 {degree: 55}] | | [:follow \"player104\"->\"player101\" @0 {degree: 50}] | | [:follow \"player104\"->\"player105\" @0 {degree: 60}] | | [:serve \"player104\"->\"team200\" @0 {end_year: 2009, start_year: 2007}] | | [:serve \"player104\"->\"team208\" @0 {end_year: 2016, start_year: 2015}] | +------------------------------------------------------------------------------+ ... Return VIDs \u00b6 Use the id() function to retrieve VIDs. nebula> MATCH (v:player{name:\"Tim Duncan\"}) \\ RETURN id(v); +-------------+ | id(v) | +-------------+ | \"player100\" | +-------------+ Return Tag \u00b6 Use the labels() function to return the list of tags on a vertex. nebula> MATCH (v:player{name:\"Tim Duncan\"}) \\ RETURN labels(v); +------------+ | labels(v) | +------------+ | [\"player\"] | +------------+ To retrieve the nth element in the labels(v) list, use labels(v)[n-1] . The following example shows how to use labels(v)[0] to return the first tag in the list. nebula> MATCH (v:player{name:\"Tim Duncan\"}) \\ RETURN labels(v)[0]; +--------------+ | labels(v)[0] | +--------------+ | \"player\" | +--------------+ Return properties \u00b6 To return a vertex or edge property, use the {<vertex_name>|<edge_name>}.<property> syntax. nebula> MATCH (v:player) \\ RETURN v.player.name, v.player.age \\ LIMIT 3; +------------------+--------------+ | v.player.name | v.player.age | +------------------+--------------+ | \"Danny Green\" | 31 | | \"Tiago Splitter\" | 34 | | \"David West\" | 38 | +------------------+--------------+ Use the properties() function to return all properties on a vertex or an edge. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[]->(v2) \\ RETURN properties(v2); +----------------------------------+ | properties(v2) | +----------------------------------+ | {name: \"Spurs\"} | | {age: 36, name: \"Tony Parker\"} | | {age: 41, name: \"Manu Ginobili\"} | +----------------------------------+ Return edge type \u00b6 Use the type() function to return the matched edge types. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[e]->() \\ RETURN DISTINCT type(e); +----------+ | type(e) | +----------+ | \"serve\" | | \"follow\" | +----------+ Return paths \u00b6 Use RETURN <path_name> to return all the information of the matched paths. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[*3]->() \\ RETURN p; +----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | p | +----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player101\" :player{age: 36, name: \"Tony Parker\"})-[:follow@0 {degree: 90}]->(\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"})-[:serve@0 {end_year: 2019, start_year: 2015}]->(\"team204\" :team{name: \"Spurs\"})> | | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player101\" :player{age: 36, name: \"Tony Parker\"})-[:follow@0 {degree: 90}]->(\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"})-[:serve@0 {end_year: 2015, start_year: 2006}]->(\"team203\" :team{name: \"Trail Blazers\"})> | | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player101\" :player{age: 36, name: \"Tony Parker\"})-[:follow@0 {degree: 90}]->(\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"})-[:follow@0 {degree: 75}]->(\"player101\" :player{age: 36, name: \"Tony Parker\"})> | +----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ ... Return vertices in a path \u00b6 Use the nodes() function to return all vertices in a path. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[]->(v2) \\ RETURN nodes(p); +---------------------------------------------------------------------------------------------------------------------+ | nodes(p) | +---------------------------------------------------------------------------------------------------------------------+ | [(\"player100\" :star{} :player{age: 42, name: \"Tim Duncan\"}), (\"player204\" :team{name: \"Spurs\"})] | | [(\"player100\" :star{} :player{age: 42, name: \"Tim Duncan\"}), (\"player101\" :player{name: \"Tony Parker\", age: 36})] | | [(\"player100\" :star{} :player{age: 42, name: \"Tim Duncan\"}), (\"player125\" :player{name: \"Manu Ginobili\", age: 41})] | +---------------------------------------------------------------------------------------------------------------------+ Return edges in a path \u00b6 Use the relationships() function to return all edges in a path. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[]->(v2) \\ RETURN relationships(p); +-------------------------------------------------------------------------+ | relationships(p) | +-------------------------------------------------------------------------+ | [[:serve \"player100\"->\"team204\" @0 {end_year: 2016, start_year: 1997}]] | | [[:follow \"player100\"->\"player101\" @0 {degree: 95}]] | | [[:follow \"player100\"->\"player125\" @0 {degree: 95}]] | +-------------------------------------------------------------------------+ Return path length \u00b6 Use the length() function to return the length of a path. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[*..2]->(v2) \\ RETURN p AS Paths, length(p) AS Length; +------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+ | Paths | Length | +------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+ | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:serve@0 {end_year: 2016, start_year: 1997}]->(\"team204\" :team{name: \"Spurs\"})> | 1 | | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player101\" :player{age: 36, name: \"Tony Parker\"})> | 1 | | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player125\" :player{age: 41, name: \"Manu Ginobili\"})> | 1 | | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player101\" :player{age: 36, name: \"Tony Parker\"})-[:serve@0 {end_year: 2018, start_year: 1999}]->(\"team204\" :team{name: \"Spurs\"})> | 2 | | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player101\" :player{age: 36, name: \"Tony Parker\"})-[:serve@0 {end_year: 2019, start_year: 2018}]->(\"team215\" :team{name: \"Hornets\"})> | 2 | | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player101\" :player{age: 36, name: \"Tony Parker\"})-[:follow@0 {degree: 95}]->(\"player100\" :player{age: 42, name: \"Tim Duncan\"})> | 2 | | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player101\" :player{age: 36, name: \"Tony Parker\"})-[:follow@0 {degree: 90}]->(\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"})> | 2 | | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player101\" :player{age: 36, name: \"Tony Parker\"})-[:follow@0 {degree: 95}]->(\"player125\" :player{age: 41, name: \"Manu Ginobili\"})> | 2 | | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player125\" :player{age: 41, name: \"Manu Ginobili\"})-[:serve@0 {end_year: 2018, start_year: 2002}]->(\"team204\" :team{name: \"Spurs\"})> | 2 | | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player125\" :player{age: 41, name: \"Manu Ginobili\"})-[:follow@0 {degree: 90}]->(\"player100\" :player{age: 42, name: \"Tim Duncan\"})> | 2 | +------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+ Return all elements \u00b6 To return all the elements that this pattern matches, use an asterisk (*). nebula> MATCH (v:player{name:\"Tim Duncan\"}) \\ RETURN *; +----------------------------------------------------+ | v | +----------------------------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | +----------------------------------------------------+ nebula> MATCH (v:player{name:\"Tim Duncan\"})-[e]->(v2) \\ RETURN *; +----------------------------------------------------+-----------------------------------------------------------------------+-------------------------------------------------------+ | v | e | v2 | +----------------------------------------------------+-----------------------------------------------------------------------+-------------------------------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | [:follow \"player100\"->\"player101\" @0 {degree: 95}] | (\"player101\" :player{age: 36, name: \"Tony Parker\"}) | | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | [:follow \"player100\"->\"player125\" @0 {degree: 95}] | (\"player125\" :player{age: 41, name: \"Manu Ginobili\"}) | | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | [:serve \"player100\"->\"team204\" @0 {end_year: 2016, start_year: 1997}] | (\"team204\" :team{name: \"Spurs\"}) | +----------------------------------------------------+-----------------------------------------------------------------------+-------------------------------------------------------+ Rename a field \u00b6 Use the AS <alias> syntax to rename a field in the output. nebula> MATCH (v:player{name:\"Tim Duncan\"})-[:serve]->(v2) \\ RETURN v2.team.name AS Team; +---------+ | Team | +---------+ | \"Spurs\" | +---------+ nebula> RETURN \"Amber\" AS Name; +---------+ | Name | +---------+ | \"Amber\" | +---------+ Return a non-existing property \u00b6 If a property matched does not exist, NULL is returned. nebula> MATCH (v:player{name:\"Tim Duncan\"})-[e]->(v2) \\ RETURN v2.player.name, type(e), v2.player.age; +-----------------+----------+---------------+ | v2.player.name | type(e) | v2.player.age | +-----------------+----------+---------------+ | \"Manu Ginobili\" | \"follow\" | 41 | | __NULL__ | \"serve\" | __NULL__ | | \"Tony Parker\" | \"follow\" | 36 | +-----------------+----------+---------------+ Return expression results \u00b6 To return the results of expressions such as literals, functions, or predicates, set them in a RETURN clause. nebula> MATCH (v:player{name:\"Tony Parker\"})-->(v2:player) \\ RETURN DISTINCT v2.player.name, \"Hello\"+\" graphs!\", v2.player.age > 35; +---------------------+----------------------+--------------------+ | v2.player.name | (\"Hello\"+\" graphs!\") | (v2.player.age>35) | +---------------------+----------------------+--------------------+ | \"LaMarcus Aldridge\" | \"Hello graphs!\" | false | | \"Tim Duncan\" | \"Hello graphs!\" | true | | \"Manu Ginobili\" | \"Hello graphs!\" | true | +---------------------+----------------------+--------------------+ nebula> RETURN 1+1; +-------+ | (1+1) | +-------+ | 2 | +-------+ nebula> RETURN 3 > 1; +-------+ | (3>1) | +-------+ | true | +-------+ nebula> RETURN 1+1, rand32(1, 5); +-------+-------------+ | (1+1) | rand32(1,5) | +-------+-------------+ | 2 | 1 | +-------+-------------+ Return unique fields \u00b6 Use DISTINCT to remove duplicate fields in the result set. # Before using DISTINCT. nebula> MATCH (v:player{name:\"Tony Parker\"})--(v2:player) \\ RETURN v2.player.name, v2.player.age; +---------------------+---------------+ | v2.player.name | v2.player.age | +---------------------+---------------+ | \"Manu Ginobili\" | 41 | | \"Boris Diaw\" | 36 | | \"Marco Belinelli\" | 32 | | \"Dejounte Murray\" | 29 | | \"Tim Duncan\" | 42 | | \"Tim Duncan\" | 42 | | \"LaMarcus Aldridge\" | 33 | | \"LaMarcus Aldridge\" | 33 | +---------------------+---------------+ # After using DISTINCT. nebula> MATCH (v:player{name:\"Tony Parker\"})--(v2:player) \\ RETURN DISTINCT v2.player.name, v2.player.age; +---------------------+---------------+ | v2.player.name | v2.player.age | +---------------------+---------------+ | \"Manu Ginobili\" | 41 | | \"Boris Diaw\" | 36 | | \"Marco Belinelli\" | 32 | | \"Dejounte Murray\" | 29 | | \"Tim Duncan\" | 42 | | \"LaMarcus Aldridge\" | 33 | +---------------------+---------------+","title":"RETURN"},{"location":"3.ngql-guide/8.clauses-and-options/return/#return","text":"The RETURN clause defines the output of an nGQL query. To return multiple fields, separate them with commas. RETURN can lead a clause or a statement: A RETURN clause can work in openCypher statements in nGQL, such as MATCH or UNWIND . A RETURN statement can work independently to output the result of an expression.","title":"RETURN"},{"location":"3.ngql-guide/8.clauses-and-options/return/#opencypher_compatibility","text":"This topic applies to the openCypher syntax in nGQL only. For native nGQL, use YIELD . RETURN does not support the following openCypher features yet. Return variables with uncommon characters, for example: MATCH (`non-english_characters`:player) \\ RETURN `non-english_characters`; Set a pattern in the RETURN clause and return all elements that this pattern matches, for example: MATCH (v:player) \\ RETURN (v)-[e]->(v2);","title":"OpenCypher compatibility"},{"location":"3.ngql-guide/8.clauses-and-options/return/#map_order_description","text":"When RETURN returns the map data structure, the order of key-value pairs is undefined. nebula> RETURN {age: 32, name: \"Marco Belinelli\"}; +------------------------------------+ | {age:32,name:\"Marco Belinelli\"} | +------------------------------------+ | {age: 32, name: \"Marco Belinelli\"} | +------------------------------------+ nebula> RETURN {zage: 32, name: \"Marco Belinelli\"}; +-------------------------------------+ | {zage:32,name:\"Marco Belinelli\"} | +-------------------------------------+ | {name: \"Marco Belinelli\", zage: 32} | +-------------------------------------+","title":"Map order description"},{"location":"3.ngql-guide/8.clauses-and-options/return/#return_vertices","text":"nebula> MATCH (v:player) \\ RETURN v; +---------------------------------------------------------------+ | v | +---------------------------------------------------------------+ | (\"player104\" :player{age: 32, name: \"Marco Belinelli\"}) | | (\"player107\" :player{age: 32, name: \"Aron Baynes\"}) | | (\"player116\" :player{age: 34, name: \"LeBron James\"}) | | (\"player120\" :player{age: 29, name: \"James Harden\"}) | | (\"player125\" :player{age: 41, name: \"Manu Ginobili\"}) | +---------------------------------------------------------------+ ...","title":"Return vertices"},{"location":"3.ngql-guide/8.clauses-and-options/return/#return_edges","text":"nebula> MATCH (v:player)-[e]->() \\ RETURN e; +------------------------------------------------------------------------------+ | e | +------------------------------------------------------------------------------+ | [:follow \"player104\"->\"player100\" @0 {degree: 55}] | | [:follow \"player104\"->\"player101\" @0 {degree: 50}] | | [:follow \"player104\"->\"player105\" @0 {degree: 60}] | | [:serve \"player104\"->\"team200\" @0 {end_year: 2009, start_year: 2007}] | | [:serve \"player104\"->\"team208\" @0 {end_year: 2016, start_year: 2015}] | +------------------------------------------------------------------------------+ ...","title":"Return edges"},{"location":"3.ngql-guide/8.clauses-and-options/return/#return_vids","text":"Use the id() function to retrieve VIDs. nebula> MATCH (v:player{name:\"Tim Duncan\"}) \\ RETURN id(v); +-------------+ | id(v) | +-------------+ | \"player100\" | +-------------+","title":"Return VIDs"},{"location":"3.ngql-guide/8.clauses-and-options/return/#return_tag","text":"Use the labels() function to return the list of tags on a vertex. nebula> MATCH (v:player{name:\"Tim Duncan\"}) \\ RETURN labels(v); +------------+ | labels(v) | +------------+ | [\"player\"] | +------------+ To retrieve the nth element in the labels(v) list, use labels(v)[n-1] . The following example shows how to use labels(v)[0] to return the first tag in the list. nebula> MATCH (v:player{name:\"Tim Duncan\"}) \\ RETURN labels(v)[0]; +--------------+ | labels(v)[0] | +--------------+ | \"player\" | +--------------+","title":"Return Tag"},{"location":"3.ngql-guide/8.clauses-and-options/return/#return_properties","text":"To return a vertex or edge property, use the {<vertex_name>|<edge_name>}.<property> syntax. nebula> MATCH (v:player) \\ RETURN v.player.name, v.player.age \\ LIMIT 3; +------------------+--------------+ | v.player.name | v.player.age | +------------------+--------------+ | \"Danny Green\" | 31 | | \"Tiago Splitter\" | 34 | | \"David West\" | 38 | +------------------+--------------+ Use the properties() function to return all properties on a vertex or an edge. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[]->(v2) \\ RETURN properties(v2); +----------------------------------+ | properties(v2) | +----------------------------------+ | {name: \"Spurs\"} | | {age: 36, name: \"Tony Parker\"} | | {age: 41, name: \"Manu Ginobili\"} | +----------------------------------+","title":"Return properties"},{"location":"3.ngql-guide/8.clauses-and-options/return/#return_edge_type","text":"Use the type() function to return the matched edge types. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[e]->() \\ RETURN DISTINCT type(e); +----------+ | type(e) | +----------+ | \"serve\" | | \"follow\" | +----------+","title":"Return edge type"},{"location":"3.ngql-guide/8.clauses-and-options/return/#return_paths","text":"Use RETURN <path_name> to return all the information of the matched paths. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[*3]->() \\ RETURN p; +----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | p | +----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player101\" :player{age: 36, name: \"Tony Parker\"})-[:follow@0 {degree: 90}]->(\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"})-[:serve@0 {end_year: 2019, start_year: 2015}]->(\"team204\" :team{name: \"Spurs\"})> | | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player101\" :player{age: 36, name: \"Tony Parker\"})-[:follow@0 {degree: 90}]->(\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"})-[:serve@0 {end_year: 2015, start_year: 2006}]->(\"team203\" :team{name: \"Trail Blazers\"})> | | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player101\" :player{age: 36, name: \"Tony Parker\"})-[:follow@0 {degree: 90}]->(\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"})-[:follow@0 {degree: 75}]->(\"player101\" :player{age: 36, name: \"Tony Parker\"})> | +----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ ...","title":"Return paths"},{"location":"3.ngql-guide/8.clauses-and-options/return/#return_vertices_in_a_path","text":"Use the nodes() function to return all vertices in a path. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[]->(v2) \\ RETURN nodes(p); +---------------------------------------------------------------------------------------------------------------------+ | nodes(p) | +---------------------------------------------------------------------------------------------------------------------+ | [(\"player100\" :star{} :player{age: 42, name: \"Tim Duncan\"}), (\"player204\" :team{name: \"Spurs\"})] | | [(\"player100\" :star{} :player{age: 42, name: \"Tim Duncan\"}), (\"player101\" :player{name: \"Tony Parker\", age: 36})] | | [(\"player100\" :star{} :player{age: 42, name: \"Tim Duncan\"}), (\"player125\" :player{name: \"Manu Ginobili\", age: 41})] | +---------------------------------------------------------------------------------------------------------------------+","title":"Return vertices in a path"},{"location":"3.ngql-guide/8.clauses-and-options/return/#return_edges_in_a_path","text":"Use the relationships() function to return all edges in a path. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[]->(v2) \\ RETURN relationships(p); +-------------------------------------------------------------------------+ | relationships(p) | +-------------------------------------------------------------------------+ | [[:serve \"player100\"->\"team204\" @0 {end_year: 2016, start_year: 1997}]] | | [[:follow \"player100\"->\"player101\" @0 {degree: 95}]] | | [[:follow \"player100\"->\"player125\" @0 {degree: 95}]] | +-------------------------------------------------------------------------+","title":"Return edges in a path"},{"location":"3.ngql-guide/8.clauses-and-options/return/#return_path_length","text":"Use the length() function to return the length of a path. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[*..2]->(v2) \\ RETURN p AS Paths, length(p) AS Length; +------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+ | Paths | Length | +------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+ | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:serve@0 {end_year: 2016, start_year: 1997}]->(\"team204\" :team{name: \"Spurs\"})> | 1 | | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player101\" :player{age: 36, name: \"Tony Parker\"})> | 1 | | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player125\" :player{age: 41, name: \"Manu Ginobili\"})> | 1 | | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player101\" :player{age: 36, name: \"Tony Parker\"})-[:serve@0 {end_year: 2018, start_year: 1999}]->(\"team204\" :team{name: \"Spurs\"})> | 2 | | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player101\" :player{age: 36, name: \"Tony Parker\"})-[:serve@0 {end_year: 2019, start_year: 2018}]->(\"team215\" :team{name: \"Hornets\"})> | 2 | | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player101\" :player{age: 36, name: \"Tony Parker\"})-[:follow@0 {degree: 95}]->(\"player100\" :player{age: 42, name: \"Tim Duncan\"})> | 2 | | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player101\" :player{age: 36, name: \"Tony Parker\"})-[:follow@0 {degree: 90}]->(\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"})> | 2 | | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player101\" :player{age: 36, name: \"Tony Parker\"})-[:follow@0 {degree: 95}]->(\"player125\" :player{age: 41, name: \"Manu Ginobili\"})> | 2 | | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player125\" :player{age: 41, name: \"Manu Ginobili\"})-[:serve@0 {end_year: 2018, start_year: 2002}]->(\"team204\" :team{name: \"Spurs\"})> | 2 | | <(\"player100\" :player{age: 42, name: \"Tim Duncan\"})-[:follow@0 {degree: 95}]->(\"player125\" :player{age: 41, name: \"Manu Ginobili\"})-[:follow@0 {degree: 90}]->(\"player100\" :player{age: 42, name: \"Tim Duncan\"})> | 2 | +------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+","title":"Return path length"},{"location":"3.ngql-guide/8.clauses-and-options/return/#return_all_elements","text":"To return all the elements that this pattern matches, use an asterisk (*). nebula> MATCH (v:player{name:\"Tim Duncan\"}) \\ RETURN *; +----------------------------------------------------+ | v | +----------------------------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | +----------------------------------------------------+ nebula> MATCH (v:player{name:\"Tim Duncan\"})-[e]->(v2) \\ RETURN *; +----------------------------------------------------+-----------------------------------------------------------------------+-------------------------------------------------------+ | v | e | v2 | +----------------------------------------------------+-----------------------------------------------------------------------+-------------------------------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | [:follow \"player100\"->\"player101\" @0 {degree: 95}] | (\"player101\" :player{age: 36, name: \"Tony Parker\"}) | | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | [:follow \"player100\"->\"player125\" @0 {degree: 95}] | (\"player125\" :player{age: 41, name: \"Manu Ginobili\"}) | | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | [:serve \"player100\"->\"team204\" @0 {end_year: 2016, start_year: 1997}] | (\"team204\" :team{name: \"Spurs\"}) | +----------------------------------------------------+-----------------------------------------------------------------------+-------------------------------------------------------+","title":"Return all elements"},{"location":"3.ngql-guide/8.clauses-and-options/return/#rename_a_field","text":"Use the AS <alias> syntax to rename a field in the output. nebula> MATCH (v:player{name:\"Tim Duncan\"})-[:serve]->(v2) \\ RETURN v2.team.name AS Team; +---------+ | Team | +---------+ | \"Spurs\" | +---------+ nebula> RETURN \"Amber\" AS Name; +---------+ | Name | +---------+ | \"Amber\" | +---------+","title":"Rename a field"},{"location":"3.ngql-guide/8.clauses-and-options/return/#return_a_non-existing_property","text":"If a property matched does not exist, NULL is returned. nebula> MATCH (v:player{name:\"Tim Duncan\"})-[e]->(v2) \\ RETURN v2.player.name, type(e), v2.player.age; +-----------------+----------+---------------+ | v2.player.name | type(e) | v2.player.age | +-----------------+----------+---------------+ | \"Manu Ginobili\" | \"follow\" | 41 | | __NULL__ | \"serve\" | __NULL__ | | \"Tony Parker\" | \"follow\" | 36 | +-----------------+----------+---------------+","title":"Return a non-existing property"},{"location":"3.ngql-guide/8.clauses-and-options/return/#return_expression_results","text":"To return the results of expressions such as literals, functions, or predicates, set them in a RETURN clause. nebula> MATCH (v:player{name:\"Tony Parker\"})-->(v2:player) \\ RETURN DISTINCT v2.player.name, \"Hello\"+\" graphs!\", v2.player.age > 35; +---------------------+----------------------+--------------------+ | v2.player.name | (\"Hello\"+\" graphs!\") | (v2.player.age>35) | +---------------------+----------------------+--------------------+ | \"LaMarcus Aldridge\" | \"Hello graphs!\" | false | | \"Tim Duncan\" | \"Hello graphs!\" | true | | \"Manu Ginobili\" | \"Hello graphs!\" | true | +---------------------+----------------------+--------------------+ nebula> RETURN 1+1; +-------+ | (1+1) | +-------+ | 2 | +-------+ nebula> RETURN 3 > 1; +-------+ | (3>1) | +-------+ | true | +-------+ nebula> RETURN 1+1, rand32(1, 5); +-------+-------------+ | (1+1) | rand32(1,5) | +-------+-------------+ | 2 | 1 | +-------+-------------+","title":"Return expression results"},{"location":"3.ngql-guide/8.clauses-and-options/return/#return_unique_fields","text":"Use DISTINCT to remove duplicate fields in the result set. # Before using DISTINCT. nebula> MATCH (v:player{name:\"Tony Parker\"})--(v2:player) \\ RETURN v2.player.name, v2.player.age; +---------------------+---------------+ | v2.player.name | v2.player.age | +---------------------+---------------+ | \"Manu Ginobili\" | 41 | | \"Boris Diaw\" | 36 | | \"Marco Belinelli\" | 32 | | \"Dejounte Murray\" | 29 | | \"Tim Duncan\" | 42 | | \"Tim Duncan\" | 42 | | \"LaMarcus Aldridge\" | 33 | | \"LaMarcus Aldridge\" | 33 | +---------------------+---------------+ # After using DISTINCT. nebula> MATCH (v:player{name:\"Tony Parker\"})--(v2:player) \\ RETURN DISTINCT v2.player.name, v2.player.age; +---------------------+---------------+ | v2.player.name | v2.player.age | +---------------------+---------------+ | \"Manu Ginobili\" | 41 | | \"Boris Diaw\" | 36 | | \"Marco Belinelli\" | 32 | | \"Dejounte Murray\" | 29 | | \"Tim Duncan\" | 42 | | \"LaMarcus Aldridge\" | 33 | +---------------------+---------------+","title":"Return unique fields"},{"location":"3.ngql-guide/8.clauses-and-options/sample/","text":"SAMPLE \u00b6 The SAMPLE clause takes samples evenly in the result set and returns the specified amount of data. SAMPLE can be used in GO statements only. The syntax is as follows: <go_statement> SAMPLE <sample_list>; sample_list is a list. Elements in the list must be natural numbers, and the number of elements must be the same as the maximum number of STEPS in the GO statement. The following takes GO 1 TO 3 STEPS FROM \"A\" OVER * SAMPLE <sample_list> as an example to introduce this usage of SAMPLE in detail. The list sample_list must contain 3 natural numbers, such as GO 1 TO 3 STEPS FROM \"A\" OVER * SAMPLE [1,2,4] . 1 in SAMPLE [1,2,4] means that the system automatically selects 1 edge to continue traversal in the first step. 2 means to select 2 edges to continue traversal in the second step. 4 indicates that 4 edges are selected to continue traversal in the third step. If there is no matched edge in a certain step or the number of matched edges is less than the specified number, the actual number will be returned. Because GO 1 TO 3 STEPS means to return all the traversal results from the first to third steps, all the red edges and their source and destination vertices in the figure below will be matched by this GO statement. And the yellow edges represent there is no path selected when the GO statement traverses. If it is not GO 1 TO 3 STEPS but GO 3 STEPS , it will only match the red edges of the third step and the vertices at both ends. In the basketballplayer dataset, the example is as follows: nebula> GO 3 STEPS FROM \"player100\" \\ OVER * \\ YIELD properties($$).name AS NAME, properties($$).age AS Age \\ SAMPLE [1,2,3]; +-----------------+--------------+ | NAME | Age | +-----------------+--------------+ | \"Spurs\" | UNKNOWN_PROP | | \"Tony Parker\" | 36 | | \"Manu Ginobili\" | 41 | +-----------------+--------------+ nebula> GO 1 TO 3 STEPS FROM \"player100\" \\ OVER * \\ YIELD properties($$).name AS NAME, properties($$).age AS Age \\ SAMPLE [2,2,2]; +---------------------+-----+ | NAME | Age | +---------------------+-----+ | \"Manu Ginobili\" | 41 | | \"Tony Parker\" | 36 | | \"Tim Duncan\" | 42 | | \"LaMarcus Aldridge\" | 33 | | \"Tony Parker\" | 36 | | \"Tim Duncan\" | 42 | +---------------------+-----+","title":"SAMPLE"},{"location":"3.ngql-guide/8.clauses-and-options/sample/#sample","text":"The SAMPLE clause takes samples evenly in the result set and returns the specified amount of data. SAMPLE can be used in GO statements only. The syntax is as follows: <go_statement> SAMPLE <sample_list>; sample_list is a list. Elements in the list must be natural numbers, and the number of elements must be the same as the maximum number of STEPS in the GO statement. The following takes GO 1 TO 3 STEPS FROM \"A\" OVER * SAMPLE <sample_list> as an example to introduce this usage of SAMPLE in detail. The list sample_list must contain 3 natural numbers, such as GO 1 TO 3 STEPS FROM \"A\" OVER * SAMPLE [1,2,4] . 1 in SAMPLE [1,2,4] means that the system automatically selects 1 edge to continue traversal in the first step. 2 means to select 2 edges to continue traversal in the second step. 4 indicates that 4 edges are selected to continue traversal in the third step. If there is no matched edge in a certain step or the number of matched edges is less than the specified number, the actual number will be returned. Because GO 1 TO 3 STEPS means to return all the traversal results from the first to third steps, all the red edges and their source and destination vertices in the figure below will be matched by this GO statement. And the yellow edges represent there is no path selected when the GO statement traverses. If it is not GO 1 TO 3 STEPS but GO 3 STEPS , it will only match the red edges of the third step and the vertices at both ends. In the basketballplayer dataset, the example is as follows: nebula> GO 3 STEPS FROM \"player100\" \\ OVER * \\ YIELD properties($$).name AS NAME, properties($$).age AS Age \\ SAMPLE [1,2,3]; +-----------------+--------------+ | NAME | Age | +-----------------+--------------+ | \"Spurs\" | UNKNOWN_PROP | | \"Tony Parker\" | 36 | | \"Manu Ginobili\" | 41 | +-----------------+--------------+ nebula> GO 1 TO 3 STEPS FROM \"player100\" \\ OVER * \\ YIELD properties($$).name AS NAME, properties($$).age AS Age \\ SAMPLE [2,2,2]; +---------------------+-----+ | NAME | Age | +---------------------+-----+ | \"Manu Ginobili\" | 41 | | \"Tony Parker\" | 36 | | \"Tim Duncan\" | 42 | | \"LaMarcus Aldridge\" | 33 | | \"Tony Parker\" | 36 | | \"Tim Duncan\" | 42 | +---------------------+-----+","title":"SAMPLE"},{"location":"3.ngql-guide/8.clauses-and-options/ttl-options/","text":"TTL \u00b6 TTL (Time To Live) specifies a timeout for a property. Once timed out, the property expires. OpenCypher Compatibility \u00b6 This topic applies to native nGQL only. Precautions \u00b6 You CANNOT modify a property schema with TTL options on it. TTL options and indexes have coexistence issues. TTL options and indexes CANNOT coexist on a tag or an edge type. If there is an index on a property, you cannot set TTL options on other properties. If there are TTL options on a tag, an edge type, or a property, you can still add an index on them. Data expiration and deletion \u00b6 Vertex property expiration \u00b6 Vertex property expiration has the following impact. If a vertex has only one tag, once a property of the vertex expires, the vertex expires. If a vertex has multiple tags, once a property of the vertex expires, properties bound to the same tag with the expired property also expire, but the vertex does not expire and other properties of it remain untouched. Edge property expiration \u00b6 Since an edge can have only one edge type, once an edge property expires, the edge expires. Data deletion \u00b6 The expired data are still stored on the disk, but queries will filter them out. Nebula Graph automatically deletes the expired data and reclaims the disk space during the next compaction . Note If TTL is disabled , the corresponding data deleted after the last compaction can be queried again. TTL options \u00b6 The native nGQL TTL feature has the following options. Option Description ttl_col Specifies the property to set a timeout on. The data type of the property must be int or timestamp . ttl_duration Specifies the timeout adds-on value in seconds. The value must be a non-negative int64 number. A property expires if the sum of its value and the ttl_duration value is smaller than the current timestamp. If the ttl_duration value is 0 , the property never expires. Use TTL options \u00b6 You must use the TTL options together to set a valid timeout on a property. Set a timeout if a tag or an edge type exists \u00b6 If a tag or an edge type is already created, to set a timeout on a property bound to the tag or edge type, use ALTER to update the tag or edge type. # Create a tag. nebula> CREATE TAG IF NOT EXISTS t1 (a timestamp); # Use ALTER to update the tag and set the TTL options. nebula> ALTER TAG t1 ttl_col = \"a\", ttl_duration = 5; # Insert a vertex with tag t1. The vertex expires 5 seconds after the insertion. nebula> INSERT VERTEX t1(a) values \"101\":(now()); Set a timeout when creating a tag or an edge type \u00b6 Use TTL options in the CREATE statement to set a timeout when creating a tag or an edge type. For more information, see CREATE TAG and CREATE EDGE . # Create a tag and set the TTL options. nebula> CREATE TAG IF NOT EXISTS t2(a int, b int, c string) ttl_duration= 100, ttl_col = \"a\"; # Insert a vertex with tag t2. The timeout timestamp is 1648197238 (1648197138 + 100). nebula> INSERT VERTEX t2(a, b, c) values \"102\":(1648197138, 30, \"Hello\"); Remove a timeout \u00b6 To disable TTL and remove the timeout on a property, you can use the following approaches. Drop the property with the timeout. nebula> ALTER TAG t1 DROP (a); Set ttl_col to an empty string. nebula> ALTER TAG t1 ttl_col = \"\"; Set ttl_duration to 0 . This operation keeps the TTL options and prevents the property from expiring and the property schema from being modified. nebula> ALTER TAG t1 ttl_duration = 0;","title":"TTL"},{"location":"3.ngql-guide/8.clauses-and-options/ttl-options/#ttl","text":"TTL (Time To Live) specifies a timeout for a property. Once timed out, the property expires.","title":"TTL"},{"location":"3.ngql-guide/8.clauses-and-options/ttl-options/#opencypher_compatibility","text":"This topic applies to native nGQL only.","title":"OpenCypher Compatibility"},{"location":"3.ngql-guide/8.clauses-and-options/ttl-options/#precautions","text":"You CANNOT modify a property schema with TTL options on it. TTL options and indexes have coexistence issues. TTL options and indexes CANNOT coexist on a tag or an edge type. If there is an index on a property, you cannot set TTL options on other properties. If there are TTL options on a tag, an edge type, or a property, you can still add an index on them.","title":"Precautions"},{"location":"3.ngql-guide/8.clauses-and-options/ttl-options/#data_expiration_and_deletion","text":"","title":"Data expiration and deletion"},{"location":"3.ngql-guide/8.clauses-and-options/ttl-options/#vertex_property_expiration","text":"Vertex property expiration has the following impact. If a vertex has only one tag, once a property of the vertex expires, the vertex expires. If a vertex has multiple tags, once a property of the vertex expires, properties bound to the same tag with the expired property also expire, but the vertex does not expire and other properties of it remain untouched.","title":"Vertex property expiration"},{"location":"3.ngql-guide/8.clauses-and-options/ttl-options/#edge_property_expiration","text":"Since an edge can have only one edge type, once an edge property expires, the edge expires.","title":"Edge property expiration"},{"location":"3.ngql-guide/8.clauses-and-options/ttl-options/#data_deletion","text":"The expired data are still stored on the disk, but queries will filter them out. Nebula Graph automatically deletes the expired data and reclaims the disk space during the next compaction . Note If TTL is disabled , the corresponding data deleted after the last compaction can be queried again.","title":"Data deletion"},{"location":"3.ngql-guide/8.clauses-and-options/ttl-options/#ttl_options","text":"The native nGQL TTL feature has the following options. Option Description ttl_col Specifies the property to set a timeout on. The data type of the property must be int or timestamp . ttl_duration Specifies the timeout adds-on value in seconds. The value must be a non-negative int64 number. A property expires if the sum of its value and the ttl_duration value is smaller than the current timestamp. If the ttl_duration value is 0 , the property never expires.","title":"TTL options"},{"location":"3.ngql-guide/8.clauses-and-options/ttl-options/#use_ttl_options","text":"You must use the TTL options together to set a valid timeout on a property.","title":"Use TTL options"},{"location":"3.ngql-guide/8.clauses-and-options/ttl-options/#set_a_timeout_if_a_tag_or_an_edge_type_exists","text":"If a tag or an edge type is already created, to set a timeout on a property bound to the tag or edge type, use ALTER to update the tag or edge type. # Create a tag. nebula> CREATE TAG IF NOT EXISTS t1 (a timestamp); # Use ALTER to update the tag and set the TTL options. nebula> ALTER TAG t1 ttl_col = \"a\", ttl_duration = 5; # Insert a vertex with tag t1. The vertex expires 5 seconds after the insertion. nebula> INSERT VERTEX t1(a) values \"101\":(now());","title":"Set a timeout if a tag or an edge type exists"},{"location":"3.ngql-guide/8.clauses-and-options/ttl-options/#set_a_timeout_when_creating_a_tag_or_an_edge_type","text":"Use TTL options in the CREATE statement to set a timeout when creating a tag or an edge type. For more information, see CREATE TAG and CREATE EDGE . # Create a tag and set the TTL options. nebula> CREATE TAG IF NOT EXISTS t2(a int, b int, c string) ttl_duration= 100, ttl_col = \"a\"; # Insert a vertex with tag t2. The timeout timestamp is 1648197238 (1648197138 + 100). nebula> INSERT VERTEX t2(a, b, c) values \"102\":(1648197138, 30, \"Hello\");","title":"Set a timeout when creating a tag or an edge type"},{"location":"3.ngql-guide/8.clauses-and-options/ttl-options/#remove_a_timeout","text":"To disable TTL and remove the timeout on a property, you can use the following approaches. Drop the property with the timeout. nebula> ALTER TAG t1 DROP (a); Set ttl_col to an empty string. nebula> ALTER TAG t1 ttl_col = \"\"; Set ttl_duration to 0 . This operation keeps the TTL options and prevents the property from expiring and the property schema from being modified. nebula> ALTER TAG t1 ttl_duration = 0;","title":"Remove a timeout"},{"location":"3.ngql-guide/8.clauses-and-options/unwind/","text":"UNWIND \u00b6 The UNWIND statement splits a list into separated rows. UNWIND can function as an individual statement or a clause in a statement. Syntax \u00b6 UNWIND <list> AS <alias> <RETURN clause>; Split a list \u00b6 The following example splits the list [1,2,3] into three rows. nebula> UNWIND [1,2,3] AS n RETURN n; +---+ | n | +---+ | 1 | | 2 | | 3 | +---+ Return a list with distinct items \u00b6 Use WITH DISTINCT in the UNWIND statement to return a list with distinct items. Example 1 \u00b6 The following statement: Splits the list [1,1,2,2,3,3] into rows. Removes duplicated rows. Sorts the rows. Transforms the rows to a list. nebula> WITH [1,1,2,2,3,3] AS n \\ UNWIND n AS r \\ WITH DISTINCT r AS r \\ ORDER BY r \\ RETURN collect(r); +------------+ | collect(r) | +------------+ | [1, 2, 3] | +------------+ Example 2 \u00b6 The following statement: Outputs the vertices on the matched path into a list. Splits the list into rows. Removes duplicated rows. Transforms the rows to a list. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})--(v2) \\ WITH nodes(p) AS n \\ UNWIND n AS r \\ WITH DISTINCT r AS r \\ RETURN collect(r); +----------------------------------------------------------------------------------------------------------------------+ | collect(r) | +----------------------------------------------------------------------------------------------------------------------+ | [(\"player100\" :player{age: 42, name: \"Tim Duncan\"}), (\"player101\" :player{age: 36, name: \"Tony Parker\"}), | |(\"team204\" :team{name: \"Spurs\"}), (\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"}), | |(\"player125\" :player{age: 41, name: \"Manu Ginobili\"}), (\"player104\" :player{age: 32, name: \"Marco Belinelli\"}), | |(\"player144\" :player{age: 47, name: \"Shaquile O'Neal\"}), (\"player105\" :player{age: 31, name: \"Danny Green\"}), | |(\"player113\" :player{age: 29, name: \"Dejounte Murray\"}), (\"player107\" :player{age: 32, name: \"Aron Baynes\"}), | |(\"player109\" :player{age: 34, name: \"Tiago Splitter\"}), (\"player108\" :player{age: 36, name: \"Boris Diaw\"})] | +----------------------------------------------------------------------------------------------------------------------+","title":"UNWIND"},{"location":"3.ngql-guide/8.clauses-and-options/unwind/#unwind","text":"The UNWIND statement splits a list into separated rows. UNWIND can function as an individual statement or a clause in a statement.","title":"UNWIND"},{"location":"3.ngql-guide/8.clauses-and-options/unwind/#syntax","text":"UNWIND <list> AS <alias> <RETURN clause>;","title":"Syntax"},{"location":"3.ngql-guide/8.clauses-and-options/unwind/#split_a_list","text":"The following example splits the list [1,2,3] into three rows. nebula> UNWIND [1,2,3] AS n RETURN n; +---+ | n | +---+ | 1 | | 2 | | 3 | +---+","title":"Split a list"},{"location":"3.ngql-guide/8.clauses-and-options/unwind/#return_a_list_with_distinct_items","text":"Use WITH DISTINCT in the UNWIND statement to return a list with distinct items.","title":"Return a list with distinct items"},{"location":"3.ngql-guide/8.clauses-and-options/unwind/#example_1","text":"The following statement: Splits the list [1,1,2,2,3,3] into rows. Removes duplicated rows. Sorts the rows. Transforms the rows to a list. nebula> WITH [1,1,2,2,3,3] AS n \\ UNWIND n AS r \\ WITH DISTINCT r AS r \\ ORDER BY r \\ RETURN collect(r); +------------+ | collect(r) | +------------+ | [1, 2, 3] | +------------+","title":"Example 1"},{"location":"3.ngql-guide/8.clauses-and-options/unwind/#example_2","text":"The following statement: Outputs the vertices on the matched path into a list. Splits the list into rows. Removes duplicated rows. Transforms the rows to a list. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})--(v2) \\ WITH nodes(p) AS n \\ UNWIND n AS r \\ WITH DISTINCT r AS r \\ RETURN collect(r); +----------------------------------------------------------------------------------------------------------------------+ | collect(r) | +----------------------------------------------------------------------------------------------------------------------+ | [(\"player100\" :player{age: 42, name: \"Tim Duncan\"}), (\"player101\" :player{age: 36, name: \"Tony Parker\"}), | |(\"team204\" :team{name: \"Spurs\"}), (\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"}), | |(\"player125\" :player{age: 41, name: \"Manu Ginobili\"}), (\"player104\" :player{age: 32, name: \"Marco Belinelli\"}), | |(\"player144\" :player{age: 47, name: \"Shaquile O'Neal\"}), (\"player105\" :player{age: 31, name: \"Danny Green\"}), | |(\"player113\" :player{age: 29, name: \"Dejounte Murray\"}), (\"player107\" :player{age: 32, name: \"Aron Baynes\"}), | |(\"player109\" :player{age: 34, name: \"Tiago Splitter\"}), (\"player108\" :player{age: 36, name: \"Boris Diaw\"})] | +----------------------------------------------------------------------------------------------------------------------+","title":"Example 2"},{"location":"3.ngql-guide/8.clauses-and-options/where/","text":"WHERE \u00b6 The WHERE clause filters the output by conditions. The WHERE clause usually works in the following queries: Native nGQL: such as GO and LOOKUP . OpenCypher syntax: such as MATCH and WITH . OpenCypher compatibility \u00b6 Filtering on edge rank is a native nGQL feature. To retrieve the rank value in openCypher statements, use the rank() function, such as MATCH (:player)-[e:follow]->() RETURN rank(e); . Basic usage \u00b6 Note In the following examples, $$ and $^ are reference operators. For more information, see Operators . Define conditions with boolean operators \u00b6 Use the boolean operators NOT , AND , OR , and XOR to define conditions in WHERE clauses. For the precedence of the operators, see Precedence . nebula> MATCH (v:player) \\ WHERE v.player.name == \"Tim Duncan\" \\ XOR (v.player.age < 30 AND v.player.name == \"Yao Ming\") \\ OR NOT (v.player.name == \"Yao Ming\" OR v.player.name == \"Tim Duncan\") \\ RETURN v.player.name, v.player.age; +-------------------------+--------------+ | v.player.name | v.player.age | +-------------------------+--------------+ | \"Danny Green\" | 31 | | \"Tiago Splitter\" | 34 | | \"David West\" | 38 | ... nebula> GO FROM \"player100\" \\ OVER follow \\ WHERE properties(edge).degree > 90 \\ OR properties($$).age != 33 \\ AND properties($$).name != \"Tony Parker\" \\ YIELD properties($$); +----------------------------------+ | properties($$) | +----------------------------------+ | {age: 41, name: \"Manu Ginobili\"} | +----------------------------------+ Filter on properties \u00b6 Use vertex or edge properties to define conditions in WHERE clauses. Filter on a vertex property: nebula> MATCH (v:player)-[e]->(v2) \\ WHERE v2.player.age < 25 \\ RETURN v2.player.name, v2.player.age; +----------------------+---------------+ | v2.player.name | v2.player.age | +----------------------+---------------+ | \"Ben Simmons\" | 22 | | \"Luka Doncic\" | 20 | | \"Kristaps Porzingis\" | 23 | +----------------------+---------------+ nebula> GO FROM \"player100\" OVER follow \\ WHERE $^.player.age >= 42 \\ YIELD dst(edge); +-------------+ | dst(EDGE) | +-------------+ +-------------+ | \"player101\" | | \"player125\" | +-------------+ Filter on an edge property: nebula> MATCH (v:player)-[e]->() \\ WHERE e.start_year < 2000 \\ RETURN DISTINCT v.player.name, v.player.age; +--------------------+--------------+ | v.player.name | v.player.age | +--------------------+--------------+ | \"Tony Parker\" | 36 | | \"Tim Duncan\" | 42 | | \"Grant Hill\" | 46 | ... nebula> GO FROM \"player100\" OVER follow \\ WHERE follow.degree > 90 \\ YIELD dst(edge); +-------------+ | dst(EDGE) | +-------------+ | \"player101\" | | \"player125\" | +-------------+ Filter on dynamically-calculated properties \u00b6 nebula> MATCH (v:player) \\ WHERE v[toLower(\"AGE\")] < 21 \\ RETURN v.player.name, v.player.age; +---------------+-------+ | v.name | v.age | +---------------+-------+ | \"Luka Doncic\" | 20 | +---------------+-------+ Filter on existing properties \u00b6 nebula> MATCH (v:player) \\ WHERE exists(v.player.age) \\ RETURN v.player.name, v.player.age; +-------------------------+--------------+ | v.player.name | v.player.age | +-------------------------+--------------+ | \"Danny Green\" | 31 | | \"Tiago Splitter\" | 34 | | \"David West\" | 38 | ... Filter on edge rank \u00b6 In nGQL, if a group of edges has the same source vertex, destination vertex, and properties, the only thing that distinguishes them is the rank. Use rank conditions in WHERE clauses to filter such edges. # The following example creates test data. nebula> CREATE SPACE IF NOT EXISTS test (vid_type=FIXED_STRING(30)); nebula> USE test; nebula> CREATE EDGE IF NOT EXISTS e1(p1 int); nebula> CREATE TAG IF NOT EXISTS person(p1 int); nebula> INSERT VERTEX person(p1) VALUES \"1\":(1); nebula> INSERT VERTEX person(p1) VALUES \"2\":(2); nebula> INSERT EDGE e1(p1) VALUES \"1\"->\"2\"@0:(10); nebula> INSERT EDGE e1(p1) VALUES \"1\"->\"2\"@1:(11); nebula> INSERT EDGE e1(p1) VALUES \"1\"->\"2\"@2:(12); nebula> INSERT EDGE e1(p1) VALUES \"1\"->\"2\"@3:(13); nebula> INSERT EDGE e1(p1) VALUES \"1\"->\"2\"@4:(14); nebula> INSERT EDGE e1(p1) VALUES \"1\"->\"2\"@5:(15); nebula> INSERT EDGE e1(p1) VALUES \"1\"->\"2\"@6:(16); # The following example use rank to filter edges and retrieves edges with a rank greater than 2. nebula> GO FROM \"1\" \\ OVER e1 \\ WHERE rank(edge) > 2 \\ YIELD src(edge), dst(edge), rank(edge) AS Rank, properties(edge).p1 | \\ ORDER BY $-.Rank DESC; +-----------+-----------+------+---------------------+ | src(EDGE) | dst(EDGE) | Rank | properties(EDGE).p1 | +-----------+-----------+------+---------------------+ | \"1\" | \"2\" | 6 | 16 | | \"1\" | \"2\" | 5 | 15 | | \"1\" | \"2\" | 4 | 14 | | \"1\" | \"2\" | 3 | 13 | +-----------+-----------+------+---------------------+ Filter on strings \u00b6 Use STARTS WITH , ENDS WITH , or CONTAINS in WHERE clauses to match a specific part of a string. String matching is case-sensitive. STARTS WITH \u00b6 STARTS WITH will match the beginning of a string. The following example uses STARTS WITH \"T\" to retrieve the information of players whose name starts with T . nebula> MATCH (v:player) \\ WHERE v.player.name STARTS WITH \"T\" \\ RETURN v.player.name, v.player.age; +------------------+--------------+ | v.player.name | v.player.age | +------------------+--------------+ | \"Tony Parker\" | 36 | | \"Tiago Splitter\" | 34 | | \"Tim Duncan\" | 42 | | \"Tracy McGrady\" | 39 | +------------------+--------------+ If you use STARTS WITH \"t\" in the preceding statement, an empty set is returned because no name in the dataset starts with the lowercase t . nebula> MATCH (v:player) \\ WHERE v.player.name STARTS WITH \"t\" \\ RETURN v.player.name, v.player.age; Empty set (time spent 5080/6474 us) ENDS WITH \u00b6 ENDS WITH will match the ending of a string. The following example uses ENDS WITH \"r\" to retrieve the information of players whose name ends with r . nebula> MATCH (v:player) \\ WHERE v.player.name ENDS WITH \"r\" \\ RETURN v.player.name, v.player.age; +------------------+--------------+ | v.player.name | v.player.age | +------------------+--------------+ | \"Tony Parker\" | 36 | | \"Tiago Splitter\" | 34 | | \"Vince Carter\" | 42 | +------------------+--------------+ CONTAINS \u00b6 CONTAINS will match a certain part of a string. The following example uses CONTAINS \"Pa\" to match the information of players whose name contains Pa . nebula> MATCH (v:player) \\ WHERE v.player.name CONTAINS \"Pa\" \\ RETURN v.player.name, v.player.age; +---------------+--------------+ | v.player.name | v.player.age | +---------------+--------------+ | \"Paul George\" | 28 | | \"Tony Parker\" | 36 | | \"Paul Gasol\" | 38 | | \"Chris Paul\" | 33 | +---------------+--------------+ Negative string matching \u00b6 You can use the boolean operator NOT to negate a string matching condition. nebula> MATCH (v:player) \\ WHERE NOT v.player.name ENDS WITH \"R\" \\ RETURN v.player.name, v.player.age; +-------------------------+--------------+ | v.player.name | v.player.age | +-------------------------+--------------+ | \"Danny Green\" | 31 | | \"Tiago Splitter\" | 34 | | \"David West\" | 38 | | \"Russell Westbrook\" | 30 | ... Filter on lists \u00b6 Match values in a list \u00b6 Use the IN operator to check if a value is in a specific list. nebula> MATCH (v:player) \\ WHERE v.player.age IN range(20,25) \\ RETURN v.player.name, v.player.age; +-------------------------+--------------+ | v.player.name | v.player.age | +-------------------------+--------------+ | \"Ben Simmons\" | 22 | | \"Giannis Antetokounmpo\" | 24 | | \"Kyle Anderson\" | 25 | | \"Joel Embiid\" | 25 | | \"Kristaps Porzingis\" | 23 | | \"Luka Doncic\" | 20 | +-------------------------+--------------+ nebula> LOOKUP ON player \\ WHERE player.age IN [25,28] \\ YIELD properties(vertex).name, properties(vertex).age; +-------------------------+------------------------+ | properties(VERTEX).name | properties(VERTEX).age | +-------------------------+------------------------+ | \"Kyle Anderson\" | 25 | | \"Damian Lillard\" | 28 | | \"Joel Embiid\" | 25 | | \"Paul George\" | 28 | | \"Ricky Rubio\" | 28 | +-------------------------+------------------------+ Match values not in a list \u00b6 Use NOT before IN to rule out the values in a list. nebula> MATCH (v:player) \\ WHERE v.player.age NOT IN range(20,25) \\ RETURN v.player.name AS Name, v.player.age AS Age \\ ORDER BY Age; +---------------------+-----+ | Name | Age | +---------------------+-----+ | \"Kyrie Irving\" | 26 | | \"Cory Joseph\" | 27 | | \"Damian Lillard\" | 28 | | \"Paul George\" | 28 | | \"Ricky Rubio\" | 28 | +---------------------+-----+ ...","title":"WHERE"},{"location":"3.ngql-guide/8.clauses-and-options/where/#where","text":"The WHERE clause filters the output by conditions. The WHERE clause usually works in the following queries: Native nGQL: such as GO and LOOKUP . OpenCypher syntax: such as MATCH and WITH .","title":"WHERE"},{"location":"3.ngql-guide/8.clauses-and-options/where/#opencypher_compatibility","text":"Filtering on edge rank is a native nGQL feature. To retrieve the rank value in openCypher statements, use the rank() function, such as MATCH (:player)-[e:follow]->() RETURN rank(e); .","title":"OpenCypher compatibility"},{"location":"3.ngql-guide/8.clauses-and-options/where/#basic_usage","text":"Note In the following examples, $$ and $^ are reference operators. For more information, see Operators .","title":"Basic usage"},{"location":"3.ngql-guide/8.clauses-and-options/where/#define_conditions_with_boolean_operators","text":"Use the boolean operators NOT , AND , OR , and XOR to define conditions in WHERE clauses. For the precedence of the operators, see Precedence . nebula> MATCH (v:player) \\ WHERE v.player.name == \"Tim Duncan\" \\ XOR (v.player.age < 30 AND v.player.name == \"Yao Ming\") \\ OR NOT (v.player.name == \"Yao Ming\" OR v.player.name == \"Tim Duncan\") \\ RETURN v.player.name, v.player.age; +-------------------------+--------------+ | v.player.name | v.player.age | +-------------------------+--------------+ | \"Danny Green\" | 31 | | \"Tiago Splitter\" | 34 | | \"David West\" | 38 | ... nebula> GO FROM \"player100\" \\ OVER follow \\ WHERE properties(edge).degree > 90 \\ OR properties($$).age != 33 \\ AND properties($$).name != \"Tony Parker\" \\ YIELD properties($$); +----------------------------------+ | properties($$) | +----------------------------------+ | {age: 41, name: \"Manu Ginobili\"} | +----------------------------------+","title":"Define conditions with boolean operators"},{"location":"3.ngql-guide/8.clauses-and-options/where/#filter_on_properties","text":"Use vertex or edge properties to define conditions in WHERE clauses. Filter on a vertex property: nebula> MATCH (v:player)-[e]->(v2) \\ WHERE v2.player.age < 25 \\ RETURN v2.player.name, v2.player.age; +----------------------+---------------+ | v2.player.name | v2.player.age | +----------------------+---------------+ | \"Ben Simmons\" | 22 | | \"Luka Doncic\" | 20 | | \"Kristaps Porzingis\" | 23 | +----------------------+---------------+ nebula> GO FROM \"player100\" OVER follow \\ WHERE $^.player.age >= 42 \\ YIELD dst(edge); +-------------+ | dst(EDGE) | +-------------+ +-------------+ | \"player101\" | | \"player125\" | +-------------+ Filter on an edge property: nebula> MATCH (v:player)-[e]->() \\ WHERE e.start_year < 2000 \\ RETURN DISTINCT v.player.name, v.player.age; +--------------------+--------------+ | v.player.name | v.player.age | +--------------------+--------------+ | \"Tony Parker\" | 36 | | \"Tim Duncan\" | 42 | | \"Grant Hill\" | 46 | ... nebula> GO FROM \"player100\" OVER follow \\ WHERE follow.degree > 90 \\ YIELD dst(edge); +-------------+ | dst(EDGE) | +-------------+ | \"player101\" | | \"player125\" | +-------------+","title":"Filter on properties"},{"location":"3.ngql-guide/8.clauses-and-options/where/#filter_on_dynamically-calculated_properties","text":"nebula> MATCH (v:player) \\ WHERE v[toLower(\"AGE\")] < 21 \\ RETURN v.player.name, v.player.age; +---------------+-------+ | v.name | v.age | +---------------+-------+ | \"Luka Doncic\" | 20 | +---------------+-------+","title":"Filter on dynamically-calculated properties"},{"location":"3.ngql-guide/8.clauses-and-options/where/#filter_on_existing_properties","text":"nebula> MATCH (v:player) \\ WHERE exists(v.player.age) \\ RETURN v.player.name, v.player.age; +-------------------------+--------------+ | v.player.name | v.player.age | +-------------------------+--------------+ | \"Danny Green\" | 31 | | \"Tiago Splitter\" | 34 | | \"David West\" | 38 | ...","title":"Filter on existing properties"},{"location":"3.ngql-guide/8.clauses-and-options/where/#filter_on_edge_rank","text":"In nGQL, if a group of edges has the same source vertex, destination vertex, and properties, the only thing that distinguishes them is the rank. Use rank conditions in WHERE clauses to filter such edges. # The following example creates test data. nebula> CREATE SPACE IF NOT EXISTS test (vid_type=FIXED_STRING(30)); nebula> USE test; nebula> CREATE EDGE IF NOT EXISTS e1(p1 int); nebula> CREATE TAG IF NOT EXISTS person(p1 int); nebula> INSERT VERTEX person(p1) VALUES \"1\":(1); nebula> INSERT VERTEX person(p1) VALUES \"2\":(2); nebula> INSERT EDGE e1(p1) VALUES \"1\"->\"2\"@0:(10); nebula> INSERT EDGE e1(p1) VALUES \"1\"->\"2\"@1:(11); nebula> INSERT EDGE e1(p1) VALUES \"1\"->\"2\"@2:(12); nebula> INSERT EDGE e1(p1) VALUES \"1\"->\"2\"@3:(13); nebula> INSERT EDGE e1(p1) VALUES \"1\"->\"2\"@4:(14); nebula> INSERT EDGE e1(p1) VALUES \"1\"->\"2\"@5:(15); nebula> INSERT EDGE e1(p1) VALUES \"1\"->\"2\"@6:(16); # The following example use rank to filter edges and retrieves edges with a rank greater than 2. nebula> GO FROM \"1\" \\ OVER e1 \\ WHERE rank(edge) > 2 \\ YIELD src(edge), dst(edge), rank(edge) AS Rank, properties(edge).p1 | \\ ORDER BY $-.Rank DESC; +-----------+-----------+------+---------------------+ | src(EDGE) | dst(EDGE) | Rank | properties(EDGE).p1 | +-----------+-----------+------+---------------------+ | \"1\" | \"2\" | 6 | 16 | | \"1\" | \"2\" | 5 | 15 | | \"1\" | \"2\" | 4 | 14 | | \"1\" | \"2\" | 3 | 13 | +-----------+-----------+------+---------------------+","title":"Filter on edge rank"},{"location":"3.ngql-guide/8.clauses-and-options/where/#filter_on_strings","text":"Use STARTS WITH , ENDS WITH , or CONTAINS in WHERE clauses to match a specific part of a string. String matching is case-sensitive.","title":"Filter on strings"},{"location":"3.ngql-guide/8.clauses-and-options/where/#starts_with","text":"STARTS WITH will match the beginning of a string. The following example uses STARTS WITH \"T\" to retrieve the information of players whose name starts with T . nebula> MATCH (v:player) \\ WHERE v.player.name STARTS WITH \"T\" \\ RETURN v.player.name, v.player.age; +------------------+--------------+ | v.player.name | v.player.age | +------------------+--------------+ | \"Tony Parker\" | 36 | | \"Tiago Splitter\" | 34 | | \"Tim Duncan\" | 42 | | \"Tracy McGrady\" | 39 | +------------------+--------------+ If you use STARTS WITH \"t\" in the preceding statement, an empty set is returned because no name in the dataset starts with the lowercase t . nebula> MATCH (v:player) \\ WHERE v.player.name STARTS WITH \"t\" \\ RETURN v.player.name, v.player.age; Empty set (time spent 5080/6474 us)","title":"STARTS WITH"},{"location":"3.ngql-guide/8.clauses-and-options/where/#ends_with","text":"ENDS WITH will match the ending of a string. The following example uses ENDS WITH \"r\" to retrieve the information of players whose name ends with r . nebula> MATCH (v:player) \\ WHERE v.player.name ENDS WITH \"r\" \\ RETURN v.player.name, v.player.age; +------------------+--------------+ | v.player.name | v.player.age | +------------------+--------------+ | \"Tony Parker\" | 36 | | \"Tiago Splitter\" | 34 | | \"Vince Carter\" | 42 | +------------------+--------------+","title":"ENDS WITH"},{"location":"3.ngql-guide/8.clauses-and-options/where/#contains","text":"CONTAINS will match a certain part of a string. The following example uses CONTAINS \"Pa\" to match the information of players whose name contains Pa . nebula> MATCH (v:player) \\ WHERE v.player.name CONTAINS \"Pa\" \\ RETURN v.player.name, v.player.age; +---------------+--------------+ | v.player.name | v.player.age | +---------------+--------------+ | \"Paul George\" | 28 | | \"Tony Parker\" | 36 | | \"Paul Gasol\" | 38 | | \"Chris Paul\" | 33 | +---------------+--------------+","title":"CONTAINS"},{"location":"3.ngql-guide/8.clauses-and-options/where/#negative_string_matching","text":"You can use the boolean operator NOT to negate a string matching condition. nebula> MATCH (v:player) \\ WHERE NOT v.player.name ENDS WITH \"R\" \\ RETURN v.player.name, v.player.age; +-------------------------+--------------+ | v.player.name | v.player.age | +-------------------------+--------------+ | \"Danny Green\" | 31 | | \"Tiago Splitter\" | 34 | | \"David West\" | 38 | | \"Russell Westbrook\" | 30 | ...","title":"Negative string matching"},{"location":"3.ngql-guide/8.clauses-and-options/where/#filter_on_lists","text":"","title":"Filter on lists"},{"location":"3.ngql-guide/8.clauses-and-options/where/#match_values_in_a_list","text":"Use the IN operator to check if a value is in a specific list. nebula> MATCH (v:player) \\ WHERE v.player.age IN range(20,25) \\ RETURN v.player.name, v.player.age; +-------------------------+--------------+ | v.player.name | v.player.age | +-------------------------+--------------+ | \"Ben Simmons\" | 22 | | \"Giannis Antetokounmpo\" | 24 | | \"Kyle Anderson\" | 25 | | \"Joel Embiid\" | 25 | | \"Kristaps Porzingis\" | 23 | | \"Luka Doncic\" | 20 | +-------------------------+--------------+ nebula> LOOKUP ON player \\ WHERE player.age IN [25,28] \\ YIELD properties(vertex).name, properties(vertex).age; +-------------------------+------------------------+ | properties(VERTEX).name | properties(VERTEX).age | +-------------------------+------------------------+ | \"Kyle Anderson\" | 25 | | \"Damian Lillard\" | 28 | | \"Joel Embiid\" | 25 | | \"Paul George\" | 28 | | \"Ricky Rubio\" | 28 | +-------------------------+------------------------+","title":"Match values in a list"},{"location":"3.ngql-guide/8.clauses-and-options/where/#match_values_not_in_a_list","text":"Use NOT before IN to rule out the values in a list. nebula> MATCH (v:player) \\ WHERE v.player.age NOT IN range(20,25) \\ RETURN v.player.name AS Name, v.player.age AS Age \\ ORDER BY Age; +---------------------+-----+ | Name | Age | +---------------------+-----+ | \"Kyrie Irving\" | 26 | | \"Cory Joseph\" | 27 | | \"Damian Lillard\" | 28 | | \"Paul George\" | 28 | | \"Ricky Rubio\" | 28 | +---------------------+-----+ ...","title":"Match values not in a list"},{"location":"3.ngql-guide/8.clauses-and-options/with/","text":"WITH \u00b6 The WITH clause can retrieve the output from a query part, process it, and pass it to the next query part as the input. OpenCypher compatibility \u00b6 This topic applies to openCypher syntax only. Note WITH has a similar function with the Pipe symbol in native nGQL, but they work in different ways. DO NOT use pipe symbols in the openCypher syntax or use WITH in native nGQL statements. Combine statements and form a composite query \u00b6 Use a WITH clause to combine statements and transfer the output of a statement as the input of another statement. Example 1 \u00b6 The following statement: Matches a path. Outputs all the vertices on the path to a list with the nodes() function. Unwinds the list into rows. Removes duplicated vertices and returns a set of distinct vertices. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})--() \\ WITH nodes(p) AS n \\ UNWIND n AS n1 \\ RETURN DISTINCT n1; +-----------------------------------------------------------+ | n1 | +-----------------------------------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | | (\"player101\" :player{age: 36, name: \"Tony Parker\"}) | | (\"team204\" :team{name: \"Spurs\"}) | | (\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"}) | | (\"player125\" :player{age: 41, name: \"Manu Ginobili\"}) | | (\"player104\" :player{age: 32, name: \"Marco Belinelli\"}) | | (\"player144\" :player{age: 47, name: \"Shaquille O'Neal\"}) | | (\"player105\" :player{age: 31, name: \"Danny Green\"}) | | (\"player113\" :player{age: 29, name: \"Dejounte Murray\"}) | | (\"player107\" :player{age: 32, name: \"Aron Baynes\"}) | | (\"player109\" :player{age: 34, name: \"Tiago Splitter\"}) | | (\"player108\" :player{age: 36, name: \"Boris Diaw\"}) | +-----------------------------------------------------------+ Example 2 \u00b6 The following statement: Matches the vertex with the VID player100 . Outputs all the tags of the vertex into a list with the labels() function. Unwinds the list into rows. Returns the output. nebula> MATCH (v) \\ WHERE id(v)==\"player100\" \\ WITH labels(v) AS tags_unf \\ UNWIND tags_unf AS tags_f \\ RETURN tags_f; +----------+ | tags_f | +----------+ | \"star\" | | \"player\" | | \"person\" | +----------+ Filter composite queries \u00b6 WITH can work as a filter in the middle of a composite query. nebula> MATCH (v:player)-->(v2:player) \\ WITH DISTINCT v2 AS v2, v2.player.age AS Age \\ ORDER BY Age \\ WHERE Age<25 \\ RETURN v2.player.name AS Name, Age; +----------------------+-----+ | Name | Age | +----------------------+-----+ | \"Luka Doncic\" | 20 | | \"Ben Simmons\" | 22 | | \"Kristaps Porzingis\" | 23 | +----------------------+-----+ Process the output before using collect() \u00b6 Use a WITH clause to sort and limit the output before using collect() to transform the output into a list. nebula> MATCH (v:player) \\ WITH v.player.name AS Name \\ ORDER BY Name DESC \\ LIMIT 3 \\ RETURN collect(Name); +-----------------------------------------------+ | collect(Name) | +-----------------------------------------------+ | [\"Yao Ming\", \"Vince Carter\", \"Tracy McGrady\"] | +-----------------------------------------------+ Use with RETURN \u00b6 Set an alias using a WITH clause, and then output the result through a RETURN clause. nebula> WITH [1, 2, 3] AS list RETURN 3 IN list AS r; +------+ | r | +------+ | true | +------+ nebula> WITH 4 AS one, 3 AS two RETURN one > two AS result; +--------+ | result | +--------+ | true | +--------+","title":"WITH"},{"location":"3.ngql-guide/8.clauses-and-options/with/#with","text":"The WITH clause can retrieve the output from a query part, process it, and pass it to the next query part as the input.","title":"WITH"},{"location":"3.ngql-guide/8.clauses-and-options/with/#opencypher_compatibility","text":"This topic applies to openCypher syntax only. Note WITH has a similar function with the Pipe symbol in native nGQL, but they work in different ways. DO NOT use pipe symbols in the openCypher syntax or use WITH in native nGQL statements.","title":"OpenCypher compatibility"},{"location":"3.ngql-guide/8.clauses-and-options/with/#combine_statements_and_form_a_composite_query","text":"Use a WITH clause to combine statements and transfer the output of a statement as the input of another statement.","title":"Combine statements and form a composite query"},{"location":"3.ngql-guide/8.clauses-and-options/with/#example_1","text":"The following statement: Matches a path. Outputs all the vertices on the path to a list with the nodes() function. Unwinds the list into rows. Removes duplicated vertices and returns a set of distinct vertices. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})--() \\ WITH nodes(p) AS n \\ UNWIND n AS n1 \\ RETURN DISTINCT n1; +-----------------------------------------------------------+ | n1 | +-----------------------------------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | | (\"player101\" :player{age: 36, name: \"Tony Parker\"}) | | (\"team204\" :team{name: \"Spurs\"}) | | (\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"}) | | (\"player125\" :player{age: 41, name: \"Manu Ginobili\"}) | | (\"player104\" :player{age: 32, name: \"Marco Belinelli\"}) | | (\"player144\" :player{age: 47, name: \"Shaquille O'Neal\"}) | | (\"player105\" :player{age: 31, name: \"Danny Green\"}) | | (\"player113\" :player{age: 29, name: \"Dejounte Murray\"}) | | (\"player107\" :player{age: 32, name: \"Aron Baynes\"}) | | (\"player109\" :player{age: 34, name: \"Tiago Splitter\"}) | | (\"player108\" :player{age: 36, name: \"Boris Diaw\"}) | +-----------------------------------------------------------+","title":"Example 1"},{"location":"3.ngql-guide/8.clauses-and-options/with/#example_2","text":"The following statement: Matches the vertex with the VID player100 . Outputs all the tags of the vertex into a list with the labels() function. Unwinds the list into rows. Returns the output. nebula> MATCH (v) \\ WHERE id(v)==\"player100\" \\ WITH labels(v) AS tags_unf \\ UNWIND tags_unf AS tags_f \\ RETURN tags_f; +----------+ | tags_f | +----------+ | \"star\" | | \"player\" | | \"person\" | +----------+","title":"Example 2"},{"location":"3.ngql-guide/8.clauses-and-options/with/#filter_composite_queries","text":"WITH can work as a filter in the middle of a composite query. nebula> MATCH (v:player)-->(v2:player) \\ WITH DISTINCT v2 AS v2, v2.player.age AS Age \\ ORDER BY Age \\ WHERE Age<25 \\ RETURN v2.player.name AS Name, Age; +----------------------+-----+ | Name | Age | +----------------------+-----+ | \"Luka Doncic\" | 20 | | \"Ben Simmons\" | 22 | | \"Kristaps Porzingis\" | 23 | +----------------------+-----+","title":"Filter composite queries"},{"location":"3.ngql-guide/8.clauses-and-options/with/#process_the_output_before_using_collect","text":"Use a WITH clause to sort and limit the output before using collect() to transform the output into a list. nebula> MATCH (v:player) \\ WITH v.player.name AS Name \\ ORDER BY Name DESC \\ LIMIT 3 \\ RETURN collect(Name); +-----------------------------------------------+ | collect(Name) | +-----------------------------------------------+ | [\"Yao Ming\", \"Vince Carter\", \"Tracy McGrady\"] | +-----------------------------------------------+","title":"Process the output before using collect()"},{"location":"3.ngql-guide/8.clauses-and-options/with/#use_with_return","text":"Set an alias using a WITH clause, and then output the result through a RETURN clause. nebula> WITH [1, 2, 3] AS list RETURN 3 IN list AS r; +------+ | r | +------+ | true | +------+ nebula> WITH 4 AS one, 3 AS two RETURN one > two AS result; +--------+ | result | +--------+ | true | +--------+","title":"Use with RETURN"},{"location":"3.ngql-guide/8.clauses-and-options/yield/","text":"YIELD \u00b6 YIELD defines the output of an nGQL query. YIELD can lead a clause or a statement: A YIELD clause works in nGQL statements such as GO , FETCH , or LOOKUP and must be defined to return the result. A YIELD statement works in a composite query or independently. OpenCypher compatibility \u00b6 This topic applies to native nGQL only. For the openCypher syntax, use RETURN . YIELD has different functions in openCypher and nGQL. In openCypher, YIELD is used in the CALL[\u2026YIELD] clause to specify the output of the procedure call. Note NGQL does not support CALL[\u2026YIELD] yet. In nGQL, YIELD works like RETURN in openCypher. Note In the following examples, $$ and $- are reference operators. For more information, see Operators . YIELD clauses \u00b6 Syntax \u00b6 YIELD [DISTINCT] <col> [AS <alias>] [, <col> [AS <alias>] ...]; Parameter Description DISTINCT Aggregates the output and makes the statement return a distinct result set. col A field to be returned. If no alias is set, col will be a column name in the output. alias An alias for col . It is set after the keyword AS and will be a column name in the output. Use a YIELD clause in a statement \u00b6 Use YIELD with GO : nebula> GO FROM \"player100\" OVER follow \\ YIELD properties($$).name AS Friend, properties($$).age AS Age; +-----------------+-----+ | Friend | Age | +-----------------+-----+ | \"Tony Parker\" | 36 | | \"Manu Ginobili\" | 41 | +-----------------+-----+ Use YIELD with FETCH : nebula> FETCH PROP ON player \"player100\" \\ YIELD properties(vertex).name; +-------------------------+ | properties(VERTEX).name | +-------------------------+ | \"Tim Duncan\" | +-------------------------+ Use YIELD with LOOKUP : nebula> LOOKUP ON player WHERE player.name == \"Tony Parker\" \\ YIELD properties(vertex).name, properties(vertex).age; +-------------------------+------------------------+ | properties(VERTEX).name | properties(VERTEX).age | +-------------------------+------------------------+ | \"Tony Parker\" | 36 | +-------------------------+------------------------+ YIELD statements \u00b6 Syntax \u00b6 YIELD [DISTINCT] <col> [AS <alias>] [, <col> [AS <alias>] ...] [WHERE <conditions>]; | Parameter | Description | |--------------+---------------------------------------------------------------------------------------------------------| | DISTINCT | Aggregates the output and makes the statement return a distinct result set. | | col | A field to be returned. If no alias is set, col will be a column name in the output. | | alias | An alias for col . It is set after the keyword AS and will be a column name in the output. | | conditions | Conditions set in a WHERE clause to filter the output. For more information, see WHERE . | Use a YIELD statement in a composite query \u00b6 In a composite query , a YIELD statement accepts, filters, and modifies the result set of the preceding statement, and then outputs it. The following query finds the players that \"player100\" follows and calculates their average age. nebula> GO FROM \"player100\" OVER follow \\ YIELD dst(edge) AS ID \\ | FETCH PROP ON player $-.ID \\ YIELD properties(vertex).age AS Age \\ | YIELD AVG($-.Age) as Avg_age, count(*)as Num_friends; +---------+-------------+ | Avg_age | Num_friends | +---------+-------------+ | 38.5 | 2 | +---------+-------------+ The following query finds the players that \"player101\" follows with the follow degrees greater than 90. nebula> $var1 = GO FROM \"player101\" OVER follow \\ YIELD properties(edge).degree AS Degree, dst(edge) as ID; \\ YIELD $var1.ID AS ID WHERE $var1.Degree > 90; +-------------+ | ID | +-------------+ | \"player100\" | | \"player125\" | +-------------+ Use a standalone YIELD statement \u00b6 A YIELD statement can calculate a valid expression and output the result. nebula> YIELD rand32(1, 6); +-------------+ | rand32(1,6) | +-------------+ | 3 | +-------------+ nebula> YIELD \"Hel\" + \"\\tlo\" AS string1, \", World!\" AS string2; +-------------+------------+ | string1 | string2 | +-------------+------------+ | \"Hel lo\" | \", World!\" | +-------------+------------+ nebula> YIELD hash(\"Tim\") % 100; +-----------------+ | (hash(Tim)%100) | +-----------------+ | 42 | +-----------------+ nebula> YIELD \\ CASE 2+3 \\ WHEN 4 THEN 0 \\ WHEN 5 THEN 1 \\ ELSE -1 \\ END \\ AS result; +--------+ | result | +--------+ | 1 | +--------+","title":"YIELD"},{"location":"3.ngql-guide/8.clauses-and-options/yield/#yield","text":"YIELD defines the output of an nGQL query. YIELD can lead a clause or a statement: A YIELD clause works in nGQL statements such as GO , FETCH , or LOOKUP and must be defined to return the result. A YIELD statement works in a composite query or independently.","title":"YIELD"},{"location":"3.ngql-guide/8.clauses-and-options/yield/#opencypher_compatibility","text":"This topic applies to native nGQL only. For the openCypher syntax, use RETURN . YIELD has different functions in openCypher and nGQL. In openCypher, YIELD is used in the CALL[\u2026YIELD] clause to specify the output of the procedure call. Note NGQL does not support CALL[\u2026YIELD] yet. In nGQL, YIELD works like RETURN in openCypher. Note In the following examples, $$ and $- are reference operators. For more information, see Operators .","title":"OpenCypher compatibility"},{"location":"3.ngql-guide/8.clauses-and-options/yield/#yield_clauses","text":"","title":"YIELD clauses"},{"location":"3.ngql-guide/8.clauses-and-options/yield/#syntax","text":"YIELD [DISTINCT] <col> [AS <alias>] [, <col> [AS <alias>] ...]; Parameter Description DISTINCT Aggregates the output and makes the statement return a distinct result set. col A field to be returned. If no alias is set, col will be a column name in the output. alias An alias for col . It is set after the keyword AS and will be a column name in the output.","title":"Syntax"},{"location":"3.ngql-guide/8.clauses-and-options/yield/#use_a_yield_clause_in_a_statement","text":"Use YIELD with GO : nebula> GO FROM \"player100\" OVER follow \\ YIELD properties($$).name AS Friend, properties($$).age AS Age; +-----------------+-----+ | Friend | Age | +-----------------+-----+ | \"Tony Parker\" | 36 | | \"Manu Ginobili\" | 41 | +-----------------+-----+ Use YIELD with FETCH : nebula> FETCH PROP ON player \"player100\" \\ YIELD properties(vertex).name; +-------------------------+ | properties(VERTEX).name | +-------------------------+ | \"Tim Duncan\" | +-------------------------+ Use YIELD with LOOKUP : nebula> LOOKUP ON player WHERE player.name == \"Tony Parker\" \\ YIELD properties(vertex).name, properties(vertex).age; +-------------------------+------------------------+ | properties(VERTEX).name | properties(VERTEX).age | +-------------------------+------------------------+ | \"Tony Parker\" | 36 | +-------------------------+------------------------+","title":"Use a YIELD clause in a statement"},{"location":"3.ngql-guide/8.clauses-and-options/yield/#yield_statements","text":"","title":"YIELD statements"},{"location":"3.ngql-guide/8.clauses-and-options/yield/#syntax_1","text":"YIELD [DISTINCT] <col> [AS <alias>] [, <col> [AS <alias>] ...] [WHERE <conditions>]; | Parameter | Description | |--------------+---------------------------------------------------------------------------------------------------------| | DISTINCT | Aggregates the output and makes the statement return a distinct result set. | | col | A field to be returned. If no alias is set, col will be a column name in the output. | | alias | An alias for col . It is set after the keyword AS and will be a column name in the output. | | conditions | Conditions set in a WHERE clause to filter the output. For more information, see WHERE . |","title":"Syntax"},{"location":"3.ngql-guide/8.clauses-and-options/yield/#use_a_yield_statement_in_a_composite_query","text":"In a composite query , a YIELD statement accepts, filters, and modifies the result set of the preceding statement, and then outputs it. The following query finds the players that \"player100\" follows and calculates their average age. nebula> GO FROM \"player100\" OVER follow \\ YIELD dst(edge) AS ID \\ | FETCH PROP ON player $-.ID \\ YIELD properties(vertex).age AS Age \\ | YIELD AVG($-.Age) as Avg_age, count(*)as Num_friends; +---------+-------------+ | Avg_age | Num_friends | +---------+-------------+ | 38.5 | 2 | +---------+-------------+ The following query finds the players that \"player101\" follows with the follow degrees greater than 90. nebula> $var1 = GO FROM \"player101\" OVER follow \\ YIELD properties(edge).degree AS Degree, dst(edge) as ID; \\ YIELD $var1.ID AS ID WHERE $var1.Degree > 90; +-------------+ | ID | +-------------+ | \"player100\" | | \"player125\" | +-------------+","title":"Use a YIELD statement in a composite query"},{"location":"3.ngql-guide/8.clauses-and-options/yield/#use_a_standalone_yield_statement","text":"A YIELD statement can calculate a valid expression and output the result. nebula> YIELD rand32(1, 6); +-------------+ | rand32(1,6) | +-------------+ | 3 | +-------------+ nebula> YIELD \"Hel\" + \"\\tlo\" AS string1, \", World!\" AS string2; +-------------+------------+ | string1 | string2 | +-------------+------------+ | \"Hel lo\" | \", World!\" | +-------------+------------+ nebula> YIELD hash(\"Tim\") % 100; +-----------------+ | (hash(Tim)%100) | +-----------------+ | 42 | +-----------------+ nebula> YIELD \\ CASE 2+3 \\ WHEN 4 THEN 0 \\ WHEN 5 THEN 1 \\ ELSE -1 \\ END \\ AS result; +--------+ | result | +--------+ | 1 | +--------+","title":"Use a standalone YIELD statement"},{"location":"3.ngql-guide/9.space-statements/1.create-space/","text":"CREATE SPACE \u00b6 Graph spaces are used to store data in a physically isolated way in Nebula Graph, which is similar to the database concept in MySQL. The CREATE SPACE statement can create a new graph space or clone the schema of an existing graph space. Prerequisites \u00b6 Only the God role can use the CREATE SPACE statement. For more information, see AUTHENTICATION . Syntax \u00b6 Create graph spaces \u00b6 CREATE SPACE [IF NOT EXISTS] <graph_space_name> ( [partition_num = <partition_number>,] [replica_factor = <replica_number>,] vid_type = {FIXED_STRING(<N>) | INT[64]} ) [COMMENT = '<comment>']; Parameter Description IF NOT EXISTS Detects if the related graph space exists. If it does not exist, a new one will be created. The graph space existence detection here only compares the graph space name (excluding properties). <graph_space_name> Uniquely identifies a graph space in a Nebula Graph instance. The name of the graph space starts with a letter, supports 1 to 4 bytes UTF-8 encoded characters, such as English letters (case-sensitive), digits, and Chinese characters, but does not support special characters except underscores. To use special characters or reserved keywords as identifiers, quote them with backticks. For more information, see Keywords and reserved words . partition_num Specifies the number of partitions in each replica. The suggested value is 20 times (2 times for HDD) the number of the hard disks in the cluster. For example, if you have three hard disks in the cluster, we recommend that you set 60 partitions. The default value is 100. replica_factor Specifies the number of replicas in the cluster. The suggested number is 3 in a production environment and 1 in a test environment. The replica number must be an odd number for the need of quorum-based voting. The default value is 1. vid_type A required parameter. Specifies the VID type in a graph space. Available values are FIXED_STRING(N) and INT64 . INT equals to INT64 . FIXED_STRING(<N>) specifies the VID as a string, while INT64 specifies it as an integer. N represents the maximum length of the VIDs. If you set a VID that is longer than N characters, Nebula Graph throws an error. COMMENT The remarks of the graph space. The maximum length is 256 bytes. By default, there is no comments on a space. Caution If the replica number is set to one, you will not be able to load balance or scale out the Nebula Graph Storage Service with the BALANCE statement. Restrictions on VID type change and VID length The length of the VID should not be longer than N characters. If it exceeds N , Nebula Graph throws The VID must be a 64-bit integer or a string fitting space vertex id length limit. . Note graph_space_name , partition_num , replica_factor , vid_type , and comment cannot be modified once set. To modify them, drop the current working graph space with DROP SPACE and create a new one with CREATE SPACE . Clone graph spaces \u00b6 CREATE SPACE <new_graph_space_name> AS <old_graph_space_name>; Parameter Description <new_graph_space_name> The name of the graph space that is newly created. The name of the graph space starts with a letter, supports 1 to 4 bytes UTF-8 encoded characters, such as English letters (case-sensitive), digits, and Chinese characters, but does not support special characters except underscores. For more information, see Keywords and reserved words . When a new graph space is created, the schema of the old graph space <old_graph_space_name> will be cloned, including its parameters (the number of partitions and replicas, etc.), Tag, Edge type and native indexes. <old_graph_space_name> The name of the graph space that already exists. Examples \u00b6 # The following example creates a graph space with a specified VID type and the maximum length. Other fields still use the default values. nebula> CREATE SPACE IF NOT EXISTS my_space_1 (vid_type=FIXED_STRING(30)); # The following example creates a graph space with a specified partition number, replica number, and VID type. nebula> CREATE SPACE IF NOT EXISTS my_space_2 (partition_num=15, replica_factor=1, vid_type=FIXED_STRING(30)); # The following example creates a graph space with a specified partition number, replica number, and VID type, and adds a comment on it. nebula> CREATE SPACE IF NOT EXISTS my_space_3 (partition_num=15, replica_factor=1, vid_type=FIXED_STRING(30)) comment=\"Test the graph space\"; # Clone a graph space. nebula> CREATE SPACE IF NOT EXISTS my_space_4 as my_space_3; nebula> SHOW CREATE SPACE my_space_4; +--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Space | Create Space | +--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | \"my_space_4\" | \"CREATE SPACE `my_space_4` (partition_num = 15, replica_factor = 1, charset = utf8, collate = utf8_bin, vid_type = FIXED_STRING(30)) ON default comment = 'Test the graph space'\" | +--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Implementation of the operation \u00b6 Caution Trying to use a newly created graph space may fail because the creation is implemented asynchronously. To make sure the follow-up operations work as expected, Wait for two heartbeat cycles, i.e., 20 seconds. To change the heartbeat interval, modify the heartbeat_interval_secs parameter in the configuration files for all services. If the heartbeat interval is too short (i.e., less than 5 seconds), disconnection between peers may happen because of the misjudgment of machines in the distributed system. Check partition distribution \u00b6 On some large clusters, the partition distribution is possibly unbalanced because of the different startup times. You can run the following command to do a check of the machine distribution. nebula> SHOW HOSTS; +-------------+------+-----------+----------+--------------+--------------------------------+--------------------------------+---------+ | Host | Port | HTTP port | Status | Leader count | Leader distribution | Partition distribution | Version | +-------------+------+-----------+----------+--------------+--------------------------------+--------------------------------+---------+ | \"storaged0\" | 9779 | 19669 | \"ONLINE\" | 8 | \"basketballplayer:3, test:5\" | \"basketballplayer:10, test:10\" | \"3.1.0\" | | \"storaged1\" | 9779 | 19669 | \"ONLINE\" | 9 | \"basketballplayer:4, test:5\" | \"basketballplayer:10, test:10\" | \"3.1.0\" | | \"storaged2\" | 9779 | 19669 | \"ONLINE\" | 3 | \"basketballplayer:3\" | \"basketballplayer:10, test:10\" | \"3.1.0\" | +-------------+------+-----------+----------+--------------+--------------------------------+--------------------------------+---------+ To balance the request loads, use the following command. nebula> BALANCE LEADER; nebula> SHOW HOSTS; +-------------+------+-----------+----------+--------------+--------------------------------+--------------------------------+---------+ | Host | Port | HTTP port | Status | Leader count | Leader distribution | Partition distribution | Version | +-------------+------+-----------+----------+--------------+--------------------------------+--------------------------------+---------+ | \"storaged0\" | 9779 | 19669 | \"ONLINE\" | 7 | \"basketballplayer:3, test:4\" | \"basketballplayer:10, test:10\" | \"3.1.0\" | | \"storaged1\" | 9779 | 19669 | \"ONLINE\" | 7 | \"basketballplayer:4, test:3\" | \"basketballplayer:10, test:10\" | \"3.1.0\" | | \"storaged2\" | 9779 | 19669 | \"ONLINE\" | 6 | \"basketballplayer:3, test:3\" | \"basketballplayer:10, test:10\" | \"3.1.0\" | +-------------+------+-----------+----------+--------------+--------------------------------+--------------------------------+---------+","title":"CREATE SPACE"},{"location":"3.ngql-guide/9.space-statements/1.create-space/#create_space","text":"Graph spaces are used to store data in a physically isolated way in Nebula Graph, which is similar to the database concept in MySQL. The CREATE SPACE statement can create a new graph space or clone the schema of an existing graph space.","title":"CREATE SPACE"},{"location":"3.ngql-guide/9.space-statements/1.create-space/#prerequisites","text":"Only the God role can use the CREATE SPACE statement. For more information, see AUTHENTICATION .","title":"Prerequisites"},{"location":"3.ngql-guide/9.space-statements/1.create-space/#syntax","text":"","title":"Syntax"},{"location":"3.ngql-guide/9.space-statements/1.create-space/#create_graph_spaces","text":"CREATE SPACE [IF NOT EXISTS] <graph_space_name> ( [partition_num = <partition_number>,] [replica_factor = <replica_number>,] vid_type = {FIXED_STRING(<N>) | INT[64]} ) [COMMENT = '<comment>']; Parameter Description IF NOT EXISTS Detects if the related graph space exists. If it does not exist, a new one will be created. The graph space existence detection here only compares the graph space name (excluding properties). <graph_space_name> Uniquely identifies a graph space in a Nebula Graph instance. The name of the graph space starts with a letter, supports 1 to 4 bytes UTF-8 encoded characters, such as English letters (case-sensitive), digits, and Chinese characters, but does not support special characters except underscores. To use special characters or reserved keywords as identifiers, quote them with backticks. For more information, see Keywords and reserved words . partition_num Specifies the number of partitions in each replica. The suggested value is 20 times (2 times for HDD) the number of the hard disks in the cluster. For example, if you have three hard disks in the cluster, we recommend that you set 60 partitions. The default value is 100. replica_factor Specifies the number of replicas in the cluster. The suggested number is 3 in a production environment and 1 in a test environment. The replica number must be an odd number for the need of quorum-based voting. The default value is 1. vid_type A required parameter. Specifies the VID type in a graph space. Available values are FIXED_STRING(N) and INT64 . INT equals to INT64 . FIXED_STRING(<N>) specifies the VID as a string, while INT64 specifies it as an integer. N represents the maximum length of the VIDs. If you set a VID that is longer than N characters, Nebula Graph throws an error. COMMENT The remarks of the graph space. The maximum length is 256 bytes. By default, there is no comments on a space. Caution If the replica number is set to one, you will not be able to load balance or scale out the Nebula Graph Storage Service with the BALANCE statement. Restrictions on VID type change and VID length The length of the VID should not be longer than N characters. If it exceeds N , Nebula Graph throws The VID must be a 64-bit integer or a string fitting space vertex id length limit. . Note graph_space_name , partition_num , replica_factor , vid_type , and comment cannot be modified once set. To modify them, drop the current working graph space with DROP SPACE and create a new one with CREATE SPACE .","title":"Create graph spaces"},{"location":"3.ngql-guide/9.space-statements/1.create-space/#clone_graph_spaces","text":"CREATE SPACE <new_graph_space_name> AS <old_graph_space_name>; Parameter Description <new_graph_space_name> The name of the graph space that is newly created. The name of the graph space starts with a letter, supports 1 to 4 bytes UTF-8 encoded characters, such as English letters (case-sensitive), digits, and Chinese characters, but does not support special characters except underscores. For more information, see Keywords and reserved words . When a new graph space is created, the schema of the old graph space <old_graph_space_name> will be cloned, including its parameters (the number of partitions and replicas, etc.), Tag, Edge type and native indexes. <old_graph_space_name> The name of the graph space that already exists.","title":"Clone graph spaces"},{"location":"3.ngql-guide/9.space-statements/1.create-space/#examples","text":"# The following example creates a graph space with a specified VID type and the maximum length. Other fields still use the default values. nebula> CREATE SPACE IF NOT EXISTS my_space_1 (vid_type=FIXED_STRING(30)); # The following example creates a graph space with a specified partition number, replica number, and VID type. nebula> CREATE SPACE IF NOT EXISTS my_space_2 (partition_num=15, replica_factor=1, vid_type=FIXED_STRING(30)); # The following example creates a graph space with a specified partition number, replica number, and VID type, and adds a comment on it. nebula> CREATE SPACE IF NOT EXISTS my_space_3 (partition_num=15, replica_factor=1, vid_type=FIXED_STRING(30)) comment=\"Test the graph space\"; # Clone a graph space. nebula> CREATE SPACE IF NOT EXISTS my_space_4 as my_space_3; nebula> SHOW CREATE SPACE my_space_4; +--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Space | Create Space | +--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | \"my_space_4\" | \"CREATE SPACE `my_space_4` (partition_num = 15, replica_factor = 1, charset = utf8, collate = utf8_bin, vid_type = FIXED_STRING(30)) ON default comment = 'Test the graph space'\" | +--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+","title":"Examples"},{"location":"3.ngql-guide/9.space-statements/1.create-space/#implementation_of_the_operation","text":"Caution Trying to use a newly created graph space may fail because the creation is implemented asynchronously. To make sure the follow-up operations work as expected, Wait for two heartbeat cycles, i.e., 20 seconds. To change the heartbeat interval, modify the heartbeat_interval_secs parameter in the configuration files for all services. If the heartbeat interval is too short (i.e., less than 5 seconds), disconnection between peers may happen because of the misjudgment of machines in the distributed system.","title":"Implementation of the operation"},{"location":"3.ngql-guide/9.space-statements/1.create-space/#check_partition_distribution","text":"On some large clusters, the partition distribution is possibly unbalanced because of the different startup times. You can run the following command to do a check of the machine distribution. nebula> SHOW HOSTS; +-------------+------+-----------+----------+--------------+--------------------------------+--------------------------------+---------+ | Host | Port | HTTP port | Status | Leader count | Leader distribution | Partition distribution | Version | +-------------+------+-----------+----------+--------------+--------------------------------+--------------------------------+---------+ | \"storaged0\" | 9779 | 19669 | \"ONLINE\" | 8 | \"basketballplayer:3, test:5\" | \"basketballplayer:10, test:10\" | \"3.1.0\" | | \"storaged1\" | 9779 | 19669 | \"ONLINE\" | 9 | \"basketballplayer:4, test:5\" | \"basketballplayer:10, test:10\" | \"3.1.0\" | | \"storaged2\" | 9779 | 19669 | \"ONLINE\" | 3 | \"basketballplayer:3\" | \"basketballplayer:10, test:10\" | \"3.1.0\" | +-------------+------+-----------+----------+--------------+--------------------------------+--------------------------------+---------+ To balance the request loads, use the following command. nebula> BALANCE LEADER; nebula> SHOW HOSTS; +-------------+------+-----------+----------+--------------+--------------------------------+--------------------------------+---------+ | Host | Port | HTTP port | Status | Leader count | Leader distribution | Partition distribution | Version | +-------------+------+-----------+----------+--------------+--------------------------------+--------------------------------+---------+ | \"storaged0\" | 9779 | 19669 | \"ONLINE\" | 7 | \"basketballplayer:3, test:4\" | \"basketballplayer:10, test:10\" | \"3.1.0\" | | \"storaged1\" | 9779 | 19669 | \"ONLINE\" | 7 | \"basketballplayer:4, test:3\" | \"basketballplayer:10, test:10\" | \"3.1.0\" | | \"storaged2\" | 9779 | 19669 | \"ONLINE\" | 6 | \"basketballplayer:3, test:3\" | \"basketballplayer:10, test:10\" | \"3.1.0\" | +-------------+------+-----------+----------+--------------+--------------------------------+--------------------------------+---------+","title":"Check partition distribution"},{"location":"3.ngql-guide/9.space-statements/2.use-space/","text":"USE \u00b6 USE specifies a graph space as the current working graph space for subsequent queries. Prerequisites \u00b6 Running the USE statement requires some privileges for the graph space. Otherwise, Nebula Graph throws an error. Syntax \u00b6 USE <graph_space_name>; Examples \u00b6 # The following example creates two sample spaces. nebula> CREATE SPACE IF NOT EXISTS space1 (vid_type=FIXED_STRING(30)); nebula> CREATE SPACE IF NOT EXISTS space2 (vid_type=FIXED_STRING(30)); # The following example specifies space1 as the current working graph space. nebula> USE space1; # The following example specifies space2 as the current working graph space. Hereafter, you cannot read any data from space1, because these vertices and edges being traversed have no relevance with space1. nebula> USE space2; Caution You cannot use two graph spaces in one statement. Different from Fabric Cypher, graph spaces in Nebula Graph are fully isolated from each other. Making a graph space as the working graph space prevents you from accessing other spaces. The only way to traverse in a new graph space is to switch by the USE statement. In Fabric Cypher, you can use two graph spaces in one statement (using the USE + CALL syntax). But in Nebula Graph, you can only use one graph space in one statement.","title":"USE SPACE"},{"location":"3.ngql-guide/9.space-statements/2.use-space/#use","text":"USE specifies a graph space as the current working graph space for subsequent queries.","title":"USE"},{"location":"3.ngql-guide/9.space-statements/2.use-space/#prerequisites","text":"Running the USE statement requires some privileges for the graph space. Otherwise, Nebula Graph throws an error.","title":"Prerequisites"},{"location":"3.ngql-guide/9.space-statements/2.use-space/#syntax","text":"USE <graph_space_name>;","title":"Syntax"},{"location":"3.ngql-guide/9.space-statements/2.use-space/#examples","text":"# The following example creates two sample spaces. nebula> CREATE SPACE IF NOT EXISTS space1 (vid_type=FIXED_STRING(30)); nebula> CREATE SPACE IF NOT EXISTS space2 (vid_type=FIXED_STRING(30)); # The following example specifies space1 as the current working graph space. nebula> USE space1; # The following example specifies space2 as the current working graph space. Hereafter, you cannot read any data from space1, because these vertices and edges being traversed have no relevance with space1. nebula> USE space2; Caution You cannot use two graph spaces in one statement. Different from Fabric Cypher, graph spaces in Nebula Graph are fully isolated from each other. Making a graph space as the working graph space prevents you from accessing other spaces. The only way to traverse in a new graph space is to switch by the USE statement. In Fabric Cypher, you can use two graph spaces in one statement (using the USE + CALL syntax). But in Nebula Graph, you can only use one graph space in one statement.","title":"Examples"},{"location":"3.ngql-guide/9.space-statements/3.show-spaces/","text":"SHOW SPACES \u00b6 SHOW SPACES lists all the graph spaces in the Nebula Graph examples. Syntax \u00b6 SHOW SPACES; Example \u00b6 nebula> SHOW SPACES; +--------------------+ | Name | +--------------------+ | \"cba\" | | \"basketballplayer\" | +--------------------+ To create graph spaces, see CREATE SPACE .","title":"SHOW SPACES"},{"location":"3.ngql-guide/9.space-statements/3.show-spaces/#show_spaces","text":"SHOW SPACES lists all the graph spaces in the Nebula Graph examples.","title":"SHOW SPACES"},{"location":"3.ngql-guide/9.space-statements/3.show-spaces/#syntax","text":"SHOW SPACES;","title":"Syntax"},{"location":"3.ngql-guide/9.space-statements/3.show-spaces/#example","text":"nebula> SHOW SPACES; +--------------------+ | Name | +--------------------+ | \"cba\" | | \"basketballplayer\" | +--------------------+ To create graph spaces, see CREATE SPACE .","title":"Example"},{"location":"3.ngql-guide/9.space-statements/4.describe-space/","text":"DESCRIBE SPACE \u00b6 DESCRIBE SPACE returns the information about the specified graph space. Syntax \u00b6 You can use DESC instead of DESCRIBE for short. DESC[RIBE] SPACE <graph_space_name>; The DESCRIBE SPACE statement is different from the SHOW SPACES statement. For details about SHOW SPACES , see SHOW SPACES . Example \u00b6 nebula> DESCRIBE SPACE basketballplayer; +----+--------------------+------------------+----------------+---------+------------+--------------------+-------------------------------+---------+ | ID | Name | Partition Number | Replica Factor | Charset | Collate | Vid Type | Zones | Comment | +----+--------------------+------------------+----------------+---------+------------+--------------------+-------------------------------+---------+ | 1 | \"basketballplayer\" | 10 | 1 | \"utf8\" | \"utf8_bin\" | \"FIXED_STRING(32)\" | \"default_zone_127.0.0.1_9779\" | | +----+--------------------+------------------+----------------+---------+------------+--------------------+-------------------------------+---------+","title":"DESCRIBE SPACE"},{"location":"3.ngql-guide/9.space-statements/4.describe-space/#describe_space","text":"DESCRIBE SPACE returns the information about the specified graph space.","title":"DESCRIBE SPACE"},{"location":"3.ngql-guide/9.space-statements/4.describe-space/#syntax","text":"You can use DESC instead of DESCRIBE for short. DESC[RIBE] SPACE <graph_space_name>; The DESCRIBE SPACE statement is different from the SHOW SPACES statement. For details about SHOW SPACES , see SHOW SPACES .","title":"Syntax"},{"location":"3.ngql-guide/9.space-statements/4.describe-space/#example","text":"nebula> DESCRIBE SPACE basketballplayer; +----+--------------------+------------------+----------------+---------+------------+--------------------+-------------------------------+---------+ | ID | Name | Partition Number | Replica Factor | Charset | Collate | Vid Type | Zones | Comment | +----+--------------------+------------------+----------------+---------+------------+--------------------+-------------------------------+---------+ | 1 | \"basketballplayer\" | 10 | 1 | \"utf8\" | \"utf8_bin\" | \"FIXED_STRING(32)\" | \"default_zone_127.0.0.1_9779\" | | +----+--------------------+------------------+----------------+---------+------------+--------------------+-------------------------------+---------+","title":"Example"},{"location":"3.ngql-guide/9.space-statements/5.drop-space/","text":"DROP SPACE \u00b6 DROP SPACE deletes the specified graph space and everything in it. Prerequisites \u00b6 Only the God role can use the DROP SPACE statement. For more information, see AUTHENTICATION . Syntax \u00b6 DROP SPACE [IF EXISTS] <graph_space_name>; You can use the IF EXISTS keywords when dropping spaces. These keywords automatically detect if the related graph space exists. If it exists, it will be deleted. Otherwise, no graph space will be deleted. Legacy version compatibility In Nebula Graph versions earlier than 3.1.0, the DROP SPACE statement does not remove all the files and directories from the disk by default. Caution BE CAUTIOUS about running the DROP SPACE statement.","title":"DROP SPACE"},{"location":"3.ngql-guide/9.space-statements/5.drop-space/#drop_space","text":"DROP SPACE deletes the specified graph space and everything in it.","title":"DROP SPACE"},{"location":"3.ngql-guide/9.space-statements/5.drop-space/#prerequisites","text":"Only the God role can use the DROP SPACE statement. For more information, see AUTHENTICATION .","title":"Prerequisites"},{"location":"3.ngql-guide/9.space-statements/5.drop-space/#syntax","text":"DROP SPACE [IF EXISTS] <graph_space_name>; You can use the IF EXISTS keywords when dropping spaces. These keywords automatically detect if the related graph space exists. If it exists, it will be deleted. Otherwise, no graph space will be deleted. Legacy version compatibility In Nebula Graph versions earlier than 3.1.0, the DROP SPACE statement does not remove all the files and directories from the disk by default. Caution BE CAUTIOUS about running the DROP SPACE statement.","title":"Syntax"},{"location":"3.ngql-guide/9.space-statements/6.clear-space/","text":"CLEAR SPACE \u00b6 CLEAR SPACE deletes the vertices and edges in a graph space, but does not delete the graph space itself and the schema information. Permission requirements \u00b6 Only the God role has the permission to run CLEAR SPACE . Caution \u00b6 Once cleared, the data CANNOT be recovered . Use CLEAR SPACE with caution. CLEAR SPACE is not an atomic operation. If an error occurs, re-run CLEAR SPACE to avoid data remaining. The larger the amount of data in the graph space, the longer it takes to clear it. If the execution fails due to client connection timeout, increase the value of the storage_client_timeout_ms parameter in the Graph Service configuration . During the execution of CLEAR SPACE , writing data into the graph space is not automatically prohibited. Such write operations can result in incomplete data clearing, and the residual data can be damaged. Enterpriseonly The Nebula Graph Community Edition does not support blocking data writing while allowing CLEAR SPACE . The Nebula Graph Enterprise Edition supports blocking data writing by setting VARIABLE read_only=true before running CLEAR SPACE . After the data are cleared successfully, run SET VARIABLE read_only=false to allow data writing again. Syntax \u00b6 CLEAR SPACE [IF EXISTS] <space_name>; Parameter/Option Description IF EXISTS Check whether the graph space to be cleared exists. If it exists, continue to clear it. If it does not exist, the execution finishes, and a message indicating that the execution succeeded is displayed. If IF EXISTS is not set and the graph space does not exist, the CLEAR SPACE statement fails to execute, and an error occurs. space_name The name of the space to be cleared. Example: CLEAR SPACE basketballplayer; Data reserved \u00b6 CLEAR SPACE does not delete the following data in a graph space: Tag information. Edge type information. Index information, containing the information of native index and full-text index. The following example shows what CLEAR SPACE deletes and reserves. # Enter the graph space basketballplayer. nebula [(none)]> use basketballplayer; Execution succeeded # List tags and Edge types. nebula[basketballplayer]> SHOW TAGS; +----------+ | Name | +----------+ | \"player\" | | \"team\" | +----------+ Got 2 rows nebula[basketballplayer]> SHOW EDGES; +----------+ | Name | +----------+ | \"follow\" | | \"serve\" | +----------+ Got 2 rows # Submit a job to make statistics of the graph space. nebula[basketballplayer]> SUBMIT JOB STATS; +------------+ | New Job Id | +------------+ | 4 | +------------+ Got 1 rows # Check the statistics. nebula[basketballplayer]> SHOW STATS; +---------+------------+-------+ | Type | Name | Count | +---------+------------+-------+ | \"Tag\" | \"player\" | 51 | | \"Tag\" | \"team\" | 30 | | \"Edge\" | \"follow\" | 81 | | \"Edge\" | \"serve\" | 152 | | \"Space\" | \"vertices\" | 81 | | \"Space\" | \"edges\" | 233 | +---------+------------+-------+ Got 6 rows # List tag indexes. nebula[basketballplayer]> SHOW TAG INDEXES; +------------------+----------+----------+ | Index Name | By Tag | Columns | +------------------+----------+----------+ | \"player_index_0\" | \"player\" | [] | | \"player_index_1\" | \"player\" | [\"name\"] | +------------------+----------+----------+ Got 2 rows # ----------------------- Dividing line for CLEAR SPACE ----------------------- # Run CLEAR SPACE to clear the graph space basketballplayer. nebula[basketballplayer]> CLEAR SPACE basketballplayer; Execution succeeded # Update the statistics. nebula[basketballplayer]> SUBMIT JOB STATS; +------------+ | New Job Id | +------------+ | 5 | +------------+ Got 1 rows # Check the statistics. The tags and edge types still exist, but all the vertices and edges are gone. nebula[basketballplayer]> SHOW STATS; +---------+------------+-------+ | Type | Name | Count | +---------+------------+-------+ | \"Tag\" | \"player\" | 0 | | \"Tag\" | \"team\" | 0 | | \"Edge\" | \"follow\" | 0 | | \"Edge\" | \"serve\" | 0 | | \"Space\" | \"vertices\" | 0 | | \"Space\" | \"edges\" | 0 | +---------+------------+-------+ Got 6 rows # Try to list the tag indexes. They still exist. nebula[basketballplayer]> SHOW TAG INDEXES; +------------------+----------+----------+ | Index Name | By Tag | Columns | +------------------+----------+----------+ | \"player_index_0\" | \"player\" | [] | | \"player_index_1\" | \"player\" | [\"name\"] | +------------------+----------+----------+ Got 2 rows (time spent 523/978 us)","title":"CLEAR SPACE"},{"location":"3.ngql-guide/9.space-statements/6.clear-space/#clear_space","text":"CLEAR SPACE deletes the vertices and edges in a graph space, but does not delete the graph space itself and the schema information.","title":"CLEAR SPACE"},{"location":"3.ngql-guide/9.space-statements/6.clear-space/#permission_requirements","text":"Only the God role has the permission to run CLEAR SPACE .","title":"Permission requirements"},{"location":"3.ngql-guide/9.space-statements/6.clear-space/#caution","text":"Once cleared, the data CANNOT be recovered . Use CLEAR SPACE with caution. CLEAR SPACE is not an atomic operation. If an error occurs, re-run CLEAR SPACE to avoid data remaining. The larger the amount of data in the graph space, the longer it takes to clear it. If the execution fails due to client connection timeout, increase the value of the storage_client_timeout_ms parameter in the Graph Service configuration . During the execution of CLEAR SPACE , writing data into the graph space is not automatically prohibited. Such write operations can result in incomplete data clearing, and the residual data can be damaged. Enterpriseonly The Nebula Graph Community Edition does not support blocking data writing while allowing CLEAR SPACE . The Nebula Graph Enterprise Edition supports blocking data writing by setting VARIABLE read_only=true before running CLEAR SPACE . After the data are cleared successfully, run SET VARIABLE read_only=false to allow data writing again.","title":"Caution"},{"location":"3.ngql-guide/9.space-statements/6.clear-space/#syntax","text":"CLEAR SPACE [IF EXISTS] <space_name>; Parameter/Option Description IF EXISTS Check whether the graph space to be cleared exists. If it exists, continue to clear it. If it does not exist, the execution finishes, and a message indicating that the execution succeeded is displayed. If IF EXISTS is not set and the graph space does not exist, the CLEAR SPACE statement fails to execute, and an error occurs. space_name The name of the space to be cleared. Example: CLEAR SPACE basketballplayer;","title":"Syntax"},{"location":"3.ngql-guide/9.space-statements/6.clear-space/#data_reserved","text":"CLEAR SPACE does not delete the following data in a graph space: Tag information. Edge type information. Index information, containing the information of native index and full-text index. The following example shows what CLEAR SPACE deletes and reserves. # Enter the graph space basketballplayer. nebula [(none)]> use basketballplayer; Execution succeeded # List tags and Edge types. nebula[basketballplayer]> SHOW TAGS; +----------+ | Name | +----------+ | \"player\" | | \"team\" | +----------+ Got 2 rows nebula[basketballplayer]> SHOW EDGES; +----------+ | Name | +----------+ | \"follow\" | | \"serve\" | +----------+ Got 2 rows # Submit a job to make statistics of the graph space. nebula[basketballplayer]> SUBMIT JOB STATS; +------------+ | New Job Id | +------------+ | 4 | +------------+ Got 1 rows # Check the statistics. nebula[basketballplayer]> SHOW STATS; +---------+------------+-------+ | Type | Name | Count | +---------+------------+-------+ | \"Tag\" | \"player\" | 51 | | \"Tag\" | \"team\" | 30 | | \"Edge\" | \"follow\" | 81 | | \"Edge\" | \"serve\" | 152 | | \"Space\" | \"vertices\" | 81 | | \"Space\" | \"edges\" | 233 | +---------+------------+-------+ Got 6 rows # List tag indexes. nebula[basketballplayer]> SHOW TAG INDEXES; +------------------+----------+----------+ | Index Name | By Tag | Columns | +------------------+----------+----------+ | \"player_index_0\" | \"player\" | [] | | \"player_index_1\" | \"player\" | [\"name\"] | +------------------+----------+----------+ Got 2 rows # ----------------------- Dividing line for CLEAR SPACE ----------------------- # Run CLEAR SPACE to clear the graph space basketballplayer. nebula[basketballplayer]> CLEAR SPACE basketballplayer; Execution succeeded # Update the statistics. nebula[basketballplayer]> SUBMIT JOB STATS; +------------+ | New Job Id | +------------+ | 5 | +------------+ Got 1 rows # Check the statistics. The tags and edge types still exist, but all the vertices and edges are gone. nebula[basketballplayer]> SHOW STATS; +---------+------------+-------+ | Type | Name | Count | +---------+------------+-------+ | \"Tag\" | \"player\" | 0 | | \"Tag\" | \"team\" | 0 | | \"Edge\" | \"follow\" | 0 | | \"Edge\" | \"serve\" | 0 | | \"Space\" | \"vertices\" | 0 | | \"Space\" | \"edges\" | 0 | +---------+------------+-------+ Got 6 rows # Try to list the tag indexes. They still exist. nebula[basketballplayer]> SHOW TAG INDEXES; +------------------+----------+----------+ | Index Name | By Tag | Columns | +------------------+----------+----------+ | \"player_index_0\" | \"player\" | [] | | \"player_index_1\" | \"player\" | [\"name\"] | +------------------+----------+----------+ Got 2 rows (time spent 523/978 us)","title":"Data reserved"},{"location":"4.deployment-and-installation/1.resource-preparations/","text":"Prepare resources for compiling, installing, and running Nebula Graph \u00b6 This topic describes the requirements and suggestions for compiling and installing Nebula Graph, as well as how to estimate the resource you need to reserve for running a Nebula Graph cluster. Requirements for compiling the Nebula Graph source code \u00b6 Hardware requirements for compiling Nebula Graph \u00b6 Item Requirement CPU architecture x86_64 Memory 4 GB Disk 10 GB, SSD Supported operating systems for compiling Nebula Graph \u00b6 For now, we can only compile Nebula Graph in the Linux system. We recommend that you use any Linux system with kernel version 4.15 or above. Note To install Nebula Graph on Linux systems with kernel version lower than required, use RPM/DEB packages or TAR files . Software requirements for compiling Nebula Graph \u00b6 You must have the correct version of the software listed below to compile Nebula Graph. If they are not as required or you are not sure, follow the steps in Prepare software for compiling Nebula Graph to get them ready. Software Version Note glibc 2.17 or above You can run ldd --version to check the glibc version. make Any stable version - m4 Any stable version - git Any stable version - wget Any stable version - unzip Any stable version - xz Any stable version - readline-devel Any stable version - ncurses-devel Any stable version - zlib-devel Any stable version - g++ 8.5.0 or above You can run gcc -v to check the gcc version. cmake 3.14.0 or above You can run cmake --version to check the cmake version. curl Any stable version - redhat-lsb-core Any stable version - libstdc++-static Any stable version Only needed in CentOS 8+, RedHat 8+, and Fedora systems. libasan Any stable version Only needed in CentOS 8+, RedHat 8+, and Fedora systems. bzip2 Any stable version - Other third-party software will be automatically downloaded and installed to the build directory at the configure (cmake) stage. Prepare software for compiling Nebula Graph \u00b6 If part of the dependencies are missing or the versions does not meet the requirements, manually install them with the following steps. You can skip unnecessary dependencies or steps according to your needs. Install dependencies. For CentOS, RedHat, and Fedora users, run the following commands. $ yum update $ yum install -y make \\ m4 \\ git \\ wget \\ unzip \\ xz \\ readline-devel \\ ncurses-devel \\ zlib-devel \\ gcc \\ gcc-c++ \\ cmake \\ curl \\ redhat-lsb-core \\ bzip2 // For CentOS 8 +, RedHat 8 +, and Fedora, install libstdc++-static and libasan as well $ yum install -y libstdc++-static libasan For Debian and Ubuntu users, run the following commands. $ apt-get update $ apt-get install -y make \\ m4 \\ git \\ wget \\ unzip \\ xz-utils \\ curl \\ lsb-core \\ build-essential \\ libreadline-dev \\ ncurses-dev \\ cmake \\ gettext Check if the GCC and cmake on your host are in the right version. See Software requirements for compiling Nebula Graph for the required versions. $ g++ --version $ cmake --version If your GCC and CMake are in the right versions, then you are all set and you can ignore the subsequent steps. If they are not, select and perform the needed steps as follows. If the CMake version is incorrect, visit the CMake official website to install the required version. If the G++ version is incorrect, visit the G++ official website or follow the instructions below to to install the required version. For CentOS users, run: yum install centos-release-scl yum install devtoolset-11 scl enable devtoolset-11 'bash' For Ubuntu users, run: add-apt-repository ppa:ubuntu-toolchain-r/test apt install gcc-11 g++-11 Requirements and suggestions for installing Nebula Graph in test environments \u00b6 Hardware requirements for test environments \u00b6 Item Requirement CPU architecture x86_64 Number of CPU core 4 Memory 8 GB Disk 100 GB, SSD Supported operating systems for test environments \u00b6 For now, we can only install Nebula Graph in the Linux system. To install Nebula Graph in a test environment, we recommend that you use any Linux system with kernel version 3.9 or above. Suggested service architecture for test environments \u00b6 Process Suggested number metad (the metadata service process) 1 storaged (the storage service process) 1 or more graphd (the query engine service process) 1 or more For example, for a single-machine test environment, you can deploy 1 metad, 1 storaged, and 1 graphd processes in the machine. For a more common test environment, such as a cluster of 3 machines (named as A, B, and C), you can deploy Nebula Graph as follows: Machine name Number of metad Number of storaged Number of graphd A 1 1 1 B None 1 1 C None 1 1 Requirements and suggestions for installing Nebula Graph in production environments \u00b6 Hardware requirements for production environments \u00b6 Item Requirement CPU architecture x86_64 Number of CPU core 48 Memory 96 GB Disk 2 * 900 GB, NVMe SSD Supported operating systems for production environments \u00b6 For now, we can only install Nebula Graph in the Linux system. To install Nebula Graph in a production environment, we recommend that you use any Linux system with kernel version 3.9 or above. Users can adjust some of the kernel parameters to better accommodate the need for running Nebula Graph. For more information, see kernel configuration . Suggested service architecture for production environments \u00b6 Danger DO NOT deploy a cluster across IDCs. Process Suggested number metad (the metadata service process) 3 storaged (the storage service process) 3 or more graphd (the query engine service process) 3 or more Each metad process automatically creates and maintains a replica of the metadata. Usually, you need to deploy three metad processes and only three. The number of storaged processes does not affect the number of graph space replicas. Users can deploy multiple processes on a single machine. For example, on a cluster of 5 machines (named as A, B, C, D, and E), you can deploy Nebula Graph as follows: Machine name Number of metad Number of storaged Number of graphd A 1 1 1 B 1 1 1 C 1 1 1 D None 1 1 E None 1 1 Capacity requirements for running a Nebula Graph cluster \u00b6 Users can estimate the memory, disk space, and partition number needed for a Nebula Graph cluster of 3 replicas as follows. Resource Unit How to estimate Description Disk space for a cluster Bytes the_sum_of_edge_number_and_vertex_number * average_bytes_of_properties * 6 * 120% - Memory for a cluster Bytes [ the_sum_of_edge_number_and_vertex_number * 16 + the_number_of_RocksDB_instances * ( write_buffer_size * max_write_buffer_number + rocksdb_block_cache )] * 120% write_buffer_size and max_write_buffer_number are RocksDB parameters. For more information, see MemTable . For details about rocksdb_block_cache , see Memory usage in RocksDB . Number of partitions for a graph space - the_number_of_disks_in_the_cluster * disk_partition_num_multiplier disk_partition_num_multiplier is an integer between 2 and 20 (both including). Its value depends on the disk performance. Use 20 for SSD and 2 for HDD. Question 1: Why do we multiply the disk space and memory by 120%? Answer: The extra 20% is for buffer. Question 2: How to get the number of RocksDB instances? Answer: Each directory in the --data_path item in the etc/nebula-storaged.conf file corresponds to a RocksDB instance. Count the number of directories to get the RocksDB instance number. Note Users can decrease the memory size occupied by the bloom filter by adding --enable_partitioned_index_filter=true in etc/nebula-storaged.conf . But it may decrease the read performance in some random-seek cases. FAQ \u00b6 About storage devices \u00b6 Nebula Graph is designed and implemented for NVMe SSD. All default parameters are optimized for the SSD devices and require extremely high IOPS and low latency. Due to the poor IOPS capability and long random seek latency, HDD is not recommended. Users may encounter many problems when using HDD. Do not use remote storage devices, such as NAS or SAN. Do not connect an external virtual hard disk based on HDFS or Ceph. Do not use RAID. Use local SSD devices. About CPU architecture \u00b6 Enterpriseonly Nebula Graph 3.1.0 Enterprise Edition can be run or compiled directly on ARM architectures (including Apple Mac M1 or Huawei Kunpeng). Contact inquiry@vesoft.com for business supports. Note Starting with 3.0.2, you can run containerized Nebula Graph databases on Docker Desktop for ARM macOS or on ARM Linux servers.","title":"Resource preparations"},{"location":"4.deployment-and-installation/1.resource-preparations/#prepare_resources_for_compiling_installing_and_running_nebula_graph","text":"This topic describes the requirements and suggestions for compiling and installing Nebula Graph, as well as how to estimate the resource you need to reserve for running a Nebula Graph cluster.","title":"Prepare resources for compiling, installing, and running Nebula Graph"},{"location":"4.deployment-and-installation/1.resource-preparations/#requirements_for_compiling_the_nebula_graph_source_code","text":"","title":"Requirements for compiling the Nebula Graph source code"},{"location":"4.deployment-and-installation/1.resource-preparations/#hardware_requirements_for_compiling_nebula_graph","text":"Item Requirement CPU architecture x86_64 Memory 4 GB Disk 10 GB, SSD","title":"Hardware requirements for compiling Nebula Graph"},{"location":"4.deployment-and-installation/1.resource-preparations/#supported_operating_systems_for_compiling_nebula_graph","text":"For now, we can only compile Nebula Graph in the Linux system. We recommend that you use any Linux system with kernel version 4.15 or above. Note To install Nebula Graph on Linux systems with kernel version lower than required, use RPM/DEB packages or TAR files .","title":"Supported operating systems for compiling Nebula Graph"},{"location":"4.deployment-and-installation/1.resource-preparations/#software_requirements_for_compiling_nebula_graph","text":"You must have the correct version of the software listed below to compile Nebula Graph. If they are not as required or you are not sure, follow the steps in Prepare software for compiling Nebula Graph to get them ready. Software Version Note glibc 2.17 or above You can run ldd --version to check the glibc version. make Any stable version - m4 Any stable version - git Any stable version - wget Any stable version - unzip Any stable version - xz Any stable version - readline-devel Any stable version - ncurses-devel Any stable version - zlib-devel Any stable version - g++ 8.5.0 or above You can run gcc -v to check the gcc version. cmake 3.14.0 or above You can run cmake --version to check the cmake version. curl Any stable version - redhat-lsb-core Any stable version - libstdc++-static Any stable version Only needed in CentOS 8+, RedHat 8+, and Fedora systems. libasan Any stable version Only needed in CentOS 8+, RedHat 8+, and Fedora systems. bzip2 Any stable version - Other third-party software will be automatically downloaded and installed to the build directory at the configure (cmake) stage.","title":"Software requirements for compiling Nebula Graph"},{"location":"4.deployment-and-installation/1.resource-preparations/#prepare_software_for_compiling_nebula_graph","text":"If part of the dependencies are missing or the versions does not meet the requirements, manually install them with the following steps. You can skip unnecessary dependencies or steps according to your needs. Install dependencies. For CentOS, RedHat, and Fedora users, run the following commands. $ yum update $ yum install -y make \\ m4 \\ git \\ wget \\ unzip \\ xz \\ readline-devel \\ ncurses-devel \\ zlib-devel \\ gcc \\ gcc-c++ \\ cmake \\ curl \\ redhat-lsb-core \\ bzip2 // For CentOS 8 +, RedHat 8 +, and Fedora, install libstdc++-static and libasan as well $ yum install -y libstdc++-static libasan For Debian and Ubuntu users, run the following commands. $ apt-get update $ apt-get install -y make \\ m4 \\ git \\ wget \\ unzip \\ xz-utils \\ curl \\ lsb-core \\ build-essential \\ libreadline-dev \\ ncurses-dev \\ cmake \\ gettext Check if the GCC and cmake on your host are in the right version. See Software requirements for compiling Nebula Graph for the required versions. $ g++ --version $ cmake --version If your GCC and CMake are in the right versions, then you are all set and you can ignore the subsequent steps. If they are not, select and perform the needed steps as follows. If the CMake version is incorrect, visit the CMake official website to install the required version. If the G++ version is incorrect, visit the G++ official website or follow the instructions below to to install the required version. For CentOS users, run: yum install centos-release-scl yum install devtoolset-11 scl enable devtoolset-11 'bash' For Ubuntu users, run: add-apt-repository ppa:ubuntu-toolchain-r/test apt install gcc-11 g++-11","title":"Prepare software for compiling Nebula Graph"},{"location":"4.deployment-and-installation/1.resource-preparations/#requirements_and_suggestions_for_installing_nebula_graph_in_test_environments","text":"","title":"Requirements and suggestions for installing Nebula Graph in test environments"},{"location":"4.deployment-and-installation/1.resource-preparations/#hardware_requirements_for_test_environments","text":"Item Requirement CPU architecture x86_64 Number of CPU core 4 Memory 8 GB Disk 100 GB, SSD","title":"Hardware requirements for test environments"},{"location":"4.deployment-and-installation/1.resource-preparations/#supported_operating_systems_for_test_environments","text":"For now, we can only install Nebula Graph in the Linux system. To install Nebula Graph in a test environment, we recommend that you use any Linux system with kernel version 3.9 or above.","title":"Supported operating systems for test environments"},{"location":"4.deployment-and-installation/1.resource-preparations/#suggested_service_architecture_for_test_environments","text":"Process Suggested number metad (the metadata service process) 1 storaged (the storage service process) 1 or more graphd (the query engine service process) 1 or more For example, for a single-machine test environment, you can deploy 1 metad, 1 storaged, and 1 graphd processes in the machine. For a more common test environment, such as a cluster of 3 machines (named as A, B, and C), you can deploy Nebula Graph as follows: Machine name Number of metad Number of storaged Number of graphd A 1 1 1 B None 1 1 C None 1 1","title":"Suggested service architecture for test environments"},{"location":"4.deployment-and-installation/1.resource-preparations/#requirements_and_suggestions_for_installing_nebula_graph_in_production_environments","text":"","title":"Requirements and suggestions for installing Nebula Graph in production environments"},{"location":"4.deployment-and-installation/1.resource-preparations/#hardware_requirements_for_production_environments","text":"Item Requirement CPU architecture x86_64 Number of CPU core 48 Memory 96 GB Disk 2 * 900 GB, NVMe SSD","title":"Hardware requirements for production environments"},{"location":"4.deployment-and-installation/1.resource-preparations/#supported_operating_systems_for_production_environments","text":"For now, we can only install Nebula Graph in the Linux system. To install Nebula Graph in a production environment, we recommend that you use any Linux system with kernel version 3.9 or above. Users can adjust some of the kernel parameters to better accommodate the need for running Nebula Graph. For more information, see kernel configuration .","title":"Supported operating systems for production environments"},{"location":"4.deployment-and-installation/1.resource-preparations/#suggested_service_architecture_for_production_environments","text":"Danger DO NOT deploy a cluster across IDCs. Process Suggested number metad (the metadata service process) 3 storaged (the storage service process) 3 or more graphd (the query engine service process) 3 or more Each metad process automatically creates and maintains a replica of the metadata. Usually, you need to deploy three metad processes and only three. The number of storaged processes does not affect the number of graph space replicas. Users can deploy multiple processes on a single machine. For example, on a cluster of 5 machines (named as A, B, C, D, and E), you can deploy Nebula Graph as follows: Machine name Number of metad Number of storaged Number of graphd A 1 1 1 B 1 1 1 C 1 1 1 D None 1 1 E None 1 1","title":"Suggested service architecture for production environments"},{"location":"4.deployment-and-installation/1.resource-preparations/#capacity_requirements_for_running_a_nebula_graph_cluster","text":"Users can estimate the memory, disk space, and partition number needed for a Nebula Graph cluster of 3 replicas as follows. Resource Unit How to estimate Description Disk space for a cluster Bytes the_sum_of_edge_number_and_vertex_number * average_bytes_of_properties * 6 * 120% - Memory for a cluster Bytes [ the_sum_of_edge_number_and_vertex_number * 16 + the_number_of_RocksDB_instances * ( write_buffer_size * max_write_buffer_number + rocksdb_block_cache )] * 120% write_buffer_size and max_write_buffer_number are RocksDB parameters. For more information, see MemTable . For details about rocksdb_block_cache , see Memory usage in RocksDB . Number of partitions for a graph space - the_number_of_disks_in_the_cluster * disk_partition_num_multiplier disk_partition_num_multiplier is an integer between 2 and 20 (both including). Its value depends on the disk performance. Use 20 for SSD and 2 for HDD. Question 1: Why do we multiply the disk space and memory by 120%? Answer: The extra 20% is for buffer. Question 2: How to get the number of RocksDB instances? Answer: Each directory in the --data_path item in the etc/nebula-storaged.conf file corresponds to a RocksDB instance. Count the number of directories to get the RocksDB instance number. Note Users can decrease the memory size occupied by the bloom filter by adding --enable_partitioned_index_filter=true in etc/nebula-storaged.conf . But it may decrease the read performance in some random-seek cases.","title":"Capacity requirements for running a Nebula Graph cluster"},{"location":"4.deployment-and-installation/1.resource-preparations/#faq","text":"","title":"FAQ"},{"location":"4.deployment-and-installation/1.resource-preparations/#about_storage_devices","text":"Nebula Graph is designed and implemented for NVMe SSD. All default parameters are optimized for the SSD devices and require extremely high IOPS and low latency. Due to the poor IOPS capability and long random seek latency, HDD is not recommended. Users may encounter many problems when using HDD. Do not use remote storage devices, such as NAS or SAN. Do not connect an external virtual hard disk based on HDFS or Ceph. Do not use RAID. Use local SSD devices.","title":"About storage devices"},{"location":"4.deployment-and-installation/1.resource-preparations/#about_cpu_architecture","text":"Enterpriseonly Nebula Graph 3.1.0 Enterprise Edition can be run or compiled directly on ARM architectures (including Apple Mac M1 or Huawei Kunpeng). Contact inquiry@vesoft.com for business supports. Note Starting with 3.0.2, you can run containerized Nebula Graph databases on Docker Desktop for ARM macOS or on ARM Linux servers.","title":"About CPU architecture"},{"location":"4.deployment-and-installation/4.uninstall-nebula-graph/","text":"Uninstall Nebula Graph \u00b6 This topic describes how to uninstall Nebula Graph. Caution Before re-installing Nebula Graph on a machine, follow this topic to completely uninstall the old Nebula Graph, in case the remaining data interferes with the new services, including inconsistencies between Meta services. Prerequisite \u00b6 The Nebula Graph services should be stopped before the uninstallation. For more information, see Manage Nebula Graph services . Step 1: Delete data files of the Storage and Meta Services \u00b6 If you have modified the data_path in the configuration files for the Meta Service and Storage Service, the directories where Nebula Graph stores data may not be in the installation path of Nebula Graph. Check the configuration files to confirm the data paths, and then manually delete the directories to clear all data. Note For a Nebula Graph cluster, delete the data files of all Storage and Meta servers. Check the Storage Service disk settings . For example: ########## Disk ########## # Root data path. Split by comma. e.g. --data_path=/disk1/path1/,/disk2/path2/ # One path per Rocksdb instance. --data_path = /nebula/data/storage Check the Metad Service configurations and find the corresponding metadata directories. Delete the data and the directories found in step 2. Step 2: Delete the installation directories \u00b6 Note Delete all installation directories, including the cluster.id file in them. The default installation path is /usr/local/nebula , which is specified by --prefix while installing Nebula Graph. Uninstall Nebula Graph deployed with source code \u00b6 Find the installation directories of Nebula Graph, and delete them all. Uninstall Nebula Graph deployed with RPM packages \u00b6 Run the following command to get the Nebula Graph version. $ rpm -qa | grep \"nebula\" The return message is as follows. nebula-graph-3.1.0-1.x86_64 Run the following command to uninstall Nebula Graph. sudo rpm -e <nebula_version> For example: sudo rpm -e nebula-graph-3.1.0-1.x86_64 Delete the installation directories. Uninstall Nebula Graph deployed with DEB packages \u00b6 Run the following command to get the Nebula Graph version. $ dpkg -l | grep \"nebula\" The return message is as follows. ii nebula-graph 3 .1.0 amd64 Nebula Package built using CMake Run the following command to uninstall Nebula Graph. sudo dpkg -r <nebula_version> For example: sudo dpkg -r nebula-graph Delete the installation directories. Uninstall Nebula Graph deployed with Docker Compose \u00b6 In the nebula-docker-compose directory, run the following command to stop the Nebula Graph services. docker-compose down -v Delete the nebula-docker-compose directory.","title":"Uninstall Nebula Graph"},{"location":"4.deployment-and-installation/4.uninstall-nebula-graph/#uninstall_nebula_graph","text":"This topic describes how to uninstall Nebula Graph. Caution Before re-installing Nebula Graph on a machine, follow this topic to completely uninstall the old Nebula Graph, in case the remaining data interferes with the new services, including inconsistencies between Meta services.","title":"Uninstall Nebula Graph"},{"location":"4.deployment-and-installation/4.uninstall-nebula-graph/#prerequisite","text":"The Nebula Graph services should be stopped before the uninstallation. For more information, see Manage Nebula Graph services .","title":"Prerequisite"},{"location":"4.deployment-and-installation/4.uninstall-nebula-graph/#step_1_delete_data_files_of_the_storage_and_meta_services","text":"If you have modified the data_path in the configuration files for the Meta Service and Storage Service, the directories where Nebula Graph stores data may not be in the installation path of Nebula Graph. Check the configuration files to confirm the data paths, and then manually delete the directories to clear all data. Note For a Nebula Graph cluster, delete the data files of all Storage and Meta servers. Check the Storage Service disk settings . For example: ########## Disk ########## # Root data path. Split by comma. e.g. --data_path=/disk1/path1/,/disk2/path2/ # One path per Rocksdb instance. --data_path = /nebula/data/storage Check the Metad Service configurations and find the corresponding metadata directories. Delete the data and the directories found in step 2.","title":"Step 1: Delete data files of the Storage and Meta Services"},{"location":"4.deployment-and-installation/4.uninstall-nebula-graph/#step_2_delete_the_installation_directories","text":"Note Delete all installation directories, including the cluster.id file in them. The default installation path is /usr/local/nebula , which is specified by --prefix while installing Nebula Graph.","title":"Step 2: Delete the installation directories"},{"location":"4.deployment-and-installation/4.uninstall-nebula-graph/#uninstall_nebula_graph_deployed_with_source_code","text":"Find the installation directories of Nebula Graph, and delete them all.","title":"Uninstall Nebula Graph deployed with source code"},{"location":"4.deployment-and-installation/4.uninstall-nebula-graph/#uninstall_nebula_graph_deployed_with_rpm_packages","text":"Run the following command to get the Nebula Graph version. $ rpm -qa | grep \"nebula\" The return message is as follows. nebula-graph-3.1.0-1.x86_64 Run the following command to uninstall Nebula Graph. sudo rpm -e <nebula_version> For example: sudo rpm -e nebula-graph-3.1.0-1.x86_64 Delete the installation directories.","title":"Uninstall Nebula Graph deployed with RPM packages"},{"location":"4.deployment-and-installation/4.uninstall-nebula-graph/#uninstall_nebula_graph_deployed_with_deb_packages","text":"Run the following command to get the Nebula Graph version. $ dpkg -l | grep \"nebula\" The return message is as follows. ii nebula-graph 3 .1.0 amd64 Nebula Package built using CMake Run the following command to uninstall Nebula Graph. sudo dpkg -r <nebula_version> For example: sudo dpkg -r nebula-graph Delete the installation directories.","title":"Uninstall Nebula Graph deployed with DEB packages"},{"location":"4.deployment-and-installation/4.uninstall-nebula-graph/#uninstall_nebula_graph_deployed_with_docker_compose","text":"In the nebula-docker-compose directory, run the following command to stop the Nebula Graph services. docker-compose down -v Delete the nebula-docker-compose directory.","title":"Uninstall Nebula Graph deployed with Docker Compose"},{"location":"4.deployment-and-installation/connect-to-nebula-graph/","text":"Connect to Nebula Graph \u00b6 This topic provides basic instruction on how to use the native CLI client Nebula Console to connect to Nebula Graph. Caution When connecting to Nebula Graph for the first time, you must register the Storage Service before querying data. Nebula Graph supports multiple types of clients, including a CLI client, a GUI client, and clients developed in popular programming languages. For more information, see the client list . Prerequisites \u00b6 You have started Nebula Graph services . The machine on which you plan to run Nebula Console has network access to the Graph Service of Nebula Graph. The Nebula Console version is compatible with the Nebula Graph version. Note Nebula Console and Nebula Graph of the same version number are the most compatible. There may be compatibility issues when connecting to Nebula Graph with a different version of Nebula Console. The error message incompatible version between client and server is displayed when there is such an issue. Steps \u00b6 On the Nebula Console releases page , select a Nebula Console version and click Assets . In the Assets area, find the correct binary file for the machine where you want to run Nebula Console and download the file to the machine. (Optional) Rename the binary file to nebula-console for convenience. Note For Windows, rename the file to nebula-console.exe . On the machine to run Nebula Console, grant the execute permission of the nebula-console binary file to the user. Note For Windows, skip this step. $ chmod 111 nebula-console In the command line interface, change the working directory to the one where the nebula-console binary file is stored. Run the following command to connect to Nebula Graph. For Linux or macOS: $ ./nebula-console -addr <ip> -port <port> -u <username> -p <password> [ -t 120 ] [ -e \"nGQL_statement\" | -f filename.nGQL ] For Windows: > nebula-console . exe -addr < ip > -port < port > -u < username > -p < password > [ -t 120 ] [ -e \"nGQL_statement\" | -f filename . nGQL ] Parameter descriptions are as follows: Parameter Description -h/-help Shows the help menu. -addr/-address Sets the IP address of the Graph service. The default address is 127.0.0.1. If Nebula Graph is deployed on Nebula Cloud , you need to create a Private Link and set the IP address of the Private Endpoint as the parameter value. -P/-port Sets the port number of the graphd service. The default port number is 9669. -u/-user Sets the username of your Nebula Graph account. Before enabling authentication, you can use any existing username. The default username is root . -p/-password Sets the password of your Nebula Graph account. Before enabling authentication, you can use any characters as the password. -t/-timeout Sets an integer-type timeout threshold of the connection. The unit is second. The default value is 120. -e/-eval Sets a string-type nGQL statement. The nGQL statement is executed once the connection succeeds. The connection stops after the result is returned. -f/-file Sets the path of an nGQL file. The nGQL statements in the file are executed once the connection succeeds. The result will be returned and the connection stops then. -enable_ssl Enables SSL encryption when connecting to Nebula Graph. -ssl_root_ca_path Sets the storage path of the certification authority file. -ssl_cert_path Sets the storage path of the certificate file. -ssl_private_key_path Sets the storage path of the private key file. For information on more parameters, see the project repository .","title":"Connect to Service"},{"location":"4.deployment-and-installation/connect-to-nebula-graph/#connect_to_nebula_graph","text":"This topic provides basic instruction on how to use the native CLI client Nebula Console to connect to Nebula Graph. Caution When connecting to Nebula Graph for the first time, you must register the Storage Service before querying data. Nebula Graph supports multiple types of clients, including a CLI client, a GUI client, and clients developed in popular programming languages. For more information, see the client list .","title":"Connect to Nebula Graph"},{"location":"4.deployment-and-installation/connect-to-nebula-graph/#prerequisites","text":"You have started Nebula Graph services . The machine on which you plan to run Nebula Console has network access to the Graph Service of Nebula Graph. The Nebula Console version is compatible with the Nebula Graph version. Note Nebula Console and Nebula Graph of the same version number are the most compatible. There may be compatibility issues when connecting to Nebula Graph with a different version of Nebula Console. The error message incompatible version between client and server is displayed when there is such an issue.","title":"Prerequisites"},{"location":"4.deployment-and-installation/connect-to-nebula-graph/#steps","text":"On the Nebula Console releases page , select a Nebula Console version and click Assets . In the Assets area, find the correct binary file for the machine where you want to run Nebula Console and download the file to the machine. (Optional) Rename the binary file to nebula-console for convenience. Note For Windows, rename the file to nebula-console.exe . On the machine to run Nebula Console, grant the execute permission of the nebula-console binary file to the user. Note For Windows, skip this step. $ chmod 111 nebula-console In the command line interface, change the working directory to the one where the nebula-console binary file is stored. Run the following command to connect to Nebula Graph. For Linux or macOS: $ ./nebula-console -addr <ip> -port <port> -u <username> -p <password> [ -t 120 ] [ -e \"nGQL_statement\" | -f filename.nGQL ] For Windows: > nebula-console . exe -addr < ip > -port < port > -u < username > -p < password > [ -t 120 ] [ -e \"nGQL_statement\" | -f filename . nGQL ] Parameter descriptions are as follows: Parameter Description -h/-help Shows the help menu. -addr/-address Sets the IP address of the Graph service. The default address is 127.0.0.1. If Nebula Graph is deployed on Nebula Cloud , you need to create a Private Link and set the IP address of the Private Endpoint as the parameter value. -P/-port Sets the port number of the graphd service. The default port number is 9669. -u/-user Sets the username of your Nebula Graph account. Before enabling authentication, you can use any existing username. The default username is root . -p/-password Sets the password of your Nebula Graph account. Before enabling authentication, you can use any characters as the password. -t/-timeout Sets an integer-type timeout threshold of the connection. The unit is second. The default value is 120. -e/-eval Sets a string-type nGQL statement. The nGQL statement is executed once the connection succeeds. The connection stops after the result is returned. -f/-file Sets the path of an nGQL file. The nGQL statements in the file are executed once the connection succeeds. The result will be returned and the connection stops then. -enable_ssl Enables SSL encryption when connecting to Nebula Graph. -ssl_root_ca_path Sets the storage path of the certification authority file. -ssl_cert_path Sets the storage path of the certificate file. -ssl_private_key_path Sets the storage path of the private key file. For information on more parameters, see the project repository .","title":"Steps"},{"location":"4.deployment-and-installation/deploy-license/","text":"Deploy a license for Nebula Graph Enterprise Edition \u00b6 Nebula Graph Enterprise Edition requires the user to deploy a license file before starting the Enterprise Edition. This topic describes how to deploy a license file for the Enterprise Edition. Enterpriseonly License is a software authorization certificate provided for users of the Enterprise Edition. Users of the Enterprise Edition can send email to inquiry@vesoft.com to apply for a license file. Precautions \u00b6 If the license file is not deployed, Nebula Graph Enterprise Edition cannot be started. Do not modify the license file, otherwise the license will become invalid. If the license is about to expire, send email to inquiry@vesoft.com to apply for renewal. The transition period after the license expires is 14 days: If you start the Enterprise Edition within 30 days before the license expires or on the day the license expires, a log will be printed as a reminder. The license can still be used for 14 days after it expires. If the license has expired for 14 days, you will not be able to start the Enterprise Edition, and a log will be printed as a reminder. License description \u00b6 The example of the content of the license file ( nebula.license ) is as follows: ----------License Content Start---------- { \"vendor\" : \"vesoft\" , \"organization\" : \"doc\" , \"issuedDate\" : \"2022-04-06T16:00:00.000Z\" , \"expirationDate\" : \"2022-05-31T15:59:59.000Z\" , \"product\" : \"nebula_graph\" , \"version\" : \">3.0.0\" , \"licenseType\" : \"enterprise\" , \"gracePeriod\" : 14 , \"graphdSpec\" : { \"nodes\" : 3 } , \"storagedSpec\" : { \"nodes\" : 3 } , \"clusterCode\" : \"BAIAEAiAQAAG\" } ----------License Content End---------- ----------License Key Start---------- cofFcOxxxxxxxxxxxxxhnZgaxrQ == ----------License Key End---------- The license file contains information such as issuedDate and expirationDate . The description is as follows. Parameter Description vendor The supplier. organization The username. issuedDate The date that the license is issued. expirationDate The date that the license expires. product The product type. The product type of Nebula Graph is nebula_graph . version The version information. licenseType The license type, including enterprise , samll_bussiness , pro , and individual . gracePeriod The buffer time (in days) for the service to continue to be used after the license expires, and the service will be stopped after the buffer period. The trial version of license has no buffer period after expiration and the default value of this parameter is 0. graphdSpec The max number of graph services in a cluster. Nebula Graph detects the number of active graph services in real-time. You are unable to connect to the cluster once the max number is reached. storagedSpec The max number of storage services in a cluster. Nebula Graph detects the number of active storage services in real-time. You are unable to connect to the cluster once the max number is reached. clusterCode The user's hardware information, which is also the unique identifier of the cluster. This parameter is not available in the trial version of the license. Deploy the license \u00b6 Send email to inquiry@vesoft.com to apply for the Nebula Graph Enterprise Edition package. Install Nebula Graph Enterprise Edition. The installation method is the same as the Community Edition. See Install Nebula Graph with RPM or DEB package . Send email to inquiry@vesoft.com to apply for the license file nebula.license . Upload the license file to all hosts that contain Meta services. The path is in the share/resources/ of each Meta service installation directory. Note For the upload address of the license file for ecosystem tools, refer to the document of Ecosystem tools overview . View the license \u00b6 View the License file directly You can use cat to view the content of the license file directly. For example: cat share/resources/nebula.license . View the License file with HTTP port When the Nebula Graph cluster is running normally, you can view the license file with the HTTP port (default port is 19559) of the meta service. For example: curl -G \"http://192.168.10.101:19559/license\" .","title":"Deploy license"},{"location":"4.deployment-and-installation/deploy-license/#deploy_a_license_for_nebula_graph_enterprise_edition","text":"Nebula Graph Enterprise Edition requires the user to deploy a license file before starting the Enterprise Edition. This topic describes how to deploy a license file for the Enterprise Edition. Enterpriseonly License is a software authorization certificate provided for users of the Enterprise Edition. Users of the Enterprise Edition can send email to inquiry@vesoft.com to apply for a license file.","title":"Deploy a license for Nebula Graph Enterprise Edition"},{"location":"4.deployment-and-installation/deploy-license/#precautions","text":"If the license file is not deployed, Nebula Graph Enterprise Edition cannot be started. Do not modify the license file, otherwise the license will become invalid. If the license is about to expire, send email to inquiry@vesoft.com to apply for renewal. The transition period after the license expires is 14 days: If you start the Enterprise Edition within 30 days before the license expires or on the day the license expires, a log will be printed as a reminder. The license can still be used for 14 days after it expires. If the license has expired for 14 days, you will not be able to start the Enterprise Edition, and a log will be printed as a reminder.","title":"Precautions"},{"location":"4.deployment-and-installation/deploy-license/#license_description","text":"The example of the content of the license file ( nebula.license ) is as follows: ----------License Content Start---------- { \"vendor\" : \"vesoft\" , \"organization\" : \"doc\" , \"issuedDate\" : \"2022-04-06T16:00:00.000Z\" , \"expirationDate\" : \"2022-05-31T15:59:59.000Z\" , \"product\" : \"nebula_graph\" , \"version\" : \">3.0.0\" , \"licenseType\" : \"enterprise\" , \"gracePeriod\" : 14 , \"graphdSpec\" : { \"nodes\" : 3 } , \"storagedSpec\" : { \"nodes\" : 3 } , \"clusterCode\" : \"BAIAEAiAQAAG\" } ----------License Content End---------- ----------License Key Start---------- cofFcOxxxxxxxxxxxxxhnZgaxrQ == ----------License Key End---------- The license file contains information such as issuedDate and expirationDate . The description is as follows. Parameter Description vendor The supplier. organization The username. issuedDate The date that the license is issued. expirationDate The date that the license expires. product The product type. The product type of Nebula Graph is nebula_graph . version The version information. licenseType The license type, including enterprise , samll_bussiness , pro , and individual . gracePeriod The buffer time (in days) for the service to continue to be used after the license expires, and the service will be stopped after the buffer period. The trial version of license has no buffer period after expiration and the default value of this parameter is 0. graphdSpec The max number of graph services in a cluster. Nebula Graph detects the number of active graph services in real-time. You are unable to connect to the cluster once the max number is reached. storagedSpec The max number of storage services in a cluster. Nebula Graph detects the number of active storage services in real-time. You are unable to connect to the cluster once the max number is reached. clusterCode The user's hardware information, which is also the unique identifier of the cluster. This parameter is not available in the trial version of the license.","title":"License description"},{"location":"4.deployment-and-installation/deploy-license/#deploy_the_license","text":"Send email to inquiry@vesoft.com to apply for the Nebula Graph Enterprise Edition package. Install Nebula Graph Enterprise Edition. The installation method is the same as the Community Edition. See Install Nebula Graph with RPM or DEB package . Send email to inquiry@vesoft.com to apply for the license file nebula.license . Upload the license file to all hosts that contain Meta services. The path is in the share/resources/ of each Meta service installation directory. Note For the upload address of the license file for ecosystem tools, refer to the document of Ecosystem tools overview .","title":"Deploy the license"},{"location":"4.deployment-and-installation/deploy-license/#view_the_license","text":"View the License file directly You can use cat to view the content of the license file directly. For example: cat share/resources/nebula.license . View the License file with HTTP port When the Nebula Graph cluster is running normally, you can view the license file with the HTTP port (default port is 19559) of the meta service. For example: curl -G \"http://192.168.10.101:19559/license\" .","title":"View the license"},{"location":"4.deployment-and-installation/manage-service/","text":"Manage Nebula Graph Service \u00b6 Nebula Graph supports managing services with scripts or systemd. This topic will describe the two methods in detail. Enterpriseonly Managing Nebula Graph with systemd is only available in the Nebula Graph Enterprise Edition. Danger The two methods are incompatible. It is recommended to use only one method in a cluster. Manage services with script \u00b6 You can use the nebula.service script to start, stop, restart, terminate, and check the Nebula Graph services. Note nebula.service is stored in the /usr/local/nebula/scripts directory by default. If you have customized the path, use the actual path in your environment. Syntax \u00b6 $ sudo /usr/local/nebula/scripts/nebula.service [ -v ] [ -c <config_file_path> ] <start | stop | restart | kill | status> <metad | graphd | storaged | all> Parameter Description -v Display detailed debugging information. -c Specify the configuration file path. The default path is /usr/local/nebula/etc/ . start Start the target services. stop Stop the target services. restart Restart the target services. kill Terminate the target services. status Check the status of the target services. metad Set the Meta Service as the target service. graphd Set the Graph Service as the target service. storaged Set the Storage Service as the target service. all Set all the Nebula Graph services as the target services. Manage services with systemd \u00b6 For easy maintenance, Nebula Graph supports managing services with systemd. You can start, stop, restart, and check services with systemctl commands. Note After installing Nebula Graph, the .service files required by systemd are located in the etc/unit path in the installation directory. Nebula Graph installed with the RPM/DEB package automatically places the .service files into the path /usr/lib/systemd/system and the parameter ExecStart is generated based on the specified Nebula Graph installation path, so you can use systemctl commands directly. Otherwise, users need to move the .service files manually into the directory /usr/lib/systemd/system , and modify the file path of the parameter ExecStart in the .service files. Syntax \u00b6 $ systemctl <start | stop | restart | status > <nebula | nebula-metad | nebula-graphd | nebula-storaged> Parameter Description start Start the target services. stop Stop the target services. restart Restart the target services. status Check the status of the target services. nebula Set all the Nebula Graph services as the target services. nebula-metad Set the Meta Service as the target service. nebula-graphd Set the Graph Service as the target service. nebula-storaged Set the Storage Service as the target service. Start Nebula Graph \u00b6 In non-container environment \u00b6 Run the following command to start Nebula Graph. $ sudo /usr/local/nebula/scripts/nebula.service start all [ INFO ] Starting nebula-metad... [ INFO ] Done [ INFO ] Starting nebula-graphd... [ INFO ] Done [ INFO ] Starting nebula-storaged... [ INFO ] Done Users can also run the following command: $ systemctl start nebula If users want to automatically start Nebula Graph when the machine starts, run the following command: $ systemctl enable nebula In docker container (deployed with docker-compose) \u00b6 Run the following command in the nebula-docker-compose/ directory to start Nebula Graph. [ nebula-docker-compose ] $ docker-compose up -d Building with native build. Learn about native build in Compose here: https://docs.docker.com/go/compose-native-build/ Creating network \"nebula-docker-compose_nebula-net\" with the default driver Creating nebula-docker-compose_metad0_1 ... done Creating nebula-docker-compose_metad2_1 ... done Creating nebula-docker-compose_metad1_1 ... done Creating nebula-docker-compose_storaged2_1 ... done Creating nebula-docker-compose_graphd1_1 ... done Creating nebula-docker-compose_storaged1_1 ... done Creating nebula-docker-compose_storaged0_1 ... done Creating nebula-docker-compose_graphd2_1 ... done Creating nebula-docker-compose_graphd_1 ... done Stop Nebula Graph \u00b6 Danger Do not run kill -9 to forcibly terminate the processes. Otherwise, there is a low probability of data loss. In non-container environment \u00b6 Run the following command to stop Nebula Graph. $ sudo /usr/local/nebula/scripts/nebula.service stop all [ INFO ] Stopping nebula-metad... [ INFO ] Done [ INFO ] Stopping nebula-graphd... [ INFO ] Done [ INFO ] Stopping nebula-storaged... [ INFO ] Done Users can also run the following command: $ systemctl stop nebula In docker container (deployed with docker-compose) \u00b6 Run the following command in the nebula-docker-compose/ directory to stop Nebula Graph. nebula-docker-compose ] $ docker-compose down Stopping nebula-docker-compose_graphd_1 ... done Stopping nebula-docker-compose_graphd2_1 ... done Stopping nebula-docker-compose_storaged0_1 ... done Stopping nebula-docker-compose_storaged1_1 ... done Stopping nebula-docker-compose_graphd1_1 ... done Stopping nebula-docker-compose_storaged2_1 ... done Stopping nebula-docker-compose_metad1_1 ... done Stopping nebula-docker-compose_metad2_1 ... done Stopping nebula-docker-compose_metad0_1 ... done Removing nebula-docker-compose_graphd_1 ... done Removing nebula-docker-compose_graphd2_1 ... done Removing nebula-docker-compose_storaged0_1 ... done Removing nebula-docker-compose_storaged1_1 ... done Removing nebula-docker-compose_graphd1_1 ... done Removing nebula-docker-compose_storaged2_1 ... done Removing nebula-docker-compose_metad1_1 ... done Removing nebula-docker-compose_metad2_1 ... done Removing nebula-docker-compose_metad0_1 ... done Removing network nebula-docker-compose_nebula-net Note If you are using a developing or nightly version for testing and have compatibility issues, try to run docker-compose down -v to DELETE all data stored in Nebula Graph and import data again. Check the service status \u00b6 In non-container environment \u00b6 Run the following command to check the service status of Nebula Graph. $ sudo /usr/local/nebula/scripts/nebula.service status all Nebula Graph is running normally if the following information is returned. [ INFO ] nebula-metad ( 02b2091 ) : Running as 26601 , Listening on 9559 [ INFO ] nebula-graphd ( 02b2091 ) : Running as 26644 , Listening on 9669 [ INFO ] nebula-storaged ( 02b2091 ) : Running as 26709 , Listening on 9779 Note After starting Nebula Graph, the port of the nebula-storaged process is shown in red. Because the nebula-storaged process waits for the nebula-metad to add the current Storage service during the startup process. The Storage works after it receives the ready signal. Starting from Nebula Graph 3.0.0, the Meta service cannot directly read or write data in the Storage service that you add in the configuration file. The configuration file only registers the Storage service to the Meta service. You must run the ADD HOSTS command to enable the Meta to read and write data in the Storage service. For more information, see Manage Storage hosts . If the returned result is similar to the following one, there is a problem. You may also go to the Nebula Graph community for help. [ INFO ] nebula-metad: Running as 25600 , Listening on 9559 [ INFO ] nebula-graphd: Exited [ INFO ] nebula-storaged: Running as 25646 , Listening on 9779 Users can also run the following command: $ systemctl status nebula \u25cf nebula.service Loaded: loaded ( /usr/lib/systemd/system/nebula.service ; disabled ; vendor preset: disabled ) Active: active ( exited ) since \u4e00 2022 -03-28 04 :13:24 UTC ; 1h 47min ago Process: 21772 ExecStart = /usr/local/ent-nightly/scripts/nebula.service start all ( code = exited, status = 0 /SUCCESS ) Main PID: 21772 ( code = exited, status = 0 /SUCCESS ) Tasks: 325 Memory: 424 .5M CGroup: /system.slice/nebula.service \u251c\u250021789 /usr/local/ent-nightly/bin/nebula-metad --flagfile /usr/local/ent-nightly/etc/nebula-metad.conf \u251c\u250021827 /usr/local/ent-nightly/bin/nebula-graphd --flagfile /usr/local/ent-nightly/etc/nebula-graphd.conf \u2514\u250021900 /usr/local/ent-nightly/bin/nebula-storaged --flagfile /usr/local/ent-nightly/etc/nebula-storaged.conf 3\u6708 28 04 :13:24 xxxxxx systemd [ 1 ] : Started nebula.service. ... The Nebula Graph services consist of the Meta Service, Graph Service, and Storage Service. The configuration files for all three services are stored in the /usr/local/nebula/etc/ directory by default. You can check the configuration files according to the returned result to troubleshoot problems. In docker container (deployed with docker-compose) \u00b6 Run the following command in the nebula-docker-compose/ directory to check the service status of Nebula Graph. nebula-docker-compose ] $ docker-compose ps Name Command State Ports --------------------------------------------------------------------------------------------------------------------------------------------------------------------- nebula-docker-compose_graphd1_1 /usr/local/nebula/bin/nebu ... Up ( healthy ) 0 .0.0.0:49223->19669/tcp, 0 .0.0.0:49222->19670/tcp, 0 .0.0.0:49224->9669/tcp nebula-docker-compose_graphd2_1 /usr/local/nebula/bin/nebu ... Up ( healthy ) 0 .0.0.0:49229->19669/tcp, 0 .0.0.0:49228->19670/tcp, 0 .0.0.0:49230->9669/tcp nebula-docker-compose_graphd_1 /usr/local/nebula/bin/nebu ... Up ( healthy ) 0 .0.0.0:49221->19669/tcp, 0 .0.0.0:49220->19670/tcp, 0 .0.0.0:9669->9669/tcp nebula-docker-compose_metad0_1 ./bin/nebula-metad --flagf ... Up ( healthy ) 0 .0.0.0:49212->19559/tcp, 0 .0.0.0:49211->19560/tcp, 0 .0.0.0:49213->9559/tcp, 9560 /tcp nebula-docker-compose_metad1_1 ./bin/nebula-metad --flagf ... Up ( healthy ) 0 .0.0.0:49209->19559/tcp, 0 .0.0.0:49208->19560/tcp, 0 .0.0.0:49210->9559/tcp, 9560 /tcp nebula-docker-compose_metad2_1 ./bin/nebula-metad --flagf ... Up ( healthy ) 0 .0.0.0:49206->19559/tcp, 0 .0.0.0:49205->19560/tcp, 0 .0.0.0:49207->9559/tcp, 9560 /tcp nebula-docker-compose_storaged0_1 ./bin/nebula-storaged --fl ... Up ( healthy ) 0 .0.0.0:49218->19779/tcp, 0 .0.0.0:49217->19780/tcp, 9777 /tcp, 9778 /tcp, 0 .0.0.0:49219->9779/tcp, 9780 /tcp nebula-docker-compose_storaged1_1 ./bin/nebula-storaged --fl ... Up ( healthy ) 0 .0.0.0:49215->19779/tcp, 0 .0.0.0:49214->19780/tcp, 9777 /tcp, 9778 /tcp, 0 .0.0.0:49216->9779/tcp, 9780 /tcp nebula-docker-compose_storaged2_1 ./bin/nebula-storaged --fl ... Up ( healthy ) 0 .0.0.0:49226->19779/tcp, 0 .0.0.0:49225->19780/tcp, 9777 /tcp, 9778 /tcp, 0 .0.0.0:49227->9779/tcp, 9780 /tcp If the service is abnormal, you can first confirm the abnormal container name (such as nebula-docker-compose_graphd2_1 ). Then you can execute docker ps to view the corresponding CONTAINER ID (such as 2a6c56c405f5 ). [ nebula-docker-compose ] $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 2a6c56c405f5 vesoft/nebula-graphd:nightly \"/usr/local/nebula/b\u2026\" 36 minutes ago Up 36 minutes ( healthy ) 0 .0.0.0:49230->9669/tcp, 0 .0.0.0:49229->19669/tcp, 0 .0.0.0:49228->19670/tcp nebula-docker-compose_graphd2_1 7042e0a8e83d vesoft/nebula-storaged:nightly \"./bin/nebula-storag\u2026\" 36 minutes ago Up 36 minutes ( healthy ) 9777 -9778/tcp, 9780 /tcp, 0 .0.0.0:49227->9779/tcp, 0 .0.0.0:49226->19779/tcp, 0 .0.0.0:49225->19780/tcp nebula-docker-compose_storaged2_1 18e3ea63ad65 vesoft/nebula-storaged:nightly \"./bin/nebula-storag\u2026\" 36 minutes ago Up 36 minutes ( healthy ) 9777 -9778/tcp, 9780 /tcp, 0 .0.0.0:49219->9779/tcp, 0 .0.0.0:49218->19779/tcp, 0 .0.0.0:49217->19780/tcp nebula-docker-compose_storaged0_1 4dcabfe8677a vesoft/nebula-graphd:nightly \"/usr/local/nebula/b\u2026\" 36 minutes ago Up 36 minutes ( healthy ) 0 .0.0.0:49224->9669/tcp, 0 .0.0.0:49223->19669/tcp, 0 .0.0.0:49222->19670/tcp nebula-docker-compose_graphd1_1 a74054c6ae25 vesoft/nebula-graphd:nightly \"/usr/local/nebula/b\u2026\" 36 minutes ago Up 36 minutes ( healthy ) 0 .0.0.0:9669->9669/tcp, 0 .0.0.0:49221->19669/tcp, 0 .0.0.0:49220->19670/tcp nebula-docker-compose_graphd_1 880025a3858c vesoft/nebula-storaged:nightly \"./bin/nebula-storag\u2026\" 36 minutes ago Up 36 minutes ( healthy ) 9777 -9778/tcp, 9780 /tcp, 0 .0.0.0:49216->9779/tcp, 0 .0.0.0:49215->19779/tcp, 0 .0.0.0:49214->19780/tcp nebula-docker-compose_storaged1_1 45736a32a23a vesoft/nebula-metad:nightly \"./bin/nebula-metad \u2026\" 36 minutes ago Up 36 minutes ( healthy ) 9560 /tcp, 0 .0.0.0:49213->9559/tcp, 0 .0.0.0:49212->19559/tcp, 0 .0.0.0:49211->19560/tcp nebula-docker-compose_metad0_1 3b2c90eb073e vesoft/nebula-metad:nightly \"./bin/nebula-metad \u2026\" 36 minutes ago Up 36 minutes ( healthy ) 9560 /tcp, 0 .0.0.0:49207->9559/tcp, 0 .0.0.0:49206->19559/tcp, 0 .0.0.0:49205->19560/tcp nebula-docker-compose_metad2_1 7bb31b7a5b3f vesoft/nebula-metad:nightly \"./bin/nebula-metad \u2026\" 36 minutes ago Up 36 minutes ( healthy ) 9560 /tcp, 0 .0.0.0:49210->9559/tcp, 0 .0.0.0:49209->19559/tcp, 0 .0.0.0:49208->19560/tcp nebula-docker-compose_metad1_1 Use the CONTAINER ID to log in the container and troubleshoot. nebula-docker-compose ] $ docker exec -it 2a6c56c405f5 bash [ root@2a6c56c405f5 nebula ] # Next to do \u00b6 Connect to Nebula Graph","title":"Manage Service"},{"location":"4.deployment-and-installation/manage-service/#manage_nebula_graph_service","text":"Nebula Graph supports managing services with scripts or systemd. This topic will describe the two methods in detail. Enterpriseonly Managing Nebula Graph with systemd is only available in the Nebula Graph Enterprise Edition. Danger The two methods are incompatible. It is recommended to use only one method in a cluster.","title":"Manage Nebula Graph Service"},{"location":"4.deployment-and-installation/manage-service/#manage_services_with_script","text":"You can use the nebula.service script to start, stop, restart, terminate, and check the Nebula Graph services. Note nebula.service is stored in the /usr/local/nebula/scripts directory by default. If you have customized the path, use the actual path in your environment.","title":"Manage services with script"},{"location":"4.deployment-and-installation/manage-service/#syntax","text":"$ sudo /usr/local/nebula/scripts/nebula.service [ -v ] [ -c <config_file_path> ] <start | stop | restart | kill | status> <metad | graphd | storaged | all> Parameter Description -v Display detailed debugging information. -c Specify the configuration file path. The default path is /usr/local/nebula/etc/ . start Start the target services. stop Stop the target services. restart Restart the target services. kill Terminate the target services. status Check the status of the target services. metad Set the Meta Service as the target service. graphd Set the Graph Service as the target service. storaged Set the Storage Service as the target service. all Set all the Nebula Graph services as the target services.","title":"Syntax"},{"location":"4.deployment-and-installation/manage-service/#manage_services_with_systemd","text":"For easy maintenance, Nebula Graph supports managing services with systemd. You can start, stop, restart, and check services with systemctl commands. Note After installing Nebula Graph, the .service files required by systemd are located in the etc/unit path in the installation directory. Nebula Graph installed with the RPM/DEB package automatically places the .service files into the path /usr/lib/systemd/system and the parameter ExecStart is generated based on the specified Nebula Graph installation path, so you can use systemctl commands directly. Otherwise, users need to move the .service files manually into the directory /usr/lib/systemd/system , and modify the file path of the parameter ExecStart in the .service files.","title":"Manage services with systemd"},{"location":"4.deployment-and-installation/manage-service/#syntax_1","text":"$ systemctl <start | stop | restart | status > <nebula | nebula-metad | nebula-graphd | nebula-storaged> Parameter Description start Start the target services. stop Stop the target services. restart Restart the target services. status Check the status of the target services. nebula Set all the Nebula Graph services as the target services. nebula-metad Set the Meta Service as the target service. nebula-graphd Set the Graph Service as the target service. nebula-storaged Set the Storage Service as the target service.","title":"Syntax"},{"location":"4.deployment-and-installation/manage-service/#start_nebula_graph","text":"","title":"Start Nebula Graph"},{"location":"4.deployment-and-installation/manage-service/#in_non-container_environment","text":"Run the following command to start Nebula Graph. $ sudo /usr/local/nebula/scripts/nebula.service start all [ INFO ] Starting nebula-metad... [ INFO ] Done [ INFO ] Starting nebula-graphd... [ INFO ] Done [ INFO ] Starting nebula-storaged... [ INFO ] Done Users can also run the following command: $ systemctl start nebula If users want to automatically start Nebula Graph when the machine starts, run the following command: $ systemctl enable nebula","title":"In non-container environment"},{"location":"4.deployment-and-installation/manage-service/#in_docker_container_deployed_with_docker-compose","text":"Run the following command in the nebula-docker-compose/ directory to start Nebula Graph. [ nebula-docker-compose ] $ docker-compose up -d Building with native build. Learn about native build in Compose here: https://docs.docker.com/go/compose-native-build/ Creating network \"nebula-docker-compose_nebula-net\" with the default driver Creating nebula-docker-compose_metad0_1 ... done Creating nebula-docker-compose_metad2_1 ... done Creating nebula-docker-compose_metad1_1 ... done Creating nebula-docker-compose_storaged2_1 ... done Creating nebula-docker-compose_graphd1_1 ... done Creating nebula-docker-compose_storaged1_1 ... done Creating nebula-docker-compose_storaged0_1 ... done Creating nebula-docker-compose_graphd2_1 ... done Creating nebula-docker-compose_graphd_1 ... done","title":"In docker container (deployed with docker-compose)"},{"location":"4.deployment-and-installation/manage-service/#stop_nebula_graph","text":"Danger Do not run kill -9 to forcibly terminate the processes. Otherwise, there is a low probability of data loss.","title":"Stop Nebula Graph"},{"location":"4.deployment-and-installation/manage-service/#in_non-container_environment_1","text":"Run the following command to stop Nebula Graph. $ sudo /usr/local/nebula/scripts/nebula.service stop all [ INFO ] Stopping nebula-metad... [ INFO ] Done [ INFO ] Stopping nebula-graphd... [ INFO ] Done [ INFO ] Stopping nebula-storaged... [ INFO ] Done Users can also run the following command: $ systemctl stop nebula","title":"In non-container environment"},{"location":"4.deployment-and-installation/manage-service/#in_docker_container_deployed_with_docker-compose_1","text":"Run the following command in the nebula-docker-compose/ directory to stop Nebula Graph. nebula-docker-compose ] $ docker-compose down Stopping nebula-docker-compose_graphd_1 ... done Stopping nebula-docker-compose_graphd2_1 ... done Stopping nebula-docker-compose_storaged0_1 ... done Stopping nebula-docker-compose_storaged1_1 ... done Stopping nebula-docker-compose_graphd1_1 ... done Stopping nebula-docker-compose_storaged2_1 ... done Stopping nebula-docker-compose_metad1_1 ... done Stopping nebula-docker-compose_metad2_1 ... done Stopping nebula-docker-compose_metad0_1 ... done Removing nebula-docker-compose_graphd_1 ... done Removing nebula-docker-compose_graphd2_1 ... done Removing nebula-docker-compose_storaged0_1 ... done Removing nebula-docker-compose_storaged1_1 ... done Removing nebula-docker-compose_graphd1_1 ... done Removing nebula-docker-compose_storaged2_1 ... done Removing nebula-docker-compose_metad1_1 ... done Removing nebula-docker-compose_metad2_1 ... done Removing nebula-docker-compose_metad0_1 ... done Removing network nebula-docker-compose_nebula-net Note If you are using a developing or nightly version for testing and have compatibility issues, try to run docker-compose down -v to DELETE all data stored in Nebula Graph and import data again.","title":"In docker container (deployed with docker-compose)"},{"location":"4.deployment-and-installation/manage-service/#check_the_service_status","text":"","title":"Check the service status"},{"location":"4.deployment-and-installation/manage-service/#in_non-container_environment_2","text":"Run the following command to check the service status of Nebula Graph. $ sudo /usr/local/nebula/scripts/nebula.service status all Nebula Graph is running normally if the following information is returned. [ INFO ] nebula-metad ( 02b2091 ) : Running as 26601 , Listening on 9559 [ INFO ] nebula-graphd ( 02b2091 ) : Running as 26644 , Listening on 9669 [ INFO ] nebula-storaged ( 02b2091 ) : Running as 26709 , Listening on 9779 Note After starting Nebula Graph, the port of the nebula-storaged process is shown in red. Because the nebula-storaged process waits for the nebula-metad to add the current Storage service during the startup process. The Storage works after it receives the ready signal. Starting from Nebula Graph 3.0.0, the Meta service cannot directly read or write data in the Storage service that you add in the configuration file. The configuration file only registers the Storage service to the Meta service. You must run the ADD HOSTS command to enable the Meta to read and write data in the Storage service. For more information, see Manage Storage hosts . If the returned result is similar to the following one, there is a problem. You may also go to the Nebula Graph community for help. [ INFO ] nebula-metad: Running as 25600 , Listening on 9559 [ INFO ] nebula-graphd: Exited [ INFO ] nebula-storaged: Running as 25646 , Listening on 9779 Users can also run the following command: $ systemctl status nebula \u25cf nebula.service Loaded: loaded ( /usr/lib/systemd/system/nebula.service ; disabled ; vendor preset: disabled ) Active: active ( exited ) since \u4e00 2022 -03-28 04 :13:24 UTC ; 1h 47min ago Process: 21772 ExecStart = /usr/local/ent-nightly/scripts/nebula.service start all ( code = exited, status = 0 /SUCCESS ) Main PID: 21772 ( code = exited, status = 0 /SUCCESS ) Tasks: 325 Memory: 424 .5M CGroup: /system.slice/nebula.service \u251c\u250021789 /usr/local/ent-nightly/bin/nebula-metad --flagfile /usr/local/ent-nightly/etc/nebula-metad.conf \u251c\u250021827 /usr/local/ent-nightly/bin/nebula-graphd --flagfile /usr/local/ent-nightly/etc/nebula-graphd.conf \u2514\u250021900 /usr/local/ent-nightly/bin/nebula-storaged --flagfile /usr/local/ent-nightly/etc/nebula-storaged.conf 3\u6708 28 04 :13:24 xxxxxx systemd [ 1 ] : Started nebula.service. ... The Nebula Graph services consist of the Meta Service, Graph Service, and Storage Service. The configuration files for all three services are stored in the /usr/local/nebula/etc/ directory by default. You can check the configuration files according to the returned result to troubleshoot problems.","title":"In non-container environment"},{"location":"4.deployment-and-installation/manage-service/#in_docker_container_deployed_with_docker-compose_2","text":"Run the following command in the nebula-docker-compose/ directory to check the service status of Nebula Graph. nebula-docker-compose ] $ docker-compose ps Name Command State Ports --------------------------------------------------------------------------------------------------------------------------------------------------------------------- nebula-docker-compose_graphd1_1 /usr/local/nebula/bin/nebu ... Up ( healthy ) 0 .0.0.0:49223->19669/tcp, 0 .0.0.0:49222->19670/tcp, 0 .0.0.0:49224->9669/tcp nebula-docker-compose_graphd2_1 /usr/local/nebula/bin/nebu ... Up ( healthy ) 0 .0.0.0:49229->19669/tcp, 0 .0.0.0:49228->19670/tcp, 0 .0.0.0:49230->9669/tcp nebula-docker-compose_graphd_1 /usr/local/nebula/bin/nebu ... Up ( healthy ) 0 .0.0.0:49221->19669/tcp, 0 .0.0.0:49220->19670/tcp, 0 .0.0.0:9669->9669/tcp nebula-docker-compose_metad0_1 ./bin/nebula-metad --flagf ... Up ( healthy ) 0 .0.0.0:49212->19559/tcp, 0 .0.0.0:49211->19560/tcp, 0 .0.0.0:49213->9559/tcp, 9560 /tcp nebula-docker-compose_metad1_1 ./bin/nebula-metad --flagf ... Up ( healthy ) 0 .0.0.0:49209->19559/tcp, 0 .0.0.0:49208->19560/tcp, 0 .0.0.0:49210->9559/tcp, 9560 /tcp nebula-docker-compose_metad2_1 ./bin/nebula-metad --flagf ... Up ( healthy ) 0 .0.0.0:49206->19559/tcp, 0 .0.0.0:49205->19560/tcp, 0 .0.0.0:49207->9559/tcp, 9560 /tcp nebula-docker-compose_storaged0_1 ./bin/nebula-storaged --fl ... Up ( healthy ) 0 .0.0.0:49218->19779/tcp, 0 .0.0.0:49217->19780/tcp, 9777 /tcp, 9778 /tcp, 0 .0.0.0:49219->9779/tcp, 9780 /tcp nebula-docker-compose_storaged1_1 ./bin/nebula-storaged --fl ... Up ( healthy ) 0 .0.0.0:49215->19779/tcp, 0 .0.0.0:49214->19780/tcp, 9777 /tcp, 9778 /tcp, 0 .0.0.0:49216->9779/tcp, 9780 /tcp nebula-docker-compose_storaged2_1 ./bin/nebula-storaged --fl ... Up ( healthy ) 0 .0.0.0:49226->19779/tcp, 0 .0.0.0:49225->19780/tcp, 9777 /tcp, 9778 /tcp, 0 .0.0.0:49227->9779/tcp, 9780 /tcp If the service is abnormal, you can first confirm the abnormal container name (such as nebula-docker-compose_graphd2_1 ). Then you can execute docker ps to view the corresponding CONTAINER ID (such as 2a6c56c405f5 ). [ nebula-docker-compose ] $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 2a6c56c405f5 vesoft/nebula-graphd:nightly \"/usr/local/nebula/b\u2026\" 36 minutes ago Up 36 minutes ( healthy ) 0 .0.0.0:49230->9669/tcp, 0 .0.0.0:49229->19669/tcp, 0 .0.0.0:49228->19670/tcp nebula-docker-compose_graphd2_1 7042e0a8e83d vesoft/nebula-storaged:nightly \"./bin/nebula-storag\u2026\" 36 minutes ago Up 36 minutes ( healthy ) 9777 -9778/tcp, 9780 /tcp, 0 .0.0.0:49227->9779/tcp, 0 .0.0.0:49226->19779/tcp, 0 .0.0.0:49225->19780/tcp nebula-docker-compose_storaged2_1 18e3ea63ad65 vesoft/nebula-storaged:nightly \"./bin/nebula-storag\u2026\" 36 minutes ago Up 36 minutes ( healthy ) 9777 -9778/tcp, 9780 /tcp, 0 .0.0.0:49219->9779/tcp, 0 .0.0.0:49218->19779/tcp, 0 .0.0.0:49217->19780/tcp nebula-docker-compose_storaged0_1 4dcabfe8677a vesoft/nebula-graphd:nightly \"/usr/local/nebula/b\u2026\" 36 minutes ago Up 36 minutes ( healthy ) 0 .0.0.0:49224->9669/tcp, 0 .0.0.0:49223->19669/tcp, 0 .0.0.0:49222->19670/tcp nebula-docker-compose_graphd1_1 a74054c6ae25 vesoft/nebula-graphd:nightly \"/usr/local/nebula/b\u2026\" 36 minutes ago Up 36 minutes ( healthy ) 0 .0.0.0:9669->9669/tcp, 0 .0.0.0:49221->19669/tcp, 0 .0.0.0:49220->19670/tcp nebula-docker-compose_graphd_1 880025a3858c vesoft/nebula-storaged:nightly \"./bin/nebula-storag\u2026\" 36 minutes ago Up 36 minutes ( healthy ) 9777 -9778/tcp, 9780 /tcp, 0 .0.0.0:49216->9779/tcp, 0 .0.0.0:49215->19779/tcp, 0 .0.0.0:49214->19780/tcp nebula-docker-compose_storaged1_1 45736a32a23a vesoft/nebula-metad:nightly \"./bin/nebula-metad \u2026\" 36 minutes ago Up 36 minutes ( healthy ) 9560 /tcp, 0 .0.0.0:49213->9559/tcp, 0 .0.0.0:49212->19559/tcp, 0 .0.0.0:49211->19560/tcp nebula-docker-compose_metad0_1 3b2c90eb073e vesoft/nebula-metad:nightly \"./bin/nebula-metad \u2026\" 36 minutes ago Up 36 minutes ( healthy ) 9560 /tcp, 0 .0.0.0:49207->9559/tcp, 0 .0.0.0:49206->19559/tcp, 0 .0.0.0:49205->19560/tcp nebula-docker-compose_metad2_1 7bb31b7a5b3f vesoft/nebula-metad:nightly \"./bin/nebula-metad \u2026\" 36 minutes ago Up 36 minutes ( healthy ) 9560 /tcp, 0 .0.0.0:49210->9559/tcp, 0 .0.0.0:49209->19559/tcp, 0 .0.0.0:49208->19560/tcp nebula-docker-compose_metad1_1 Use the CONTAINER ID to log in the container and troubleshoot. nebula-docker-compose ] $ docker exec -it 2a6c56c405f5 bash [ root@2a6c56c405f5 nebula ] #","title":"In docker container (deployed with docker-compose)"},{"location":"4.deployment-and-installation/manage-service/#next_to_do","text":"Connect to Nebula Graph","title":"Next to do"},{"location":"4.deployment-and-installation/manage-storage-host/","text":"Manage Storage hosts \u00b6 Starting from Nebula Graph 3.0.0, setting Storage hosts in the configuration files only registers the hosts on the Meta side, but does not add them into the cluster. You must run the ADD HOSTS statement to add the Storage hosts. Add Storage hosts \u00b6 Add the Storage hosts to a Nebula Graph cluster. ADD HOSTS <ip>:<port> [,<ip>:<port> ...]; Note - To make sure the follow-up operations work as expected, wait for two heartbeat cycles, i.e., 20 seconds, and then run SHOW HOSTS to check whether the host is online. - Make sure that the IP address and port number are the same as those in the configuration file. For example, the default IP address and port number in standalone deployment are 127.0.0.1:9779 . Drop Storage hosts \u00b6 Delete the Storage hosts from cluster. Note You can not delete an in-use Storage host directly. Delete the associated graph space before deleting the Storage host. DROP HOSTS <ip>:<port> [,<ip>:<port> ...];","title":"Manage Storage host"},{"location":"4.deployment-and-installation/manage-storage-host/#manage_storage_hosts","text":"Starting from Nebula Graph 3.0.0, setting Storage hosts in the configuration files only registers the hosts on the Meta side, but does not add them into the cluster. You must run the ADD HOSTS statement to add the Storage hosts.","title":"Manage Storage hosts"},{"location":"4.deployment-and-installation/manage-storage-host/#add_storage_hosts","text":"Add the Storage hosts to a Nebula Graph cluster. ADD HOSTS <ip>:<port> [,<ip>:<port> ...]; Note - To make sure the follow-up operations work as expected, wait for two heartbeat cycles, i.e., 20 seconds, and then run SHOW HOSTS to check whether the host is online. - Make sure that the IP address and port number are the same as those in the configuration file. For example, the default IP address and port number in standalone deployment are 127.0.0.1:9779 .","title":"Add Storage hosts"},{"location":"4.deployment-and-installation/manage-storage-host/#drop_storage_hosts","text":"Delete the Storage hosts from cluster. Note You can not delete an in-use Storage host directly. Delete the associated graph space before deleting the Storage host. DROP HOSTS <ip>:<port> [,<ip>:<port> ...];","title":"Drop Storage hosts"},{"location":"4.deployment-and-installation/standalone-deployment/","text":"Standalone Nebula Graph \u00b6 Standalone Nebula Graph merges the Meta, Storage, and Graph services into a single process deployed on a single machine. This topic introduces scenarios, deployment steps, etc. of standalone Nebula Graph. Background \u00b6 The traditional Nebula Graph consists of three services, each service having executable binary files and the corresponding process. Processes communicate with each other by RPC. In standalone Nebula Graph, the three processes corresponding to the three services are combined into one process. For more information about Nebula Graph, see Architecture overview . Scenarios \u00b6 Small data sizes and low availability requirements. For example, test environments that are limited by the number of machines, scenarios that are only used to verify functionality. Danger Do not use standalone Nebula Graph in production environments. Limitations \u00b6 Single service instance per machine. High availability and reliability not supported. Resource requirements \u00b6 For information about the resource requirements for standalone Nebula Graph, see Software requirements for compiling Nebula Graph . Steps \u00b6 Currently, you can only install standalone Nebula Graph with the source code. The steps are similar to those of the multi-process Nebula Graph. You only need to modify the step Generate Makefile with CMake by adding -DENABLE_STANDALONE_VERSION=on to the command. For example: cmake -DCMAKE_INSTALL_PREFIX = /usr/local/nebula -DENABLE_TESTING = OFF -DENABLE_STANDALONE_VERSION = on -DCMAKE_BUILD_TYPE = Release .. For more information about installation details, see Install Nebula Graph by compiling the source code . After installing standalone Nebula Graph, see the topic connect to Service to connect to Nebula Graph databases. Configuration file \u00b6 The path to the configuration file for standalone Nebula Graph is /usr/local/nebula/etc by default. You can run sudo cat nebula-standalone.conf.default to see the file content. The parameters and the corresponding descriptions in the file are generally the same as the configurations for multi-process Nebula Graph except for the following parameters. Parameter Predefined value Description meta_port 9559 The port number of the Meta service. storage_port 9779 The port number of the Storage Service. meta_data_path data/meta The path to Meta data. You can run commands to check configurable parameters and the corresponding descriptions. For details, see Configurations .","title":"Deploy standalone Nebula Graph"},{"location":"4.deployment-and-installation/standalone-deployment/#standalone_nebula_graph","text":"Standalone Nebula Graph merges the Meta, Storage, and Graph services into a single process deployed on a single machine. This topic introduces scenarios, deployment steps, etc. of standalone Nebula Graph.","title":"Standalone Nebula Graph"},{"location":"4.deployment-and-installation/standalone-deployment/#background","text":"The traditional Nebula Graph consists of three services, each service having executable binary files and the corresponding process. Processes communicate with each other by RPC. In standalone Nebula Graph, the three processes corresponding to the three services are combined into one process. For more information about Nebula Graph, see Architecture overview .","title":"Background"},{"location":"4.deployment-and-installation/standalone-deployment/#scenarios","text":"Small data sizes and low availability requirements. For example, test environments that are limited by the number of machines, scenarios that are only used to verify functionality. Danger Do not use standalone Nebula Graph in production environments.","title":"Scenarios"},{"location":"4.deployment-and-installation/standalone-deployment/#limitations","text":"Single service instance per machine. High availability and reliability not supported.","title":"Limitations"},{"location":"4.deployment-and-installation/standalone-deployment/#resource_requirements","text":"For information about the resource requirements for standalone Nebula Graph, see Software requirements for compiling Nebula Graph .","title":"Resource requirements"},{"location":"4.deployment-and-installation/standalone-deployment/#steps","text":"Currently, you can only install standalone Nebula Graph with the source code. The steps are similar to those of the multi-process Nebula Graph. You only need to modify the step Generate Makefile with CMake by adding -DENABLE_STANDALONE_VERSION=on to the command. For example: cmake -DCMAKE_INSTALL_PREFIX = /usr/local/nebula -DENABLE_TESTING = OFF -DENABLE_STANDALONE_VERSION = on -DCMAKE_BUILD_TYPE = Release .. For more information about installation details, see Install Nebula Graph by compiling the source code . After installing standalone Nebula Graph, see the topic connect to Service to connect to Nebula Graph databases.","title":"Steps"},{"location":"4.deployment-and-installation/standalone-deployment/#configuration_file","text":"The path to the configuration file for standalone Nebula Graph is /usr/local/nebula/etc by default. You can run sudo cat nebula-standalone.conf.default to see the file content. The parameters and the corresponding descriptions in the file are generally the same as the configurations for multi-process Nebula Graph except for the following parameters. Parameter Predefined value Description meta_port 9559 The port number of the Meta service. storage_port 9779 The port number of the Storage Service. meta_data_path data/meta The path to Meta data. You can run commands to check configurable parameters and the corresponding descriptions. For details, see Configurations .","title":"Configuration file"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/","text":"Install Nebula Graph by compiling the source code \u00b6 Installing Nebula Graph from the source code allows you to customize the compiling and installation settings and test the latest features. Prerequisites \u00b6 Users have to prepare correct resources described in Prepare resources for compiling, installing, and running Nebula Graph . The host to be installed with Nebula Graph has access to the Internet. Installation steps \u00b6 Note Starting with the Nebula Graph 3.1.0 release, the code repositories for Nebula-Graph, Nebula-Storage, and Nebula-Common have been merged into the Nebula code repository, so the compilation steps are different from those in previous releases. Use Git to clone the source code of Nebula Graph to the host. [Recommended] To install Nebula Graph 3.1.0, run the following command. $ git clone --branch release-3.1 https://github.com/vesoft-inc/nebula.git To install the latest developing release, run the following command to clone the source code from the master branch. $ git clone https://github.com/vesoft-inc/nebula.git Make the nebula directory the current working directory. $ cd nebula Create a build directory and make it the current working directory. $ mkdir build && cd build Generate Makefile with CMake. Note The installation path is /usr/local/nebula by default. To customize it, add the -DCMAKE_INSTALL_PREFIX=<installation_path> CMake variable in the following command. For more information about CMake variables, see CMake variables . $ cmake -DCMAKE_INSTALL_PREFIX = /usr/local/nebula -DENABLE_TESTING = OFF -DCMAKE_BUILD_TYPE = Release .. Compile Nebula Graph. Note Check Prepare resources for compiling, installing, and running Nebula Graph . To speed up the compiling, use the -j option to set a concurrent number N . It should be \\(\\min(\\text{CPU}core number,\\frac{the_memory_size(GB)}{2})\\) . $ make -j { N } # E.g., make -j2 Install Nebula Graph. $ sudo make install The configuration files in the etc/ directory ( /usr/local/nebula/etc by default) are references. Users can create their own configuration files accordingly. If you want to use the scripts in the script directory to start, stop, restart, and kill the service, and check the service status, the configuration files have to be named as nebula-graph.conf , nebula-metad.conf , and nebula-storaged.conf . Update the master branch \u00b6 The source code of the master branch changes frequently. If the corresponding Nebula Graph release is installed, update it in the following steps. In the nebula directory, run git pull upstream master to update the source code. In the nebula/build directory, run make -j{N} and make install again. Next to do \u00b6 (Enterprise Edition) Deploy license Manage Nebula Graph services CMake variables \u00b6 Usage of CMake variables \u00b6 $ cmake -D<variable> = <value> ... The following CMake variables can be used at the configure (cmake) stage to adjust the compiling settings. CMAKE_INSTALL_PREFIX \u00b6 CMAKE_INSTALL_PREFIX specifies the path where the service modules, scripts, configuration files are installed. The default path is /usr/local/nebula . ENABLE_WERROR \u00b6 ENABLE_WERROR is ON by default and it makes all warnings into errors. You can set it to OFF if needed. ENABLE_TESTING \u00b6 ENABLE_TESTING is ON by default and unit tests are built with the Nebula Graph services. If you just need the service modules, set it to OFF . ENABLE_ASAN \u00b6 ENABLE_ASAN is OFF by default and the building of ASan (AddressSanitizer), a memory error detector, is disabled. To enable it, set ENABLE_ASAN to ON . This variable is intended for Nebula Graph developers. CMAKE_BUILD_TYPE \u00b6 Nebula Graph supports the following building types of MAKE_BUILD_TYPE : Debug The default value of CMAKE_BUILD_TYPE . It indicates building Nebula Graph with the debug info but not the optimization options. Release It indicates building Nebula Graph with the optimization options but not the debug info. RelWithDebInfo It indicates building Nebula Graph with the optimization options and the debug info. MinSizeRel It indicates building Nebula Graph with the optimization options for controlling the code size but not the debug info. ENABLE_INCLUDE_WHAT_YOU_USE \u00b6 ENABLE_INCLUDE_WHAT_YOU_USE is OFF by default. When set to ON and include-what-you-use is installed on the system, the system reports redundant headers contained in the project source code during makefile generation. NEBULA_USE_LINKER \u00b6 Specifies the program linker on the system. The available values are: bfd , the default value, indicates that ld.bfd is applied as the linker. lld , indicates that ld.lld, if installed on the system, is applied as the linker. gold , indicates that ld.gold, if installed on the system, is applied as the linker. CMAKE_C_COMPILER/CMAKE_CXX_COMPILER \u00b6 Usually, CMake locates and uses a C/C++ compiler installed in the host automatically. But if your compiler is not installed at the standard path, or if you want to use a different one, run the command as follows to specify the installation path of the target compiler: $ cmake -DCMAKE_C_COMPILER = <path_to_gcc/bin/gcc> -DCMAKE_CXX_COMPILER = <path_to_gcc/bin/g++> .. $ cmake -DCMAKE_C_COMPILER = <path_to_clang/bin/clang> -DCMAKE_CXX_COMPILER = <path_to_clang/bin/clang++> .. ENABLE_CCACHE \u00b6 ENABLE_CCACHE is ON by default and Ccache (compiler cache) is used to speed up the compiling of Nebula Graph. To disable ccache , setting ENABLE_CCACHE to OFF is not enough. On some platforms, the ccache installation hooks up or precedes the compiler. In such a case, you have to set an environment variable export CCACHE_DISABLE=true or add a line disable=true in ~/.ccache/ccache.conf as well. For more information, see the ccache official documentation . NEBULA_THIRDPARTY_ROOT \u00b6 NEBULA_THIRDPARTY_ROOT specifies the path where the third party software is installed. By default it is /opt/vesoft/third-party . Examine problems \u00b6 If the compiling fails, we suggest you: Check whether the operating system release meets the requirements and whether the memory and hard disk space are sufficient. Check whether the third-party is installed correctly. Use make -j1 to reduce the compiling concurrency.","title":"Install Nebula\u00a0Graph by compiling the source code"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#install_nebula_graph_by_compiling_the_source_code","text":"Installing Nebula Graph from the source code allows you to customize the compiling and installation settings and test the latest features.","title":"Install Nebula Graph by compiling the source code"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#prerequisites","text":"Users have to prepare correct resources described in Prepare resources for compiling, installing, and running Nebula Graph . The host to be installed with Nebula Graph has access to the Internet.","title":"Prerequisites"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#installation_steps","text":"Note Starting with the Nebula Graph 3.1.0 release, the code repositories for Nebula-Graph, Nebula-Storage, and Nebula-Common have been merged into the Nebula code repository, so the compilation steps are different from those in previous releases. Use Git to clone the source code of Nebula Graph to the host. [Recommended] To install Nebula Graph 3.1.0, run the following command. $ git clone --branch release-3.1 https://github.com/vesoft-inc/nebula.git To install the latest developing release, run the following command to clone the source code from the master branch. $ git clone https://github.com/vesoft-inc/nebula.git Make the nebula directory the current working directory. $ cd nebula Create a build directory and make it the current working directory. $ mkdir build && cd build Generate Makefile with CMake. Note The installation path is /usr/local/nebula by default. To customize it, add the -DCMAKE_INSTALL_PREFIX=<installation_path> CMake variable in the following command. For more information about CMake variables, see CMake variables . $ cmake -DCMAKE_INSTALL_PREFIX = /usr/local/nebula -DENABLE_TESTING = OFF -DCMAKE_BUILD_TYPE = Release .. Compile Nebula Graph. Note Check Prepare resources for compiling, installing, and running Nebula Graph . To speed up the compiling, use the -j option to set a concurrent number N . It should be \\(\\min(\\text{CPU}core number,\\frac{the_memory_size(GB)}{2})\\) . $ make -j { N } # E.g., make -j2 Install Nebula Graph. $ sudo make install The configuration files in the etc/ directory ( /usr/local/nebula/etc by default) are references. Users can create their own configuration files accordingly. If you want to use the scripts in the script directory to start, stop, restart, and kill the service, and check the service status, the configuration files have to be named as nebula-graph.conf , nebula-metad.conf , and nebula-storaged.conf .","title":"Installation steps"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#update_the_master_branch","text":"The source code of the master branch changes frequently. If the corresponding Nebula Graph release is installed, update it in the following steps. In the nebula directory, run git pull upstream master to update the source code. In the nebula/build directory, run make -j{N} and make install again.","title":"Update the master branch"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#next_to_do","text":"(Enterprise Edition) Deploy license Manage Nebula Graph services","title":"Next to do"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#cmake_variables","text":"","title":"CMake variables"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#usage_of_cmake_variables","text":"$ cmake -D<variable> = <value> ... The following CMake variables can be used at the configure (cmake) stage to adjust the compiling settings.","title":"Usage of CMake variables"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#cmake_install_prefix","text":"CMAKE_INSTALL_PREFIX specifies the path where the service modules, scripts, configuration files are installed. The default path is /usr/local/nebula .","title":"CMAKE_INSTALL_PREFIX"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#enable_werror","text":"ENABLE_WERROR is ON by default and it makes all warnings into errors. You can set it to OFF if needed.","title":"ENABLE_WERROR"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#enable_testing","text":"ENABLE_TESTING is ON by default and unit tests are built with the Nebula Graph services. If you just need the service modules, set it to OFF .","title":"ENABLE_TESTING"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#enable_asan","text":"ENABLE_ASAN is OFF by default and the building of ASan (AddressSanitizer), a memory error detector, is disabled. To enable it, set ENABLE_ASAN to ON . This variable is intended for Nebula Graph developers.","title":"ENABLE_ASAN"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#cmake_build_type","text":"Nebula Graph supports the following building types of MAKE_BUILD_TYPE : Debug The default value of CMAKE_BUILD_TYPE . It indicates building Nebula Graph with the debug info but not the optimization options. Release It indicates building Nebula Graph with the optimization options but not the debug info. RelWithDebInfo It indicates building Nebula Graph with the optimization options and the debug info. MinSizeRel It indicates building Nebula Graph with the optimization options for controlling the code size but not the debug info.","title":"CMAKE_BUILD_TYPE"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#enable_include_what_you_use","text":"ENABLE_INCLUDE_WHAT_YOU_USE is OFF by default. When set to ON and include-what-you-use is installed on the system, the system reports redundant headers contained in the project source code during makefile generation.","title":"ENABLE_INCLUDE_WHAT_YOU_USE"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#nebula_use_linker","text":"Specifies the program linker on the system. The available values are: bfd , the default value, indicates that ld.bfd is applied as the linker. lld , indicates that ld.lld, if installed on the system, is applied as the linker. gold , indicates that ld.gold, if installed on the system, is applied as the linker.","title":"NEBULA_USE_LINKER"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#cmake_c_compilercmake_cxx_compiler","text":"Usually, CMake locates and uses a C/C++ compiler installed in the host automatically. But if your compiler is not installed at the standard path, or if you want to use a different one, run the command as follows to specify the installation path of the target compiler: $ cmake -DCMAKE_C_COMPILER = <path_to_gcc/bin/gcc> -DCMAKE_CXX_COMPILER = <path_to_gcc/bin/g++> .. $ cmake -DCMAKE_C_COMPILER = <path_to_clang/bin/clang> -DCMAKE_CXX_COMPILER = <path_to_clang/bin/clang++> ..","title":"CMAKE_C_COMPILER/CMAKE_CXX_COMPILER"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#enable_ccache","text":"ENABLE_CCACHE is ON by default and Ccache (compiler cache) is used to speed up the compiling of Nebula Graph. To disable ccache , setting ENABLE_CCACHE to OFF is not enough. On some platforms, the ccache installation hooks up or precedes the compiler. In such a case, you have to set an environment variable export CCACHE_DISABLE=true or add a line disable=true in ~/.ccache/ccache.conf as well. For more information, see the ccache official documentation .","title":"ENABLE_CCACHE"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#nebula_thirdparty_root","text":"NEBULA_THIRDPARTY_ROOT specifies the path where the third party software is installed. By default it is /opt/vesoft/third-party .","title":"NEBULA_THIRDPARTY_ROOT"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#examine_problems","text":"If the compiling fails, we suggest you: Check whether the operating system release meets the requirements and whether the memory and hard disk space are sufficient. Check whether the third-party is installed correctly. Use make -j1 to reduce the compiling concurrency.","title":"Examine problems"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/2.install-nebula-graph-by-rpm-or-deb/","text":"Install Nebula Graph with RPM or DEB package \u00b6 RPM and DEB are common package formats on Linux systems. This topic shows how to quickly install Nebula Graph with the RPM or DEB package. Prerequisites \u00b6 Prepare the right resources . Note The console is not complied or packaged with Nebula Graph server binaries. You can install nebula-console by yourself. Enterpriseonly For the Enterprise Edition, please send email to inquiry@vesoft.com. Download the package from cloud service \u00b6 Download the released version. URL: //Centos 6 https://oss-cdn.nebula-graph.io/package/<release_version>/nebula-graph-<release_version>.el6.x86_64.rpm //Centos 7 https://oss-cdn.nebula-graph.io/package/<release_version>/nebula-graph-<release_version>.el7.x86_64.rpm //Centos 8 https://oss-cdn.nebula-graph.io/package/<release_version>/nebula-graph-<release_version>.el8.x86_64.rpm //Ubuntu 1604 https://oss-cdn.nebula-graph.io/package/<release_version>/nebula-graph-<release_version>.ubuntu1604.amd64.deb //Ubuntu 1804 https://oss-cdn.nebula-graph.io/package/<release_version>/nebula-graph-<release_version>.ubuntu1804.amd64.deb //Ubuntu 2004 https://oss-cdn.nebula-graph.io/package/<release_version>/nebula-graph-<release_version>.ubuntu2004.amd64.deb For example, download the release package 3.1.0 for Centos 7.5 : wget https://oss-cdn.nebula-graph.io/package/3.1.0/nebula-graph-3.1.0.el7.x86_64.rpm wget https://oss-cdn.nebula-graph.io/package/3.1.0/nebula-graph-3.1.0.el7.x86_64.rpm.sha256sum.txt Download the release package 3.1.0 for Ubuntu 1804 : wget https://oss-cdn.nebula-graph.io/package/3.1.0/nebula-graph-3.1.0.ubuntu1804.amd64.deb wget https://oss-cdn.nebula-graph.io/package/3.1.0/nebula-graph-3.1.0.ubuntu1804.amd64.deb.sha256sum.txt Download the nightly version. Danger Nightly versions are usually used to test new features. Do not use it in a production environment. Nightly versions may not be built successfully every night. And the names may change from day to day. URL: //Centos 6 https://oss-cdn.nebula-graph.io/package/nightly/<yyyy.mm.dd>/nebula-graph-<yyyy.mm.dd>-nightly.el6.x86_64.rpm //Centos 7 https://oss-cdn.nebula-graph.io/package/nightly/<yyyy.mm.dd>/nebula-graph-<yyyy.mm.dd>-nightly.el7.x86_64.rpm //Centos 8 https://oss-cdn.nebula-graph.io/package/nightly/<yyyy.mm.dd>/nebula-graph-<yyyy.mm.dd>-nightly.el8.x86_64.rpm //Ubuntu 1604 https://oss-cdn.nebula-graph.io/package/nightly/<yyyy.mm.dd>/nebula-graph-<yyyy.mm.dd>-nightly.ubuntu1604.amd64.deb //Ubuntu 1804 https://oss-cdn.nebula-graph.io/package/nightly/<yyyy.mm.dd>/nebula-graph-<yyyy.mm.dd>-nightly.ubuntu1804.amd64.deb //Ubuntu 2004 https://oss-cdn.nebula-graph.io/package/nightly/<yyyy.mm.dd>/nebula-graph-<yyyy.mm.dd>-nightly.ubuntu2004.amd64.deb For example, download the Centos 7.5 package developed and built in 2021.11.28 : wget https://oss-cdn.nebula-graph.io/package/nightly/2021.11.28/nebula-graph-2021.11.28-nightly.el7.x86_64.rpm wget https://oss-cdn.nebula-graph.io/package/nightly/2021.11.28/nebula-graph-2021.11.28-nightly.el7.x86_64.rpm.sha256sum.txt For example, download the Ubuntu 1804 package developed and built in 2021.11.28 : wget https://oss-cdn.nebula-graph.io/package/nightly/2021.11.28/nebula-graph-2021.11.28-nightly.ubuntu1804.amd64.deb wget https://oss-cdn.nebula-graph.io/package/nightly/2021.11.28/nebula-graph-2021.11.28-nightly.ubuntu1804.amd64.deb.sha256sum.txt Install Nebula Graph \u00b6 Use the following syntax to install with an RPM package. $ sudo rpm -ivh --prefix = <installation_path> <package_name> The option --prefix indicates the installation path. The default path is /usr/local/nebula/ . For example, to install an RPM package in the default path for the 3.1.0 version, run the following command. sudo rpm -ivh nebula-graph-3.1.0.el7.x86_64.rpm Use the following syntax to install with a DEB package. $ sudo dpkg -i <package_name> Note Customizing the installation path is not supported when installing Nebula Graph with a DEB package. The default installation path is /usr/local/nebula/ . For example, to install a DEB package for the 3.1.0 version, run the following command. sudo dpkg -i nebula-graph-3.1.0.ubuntu1804.amd64.deb Note The default installation path is /usr/local/nebula/ . Next to do \u00b6 (Enterprise Edition) Deploy license Start Nebula Graph Connect to Nebula Graph","title":"Install Nebula Graph with RPM or DEB package"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/2.install-nebula-graph-by-rpm-or-deb/#install_nebula_graph_with_rpm_or_deb_package","text":"RPM and DEB are common package formats on Linux systems. This topic shows how to quickly install Nebula Graph with the RPM or DEB package.","title":"Install Nebula Graph with RPM or DEB package"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/2.install-nebula-graph-by-rpm-or-deb/#prerequisites","text":"Prepare the right resources . Note The console is not complied or packaged with Nebula Graph server binaries. You can install nebula-console by yourself. Enterpriseonly For the Enterprise Edition, please send email to inquiry@vesoft.com.","title":"Prerequisites"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/2.install-nebula-graph-by-rpm-or-deb/#download_the_package_from_cloud_service","text":"Download the released version. URL: //Centos 6 https://oss-cdn.nebula-graph.io/package/<release_version>/nebula-graph-<release_version>.el6.x86_64.rpm //Centos 7 https://oss-cdn.nebula-graph.io/package/<release_version>/nebula-graph-<release_version>.el7.x86_64.rpm //Centos 8 https://oss-cdn.nebula-graph.io/package/<release_version>/nebula-graph-<release_version>.el8.x86_64.rpm //Ubuntu 1604 https://oss-cdn.nebula-graph.io/package/<release_version>/nebula-graph-<release_version>.ubuntu1604.amd64.deb //Ubuntu 1804 https://oss-cdn.nebula-graph.io/package/<release_version>/nebula-graph-<release_version>.ubuntu1804.amd64.deb //Ubuntu 2004 https://oss-cdn.nebula-graph.io/package/<release_version>/nebula-graph-<release_version>.ubuntu2004.amd64.deb For example, download the release package 3.1.0 for Centos 7.5 : wget https://oss-cdn.nebula-graph.io/package/3.1.0/nebula-graph-3.1.0.el7.x86_64.rpm wget https://oss-cdn.nebula-graph.io/package/3.1.0/nebula-graph-3.1.0.el7.x86_64.rpm.sha256sum.txt Download the release package 3.1.0 for Ubuntu 1804 : wget https://oss-cdn.nebula-graph.io/package/3.1.0/nebula-graph-3.1.0.ubuntu1804.amd64.deb wget https://oss-cdn.nebula-graph.io/package/3.1.0/nebula-graph-3.1.0.ubuntu1804.amd64.deb.sha256sum.txt Download the nightly version. Danger Nightly versions are usually used to test new features. Do not use it in a production environment. Nightly versions may not be built successfully every night. And the names may change from day to day. URL: //Centos 6 https://oss-cdn.nebula-graph.io/package/nightly/<yyyy.mm.dd>/nebula-graph-<yyyy.mm.dd>-nightly.el6.x86_64.rpm //Centos 7 https://oss-cdn.nebula-graph.io/package/nightly/<yyyy.mm.dd>/nebula-graph-<yyyy.mm.dd>-nightly.el7.x86_64.rpm //Centos 8 https://oss-cdn.nebula-graph.io/package/nightly/<yyyy.mm.dd>/nebula-graph-<yyyy.mm.dd>-nightly.el8.x86_64.rpm //Ubuntu 1604 https://oss-cdn.nebula-graph.io/package/nightly/<yyyy.mm.dd>/nebula-graph-<yyyy.mm.dd>-nightly.ubuntu1604.amd64.deb //Ubuntu 1804 https://oss-cdn.nebula-graph.io/package/nightly/<yyyy.mm.dd>/nebula-graph-<yyyy.mm.dd>-nightly.ubuntu1804.amd64.deb //Ubuntu 2004 https://oss-cdn.nebula-graph.io/package/nightly/<yyyy.mm.dd>/nebula-graph-<yyyy.mm.dd>-nightly.ubuntu2004.amd64.deb For example, download the Centos 7.5 package developed and built in 2021.11.28 : wget https://oss-cdn.nebula-graph.io/package/nightly/2021.11.28/nebula-graph-2021.11.28-nightly.el7.x86_64.rpm wget https://oss-cdn.nebula-graph.io/package/nightly/2021.11.28/nebula-graph-2021.11.28-nightly.el7.x86_64.rpm.sha256sum.txt For example, download the Ubuntu 1804 package developed and built in 2021.11.28 : wget https://oss-cdn.nebula-graph.io/package/nightly/2021.11.28/nebula-graph-2021.11.28-nightly.ubuntu1804.amd64.deb wget https://oss-cdn.nebula-graph.io/package/nightly/2021.11.28/nebula-graph-2021.11.28-nightly.ubuntu1804.amd64.deb.sha256sum.txt","title":"Download the package from cloud service"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/2.install-nebula-graph-by-rpm-or-deb/#install_nebula_graph","text":"Use the following syntax to install with an RPM package. $ sudo rpm -ivh --prefix = <installation_path> <package_name> The option --prefix indicates the installation path. The default path is /usr/local/nebula/ . For example, to install an RPM package in the default path for the 3.1.0 version, run the following command. sudo rpm -ivh nebula-graph-3.1.0.el7.x86_64.rpm Use the following syntax to install with a DEB package. $ sudo dpkg -i <package_name> Note Customizing the installation path is not supported when installing Nebula Graph with a DEB package. The default installation path is /usr/local/nebula/ . For example, to install a DEB package for the 3.1.0 version, run the following command. sudo dpkg -i nebula-graph-3.1.0.ubuntu1804.amd64.deb Note The default installation path is /usr/local/nebula/ .","title":"Install Nebula Graph"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/2.install-nebula-graph-by-rpm-or-deb/#next_to_do","text":"(Enterprise Edition) Deploy license Start Nebula Graph Connect to Nebula Graph","title":"Next to do"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/3.deploy-nebula-graph-with-docker-compose/","text":"Deploy Nebula Graph with Docker Compose \u00b6 Using Docker Compose can quickly deploy Nebula Graph services based on the prepared configuration file. It is only recommended to use this method when testing functions of Nebula Graph. Prerequisites \u00b6 You have installed the following applications on your host. Application Recommended version Official installation reference Docker Latest Install Docker Engine Docker Compose Latest Install Docker Compose Git Latest Download Git If you are deploying Nebula Graph as a non-root user, grant the user with Docker-related privileges. For detailed instructions, see Manage Docker as a non-root user . You have started the Docker service on your host. If you have already deployed another version of Nebula Graph with Docker Compose on your host, to avoid compatibility issues, you need to delete the nebula-docker-compose/data directory. How to deploy and connect to Nebula Graph \u00b6 Clone the 3.1.0 branch of the nebula-docker-compose repository to your host with Git. Danger The master branch contains the untested code for the latest Nebula Graph development release. DO NOT use this release in a production environment. $ git clone -b release-3.1 https://github.com/vesoft-inc/nebula-docker-compose.git Note The x.y version of Docker Compose aligns to the x.y version of Nebula Graph. For the Nebula Graph z version, Docker Compose does not publish the corresponding z version, but pulls the z version of the Nebula Graph image. Go to the nebula-docker-compose directory. $ cd nebula-docker-compose/ Run the following command to start all the Nebula Graph services. Starting with 3.0.2, Nebula Graph comes with ARM64 Linux Docker images. You can run containerized Nebula Graph databases on Docker Desktop for ARM macOS or on ARM Linux servers. Note Update the Nebula Graph images and Nebula Console images first if they are out of date. [ nebula-docker-compose ] $ docker-compose up -d Creating nebula-docker-compose_metad0_1 ... done Creating nebula-docker-compose_metad2_1 ... done Creating nebula-docker-compose_metad1_1 ... done Creating nebula-docker-compose_graphd2_1 ... done Creating nebula-docker-compose_graphd_1 ... done Creating nebula-docker-compose_graphd1_1 ... done Creating nebula-docker-compose_storaged0_1 ... done Creating nebula-docker-compose_storaged2_1 ... done Creating nebula-docker-compose_storaged1_1 ... done Note For more information of the preceding services, see Nebula Graph architecture . Connect to Nebula Graph. Run the following command to start a new docker container with the Nebula Console image, and connect the container to the network where Nebula Graph is deployed (nebula-docker-compose_nebula-net). $ docker run --rm -ti --network nebula-docker-compose_nebula-net --entrypoint = /bin/sh vesoft/nebula-console:v3.0.0 Note The local network may be different from the nebula-docker-compose_nebula-net in the above example. Use the following command. $ docker network ls NETWORK ID NAME DRIVER SCOPE a74c312b1d16 bridge bridge local dbfa82505f0e host host local ed55ccf356ae nebula-docker-compose_nebula-net bridge local 93ba48b4b288 none null local Connect to Nebula Graph with Nebula Console. docker> nebula-console -u <user_name> -p <password> --address = graphd --port = 9669 Note By default, the authentication is off, you can only log in with an existing username (the default is root ) and any password. To turn it on, see Enable authentication . Run the following commands to make the nebula-storaged processes to the available state. nebula> ADD HOSTS \"storaged0\" :9779, \"storaged1\" :9779, \"storaged2\" :9779 ; nebula> SHOW HOSTS ; +-------------+------+-----------+----------+--------------+----------------------+------------------------+---------+ | Host | Port | HTTP port | Status | Leader count | Leader distribution | Partition distribution | Version | +-------------+------+-----------+----------+--------------+----------------------+------------------------+---------+ | \"storaged0\" | 9779 | 19669 | \"ONLINE\" | 0 | \"No valid partition\" | \"No valid partition\" | \"3.1.0\" | | \"storaged1\" | 9779 | 19669 | \"ONLINE\" | 0 | \"No valid partition\" | \"No valid partition\" | \"3.1.0\" | | \"storaged2\" | 9779 | 19669 | \"ONLINE\" | 0 | \"No valid partition\" | \"No valid partition\" | \"3.1.0\" | +-------------+------+-----------+----------+--------------+----------------------+------------------------+---------+ Note Starting from Nebula Graph version 3.0.0, the nebula-storaged service reported to metad needs to execute 'ADD HOSTS' manually before it can be used. Run exit twice to switch back to your terminal (shell). You can run Step 4 to log in to Nebula Graph again. Check the Nebula Graph service status and ports \u00b6 Run docker-compose ps to list all the services of Nebula Graph and their status and ports. $ docker-compose ps Name Command State Ports ------------------------------------------------------------------------------------------------------------------------------------------------------------------- nebula-docker-compose_graphd1_1 ./bin/nebula-graphd --flag ... Up ( health: starting ) 13000 /tcp, 13002 /tcp, 0 .0.0.0:33295->19669/tcp, 0 .0.0.0:33291->19670/tcp, 3699 /tcp, 0 .0.0.0:33298->9669/tcp nebula-docker-compose_graphd2_1 ./bin/nebula-graphd --flag ... Up ( health: starting ) 13000 /tcp, 13002 /tcp, 0 .0.0.0:33285->19669/tcp, 0 .0.0.0:33284->19670/tcp, 3699 /tcp, 0 .0.0.0:33286->9669/tcp nebula-docker-compose_graphd_1 ./bin/nebula-graphd --flag ... Up ( health: starting ) 13000 /tcp, 13002 /tcp, 0 .0.0.0:33288->19669/tcp, 0 .0.0.0:33287->19670/tcp, 3699 /tcp, 0 .0.0.0:9669->9669/tcp nebula-docker-compose_metad0_1 ./bin/nebula-metad --flagf ... Up ( health: starting ) 11000 /tcp, 11002 /tcp, 0 .0.0.0:33276->19559/tcp, 0 .0.0.0:33275->19560/tcp, 45500 /tcp, 45501 /tcp, 0 .0.0.0:33278->9559/tcp nebula-docker-compose_metad1_1 ./bin/nebula-metad --flagf ... Up ( health: starting ) 11000 /tcp, 11002 /tcp, 0 .0.0.0:33279->19559/tcp, 0 .0.0.0:33277->19560/tcp, 45500 /tcp, 45501 /tcp, 0 .0.0.0:33281->9559/tcp nebula-docker-compose_metad2_1 ./bin/nebula-metad --flagf ... Up ( health: starting ) 11000 /tcp, 11002 /tcp, 0 .0.0.0:33282->19559/tcp, 0 .0.0.0:33280->19560/tcp, 45500 /tcp, 45501 /tcp, 0 .0.0.0:33283->9559/tcp nebula-docker-compose_storaged0_1 ./bin/nebula-storaged --fl ... Up ( health: starting ) 12000 /tcp, 12002 /tcp, 0 .0.0.0:33290->19779/tcp, 0 .0.0.0:33289->19780/tcp, 44500 /tcp, 44501 /tcp, 0 .0.0.0:33294->9779/tcp nebula-docker-compose_storaged1_1 ./bin/nebula-storaged --fl ... Up ( health: starting ) 12000 /tcp, 12002 /tcp, 0 .0.0.0:33296->19779/tcp, 0 .0.0.0:33292->19780/tcp, 44500 /tcp, 44501 /tcp, 0 .0.0.0:33299->9779/tcp nebula-docker-compose_storaged2_1 ./bin/nebula-storaged --fl ... Up ( health: starting ) 12000 /tcp, 12002 /tcp, 0 .0.0.0:33297->19779/tcp, 0 .0.0.0:33293->19780/tcp, 44500 /tcp, 44501 /tcp, 0 .0.0.0:33300->9779/tcp Nebula Graph provides services to the clients through port 9669 by default. To use other ports, modify the docker-compose.yaml file in the nebula-docker-compose directory and restart the Nebula Graph services. Check the service data and logs \u00b6 All the data and logs of Nebula Graph are stored persistently in the nebula-docker-compose/data and nebula-docker-compose/logs directories. The structure of the directories is as follows: nebula-docker-compose/ |-- docker-compose.yaml \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 meta0 \u2502 \u251c\u2500\u2500 meta1 \u2502 \u251c\u2500\u2500 meta2 \u2502 \u251c\u2500\u2500 storage0 \u2502 \u251c\u2500\u2500 storage1 \u2502 \u2514\u2500\u2500 storage2 \u2514\u2500\u2500 logs \u251c\u2500\u2500 graph \u251c\u2500\u2500 graph1 \u251c\u2500\u2500 graph2 \u251c\u2500\u2500 meta0 \u251c\u2500\u2500 meta1 \u251c\u2500\u2500 meta2 \u251c\u2500\u2500 storage0 \u251c\u2500\u2500 storage1 \u2514\u2500\u2500 storage2 Stop the Nebula Graph services \u00b6 You can run the following command to stop the Nebula Graph services: $ docker-compose down The following information indicates you have successfully stopped the Nebula Graph services: Stopping nebula-docker-compose_graphd2_1 ... done Stopping nebula-docker-compose_graphd1_1 ... done Stopping nebula-docker-compose_graphd_1 ... done Stopping nebula-docker-compose_storaged1_1 ... done Stopping nebula-docker-compose_storaged2_1 ... done Stopping nebula-docker-compose_storaged0_1 ... done Stopping nebula-docker-compose_metad0_1 ... done Stopping nebula-docker-compose_metad1_1 ... done Stopping nebula-docker-compose_metad2_1 ... done Removing nebula-docker-compose_graphd2_1 ... done Removing nebula-docker-compose_graphd1_1 ... done Removing nebula-docker-compose_graphd_1 ... done Removing nebula-docker-compose_storaged1_1 ... done Removing nebula-docker-compose_storaged2_1 ... done Removing nebula-docker-compose_storaged0_1 ... done Removing nebula-docker-compose_metad0_1 ... done Removing nebula-docker-compose_metad1_1 ... done Removing nebula-docker-compose_metad2_1 ... done Removing network nebula-docker-compose_nebula-net Danger The parameter -v in the command docker-compose down -v will delete all your local Nebula Graph storage data. Try this command if you are using the nightly release and having some compatibility issues. Modify configurations \u00b6 The configuration file of Nebula Graph deployed by Docker Compose is nebula-docker-compose/docker-compose.yaml . To make the new configuration take effect, modify the configuration in this file and restart the service. For more instructions, see Configurations . FAQ \u00b6 How to fix the docker mapping to external ports? \u00b6 To set the ports of corresponding services as fixed mapping, modify the docker-compose.yaml in the nebula-docker-compose directory. For example: graphd: image: vesoft/nebula-graphd:release-3.1 ... ports: - 9669 :9669 - 19669 - 19670 9669:9669 indicates the internal port 9669 is uniformly mapped to external ports, while 19669 indicates the internal port 19669 is randomly mapped to external ports. How to upgrade or update the docker images of Nebula Graph services \u00b6 In the nebula-docker-compose/docker-compose.yaml file, change all the image values to the required image version. In the nebula-docker-compose directory, run docker-compose pull to update the images of the Graph Service, Storage Service, and Meta Service. Run docker-compose up -d to start the Nebula Graph services again. After connecting to Nebula Graph with Nebula Console, run SHOW HOSTS GRAPH , SHOW HOSTS STORAGE , or SHOW HOSTS META to check the version of the responding service respectively. ERROR: toomanyrequests when docker-compose pull \u00b6 You may meet the following error. ERROR: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit . You have met the rate limit of Docker Hub. Learn more on Understanding Docker Hub Rate Limiting . How to update the Nebula Console client \u00b6 To update the Nebula Console client, run the following command. docker pull vesoft/nebula-console:v3.0.0 Related documents \u00b6 Install and deploy Nebula Graph with the source code Install Nebula Graph by RPM or DEB Connect to Nebula Graph","title":"Deploy Nebula Graph with Docker Compose"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/3.deploy-nebula-graph-with-docker-compose/#deploy_nebula_graph_with_docker_compose","text":"Using Docker Compose can quickly deploy Nebula Graph services based on the prepared configuration file. It is only recommended to use this method when testing functions of Nebula Graph.","title":"Deploy Nebula Graph with Docker Compose"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/3.deploy-nebula-graph-with-docker-compose/#prerequisites","text":"You have installed the following applications on your host. Application Recommended version Official installation reference Docker Latest Install Docker Engine Docker Compose Latest Install Docker Compose Git Latest Download Git If you are deploying Nebula Graph as a non-root user, grant the user with Docker-related privileges. For detailed instructions, see Manage Docker as a non-root user . You have started the Docker service on your host. If you have already deployed another version of Nebula Graph with Docker Compose on your host, to avoid compatibility issues, you need to delete the nebula-docker-compose/data directory.","title":"Prerequisites"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/3.deploy-nebula-graph-with-docker-compose/#how_to_deploy_and_connect_to_nebula_graph","text":"Clone the 3.1.0 branch of the nebula-docker-compose repository to your host with Git. Danger The master branch contains the untested code for the latest Nebula Graph development release. DO NOT use this release in a production environment. $ git clone -b release-3.1 https://github.com/vesoft-inc/nebula-docker-compose.git Note The x.y version of Docker Compose aligns to the x.y version of Nebula Graph. For the Nebula Graph z version, Docker Compose does not publish the corresponding z version, but pulls the z version of the Nebula Graph image. Go to the nebula-docker-compose directory. $ cd nebula-docker-compose/ Run the following command to start all the Nebula Graph services. Starting with 3.0.2, Nebula Graph comes with ARM64 Linux Docker images. You can run containerized Nebula Graph databases on Docker Desktop for ARM macOS or on ARM Linux servers. Note Update the Nebula Graph images and Nebula Console images first if they are out of date. [ nebula-docker-compose ] $ docker-compose up -d Creating nebula-docker-compose_metad0_1 ... done Creating nebula-docker-compose_metad2_1 ... done Creating nebula-docker-compose_metad1_1 ... done Creating nebula-docker-compose_graphd2_1 ... done Creating nebula-docker-compose_graphd_1 ... done Creating nebula-docker-compose_graphd1_1 ... done Creating nebula-docker-compose_storaged0_1 ... done Creating nebula-docker-compose_storaged2_1 ... done Creating nebula-docker-compose_storaged1_1 ... done Note For more information of the preceding services, see Nebula Graph architecture . Connect to Nebula Graph. Run the following command to start a new docker container with the Nebula Console image, and connect the container to the network where Nebula Graph is deployed (nebula-docker-compose_nebula-net). $ docker run --rm -ti --network nebula-docker-compose_nebula-net --entrypoint = /bin/sh vesoft/nebula-console:v3.0.0 Note The local network may be different from the nebula-docker-compose_nebula-net in the above example. Use the following command. $ docker network ls NETWORK ID NAME DRIVER SCOPE a74c312b1d16 bridge bridge local dbfa82505f0e host host local ed55ccf356ae nebula-docker-compose_nebula-net bridge local 93ba48b4b288 none null local Connect to Nebula Graph with Nebula Console. docker> nebula-console -u <user_name> -p <password> --address = graphd --port = 9669 Note By default, the authentication is off, you can only log in with an existing username (the default is root ) and any password. To turn it on, see Enable authentication . Run the following commands to make the nebula-storaged processes to the available state. nebula> ADD HOSTS \"storaged0\" :9779, \"storaged1\" :9779, \"storaged2\" :9779 ; nebula> SHOW HOSTS ; +-------------+------+-----------+----------+--------------+----------------------+------------------------+---------+ | Host | Port | HTTP port | Status | Leader count | Leader distribution | Partition distribution | Version | +-------------+------+-----------+----------+--------------+----------------------+------------------------+---------+ | \"storaged0\" | 9779 | 19669 | \"ONLINE\" | 0 | \"No valid partition\" | \"No valid partition\" | \"3.1.0\" | | \"storaged1\" | 9779 | 19669 | \"ONLINE\" | 0 | \"No valid partition\" | \"No valid partition\" | \"3.1.0\" | | \"storaged2\" | 9779 | 19669 | \"ONLINE\" | 0 | \"No valid partition\" | \"No valid partition\" | \"3.1.0\" | +-------------+------+-----------+----------+--------------+----------------------+------------------------+---------+ Note Starting from Nebula Graph version 3.0.0, the nebula-storaged service reported to metad needs to execute 'ADD HOSTS' manually before it can be used. Run exit twice to switch back to your terminal (shell). You can run Step 4 to log in to Nebula Graph again.","title":"How to deploy and connect to Nebula Graph"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/3.deploy-nebula-graph-with-docker-compose/#check_the_nebula_graph_service_status_and_ports","text":"Run docker-compose ps to list all the services of Nebula Graph and their status and ports. $ docker-compose ps Name Command State Ports ------------------------------------------------------------------------------------------------------------------------------------------------------------------- nebula-docker-compose_graphd1_1 ./bin/nebula-graphd --flag ... Up ( health: starting ) 13000 /tcp, 13002 /tcp, 0 .0.0.0:33295->19669/tcp, 0 .0.0.0:33291->19670/tcp, 3699 /tcp, 0 .0.0.0:33298->9669/tcp nebula-docker-compose_graphd2_1 ./bin/nebula-graphd --flag ... Up ( health: starting ) 13000 /tcp, 13002 /tcp, 0 .0.0.0:33285->19669/tcp, 0 .0.0.0:33284->19670/tcp, 3699 /tcp, 0 .0.0.0:33286->9669/tcp nebula-docker-compose_graphd_1 ./bin/nebula-graphd --flag ... Up ( health: starting ) 13000 /tcp, 13002 /tcp, 0 .0.0.0:33288->19669/tcp, 0 .0.0.0:33287->19670/tcp, 3699 /tcp, 0 .0.0.0:9669->9669/tcp nebula-docker-compose_metad0_1 ./bin/nebula-metad --flagf ... Up ( health: starting ) 11000 /tcp, 11002 /tcp, 0 .0.0.0:33276->19559/tcp, 0 .0.0.0:33275->19560/tcp, 45500 /tcp, 45501 /tcp, 0 .0.0.0:33278->9559/tcp nebula-docker-compose_metad1_1 ./bin/nebula-metad --flagf ... Up ( health: starting ) 11000 /tcp, 11002 /tcp, 0 .0.0.0:33279->19559/tcp, 0 .0.0.0:33277->19560/tcp, 45500 /tcp, 45501 /tcp, 0 .0.0.0:33281->9559/tcp nebula-docker-compose_metad2_1 ./bin/nebula-metad --flagf ... Up ( health: starting ) 11000 /tcp, 11002 /tcp, 0 .0.0.0:33282->19559/tcp, 0 .0.0.0:33280->19560/tcp, 45500 /tcp, 45501 /tcp, 0 .0.0.0:33283->9559/tcp nebula-docker-compose_storaged0_1 ./bin/nebula-storaged --fl ... Up ( health: starting ) 12000 /tcp, 12002 /tcp, 0 .0.0.0:33290->19779/tcp, 0 .0.0.0:33289->19780/tcp, 44500 /tcp, 44501 /tcp, 0 .0.0.0:33294->9779/tcp nebula-docker-compose_storaged1_1 ./bin/nebula-storaged --fl ... Up ( health: starting ) 12000 /tcp, 12002 /tcp, 0 .0.0.0:33296->19779/tcp, 0 .0.0.0:33292->19780/tcp, 44500 /tcp, 44501 /tcp, 0 .0.0.0:33299->9779/tcp nebula-docker-compose_storaged2_1 ./bin/nebula-storaged --fl ... Up ( health: starting ) 12000 /tcp, 12002 /tcp, 0 .0.0.0:33297->19779/tcp, 0 .0.0.0:33293->19780/tcp, 44500 /tcp, 44501 /tcp, 0 .0.0.0:33300->9779/tcp Nebula Graph provides services to the clients through port 9669 by default. To use other ports, modify the docker-compose.yaml file in the nebula-docker-compose directory and restart the Nebula Graph services.","title":"Check the Nebula Graph service status and ports"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/3.deploy-nebula-graph-with-docker-compose/#check_the_service_data_and_logs","text":"All the data and logs of Nebula Graph are stored persistently in the nebula-docker-compose/data and nebula-docker-compose/logs directories. The structure of the directories is as follows: nebula-docker-compose/ |-- docker-compose.yaml \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 meta0 \u2502 \u251c\u2500\u2500 meta1 \u2502 \u251c\u2500\u2500 meta2 \u2502 \u251c\u2500\u2500 storage0 \u2502 \u251c\u2500\u2500 storage1 \u2502 \u2514\u2500\u2500 storage2 \u2514\u2500\u2500 logs \u251c\u2500\u2500 graph \u251c\u2500\u2500 graph1 \u251c\u2500\u2500 graph2 \u251c\u2500\u2500 meta0 \u251c\u2500\u2500 meta1 \u251c\u2500\u2500 meta2 \u251c\u2500\u2500 storage0 \u251c\u2500\u2500 storage1 \u2514\u2500\u2500 storage2","title":"Check the service data and logs"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/3.deploy-nebula-graph-with-docker-compose/#stop_the_nebula_graph_services","text":"You can run the following command to stop the Nebula Graph services: $ docker-compose down The following information indicates you have successfully stopped the Nebula Graph services: Stopping nebula-docker-compose_graphd2_1 ... done Stopping nebula-docker-compose_graphd1_1 ... done Stopping nebula-docker-compose_graphd_1 ... done Stopping nebula-docker-compose_storaged1_1 ... done Stopping nebula-docker-compose_storaged2_1 ... done Stopping nebula-docker-compose_storaged0_1 ... done Stopping nebula-docker-compose_metad0_1 ... done Stopping nebula-docker-compose_metad1_1 ... done Stopping nebula-docker-compose_metad2_1 ... done Removing nebula-docker-compose_graphd2_1 ... done Removing nebula-docker-compose_graphd1_1 ... done Removing nebula-docker-compose_graphd_1 ... done Removing nebula-docker-compose_storaged1_1 ... done Removing nebula-docker-compose_storaged2_1 ... done Removing nebula-docker-compose_storaged0_1 ... done Removing nebula-docker-compose_metad0_1 ... done Removing nebula-docker-compose_metad1_1 ... done Removing nebula-docker-compose_metad2_1 ... done Removing network nebula-docker-compose_nebula-net Danger The parameter -v in the command docker-compose down -v will delete all your local Nebula Graph storage data. Try this command if you are using the nightly release and having some compatibility issues.","title":"Stop the Nebula Graph services"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/3.deploy-nebula-graph-with-docker-compose/#modify_configurations","text":"The configuration file of Nebula Graph deployed by Docker Compose is nebula-docker-compose/docker-compose.yaml . To make the new configuration take effect, modify the configuration in this file and restart the service. For more instructions, see Configurations .","title":"Modify configurations"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/3.deploy-nebula-graph-with-docker-compose/#faq","text":"","title":"FAQ"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/3.deploy-nebula-graph-with-docker-compose/#how_to_fix_the_docker_mapping_to_external_ports","text":"To set the ports of corresponding services as fixed mapping, modify the docker-compose.yaml in the nebula-docker-compose directory. For example: graphd: image: vesoft/nebula-graphd:release-3.1 ... ports: - 9669 :9669 - 19669 - 19670 9669:9669 indicates the internal port 9669 is uniformly mapped to external ports, while 19669 indicates the internal port 19669 is randomly mapped to external ports.","title":"How to fix the docker mapping to external ports?"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/3.deploy-nebula-graph-with-docker-compose/#how_to_upgrade_or_update_the_docker_images_of_nebula_graph_services","text":"In the nebula-docker-compose/docker-compose.yaml file, change all the image values to the required image version. In the nebula-docker-compose directory, run docker-compose pull to update the images of the Graph Service, Storage Service, and Meta Service. Run docker-compose up -d to start the Nebula Graph services again. After connecting to Nebula Graph with Nebula Console, run SHOW HOSTS GRAPH , SHOW HOSTS STORAGE , or SHOW HOSTS META to check the version of the responding service respectively.","title":"How to upgrade or update the docker images of Nebula Graph services"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/3.deploy-nebula-graph-with-docker-compose/#error_toomanyrequests_when_docker-compose_pull","text":"You may meet the following error. ERROR: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit . You have met the rate limit of Docker Hub. Learn more on Understanding Docker Hub Rate Limiting .","title":"ERROR: toomanyrequests when docker-compose pull"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/3.deploy-nebula-graph-with-docker-compose/#how_to_update_the_nebula_console_client","text":"To update the Nebula Console client, run the following command. docker pull vesoft/nebula-console:v3.0.0","title":"How to update the Nebula Console client"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/3.deploy-nebula-graph-with-docker-compose/#related_documents","text":"Install and deploy Nebula Graph with the source code Install Nebula Graph by RPM or DEB Connect to Nebula Graph","title":"Related documents"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/4.install-nebula-graph-from-tar/","text":"Install Nebula graph with the tar.gz file \u00b6 You can install Nebula Graph by downloading the tar.gz file. Note Nebula Graph provides installing with the tar.gz file starting from version 2.6.0. Installation steps \u00b6 Download the Nebula Graph tar.gz file using the following address. Before downloading, you need to replace <release_version> with the version you want to download. //Centos 7 https://oss-cdn.nebula-graph.com.cn/package/<release_version>/nebula-graph-<release_version>.el7.x86_64.tar.gz //Checksum https://oss-cdn.nebula-graph.com.cn/package/<release_version>/nebula-graph-<release_version>.el7.x86_64.tar.gz.sha256sum.txt //Centos 8 https://oss-cdn.nebula-graph.com.cn/package/<release_version>/nebula-graph-<release_version>.el8.x86_64.tar.gz //Checksum https://oss-cdn.nebula-graph.com.cn/package/<release_version>/nebula-graph-<release_version>.el8.x86_64.tar.gz.sha256sum.txt //Ubuntu 1604 https://oss-cdn.nebula-graph.com.cn/package/<release_version>/nebula-graph-<release_version>.ubuntu1604.amd64.tar.gz //Checksum https://oss-cdn.nebula-graph.com.cn/package/<release_version>/nebula-graph-<release_version>.ubuntu1604.amd64.tar.gz.sha256sum.txt //Ubuntu 1804 https://oss-cdn.nebula-graph.com.cn/package/<release_version>/nebula-graph-<release_version>.ubuntu1804.amd64.tar.gz //Checksum https://oss-cdn.nebula-graph.com.cn/package/<release_version>/nebula-graph-<release_version>.ubuntu1804.amd64.tar.gz.sha256sum.txt //Ubuntu 2004 https://oss-cdn.nebula-graph.com.cn/package/<release_version>/nebula-graph-<release_version>.ubuntu2004.amd64.tar.gz //Checksum https://oss-cdn.nebula-graph.com.cn/package/<release_version>/nebula-graph-<release_version>.ubuntu2004.amd64.tar.gz.sha256sum.txt For example, to download the Nebula Graph release-3.1 tar.gz file for CentOS 7.5 , run the following command: wget https://oss-cdn.nebula-graph.com.cn/package/3.1.0/nebula-graph-3.1.0.el7.x86_64.tar.gz Decompress the tar.gz file to the Nebula Graph installation directory. tar -xvzf <tar.gz_file_name> -C <install_path> tar.gz_file_name specifies the name of the tar.gz file. install_path specifies the installation path. For example: tar -xvzf nebula-graph-2.6.0.el7.x86_64.tar.gz -C /home/joe/nebula/install Modify the name of the configuration file. Enter the decompressed directory, rename the files nebula-graphd.conf.default , nebula-metad.conf.default , and nebula-storaged.conf.default in the subdirectory etc , and delete .default to apply the default configuration of Nebula Graph. To modify the configuration, see Configurations . So far, you have installed Nebula Graph successfully. Next to do \u00b6 (Enterprise Edition) Deploy license Manage Nebula Graph services","title":"Install Nebula Graph with the tar.gz file"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/4.install-nebula-graph-from-tar/#install_nebula_graph_with_the_targz_file","text":"You can install Nebula Graph by downloading the tar.gz file. Note Nebula Graph provides installing with the tar.gz file starting from version 2.6.0.","title":"Install Nebula graph with the tar.gz file"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/4.install-nebula-graph-from-tar/#installation_steps","text":"Download the Nebula Graph tar.gz file using the following address. Before downloading, you need to replace <release_version> with the version you want to download. //Centos 7 https://oss-cdn.nebula-graph.com.cn/package/<release_version>/nebula-graph-<release_version>.el7.x86_64.tar.gz //Checksum https://oss-cdn.nebula-graph.com.cn/package/<release_version>/nebula-graph-<release_version>.el7.x86_64.tar.gz.sha256sum.txt //Centos 8 https://oss-cdn.nebula-graph.com.cn/package/<release_version>/nebula-graph-<release_version>.el8.x86_64.tar.gz //Checksum https://oss-cdn.nebula-graph.com.cn/package/<release_version>/nebula-graph-<release_version>.el8.x86_64.tar.gz.sha256sum.txt //Ubuntu 1604 https://oss-cdn.nebula-graph.com.cn/package/<release_version>/nebula-graph-<release_version>.ubuntu1604.amd64.tar.gz //Checksum https://oss-cdn.nebula-graph.com.cn/package/<release_version>/nebula-graph-<release_version>.ubuntu1604.amd64.tar.gz.sha256sum.txt //Ubuntu 1804 https://oss-cdn.nebula-graph.com.cn/package/<release_version>/nebula-graph-<release_version>.ubuntu1804.amd64.tar.gz //Checksum https://oss-cdn.nebula-graph.com.cn/package/<release_version>/nebula-graph-<release_version>.ubuntu1804.amd64.tar.gz.sha256sum.txt //Ubuntu 2004 https://oss-cdn.nebula-graph.com.cn/package/<release_version>/nebula-graph-<release_version>.ubuntu2004.amd64.tar.gz //Checksum https://oss-cdn.nebula-graph.com.cn/package/<release_version>/nebula-graph-<release_version>.ubuntu2004.amd64.tar.gz.sha256sum.txt For example, to download the Nebula Graph release-3.1 tar.gz file for CentOS 7.5 , run the following command: wget https://oss-cdn.nebula-graph.com.cn/package/3.1.0/nebula-graph-3.1.0.el7.x86_64.tar.gz Decompress the tar.gz file to the Nebula Graph installation directory. tar -xvzf <tar.gz_file_name> -C <install_path> tar.gz_file_name specifies the name of the tar.gz file. install_path specifies the installation path. For example: tar -xvzf nebula-graph-2.6.0.el7.x86_64.tar.gz -C /home/joe/nebula/install Modify the name of the configuration file. Enter the decompressed directory, rename the files nebula-graphd.conf.default , nebula-metad.conf.default , and nebula-storaged.conf.default in the subdirectory etc , and delete .default to apply the default configuration of Nebula Graph. To modify the configuration, see Configurations . So far, you have installed Nebula Graph successfully.","title":"Installation steps"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/4.install-nebula-graph-from-tar/#next_to_do","text":"(Enterprise Edition) Deploy license Manage Nebula Graph services","title":"Next to do"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/deploy-nebula-graph-cluster/","text":"Deploy a Nebula Graph cluster with RPM/DEB package on multiple servers \u00b6 For now, Nebula Graph does not provide an official deployment tool. Users can deploy a Nebula Graph cluster with RPM or DEB package manually. This topic provides an example of deploying a Nebula Graph cluster on multiple servers (machines). Deployment \u00b6 Machine name IP address Number of graphd Number of storaged Number of metad A 192.168.10.111 1 1 1 B 192.168.10.112 1 1 1 C 192.168.10.113 1 1 1 D 192.168.10.114 1 1 None E 192.168.10.115 1 1 None Prerequisites \u00b6 Prepare 5 machines for deploying the cluster. Use the NTP service to synchronize time in the cluster. Manual deployment process \u00b6 Step 1: Install Nebula Graph \u00b6 Install Nebula Graph on each machine in the cluster. Available approaches of installation are as follows. Install Nebula Graph with RPM or DEB package Install Nebula Graph by compiling the source code Step 2: Modify the configurations \u00b6 To deploy Nebula Graph according to your requirements, you have to modify the configuration files. All the configuration files for Nebula Graph, including nebula-graphd.conf , nebula-metad.conf , and nebula-storaged.conf , are stored in the etc directory in the installation path. You only need to modify the configuration for the corresponding service on the machines. The configurations that need to be modified for each machine are as follows. Machine name The configuration to be modified A nebula-graphd.conf , nebula-storaged.conf , nebula-metad.conf B nebula-graphd.conf , nebula-storaged.conf , nebula-metad.conf C nebula-graphd.conf , nebula-storaged.conf , nebula-metad.conf D nebula-graphd.conf , nebula-storaged.conf E nebula-graphd.conf , nebula-storaged.conf Users can refer to the content of the following configurations, which only show part of the cluster settings. The hidden content uses the default setting so that users can better understand the relationship between the servers in the Nebula Graph cluster. Note The main configuration to be modified is meta_server_addrs . All configurations need to fill in the IP addresses and ports of all Meta services. At the same time, local_ip needs to be modified as the network IP address of the machine itself. For detailed descriptions of the configuration parameters, see: Meta Service configurations Graph Service configurations Storage Service configurations Deploy machine A nebula-graphd.conf ########## networking ########## # Comma separated Meta Server Addresses --meta_server_addrs = 192 .168.10.111:9559,192.168.10.112:9559,192.168.10.113:9559 # Local IP used to identify the nebula-graphd process. # Change it to an address other than loopback if the service is distributed or # will be accessed remotely. --local_ip = 192 .168.10.111 # Network device to listen on --listen_netdev = any # Port to listen on --port = 9669 nebula-storaged.conf ########## networking ########## # Comma separated Meta server addresses --meta_server_addrs = 192 .168.10.111:9559,192.168.10.112:9559,192.168.10.113:9559 # Local IP used to identify the nebula-storaged process. # Change it to an address other than loopback if the service is distributed or # will be accessed remotely. --local_ip = 192 .168.10.111 # Storage daemon listening port --port = 9779 nebula-metad.conf ########## networking ########## # Comma separated Meta Server addresses --meta_server_addrs = 192 .168.10.111:9559,192.168.10.112:9559,192.168.10.113:9559 # Local IP used to identify the nebula-metad process. # Change it to an address other than loopback if the service is distributed or # will be accessed remotely. --local_ip = 192 .168.10.111 # Meta daemon listening port --port = 9559 Deploy machine B nebula-graphd.conf ########## networking ########## # Comma separated Meta Server Addresses --meta_server_addrs = 192 .168.10.111:9559,192.168.10.112:9559,192.168.10.113:9559 # Local IP used to identify the nebula-graphd process. # Change it to an address other than loopback if the service is distributed or # will be accessed remotely. --local_ip = 192 .168.10.112 # Network device to listen on --listen_netdev = any # Port to listen on --port = 9669 nebula-storaged.conf ########## networking ########## # Comma separated Meta server addresses --meta_server_addrs = 192 .168.10.111:9559,192.168.10.112:9559,192.168.10.113:9559 # Local IP used to identify the nebula-storaged process. # Change it to an address other than loopback if the service is distributed or # will be accessed remotely. --local_ip = 192 .168.10.112 # Storage daemon listening port --port = 9779 nebula-metad.conf ########## networking ########## # Comma separated Meta Server addresses --meta_server_addrs = 192 .168.10.111:9559,192.168.10.112:9559,192.168.10.113:9559 # Local IP used to identify the nebula-metad process. # Change it to an address other than loopback if the service is distributed or # will be accessed remotely. --local_ip = 192 .168.10.112 # Meta daemon listening port --port = 9559 Deploy machine C nebula-graphd.conf ########## networking ########## # Comma separated Meta Server Addresses --meta_server_addrs = 192 .168.10.111:9559,192.168.10.112:9559,192.168.10.113:9559 # Local IP used to identify the nebula-graphd process. # Change it to an address other than loopback if the service is distributed or # will be accessed remotely. --local_ip = 192 .168.10.113 # Network device to listen on --listen_netdev = any # Port to listen on --port = 9669 nebula-storaged.conf ########## networking ########## # Comma separated Meta server addresses --meta_server_addrs = 192 .168.10.111:9559,192.168.10.112:9559,192.168.10.113:9559 # Local IP used to identify the nebula-storaged process. # Change it to an address other than loopback if the service is distributed or # will be accessed remotely. --local_ip = 192 .168.10.113 # Storage daemon listening port --port = 9779 nebula-metad.conf ########## networking ########## # Comma separated Meta Server addresses --meta_server_addrs = 192 .168.10.111:9559,192.168.10.112:9559,192.168.10.113:9559 # Local IP used to identify the nebula-metad process. # Change it to an address other than loopback if the service is distributed or # will be accessed remotely. --local_ip = 192 .168.10.113 # Meta daemon listening port --port = 9559 Deploy machine D nebula-graphd.conf ########## networking ########## # Comma separated Meta Server Addresses --meta_server_addrs = 192 .168.10.111:9559,192.168.10.112:9559,192.168.10.113:9559 # Local IP used to identify the nebula-graphd process. # Change it to an address other than loopback if the service is distributed or # will be accessed remotely. --local_ip = 192 .168.10.114 # Network device to listen on --listen_netdev = any # Port to listen on --port = 9669 nebula-storaged.conf ########## networking ########## # Comma separated Meta server addresses --meta_server_addrs = 192 .168.10.111:9559,192.168.10.112:9559,192.168.10.113:9559 # Local IP used to identify the nebula-storaged process. # Change it to an address other than loopback if the service is distributed or # will be accessed remotely. --local_ip = 192 .168.10.114 # Storage daemon listening port --port = 9779 Deploy machine E nebula-graphd.conf ########## networking ########## # Comma separated Meta Server Addresses --meta_server_addrs = 192 .168.10.111:9559,192.168.10.112:9559,192.168.10.113:9559 # Local IP used to identify the nebula-graphd process. # Change it to an address other than loopback if the service is distributed or # will be accessed remotely. --local_ip = 192 .168.10.115 # Network device to listen on --listen_netdev = any # Port to listen on --port = 9669 nebula-storaged.conf ########## networking ########## # Comma separated Meta server addresses --meta_server_addrs = 192 .168.10.111:9559,192.168.10.112:9559,192.168.10.113:9559 # Local IP used to identify the nebula-storaged process. # Change it to an address other than loopback if the service is distributed or # will be accessed remotely. --local_ip = 192 .168.10.115 # Storage daemon listening port --port = 9779 Step 3: Start the cluster \u00b6 Start the corresponding service on each machine . Descriptions are as follows. Machine name The process to be started A graphd, storaged, metad B graphd, storaged, metad C graphd, storaged, metad D graphd, storaged E graphd, storaged The command to start the Nebula Graph services is as follows. sudo /usr/local/nebula/scripts/nebula.service start <metad | graphd | storaged | all> Note Make sure all the processes of services on each machine are started. Otherwise, you will fail to start Nebula Graph. When the graphd process, the storaged process, and the metad process are all started, you can use all instead. /usr/local/nebula is the default installation path for Nebula Graph. Use the actual path if you have customized the path. For more information about how to start and stop the services, see Manage Nebula Graph services . Step 4: Check the cluster status \u00b6 Install the native CLI client Nebula Console , then connect to any machine that has started the graphd process, and run SHOW HOSTS to check the cluster status. For example: $ ./nebula-console --addr 192 .168.10.111 --port 9669 -u root -p nebula 2021 /05/25 01 :41:19 [ INFO ] connection pool is initialized successfully Welcome to Nebula Graph! > SHOW HOSTS ; +------------------+------+-----------+----------+--------------+----------------------+------------------------+---------+ | Host | Port | HTTP port | Status | Leader count | Leader distribution | Partition distribution | Version | +------------------+------+-----------+----------+--------------+----------------------+------------------------+---------+ | \"192.168.10.111\" | 9779 | 19669 | \"ONLINE\" | 0 | \"No valid partition\" | \"No valid partition\" | \"3.1.0\" | | \"192.168.10.112\" | 9779 | 19669 | \"ONLINE\" | 0 | \"No valid partition\" | \"No valid partition\" | \"3.1.0\" | | \"192.168.10.113\" | 9779 | 19669 | \"ONLINE\" | 0 | \"No valid partition\" | \"No valid partition\" | \"3.1.0\" | | \"192.168.10.114\" | 9779 | 19669 | \"ONLINE\" | 0 | \"No valid partition\" | \"No valid partition\" | \"3.1.0\" | | \"192.168.10.115\" | 9779 | 19669 | \"ONLINE\" | 0 | \"No valid partition\" | \"No valid partition\" | \"3.1.0\" | +------------------+------+-----------+----------+--------------+----------------------+------------------------+---------+","title":"Deploy a Nebula Graph cluster on multiple servers"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/deploy-nebula-graph-cluster/#deploy_a_nebula_graph_cluster_with_rpmdeb_package_on_multiple_servers","text":"For now, Nebula Graph does not provide an official deployment tool. Users can deploy a Nebula Graph cluster with RPM or DEB package manually. This topic provides an example of deploying a Nebula Graph cluster on multiple servers (machines).","title":"Deploy a Nebula Graph cluster with RPM/DEB package on multiple servers"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/deploy-nebula-graph-cluster/#deployment","text":"Machine name IP address Number of graphd Number of storaged Number of metad A 192.168.10.111 1 1 1 B 192.168.10.112 1 1 1 C 192.168.10.113 1 1 1 D 192.168.10.114 1 1 None E 192.168.10.115 1 1 None","title":"Deployment"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/deploy-nebula-graph-cluster/#prerequisites","text":"Prepare 5 machines for deploying the cluster. Use the NTP service to synchronize time in the cluster.","title":"Prerequisites"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/deploy-nebula-graph-cluster/#manual_deployment_process","text":"","title":"Manual deployment process"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/deploy-nebula-graph-cluster/#step_1_install_nebula_graph","text":"Install Nebula Graph on each machine in the cluster. Available approaches of installation are as follows. Install Nebula Graph with RPM or DEB package Install Nebula Graph by compiling the source code","title":"Step 1: Install Nebula Graph"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/deploy-nebula-graph-cluster/#step_2_modify_the_configurations","text":"To deploy Nebula Graph according to your requirements, you have to modify the configuration files. All the configuration files for Nebula Graph, including nebula-graphd.conf , nebula-metad.conf , and nebula-storaged.conf , are stored in the etc directory in the installation path. You only need to modify the configuration for the corresponding service on the machines. The configurations that need to be modified for each machine are as follows. Machine name The configuration to be modified A nebula-graphd.conf , nebula-storaged.conf , nebula-metad.conf B nebula-graphd.conf , nebula-storaged.conf , nebula-metad.conf C nebula-graphd.conf , nebula-storaged.conf , nebula-metad.conf D nebula-graphd.conf , nebula-storaged.conf E nebula-graphd.conf , nebula-storaged.conf Users can refer to the content of the following configurations, which only show part of the cluster settings. The hidden content uses the default setting so that users can better understand the relationship between the servers in the Nebula Graph cluster. Note The main configuration to be modified is meta_server_addrs . All configurations need to fill in the IP addresses and ports of all Meta services. At the same time, local_ip needs to be modified as the network IP address of the machine itself. For detailed descriptions of the configuration parameters, see: Meta Service configurations Graph Service configurations Storage Service configurations Deploy machine A nebula-graphd.conf ########## networking ########## # Comma separated Meta Server Addresses --meta_server_addrs = 192 .168.10.111:9559,192.168.10.112:9559,192.168.10.113:9559 # Local IP used to identify the nebula-graphd process. # Change it to an address other than loopback if the service is distributed or # will be accessed remotely. --local_ip = 192 .168.10.111 # Network device to listen on --listen_netdev = any # Port to listen on --port = 9669 nebula-storaged.conf ########## networking ########## # Comma separated Meta server addresses --meta_server_addrs = 192 .168.10.111:9559,192.168.10.112:9559,192.168.10.113:9559 # Local IP used to identify the nebula-storaged process. # Change it to an address other than loopback if the service is distributed or # will be accessed remotely. --local_ip = 192 .168.10.111 # Storage daemon listening port --port = 9779 nebula-metad.conf ########## networking ########## # Comma separated Meta Server addresses --meta_server_addrs = 192 .168.10.111:9559,192.168.10.112:9559,192.168.10.113:9559 # Local IP used to identify the nebula-metad process. # Change it to an address other than loopback if the service is distributed or # will be accessed remotely. --local_ip = 192 .168.10.111 # Meta daemon listening port --port = 9559 Deploy machine B nebula-graphd.conf ########## networking ########## # Comma separated Meta Server Addresses --meta_server_addrs = 192 .168.10.111:9559,192.168.10.112:9559,192.168.10.113:9559 # Local IP used to identify the nebula-graphd process. # Change it to an address other than loopback if the service is distributed or # will be accessed remotely. --local_ip = 192 .168.10.112 # Network device to listen on --listen_netdev = any # Port to listen on --port = 9669 nebula-storaged.conf ########## networking ########## # Comma separated Meta server addresses --meta_server_addrs = 192 .168.10.111:9559,192.168.10.112:9559,192.168.10.113:9559 # Local IP used to identify the nebula-storaged process. # Change it to an address other than loopback if the service is distributed or # will be accessed remotely. --local_ip = 192 .168.10.112 # Storage daemon listening port --port = 9779 nebula-metad.conf ########## networking ########## # Comma separated Meta Server addresses --meta_server_addrs = 192 .168.10.111:9559,192.168.10.112:9559,192.168.10.113:9559 # Local IP used to identify the nebula-metad process. # Change it to an address other than loopback if the service is distributed or # will be accessed remotely. --local_ip = 192 .168.10.112 # Meta daemon listening port --port = 9559 Deploy machine C nebula-graphd.conf ########## networking ########## # Comma separated Meta Server Addresses --meta_server_addrs = 192 .168.10.111:9559,192.168.10.112:9559,192.168.10.113:9559 # Local IP used to identify the nebula-graphd process. # Change it to an address other than loopback if the service is distributed or # will be accessed remotely. --local_ip = 192 .168.10.113 # Network device to listen on --listen_netdev = any # Port to listen on --port = 9669 nebula-storaged.conf ########## networking ########## # Comma separated Meta server addresses --meta_server_addrs = 192 .168.10.111:9559,192.168.10.112:9559,192.168.10.113:9559 # Local IP used to identify the nebula-storaged process. # Change it to an address other than loopback if the service is distributed or # will be accessed remotely. --local_ip = 192 .168.10.113 # Storage daemon listening port --port = 9779 nebula-metad.conf ########## networking ########## # Comma separated Meta Server addresses --meta_server_addrs = 192 .168.10.111:9559,192.168.10.112:9559,192.168.10.113:9559 # Local IP used to identify the nebula-metad process. # Change it to an address other than loopback if the service is distributed or # will be accessed remotely. --local_ip = 192 .168.10.113 # Meta daemon listening port --port = 9559 Deploy machine D nebula-graphd.conf ########## networking ########## # Comma separated Meta Server Addresses --meta_server_addrs = 192 .168.10.111:9559,192.168.10.112:9559,192.168.10.113:9559 # Local IP used to identify the nebula-graphd process. # Change it to an address other than loopback if the service is distributed or # will be accessed remotely. --local_ip = 192 .168.10.114 # Network device to listen on --listen_netdev = any # Port to listen on --port = 9669 nebula-storaged.conf ########## networking ########## # Comma separated Meta server addresses --meta_server_addrs = 192 .168.10.111:9559,192.168.10.112:9559,192.168.10.113:9559 # Local IP used to identify the nebula-storaged process. # Change it to an address other than loopback if the service is distributed or # will be accessed remotely. --local_ip = 192 .168.10.114 # Storage daemon listening port --port = 9779 Deploy machine E nebula-graphd.conf ########## networking ########## # Comma separated Meta Server Addresses --meta_server_addrs = 192 .168.10.111:9559,192.168.10.112:9559,192.168.10.113:9559 # Local IP used to identify the nebula-graphd process. # Change it to an address other than loopback if the service is distributed or # will be accessed remotely. --local_ip = 192 .168.10.115 # Network device to listen on --listen_netdev = any # Port to listen on --port = 9669 nebula-storaged.conf ########## networking ########## # Comma separated Meta server addresses --meta_server_addrs = 192 .168.10.111:9559,192.168.10.112:9559,192.168.10.113:9559 # Local IP used to identify the nebula-storaged process. # Change it to an address other than loopback if the service is distributed or # will be accessed remotely. --local_ip = 192 .168.10.115 # Storage daemon listening port --port = 9779","title":"Step 2: Modify the configurations"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/deploy-nebula-graph-cluster/#step_3_start_the_cluster","text":"Start the corresponding service on each machine . Descriptions are as follows. Machine name The process to be started A graphd, storaged, metad B graphd, storaged, metad C graphd, storaged, metad D graphd, storaged E graphd, storaged The command to start the Nebula Graph services is as follows. sudo /usr/local/nebula/scripts/nebula.service start <metad | graphd | storaged | all> Note Make sure all the processes of services on each machine are started. Otherwise, you will fail to start Nebula Graph. When the graphd process, the storaged process, and the metad process are all started, you can use all instead. /usr/local/nebula is the default installation path for Nebula Graph. Use the actual path if you have customized the path. For more information about how to start and stop the services, see Manage Nebula Graph services .","title":"Step 3: Start the cluster"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/deploy-nebula-graph-cluster/#step_4_check_the_cluster_status","text":"Install the native CLI client Nebula Console , then connect to any machine that has started the graphd process, and run SHOW HOSTS to check the cluster status. For example: $ ./nebula-console --addr 192 .168.10.111 --port 9669 -u root -p nebula 2021 /05/25 01 :41:19 [ INFO ] connection pool is initialized successfully Welcome to Nebula Graph! > SHOW HOSTS ; +------------------+------+-----------+----------+--------------+----------------------+------------------------+---------+ | Host | Port | HTTP port | Status | Leader count | Leader distribution | Partition distribution | Version | +------------------+------+-----------+----------+--------------+----------------------+------------------------+---------+ | \"192.168.10.111\" | 9779 | 19669 | \"ONLINE\" | 0 | \"No valid partition\" | \"No valid partition\" | \"3.1.0\" | | \"192.168.10.112\" | 9779 | 19669 | \"ONLINE\" | 0 | \"No valid partition\" | \"No valid partition\" | \"3.1.0\" | | \"192.168.10.113\" | 9779 | 19669 | \"ONLINE\" | 0 | \"No valid partition\" | \"No valid partition\" | \"3.1.0\" | | \"192.168.10.114\" | 9779 | 19669 | \"ONLINE\" | 0 | \"No valid partition\" | \"No valid partition\" | \"3.1.0\" | | \"192.168.10.115\" | 9779 | 19669 | \"ONLINE\" | 0 | \"No valid partition\" | \"No valid partition\" | \"3.1.0\" | +------------------+------+-----------+----------+--------------+----------------------+------------------------+---------+","title":"Step 4: Check the cluster status"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/upgrade-nebula-graph-to-latest/","text":"Upgrade Nebula Graph from version 2.x to 3.1.0 \u00b6 This topic describes how to upgrade Nebula Graph from version 2.x to 3.1.0, taking upgrading from version 2.6.1 to 3.1.0 as an example. Applicable source versions \u00b6 This topic applies to upgrading Nebula Graph from 2.0.0 and later 2.x versions to 3.1.0. It does not apply to historical versions earlier than 2.0.0, including the 1.x versions. To upgrade Nebula Graph from historical versions to 3.1.0: Upgrade it to the latest 2.x version according to the docs of that version. Follow this topic to upgrade it to 3.1.0. Caution To upgrade Nebula Graph from versions earlier than 2.0.0 (including the 1.x versions) to 3.1.0, you need to find the date_time_zonespec.csv in the share/resources directory of 3.1.0 files, and then copy it to the same directory in the Nebula Graph installation path. Limitations \u00b6 Rolling Upgrade is not supported. You must stop all the Nebula Graph services before the upgrade. There is no upgrade script. You have to manually upgrade each server in the cluster. This topic does not apply to scenarios where Nebula Graph is deployed with Docker, including Docker Swarm, Docker Compose, and K8s. You must upgrade the old Nebula Graph services on the same machines they are deployed. DO NOT change the IP addresses, configuration files of the machines, and DO NOT change the cluster topology. The hard disk space left on each machine should be two times as much as the space taken by the original data directories. Half of the reserved space is for storing the manual backup of data. The other half is for storing the WALs copied to the dst_db_path and the new keys generated to support vertices with no tags. Known issues that could cause data loss are listed on GitHub known issues . The issues are all related to altering schema or default values. DO NOT use soft links to switch the data directories. You must have the sudo privileges to complete the steps in this topic. Upgrade influences \u00b6 Data swelling The Nebula Graph 3.x version expands the original data format with one more key per vertex, so the data takes up more space after the upgrade. The format of the new key is: Type (1 byte) + Partition ID (3 bytes) + VID (size depends on the data type). The value of the new key is empty. The extra space taken can be calculated based on the number of vertices and the data type of the VID. For example, if there are 100 million vertices in the dataset and the VIDs are INT64, the new key will take 100 million x (1 + 3 + 8) = 1.2 billion bytes, i.e., about 1.2 GB. Client compatibility After the upgrade, you will not be able to connect to Nebula Graph from old clients. You will need to upgrade all clients to a version compatible with Nebula Graph 3.1.0. Configuration changes A few configuration parameters have been changed. For more information, see the release notes and configuration docs. nGQL compatibility The nGQL syntax is partially incompatible: Disable the YIELD clause to return custom variables. The YIELD clause is required in the FETCH , GO , LOOKUP , FIND PATH and GET SUBGRAPH statements. It is required to specify a tag to query properties of a vertex in a MATCH statement. For example, from return v.name to return v.player.name . Caution There may be other undiscovered influences. Before the upgrade, we recommend that you read the release notes and user manual carefully, and keep an eye on the posts on the forum and issues on Github. Preparations before the upgrade \u00b6 Download the TAR file of Nebula Graph 3.1.0 according to your operating system and system architecture. You need the binary files during the upgrade. Find the TAR file on the download page . Note You can also get the new binaries from the source code or the RPM/DEB package. Locate the data files based on the value of the data_path parameters in the Storage and Meta configurations, and backup the data files. The default paths are nebula/data/storage and nebula/data/meta . Danger The old data will not be automatically backed up during the upgrade. You must manually back up the data to avoid data loss. Backup the configuration files. Collect the statistics of all graph spaces before the upgrade. After the upgrade, you can collect again and compare the results to make sure that no data is lost. To collect the statistics: Run SUBMIT JOB STATS . Run SHOW JOBS and record the result. Upgrade steps \u00b6 Stop all Nebula Graph services. <nebula_install_path>/scripts/nebula.service stop all nebula_install_path indicates the installation path of Nebula Graph. The storaged progress needs around 1 minute to flush data. You can run nebula.service status all to check if all services are stopped. For more information about starting and stopping services, see Manage services . Note If the services are not fully stopped in 20 minutes, stop upgrading and ask for help on the forum or Github . In the target path where you unpacked the TAR file, use the binaries in the bin directory to replace the old binaries in the bin directory in the Nebula Graph installation path. Note Update the binary of the corresponding service on each Nebula Graph server. Modify the following parameters in all Graph configuration files to accommodate the value range of the new version. If the parameter values are within the specified range, skip this step. Set a value in [1,604800] for session_idle_timeout_secs . The recommended value is 28800. Set a value in [1,604800] for client_idle_timeout_secs . The recommended value is 28800. The default values of these parameters in the 2.x versions are not within the range of the new version. If you do not change the default values, the upgrade will fail. For detailed parameter description, see Graph Service Configuration . Start all Meta services. <nebula_install_path>/scripts/nebula-metad.service start Once started, the Meta services take several seconds to elect a leader. To verify that Meta services are all started, you can start any Graph server, connect to it through Nebula Console, and run SHOW HOSTS meta and SHOW META LEADER . If the status of Meta services are correctly returned, the services are successfully started. Note If the operation fails, stop the upgrade and ask for help on the forum or GitHub . Use the new db_upgrader file in the bin directory to upgrade the format of old data. Danger This step DOES NOT back up the Storage data. To avoid data loss, before executing this step, make sure that you have followed the Preparations before the upgrade section and backed up the Meta data and Storage data. Command syntax: <nebula_install_path>/bin/db_upgrader \\ --src_db_path=<old_storage_data_path> \\ --dst_db_path=<data_backup_path> \\ --upgrade_meta_server=<meta_server_ip>:<port>[, <meta_server_ip>:<port> ...] \\ --upgrade_version=2:3 old_storage_data_path indicates the path of the Storage data. It is defined by the data_path parameter in the Storage configuration files. data_backup_path indicates a custom path for data backup. This option does not work for the current version and the old data will not be backed up to any path. meta_server_ip and port indicate the IP address and port number of a Meta server. 2:3 indicates that the upgrade is from version 2.x to 3.x. Example for the test in this topic: <nebula_install_path>/bin/db_upgrader \\ --src_db_path=/usr/local/nebula/data/storage \\ --dst_db_path=/home/vesoft/nebula/data-backup \\ --upgrade_meta_server=192.168.8.132:9559 \\ --upgrade_version=2:3 Note If the operation fails, stop the upgrade and ask for help on the forum or GitHub . Start all the Graph and Storage services. Note If the operation fails, stop the upgrade and ask for help on the forum or GitHub . Connect to the new version of Nebula Graph to verify that services are available and data are complete. For how to connect, see Connect to Nebula Graph . Currently, there is no official way to check whether the upgrade is successful. You can run the following reference statements to test the upgrade: nebula> SHOW HOSTS; nebula> SHOW HOSTS storage; nebula> SHOW SPACES; nebula> USE <space_name> nebula> SHOW PARTS; nebula> SUBMIT JOB STATS; nebula> SHOW STATS; nebula> MATCH (v) RETURN v LIMIT 5; You can also test against new features in version 3.1.0. Upgrade failure and rollback \u00b6 If the upgrade fails, stop all Nebula Graph services of the new version, recover the old configuration files and binaries, and start the services of the old version. All Nebula Graph clients in use must be switched to the old version. FAQ \u00b6 Can I write through the client during the upgrade? \u00b6 A: No. You must stop all Nebula Graph services during the upgrade. How to upgrade if a machine has only the Graph Service, but not the Storage Service? \u00b6 A: You only need to update the configuration files and binaries of the Graph Service. How to resolve the error Permission denied ? \u00b6 A: Try again with the sudo privileges. Is there any change in gflags? \u00b6 A: Yes. For more information, see the release notes and configuration docs. Is there a tool or solution for verifying data consistency after the upgrade? \u00b6 A: No. But if you only want to check the number of vertices and edges, run SUBMIT JOB STATS and SHOW STATS after the upgrade, and compare the result with the result that you recorded before the upgrade. How to solve the issue that Storage is OFFLINE and Leader count is 0 ? \u00b6 A: Run the following statement to add the Storage hosts into the cluster manually. ADD HOSTS <ip>:<port>[, <ip>:<port> ...]; For example: ADD HOSTS 192.168.10.100:9779, 192.168.10.101:9779, 192.168.10.102:9779; If the issue persists, ask for help on the forum or GitHub . Why the job type changed after the upgrade, but job ID remains the same? \u00b6 A: SHOW JOBS depends on an internal ID to identify job types, but in Nebula Graph 2.5.0 the internal ID changed in this pull request , so this issue happens after upgrading from a version earlier than 2.5.0.","title":"Upgrade Nebula Graph to the latest version"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/upgrade-nebula-graph-to-latest/#upgrade_nebula_graph_from_version_2x_to_310","text":"This topic describes how to upgrade Nebula Graph from version 2.x to 3.1.0, taking upgrading from version 2.6.1 to 3.1.0 as an example.","title":"Upgrade Nebula Graph from version 2.x to 3.1.0"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/upgrade-nebula-graph-to-latest/#applicable_source_versions","text":"This topic applies to upgrading Nebula Graph from 2.0.0 and later 2.x versions to 3.1.0. It does not apply to historical versions earlier than 2.0.0, including the 1.x versions. To upgrade Nebula Graph from historical versions to 3.1.0: Upgrade it to the latest 2.x version according to the docs of that version. Follow this topic to upgrade it to 3.1.0. Caution To upgrade Nebula Graph from versions earlier than 2.0.0 (including the 1.x versions) to 3.1.0, you need to find the date_time_zonespec.csv in the share/resources directory of 3.1.0 files, and then copy it to the same directory in the Nebula Graph installation path.","title":"Applicable source versions"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/upgrade-nebula-graph-to-latest/#limitations","text":"Rolling Upgrade is not supported. You must stop all the Nebula Graph services before the upgrade. There is no upgrade script. You have to manually upgrade each server in the cluster. This topic does not apply to scenarios where Nebula Graph is deployed with Docker, including Docker Swarm, Docker Compose, and K8s. You must upgrade the old Nebula Graph services on the same machines they are deployed. DO NOT change the IP addresses, configuration files of the machines, and DO NOT change the cluster topology. The hard disk space left on each machine should be two times as much as the space taken by the original data directories. Half of the reserved space is for storing the manual backup of data. The other half is for storing the WALs copied to the dst_db_path and the new keys generated to support vertices with no tags. Known issues that could cause data loss are listed on GitHub known issues . The issues are all related to altering schema or default values. DO NOT use soft links to switch the data directories. You must have the sudo privileges to complete the steps in this topic.","title":"Limitations"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/upgrade-nebula-graph-to-latest/#upgrade_influences","text":"Data swelling The Nebula Graph 3.x version expands the original data format with one more key per vertex, so the data takes up more space after the upgrade. The format of the new key is: Type (1 byte) + Partition ID (3 bytes) + VID (size depends on the data type). The value of the new key is empty. The extra space taken can be calculated based on the number of vertices and the data type of the VID. For example, if there are 100 million vertices in the dataset and the VIDs are INT64, the new key will take 100 million x (1 + 3 + 8) = 1.2 billion bytes, i.e., about 1.2 GB. Client compatibility After the upgrade, you will not be able to connect to Nebula Graph from old clients. You will need to upgrade all clients to a version compatible with Nebula Graph 3.1.0. Configuration changes A few configuration parameters have been changed. For more information, see the release notes and configuration docs. nGQL compatibility The nGQL syntax is partially incompatible: Disable the YIELD clause to return custom variables. The YIELD clause is required in the FETCH , GO , LOOKUP , FIND PATH and GET SUBGRAPH statements. It is required to specify a tag to query properties of a vertex in a MATCH statement. For example, from return v.name to return v.player.name . Caution There may be other undiscovered influences. Before the upgrade, we recommend that you read the release notes and user manual carefully, and keep an eye on the posts on the forum and issues on Github.","title":"Upgrade influences"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/upgrade-nebula-graph-to-latest/#preparations_before_the_upgrade","text":"Download the TAR file of Nebula Graph 3.1.0 according to your operating system and system architecture. You need the binary files during the upgrade. Find the TAR file on the download page . Note You can also get the new binaries from the source code or the RPM/DEB package. Locate the data files based on the value of the data_path parameters in the Storage and Meta configurations, and backup the data files. The default paths are nebula/data/storage and nebula/data/meta . Danger The old data will not be automatically backed up during the upgrade. You must manually back up the data to avoid data loss. Backup the configuration files. Collect the statistics of all graph spaces before the upgrade. After the upgrade, you can collect again and compare the results to make sure that no data is lost. To collect the statistics: Run SUBMIT JOB STATS . Run SHOW JOBS and record the result.","title":"Preparations before the upgrade"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/upgrade-nebula-graph-to-latest/#upgrade_steps","text":"Stop all Nebula Graph services. <nebula_install_path>/scripts/nebula.service stop all nebula_install_path indicates the installation path of Nebula Graph. The storaged progress needs around 1 minute to flush data. You can run nebula.service status all to check if all services are stopped. For more information about starting and stopping services, see Manage services . Note If the services are not fully stopped in 20 minutes, stop upgrading and ask for help on the forum or Github . In the target path where you unpacked the TAR file, use the binaries in the bin directory to replace the old binaries in the bin directory in the Nebula Graph installation path. Note Update the binary of the corresponding service on each Nebula Graph server. Modify the following parameters in all Graph configuration files to accommodate the value range of the new version. If the parameter values are within the specified range, skip this step. Set a value in [1,604800] for session_idle_timeout_secs . The recommended value is 28800. Set a value in [1,604800] for client_idle_timeout_secs . The recommended value is 28800. The default values of these parameters in the 2.x versions are not within the range of the new version. If you do not change the default values, the upgrade will fail. For detailed parameter description, see Graph Service Configuration . Start all Meta services. <nebula_install_path>/scripts/nebula-metad.service start Once started, the Meta services take several seconds to elect a leader. To verify that Meta services are all started, you can start any Graph server, connect to it through Nebula Console, and run SHOW HOSTS meta and SHOW META LEADER . If the status of Meta services are correctly returned, the services are successfully started. Note If the operation fails, stop the upgrade and ask for help on the forum or GitHub . Use the new db_upgrader file in the bin directory to upgrade the format of old data. Danger This step DOES NOT back up the Storage data. To avoid data loss, before executing this step, make sure that you have followed the Preparations before the upgrade section and backed up the Meta data and Storage data. Command syntax: <nebula_install_path>/bin/db_upgrader \\ --src_db_path=<old_storage_data_path> \\ --dst_db_path=<data_backup_path> \\ --upgrade_meta_server=<meta_server_ip>:<port>[, <meta_server_ip>:<port> ...] \\ --upgrade_version=2:3 old_storage_data_path indicates the path of the Storage data. It is defined by the data_path parameter in the Storage configuration files. data_backup_path indicates a custom path for data backup. This option does not work for the current version and the old data will not be backed up to any path. meta_server_ip and port indicate the IP address and port number of a Meta server. 2:3 indicates that the upgrade is from version 2.x to 3.x. Example for the test in this topic: <nebula_install_path>/bin/db_upgrader \\ --src_db_path=/usr/local/nebula/data/storage \\ --dst_db_path=/home/vesoft/nebula/data-backup \\ --upgrade_meta_server=192.168.8.132:9559 \\ --upgrade_version=2:3 Note If the operation fails, stop the upgrade and ask for help on the forum or GitHub . Start all the Graph and Storage services. Note If the operation fails, stop the upgrade and ask for help on the forum or GitHub . Connect to the new version of Nebula Graph to verify that services are available and data are complete. For how to connect, see Connect to Nebula Graph . Currently, there is no official way to check whether the upgrade is successful. You can run the following reference statements to test the upgrade: nebula> SHOW HOSTS; nebula> SHOW HOSTS storage; nebula> SHOW SPACES; nebula> USE <space_name> nebula> SHOW PARTS; nebula> SUBMIT JOB STATS; nebula> SHOW STATS; nebula> MATCH (v) RETURN v LIMIT 5; You can also test against new features in version 3.1.0.","title":"Upgrade steps"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/upgrade-nebula-graph-to-latest/#upgrade_failure_and_rollback","text":"If the upgrade fails, stop all Nebula Graph services of the new version, recover the old configuration files and binaries, and start the services of the old version. All Nebula Graph clients in use must be switched to the old version.","title":"Upgrade failure and rollback"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/upgrade-nebula-graph-to-latest/#faq","text":"","title":"FAQ"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/upgrade-nebula-graph-to-latest/#can_i_write_through_the_client_during_the_upgrade","text":"A: No. You must stop all Nebula Graph services during the upgrade.","title":"Can I write through the client during the upgrade?"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/upgrade-nebula-graph-to-latest/#how_to_upgrade_if_a_machine_has_only_the_graph_service_but_not_the_storage_service","text":"A: You only need to update the configuration files and binaries of the Graph Service.","title":"How to upgrade if a machine has only the Graph Service, but not the Storage Service?"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/upgrade-nebula-graph-to-latest/#how_to_resolve_the_error_permission_denied","text":"A: Try again with the sudo privileges.","title":"How to resolve the error Permission denied?"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/upgrade-nebula-graph-to-latest/#is_there_any_change_in_gflags","text":"A: Yes. For more information, see the release notes and configuration docs.","title":"Is there any change in gflags?"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/upgrade-nebula-graph-to-latest/#is_there_a_tool_or_solution_for_verifying_data_consistency_after_the_upgrade","text":"A: No. But if you only want to check the number of vertices and edges, run SUBMIT JOB STATS and SHOW STATS after the upgrade, and compare the result with the result that you recorded before the upgrade.","title":"Is there a tool or solution for verifying data consistency after the upgrade?"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/upgrade-nebula-graph-to-latest/#how_to_solve_the_issue_that_storage_is_offline_and_leader_count_is_0","text":"A: Run the following statement to add the Storage hosts into the cluster manually. ADD HOSTS <ip>:<port>[, <ip>:<port> ...]; For example: ADD HOSTS 192.168.10.100:9779, 192.168.10.101:9779, 192.168.10.102:9779; If the issue persists, ask for help on the forum or GitHub .","title":"How to solve the issue that Storage is OFFLINE and Leader count is 0?"},{"location":"4.deployment-and-installation/3.upgrade-nebula-graph/upgrade-nebula-graph-to-latest/#why_the_job_type_changed_after_the_upgrade_but_job_id_remains_the_same","text":"A: SHOW JOBS depends on an internal ID to identify job types, but in Nebula Graph 2.5.0 the internal ID changed in this pull request , so this issue happens after upgrading from a version earlier than 2.5.0.","title":"Why the job type changed after the upgrade, but job ID remains the same?"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/1.text-based-index-restrictions/","text":"Full-text index restrictions \u00b6 Caution This topic introduces the restrictions for full-text indexes. Please read the restrictions very carefully before using the full-text indexes. For now, full-text search has the following limitations: Currently, full-text search supports LOOKUP statements only. The maximum indexing string length is 256 bytes. The part of data that exceeds 256 bytes will not be indexed. If there is a full-text index on the tag/edge type, the tag/edge type cannot be deleted or modified. One tag/edge type can only have one full-text index. The type of properties must be string . Full-text index can not be applied to search multiple tags/edge types. Sorting for the returned results of the full-text search is not supported. Data is returned in the order of data insertion. Full-text index can not search properties with value NULL . Altering Elasticsearch indexes is not supported at this time. The pipe operator is not supported. WHERE clauses supports full-text search only working on single terms. Full-text indexes are not deleted together with the graph space. Make sure that you start the Elasticsearch cluster and Nebula Graph at the same time. If not, the data writing on the Elasticsearch cluster can be incomplete. Do not contain ' or \\ in the vertex or edge values. If not, an error will be caused in the Elasticsearch cluster storage. It may take a while for Elasticsearch to create indexes. If Nebula Graph warns no index is found, wait for the index to take effect (however, the waiting time is unknown and there is no code to check). Nebula Graph clusters deployed with K8s do not support the full-text search feature.","title":"Full-text restrictions"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/1.text-based-index-restrictions/#full-text_index_restrictions","text":"Caution This topic introduces the restrictions for full-text indexes. Please read the restrictions very carefully before using the full-text indexes. For now, full-text search has the following limitations: Currently, full-text search supports LOOKUP statements only. The maximum indexing string length is 256 bytes. The part of data that exceeds 256 bytes will not be indexed. If there is a full-text index on the tag/edge type, the tag/edge type cannot be deleted or modified. One tag/edge type can only have one full-text index. The type of properties must be string . Full-text index can not be applied to search multiple tags/edge types. Sorting for the returned results of the full-text search is not supported. Data is returned in the order of data insertion. Full-text index can not search properties with value NULL . Altering Elasticsearch indexes is not supported at this time. The pipe operator is not supported. WHERE clauses supports full-text search only working on single terms. Full-text indexes are not deleted together with the graph space. Make sure that you start the Elasticsearch cluster and Nebula Graph at the same time. If not, the data writing on the Elasticsearch cluster can be incomplete. Do not contain ' or \\ in the vertex or edge values. If not, an error will be caused in the Elasticsearch cluster storage. It may take a while for Elasticsearch to create indexes. If Nebula Graph warns no index is found, wait for the index to take effect (however, the waiting time is unknown and there is no code to check). Nebula Graph clusters deployed with K8s do not support the full-text search feature.","title":"Full-text index restrictions"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/2.deploy-es/","text":"Deploy full-text index \u00b6 Nebula Graph full-text indexes are powered by Elasticsearch . This means that you can use Elasticsearch full-text query language to retrieve what you want. Full-text indexes are managed through built-in procedures. They can be created only for variable STRING and FIXED_STRING properties when the listener cluster and the Elasticsearch cluster are deployed. Precaution \u00b6 Before you start using the full-text index, please make sure that you know the restrictions . Deploy Elasticsearch cluster \u00b6 To deploy an Elasticsearch cluster, see Kubernetes Elasticsearch deployment or Elasticsearch installation . When the Elasticsearch cluster is started, add the template file for the Nebula Graph full-text index. For more information on index templates, see Elasticsearch Document . Take the following sample template for example: { \"template\" : \"nebula*\" , \"settings\" : { \"index\" : { \"number_of_shards\" : 3 , \"number_of_replicas\" : 1 } }, \"mappings\" : { \"properties\" : { \"tag_id\" : { \"type\" : \"long\" }, \"column_id\" : { \"type\" : \"text\" }, \"value\" :{ \"type\" : \"keyword\" } } } } Make sure that you specify the following fields in strict accordance with the preceding template format: \"template\" : \"nebula*\" \"tag_id\" : { \"type\" : \"long\" }, \"column_id\" : { \"type\" : \"text\" }, \"value\" :{ \"type\" : \"keyword\" } Caution When creating a full-text index, start the index name with nebula . For example: curl -H \"Content-Type: application/json; charset=utf-8\" -XPUT http://127.0.0.1:9200/_template/nebula_index_template -d ' { \"template\": \"nebula*\", \"settings\": { \"index\": { \"number_of_shards\": 3, \"number_of_replicas\": 1 } }, \"mappings\": { \"properties\" : { \"tag_id\" : { \"type\" : \"long\" }, \"column_id\" : { \"type\" : \"text\" }, \"value\" :{ \"type\" : \"keyword\"} } } }' You can configure the Elasticsearch to meet your business needs. To customize the Elasticsearch, see Elasticsearch Document . Sign in to the text search clients \u00b6 When the Elasticsearch cluster is deployed, use the SIGN IN statement to sign in to the Elasticsearch clients. Multiple elastic_ip:port pairs are separated with commas. You must use the IPs and the port number in the configuration file for the Elasticsearch. Syntax \u00b6 SIGN IN TEXT SERVICE (<elastic_ip:port>, {HTTP | HTTPS} [,\"<username>\", \"<password>\"]) [, (<elastic_ip:port>, ...)]; Example \u00b6 nebula> SIGN IN TEXT SERVICE (127.0.0.1:9200, HTTP); Note Elasticsearch does not have a username or password by default. If you configured a username and password, you need to specify them in the SIGN IN statement. Show text search clients \u00b6 The SHOW TEXT SEARCH CLIENTS statement can list the text search clients. Syntax \u00b6 SHOW TEXT SEARCH CLIENTS; Example \u00b6 nebula> SHOW TEXT SEARCH CLIENTS; +-------------+------+ | Host | Port | +-------------+------+ | \"127.0.0.1\" | 9200 | | \"127.0.0.1\" | 9200 | | \"127.0.0.1\" | 9200 | +-------------+------+ Sign out to the text search clients \u00b6 The SIGN OUT TEXT SERVICE statement can sign out all the text search clients. Syntax \u00b6 SIGN OUT TEXT SERVICE; Example \u00b6 nebula> SIGN OUT TEXT SERVICE;","title":"Deploy Elasticsearch cluster"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/2.deploy-es/#deploy_full-text_index","text":"Nebula Graph full-text indexes are powered by Elasticsearch . This means that you can use Elasticsearch full-text query language to retrieve what you want. Full-text indexes are managed through built-in procedures. They can be created only for variable STRING and FIXED_STRING properties when the listener cluster and the Elasticsearch cluster are deployed.","title":"Deploy full-text index"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/2.deploy-es/#precaution","text":"Before you start using the full-text index, please make sure that you know the restrictions .","title":"Precaution"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/2.deploy-es/#deploy_elasticsearch_cluster","text":"To deploy an Elasticsearch cluster, see Kubernetes Elasticsearch deployment or Elasticsearch installation . When the Elasticsearch cluster is started, add the template file for the Nebula Graph full-text index. For more information on index templates, see Elasticsearch Document . Take the following sample template for example: { \"template\" : \"nebula*\" , \"settings\" : { \"index\" : { \"number_of_shards\" : 3 , \"number_of_replicas\" : 1 } }, \"mappings\" : { \"properties\" : { \"tag_id\" : { \"type\" : \"long\" }, \"column_id\" : { \"type\" : \"text\" }, \"value\" :{ \"type\" : \"keyword\" } } } } Make sure that you specify the following fields in strict accordance with the preceding template format: \"template\" : \"nebula*\" \"tag_id\" : { \"type\" : \"long\" }, \"column_id\" : { \"type\" : \"text\" }, \"value\" :{ \"type\" : \"keyword\" } Caution When creating a full-text index, start the index name with nebula . For example: curl -H \"Content-Type: application/json; charset=utf-8\" -XPUT http://127.0.0.1:9200/_template/nebula_index_template -d ' { \"template\": \"nebula*\", \"settings\": { \"index\": { \"number_of_shards\": 3, \"number_of_replicas\": 1 } }, \"mappings\": { \"properties\" : { \"tag_id\" : { \"type\" : \"long\" }, \"column_id\" : { \"type\" : \"text\" }, \"value\" :{ \"type\" : \"keyword\"} } } }' You can configure the Elasticsearch to meet your business needs. To customize the Elasticsearch, see Elasticsearch Document .","title":"Deploy Elasticsearch cluster"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/2.deploy-es/#sign_in_to_the_text_search_clients","text":"When the Elasticsearch cluster is deployed, use the SIGN IN statement to sign in to the Elasticsearch clients. Multiple elastic_ip:port pairs are separated with commas. You must use the IPs and the port number in the configuration file for the Elasticsearch.","title":"Sign in to the text search clients"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/2.deploy-es/#syntax","text":"SIGN IN TEXT SERVICE (<elastic_ip:port>, {HTTP | HTTPS} [,\"<username>\", \"<password>\"]) [, (<elastic_ip:port>, ...)];","title":"Syntax"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/2.deploy-es/#example","text":"nebula> SIGN IN TEXT SERVICE (127.0.0.1:9200, HTTP); Note Elasticsearch does not have a username or password by default. If you configured a username and password, you need to specify them in the SIGN IN statement.","title":"Example"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/2.deploy-es/#show_text_search_clients","text":"The SHOW TEXT SEARCH CLIENTS statement can list the text search clients.","title":"Show text search clients"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/2.deploy-es/#syntax_1","text":"SHOW TEXT SEARCH CLIENTS;","title":"Syntax"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/2.deploy-es/#example_1","text":"nebula> SHOW TEXT SEARCH CLIENTS; +-------------+------+ | Host | Port | +-------------+------+ | \"127.0.0.1\" | 9200 | | \"127.0.0.1\" | 9200 | | \"127.0.0.1\" | 9200 | +-------------+------+","title":"Example"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/2.deploy-es/#sign_out_to_the_text_search_clients","text":"The SIGN OUT TEXT SERVICE statement can sign out all the text search clients.","title":"Sign out to the text search clients"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/2.deploy-es/#syntax_2","text":"SIGN OUT TEXT SERVICE;","title":"Syntax"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/2.deploy-es/#example_2","text":"nebula> SIGN OUT TEXT SERVICE;","title":"Example"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/","text":"Deploy Raft Listener for Nebula Storage service \u00b6 Full-text index data is written to the Elasticsearch cluster asynchronously. The Raft Listener (Listener for short) is a separate process that fetches data from the Storage Service and writes them into the Elasticsearch cluster. Prerequisites \u00b6 You have read and fully understood the restrictions for using full-text indexes. You have deployed a Nebula Graph cluster . You have deploy a Elasticsearch cluster . You have prepared at least one extra Storage Server. To use the full-text search, you must run one or more Storage Server as the Raft Listener. Precautions \u00b6 The Storage Service that you want to run as the Listener must have the same or later release with all the other Nebula Graph services in the cluster. For now, you can only add all Listeners to a graph space once and for all. Trying to add a new Listener to a graph space that already has a Listener will fail. To add all Listeners, set them in one statement . Deployment process \u00b6 Step 1: Install the Storage service \u00b6 The Listener process and the storaged process use the same binary file. However, their configuration files and using ports are different. You can install Nebula Graph on all servers that need to deploy a Listener, but only the Storage service can be used. For details, see Install Nebula Graph by RPM or DEB Package . Step 2: Prepare the configuration file for the Listener \u00b6 You have to prepare a corresponding configuration file on the machine that you want to deploy a Listener. The file must be named as nebula-storaged-listener.conf and stored in the etc directory. A template is provided for your reference. Note that the file suffix .production should be removed. Most configurations are the same as the configurations of Storage Service . This topic only introduces the differences. Name Default value Description daemonize true When set to true , the process is a daemon process. pid_file pids_listener/nebula-storaged.pid The file that records the process ID. meta_server_addrs - IP addresses and ports of all Meta services. Multiple Meta services are separated by commas. local_ip - The local IP address of the Listener service. port - The listening port of the RPC daemon of the Listener service. heartbeat_interval_secs 10 The heartbeat interval of the Meta service. The unit is second (s). listener_path data/listener The WAL directory of the Listener. Only one directory is allowed. data_path data For compatibility reasons, this parameter can be ignored. Fill in the default value data . part_man_type memory The type of the part manager. Optional values \u200b\u200bare memory and meta . rocksdb_batch_size 4096 The default reserved bytes for batch operations. rocksdb_block_cache 4 The default block cache size of BlockBasedTable. The unit is Megabyte (MB). engine_type rocksdb The type of the Storage engine, such as rocksdb , memory , etc. part_type simple The type of the part, such as simple , consensus , etc. Note Use real IP addresses in the configuration file instead of domain names or loopback IP addresses such as 127.0.0.1 . Step 3: Start Listeners \u00b6 Run the following command to start the Listener. ./bin/nebula-storaged --flagfile <listener_config_path>/nebula-storaged-listener.conf ${listener_config_path} is the path where you store the Listener configuration file. Step 4: Add Listeners to Nebula Graph \u00b6 Connect to Nebula Graph and run USE <space> to enter the graph space that you want to create full-text indexes for. Then run the following statement to add a Listener into Nebula Graph. ADD LISTENER ELASTICSEARCH <listener_ip:port> [,<listener_ip:port>, ...] Warning You must use real IPs for a Listener. Add all Listeners in one statement completely. nebula> ADD LISTENER ELASTICSEARCH 192.168.8.5:9789,192.168.8.6:9789; Show Listeners \u00b6 Run the SHOW LISTENER statement to list all Listeners. Example \u00b6 nebula> SHOW LISTENER; +--------+-----------------+-----------------------+----------+ | PartId | Type | Host | Status | +--------+-----------------+-----------------------+----------+ | 1 | \"ELASTICSEARCH\" | \"[192.168.8.5:46780]\" | \"ONLINE\" | | 2 | \"ELASTICSEARCH\" | \"[192.168.8.5:46780]\" | \"ONLINE\" | | 3 | \"ELASTICSEARCH\" | \"[192.168.8.5:46780]\" | \"ONLINE\" | +--------+-----------------+-----------------------+----------+ Remove Listeners \u00b6 Run the REMOVE LISTENER ELASTICSEARCH statement to remove all Listeners in a graph space. Example \u00b6 nebula> REMOVE LISTENER ELASTICSEARCH; Danger After the Listener is deleted, it cannot be added again. Therefore, the synchronization to the ES cluster cannot be continued and the text index data will be incomplete. If needed, you can only recreate the graph space. Next \u00b6 After deploying the Elasticsearch cluster and the Listener, full-text indexes are created automatically on the Elasticsearch cluster. Users can do full-text search now. For more information, see Full-Text search .","title":"Deploy Raft Listener cluster"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/#deploy_raft_listener_for_nebula_storage_service","text":"Full-text index data is written to the Elasticsearch cluster asynchronously. The Raft Listener (Listener for short) is a separate process that fetches data from the Storage Service and writes them into the Elasticsearch cluster.","title":"Deploy Raft Listener for Nebula Storage service"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/#prerequisites","text":"You have read and fully understood the restrictions for using full-text indexes. You have deployed a Nebula Graph cluster . You have deploy a Elasticsearch cluster . You have prepared at least one extra Storage Server. To use the full-text search, you must run one or more Storage Server as the Raft Listener.","title":"Prerequisites"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/#precautions","text":"The Storage Service that you want to run as the Listener must have the same or later release with all the other Nebula Graph services in the cluster. For now, you can only add all Listeners to a graph space once and for all. Trying to add a new Listener to a graph space that already has a Listener will fail. To add all Listeners, set them in one statement .","title":"Precautions"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/#deployment_process","text":"","title":"Deployment process"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/#step_1_install_the_storage_service","text":"The Listener process and the storaged process use the same binary file. However, their configuration files and using ports are different. You can install Nebula Graph on all servers that need to deploy a Listener, but only the Storage service can be used. For details, see Install Nebula Graph by RPM or DEB Package .","title":"Step 1: Install the Storage service"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/#step_2_prepare_the_configuration_file_for_the_listener","text":"You have to prepare a corresponding configuration file on the machine that you want to deploy a Listener. The file must be named as nebula-storaged-listener.conf and stored in the etc directory. A template is provided for your reference. Note that the file suffix .production should be removed. Most configurations are the same as the configurations of Storage Service . This topic only introduces the differences. Name Default value Description daemonize true When set to true , the process is a daemon process. pid_file pids_listener/nebula-storaged.pid The file that records the process ID. meta_server_addrs - IP addresses and ports of all Meta services. Multiple Meta services are separated by commas. local_ip - The local IP address of the Listener service. port - The listening port of the RPC daemon of the Listener service. heartbeat_interval_secs 10 The heartbeat interval of the Meta service. The unit is second (s). listener_path data/listener The WAL directory of the Listener. Only one directory is allowed. data_path data For compatibility reasons, this parameter can be ignored. Fill in the default value data . part_man_type memory The type of the part manager. Optional values \u200b\u200bare memory and meta . rocksdb_batch_size 4096 The default reserved bytes for batch operations. rocksdb_block_cache 4 The default block cache size of BlockBasedTable. The unit is Megabyte (MB). engine_type rocksdb The type of the Storage engine, such as rocksdb , memory , etc. part_type simple The type of the part, such as simple , consensus , etc. Note Use real IP addresses in the configuration file instead of domain names or loopback IP addresses such as 127.0.0.1 .","title":"Step 2: Prepare the configuration file for the Listener"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/#step_3_start_listeners","text":"Run the following command to start the Listener. ./bin/nebula-storaged --flagfile <listener_config_path>/nebula-storaged-listener.conf ${listener_config_path} is the path where you store the Listener configuration file.","title":"Step 3: Start Listeners"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/#step_4_add_listeners_to_nebula_graph","text":"Connect to Nebula Graph and run USE <space> to enter the graph space that you want to create full-text indexes for. Then run the following statement to add a Listener into Nebula Graph. ADD LISTENER ELASTICSEARCH <listener_ip:port> [,<listener_ip:port>, ...] Warning You must use real IPs for a Listener. Add all Listeners in one statement completely. nebula> ADD LISTENER ELASTICSEARCH 192.168.8.5:9789,192.168.8.6:9789;","title":"Step 4: Add Listeners to Nebula Graph"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/#show_listeners","text":"Run the SHOW LISTENER statement to list all Listeners.","title":"Show Listeners"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/#example","text":"nebula> SHOW LISTENER; +--------+-----------------+-----------------------+----------+ | PartId | Type | Host | Status | +--------+-----------------+-----------------------+----------+ | 1 | \"ELASTICSEARCH\" | \"[192.168.8.5:46780]\" | \"ONLINE\" | | 2 | \"ELASTICSEARCH\" | \"[192.168.8.5:46780]\" | \"ONLINE\" | | 3 | \"ELASTICSEARCH\" | \"[192.168.8.5:46780]\" | \"ONLINE\" | +--------+-----------------+-----------------------+----------+","title":"Example"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/#remove_listeners","text":"Run the REMOVE LISTENER ELASTICSEARCH statement to remove all Listeners in a graph space.","title":"Remove Listeners"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/#example_1","text":"nebula> REMOVE LISTENER ELASTICSEARCH; Danger After the Listener is deleted, it cannot be added again. Therefore, the synchronization to the ES cluster cannot be continued and the text index data will be incomplete. If needed, you can only recreate the graph space.","title":"Example"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/#next","text":"After deploying the Elasticsearch cluster and the Listener, full-text indexes are created automatically on the Elasticsearch cluster. Users can do full-text search now. For more information, see Full-Text search .","title":"Next"},{"location":"5.configurations-and-logs/1.configurations/1.configurations/","text":"Configurations \u00b6 Nebula Graph builds the configurations based on the gflags repository. Most configurations are flags. When the Nebula Graph service starts, it will get the configuration information from Configuration files by default. Configurations that are not in the file apply the default values. Note Because there are many configurations and they may change as Nebula Graph develops, this topic will not introduce all configurations. To get detailed descriptions of configurations, follow the instructions below. It is not recommended to modify the configurations that are not introduced in this topic, unless you are familiar with the source code and fully understand the function of configurations. Legacy version compatibility In the topic of 1.x, we provide a method of using the CONFIGS command to modify the configurations in the cache. However, using this method in a production environment can easily cause inconsistencies of configurations between clusters and the local. Therefore, this method will no longer be introduced in the topic of 2.x. Get the configuration list and descriptions \u00b6 Use the following command to get all the configuration information of the service corresponding to the binary file: <binary> --help For example: # Get the help information from Meta $ /usr/local/nebula/bin/nebula-metad --help # Get the help information from Graph $ /usr/local/nebula/bin/nebula-graphd --help # Get the help information from Storage $ /usr/local/nebula/bin/nebula-storaged --help The above examples use the default storage path /usr/local/nebula/bin/ . If you modify the installation path of Nebula Graph, use the actual path to query the configurations. Get configurations \u00b6 Use the curl command to get the value of the running configurations. Legacy version compatibility The curl commands and parameters in Nebula Graph v2.x. are different from Nebula Graph v1.x. For example: # Get the running configurations from Meta curl 127 .0.0.1:19559/flags # Get the running configurations from Graph curl 127 .0.0.1:19669/flags # Get the running configurations from Storage curl 127 .0.0.1:19779/flags Note In an actual environment, use the real host IP address instead of 127.0.0.1 in the above example. Configuration files \u00b6 Configuration files for clusters installed from source, with an RPM/DEB package, or a TAR package \u00b6 Nebula Graph provides two initial configuration files for each service, <service_name>.conf.default and <service_name>.conf.production . You can use them in different scenarios conveniently. For clusters installed from source and with a RPM/DEB package, the default path is /usr/local/nebula/etc/ . For clusters installed with a TAR package, the path is <install_path>/<tar_package_directory>/etc . The configuration values in the initial configuration file are for reference only and can be adjusted according to actual needs. To use the initial configuration file, choose one of the above two files and delete the suffix .default or .production to make it valid. Caution To ensure the availability of services, the configurations of the same service must be consistent, except for the local IP address local_ip . For example, three Storage servers are deployed in one Nebula Graph cluster. The configurations of the three Storage servers need to be the same, except for the IP address. The initial configuration files corresponding to each service are as follows. Nebula Graph service Initial configuration file Description Meta nebula-metad.conf.default and nebula-metad.conf.production Meta service configuration Graph nebula-graphd.conf.default and nebula-graphd.conf.production Graph service configuration Storage nebula-storaged.conf.default and nebula-storaged.conf.production Storage service configuration Each initial configuration file of all services contains local_config . The default value is true , which means that the Nebula Graph service will get configurations from its configuration files and start it. Caution It is not recommended to modify the value of local_config to false . If modified, the Nebula Graph service will first read the cached configurations, which may cause configuration inconsistencies between clusters and cause unknown risks. Configuration files for clusters installed with Docker Compose \u00b6 For clusters installed with Docker Compose, the configuration file's default installation path of the cluster is <install_path>/nebula-docker-compose/docker-compose.yaml . The parameters in the command field of the file are the launch parameters for each service. Configuration files for clusters installed with Nebula Operator \u00b6 For clusters installed with Kubectl through Nebula Operator, the configuration file's path is the path of the cluster YAML file. You can modify the configuration of each service through the spec.{graphd|storaged|metad}.config parameter. Note The services cannot be configured for clusters installed with Helm. Modify configurations \u00b6 By default, each Nebula Graph service gets configured from its configuration files. You can modify configurations and make them valid according to the following steps: For clusters installed from source, with a RPM/DEB, or a TAR package Use a text editor to modify the configuration files of the target service and save the modification. Choose an appropriate time to restart all Nebula Graph services to make the modifications valid. For clusters installed with Docker Compose In the <install_path>/nebula-docker-compose/docker-compose.yaml file, modify the configurations of the target service. In the nebula-docker-compose directory, run the command docker-compose up -d to restart the service involving configuration modifications. For clusters installed with Kubectl For details, see Customize configuration parameters for a Nebula Graph cluster . Use a text editor to modify the configuration files of the target service and save the modification. Choose an appropriate time to restart all Nebula Graph services to make the modifications valid.","title":"Configurations"},{"location":"5.configurations-and-logs/1.configurations/1.configurations/#configurations","text":"Nebula Graph builds the configurations based on the gflags repository. Most configurations are flags. When the Nebula Graph service starts, it will get the configuration information from Configuration files by default. Configurations that are not in the file apply the default values. Note Because there are many configurations and they may change as Nebula Graph develops, this topic will not introduce all configurations. To get detailed descriptions of configurations, follow the instructions below. It is not recommended to modify the configurations that are not introduced in this topic, unless you are familiar with the source code and fully understand the function of configurations. Legacy version compatibility In the topic of 1.x, we provide a method of using the CONFIGS command to modify the configurations in the cache. However, using this method in a production environment can easily cause inconsistencies of configurations between clusters and the local. Therefore, this method will no longer be introduced in the topic of 2.x.","title":"Configurations"},{"location":"5.configurations-and-logs/1.configurations/1.configurations/#get_the_configuration_list_and_descriptions","text":"Use the following command to get all the configuration information of the service corresponding to the binary file: <binary> --help For example: # Get the help information from Meta $ /usr/local/nebula/bin/nebula-metad --help # Get the help information from Graph $ /usr/local/nebula/bin/nebula-graphd --help # Get the help information from Storage $ /usr/local/nebula/bin/nebula-storaged --help The above examples use the default storage path /usr/local/nebula/bin/ . If you modify the installation path of Nebula Graph, use the actual path to query the configurations.","title":"Get the configuration list and descriptions"},{"location":"5.configurations-and-logs/1.configurations/1.configurations/#get_configurations","text":"Use the curl command to get the value of the running configurations. Legacy version compatibility The curl commands and parameters in Nebula Graph v2.x. are different from Nebula Graph v1.x. For example: # Get the running configurations from Meta curl 127 .0.0.1:19559/flags # Get the running configurations from Graph curl 127 .0.0.1:19669/flags # Get the running configurations from Storage curl 127 .0.0.1:19779/flags Note In an actual environment, use the real host IP address instead of 127.0.0.1 in the above example.","title":"Get configurations"},{"location":"5.configurations-and-logs/1.configurations/1.configurations/#configuration_files","text":"","title":"Configuration files"},{"location":"5.configurations-and-logs/1.configurations/1.configurations/#configuration_files_for_clusters_installed_from_source_with_an_rpmdeb_package_or_a_tar_package","text":"Nebula Graph provides two initial configuration files for each service, <service_name>.conf.default and <service_name>.conf.production . You can use them in different scenarios conveniently. For clusters installed from source and with a RPM/DEB package, the default path is /usr/local/nebula/etc/ . For clusters installed with a TAR package, the path is <install_path>/<tar_package_directory>/etc . The configuration values in the initial configuration file are for reference only and can be adjusted according to actual needs. To use the initial configuration file, choose one of the above two files and delete the suffix .default or .production to make it valid. Caution To ensure the availability of services, the configurations of the same service must be consistent, except for the local IP address local_ip . For example, three Storage servers are deployed in one Nebula Graph cluster. The configurations of the three Storage servers need to be the same, except for the IP address. The initial configuration files corresponding to each service are as follows. Nebula Graph service Initial configuration file Description Meta nebula-metad.conf.default and nebula-metad.conf.production Meta service configuration Graph nebula-graphd.conf.default and nebula-graphd.conf.production Graph service configuration Storage nebula-storaged.conf.default and nebula-storaged.conf.production Storage service configuration Each initial configuration file of all services contains local_config . The default value is true , which means that the Nebula Graph service will get configurations from its configuration files and start it. Caution It is not recommended to modify the value of local_config to false . If modified, the Nebula Graph service will first read the cached configurations, which may cause configuration inconsistencies between clusters and cause unknown risks.","title":"Configuration files for clusters installed from source, with an RPM/DEB package, or a TAR package"},{"location":"5.configurations-and-logs/1.configurations/1.configurations/#configuration_files_for_clusters_installed_with_docker_compose","text":"For clusters installed with Docker Compose, the configuration file's default installation path of the cluster is <install_path>/nebula-docker-compose/docker-compose.yaml . The parameters in the command field of the file are the launch parameters for each service.","title":"Configuration files for clusters installed with Docker Compose"},{"location":"5.configurations-and-logs/1.configurations/1.configurations/#configuration_files_for_clusters_installed_with_nebula_operator","text":"For clusters installed with Kubectl through Nebula Operator, the configuration file's path is the path of the cluster YAML file. You can modify the configuration of each service through the spec.{graphd|storaged|metad}.config parameter. Note The services cannot be configured for clusters installed with Helm.","title":"Configuration files for clusters installed with Nebula Operator"},{"location":"5.configurations-and-logs/1.configurations/1.configurations/#modify_configurations","text":"By default, each Nebula Graph service gets configured from its configuration files. You can modify configurations and make them valid according to the following steps: For clusters installed from source, with a RPM/DEB, or a TAR package Use a text editor to modify the configuration files of the target service and save the modification. Choose an appropriate time to restart all Nebula Graph services to make the modifications valid. For clusters installed with Docker Compose In the <install_path>/nebula-docker-compose/docker-compose.yaml file, modify the configurations of the target service. In the nebula-docker-compose directory, run the command docker-compose up -d to restart the service involving configuration modifications. For clusters installed with Kubectl For details, see Customize configuration parameters for a Nebula Graph cluster . Use a text editor to modify the configuration files of the target service and save the modification. Choose an appropriate time to restart all Nebula Graph services to make the modifications valid.","title":"Modify configurations"},{"location":"5.configurations-and-logs/1.configurations/2.meta-config/","text":"Meta Service configuration \u00b6 Nebula Graph provides two initial configuration files for the Meta Service, nebula-metad.conf.default and nebula-metad.conf.production . Users can use them in different scenarios conveniently. The default file path is /usr/local/nebula/etc/ . Caution It is not recommended to modify the value of local_config to false . If modified, the Nebula Graph service will first read the cached configurations, which may cause configuration inconsistencies between clusters and cause unknown risks. It is not recommended to modify the configurations that are not introduced in this topic, unless you are familiar with the source code and fully understand the function of configurations. How to use the configuration files \u00b6 To use the initial configuration file, choose one of the above two files and delete the suffix .default or .production from the initial configuration file for the Meta Service to apply the configurations defined in it. About parameter values \u00b6 If a parameter is not set in the configuration file, Nebula Graph uses the default value. Not all parameters are predefined. And the predefined parameters in the two initial configuration files are different. This topic uses the parameters in nebula-metad.conf.default . For all parameters and their current values, see Configurations . Basics configurations \u00b6 Name Predefined value Description daemonize true When set to true , the process is a daemon process. pid_file pids/nebula-metad.pid The file that records the process ID. timezone_name - Specifies the Nebula Graph time zone. This parameter is not predefined in the initial configuration files. You can manually set it if you need it. The system default value is UTC+00:00:00 . For the format of the parameter value, see Specifying the Time Zone with TZ . For example, --timezone_name=UTC+08:00 represents the GMT+8 time zone. license_path share/resources/nebula.license Path of the license of the Nebula Graph Enterprise Edition. Users need to deploy a license file before starting the Enterprise Edition. This parameter is required only for the Nebula Graph Enterprise Edition. For details about how to configure licenses for other ecosystem tools, see the deployment documents of the corresponding ecosystem tools. Note While inserting property values of time types , Nebula Graph transforms time types (except TIMESTAMP) to the corresponding UTC according to the time zone specified by timezone_name . The time-type values returned by nGQL queries are all UTC time. timezone_name is only used to transform the data stored in Nebula Graph. Other time-related data of the Nebula Graph processes still uses the default time zone of the host, such as the log printing time. Logging configurations \u00b6 Name Predefined value Description log_dir logs The directory that stores the Meta Service log. It is recommended to put logs on a different hard disk from the data. minloglevel 0 Specifies the minimum level of the log. That is, no logs below this level will be printed. Optional values are 0 (INFO), 1 (WARNING), 2 (ERROR), 3 (FATAL). It is recommended to set it to 0 during debugging and 1 in a production environment. If it is set to 4 , Nebula Graph will not print any logs. v 0 Specifies the detailed level of the log. The larger the value, the more detailed the log is. Optional values are 0 , 1 , 2 , 3 . logbufsecs 0 Specifies the maximum time to buffer the logs. If there is a timeout, it will output the buffered log to the log file. 0 means real-time output. This configuration is measured in seconds. redirect_stdout true When set to true , the process redirects the stdout and stderr to separate output files. stdout_log_file metad-stdout.log Specifies the filename for the stdout log. stderr_log_file metad-stderr.log Specifies the filename for the stderr log. stderrthreshold 2 Specifies the minloglevel to be copied to the stderr log. timestamp_in_logfile_name true Specifies if the log file name contains a timestamp. true indicates yes, false indicates no. Networking configurations \u00b6 Name Predefined value Description meta_server_addrs 127.0.0.1:9559 Specifies the IP addresses and ports of all Meta Services. Multiple addresses are separated with commas. local_ip 127.0.0.1 Specifies the local IP for the Meta Service. The local IP address is used to identify the nebula-metad process. If it is a distributed cluster or requires remote access, modify it to the corresponding address. port 9559 Specifies RPC daemon listening port of the Meta service. The external port for the Meta Service is predefined to 9559 . The internal port is predefined to port + 1 , i.e., 9560 . Nebula Graph uses the internal port for multi-replica interactions. ws_ip 0.0.0.0 Specifies the IP address for the HTTP service. ws_http_port 19559 Specifies the port for the HTTP service. ws_storage_http_port 19779 Specifies the Storage service listening port used by the HTTP protocol. It must be consistent with the ws_http_port in the Storage service configuration file. heartbeat_interval_secs 10 Specifies the default heartbeat interval. Make sure the heartbeat_interval_secs values for all services are the same, otherwise Nebula Graph CANNOT work normally. This configuration is measured in seconds. Caution The real IP address must be used in the configuration file. Otherwise, 127.0.0.1/0.0.0.0 cannot be parsed correctly in some cases. Storage configurations \u00b6 Name Predefined Value Description data_path data/meta The storage path for Meta data. Misc configurations \u00b6 Name Predefined Value Description default_parts_num 100 Specifies the default partition number when creating a new graph space. default_replica_factor 1 Specifies the default replica number when creating a new graph space. RocksDB options configurations \u00b6 Name Predefined Value Description rocksdb_wal_sync true Enables or disables RocksDB WAL synchronization. Available values are true (enable) and false (disable).","title":"Meta Service configurations"},{"location":"5.configurations-and-logs/1.configurations/2.meta-config/#meta_service_configuration","text":"Nebula Graph provides two initial configuration files for the Meta Service, nebula-metad.conf.default and nebula-metad.conf.production . Users can use them in different scenarios conveniently. The default file path is /usr/local/nebula/etc/ . Caution It is not recommended to modify the value of local_config to false . If modified, the Nebula Graph service will first read the cached configurations, which may cause configuration inconsistencies between clusters and cause unknown risks. It is not recommended to modify the configurations that are not introduced in this topic, unless you are familiar with the source code and fully understand the function of configurations.","title":"Meta Service configuration"},{"location":"5.configurations-and-logs/1.configurations/2.meta-config/#how_to_use_the_configuration_files","text":"To use the initial configuration file, choose one of the above two files and delete the suffix .default or .production from the initial configuration file for the Meta Service to apply the configurations defined in it.","title":"How to use the configuration files"},{"location":"5.configurations-and-logs/1.configurations/2.meta-config/#about_parameter_values","text":"If a parameter is not set in the configuration file, Nebula Graph uses the default value. Not all parameters are predefined. And the predefined parameters in the two initial configuration files are different. This topic uses the parameters in nebula-metad.conf.default . For all parameters and their current values, see Configurations .","title":"About parameter values"},{"location":"5.configurations-and-logs/1.configurations/2.meta-config/#basics_configurations","text":"Name Predefined value Description daemonize true When set to true , the process is a daemon process. pid_file pids/nebula-metad.pid The file that records the process ID. timezone_name - Specifies the Nebula Graph time zone. This parameter is not predefined in the initial configuration files. You can manually set it if you need it. The system default value is UTC+00:00:00 . For the format of the parameter value, see Specifying the Time Zone with TZ . For example, --timezone_name=UTC+08:00 represents the GMT+8 time zone. license_path share/resources/nebula.license Path of the license of the Nebula Graph Enterprise Edition. Users need to deploy a license file before starting the Enterprise Edition. This parameter is required only for the Nebula Graph Enterprise Edition. For details about how to configure licenses for other ecosystem tools, see the deployment documents of the corresponding ecosystem tools. Note While inserting property values of time types , Nebula Graph transforms time types (except TIMESTAMP) to the corresponding UTC according to the time zone specified by timezone_name . The time-type values returned by nGQL queries are all UTC time. timezone_name is only used to transform the data stored in Nebula Graph. Other time-related data of the Nebula Graph processes still uses the default time zone of the host, such as the log printing time.","title":"Basics configurations"},{"location":"5.configurations-and-logs/1.configurations/2.meta-config/#logging_configurations","text":"Name Predefined value Description log_dir logs The directory that stores the Meta Service log. It is recommended to put logs on a different hard disk from the data. minloglevel 0 Specifies the minimum level of the log. That is, no logs below this level will be printed. Optional values are 0 (INFO), 1 (WARNING), 2 (ERROR), 3 (FATAL). It is recommended to set it to 0 during debugging and 1 in a production environment. If it is set to 4 , Nebula Graph will not print any logs. v 0 Specifies the detailed level of the log. The larger the value, the more detailed the log is. Optional values are 0 , 1 , 2 , 3 . logbufsecs 0 Specifies the maximum time to buffer the logs. If there is a timeout, it will output the buffered log to the log file. 0 means real-time output. This configuration is measured in seconds. redirect_stdout true When set to true , the process redirects the stdout and stderr to separate output files. stdout_log_file metad-stdout.log Specifies the filename for the stdout log. stderr_log_file metad-stderr.log Specifies the filename for the stderr log. stderrthreshold 2 Specifies the minloglevel to be copied to the stderr log. timestamp_in_logfile_name true Specifies if the log file name contains a timestamp. true indicates yes, false indicates no.","title":"Logging configurations"},{"location":"5.configurations-and-logs/1.configurations/2.meta-config/#networking_configurations","text":"Name Predefined value Description meta_server_addrs 127.0.0.1:9559 Specifies the IP addresses and ports of all Meta Services. Multiple addresses are separated with commas. local_ip 127.0.0.1 Specifies the local IP for the Meta Service. The local IP address is used to identify the nebula-metad process. If it is a distributed cluster or requires remote access, modify it to the corresponding address. port 9559 Specifies RPC daemon listening port of the Meta service. The external port for the Meta Service is predefined to 9559 . The internal port is predefined to port + 1 , i.e., 9560 . Nebula Graph uses the internal port for multi-replica interactions. ws_ip 0.0.0.0 Specifies the IP address for the HTTP service. ws_http_port 19559 Specifies the port for the HTTP service. ws_storage_http_port 19779 Specifies the Storage service listening port used by the HTTP protocol. It must be consistent with the ws_http_port in the Storage service configuration file. heartbeat_interval_secs 10 Specifies the default heartbeat interval. Make sure the heartbeat_interval_secs values for all services are the same, otherwise Nebula Graph CANNOT work normally. This configuration is measured in seconds. Caution The real IP address must be used in the configuration file. Otherwise, 127.0.0.1/0.0.0.0 cannot be parsed correctly in some cases.","title":"Networking configurations"},{"location":"5.configurations-and-logs/1.configurations/2.meta-config/#storage_configurations","text":"Name Predefined Value Description data_path data/meta The storage path for Meta data.","title":"Storage configurations"},{"location":"5.configurations-and-logs/1.configurations/2.meta-config/#misc_configurations","text":"Name Predefined Value Description default_parts_num 100 Specifies the default partition number when creating a new graph space. default_replica_factor 1 Specifies the default replica number when creating a new graph space.","title":"Misc configurations"},{"location":"5.configurations-and-logs/1.configurations/2.meta-config/#rocksdb_options_configurations","text":"Name Predefined Value Description rocksdb_wal_sync true Enables or disables RocksDB WAL synchronization. Available values are true (enable) and false (disable).","title":"RocksDB options configurations"},{"location":"5.configurations-and-logs/1.configurations/3.graph-config/","text":"Graph Service configuration \u00b6 Nebula Graph provides two initial configuration files for the Graph Service, nebula-graphd.conf.default and nebula-graphd.conf.production . Users can use them in different scenarios conveniently. The default file path is /usr/local/nebula/etc/ . Caution It is not recommended to modify the value of local_config to false . If modified, the Nebula Graph service will first read the cached configurations, which may cause configuration inconsistencies between clusters and cause unknown risks. It is not recommended to modify the configurations that are not introduced in this topic, unless you are familiar with the source code and fully understand the function of configurations. How to use the configuration files \u00b6 To use the initial configuration file, choose one of the above two files and delete the suffix .default or .production from the initial configuration file for the Meta Service to apply the configurations defined in it. About parameter values \u00b6 If a parameter is not set in the configuration file, Nebula Graph uses the default value. Not all parameters are predefined. And the predefined parameters in the two initial configuration files are different. This topic uses the parameters in nebula-metad.conf.default . For all parameters and their current values, see Configurations . Basics configurations \u00b6 Name Predefined value Description daemonize true When set to true , the process is a daemon process. pid_file pids/nebula-graphd.pid The file that records the process ID. enable_optimizer true When set to true , the optimizer is enabled. timezone_name - Specifies the Nebula Graph time zone. This parameter is not predefined in the initial configuration files. The system default value is UTC+00:00:00 . For the format of the parameter value, see Specifying the Time Zone with TZ . For example\uff0c --timezone_name=UTC+08:00 represents the GMT+8 time zone. local_config true When set to true , the process gets configurations from the configuration files. Note While inserting property values of time types , Nebula Graph transforms time types (except TIMESTAMP) to the corresponding UTC according to the time zone specified by timezone_name . The time-type values returned by nGQL queries are all UTC time. timezone_name is only used to transform the data stored in Nebula Graph. Other time-related data of the Nebula Graph processes still uses the default time zone of the host, such as the log printing time. Logging configurations \u00b6 Name Predefined value Description log_dir logs The directory that stores the Meta Service log. It is recommended to put logs on a different hard disk from the data. minloglevel 0 Specifies the minimum level of the log. That is, no logs below this level will be printed. Optional values are 0 (INFO), 1 (WARNING), 2 (ERROR), 3 (FATAL). It is recommended to set it to 0 during debugging and 1 in a production environment. If it is set to 4 , Nebula Graph will not print any logs. v 0 Specifies the detailed level of the log. The larger the value, the more detailed the log is. Optional values are 0 , 1 , 2 , 3 . logbufsecs 0 Specifies the maximum time to buffer the logs. If there is a timeout, it will output the buffered log to the log file. 0 means real-time output. This configuration is measured in seconds. redirect_stdout true When set to true , the process redirects the stdout and stderr to separate output files. stdout_log_file graphd-stdout.log Specifies the filename for the stdout log. stderr_log_file graphd-stderr.log Specifies the filename for the stderr log. stderrthreshold 2 Specifies the minloglevel to be copied to the stderr log. timestamp_in_logfile_name true Specifies if the log file name contains a timestamp. true indicates yes, false indicates no. Query configurations \u00b6 Name Predefined value Description accept_partial_success false When set to false , the process treats partial success as an error. This configuration only applies to read-only requests. Write requests always treat partial success as an error. session_reclaim_interval_secs 10 Specifies the interval that the Session information is sent to the Meta service. This configuration is measured in seconds. max_allowed_query_size 4194304 Specifies the maximum length of queries. Unit: bytes. The default value is 4194304 , namely 4MB. Networking configurations \u00b6 Name Predefined value Description meta_server_addrs 127.0.0.1:9559 Specifies the IP addresses and ports of all Meta Services. Multiple addresses are separated with commas. local_ip 127.0.0.1 Specifies the local IP for the Graph Service. The local IP address is used to identify the nebula-graphd process. If it is a distributed cluster or requires remote access, modify it to the corresponding address. listen_netdev any Specifies the listening network device. port 9669 Specifies RPC daemon listening port of the Graph service. reuse_port false When set to false , the SO_REUSEPORT is closed. listen_backlog 1024 Specifies the maximum length of the connection queue for socket monitoring. This configuration must be modified together with the net.core.somaxconn . client_idle_timeout_secs 28800 Specifies the time to expire an idle connection. The default is 8 hours. 0 means that the connection will never expire. This configuration is measured in seconds. session_idle_timeout_secs 28800 Specifies the time to expire an idle session. The value ranges from 1 to 604800. The default is 8 hours. This configuration is measured in seconds. num_accept_threads 1 Specifies the number of threads that accept incoming connections. num_netio_threads 0 Specifies the number of networking IO threads. 0 is the number of CPU cores. num_worker_threads 0 Specifies the number of threads that execute queries. 0 is the number of CPU cores. ws_ip 0.0.0.0 Specifies the IP address for the HTTP service. ws_http_port 19669 Specifies the port for the HTTP service. heartbeat_interval_secs 10 Specifies the default heartbeat interval. Make sure the heartbeat_interval_secs values for all services are the same, otherwise Nebula Graph CANNOT work normally. This configuration is measured in seconds. storage_client_timeout_ms - Specifies the RPC connection timeout threshold between the Graph Service and the Storage Service. This parameter is not predefined in the initial configuration files. You can manually set it if you need it. The system default value is 60000 ms. ws_meta_http_port 19559 Specifies the Meta service listening port used by the HTTP protocol. It must be consistent with the ws_http_port in the Meta service configuration file. Caution The real IP address must be used in the configuration file. Otherwise, 127.0.0.1/0.0.0.0 cannot be parsed correctly in some cases. Charset and collate configurations \u00b6 Name Predefined value Description default_charset utf8 Specifies the default charset when creating a new graph space. default_collate utf8_bin Specifies the default collate when creating a new graph space. Authorization configurations \u00b6 Name Predefined value Description enable_authorize false When set to false , the system authentication is not enabled. For more information, see Authentication . auth_type password Specifies the login method. Available values are password , ldap , and cloud . Memory configurations \u00b6 Name Predefined value Description system_memory_high_watermark_ratio 0.8 Specifies the trigger threshold of the high-level memory alarm mechanism. If the system memory usage is higher than this value, an alarm mechanism will be triggered, and Nebula Graph will stop querying. This parameter is not predefined in the initial configuration files. Audit configurations \u00b6 The audit log is only available in the Enterprise Edition. For details, see Audit log . Metrics configurations \u00b6 Name Predefined value Description enable_space_level_metrics false Enable or disable space-level metrics. Such metric names contain the name of the graph space that it monitors, for example, query_latency_us{space=basketballplayer}.avg.3600 . You can view the supported metrics with the curl command. For more information, see Query Nebula Graph metrics . session configurations \u00b6 Name Predefined value Description max_sessions_per_ip_per_user 300 The maximum number of sessions that can be created with the same user and IP address. Experimental configurations \u00b6 Name Predefined value Description enable_experimental_feature false Specifies the experimental feature. Optional values are true and false . For currently supported experimental features, see below. Experimental features \u00b6 Name Description TOSS The TOSS (Transaction on Storage Side) function is used to ensure the final consistency of the INSERT , UPDATE , UPSERT , or DELETE operations on edges (because one edge logically corresponds to two key-value pairs on the hard disk). After the TOSS function is enabled, the time delay of related operations will be increased by about one time.","title":"Graph Service configurations"},{"location":"5.configurations-and-logs/1.configurations/3.graph-config/#graph_service_configuration","text":"Nebula Graph provides two initial configuration files for the Graph Service, nebula-graphd.conf.default and nebula-graphd.conf.production . Users can use them in different scenarios conveniently. The default file path is /usr/local/nebula/etc/ . Caution It is not recommended to modify the value of local_config to false . If modified, the Nebula Graph service will first read the cached configurations, which may cause configuration inconsistencies between clusters and cause unknown risks. It is not recommended to modify the configurations that are not introduced in this topic, unless you are familiar with the source code and fully understand the function of configurations.","title":"Graph Service configuration"},{"location":"5.configurations-and-logs/1.configurations/3.graph-config/#how_to_use_the_configuration_files","text":"To use the initial configuration file, choose one of the above two files and delete the suffix .default or .production from the initial configuration file for the Meta Service to apply the configurations defined in it.","title":"How to use the configuration files"},{"location":"5.configurations-and-logs/1.configurations/3.graph-config/#about_parameter_values","text":"If a parameter is not set in the configuration file, Nebula Graph uses the default value. Not all parameters are predefined. And the predefined parameters in the two initial configuration files are different. This topic uses the parameters in nebula-metad.conf.default . For all parameters and their current values, see Configurations .","title":"About parameter values"},{"location":"5.configurations-and-logs/1.configurations/3.graph-config/#basics_configurations","text":"Name Predefined value Description daemonize true When set to true , the process is a daemon process. pid_file pids/nebula-graphd.pid The file that records the process ID. enable_optimizer true When set to true , the optimizer is enabled. timezone_name - Specifies the Nebula Graph time zone. This parameter is not predefined in the initial configuration files. The system default value is UTC+00:00:00 . For the format of the parameter value, see Specifying the Time Zone with TZ . For example\uff0c --timezone_name=UTC+08:00 represents the GMT+8 time zone. local_config true When set to true , the process gets configurations from the configuration files. Note While inserting property values of time types , Nebula Graph transforms time types (except TIMESTAMP) to the corresponding UTC according to the time zone specified by timezone_name . The time-type values returned by nGQL queries are all UTC time. timezone_name is only used to transform the data stored in Nebula Graph. Other time-related data of the Nebula Graph processes still uses the default time zone of the host, such as the log printing time.","title":"Basics configurations"},{"location":"5.configurations-and-logs/1.configurations/3.graph-config/#logging_configurations","text":"Name Predefined value Description log_dir logs The directory that stores the Meta Service log. It is recommended to put logs on a different hard disk from the data. minloglevel 0 Specifies the minimum level of the log. That is, no logs below this level will be printed. Optional values are 0 (INFO), 1 (WARNING), 2 (ERROR), 3 (FATAL). It is recommended to set it to 0 during debugging and 1 in a production environment. If it is set to 4 , Nebula Graph will not print any logs. v 0 Specifies the detailed level of the log. The larger the value, the more detailed the log is. Optional values are 0 , 1 , 2 , 3 . logbufsecs 0 Specifies the maximum time to buffer the logs. If there is a timeout, it will output the buffered log to the log file. 0 means real-time output. This configuration is measured in seconds. redirect_stdout true When set to true , the process redirects the stdout and stderr to separate output files. stdout_log_file graphd-stdout.log Specifies the filename for the stdout log. stderr_log_file graphd-stderr.log Specifies the filename for the stderr log. stderrthreshold 2 Specifies the minloglevel to be copied to the stderr log. timestamp_in_logfile_name true Specifies if the log file name contains a timestamp. true indicates yes, false indicates no.","title":"Logging configurations"},{"location":"5.configurations-and-logs/1.configurations/3.graph-config/#query_configurations","text":"Name Predefined value Description accept_partial_success false When set to false , the process treats partial success as an error. This configuration only applies to read-only requests. Write requests always treat partial success as an error. session_reclaim_interval_secs 10 Specifies the interval that the Session information is sent to the Meta service. This configuration is measured in seconds. max_allowed_query_size 4194304 Specifies the maximum length of queries. Unit: bytes. The default value is 4194304 , namely 4MB.","title":"Query configurations"},{"location":"5.configurations-and-logs/1.configurations/3.graph-config/#networking_configurations","text":"Name Predefined value Description meta_server_addrs 127.0.0.1:9559 Specifies the IP addresses and ports of all Meta Services. Multiple addresses are separated with commas. local_ip 127.0.0.1 Specifies the local IP for the Graph Service. The local IP address is used to identify the nebula-graphd process. If it is a distributed cluster or requires remote access, modify it to the corresponding address. listen_netdev any Specifies the listening network device. port 9669 Specifies RPC daemon listening port of the Graph service. reuse_port false When set to false , the SO_REUSEPORT is closed. listen_backlog 1024 Specifies the maximum length of the connection queue for socket monitoring. This configuration must be modified together with the net.core.somaxconn . client_idle_timeout_secs 28800 Specifies the time to expire an idle connection. The default is 8 hours. 0 means that the connection will never expire. This configuration is measured in seconds. session_idle_timeout_secs 28800 Specifies the time to expire an idle session. The value ranges from 1 to 604800. The default is 8 hours. This configuration is measured in seconds. num_accept_threads 1 Specifies the number of threads that accept incoming connections. num_netio_threads 0 Specifies the number of networking IO threads. 0 is the number of CPU cores. num_worker_threads 0 Specifies the number of threads that execute queries. 0 is the number of CPU cores. ws_ip 0.0.0.0 Specifies the IP address for the HTTP service. ws_http_port 19669 Specifies the port for the HTTP service. heartbeat_interval_secs 10 Specifies the default heartbeat interval. Make sure the heartbeat_interval_secs values for all services are the same, otherwise Nebula Graph CANNOT work normally. This configuration is measured in seconds. storage_client_timeout_ms - Specifies the RPC connection timeout threshold between the Graph Service and the Storage Service. This parameter is not predefined in the initial configuration files. You can manually set it if you need it. The system default value is 60000 ms. ws_meta_http_port 19559 Specifies the Meta service listening port used by the HTTP protocol. It must be consistent with the ws_http_port in the Meta service configuration file. Caution The real IP address must be used in the configuration file. Otherwise, 127.0.0.1/0.0.0.0 cannot be parsed correctly in some cases.","title":"Networking configurations"},{"location":"5.configurations-and-logs/1.configurations/3.graph-config/#charset_and_collate_configurations","text":"Name Predefined value Description default_charset utf8 Specifies the default charset when creating a new graph space. default_collate utf8_bin Specifies the default collate when creating a new graph space.","title":"Charset and collate configurations"},{"location":"5.configurations-and-logs/1.configurations/3.graph-config/#authorization_configurations","text":"Name Predefined value Description enable_authorize false When set to false , the system authentication is not enabled. For more information, see Authentication . auth_type password Specifies the login method. Available values are password , ldap , and cloud .","title":"Authorization configurations"},{"location":"5.configurations-and-logs/1.configurations/3.graph-config/#memory_configurations","text":"Name Predefined value Description system_memory_high_watermark_ratio 0.8 Specifies the trigger threshold of the high-level memory alarm mechanism. If the system memory usage is higher than this value, an alarm mechanism will be triggered, and Nebula Graph will stop querying. This parameter is not predefined in the initial configuration files.","title":"Memory configurations"},{"location":"5.configurations-and-logs/1.configurations/3.graph-config/#audit_configurations","text":"The audit log is only available in the Enterprise Edition. For details, see Audit log .","title":"Audit configurations"},{"location":"5.configurations-and-logs/1.configurations/3.graph-config/#metrics_configurations","text":"Name Predefined value Description enable_space_level_metrics false Enable or disable space-level metrics. Such metric names contain the name of the graph space that it monitors, for example, query_latency_us{space=basketballplayer}.avg.3600 . You can view the supported metrics with the curl command. For more information, see Query Nebula Graph metrics .","title":"Metrics configurations"},{"location":"5.configurations-and-logs/1.configurations/3.graph-config/#session_configurations","text":"Name Predefined value Description max_sessions_per_ip_per_user 300 The maximum number of sessions that can be created with the same user and IP address.","title":"session configurations"},{"location":"5.configurations-and-logs/1.configurations/3.graph-config/#experimental_configurations","text":"Name Predefined value Description enable_experimental_feature false Specifies the experimental feature. Optional values are true and false . For currently supported experimental features, see below.","title":"Experimental configurations"},{"location":"5.configurations-and-logs/1.configurations/3.graph-config/#experimental_features","text":"Name Description TOSS The TOSS (Transaction on Storage Side) function is used to ensure the final consistency of the INSERT , UPDATE , UPSERT , or DELETE operations on edges (because one edge logically corresponds to two key-value pairs on the hard disk). After the TOSS function is enabled, the time delay of related operations will be increased by about one time.","title":"Experimental features"},{"location":"5.configurations-and-logs/1.configurations/4.storage-config/","text":"Storage Service configurations \u00b6 Nebula Graph provides two initial configuration files for the Storage Service, nebula-storaged.conf.default and nebula-storaged.conf.production . Users can use them in different scenarios conveniently. The default file path is /usr/local/nebula/etc/ . Caution It is not recommended to modify the value of local_config to false . If modified, the Nebula Graph service will first read the cached configurations, which may cause configuration inconsistencies between clusters and cause unknown risks. It is not recommended to modify the configurations that are not introduced in this topic, unless you are familiar with the source code and fully understand the function of configurations. How to use the configuration files \u00b6 To use the initial configuration file, choose one of the above two files and delete the suffix .default or .production from the initial configuration file for the Meta Service to apply the configurations defined in it. About parameter values \u00b6 If a parameter is not set in the configuration file, Nebula Graph uses the default value. Not all parameters are predefined. And the predefined parameters in the two initial configuration files are different. This topic uses the parameters in nebula-metad.conf.default . For parameters that are not included in nebula-metad.conf.default , see nebula-storaged.conf.production . Note The configurations of the Raft Listener and the Storage service are different. For details, see Deploy Raft listener . For all parameters and their current values, see Configurations . Basics configurations \u00b6 Name Predefined value Description daemonize true When set to true , the process is a daemon process. pid_file pids/nebula-storaged.pid The file that records the process ID. timezone_name - Specifies the Nebula Graph time zone. This parameter is not predefined in the initial configuration files. The system default value is UTC+00:00:00 . For the format of the parameter value, see Specifying the Time Zone with TZ . For example, --timezone_name=UTC+08:00 represents the GMT+8 time zone. local_config true When set to true , the process gets configurations from the configuration files. Note While inserting property values of time types , Nebula Graph transforms time types (except TIMESTAMP) to the corresponding UTC according to the time zone specified by timezone_name . The time-type values returned by nGQL queries are all UTC. timezone_name is only used to transform the data stored in Nebula Graph. Other time-related data of the Nebula Graph processes still uses the default time zone of the host, such as the log printing time. Logging configurations \u00b6 Name Predefined value Description log_dir logs The directory that stores the Meta Service log. It is recommended to put logs on a different hard disk from the data. minloglevel 0 Specifies the minimum level of the log. That is, no logs below this level will be printed. Optional values are 0 (INFO), 1 (WARNING), 2 (ERROR), 3 (FATAL). It is recommended to set it to 0 during debugging and 1 in a production environment. If it is set to 4 , Nebula Graph will not print any logs. v 0 Specifies the detailed level of the log. The larger the value, the more detailed the log is. Optional values are 0 , 1 , 2 , 3 . logbufsecs 0 Specifies the maximum time to buffer the logs. If there is a timeout, it will output the buffered log to the log file. 0 means real-time output. This configuration is measured in seconds. redirect_stdout true When set to true , the process redirects the stdout and stderr to separate output files. stdout_log_file graphd-stdout.log Specifies the filename for the stdout log. stderr_log_file graphd-stderr.log Specifies the filename for the stderr log. stderrthreshold 2 Specifies the minloglevel to be copied to the stderr log. timestamp_in_logfile_name true Specifies if the log file name contains a timestamp. true indicates yes, false indicates no. Networking configurations \u00b6 Name Predefined value Description meta_server_addrs 127.0.0.1:9559 Specifies the IP addresses and ports of all Meta Services. Multiple addresses are separated with commas. local_ip 127.0.0.1 Specifies the local IP for the Storage Service. The local IP address is used to identify the nebula-storaged process. If it is a distributed cluster or requires remote access, modify it to the corresponding address. port 9779 Specifies RPC daemon listening port of the Storage service. The external port for the Meta Service is predefined to 9779 . The internal port is predefined to 9777 , 9778 , and 9780 . Nebula Graph uses the internal port for multi-replica interactions. ws_ip 0.0.0.0 Specifies the IP address for the HTTP service. ws_http_port 19779 Specifies the port for the HTTP service. heartbeat_interval_secs 10 Specifies the default heartbeat interval. Make sure the heartbeat_interval_secs values for all services are the same, otherwise Nebula Graph CANNOT work normally. This configuration is measured in seconds. Caution The real IP address must be used in the configuration file. Otherwise, 127.0.0.1/0.0.0.0 cannot be parsed correctly in some cases. Raft configurations \u00b6 Name Predefined value Description raft_heartbeat_interval_secs 30 Specifies the time to expire the Raft election. The configuration is measured in seconds. raft_rpc_timeout_ms 500 Specifies the time to expire the Raft RPC. The configuration is measured in milliseconds. wal_ttl 14400 Specifies the lifetime of the RAFT WAL. The configuration is measured in seconds. Disk configurations \u00b6 Name Predefined value Description data_path data/storage Specifies the data storage path. Multiple paths are separated with commas. One RocksDB example corresponds to one path. minimum_reserved_bytes 268435456 Specifies the minimum remaining space of each data storage path. When the value is lower than this standard, the cluster data writing may fail. This configuration is measured in bytes. rocksdb_batch_size 4096 Specifies the block cache for a batch operation. The configuration is measured in bytes. rocksdb_block_cache 4 Specifies the block cache for BlockBasedTable. The configuration is measured in megabytes. disable_page_cache false Enables or disables the operating system's page cache for Nebula Graph. By default, the parameter value is false and page cache is enabled. If the value is set to true , page cache is disabled and sufficient block cache space must be configured for Nebula Graph. engine_type rocksdb Specifies the engine type. rocksdb_compression lz4 Specifies the compression algorithm for RocksDB. Optional values are no , snappy , lz4 , lz4hc , zlib , bzip2 , and zstd . rocksdb_compression_per_level \\ Specifies the compression algorithm for each level. enable_rocksdb_statistics false When set to false , RocksDB statistics is disabled. rocksdb_stats_level kExceptHistogramOrTimers Specifies the stats level for RocksDB. Optional values are kExceptHistogramOrTimers , kExceptTimers , kExceptDetailedTimers , kExceptTimeForMutex , and kAll . enable_rocksdb_prefix_filtering true When set to true , the prefix bloom filter for RocksDB is enabled. Enabling prefix bloom filter makes the graph traversal faster but occupies more memory. enable_rocksdb_whole_key_filtering false When set to true , the whole key bloom filter for RocksDB is enabled. rocksdb_filtering_prefix_length 12 Specifies the prefix length for each key. Optional values are 12 and 16 . The configuration is measured in bytes. enable_partitioned_index_filter - When set to true , it reduces the amount of memory used by the bloom filter. But in some random-seek situations, it may reduce the read performance. Key-Value separation configurations \u00b6 Name Predefined value Description rocksdb_enable_kv_separation false Whether or not to enable BlobDB (RocksDB key-value separation support). This function improves query performance. rocksdb_kv_separation_threshold 100 RocksDB key value separation threshold. Values at or above this threshold will be written to blob files during flush or compaction. Unit: bytes. rocksdb_blob_compression lz4 Compression algorithm for BlobDB. Optional values are no , snappy , lz4 , lz4hc , zlib , bzip2 , and zstd . rocksdb_enable_blob_garbage_collection true Whether to perform BlobDB garbage collection during compaction. misc configurations \u00b6 Caution The configuration snapshot in the following table is different from the snapshot in Nebula Graph. The snapshot here refers to the stock data on the leader when synchronizing Raft. Name Predefined value Description snapshot_part_rate_limit 8388608 The rate limit when the Raft leader synchronizes the stock data with other members of the Raft group. Unit: bytes/s. snapshot_batch_size 1048576 The amount of data sent in each batch when the Raft leader synchronizes the stock data with other members of the Raft group. Unit: bytes. rebuild_index_part_rate_limit 4194304 The rate limit when the Raft leader synchronizes the index data rate with other members of the Raft group during the index rebuilding process. Unit: bytes/s. rebuild_index_batch_size 1048576 The amount of data sent in each batch when the Raft leader synchronizes the index data with other members of the Raft group during the index rebuilding process. Unit: bytes. RocksDB options \u00b6 Name Predefined value Description rocksdb_db_options {} Specifies the RocksDB database options. rocksdb_column_family_options {\"write_buffer_size\":\"67108864\", \"max_write_buffer_number\":\"4\", \"max_bytes_for_level_base\":\"268435456\"} Specifies the RocksDB column family options. rocksdb_block_based_table_options {\"block_size\":\"8192\"} Specifies the RocksDB block based table options. The format of the RocksDB option is {\"<option_name>\":\"<option_value>\"} . Multiple options are separated with commas. Supported options of rocksdb_db_options and rocksdb_column_family_options are listed as follows. rocksdb_db_options max_total_wal_size delete_obsolete_files_period_micros max_background_jobs stats_dump_period_sec compaction_readahead_size writable_file_max_buffer_size bytes_per_sync wal_bytes_per_sync delayed_write_rate avoid_flush_during_shutdown max_open_files stats_persist_period_sec stats_history_buffer_size strict_bytes_per_sync enable_rocksdb_prefix_filtering enable_rocksdb_whole_key_filtering rocksdb_filtering_prefix_length num_compaction_threads rate_limit rocksdb_column_family_options write_buffer_size max_write_buffer_number level0_file_num_compaction_trigger level0_slowdown_writes_trigger level0_stop_writes_trigger target_file_size_base target_file_size_multiplier max_bytes_for_level_base max_bytes_for_level_multiplier disable_auto_compactions For more information, see RocksDB official documentation . Storage cache configurations \u00b6 Enterpriseonly Only available for the Nebula Graph Enterprise Edition. Name Predefined value Description enable_storage_cache false Whether or not to cache Storage data. storage_cache_capacity 0 The size of memory reserved for Storage caches. The value must be slightly greater than the sum of vertex_pool_capacity and empty_key_pool_capacity . The configuration is measured in MB. storage_cache_buckets_power 20 The number of buckets. The value is a logarithm with a base of 2. Optional values are 0~32. For example, the value 20 indicates that the number of buckets is 2 \\(^{20}\\) . The recommended value is ceil(log2(cacheEntries * 1.6)) . cacheEntries indicates the total number of cache items. storage_cache_locks_power 10 The number of locks. The value is a logarithm with a base of 2. Optional values are 0~32. For example, the value 10 indicates that the number of locks is 2 \\(^{10}\\) . The recommended value is max(1, storage_cache_buckets_power - 10) . enable_vertex_pool false Whether or not to add a vertex cache pool. Only valid when the storage cache feature is enabled. vertex_pool_capacity 50 The size of the vertex cache pool. The configuration is measured in MB. vertex_item_ttl 300 The TTL of vertex cache pool items. The configuration is measured in seconds. enable_empty_key_pool false Whether or not to add an empty_key pool in the cache. Only valid when the storage cache is enabled. The empty_key indicates a key that was queried but does not actually exist. empty_key_pool_capacity 50 The size of the empty_key cache pool. The configuration is measured in MB. empty_key_item_ttl 300 The TTL of the empty_key cache pool items. The configuration is measured in seconds. For super-Large vertices \u00b6 When the query starting from each vertex gets an edge, truncate it directly to avoid too many neighboring edges on the super-large vertex, because a single query occupies too much hard disk and memory. Or you can truncate a certain number of edges specified in the Max_edge_returned_per_vertex parameter. Excess edges will not be returned. This parameter applies to all spaces. Property name Default value Description max_edge_returned_per_vertex 2147483647 Specifies the maximum number of edges returned for each dense vertex. Excess edges are truncated and not returned. This parameter is not predefined in the configuration files. Compatibility The reservoir sampling algorithm in Nebula Graph 1.x is no longer supported in Nebula Graph 3.1.0. Storage configurations for large dataset \u00b6 When you have a large dataset (in the RocksDB directory) and your memory is tight, we suggest that you set the enable_partitioned_index_filter parameter to true . The performance is affected because RocksDB indexes are cached.","title":"Storage Service configurations"},{"location":"5.configurations-and-logs/1.configurations/4.storage-config/#storage_service_configurations","text":"Nebula Graph provides two initial configuration files for the Storage Service, nebula-storaged.conf.default and nebula-storaged.conf.production . Users can use them in different scenarios conveniently. The default file path is /usr/local/nebula/etc/ . Caution It is not recommended to modify the value of local_config to false . If modified, the Nebula Graph service will first read the cached configurations, which may cause configuration inconsistencies between clusters and cause unknown risks. It is not recommended to modify the configurations that are not introduced in this topic, unless you are familiar with the source code and fully understand the function of configurations.","title":"Storage Service configurations"},{"location":"5.configurations-and-logs/1.configurations/4.storage-config/#how_to_use_the_configuration_files","text":"To use the initial configuration file, choose one of the above two files and delete the suffix .default or .production from the initial configuration file for the Meta Service to apply the configurations defined in it.","title":"How to use the configuration files"},{"location":"5.configurations-and-logs/1.configurations/4.storage-config/#about_parameter_values","text":"If a parameter is not set in the configuration file, Nebula Graph uses the default value. Not all parameters are predefined. And the predefined parameters in the two initial configuration files are different. This topic uses the parameters in nebula-metad.conf.default . For parameters that are not included in nebula-metad.conf.default , see nebula-storaged.conf.production . Note The configurations of the Raft Listener and the Storage service are different. For details, see Deploy Raft listener . For all parameters and their current values, see Configurations .","title":"About parameter values"},{"location":"5.configurations-and-logs/1.configurations/4.storage-config/#basics_configurations","text":"Name Predefined value Description daemonize true When set to true , the process is a daemon process. pid_file pids/nebula-storaged.pid The file that records the process ID. timezone_name - Specifies the Nebula Graph time zone. This parameter is not predefined in the initial configuration files. The system default value is UTC+00:00:00 . For the format of the parameter value, see Specifying the Time Zone with TZ . For example, --timezone_name=UTC+08:00 represents the GMT+8 time zone. local_config true When set to true , the process gets configurations from the configuration files. Note While inserting property values of time types , Nebula Graph transforms time types (except TIMESTAMP) to the corresponding UTC according to the time zone specified by timezone_name . The time-type values returned by nGQL queries are all UTC. timezone_name is only used to transform the data stored in Nebula Graph. Other time-related data of the Nebula Graph processes still uses the default time zone of the host, such as the log printing time.","title":"Basics configurations"},{"location":"5.configurations-and-logs/1.configurations/4.storage-config/#logging_configurations","text":"Name Predefined value Description log_dir logs The directory that stores the Meta Service log. It is recommended to put logs on a different hard disk from the data. minloglevel 0 Specifies the minimum level of the log. That is, no logs below this level will be printed. Optional values are 0 (INFO), 1 (WARNING), 2 (ERROR), 3 (FATAL). It is recommended to set it to 0 during debugging and 1 in a production environment. If it is set to 4 , Nebula Graph will not print any logs. v 0 Specifies the detailed level of the log. The larger the value, the more detailed the log is. Optional values are 0 , 1 , 2 , 3 . logbufsecs 0 Specifies the maximum time to buffer the logs. If there is a timeout, it will output the buffered log to the log file. 0 means real-time output. This configuration is measured in seconds. redirect_stdout true When set to true , the process redirects the stdout and stderr to separate output files. stdout_log_file graphd-stdout.log Specifies the filename for the stdout log. stderr_log_file graphd-stderr.log Specifies the filename for the stderr log. stderrthreshold 2 Specifies the minloglevel to be copied to the stderr log. timestamp_in_logfile_name true Specifies if the log file name contains a timestamp. true indicates yes, false indicates no.","title":"Logging configurations"},{"location":"5.configurations-and-logs/1.configurations/4.storage-config/#networking_configurations","text":"Name Predefined value Description meta_server_addrs 127.0.0.1:9559 Specifies the IP addresses and ports of all Meta Services. Multiple addresses are separated with commas. local_ip 127.0.0.1 Specifies the local IP for the Storage Service. The local IP address is used to identify the nebula-storaged process. If it is a distributed cluster or requires remote access, modify it to the corresponding address. port 9779 Specifies RPC daemon listening port of the Storage service. The external port for the Meta Service is predefined to 9779 . The internal port is predefined to 9777 , 9778 , and 9780 . Nebula Graph uses the internal port for multi-replica interactions. ws_ip 0.0.0.0 Specifies the IP address for the HTTP service. ws_http_port 19779 Specifies the port for the HTTP service. heartbeat_interval_secs 10 Specifies the default heartbeat interval. Make sure the heartbeat_interval_secs values for all services are the same, otherwise Nebula Graph CANNOT work normally. This configuration is measured in seconds. Caution The real IP address must be used in the configuration file. Otherwise, 127.0.0.1/0.0.0.0 cannot be parsed correctly in some cases.","title":"Networking configurations"},{"location":"5.configurations-and-logs/1.configurations/4.storage-config/#raft_configurations","text":"Name Predefined value Description raft_heartbeat_interval_secs 30 Specifies the time to expire the Raft election. The configuration is measured in seconds. raft_rpc_timeout_ms 500 Specifies the time to expire the Raft RPC. The configuration is measured in milliseconds. wal_ttl 14400 Specifies the lifetime of the RAFT WAL. The configuration is measured in seconds.","title":"Raft configurations"},{"location":"5.configurations-and-logs/1.configurations/4.storage-config/#disk_configurations","text":"Name Predefined value Description data_path data/storage Specifies the data storage path. Multiple paths are separated with commas. One RocksDB example corresponds to one path. minimum_reserved_bytes 268435456 Specifies the minimum remaining space of each data storage path. When the value is lower than this standard, the cluster data writing may fail. This configuration is measured in bytes. rocksdb_batch_size 4096 Specifies the block cache for a batch operation. The configuration is measured in bytes. rocksdb_block_cache 4 Specifies the block cache for BlockBasedTable. The configuration is measured in megabytes. disable_page_cache false Enables or disables the operating system's page cache for Nebula Graph. By default, the parameter value is false and page cache is enabled. If the value is set to true , page cache is disabled and sufficient block cache space must be configured for Nebula Graph. engine_type rocksdb Specifies the engine type. rocksdb_compression lz4 Specifies the compression algorithm for RocksDB. Optional values are no , snappy , lz4 , lz4hc , zlib , bzip2 , and zstd . rocksdb_compression_per_level \\ Specifies the compression algorithm for each level. enable_rocksdb_statistics false When set to false , RocksDB statistics is disabled. rocksdb_stats_level kExceptHistogramOrTimers Specifies the stats level for RocksDB. Optional values are kExceptHistogramOrTimers , kExceptTimers , kExceptDetailedTimers , kExceptTimeForMutex , and kAll . enable_rocksdb_prefix_filtering true When set to true , the prefix bloom filter for RocksDB is enabled. Enabling prefix bloom filter makes the graph traversal faster but occupies more memory. enable_rocksdb_whole_key_filtering false When set to true , the whole key bloom filter for RocksDB is enabled. rocksdb_filtering_prefix_length 12 Specifies the prefix length for each key. Optional values are 12 and 16 . The configuration is measured in bytes. enable_partitioned_index_filter - When set to true , it reduces the amount of memory used by the bloom filter. But in some random-seek situations, it may reduce the read performance.","title":"Disk configurations"},{"location":"5.configurations-and-logs/1.configurations/4.storage-config/#key-value_separation_configurations","text":"Name Predefined value Description rocksdb_enable_kv_separation false Whether or not to enable BlobDB (RocksDB key-value separation support). This function improves query performance. rocksdb_kv_separation_threshold 100 RocksDB key value separation threshold. Values at or above this threshold will be written to blob files during flush or compaction. Unit: bytes. rocksdb_blob_compression lz4 Compression algorithm for BlobDB. Optional values are no , snappy , lz4 , lz4hc , zlib , bzip2 , and zstd . rocksdb_enable_blob_garbage_collection true Whether to perform BlobDB garbage collection during compaction.","title":"Key-Value separation configurations"},{"location":"5.configurations-and-logs/1.configurations/4.storage-config/#misc_configurations","text":"Caution The configuration snapshot in the following table is different from the snapshot in Nebula Graph. The snapshot here refers to the stock data on the leader when synchronizing Raft. Name Predefined value Description snapshot_part_rate_limit 8388608 The rate limit when the Raft leader synchronizes the stock data with other members of the Raft group. Unit: bytes/s. snapshot_batch_size 1048576 The amount of data sent in each batch when the Raft leader synchronizes the stock data with other members of the Raft group. Unit: bytes. rebuild_index_part_rate_limit 4194304 The rate limit when the Raft leader synchronizes the index data rate with other members of the Raft group during the index rebuilding process. Unit: bytes/s. rebuild_index_batch_size 1048576 The amount of data sent in each batch when the Raft leader synchronizes the index data with other members of the Raft group during the index rebuilding process. Unit: bytes.","title":"misc configurations"},{"location":"5.configurations-and-logs/1.configurations/4.storage-config/#rocksdb_options","text":"Name Predefined value Description rocksdb_db_options {} Specifies the RocksDB database options. rocksdb_column_family_options {\"write_buffer_size\":\"67108864\", \"max_write_buffer_number\":\"4\", \"max_bytes_for_level_base\":\"268435456\"} Specifies the RocksDB column family options. rocksdb_block_based_table_options {\"block_size\":\"8192\"} Specifies the RocksDB block based table options. The format of the RocksDB option is {\"<option_name>\":\"<option_value>\"} . Multiple options are separated with commas. Supported options of rocksdb_db_options and rocksdb_column_family_options are listed as follows. rocksdb_db_options max_total_wal_size delete_obsolete_files_period_micros max_background_jobs stats_dump_period_sec compaction_readahead_size writable_file_max_buffer_size bytes_per_sync wal_bytes_per_sync delayed_write_rate avoid_flush_during_shutdown max_open_files stats_persist_period_sec stats_history_buffer_size strict_bytes_per_sync enable_rocksdb_prefix_filtering enable_rocksdb_whole_key_filtering rocksdb_filtering_prefix_length num_compaction_threads rate_limit rocksdb_column_family_options write_buffer_size max_write_buffer_number level0_file_num_compaction_trigger level0_slowdown_writes_trigger level0_stop_writes_trigger target_file_size_base target_file_size_multiplier max_bytes_for_level_base max_bytes_for_level_multiplier disable_auto_compactions For more information, see RocksDB official documentation .","title":"RocksDB options"},{"location":"5.configurations-and-logs/1.configurations/4.storage-config/#storage_cache_configurations","text":"Enterpriseonly Only available for the Nebula Graph Enterprise Edition. Name Predefined value Description enable_storage_cache false Whether or not to cache Storage data. storage_cache_capacity 0 The size of memory reserved for Storage caches. The value must be slightly greater than the sum of vertex_pool_capacity and empty_key_pool_capacity . The configuration is measured in MB. storage_cache_buckets_power 20 The number of buckets. The value is a logarithm with a base of 2. Optional values are 0~32. For example, the value 20 indicates that the number of buckets is 2 \\(^{20}\\) . The recommended value is ceil(log2(cacheEntries * 1.6)) . cacheEntries indicates the total number of cache items. storage_cache_locks_power 10 The number of locks. The value is a logarithm with a base of 2. Optional values are 0~32. For example, the value 10 indicates that the number of locks is 2 \\(^{10}\\) . The recommended value is max(1, storage_cache_buckets_power - 10) . enable_vertex_pool false Whether or not to add a vertex cache pool. Only valid when the storage cache feature is enabled. vertex_pool_capacity 50 The size of the vertex cache pool. The configuration is measured in MB. vertex_item_ttl 300 The TTL of vertex cache pool items. The configuration is measured in seconds. enable_empty_key_pool false Whether or not to add an empty_key pool in the cache. Only valid when the storage cache is enabled. The empty_key indicates a key that was queried but does not actually exist. empty_key_pool_capacity 50 The size of the empty_key cache pool. The configuration is measured in MB. empty_key_item_ttl 300 The TTL of the empty_key cache pool items. The configuration is measured in seconds.","title":"Storage cache configurations"},{"location":"5.configurations-and-logs/1.configurations/4.storage-config/#for_super-large_vertices","text":"When the query starting from each vertex gets an edge, truncate it directly to avoid too many neighboring edges on the super-large vertex, because a single query occupies too much hard disk and memory. Or you can truncate a certain number of edges specified in the Max_edge_returned_per_vertex parameter. Excess edges will not be returned. This parameter applies to all spaces. Property name Default value Description max_edge_returned_per_vertex 2147483647 Specifies the maximum number of edges returned for each dense vertex. Excess edges are truncated and not returned. This parameter is not predefined in the configuration files. Compatibility The reservoir sampling algorithm in Nebula Graph 1.x is no longer supported in Nebula Graph 3.1.0.","title":"For super-Large vertices"},{"location":"5.configurations-and-logs/1.configurations/4.storage-config/#storage_configurations_for_large_dataset","text":"When you have a large dataset (in the RocksDB directory) and your memory is tight, we suggest that you set the enable_partitioned_index_filter parameter to true . The performance is affected because RocksDB indexes are cached.","title":"Storage configurations for large dataset"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/","text":"Kernel configurations \u00b6 This topic introduces the Kernel configurations in Nebula Graph. Resource control \u00b6 ulimit precautions \u00b6 The ulimit command specifies the resource threshold for the current shell session. The precautions are as follows: The changes made by ulimit only take effect for the current session or child process. The resource threshold (soft threshold) cannot exceed the hard threshold. Common users cannot use commands to adjust the hard threshold, even with sudo . To modify the system level or adjust the hard threshold, edit the file /etc/security/limits.conf . This method requires re-login to take effect. ulimit -c \u00b6 ulimit -c limits the size of the core dumps. We recommend that you set it to unlimited . The command is: ulimit -c unlimited ulimit -n \u00b6 ulimit -n limits the number of open files. We recommend that you set it to more than 100,000. For example: ulimit -n 130000 Memory \u00b6 vm.swappiness \u00b6 vm.swappiness specifies the percentage of the available memory before starting swap. The greater the value, the more likely the swap occurs. We recommend that you set it to 0. When set to 0, the page cache is removed first. Note that when vm.swappiness is 0, it does not mean that there is no swap. vm.min_free_kbytes \u00b6 vm.min_free_kbytes specifies the minimum number of kilobytes available kept by Linux VM. If you have a large system memory, we recommend that you increase this value. For example, if your physical memory 128GB, set it to 5GB. If the value is not big enough, the system cannot apply for enough continuous physical memory. vm.max_map_count \u00b6 vm.max_map_count limits the maximum number of vma (virtual memory area) for a process. The default value is 65530 . It is enough for most applications. If your memory application fails because the memory consumption is large, increase the vm.max_map_count value. vm.dirty_* \u00b6 These values control the dirty data cache for the system. For write-intensive scenarios, you can make adjustments based on your needs (throughput priority or delay priority). We recommend that you use the system default value. Transparent huge page \u00b6 For better delay performance, you must disable the transparent huge pages (THP). The command is: root# echo never > /sys/kernel/mm/transparent_hugepage/enabled root# echo never > /sys/kernel/mm/transparent_hugepage/defrag root# swapoff -a && swapon -a Networking \u00b6 net.ipv4.tcp_slow_start_after_idle \u00b6 The default value of net.ipv4.tcp_slow_start_after_idle is 1 . If set, the congestion window is timed out after an idle period. We recommend that you set it to 0 , especially for long fat scenarios (high latency and large bandwidth). net.core.somaxconn \u00b6 net.core.somaxconn specifies the maximum number of connection queues listened by the socket. The default value is 128 . For scenarios with a large number of burst connections, we recommend that you set it to greater than 1024 . net.ipv4.tcp_max_syn_backlog \u00b6 net.ipv4.tcp_max_syn_backlog specifies the maximum number of TCP connections in the SYN_RECV (semi-connected) state. The setting rule for this parameter is the same as that of net.core.somaxconn . net.core.netdev_max_backlog \u00b6 net.core.netdev_max_backlog specifies the maximum number of packets. The default value is 1000 . We recommend that you increase it to greater than 10,000 , especially for 10G network adapters. net.ipv4.tcp_keepalive_* \u00b6 These values keep parameters alive for TCP connections. For applications that use a 4-layer transparent load balancer, if the idle connection is disconnected unexpectedly, decrease the values of tcp_keepalive_time and tcp_keepalive_intvl . net.ipv4.tcp_rmem/wmem \u00b6 net.ipv4.tcp_wmem/rmem specifies the minimum, default, and maximum size of the buffer pool sent/received by the TCP socket. For long fat links, we recommend that you increase the default value to bandwidth (GB) * RTT (ms) . scheduler \u00b6 For SSD devices, we recommend that you set scheduler to noop or none . The path is /sys/block/DEV_NAME/queue/scheduler . Other parameters \u00b6 kernel.core_pattern \u00b6 we recommend that you set it to core and set kernel.core_uses_pid to 1 . Modify parameters \u00b6 sysctl \u00b6 sysctl <conf_name> Checks the current parameter value. sysctl -w <conf_name>=<value> Modifies the parameter value. The modification takes effect immediately. The original value is restored after restarting. sysctl -p [<file_path>] Loads Linux parameter values \u200b\u200bfrom the specified configuration file. The default path is /etc/sysctl.conf . prlimit \u00b6 The prlimit command gets and sets process resource limits. You can modify the hard threshold by using it and the sudo command. For example, prlimit --nofile = 130000 --pid = $$ adjusts the maximum number of open files permitted by the current process to 14000 . And the modification takes effect immediately. Note that this command is only available in RedHat 7u or higher versions.","title":"Kernel configurations"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#kernel_configurations","text":"This topic introduces the Kernel configurations in Nebula Graph.","title":"Kernel configurations"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#resource_control","text":"","title":"Resource control"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#ulimit_precautions","text":"The ulimit command specifies the resource threshold for the current shell session. The precautions are as follows: The changes made by ulimit only take effect for the current session or child process. The resource threshold (soft threshold) cannot exceed the hard threshold. Common users cannot use commands to adjust the hard threshold, even with sudo . To modify the system level or adjust the hard threshold, edit the file /etc/security/limits.conf . This method requires re-login to take effect.","title":"ulimit precautions"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#ulimit_-c","text":"ulimit -c limits the size of the core dumps. We recommend that you set it to unlimited . The command is: ulimit -c unlimited","title":"ulimit -c"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#ulimit_-n","text":"ulimit -n limits the number of open files. We recommend that you set it to more than 100,000. For example: ulimit -n 130000","title":"ulimit -n"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#memory","text":"","title":"Memory"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#vmswappiness","text":"vm.swappiness specifies the percentage of the available memory before starting swap. The greater the value, the more likely the swap occurs. We recommend that you set it to 0. When set to 0, the page cache is removed first. Note that when vm.swappiness is 0, it does not mean that there is no swap.","title":"vm.swappiness"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#vmmin_free_kbytes","text":"vm.min_free_kbytes specifies the minimum number of kilobytes available kept by Linux VM. If you have a large system memory, we recommend that you increase this value. For example, if your physical memory 128GB, set it to 5GB. If the value is not big enough, the system cannot apply for enough continuous physical memory.","title":"vm.min_free_kbytes"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#vmmax_map_count","text":"vm.max_map_count limits the maximum number of vma (virtual memory area) for a process. The default value is 65530 . It is enough for most applications. If your memory application fails because the memory consumption is large, increase the vm.max_map_count value.","title":"vm.max_map_count"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#vmdirty_","text":"These values control the dirty data cache for the system. For write-intensive scenarios, you can make adjustments based on your needs (throughput priority or delay priority). We recommend that you use the system default value.","title":"vm.dirty_*"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#transparent_huge_page","text":"For better delay performance, you must disable the transparent huge pages (THP). The command is: root# echo never > /sys/kernel/mm/transparent_hugepage/enabled root# echo never > /sys/kernel/mm/transparent_hugepage/defrag root# swapoff -a && swapon -a","title":"Transparent huge page"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#networking","text":"","title":"Networking"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#netipv4tcp_slow_start_after_idle","text":"The default value of net.ipv4.tcp_slow_start_after_idle is 1 . If set, the congestion window is timed out after an idle period. We recommend that you set it to 0 , especially for long fat scenarios (high latency and large bandwidth).","title":"net.ipv4.tcp_slow_start_after_idle"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#netcoresomaxconn","text":"net.core.somaxconn specifies the maximum number of connection queues listened by the socket. The default value is 128 . For scenarios with a large number of burst connections, we recommend that you set it to greater than 1024 .","title":"net.core.somaxconn"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#netipv4tcp_max_syn_backlog","text":"net.ipv4.tcp_max_syn_backlog specifies the maximum number of TCP connections in the SYN_RECV (semi-connected) state. The setting rule for this parameter is the same as that of net.core.somaxconn .","title":"net.ipv4.tcp_max_syn_backlog"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#netcorenetdev_max_backlog","text":"net.core.netdev_max_backlog specifies the maximum number of packets. The default value is 1000 . We recommend that you increase it to greater than 10,000 , especially for 10G network adapters.","title":"net.core.netdev_max_backlog"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#netipv4tcp_keepalive_","text":"These values keep parameters alive for TCP connections. For applications that use a 4-layer transparent load balancer, if the idle connection is disconnected unexpectedly, decrease the values of tcp_keepalive_time and tcp_keepalive_intvl .","title":"net.ipv4.tcp_keepalive_*"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#netipv4tcp_rmemwmem","text":"net.ipv4.tcp_wmem/rmem specifies the minimum, default, and maximum size of the buffer pool sent/received by the TCP socket. For long fat links, we recommend that you increase the default value to bandwidth (GB) * RTT (ms) .","title":"net.ipv4.tcp_rmem/wmem"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#scheduler","text":"For SSD devices, we recommend that you set scheduler to noop or none . The path is /sys/block/DEV_NAME/queue/scheduler .","title":"scheduler"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#other_parameters","text":"","title":"Other parameters"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#kernelcore_pattern","text":"we recommend that you set it to core and set kernel.core_uses_pid to 1 .","title":"kernel.core_pattern"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#modify_parameters","text":"","title":"Modify parameters"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#sysctl","text":"sysctl <conf_name> Checks the current parameter value. sysctl -w <conf_name>=<value> Modifies the parameter value. The modification takes effect immediately. The original value is restored after restarting. sysctl -p [<file_path>] Loads Linux parameter values \u200b\u200bfrom the specified configuration file. The default path is /etc/sysctl.conf .","title":"sysctl"},{"location":"5.configurations-and-logs/1.configurations/6.kernel-config/#prlimit","text":"The prlimit command gets and sets process resource limits. You can modify the hard threshold by using it and the sudo command. For example, prlimit --nofile = 130000 --pid = $$ adjusts the maximum number of open files permitted by the current process to 14000 . And the modification takes effect immediately. Note that this command is only available in RedHat 7u or higher versions.","title":"prlimit"},{"location":"5.configurations-and-logs/2.log-management/audit-log/","text":"Audit logs \u00b6 The Nebula Graph audit logs store all operations received by graph service in categories, then provide the logs for users to track specific types of operations as needed. Enterpriseonly Only available for the Nebula Graph Enterprise Edition. Log categories \u00b6 Category Statement Description login - Logs the information when the client tries to connect to graph service. exit - Logs the information when the client disconnect from graph service. ddl CREATE SPACE \u3001 DROP SPACE \u3001 CREATE TAG \u3001 DROP TAG \u3001 ALTER TAG \u3001 DELETE TAG \u3001 CREATE EDGE \u3001 DROP EDGE \u3001 ALTER EDGE \u3001 CREATE INDEX \u3001 REBUILD INDEX \u3001 DROP INDEX \u3001 CREATE FULLTEXT INDEX \u3001 REBUILD FULLTEXT INDEX \u3001 DROP FULLTEXT INDEX Logs the information about DDL statements. dql MATCH \u3001 LOOKUP \u3001 GO \u3001 FETCH \u3001 GET SUBGRAPH \u3001 FIND PATH \u3001 UNWIND \u3001 GROUP BY \u3001 ORDER BY \u3001 YIELD \u3001 LIMIT \u3001 RETURN Logs the information about DQL statements. dml INSERT VERTEX \u3001 DELETE VERTEX \u3001 UPDATE VERTEX \u3001 UPSERT VERTEX \u3001 INSERT EDGE \u3001 DELETE EDGE \u3001 UPDATE EDGE \u3001 UPSERT EDGE Logs the information about DML statements. dcl CREATE USER \u3001 GRANT ROLE \u3001 REVOKE ROLE \u3001 CHANGE PASSWORD \u3001 ALTER USER \u3001 DROP USER \u3001 CREATE SNAPSHOT \u3001 DROP SNAPSHOT \u3001 ADD LISTENER \u3001 REMOVE LISTENER \u3001 BALANCE \u3001 SUBMIT JOB \u3001 STOP JOB \u3001 RECOVER JOB \u3001 ADD DRAINER \u3001 REMOVE DRAINER Logs the information about DCL statements. util SHOW HOSTS \u3001 SHOW USERS \u3001 SHOW ROLES \u3001 SHOW SNAPSHOTS \u3001 SHOW SPACES \u3001 SHOW PARTS \u3001 SHOW TAGS \u3001 SHOW EDGES \u3001 SHOW INDEXES \u3001 SHOW CREATE SPACE \u3001 SHOW CREATE TAG/EDGE \u3001 SHOW CREATE INDEX \u3001 SHOW INDEX STATUS \u3001 SHOW LISTENER \u3001 SHOW TEXT SEARCH CLIENTS \u3001 SHOW DRAINER CLIENTS \u3001 SHOW FULLTEXT INDEXES \u3001 SHOW CONFIGS \u3001 SHOW CHARSET \u3001 SHOW COLLATION \u3001 SHOW STATS \u3001 SHOW SESSIONS \u3001 SHOW META LEADER \u3001 SHOW DRAINERS \u3001 SHOW QUERIES \u3001 SHOW JOB \u3001 SHOW JOBS \u3001 DESCRIBE INDEX \u3001 DESCRIBE EDGE \u3001 DESCRIBE TAG \u3001 DESCRIBE SPACE \u3001 DESCRIBE USER \u3001 USE SPACE \u3001 SIGN IN TEXT SERVICE \u3001 SIGN OUT TEXT SERVICE \u3001 SIGN IN DRAINER SERVICE \u3001 SIGN OUT DRAINER SERVICE \u3001 EXPLAIN \u3001 PROFILE \u3001 KILL QUERY \u3001 DOWNLOAD HDFS \u3001 INGEST Logs the information about util statements. unknown - Logs the information about unrecognized statements. Configure audit logs \u00b6 You need to configure the graph service file to view audit logs. The default file path of configuration is /usr/local/nebula/etc/nebula-graphd.conf . Note After modifying the configuration, you need to restart the graph service to take effect. Parameter descriptions are as follows: Parameter Predefined value Description enable_audit false Whether or not to enable audit logs. audit_log_handler file Where will the audit logs be written. Optional values are file \uff08local file\uff09 and es (Elasticsearch). audit_log_file ./logs/audit/audit.log Takes effect only when audit_log_handler=file . The path for storing audit logs. The value can be absolute or relative. audit_log_strategy synchronous Sets the method to synchronize audit logs. Takes effect only when audit_log_handler=file . Optional values are asynchronous and synchronous . When asynchronous , log events are cached in memory and do not block the main thread, but may result in missing logs due to insufficient cache. When synchronous , log events are refreshed and synchronized to the file each time. audit_log_max_buffer_size 1048576 Take effect only when audit_log_handler=file and audit_log_strategy=asynchronous . The size of the memory buffer used for logging. Unit: bytes. audit_log_format xml Takes effect only when audit_log_handler=file . The format of the the audit logs. Optional values are xml , json and csv . audit_log_es_address - Takes effect only when audit_log_handler=es . The address of Elasticsearch server. The format is IP1:port1, IP2:port2, ... . audit_log_es_user - Takes effect only when audit_log_handler=es . The user name of the Elasticsearch. audit_log_es_password - Takes effect only when audit_log_handler=es . The user password of the Elasticsearch. audit_log_es_batch_size 1000 Takes effect only when audit_log_handler=es . The number of logs sent to Elasticsearch at one time. audit_log_exclude_spaces - The list of spaces for not tracking. Multiple graph spaces are separated by commas. audit_log_categories login,exit The list of log categories for tracking. Multiple categories are separated by commas. Audit logs format \u00b6 The fields of audit logs are the same for different handlers and formats. For example, when the audit logs are stored in the default path logs/audit/audit.log and in the format of XML, the fields in the audit logs are described as follows: <AUDIT_RECORD CATEGORY = \"util\" TIMESTAMP = \"2022-04-07 02:31:38\" TERMINAL = \"\" CONNECTION_ID = \"1649298693144580\" CONNECTION_STATUS = \"0\" CONNECTION_MESSAGE = \"\" USER = \"root\" CLIENT_HOST = \"127.0.0.1\" HOST = \"192.168.8.111\" SPACE = \"\" QUERY = \"use basketballplayer1\" QUERY_STATUS = \"-1005\" QUERY_MESSAGE = \"SpaceNotFound: \" /> <AUDIT_RECORD CATEGORY = \"util\" TIMESTAMP = \"2022-04-07 02:31:39\" TERMINAL = \"\" CONNECTION_ID = \"1649298693144580\" CONNECTION_STATUS = \"0\" CONNECTION_MESSAGE = \"\" USER = \"root\" CLIENT_HOST = \"127.0.0.1\" HOST = \"192.168.8.111\" SPACE = \"\" QUERY = \"use basketballplayer\" QUERY_STATUS = \"0\" QUERY_MESSAGE = \"\" /> Field Description CATEGORY The category of the audit logs. TIMESTAMP The generation time of the audit logs. TERMINAL The reserved field. CONNECTION_ID The session ID of the connection. CONNECTION_STATUS The status of the connection. 0 indicates success, and other numbers indicate different error messages. CONNECTION_MESSAGE An error message is displayed when the connection fails. USER The user name of the Nebula Graph connection. CLIENT_HOST The IP address of the client. HOST The IP address of the host. SPACE The graph space where you perform queries. QUERY The query statement. QUERY_STATUS The status of the query. 0 indicates success, and other numbers indicate different error messages. QUERY_MESSAGE An error message is displayed when the query fails.","title":"Audit logs(Enterprise)"},{"location":"5.configurations-and-logs/2.log-management/audit-log/#audit_logs","text":"The Nebula Graph audit logs store all operations received by graph service in categories, then provide the logs for users to track specific types of operations as needed. Enterpriseonly Only available for the Nebula Graph Enterprise Edition.","title":"Audit logs"},{"location":"5.configurations-and-logs/2.log-management/audit-log/#log_categories","text":"Category Statement Description login - Logs the information when the client tries to connect to graph service. exit - Logs the information when the client disconnect from graph service. ddl CREATE SPACE \u3001 DROP SPACE \u3001 CREATE TAG \u3001 DROP TAG \u3001 ALTER TAG \u3001 DELETE TAG \u3001 CREATE EDGE \u3001 DROP EDGE \u3001 ALTER EDGE \u3001 CREATE INDEX \u3001 REBUILD INDEX \u3001 DROP INDEX \u3001 CREATE FULLTEXT INDEX \u3001 REBUILD FULLTEXT INDEX \u3001 DROP FULLTEXT INDEX Logs the information about DDL statements. dql MATCH \u3001 LOOKUP \u3001 GO \u3001 FETCH \u3001 GET SUBGRAPH \u3001 FIND PATH \u3001 UNWIND \u3001 GROUP BY \u3001 ORDER BY \u3001 YIELD \u3001 LIMIT \u3001 RETURN Logs the information about DQL statements. dml INSERT VERTEX \u3001 DELETE VERTEX \u3001 UPDATE VERTEX \u3001 UPSERT VERTEX \u3001 INSERT EDGE \u3001 DELETE EDGE \u3001 UPDATE EDGE \u3001 UPSERT EDGE Logs the information about DML statements. dcl CREATE USER \u3001 GRANT ROLE \u3001 REVOKE ROLE \u3001 CHANGE PASSWORD \u3001 ALTER USER \u3001 DROP USER \u3001 CREATE SNAPSHOT \u3001 DROP SNAPSHOT \u3001 ADD LISTENER \u3001 REMOVE LISTENER \u3001 BALANCE \u3001 SUBMIT JOB \u3001 STOP JOB \u3001 RECOVER JOB \u3001 ADD DRAINER \u3001 REMOVE DRAINER Logs the information about DCL statements. util SHOW HOSTS \u3001 SHOW USERS \u3001 SHOW ROLES \u3001 SHOW SNAPSHOTS \u3001 SHOW SPACES \u3001 SHOW PARTS \u3001 SHOW TAGS \u3001 SHOW EDGES \u3001 SHOW INDEXES \u3001 SHOW CREATE SPACE \u3001 SHOW CREATE TAG/EDGE \u3001 SHOW CREATE INDEX \u3001 SHOW INDEX STATUS \u3001 SHOW LISTENER \u3001 SHOW TEXT SEARCH CLIENTS \u3001 SHOW DRAINER CLIENTS \u3001 SHOW FULLTEXT INDEXES \u3001 SHOW CONFIGS \u3001 SHOW CHARSET \u3001 SHOW COLLATION \u3001 SHOW STATS \u3001 SHOW SESSIONS \u3001 SHOW META LEADER \u3001 SHOW DRAINERS \u3001 SHOW QUERIES \u3001 SHOW JOB \u3001 SHOW JOBS \u3001 DESCRIBE INDEX \u3001 DESCRIBE EDGE \u3001 DESCRIBE TAG \u3001 DESCRIBE SPACE \u3001 DESCRIBE USER \u3001 USE SPACE \u3001 SIGN IN TEXT SERVICE \u3001 SIGN OUT TEXT SERVICE \u3001 SIGN IN DRAINER SERVICE \u3001 SIGN OUT DRAINER SERVICE \u3001 EXPLAIN \u3001 PROFILE \u3001 KILL QUERY \u3001 DOWNLOAD HDFS \u3001 INGEST Logs the information about util statements. unknown - Logs the information about unrecognized statements.","title":"Log categories"},{"location":"5.configurations-and-logs/2.log-management/audit-log/#configure_audit_logs","text":"You need to configure the graph service file to view audit logs. The default file path of configuration is /usr/local/nebula/etc/nebula-graphd.conf . Note After modifying the configuration, you need to restart the graph service to take effect. Parameter descriptions are as follows: Parameter Predefined value Description enable_audit false Whether or not to enable audit logs. audit_log_handler file Where will the audit logs be written. Optional values are file \uff08local file\uff09 and es (Elasticsearch). audit_log_file ./logs/audit/audit.log Takes effect only when audit_log_handler=file . The path for storing audit logs. The value can be absolute or relative. audit_log_strategy synchronous Sets the method to synchronize audit logs. Takes effect only when audit_log_handler=file . Optional values are asynchronous and synchronous . When asynchronous , log events are cached in memory and do not block the main thread, but may result in missing logs due to insufficient cache. When synchronous , log events are refreshed and synchronized to the file each time. audit_log_max_buffer_size 1048576 Take effect only when audit_log_handler=file and audit_log_strategy=asynchronous . The size of the memory buffer used for logging. Unit: bytes. audit_log_format xml Takes effect only when audit_log_handler=file . The format of the the audit logs. Optional values are xml , json and csv . audit_log_es_address - Takes effect only when audit_log_handler=es . The address of Elasticsearch server. The format is IP1:port1, IP2:port2, ... . audit_log_es_user - Takes effect only when audit_log_handler=es . The user name of the Elasticsearch. audit_log_es_password - Takes effect only when audit_log_handler=es . The user password of the Elasticsearch. audit_log_es_batch_size 1000 Takes effect only when audit_log_handler=es . The number of logs sent to Elasticsearch at one time. audit_log_exclude_spaces - The list of spaces for not tracking. Multiple graph spaces are separated by commas. audit_log_categories login,exit The list of log categories for tracking. Multiple categories are separated by commas.","title":"Configure audit logs"},{"location":"5.configurations-and-logs/2.log-management/audit-log/#audit_logs_format","text":"The fields of audit logs are the same for different handlers and formats. For example, when the audit logs are stored in the default path logs/audit/audit.log and in the format of XML, the fields in the audit logs are described as follows: <AUDIT_RECORD CATEGORY = \"util\" TIMESTAMP = \"2022-04-07 02:31:38\" TERMINAL = \"\" CONNECTION_ID = \"1649298693144580\" CONNECTION_STATUS = \"0\" CONNECTION_MESSAGE = \"\" USER = \"root\" CLIENT_HOST = \"127.0.0.1\" HOST = \"192.168.8.111\" SPACE = \"\" QUERY = \"use basketballplayer1\" QUERY_STATUS = \"-1005\" QUERY_MESSAGE = \"SpaceNotFound: \" /> <AUDIT_RECORD CATEGORY = \"util\" TIMESTAMP = \"2022-04-07 02:31:39\" TERMINAL = \"\" CONNECTION_ID = \"1649298693144580\" CONNECTION_STATUS = \"0\" CONNECTION_MESSAGE = \"\" USER = \"root\" CLIENT_HOST = \"127.0.0.1\" HOST = \"192.168.8.111\" SPACE = \"\" QUERY = \"use basketballplayer\" QUERY_STATUS = \"0\" QUERY_MESSAGE = \"\" /> Field Description CATEGORY The category of the audit logs. TIMESTAMP The generation time of the audit logs. TERMINAL The reserved field. CONNECTION_ID The session ID of the connection. CONNECTION_STATUS The status of the connection. 0 indicates success, and other numbers indicate different error messages. CONNECTION_MESSAGE An error message is displayed when the connection fails. USER The user name of the Nebula Graph connection. CLIENT_HOST The IP address of the client. HOST The IP address of the host. SPACE The graph space where you perform queries. QUERY The query statement. QUERY_STATUS The status of the query. 0 indicates success, and other numbers indicate different error messages. QUERY_MESSAGE An error message is displayed when the query fails.","title":"Audit logs format"},{"location":"5.configurations-and-logs/2.log-management/logs/","text":"Runtime logs \u00b6 Runtime logs are provided for DBAs and developers to locate faults when the system fails. Nebula Graph uses glog to print runtime logs, uses gflags to control the severity level of the log, and provides an HTTP interface to dynamically change the log level at runtime to facilitate tracking. Log directory \u00b6 The default runtime log directory is /usr/local/nebula/logs/ . If the log directory is deleted while Nebula Graph is running, the log would not continue to be printed. However, this operation will not affect the services. To recover the logs, restart the services. Parameter descriptions \u00b6 minloglevel : Specifies the minimum level of the log. That is, no logs below this level will be printed. Optional values are 0 (INFO), 1 (WARNING), 2 (ERROR), 3 (FATAL). It is recommended to set it to 0 during debugging and 1 in a production environment. If it is set to 4 , Nebula Graph will not print any logs. v : Specifies the detailed level of the log. The larger the value, the more detailed the log is. Optional values are 0 , 1 , 2 , 3 . The default severity level for the metad, graphd, and storaged logs can be found in their respective configuration files. The default path is /usr/local/nebula/etc/ . Check the severity level \u00b6 Check all the flag values (log values included) of the current gflags with the following command. $ curl <ws_ip>:<ws_port>/flags Parameter Description ws_ip The IP address for the HTTP service, which can be found in the configuration files above. The default value is 127.0.0.1 . ws_port The port for the HTTP service, which can be found in the configuration files above. The default values are 19559 (Meta), 19669 (Graph), and 19779 (Storage) respectively. Examples are as follows: Check the current minloglevel in the Meta service: $ curl 127 .0.0.1:19559/flags | grep 'minloglevel' Check the current v in the Storage service: $ curl 127 .0.0.1:19779/flags | grep -w 'v' Change the severity level \u00b6 Change the severity level of the log with the following command. $ curl -X PUT -H \"Content-Type: application/json\" -d '{\"<key>\":<value>[,\"<key>\":<value>]}' \"<ws_ip>:<ws_port>/flags\" Parameter Description key The type of the log to be changed. For optional values, see Parameter descriptions . value The level of the log. For optional values, see Parameter descriptions . ws_ip The IP address for the HTTP service, which can be found in the configuration files above. The default value is 127.0.0.1 . ws_port The port for the HTTP service, which can be found in the configuration files above. The default values are 19559 (Meta), 19669 (Graph), and 19779 (Storage) respectively. Examples are as follows: $ curl -X PUT -H \"Content-Type: application/json\" -d '{\"minloglevel\":0,\"v\":3}' \"127.0.0.1:19779/flags\" # storaged $ curl -X PUT -H \"Content-Type: application/json\" -d '{\"minloglevel\":0,\"v\":3}' \"127.0.0.1:19669/flags\" # graphd $ curl -X PUT -H \"Content-Type: application/json\" -d '{\"minloglevel\":0,\"v\":3}' \"127.0.0.1:19559/flags\" # metad If the log level is changed while Nebula Graph is running, it will be restored to the level set in the configuration file after restarting the service. To permanently modify it, see Configuration files . RocksDB runtime logs \u00b6 RocksDB runtime logs are usually used to debug RocksDB parameters and stored in /usr/local/nebula/data/storage/nebula/$id/data/LOG . $id is the ID of the example.","title":"Runtime logs"},{"location":"5.configurations-and-logs/2.log-management/logs/#runtime_logs","text":"Runtime logs are provided for DBAs and developers to locate faults when the system fails. Nebula Graph uses glog to print runtime logs, uses gflags to control the severity level of the log, and provides an HTTP interface to dynamically change the log level at runtime to facilitate tracking.","title":"Runtime logs"},{"location":"5.configurations-and-logs/2.log-management/logs/#log_directory","text":"The default runtime log directory is /usr/local/nebula/logs/ . If the log directory is deleted while Nebula Graph is running, the log would not continue to be printed. However, this operation will not affect the services. To recover the logs, restart the services.","title":"Log directory"},{"location":"5.configurations-and-logs/2.log-management/logs/#parameter_descriptions","text":"minloglevel : Specifies the minimum level of the log. That is, no logs below this level will be printed. Optional values are 0 (INFO), 1 (WARNING), 2 (ERROR), 3 (FATAL). It is recommended to set it to 0 during debugging and 1 in a production environment. If it is set to 4 , Nebula Graph will not print any logs. v : Specifies the detailed level of the log. The larger the value, the more detailed the log is. Optional values are 0 , 1 , 2 , 3 . The default severity level for the metad, graphd, and storaged logs can be found in their respective configuration files. The default path is /usr/local/nebula/etc/ .","title":"Parameter descriptions"},{"location":"5.configurations-and-logs/2.log-management/logs/#check_the_severity_level","text":"Check all the flag values (log values included) of the current gflags with the following command. $ curl <ws_ip>:<ws_port>/flags Parameter Description ws_ip The IP address for the HTTP service, which can be found in the configuration files above. The default value is 127.0.0.1 . ws_port The port for the HTTP service, which can be found in the configuration files above. The default values are 19559 (Meta), 19669 (Graph), and 19779 (Storage) respectively. Examples are as follows: Check the current minloglevel in the Meta service: $ curl 127 .0.0.1:19559/flags | grep 'minloglevel' Check the current v in the Storage service: $ curl 127 .0.0.1:19779/flags | grep -w 'v'","title":"Check the severity level"},{"location":"5.configurations-and-logs/2.log-management/logs/#change_the_severity_level","text":"Change the severity level of the log with the following command. $ curl -X PUT -H \"Content-Type: application/json\" -d '{\"<key>\":<value>[,\"<key>\":<value>]}' \"<ws_ip>:<ws_port>/flags\" Parameter Description key The type of the log to be changed. For optional values, see Parameter descriptions . value The level of the log. For optional values, see Parameter descriptions . ws_ip The IP address for the HTTP service, which can be found in the configuration files above. The default value is 127.0.0.1 . ws_port The port for the HTTP service, which can be found in the configuration files above. The default values are 19559 (Meta), 19669 (Graph), and 19779 (Storage) respectively. Examples are as follows: $ curl -X PUT -H \"Content-Type: application/json\" -d '{\"minloglevel\":0,\"v\":3}' \"127.0.0.1:19779/flags\" # storaged $ curl -X PUT -H \"Content-Type: application/json\" -d '{\"minloglevel\":0,\"v\":3}' \"127.0.0.1:19669/flags\" # graphd $ curl -X PUT -H \"Content-Type: application/json\" -d '{\"minloglevel\":0,\"v\":3}' \"127.0.0.1:19559/flags\" # metad If the log level is changed while Nebula Graph is running, it will be restored to the level set in the configuration file after restarting the service. To permanently modify it, see Configuration files .","title":"Change the severity level"},{"location":"5.configurations-and-logs/2.log-management/logs/#rocksdb_runtime_logs","text":"RocksDB runtime logs are usually used to debug RocksDB parameters and stored in /usr/local/nebula/data/storage/nebula/$id/data/LOG . $id is the ID of the example.","title":"RocksDB runtime logs"},{"location":"6.monitor-and-metrics/1.query-performance-metrics/","text":"Query Nebula Graph metrics \u00b6 Nebula Graph supports querying the monitoring metrics through HTTP ports. Metrics structure \u00b6 Each metric of Nebula Graph consists of three fields: name, type, and time range. The fields are separated by periods, for example, num_queries.sum.600 . Different Nebula Graph services (Graph, Storage, or Meta) support different metrics. The detailed description is as follows. Field Example Description Metric name num_queries Indicates the function of the metric. Metric type sum Indicates how the metrics are collected. Supported types are SUM, AVG, RATE, and the P-th sample quantiles such as P75, P95, P99, and P99.9. Time range 600 The time range in seconds for the metric collection. Supported values are 5, 60, 600, and 3600, representing the last 5 seconds, 1 minute, 10 minutes, and 1 hour. Space-level metrics \u00b6 The Graph service supports a set of space-level metrics that record the information of different graph spaces separately. To enable space-level metrics, set the value of enable_space_level_metrics to true in the Graph service configuration file before starting Nebula Graph. For details about how to modify the configuration, see Configuration Management . Note Space-level metrics can be queried only by querying all metrics. For example, run curl -G \"http://192.168.8.40:19559/stats\" to show all metrics. The returned result contains the graph space name in the form of '{space=space_name}', such as num_active_queries{space=basketballplayer}.sum.5=0 . Query metrics over HTTP \u00b6 Syntax \u00b6 curl -G \"http://<ip>:<port>/stats?stats=<metric_name_list> [&format=json]\" Parameter Description ip The IP address of the server. You can find it in the configuration file in the installation directory. port The HTTP port of the server. You can find it in the configuration file in the installation directory. The default ports are 19559 (Meta), 19669 (Graph), and 19779 (Storage). metric_name_list The metrics names. Multiple metrics are separated by commas (,). &format=json Optional. Returns the result in the JSON format. Note If Nebula Graph is deployed with Docker Compose , run docker-compose ps to check the ports that are mapped from the service ports inside of the container and then query through them. Examples \u00b6 Query a single metric Query the query number in the last 10 minutes in the Graph Service. $ curl -G \"http://192.168.8.40:19669/stats?stats=num_queries.sum.600\" num_queries.sum.600 = 400 Query multiple metrics Query the following metrics together: The average heartbeat latency in the last 1 minute. The average latency of the slowest 1% heartbeats, i.e., the P99 heartbeats, in the last 10 minutes. $ curl -G \"http://192.168.8.40:19559/stats?stats=heartbeat_latency_us.avg.60,heartbeat_latency_us.p99.600\" heartbeat_latency_us.avg.60 = 281 heartbeat_latency_us.p99.600 = 985 Return a JSON result. Query the number of new vertices in the Storage Service in the last 10 minutes and return the result in the JSON format. $ curl -G \"http://192.168.8.40:19779/stats?stats=num_add_vertices.sum.600&format=json\" [{ \"value\" :1, \"name\" : \"num_add_vertices.sum.600\" }] Query all metrics in a service. If no metric is specified in the query, Nebula Graph returns all metrics in the service. $ curl -G \"http://192.168.8.40:19559/stats\" heartbeat_latency_us.avg.5 = 304 heartbeat_latency_us.avg.60 = 308 heartbeat_latency_us.avg.600 = 299 heartbeat_latency_us.avg.3600 = 285 heartbeat_latency_us.p75.5 = 652 heartbeat_latency_us.p75.60 = 669 heartbeat_latency_us.p75.600 = 651 heartbeat_latency_us.p75.3600 = 642 heartbeat_latency_us.p95.5 = 930 heartbeat_latency_us.p95.60 = 963 heartbeat_latency_us.p95.600 = 933 heartbeat_latency_us.p95.3600 = 929 heartbeat_latency_us.p99.5 = 986 heartbeat_latency_us.p99.60 = 1409 heartbeat_latency_us.p99.600 = 989 heartbeat_latency_us.p99.3600 = 986 num_heartbeats.rate.5 = 0 num_heartbeats.rate.60 = 0 num_heartbeats.rate.600 = 0 num_heartbeats.rate.3600 = 0 num_heartbeats.sum.5 = 2 num_heartbeats.sum.60 = 40 num_heartbeats.sum.600 = 394 num_heartbeats.sum.3600 = 2364 ... Metric description \u00b6 Graph \u00b6 Parameter Description num_active_queries The number of queries currently being executed. num_active_sessions The number of currently active sessions. num_aggregate_executors The number of executions for the Aggregation operator. num_auth_failed_sessions_bad_username_password The number of sessions where authentication failed due to incorrect username and password. num_auth_failed_sessions_out_of_max_allowed The number of sessions that failed to authenticate logins because the value of the parameter FLAG_OUT_OF_MAX_ALLOWED_CONNECTIONS was exceeded. num_auth_failed_sessions The number of sessions in which login authentication failed. num_indexscan_executors The number of executions for index scan operators. num_killed_queries The number of killed queries. num_opened_sessions The number of sessions connected to the server. num_queries The number of queries. num_query_errors_leader_changes The number of the raft leader changes due to query errors. num_query_errors The number of query errors. num_reclaimed_expired_sessions The number of expired sessions actively reclaimed by the server. num_rpc_sent_to_metad_failed The number of failed RPC requests that the Graphd service sends to the Metad service. num_rpc_sent_to_metad The number of RPC requests that the Graphd service sent to the Metad service. num_rpc_sent_to_storaged_failed The number of failed RPC requests that the Graphd service sent to the Storaged service. num_rpc_sent_to_storaged The number of RPC requests that the Graphd service sent to the Storaged service. num_sentences The number of statements received by the Graphd service. num_slow_queries The number of slow queries. num_sort_executors The number of executions for the Sort operator. optimizer_latency_us The latency of executing optimizer statements. query_latency_us The average latency of queries. slow_query_latency_us The average latency of slow queries. num_queries_hit_memory_watermark The number of queries that reached the memory watermark. Meta \u00b6 Parameter Description commit_log_latency_us The latency of committing logs in Raft. commit_snapshot_latency_us The latency of committing snapshots in Raft. heartbeat_latency_us The latency of heartbeats. num_heartbeats The number of heartbeats. num_raft_votes The number of votes in Raft. transfer_leader_latency_us The latency of transferring the raft leader. num_agent_heartbeats The number of heartbeats for the AgentHBProcessor. agent_heartbeat_latency_us The average latency of the AgentHBProcessor. Storage \u00b6 Parameter Description add_edges_atomic_latency_us The average latency of adding edge single. add_edges_latency_us The average latency of adding edges. add_vertices_latency_us The average latency of adding vertices. commit_log_latency_us The latency of committing logs in Raft. commit_snapshot_latency_us The latency of committing snapshots in Raft. delete_edges_latency_us The average latency of deleting edges. delete_vertices_latency_us The average latency of deleting vertices. get_neighbors_latency_us The average latency of querying neighbor vertices. num_get_prop The number of executions for the GetPropProcessor. num_get_neighbors_errors The number of execution errors for the GetNeighborsProcessor. get_prop_latency_us The average latency of executions for the GetPropProcessor. num_edges_deleted The number of deleted edges. num_edges_inserted The number of inserted edges. num_raft_votes The number of votes in Raft. num_rpc_sent_to_metad_failed The number of failed RPC requests that the Storage service sent to the Meta service. num_rpc_sent_to_metad The number of RPC requests that the Storaged service sent to the Metad service. num_tags_deleted The number of deleted tags. num_vertices_deleted The number of deleted vertices. num_vertices_inserted The number of inserted vertices. transfer_leader_latency_us The latency of transferring the raft leader. lookup_latency_us The average latency of executions for the LookupProcessor. num_lookup_errors The number of execution errors for the LookupProcessor. num_scan_vertex The number of executions for the ScanVertexProcessor. num_scan_vertex_errors The number of execution errors for the ScanVertexProcessor. update_edge_latency_us The average latency of executions for the UpdateEdgeProcessor. num_update_vertex The number of executions for the UpdateVertexProcessor. num_update_vertex_errors The number of execution errors for the UpdateVertexProcessor. kv_get_latency_us The average latency of executions for the Getprocessor. kv_put_latency_us The average latency of executions for the PutProcessor. kv_remove_latency_us The average latency of executions for the RemoveProcessor. num_kv_get_errors The number of execution errors for the GetProcessor. num_kv_get The number of executions for the GetProcessor. num_kv_put_errors The number of execution errors for the PutProcessor. num_kv_put The number of executions for the PutProcessor. num_kv_remove_errors The number of execution errors for the RemoveProcessor. num_kv_remove The number of executions for the RemoveProcessor. forward_tranx_latency_us The average latency of transmission. Space-level \u00b6 Parameter Description num_active_queries The number of queries currently being executed. num_queries The number of queries. num_sentences The number of statements received by the Graphd service. optimizer_latency_us The latency of executing optimizer statements. query_latency_us The average latency of queries. num_slow_queries The number of slow queries. num_query_errors The number of query errors. num_query_errors_leader_changes The number of raft leader changes due to query errors. num_killed_queries The number of killed queries. num_aggregate_executors The number of executions for the Aggregation operator. num_sort_executors The number of executions for the Sort operator. num_indexscan_executors The number of executions for index scan operators. num_oom_queries The number of queries that caused memory to run out.","title":"Query Nebula Graph metrics"},{"location":"6.monitor-and-metrics/1.query-performance-metrics/#query_nebula_graph_metrics","text":"Nebula Graph supports querying the monitoring metrics through HTTP ports.","title":"Query Nebula Graph metrics"},{"location":"6.monitor-and-metrics/1.query-performance-metrics/#metrics_structure","text":"Each metric of Nebula Graph consists of three fields: name, type, and time range. The fields are separated by periods, for example, num_queries.sum.600 . Different Nebula Graph services (Graph, Storage, or Meta) support different metrics. The detailed description is as follows. Field Example Description Metric name num_queries Indicates the function of the metric. Metric type sum Indicates how the metrics are collected. Supported types are SUM, AVG, RATE, and the P-th sample quantiles such as P75, P95, P99, and P99.9. Time range 600 The time range in seconds for the metric collection. Supported values are 5, 60, 600, and 3600, representing the last 5 seconds, 1 minute, 10 minutes, and 1 hour.","title":"Metrics structure"},{"location":"6.monitor-and-metrics/1.query-performance-metrics/#space-level_metrics","text":"The Graph service supports a set of space-level metrics that record the information of different graph spaces separately. To enable space-level metrics, set the value of enable_space_level_metrics to true in the Graph service configuration file before starting Nebula Graph. For details about how to modify the configuration, see Configuration Management . Note Space-level metrics can be queried only by querying all metrics. For example, run curl -G \"http://192.168.8.40:19559/stats\" to show all metrics. The returned result contains the graph space name in the form of '{space=space_name}', such as num_active_queries{space=basketballplayer}.sum.5=0 .","title":"Space-level metrics"},{"location":"6.monitor-and-metrics/1.query-performance-metrics/#query_metrics_over_http","text":"","title":"Query metrics over HTTP"},{"location":"6.monitor-and-metrics/1.query-performance-metrics/#syntax","text":"curl -G \"http://<ip>:<port>/stats?stats=<metric_name_list> [&format=json]\" Parameter Description ip The IP address of the server. You can find it in the configuration file in the installation directory. port The HTTP port of the server. You can find it in the configuration file in the installation directory. The default ports are 19559 (Meta), 19669 (Graph), and 19779 (Storage). metric_name_list The metrics names. Multiple metrics are separated by commas (,). &format=json Optional. Returns the result in the JSON format. Note If Nebula Graph is deployed with Docker Compose , run docker-compose ps to check the ports that are mapped from the service ports inside of the container and then query through them.","title":"Syntax"},{"location":"6.monitor-and-metrics/1.query-performance-metrics/#examples","text":"Query a single metric Query the query number in the last 10 minutes in the Graph Service. $ curl -G \"http://192.168.8.40:19669/stats?stats=num_queries.sum.600\" num_queries.sum.600 = 400 Query multiple metrics Query the following metrics together: The average heartbeat latency in the last 1 minute. The average latency of the slowest 1% heartbeats, i.e., the P99 heartbeats, in the last 10 minutes. $ curl -G \"http://192.168.8.40:19559/stats?stats=heartbeat_latency_us.avg.60,heartbeat_latency_us.p99.600\" heartbeat_latency_us.avg.60 = 281 heartbeat_latency_us.p99.600 = 985 Return a JSON result. Query the number of new vertices in the Storage Service in the last 10 minutes and return the result in the JSON format. $ curl -G \"http://192.168.8.40:19779/stats?stats=num_add_vertices.sum.600&format=json\" [{ \"value\" :1, \"name\" : \"num_add_vertices.sum.600\" }] Query all metrics in a service. If no metric is specified in the query, Nebula Graph returns all metrics in the service. $ curl -G \"http://192.168.8.40:19559/stats\" heartbeat_latency_us.avg.5 = 304 heartbeat_latency_us.avg.60 = 308 heartbeat_latency_us.avg.600 = 299 heartbeat_latency_us.avg.3600 = 285 heartbeat_latency_us.p75.5 = 652 heartbeat_latency_us.p75.60 = 669 heartbeat_latency_us.p75.600 = 651 heartbeat_latency_us.p75.3600 = 642 heartbeat_latency_us.p95.5 = 930 heartbeat_latency_us.p95.60 = 963 heartbeat_latency_us.p95.600 = 933 heartbeat_latency_us.p95.3600 = 929 heartbeat_latency_us.p99.5 = 986 heartbeat_latency_us.p99.60 = 1409 heartbeat_latency_us.p99.600 = 989 heartbeat_latency_us.p99.3600 = 986 num_heartbeats.rate.5 = 0 num_heartbeats.rate.60 = 0 num_heartbeats.rate.600 = 0 num_heartbeats.rate.3600 = 0 num_heartbeats.sum.5 = 2 num_heartbeats.sum.60 = 40 num_heartbeats.sum.600 = 394 num_heartbeats.sum.3600 = 2364 ...","title":"Examples"},{"location":"6.monitor-and-metrics/1.query-performance-metrics/#metric_description","text":"","title":"Metric description"},{"location":"6.monitor-and-metrics/1.query-performance-metrics/#graph","text":"Parameter Description num_active_queries The number of queries currently being executed. num_active_sessions The number of currently active sessions. num_aggregate_executors The number of executions for the Aggregation operator. num_auth_failed_sessions_bad_username_password The number of sessions where authentication failed due to incorrect username and password. num_auth_failed_sessions_out_of_max_allowed The number of sessions that failed to authenticate logins because the value of the parameter FLAG_OUT_OF_MAX_ALLOWED_CONNECTIONS was exceeded. num_auth_failed_sessions The number of sessions in which login authentication failed. num_indexscan_executors The number of executions for index scan operators. num_killed_queries The number of killed queries. num_opened_sessions The number of sessions connected to the server. num_queries The number of queries. num_query_errors_leader_changes The number of the raft leader changes due to query errors. num_query_errors The number of query errors. num_reclaimed_expired_sessions The number of expired sessions actively reclaimed by the server. num_rpc_sent_to_metad_failed The number of failed RPC requests that the Graphd service sends to the Metad service. num_rpc_sent_to_metad The number of RPC requests that the Graphd service sent to the Metad service. num_rpc_sent_to_storaged_failed The number of failed RPC requests that the Graphd service sent to the Storaged service. num_rpc_sent_to_storaged The number of RPC requests that the Graphd service sent to the Storaged service. num_sentences The number of statements received by the Graphd service. num_slow_queries The number of slow queries. num_sort_executors The number of executions for the Sort operator. optimizer_latency_us The latency of executing optimizer statements. query_latency_us The average latency of queries. slow_query_latency_us The average latency of slow queries. num_queries_hit_memory_watermark The number of queries that reached the memory watermark.","title":"Graph"},{"location":"6.monitor-and-metrics/1.query-performance-metrics/#meta","text":"Parameter Description commit_log_latency_us The latency of committing logs in Raft. commit_snapshot_latency_us The latency of committing snapshots in Raft. heartbeat_latency_us The latency of heartbeats. num_heartbeats The number of heartbeats. num_raft_votes The number of votes in Raft. transfer_leader_latency_us The latency of transferring the raft leader. num_agent_heartbeats The number of heartbeats for the AgentHBProcessor. agent_heartbeat_latency_us The average latency of the AgentHBProcessor.","title":"Meta"},{"location":"6.monitor-and-metrics/1.query-performance-metrics/#storage","text":"Parameter Description add_edges_atomic_latency_us The average latency of adding edge single. add_edges_latency_us The average latency of adding edges. add_vertices_latency_us The average latency of adding vertices. commit_log_latency_us The latency of committing logs in Raft. commit_snapshot_latency_us The latency of committing snapshots in Raft. delete_edges_latency_us The average latency of deleting edges. delete_vertices_latency_us The average latency of deleting vertices. get_neighbors_latency_us The average latency of querying neighbor vertices. num_get_prop The number of executions for the GetPropProcessor. num_get_neighbors_errors The number of execution errors for the GetNeighborsProcessor. get_prop_latency_us The average latency of executions for the GetPropProcessor. num_edges_deleted The number of deleted edges. num_edges_inserted The number of inserted edges. num_raft_votes The number of votes in Raft. num_rpc_sent_to_metad_failed The number of failed RPC requests that the Storage service sent to the Meta service. num_rpc_sent_to_metad The number of RPC requests that the Storaged service sent to the Metad service. num_tags_deleted The number of deleted tags. num_vertices_deleted The number of deleted vertices. num_vertices_inserted The number of inserted vertices. transfer_leader_latency_us The latency of transferring the raft leader. lookup_latency_us The average latency of executions for the LookupProcessor. num_lookup_errors The number of execution errors for the LookupProcessor. num_scan_vertex The number of executions for the ScanVertexProcessor. num_scan_vertex_errors The number of execution errors for the ScanVertexProcessor. update_edge_latency_us The average latency of executions for the UpdateEdgeProcessor. num_update_vertex The number of executions for the UpdateVertexProcessor. num_update_vertex_errors The number of execution errors for the UpdateVertexProcessor. kv_get_latency_us The average latency of executions for the Getprocessor. kv_put_latency_us The average latency of executions for the PutProcessor. kv_remove_latency_us The average latency of executions for the RemoveProcessor. num_kv_get_errors The number of execution errors for the GetProcessor. num_kv_get The number of executions for the GetProcessor. num_kv_put_errors The number of execution errors for the PutProcessor. num_kv_put The number of executions for the PutProcessor. num_kv_remove_errors The number of execution errors for the RemoveProcessor. num_kv_remove The number of executions for the RemoveProcessor. forward_tranx_latency_us The average latency of transmission.","title":"Storage"},{"location":"6.monitor-and-metrics/1.query-performance-metrics/#space-level","text":"Parameter Description num_active_queries The number of queries currently being executed. num_queries The number of queries. num_sentences The number of statements received by the Graphd service. optimizer_latency_us The latency of executing optimizer statements. query_latency_us The average latency of queries. num_slow_queries The number of slow queries. num_query_errors The number of query errors. num_query_errors_leader_changes The number of raft leader changes due to query errors. num_killed_queries The number of killed queries. num_aggregate_executors The number of executions for the Aggregation operator. num_sort_executors The number of executions for the Sort operator. num_indexscan_executors The number of executions for index scan operators. num_oom_queries The number of queries that caused memory to run out.","title":"Space-level"},{"location":"6.monitor-and-metrics/2.rocksdb-statistics/","text":"RocksDB statistics \u00b6 Nebula Graph uses RocksDB as the underlying storage. This topic describes how to collect and show the RocksDB statistics of Nebula Graph. Enable RocksDB \u00b6 By default, the function of RocksDB statistics is disabled. To enable RocksDB statistics, you need to: Modify the --enable_rocksdb_statistics parameter as true in the nebula-storaged.conf file. The default path of the configuration file is /use/local/nebula/etc . Restart the service to make the modification valid. Get RocksDB statistics \u00b6 Users can use the built-in HTTP service in the storage service to get the following types of statistics. Results in the JSON format are supported. All RocksDB statistics. Specified RocksDB statistics. Examples \u00b6 Use the following command to get all RocksDB statistics: curl -L \"http:// ${ storage_ip } : ${ port } /rocksdb_stats\" For example: curl -L \"http://172.28.2.1:19779/rocksdb_stats\" rocksdb.blobdb.blob.file.bytes.read=0 rocksdb.blobdb.blob.file.bytes.written=0 rocksdb.blobdb.blob.file.bytes.synced=0 ... Use the following command to get specified RocksDB statistics: curl -L \"http:// ${ storage_ip } : ${ port } /rocksdb_stats?stats= ${ stats_name } \" For example, use the following command to get the information of rocksdb.bytes.read and rocksdb.block.cache.add . curl -L \"http://172.28.2.1:19779/rocksdb_stats?stats=rocksdb.bytes.read,rocksdb.block.cache.add\" rocksdb.block.cache.add = 14 rocksdb.bytes.read = 1632 Use the following command to get specified RocksDB statistics in the JSON format: curl -L \"http:// ${ storage_ip } : ${ port } /rocksdb_stats?stats= ${ stats_name } &format=json\" For example, use the following command to get the information of rocksdb.bytes.read and rocksdb.block.cache.add and return the results in the JSON format. curl -L \"http://172.28.2.1:19779/rocksdb_stats?stats=rocksdb.bytes.read,rocksdb.block.cache.add&format=json\" [ { \"rocksdb.block.cache.add\" : 1 } , { \"rocksdb.bytes.read\" : 160 } ]","title":"RocksDB Statistics"},{"location":"6.monitor-and-metrics/2.rocksdb-statistics/#rocksdb_statistics","text":"Nebula Graph uses RocksDB as the underlying storage. This topic describes how to collect and show the RocksDB statistics of Nebula Graph.","title":"RocksDB statistics"},{"location":"6.monitor-and-metrics/2.rocksdb-statistics/#enable_rocksdb","text":"By default, the function of RocksDB statistics is disabled. To enable RocksDB statistics, you need to: Modify the --enable_rocksdb_statistics parameter as true in the nebula-storaged.conf file. The default path of the configuration file is /use/local/nebula/etc . Restart the service to make the modification valid.","title":"Enable RocksDB"},{"location":"6.monitor-and-metrics/2.rocksdb-statistics/#get_rocksdb_statistics","text":"Users can use the built-in HTTP service in the storage service to get the following types of statistics. Results in the JSON format are supported. All RocksDB statistics. Specified RocksDB statistics.","title":"Get RocksDB statistics"},{"location":"6.monitor-and-metrics/2.rocksdb-statistics/#examples","text":"Use the following command to get all RocksDB statistics: curl -L \"http:// ${ storage_ip } : ${ port } /rocksdb_stats\" For example: curl -L \"http://172.28.2.1:19779/rocksdb_stats\" rocksdb.blobdb.blob.file.bytes.read=0 rocksdb.blobdb.blob.file.bytes.written=0 rocksdb.blobdb.blob.file.bytes.synced=0 ... Use the following command to get specified RocksDB statistics: curl -L \"http:// ${ storage_ip } : ${ port } /rocksdb_stats?stats= ${ stats_name } \" For example, use the following command to get the information of rocksdb.bytes.read and rocksdb.block.cache.add . curl -L \"http://172.28.2.1:19779/rocksdb_stats?stats=rocksdb.bytes.read,rocksdb.block.cache.add\" rocksdb.block.cache.add = 14 rocksdb.bytes.read = 1632 Use the following command to get specified RocksDB statistics in the JSON format: curl -L \"http:// ${ storage_ip } : ${ port } /rocksdb_stats?stats= ${ stats_name } &format=json\" For example, use the following command to get the information of rocksdb.bytes.read and rocksdb.block.cache.add and return the results in the JSON format. curl -L \"http://172.28.2.1:19779/rocksdb_stats?stats=rocksdb.bytes.read,rocksdb.block.cache.add&format=json\" [ { \"rocksdb.block.cache.add\" : 1 } , { \"rocksdb.bytes.read\" : 160 } ]","title":"Examples"},{"location":"7.data-security/4.ssl/","text":"SSL encryption \u00b6 Nebula Graph supports data transmission with SSL encryption between clients, the Graph service, the Meta service, and the Storage service. This topic describes how to enable SSL encryption. Precaution \u00b6 Enabling SSL encryption will slightly affect the performance, such as causing operation latency. Parameters \u00b6 Parameter Default value Description cert_path - The path to the PEM certification. key_path - The path to the key certification. password_path - The path to the password file certification. ca_path - The path to the trusted CA file. enable_ssl false Whether to enable SSL encryption. enable_graph_ssl false Whether to enable SSL encryption in the Graph service only. enable_meta_ssl false Whether to enable SSL encryption in the Meta service only. Certificate modes \u00b6 To use SSL encryption, SSL certificates are required. Nebula Graph supports two certificate modes. Self-signed certificate mode In this mode, users need to make the signed certificate by themselves and set cert_path , key_path , and password_path in the corresponding file according to encryption policies. CA-signed certificate mode In this mode, users need to apply for the signed certificate from a certificate authority and set cert_path , key_path , and password_path in the corresponding file according to encryption policies. Encryption policies \u00b6 Nebula Graph supports three encryption policies. For details, see Usage explanation . Encrypt the data transmission between clients, the Graph service, the Meta service, and the Storage service. Add enable_ssl = true to the configuration files of nebula-graphd.conf , nebula-metad.conf , and nebula-storaged.conf . Encrypt the data transmission between clients and the Graph service. This policy applies to the case that the clusters are set in the same server room. Only the port of the Graph service is open to the outside because other services can communicate over the internal network without encryption. Add enable_graph_ssl = true to the configuration file of nebula-graphd.conf . Encrypt the data transmission related to the Meta service in the cluster. This policy applies to transporting classified information to the Meta service. Add enable_meta_ssl = true to the configuration files of nebula-graphd.conf , nebula-metad.conf , and nebula-storaged.conf . Steps \u00b6 Ensure the certificate mode and the encryption policy. Add the certificate configuration and the policy configuration in corresponding files. For example, the three configuration files need to be set as follows when using a self-signed certificate and encrypt data transmission between clients, the Graph service, the Meta service, and the Storage service. --cert_path = xxxxxx --key_path = xxxxx --password_path = xxxxxx --enable_ssl = true Set the SSL and the trusted CA in clients. For code examples, see nebula-test-run.py .","title":"SSL"},{"location":"7.data-security/4.ssl/#ssl_encryption","text":"Nebula Graph supports data transmission with SSL encryption between clients, the Graph service, the Meta service, and the Storage service. This topic describes how to enable SSL encryption.","title":"SSL encryption"},{"location":"7.data-security/4.ssl/#precaution","text":"Enabling SSL encryption will slightly affect the performance, such as causing operation latency.","title":"Precaution"},{"location":"7.data-security/4.ssl/#parameters","text":"Parameter Default value Description cert_path - The path to the PEM certification. key_path - The path to the key certification. password_path - The path to the password file certification. ca_path - The path to the trusted CA file. enable_ssl false Whether to enable SSL encryption. enable_graph_ssl false Whether to enable SSL encryption in the Graph service only. enable_meta_ssl false Whether to enable SSL encryption in the Meta service only.","title":"Parameters"},{"location":"7.data-security/4.ssl/#certificate_modes","text":"To use SSL encryption, SSL certificates are required. Nebula Graph supports two certificate modes. Self-signed certificate mode In this mode, users need to make the signed certificate by themselves and set cert_path , key_path , and password_path in the corresponding file according to encryption policies. CA-signed certificate mode In this mode, users need to apply for the signed certificate from a certificate authority and set cert_path , key_path , and password_path in the corresponding file according to encryption policies.","title":"Certificate modes"},{"location":"7.data-security/4.ssl/#encryption_policies","text":"Nebula Graph supports three encryption policies. For details, see Usage explanation . Encrypt the data transmission between clients, the Graph service, the Meta service, and the Storage service. Add enable_ssl = true to the configuration files of nebula-graphd.conf , nebula-metad.conf , and nebula-storaged.conf . Encrypt the data transmission between clients and the Graph service. This policy applies to the case that the clusters are set in the same server room. Only the port of the Graph service is open to the outside because other services can communicate over the internal network without encryption. Add enable_graph_ssl = true to the configuration file of nebula-graphd.conf . Encrypt the data transmission related to the Meta service in the cluster. This policy applies to transporting classified information to the Meta service. Add enable_meta_ssl = true to the configuration files of nebula-graphd.conf , nebula-metad.conf , and nebula-storaged.conf .","title":"Encryption policies"},{"location":"7.data-security/4.ssl/#steps","text":"Ensure the certificate mode and the encryption policy. Add the certificate configuration and the policy configuration in corresponding files. For example, the three configuration files need to be set as follows when using a self-signed certificate and encrypt data transmission between clients, the Graph service, the Meta service, and the Storage service. --cert_path = xxxxxx --key_path = xxxxx --password_path = xxxxxx --enable_ssl = true Set the SSL and the trusted CA in clients. For code examples, see nebula-test-run.py .","title":"Steps"},{"location":"7.data-security/1.authentication/1.authentication/","text":"Authentication \u00b6 Nebula Graph replies on local authentication or LDAP authentication to implement access control. Nebula Graph creates a session when a client connects to it. The session stores information about the connection, including the user information. If the authentication system is enabled, the session will be mapped to corresponding users. Note By default, the authentication is disabled and Nebula Graph allows connections with the username root and any password. Nebula Graph supports local authentication and LDAP authentication. Local authentication \u00b6 Local authentication indicates that usernames and passwords are stored locally on the server, with the passwords encrypted. Users will be authenticated when trying to visit Nebula Graph. Enable local authentication \u00b6 Modify the nebula-graphd.conf file ( /usr/local/nebula/etc/ is the default path) to set the following parameters: --enable_authorize : Set its value to true to enable authentication. --failed_login_attempts : This parameter is optional, and you need to add this parameter manually. Specify the attempts of continuously entering incorrect passwords for a single Graph service. When the number exceeds the limitation, your account will be locked. For multiple Graph services, the allowed attempts are number of services * failed_login_attempts . --password_lock_time_in_secs : This parameter is optional, and you need to add this parameter manually. Specify the time how long your account is locked after multiple incorrect password entries are entered. Unit: second. Restart the Nebula Graph services. For how to restart, see Manage Nebula Graph services . Note You can use the username root and password nebula to log into Nebula Graph after enabling local authentication. This account has the build-in God role. For more information about roles, see Roles and privileges . LDAP authentication \u00b6 Lightweight Directory Access Protocol (LDAP) is a lightweight client-server protocol for accessing directories and building a centralized account management system. LDAP authentication and local authentication can be enabled at the same time, but LDAP authentication has a higher priority. If the local authentication server and the LDAP server both have the information of user Amber , Nebula Graph reads from the LDAP server first. Enable LDAP authentication \u00b6 Enterpriseonly Contact inquiry@vesoft.com.","title":"Authentication"},{"location":"7.data-security/1.authentication/1.authentication/#authentication","text":"Nebula Graph replies on local authentication or LDAP authentication to implement access control. Nebula Graph creates a session when a client connects to it. The session stores information about the connection, including the user information. If the authentication system is enabled, the session will be mapped to corresponding users. Note By default, the authentication is disabled and Nebula Graph allows connections with the username root and any password. Nebula Graph supports local authentication and LDAP authentication.","title":"Authentication"},{"location":"7.data-security/1.authentication/1.authentication/#local_authentication","text":"Local authentication indicates that usernames and passwords are stored locally on the server, with the passwords encrypted. Users will be authenticated when trying to visit Nebula Graph.","title":"Local authentication"},{"location":"7.data-security/1.authentication/1.authentication/#enable_local_authentication","text":"Modify the nebula-graphd.conf file ( /usr/local/nebula/etc/ is the default path) to set the following parameters: --enable_authorize : Set its value to true to enable authentication. --failed_login_attempts : This parameter is optional, and you need to add this parameter manually. Specify the attempts of continuously entering incorrect passwords for a single Graph service. When the number exceeds the limitation, your account will be locked. For multiple Graph services, the allowed attempts are number of services * failed_login_attempts . --password_lock_time_in_secs : This parameter is optional, and you need to add this parameter manually. Specify the time how long your account is locked after multiple incorrect password entries are entered. Unit: second. Restart the Nebula Graph services. For how to restart, see Manage Nebula Graph services . Note You can use the username root and password nebula to log into Nebula Graph after enabling local authentication. This account has the build-in God role. For more information about roles, see Roles and privileges .","title":"Enable local authentication"},{"location":"7.data-security/1.authentication/1.authentication/#ldap_authentication","text":"Lightweight Directory Access Protocol (LDAP) is a lightweight client-server protocol for accessing directories and building a centralized account management system. LDAP authentication and local authentication can be enabled at the same time, but LDAP authentication has a higher priority. If the local authentication server and the LDAP server both have the information of user Amber , Nebula Graph reads from the LDAP server first.","title":"LDAP authentication"},{"location":"7.data-security/1.authentication/1.authentication/#enable_ldap_authentication","text":"Enterpriseonly Contact inquiry@vesoft.com.","title":"Enable LDAP authentication"},{"location":"7.data-security/1.authentication/2.management-user/","text":"User management \u00b6 User management is an indispensable part of Nebula Graph access control. This topic describes how to manage users and roles. After enabling authentication , only valid users can connect to Nebula Graph and access the resources according to the user roles . Note By default, the authentication is disabled. Nebula Graph allows connections with the username root and any password. Once the role of a user is modified, the user has to re-login to make the new role takes effect. CREATE USER \u00b6 The root user with the GOD role can run CREATE USER to create a new user. Syntax CREATE USER [IF NOT EXISTS] <user_name> [WITH PASSWORD '<password>'] [WITH IP WHITELIST <ip_list>]; IF NOT EXISTS : Detects if the user name exists. The user will be created only if the user name does not exist. user_name : Sets the name of the user. password : Sets the password of the user. ip_list (Enterprise): Sets the IP address whitelist. The user can connect to Nebula Graph only from IP addresses in the list. Use commas to separate multiple IP addresses. Example nebula> CREATE USER user1 WITH PASSWORD 'nebula'; nebula> CREATE USER user2 WITH PASSWORD 'nebula' WITH IP WHITELIST 192.168.10.10,192.168.10.12; nebula> SHOW USERS; +---------+-------------------------------+ | Account | IP Whitelist | +---------+-------------------------------+ | \"root\" | \"\" | | \"user1\" | \"\" | | \"user2\" | \"192.168.10.10,192.168.10.12\" | +---------+-------------------------------+ GRANT ROLE \u00b6 Users with the GOD role or the ADMIN role can run GRANT ROLE to assign a built-in role in a graph space to a user. For more information about Nebula Graph built-in roles, see Roles and privileges . Syntax GRANT ROLE <role_type> ON <space_name> TO <user_name>; Example nebula> GRANT ROLE USER ON basketballplayer TO user1; REVOKE ROLE \u00b6 Users with the GOD role or the ADMIN role can run REVOKE ROLE to revoke the built-in role of a user in a graph space. For more information about Nebula Graph built-in roles, see Roles and privileges . Syntax REVOKE ROLE <role_type> ON <space_name> FROM <user_name>; Example nebula> REVOKE ROLE USER ON basketballplayer FROM user1; DESCRIBE USER \u00b6 Users can run DESCRIBE USER to list the roles for a specified user. Syntax DESCRIBE USER <user_name>; DESC USER <user_name>; Example nebula> DESCRIBE USER user1; +---------+--------------------+ | role | space | +---------+--------------------+ | \"ADMIN\" | \"basketballplayer\" | +---------+--------------------+ SHOW ROLES \u00b6 Users can run SHOW ROLES to list the roles in a graph space. Syntax SHOW ROLES IN <space_name>; Example nebula> SHOW ROLES IN basketballplayer; +---------+-----------+ | Account | Role Type | +---------+-----------+ | \"user1\" | \"ADMIN\" | +---------+-----------+ CHANGE PASSWORD \u00b6 Users can run CHANGE PASSWORD to set a new password for a user. The old password is needed when setting a new one. Syntax CHANGE PASSWORD <user_name> FROM '<old_password>' TO '<new_password>'; Example nebula> CHANGE PASSWORD user1 FROM 'nebula' TO 'nebula123'; ALTER USER \u00b6 The root user with the GOD role can run ALTER USER to set a new password and IP address whitelist for a user. The old password is not needed when altering the user. Syntax ALTER USER <user_name> WITH PASSWORD '<password>' [WITH IP WHITELIST <ip_list>];; Example Enterpriseonly When WITH IP WHITELIST is not used, the IP address whitelist is removed and the user can connect to the Nebula Graph by any IP address. nebula> ALTER USER user2 WITH PASSWORD 'nebula'; nebula> SHOW USERS; +---------+--------------+ | Account | IP Whitelist | +---------+--------------+ | \"root\" | \"\" | | \"user1\" | \"\" | | \"user2\" | \"\" | +---------+--------------+ nebula> ALTER USER user2 WITH PASSWORD 'nebula' WITH IP WHITELIST 192.168.10.10; DROP USER \u00b6 The root user with the GOD role can run DROP USER to remove a user. Note Removing a user does not close the current session of the user, and the user role still takes effect in the session until the session is closed. Syntax DROP USER [IF EXISTS] <user_name>; Example nebula> DROP USER user1; SHOW USERS \u00b6 The root user with the GOD role can run SHOW USERS to list all the users. Syntax SHOW USERS; Example nebula> SHOW USERS; +---------+-----------------+ | Account | IP Whitelist | +---------+-----------------+ | \"root\" | \"\" | | \"user1\" | \"\" | | \"user2\" | \"192.168.10.10\" | +---------+-----------------+","title":"User management"},{"location":"7.data-security/1.authentication/2.management-user/#user_management","text":"User management is an indispensable part of Nebula Graph access control. This topic describes how to manage users and roles. After enabling authentication , only valid users can connect to Nebula Graph and access the resources according to the user roles . Note By default, the authentication is disabled. Nebula Graph allows connections with the username root and any password. Once the role of a user is modified, the user has to re-login to make the new role takes effect.","title":"User management"},{"location":"7.data-security/1.authentication/2.management-user/#create_user","text":"The root user with the GOD role can run CREATE USER to create a new user. Syntax CREATE USER [IF NOT EXISTS] <user_name> [WITH PASSWORD '<password>'] [WITH IP WHITELIST <ip_list>]; IF NOT EXISTS : Detects if the user name exists. The user will be created only if the user name does not exist. user_name : Sets the name of the user. password : Sets the password of the user. ip_list (Enterprise): Sets the IP address whitelist. The user can connect to Nebula Graph only from IP addresses in the list. Use commas to separate multiple IP addresses. Example nebula> CREATE USER user1 WITH PASSWORD 'nebula'; nebula> CREATE USER user2 WITH PASSWORD 'nebula' WITH IP WHITELIST 192.168.10.10,192.168.10.12; nebula> SHOW USERS; +---------+-------------------------------+ | Account | IP Whitelist | +---------+-------------------------------+ | \"root\" | \"\" | | \"user1\" | \"\" | | \"user2\" | \"192.168.10.10,192.168.10.12\" | +---------+-------------------------------+","title":"CREATE USER"},{"location":"7.data-security/1.authentication/2.management-user/#grant_role","text":"Users with the GOD role or the ADMIN role can run GRANT ROLE to assign a built-in role in a graph space to a user. For more information about Nebula Graph built-in roles, see Roles and privileges . Syntax GRANT ROLE <role_type> ON <space_name> TO <user_name>; Example nebula> GRANT ROLE USER ON basketballplayer TO user1;","title":"GRANT ROLE"},{"location":"7.data-security/1.authentication/2.management-user/#revoke_role","text":"Users with the GOD role or the ADMIN role can run REVOKE ROLE to revoke the built-in role of a user in a graph space. For more information about Nebula Graph built-in roles, see Roles and privileges . Syntax REVOKE ROLE <role_type> ON <space_name> FROM <user_name>; Example nebula> REVOKE ROLE USER ON basketballplayer FROM user1;","title":"REVOKE ROLE"},{"location":"7.data-security/1.authentication/2.management-user/#describe_user","text":"Users can run DESCRIBE USER to list the roles for a specified user. Syntax DESCRIBE USER <user_name>; DESC USER <user_name>; Example nebula> DESCRIBE USER user1; +---------+--------------------+ | role | space | +---------+--------------------+ | \"ADMIN\" | \"basketballplayer\" | +---------+--------------------+","title":"DESCRIBE USER"},{"location":"7.data-security/1.authentication/2.management-user/#show_roles","text":"Users can run SHOW ROLES to list the roles in a graph space. Syntax SHOW ROLES IN <space_name>; Example nebula> SHOW ROLES IN basketballplayer; +---------+-----------+ | Account | Role Type | +---------+-----------+ | \"user1\" | \"ADMIN\" | +---------+-----------+","title":"SHOW ROLES"},{"location":"7.data-security/1.authentication/2.management-user/#change_password","text":"Users can run CHANGE PASSWORD to set a new password for a user. The old password is needed when setting a new one. Syntax CHANGE PASSWORD <user_name> FROM '<old_password>' TO '<new_password>'; Example nebula> CHANGE PASSWORD user1 FROM 'nebula' TO 'nebula123';","title":"CHANGE PASSWORD"},{"location":"7.data-security/1.authentication/2.management-user/#alter_user","text":"The root user with the GOD role can run ALTER USER to set a new password and IP address whitelist for a user. The old password is not needed when altering the user. Syntax ALTER USER <user_name> WITH PASSWORD '<password>' [WITH IP WHITELIST <ip_list>];; Example Enterpriseonly When WITH IP WHITELIST is not used, the IP address whitelist is removed and the user can connect to the Nebula Graph by any IP address. nebula> ALTER USER user2 WITH PASSWORD 'nebula'; nebula> SHOW USERS; +---------+--------------+ | Account | IP Whitelist | +---------+--------------+ | \"root\" | \"\" | | \"user1\" | \"\" | | \"user2\" | \"\" | +---------+--------------+ nebula> ALTER USER user2 WITH PASSWORD 'nebula' WITH IP WHITELIST 192.168.10.10;","title":"ALTER USER"},{"location":"7.data-security/1.authentication/2.management-user/#drop_user","text":"The root user with the GOD role can run DROP USER to remove a user. Note Removing a user does not close the current session of the user, and the user role still takes effect in the session until the session is closed. Syntax DROP USER [IF EXISTS] <user_name>; Example nebula> DROP USER user1;","title":"DROP USER"},{"location":"7.data-security/1.authentication/2.management-user/#show_users","text":"The root user with the GOD role can run SHOW USERS to list all the users. Syntax SHOW USERS; Example nebula> SHOW USERS; +---------+-----------------+ | Account | IP Whitelist | +---------+-----------------+ | \"root\" | \"\" | | \"user1\" | \"\" | | \"user2\" | \"192.168.10.10\" | +---------+-----------------+","title":"SHOW USERS"},{"location":"7.data-security/1.authentication/3.role-list/","text":"Roles and privileges \u00b6 A role is a collection of privileges. You can assign a role to a user for access control. Built-in roles \u00b6 Nebula Graph does not support custom roles, but it has multiple built-in roles: GOD GOD is the original role with all privileges not limited to graph spaces. It is similar to root in Linux and administrator in Windows. When the Meta Service is initialized, the one and only GOD role user root is automatically created with the password nebula . Caution Modify the password for root timely for security. One cluster can only have one user with the GOD role. This user can manage all graph spaces in a cluster. Manual authorization of the God role is not supported. Only the root user with the default God role can be used. ADMIN An ADMIN role can read and write both the Schema and the data in a specific graph space. An ADMIN role of a graph space can grant DBA, USER, and GUEST roles in the graph space to other users. Note Only roles lower than ADMIN can be authorized to other users. DBA A DBA role can read and write both the Schema and the data in a specific graph space. A DBA role of a graph space CANNOT grant roles to other users. USER A USER role can read and write data in a specific graph space. The Schema information is read-only to the USER roles in a graph space. GUEST A GUEST role can only read the Schema and the data in a specific graph space. Note Nebula Graph does not support custom roles. Users can only use the default built-in roles. A user can have only one role in a graph space. For authenticated users, see User management . Role privileges and allowed nGQL \u00b6 The privileges of roles and the nGQL statements that each role can use are listed as follows. Privilege God Admin DBA User Guest Allowed nGQL Read space Y Y Y Y Y USE , DESCRIBE SPACE Read schema Y Y Y Y Y DESCRIBE TAG , DESCRIBE EDGE , DESCRIBE TAG INDEX , DESCRIBE EDGE INDEX Write schema Y Y Y CREATE TAG , ALTER TAG , CREATE EDGE , ALTER EDGE , DROP TAG , DELETE TAG , DROP EDGE , CREATE TAG INDEX , CREATE EDGE INDEX , DROP TAG INDEX , DROP EDGE INDEX Write user Y CREATE USER , DROP USER , ALTER USER Write role Y Y GRANT , REVOKE Read data Y Y Y Y Y GO , SET , PIPE , MATCH , ASSIGNMENT , LOOKUP , YIELD , ORDER BY , FETCH VERTICES , Find , FETCH EDGES , FIND PATH , LIMIT , GROUP BY , RETURN Write data Y Y Y Y INSERT VERTEX , UPDATE VERTEX , INSERT EDGE , UPDATE EDGE , DELETE VERTEX , DELETE EDGES , DELETE TAG Show operations Y Y Y Y Y SHOW , CHANGE PASSWORD Job Y Y Y Y SUBMIT JOB COMPACT , SUBMIT JOB FLUSH , SUBMIT JOB STATS , STOP JOB , RECOVER JOB , BUILD TAG INDEX , BUILD EDGE INDEX \u3001 INGEST , DOWNLOAD Write space Y CREATE SPACE , DROP SPACE , CREATE SNAPSHOT , DROP SNAPSHOT , BALANCE , ADMIN , CONFIG Caution The results of SHOW operations are limited to the role of a user. For example, all users can run SHOW SPACES , but the results only include the graph spaces that the users have privileges. Only the GOD role can run SHOW USERS and SHOW SNAPSHOTS .","title":"Roles and privileges"},{"location":"7.data-security/1.authentication/3.role-list/#roles_and_privileges","text":"A role is a collection of privileges. You can assign a role to a user for access control.","title":"Roles and privileges"},{"location":"7.data-security/1.authentication/3.role-list/#built-in_roles","text":"Nebula Graph does not support custom roles, but it has multiple built-in roles: GOD GOD is the original role with all privileges not limited to graph spaces. It is similar to root in Linux and administrator in Windows. When the Meta Service is initialized, the one and only GOD role user root is automatically created with the password nebula . Caution Modify the password for root timely for security. One cluster can only have one user with the GOD role. This user can manage all graph spaces in a cluster. Manual authorization of the God role is not supported. Only the root user with the default God role can be used. ADMIN An ADMIN role can read and write both the Schema and the data in a specific graph space. An ADMIN role of a graph space can grant DBA, USER, and GUEST roles in the graph space to other users. Note Only roles lower than ADMIN can be authorized to other users. DBA A DBA role can read and write both the Schema and the data in a specific graph space. A DBA role of a graph space CANNOT grant roles to other users. USER A USER role can read and write data in a specific graph space. The Schema information is read-only to the USER roles in a graph space. GUEST A GUEST role can only read the Schema and the data in a specific graph space. Note Nebula Graph does not support custom roles. Users can only use the default built-in roles. A user can have only one role in a graph space. For authenticated users, see User management .","title":"Built-in roles"},{"location":"7.data-security/1.authentication/3.role-list/#role_privileges_and_allowed_ngql","text":"The privileges of roles and the nGQL statements that each role can use are listed as follows. Privilege God Admin DBA User Guest Allowed nGQL Read space Y Y Y Y Y USE , DESCRIBE SPACE Read schema Y Y Y Y Y DESCRIBE TAG , DESCRIBE EDGE , DESCRIBE TAG INDEX , DESCRIBE EDGE INDEX Write schema Y Y Y CREATE TAG , ALTER TAG , CREATE EDGE , ALTER EDGE , DROP TAG , DELETE TAG , DROP EDGE , CREATE TAG INDEX , CREATE EDGE INDEX , DROP TAG INDEX , DROP EDGE INDEX Write user Y CREATE USER , DROP USER , ALTER USER Write role Y Y GRANT , REVOKE Read data Y Y Y Y Y GO , SET , PIPE , MATCH , ASSIGNMENT , LOOKUP , YIELD , ORDER BY , FETCH VERTICES , Find , FETCH EDGES , FIND PATH , LIMIT , GROUP BY , RETURN Write data Y Y Y Y INSERT VERTEX , UPDATE VERTEX , INSERT EDGE , UPDATE EDGE , DELETE VERTEX , DELETE EDGES , DELETE TAG Show operations Y Y Y Y Y SHOW , CHANGE PASSWORD Job Y Y Y Y SUBMIT JOB COMPACT , SUBMIT JOB FLUSH , SUBMIT JOB STATS , STOP JOB , RECOVER JOB , BUILD TAG INDEX , BUILD EDGE INDEX \u3001 INGEST , DOWNLOAD Write space Y CREATE SPACE , DROP SPACE , CREATE SNAPSHOT , DROP SNAPSHOT , BALANCE , ADMIN , CONFIG Caution The results of SHOW operations are limited to the role of a user. For example, all users can run SHOW SPACES , but the results only include the graph spaces that the users have privileges. Only the GOD role can run SHOW USERS and SHOW SNAPSHOTS .","title":"Role privileges and allowed nGQL"},{"location":"7.data-security/1.authentication/4.ldap/","text":"OpenLDAP authentication \u00b6 This topic introduces how to connect Nebula Graph to the OpenLDAP server and use the DN (Distinguished Name) and password defined in OpenLDAP for authentication. Enterpriseonly This feature is supported by the Enterprise Edition only. Authentication method \u00b6 After the OpenLDAP authentication is enabled and users log into Nebula Graph with the account and password, Nebula Graph checks whether the login account exists in the Meta service. If the account exists, Nebula Graph finds the corresponding DN in OpenLDAP according to the authentication method and verifies the password. OpenLDAP supports two authentication methods: simple bind authentication (SimpleBindAuth) and search bind authentication (SearchBindAuth). SimpleBindAuth \u00b6 Simple bind authentication splices the login account and the configuration information of Graph services into a DN that can be recognized by OpenLDAP, and then authenticates on OpenLDAP based on the DN and password. SearchBindAuth \u00b6 Search bind authentication reads the Graph service configuration information and queries whether the uid in the configuration matches the login account. If they match, search bind authentication reads the DN, and then uses the DN and password to verify on OpenLDAP. Prerequisites \u00b6 OpenLDAP is installed. The account and password are imported on OpenLDAP. The server where OpenLDAP is located has opened the corresponding authentication port. Procedures \u00b6 Take the existing account test2 and password passwdtest2 on OpenLDAP as an example. Connect to Nebula Graph , create and authorize the shadow account test2 corresponding to OpenLDAP. nebula> CREATE USER test2 WITH PASSWORD ''; nebula> GRANT ROLE ADMIN ON basketballplayer TO test2; !!! note When creating an account in Nebula Graph, the password can be set arbitrarily. Edit the configuration file nebula-graphd.conf (The default path is /usr/local/nebula/etc/ ): SimpleBindAuth (Recommended) # Whether to get the configuration information from the configuration file. --local_config = true # Whether to enable authentication. --enable_authorize = true # Authentication methods include password, ldap, and cloud. --auth_type = ldap # The address of the OpenLDAP server. --ldap_server = 192 .168.8.211 # The port of the OpenLDAP server. --ldap_port = 389 # The name of the Schema in OpenLDAP. --ldap_scheme = ldap # The prefix of DN. --ldap_prefix = uid = # The suffix of DN. --ldap_suffix = ,ou = it,dc = sys,dc = com SearchBindAuth # Whether to get the configuration information from the configuration file. --local_config = true # Whether to enable authentication. --enable_authorize = true # Authentication methods include password, ldap, and cloud. --auth_type = ldap # The address of the OpenLDAP server. --ldap_server = 192 .168.8.211 # The port of the OpenLDAP server. --ldap_port = 389 # The name of the Schema in OpenLDAP. --ldap_scheme = ldap # The DN that binds the target. --ldap_basedn = ou = it,dc = sys,dc = com Restart Nebula Graph services to make the new configuration valid. Run the login test. $ ./nebula-console --addr 127 .0.0.1 --port 9669 -u test2 -p passwdtest2 2021 /09/08 03 :49:39 [ INFO ] connection pool is initialized successfully Welcome to Nebula Graph! !!! note After using OpenLDAP for authentication, local users (including `root`) cannot log in normally.","title":"OpenLDAP authentication"},{"location":"7.data-security/1.authentication/4.ldap/#openldap_authentication","text":"This topic introduces how to connect Nebula Graph to the OpenLDAP server and use the DN (Distinguished Name) and password defined in OpenLDAP for authentication. Enterpriseonly This feature is supported by the Enterprise Edition only.","title":"OpenLDAP authentication"},{"location":"7.data-security/1.authentication/4.ldap/#authentication_method","text":"After the OpenLDAP authentication is enabled and users log into Nebula Graph with the account and password, Nebula Graph checks whether the login account exists in the Meta service. If the account exists, Nebula Graph finds the corresponding DN in OpenLDAP according to the authentication method and verifies the password. OpenLDAP supports two authentication methods: simple bind authentication (SimpleBindAuth) and search bind authentication (SearchBindAuth).","title":"Authentication method"},{"location":"7.data-security/1.authentication/4.ldap/#simplebindauth","text":"Simple bind authentication splices the login account and the configuration information of Graph services into a DN that can be recognized by OpenLDAP, and then authenticates on OpenLDAP based on the DN and password.","title":"SimpleBindAuth"},{"location":"7.data-security/1.authentication/4.ldap/#searchbindauth","text":"Search bind authentication reads the Graph service configuration information and queries whether the uid in the configuration matches the login account. If they match, search bind authentication reads the DN, and then uses the DN and password to verify on OpenLDAP.","title":"SearchBindAuth"},{"location":"7.data-security/1.authentication/4.ldap/#prerequisites","text":"OpenLDAP is installed. The account and password are imported on OpenLDAP. The server where OpenLDAP is located has opened the corresponding authentication port.","title":"Prerequisites"},{"location":"7.data-security/1.authentication/4.ldap/#procedures","text":"Take the existing account test2 and password passwdtest2 on OpenLDAP as an example. Connect to Nebula Graph , create and authorize the shadow account test2 corresponding to OpenLDAP. nebula> CREATE USER test2 WITH PASSWORD ''; nebula> GRANT ROLE ADMIN ON basketballplayer TO test2; !!! note When creating an account in Nebula Graph, the password can be set arbitrarily. Edit the configuration file nebula-graphd.conf (The default path is /usr/local/nebula/etc/ ): SimpleBindAuth (Recommended) # Whether to get the configuration information from the configuration file. --local_config = true # Whether to enable authentication. --enable_authorize = true # Authentication methods include password, ldap, and cloud. --auth_type = ldap # The address of the OpenLDAP server. --ldap_server = 192 .168.8.211 # The port of the OpenLDAP server. --ldap_port = 389 # The name of the Schema in OpenLDAP. --ldap_scheme = ldap # The prefix of DN. --ldap_prefix = uid = # The suffix of DN. --ldap_suffix = ,ou = it,dc = sys,dc = com SearchBindAuth # Whether to get the configuration information from the configuration file. --local_config = true # Whether to enable authentication. --enable_authorize = true # Authentication methods include password, ldap, and cloud. --auth_type = ldap # The address of the OpenLDAP server. --ldap_server = 192 .168.8.211 # The port of the OpenLDAP server. --ldap_port = 389 # The name of the Schema in OpenLDAP. --ldap_scheme = ldap # The DN that binds the target. --ldap_basedn = ou = it,dc = sys,dc = com Restart Nebula Graph services to make the new configuration valid. Run the login test. $ ./nebula-console --addr 127 .0.0.1 --port 9669 -u test2 -p passwdtest2 2021 /09/08 03 :49:39 [ INFO ] connection pool is initialized successfully Welcome to Nebula Graph! !!! note After using OpenLDAP for authentication, local users (including `root`) cannot log in normally.","title":"Procedures"},{"location":"8.service-tuning/2.graph-modeling/","text":"Graph data modeling suggestions \u00b6 This topic provides general suggestions for modeling data in Nebula Graph. Note The following suggestions may not apply to some special scenarios. In these cases, find help in the Nebula Graph community . Model for performance \u00b6 There is no perfect method to model in Nebula Graph. Graph modeling depends on the questions that you want to know from the data. Your data drives your graph model. Graph data modeling is intuitive and convenient. Create your data model based on your business model. Test your model and gradually optimize it to fit your business. To get better performance, you can change or re-design your model multiple times. Design and evaluate the most important queries \u00b6 Usually, various types of queries are validated in test scenarios to assess the overall capabilities of the system. However, in most production scenarios, there are not many types of frequently used queries. You can optimize the data model based on key queries selected according to the Pareto (80/20) principle. Full-graph scanning avoidance \u00b6 Graph traversal can be performed after one or more vertices/edges are located through property indexes or VIDs. But for some query patterns, such as subgraph and path query patterns, the source vertex or edge of the traversal cannot be located through property indexes or VIDs. These queries find all the subgraphs that satisfy the query pattern by scanning the whole graph space which will have poor query performance. Nebula Graph does not implement indexing for the graph structures of subgraphs or paths. No predefined bonds between Tags and Edge types \u00b6 Define the bonds between Tags and Edge types in the application, not Nebula Graph. There are no statements that could get the bonds between Tags and Edge types. Tags/Edge types predefine a set of properties \u00b6 While creating Tags or Edge types, you need to define a set of properties. Properties are part of the Nebula Graph Schema. Control changes in the business model and the data model \u00b6 Changes here refer to changes in business models and data models (meta-information), not changes in the data itself. Some graph databases are designed to be Schema-free, so their data modeling, including the modeling of the graph topology and properties, can be very flexible. Properties can be re-modeled to graph topology, and vice versa. Such systems are often specifically optimized for graph topology access. Nebula Graph 3.1.0 is a strong-Schema (row storage) system, which means that the business model should not change frequently. For example, the property Schema should not change. It is similar to avoiding ALTER TABLE in MySQL. On the contrary, vertices and their edges can be added or deleted at low costs. Thus, the easy-to-change part of the business model should be transformed to vertices or edges, rather than properties. For example, in a business model, people have relatively fixed properties such as age, gender, and name. But their contact, place of visit, trade account, and login device are often changing. The former is suitable for modeling as properties and the latter as vertices or edges. Set temporary properties through self-loop edges \u00b6 As a strong Schema system, Nebula Graph does not support List-type properties. And using ALTER TAG costs too much. If you need to add some temporary properties or List-type properties to a vertex, you can first create an edge type with the required properties, and then insert one or more edges that direct to the vertex itself. The figure is as follows. To retrieve temporary properties of vertices, fetch from self-loop edges. For example: //Create the edge type and insert the loop property. nebula> CREATE EDGE IF NOT EXISTS temp(tmp int); nebula> INSERT EDGE temp(tmp) VALUES \"player100\"->\"player100\"@1:(1); nebula> INSERT EDGE temp(tmp) VALUES \"player100\"->\"player100\"@2:(2); nebula> INSERT EDGE temp(tmp) VALUES \"player100\"->\"player100\"@3:(3); //After the data is inserted, you can query the loop property by general query statements, for example: nebula> GO FROM \"player100\" OVER temp YIELD properties(edge).tmp; +----------------------+ | properties(EDGE).tmp | +----------------------+ | 1 | | 2 | | 3 | +----------------------+ //If you want the results to be returned in the form of a List, you can use a function, for example: nebula> MATCH (v1:player)-[e:temp]->() return collect(e.tmp); +----------------+ | collect(e.tmp) | +----------------+ | [1, 2, 3] | +----------------+ About dangling edges \u00b6 A dangling edge is an edge that only connects to a single vertex and only one part of the edge connects to the vertex. In Nebula Graph 3.1.0, dangling edges may appear in the following two cases. Insert edges with INSERT EDGE statement before the source vertex or the destination vertex exists. Delete vertices with DELETE VERTEX statement and the WITH EDGE option is not used. At this time, the system does not delete the related outgoing and incoming edges of the vertices. There will be dangling edges by default. Dangling edges may appear in Nebula Graph 3.1.0 as the design allow it to exist. And there is no MERGE statement like openCypher has. The existence of dangling edges depends entirely on the application level. You can use GO and LOOKUP statements to find a dangling edge, but cannot use the MATCH statement to find a dangling edge. Examples: // Insert an edge that connects two vertices which do not exist in the graph. The source vertex 's ID is ' 11 '. The destination vertex' s ID is '13' . nebula> CREATE EDGE IF NOT EXISTS e1 ( name string, age int ) ; nebula> INSERT EDGE e1 ( name, age ) VALUES \"11\" -> \"13\" : ( \"n1\" , 1 ) ; // Query using the ` GO ` statement nebula> GO FROM \"11\" over e1 YIELD properties ( edge ) ; +----------------------+ | properties ( EDGE ) | +----------------------+ | { age: 1 , name: \"n1\" } | +----------------------+ // Query using the ` LOOKUP ` statement nebula> LOOKUP ON e1 YIELD EDGE AS r ; +-------------------------------------------------------+ | r | +-------------------------------------------------------+ | [ :e2 \"11\" -> \"13\" @0 { age: 1 , name: \"n1\" }] | +-------------------------------------------------------+ // Query using the ` MATCH ` statement nebula> MATCH () - [ e:e1 ] -> () RETURN e LIMIT 100 ; +---+ | e | +---+ +---+ Empty set ( time spent 3153 /3573 us ) Breadth-first traversal over depth-first traversal \u00b6 Nebula Graph has lower performance for depth-first traversal based on the Graph topology, and better performance for breadth-first traversal and obtaining properties. For example, if model A contains properties \"name\", \"age\", and \"eye color\", it is recommended to create a tag person and add properties name , age , and eye_color to it. If you create a tag eye_color and an edge type has , and then create an edge to represent the eye color owned by the person, the traversal performance will not be high. The performance of finding an edge by an edge property is close to that of finding a vertex by a vertex property. For some databases, it is recommended to re-model edge properties as those of the intermediate vertices. For example, model the pattern (src)-[edge {P1, P2}]->(dst) as (src)-[edge1]->(i_node {P1, P2})-[edge2]->(dst) . With Nebula Graph 3.1.0, you can use (src)-[edge {P1, P2}]->(dst) directly to decrease the depth of the traversal and increase the performance. Edge directions \u00b6 To query in the opposite direction of an edge, use the following syntax: (dst)<-[edge]-(src) or GO FROM dst REVERSELY . If you do not care about the directions or want to query against both directions, use the following syntax: (src)-[edge]-(dst) or GO FROM src BIDIRECT . Therefore, there is no need to insert the same edge redundantly in the reversed direction. Set tag properties appropriately \u00b6 Put a group of properties that are on the same level into the same tag. Different groups represent different concepts. Use indexes correctly \u00b6 Using property indexes helps find VIDs through properties, but can lead to performance reduction by 90% or even more. Only use an index when you need to find vertices or edges through their properties. Design VIDs appropriately \u00b6 See VID . Long texts \u00b6 Do not use long texts to create edge properties. Edge properties are stored twice and long texts lead to greater write amplification. For how edges properties are stored, see Storage architecture . It is recommended to store long texts in HBase or Elasticsearch and store its address in Nebula Graph. Dynamic graphs (sequence graphs) are not supported \u00b6 In some scenarios, graphs need to have the time information to describe how the structure of the entire graph changes over time. 1 The Rank field on Edges in Nebula Graph 3.1.0 can be used to store time in int64, but no field on vertices can do this because if you store the time information as property values, it will be covered by new insertion. Thus Nebula Graph does not support sequence graphs. https://blog.twitter.com/engineering/en_us/topics/insights/2021/temporal-graph-networks \u21a9","title":"Modeling suggestions"},{"location":"8.service-tuning/2.graph-modeling/#graph_data_modeling_suggestions","text":"This topic provides general suggestions for modeling data in Nebula Graph. Note The following suggestions may not apply to some special scenarios. In these cases, find help in the Nebula Graph community .","title":"Graph data modeling suggestions"},{"location":"8.service-tuning/2.graph-modeling/#model_for_performance","text":"There is no perfect method to model in Nebula Graph. Graph modeling depends on the questions that you want to know from the data. Your data drives your graph model. Graph data modeling is intuitive and convenient. Create your data model based on your business model. Test your model and gradually optimize it to fit your business. To get better performance, you can change or re-design your model multiple times.","title":"Model for performance"},{"location":"8.service-tuning/2.graph-modeling/#design_and_evaluate_the_most_important_queries","text":"Usually, various types of queries are validated in test scenarios to assess the overall capabilities of the system. However, in most production scenarios, there are not many types of frequently used queries. You can optimize the data model based on key queries selected according to the Pareto (80/20) principle.","title":"Design and evaluate the most important queries"},{"location":"8.service-tuning/2.graph-modeling/#full-graph_scanning_avoidance","text":"Graph traversal can be performed after one or more vertices/edges are located through property indexes or VIDs. But for some query patterns, such as subgraph and path query patterns, the source vertex or edge of the traversal cannot be located through property indexes or VIDs. These queries find all the subgraphs that satisfy the query pattern by scanning the whole graph space which will have poor query performance. Nebula Graph does not implement indexing for the graph structures of subgraphs or paths.","title":"Full-graph scanning avoidance"},{"location":"8.service-tuning/2.graph-modeling/#no_predefined_bonds_between_tags_and_edge_types","text":"Define the bonds between Tags and Edge types in the application, not Nebula Graph. There are no statements that could get the bonds between Tags and Edge types.","title":"No predefined bonds between Tags and Edge types"},{"location":"8.service-tuning/2.graph-modeling/#tagsedge_types_predefine_a_set_of_properties","text":"While creating Tags or Edge types, you need to define a set of properties. Properties are part of the Nebula Graph Schema.","title":"Tags/Edge types predefine a set of properties"},{"location":"8.service-tuning/2.graph-modeling/#control_changes_in_the_business_model_and_the_data_model","text":"Changes here refer to changes in business models and data models (meta-information), not changes in the data itself. Some graph databases are designed to be Schema-free, so their data modeling, including the modeling of the graph topology and properties, can be very flexible. Properties can be re-modeled to graph topology, and vice versa. Such systems are often specifically optimized for graph topology access. Nebula Graph 3.1.0 is a strong-Schema (row storage) system, which means that the business model should not change frequently. For example, the property Schema should not change. It is similar to avoiding ALTER TABLE in MySQL. On the contrary, vertices and their edges can be added or deleted at low costs. Thus, the easy-to-change part of the business model should be transformed to vertices or edges, rather than properties. For example, in a business model, people have relatively fixed properties such as age, gender, and name. But their contact, place of visit, trade account, and login device are often changing. The former is suitable for modeling as properties and the latter as vertices or edges.","title":"Control changes in the business model and the data model"},{"location":"8.service-tuning/2.graph-modeling/#set_temporary_properties_through_self-loop_edges","text":"As a strong Schema system, Nebula Graph does not support List-type properties. And using ALTER TAG costs too much. If you need to add some temporary properties or List-type properties to a vertex, you can first create an edge type with the required properties, and then insert one or more edges that direct to the vertex itself. The figure is as follows. To retrieve temporary properties of vertices, fetch from self-loop edges. For example: //Create the edge type and insert the loop property. nebula> CREATE EDGE IF NOT EXISTS temp(tmp int); nebula> INSERT EDGE temp(tmp) VALUES \"player100\"->\"player100\"@1:(1); nebula> INSERT EDGE temp(tmp) VALUES \"player100\"->\"player100\"@2:(2); nebula> INSERT EDGE temp(tmp) VALUES \"player100\"->\"player100\"@3:(3); //After the data is inserted, you can query the loop property by general query statements, for example: nebula> GO FROM \"player100\" OVER temp YIELD properties(edge).tmp; +----------------------+ | properties(EDGE).tmp | +----------------------+ | 1 | | 2 | | 3 | +----------------------+ //If you want the results to be returned in the form of a List, you can use a function, for example: nebula> MATCH (v1:player)-[e:temp]->() return collect(e.tmp); +----------------+ | collect(e.tmp) | +----------------+ | [1, 2, 3] | +----------------+","title":"Set temporary properties through self-loop edges"},{"location":"8.service-tuning/2.graph-modeling/#about_dangling_edges","text":"A dangling edge is an edge that only connects to a single vertex and only one part of the edge connects to the vertex. In Nebula Graph 3.1.0, dangling edges may appear in the following two cases. Insert edges with INSERT EDGE statement before the source vertex or the destination vertex exists. Delete vertices with DELETE VERTEX statement and the WITH EDGE option is not used. At this time, the system does not delete the related outgoing and incoming edges of the vertices. There will be dangling edges by default. Dangling edges may appear in Nebula Graph 3.1.0 as the design allow it to exist. And there is no MERGE statement like openCypher has. The existence of dangling edges depends entirely on the application level. You can use GO and LOOKUP statements to find a dangling edge, but cannot use the MATCH statement to find a dangling edge. Examples: // Insert an edge that connects two vertices which do not exist in the graph. The source vertex 's ID is ' 11 '. The destination vertex' s ID is '13' . nebula> CREATE EDGE IF NOT EXISTS e1 ( name string, age int ) ; nebula> INSERT EDGE e1 ( name, age ) VALUES \"11\" -> \"13\" : ( \"n1\" , 1 ) ; // Query using the ` GO ` statement nebula> GO FROM \"11\" over e1 YIELD properties ( edge ) ; +----------------------+ | properties ( EDGE ) | +----------------------+ | { age: 1 , name: \"n1\" } | +----------------------+ // Query using the ` LOOKUP ` statement nebula> LOOKUP ON e1 YIELD EDGE AS r ; +-------------------------------------------------------+ | r | +-------------------------------------------------------+ | [ :e2 \"11\" -> \"13\" @0 { age: 1 , name: \"n1\" }] | +-------------------------------------------------------+ // Query using the ` MATCH ` statement nebula> MATCH () - [ e:e1 ] -> () RETURN e LIMIT 100 ; +---+ | e | +---+ +---+ Empty set ( time spent 3153 /3573 us )","title":"About dangling edges"},{"location":"8.service-tuning/2.graph-modeling/#breadth-first_traversal_over_depth-first_traversal","text":"Nebula Graph has lower performance for depth-first traversal based on the Graph topology, and better performance for breadth-first traversal and obtaining properties. For example, if model A contains properties \"name\", \"age\", and \"eye color\", it is recommended to create a tag person and add properties name , age , and eye_color to it. If you create a tag eye_color and an edge type has , and then create an edge to represent the eye color owned by the person, the traversal performance will not be high. The performance of finding an edge by an edge property is close to that of finding a vertex by a vertex property. For some databases, it is recommended to re-model edge properties as those of the intermediate vertices. For example, model the pattern (src)-[edge {P1, P2}]->(dst) as (src)-[edge1]->(i_node {P1, P2})-[edge2]->(dst) . With Nebula Graph 3.1.0, you can use (src)-[edge {P1, P2}]->(dst) directly to decrease the depth of the traversal and increase the performance.","title":"Breadth-first traversal over depth-first traversal"},{"location":"8.service-tuning/2.graph-modeling/#edge_directions","text":"To query in the opposite direction of an edge, use the following syntax: (dst)<-[edge]-(src) or GO FROM dst REVERSELY . If you do not care about the directions or want to query against both directions, use the following syntax: (src)-[edge]-(dst) or GO FROM src BIDIRECT . Therefore, there is no need to insert the same edge redundantly in the reversed direction.","title":"Edge directions"},{"location":"8.service-tuning/2.graph-modeling/#set_tag_properties_appropriately","text":"Put a group of properties that are on the same level into the same tag. Different groups represent different concepts.","title":"Set tag properties appropriately"},{"location":"8.service-tuning/2.graph-modeling/#use_indexes_correctly","text":"Using property indexes helps find VIDs through properties, but can lead to performance reduction by 90% or even more. Only use an index when you need to find vertices or edges through their properties.","title":"Use indexes correctly"},{"location":"8.service-tuning/2.graph-modeling/#design_vids_appropriately","text":"See VID .","title":"Design VIDs appropriately"},{"location":"8.service-tuning/2.graph-modeling/#long_texts","text":"Do not use long texts to create edge properties. Edge properties are stored twice and long texts lead to greater write amplification. For how edges properties are stored, see Storage architecture . It is recommended to store long texts in HBase or Elasticsearch and store its address in Nebula Graph.","title":"Long texts"},{"location":"8.service-tuning/2.graph-modeling/#dynamic_graphs_sequence_graphs_are_not_supported","text":"In some scenarios, graphs need to have the time information to describe how the structure of the entire graph changes over time. 1 The Rank field on Edges in Nebula Graph 3.1.0 can be used to store time in int64, but no field on vertices can do this because if you store the time information as property values, it will be covered by new insertion. Thus Nebula Graph does not support sequence graphs. https://blog.twitter.com/engineering/en_us/topics/insights/2021/temporal-graph-networks \u21a9","title":"Dynamic graphs (sequence graphs) are not supported"},{"location":"8.service-tuning/3.system-design/","text":"System design suggestions \u00b6 QPS or low-latency first \u00b6 Nebula Graph 3.1.0 is good at handling small requests with high concurrency. In such scenarios, the whole graph is huge, containing maybe trillions of vertices or edges, but the subgraphs accessed by each request are not large (containing millions of vertices or edges), and the latency of a single request is low. The concurrent number of such requests, i.e., the QPS, can be huge. On the other hand, in interactive analysis scenarios, the request concurrency is usually not high, but the subgraphs accessed by each request are large, with thousands of millions of vertices or edges. To lower the latency of big requests in such scenarios, you can split big requests into multiple small requests in the application, and concurrently send them to multiple graphd processes. This can decrease the memory used by each graphd process as well. Besides, you can use Nebula Algorithm for such scenarios. Data transmission and optimization \u00b6 Read/write balance. Nebula Graph fits into OLTP scenarios with balanced read/write, i.e., concurrent write and read. It is not suitable for OLAP scenarios that usually need to write once and read many times. Select different write methods. For large batches of data writing, use SST files. For small batches of data writing, use INSERT . Run COMPACTION and BALANCE jobs to optimize data format and storage distribution at the right time. Nebula Graph 3.1.0 does not support transactions and isolation in the relational database and is closer to NoSQL. Query preheating and data preheating \u00b6 Preheat on the application side: The Grapd process does not support pre-compiling queries and generating corresponding query plans, nor can it cache previous query results. The Storagd process does not support preheating data. Only the LSM-Tree and BloomFilter of RocksDB are loaded into memory at startup. Once accessed, vertices and edges are cached respectively in two types of LRU cache of the Storage Service.","title":"System design suggestions"},{"location":"8.service-tuning/3.system-design/#system_design_suggestions","text":"","title":"System design suggestions"},{"location":"8.service-tuning/3.system-design/#qps_or_low-latency_first","text":"Nebula Graph 3.1.0 is good at handling small requests with high concurrency. In such scenarios, the whole graph is huge, containing maybe trillions of vertices or edges, but the subgraphs accessed by each request are not large (containing millions of vertices or edges), and the latency of a single request is low. The concurrent number of such requests, i.e., the QPS, can be huge. On the other hand, in interactive analysis scenarios, the request concurrency is usually not high, but the subgraphs accessed by each request are large, with thousands of millions of vertices or edges. To lower the latency of big requests in such scenarios, you can split big requests into multiple small requests in the application, and concurrently send them to multiple graphd processes. This can decrease the memory used by each graphd process as well. Besides, you can use Nebula Algorithm for such scenarios.","title":"QPS or low-latency first"},{"location":"8.service-tuning/3.system-design/#data_transmission_and_optimization","text":"Read/write balance. Nebula Graph fits into OLTP scenarios with balanced read/write, i.e., concurrent write and read. It is not suitable for OLAP scenarios that usually need to write once and read many times. Select different write methods. For large batches of data writing, use SST files. For small batches of data writing, use INSERT . Run COMPACTION and BALANCE jobs to optimize data format and storage distribution at the right time. Nebula Graph 3.1.0 does not support transactions and isolation in the relational database and is closer to NoSQL.","title":"Data transmission and optimization"},{"location":"8.service-tuning/3.system-design/#query_preheating_and_data_preheating","text":"Preheat on the application side: The Grapd process does not support pre-compiling queries and generating corresponding query plans, nor can it cache previous query results. The Storagd process does not support preheating data. Only the LSM-Tree and BloomFilter of RocksDB are loaded into memory at startup. Once accessed, vertices and edges are cached respectively in two types of LRU cache of the Storage Service.","title":"Query preheating and data preheating"},{"location":"8.service-tuning/4.plan/","text":"Execution plan \u00b6 Nebula Graph 3.1.0 applies rule-based execution plans. Users cannot change execution plans, pre-compile queries (and corresponding plan cache), or accelerate queries by specifying indexes. To view the execution plan and executive summary, see EXPLAIN and PROFILE .","title":"Execution plan"},{"location":"8.service-tuning/4.plan/#execution_plan","text":"Nebula Graph 3.1.0 applies rule-based execution plans. Users cannot change execution plans, pre-compile queries (and corresponding plan cache), or accelerate queries by specifying indexes. To view the execution plan and executive summary, see EXPLAIN and PROFILE .","title":"Execution plan"},{"location":"8.service-tuning/compaction/","text":"Compaction \u00b6 This topic gives some information about compaction. In Nebula Graph, Compaction is the most important background process and has an important effect on performance. Compaction reads the data that is written on the hard disk, then re-organizes the data structure and the indexes, and then writes back to the hard disk. The read performance can increase by times after compaction. Thus, to get high read performance, trigger compaction (full compaction ) manually when writing a large amount of data into Nebula Graph. Note Note that compaction leads to long-time hard disk IO. We suggest that users do compaction during off-peak hours (for example, early morning). Nebula Graph has two types of compaction : automatic compaction and full compaction . Automatic compaction \u00b6 Automatic compaction is automatically triggered when the system reads data, writes data, or the system restarts. The read performance can increase in a short time. Automatic compaction is enabled by default. But once triggered during peak hours, it can cause unexpected IO occupancy that has an unwanted effect on the performance. Full compaction \u00b6 Full compaction enables large-scale background operations for a graph space such as merging files, deleting the data expired by TTL. This operation needs to be initiated manually. Use the following statements to enable full compaction : Note We recommend you to do the full compaction during off-peak hours because full compaction has a lot of IO operations. nebula> USE <your_graph_space>; nebula> SUBMIT JOB COMPACT; The preceding statement returns the job ID. To show the compaction progress, use the following statement: nebula> SHOW JOB <job_id>; Operation suggestions \u00b6 These are some operation suggestions to keep Nebula Graph performing well. After data import is done, run SUBMIT JOB COMPACT . Run SUBMIT JOB COMPACT periodically during off-peak hours (e.g. early morning). To control the read and write traffic limitation for compactions , set the following parameter in the nebula-storaged.conf configuration file. # Limit the read/write rate to 20MB/s. --rocksdb_rate_limit = 20 ( in MB/s ) FAQ \u00b6 \"Where are the logs related to Compaction stored?\" \u00b6 By default, the logs are stored under the LOG file in the /usr/local/nebula/data/storage/nebula/{1}/data/ directory, or similar to LOG.old.1625797988509303 . You can find the following content. ** Compaction Stats [default] ** Level Files Size Score Read(GB) Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- L0 2/0 2.46 KB 0.5 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.53 0.51 2 0.264 0 0 Sum 2/0 2.46 KB 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.53 0.51 2 0.264 0 0 Int 0/0 0.00 KB 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.00 0.00 0 0.000 0 0 If the number of L0 files is large, the read performance will be greatly affected and compaction can be triggered. \"Can I do full compactions for multiple graph spaces at the same time?\" \u00b6 Yes, you can. But the IO is much larger at this time and the efficiency may be affected. \"How much time does it take for full compactions ?\" \u00b6 When rocksdb_rate_limit is set to 20 , you can estimate the full compaction time by dividing the hard disk usage by the rocksdb_rate_limit . If you do not set the rocksdb_rate_limit value, the empirical value is around 50 MB/s. \"Can I modify --rocksdb_rate_limit dynamically?\" \u00b6 No, you cannot. \"Can I stop a full compaction after it starts?\" \u00b6 No, you cannot. When you start a full compaction, you have to wait till it is done. This is the limitation of RocksDB.","title":"Compaction"},{"location":"8.service-tuning/compaction/#compaction","text":"This topic gives some information about compaction. In Nebula Graph, Compaction is the most important background process and has an important effect on performance. Compaction reads the data that is written on the hard disk, then re-organizes the data structure and the indexes, and then writes back to the hard disk. The read performance can increase by times after compaction. Thus, to get high read performance, trigger compaction (full compaction ) manually when writing a large amount of data into Nebula Graph. Note Note that compaction leads to long-time hard disk IO. We suggest that users do compaction during off-peak hours (for example, early morning). Nebula Graph has two types of compaction : automatic compaction and full compaction .","title":"Compaction"},{"location":"8.service-tuning/compaction/#automatic_compaction","text":"Automatic compaction is automatically triggered when the system reads data, writes data, or the system restarts. The read performance can increase in a short time. Automatic compaction is enabled by default. But once triggered during peak hours, it can cause unexpected IO occupancy that has an unwanted effect on the performance.","title":"Automatic compaction"},{"location":"8.service-tuning/compaction/#full_compaction","text":"Full compaction enables large-scale background operations for a graph space such as merging files, deleting the data expired by TTL. This operation needs to be initiated manually. Use the following statements to enable full compaction : Note We recommend you to do the full compaction during off-peak hours because full compaction has a lot of IO operations. nebula> USE <your_graph_space>; nebula> SUBMIT JOB COMPACT; The preceding statement returns the job ID. To show the compaction progress, use the following statement: nebula> SHOW JOB <job_id>;","title":"Full compaction"},{"location":"8.service-tuning/compaction/#operation_suggestions","text":"These are some operation suggestions to keep Nebula Graph performing well. After data import is done, run SUBMIT JOB COMPACT . Run SUBMIT JOB COMPACT periodically during off-peak hours (e.g. early morning). To control the read and write traffic limitation for compactions , set the following parameter in the nebula-storaged.conf configuration file. # Limit the read/write rate to 20MB/s. --rocksdb_rate_limit = 20 ( in MB/s )","title":"Operation suggestions"},{"location":"8.service-tuning/compaction/#faq","text":"","title":"FAQ"},{"location":"8.service-tuning/compaction/#where_are_the_logs_related_to_compaction_stored","text":"By default, the logs are stored under the LOG file in the /usr/local/nebula/data/storage/nebula/{1}/data/ directory, or similar to LOG.old.1625797988509303 . You can find the following content. ** Compaction Stats [default] ** Level Files Size Score Read(GB) Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- L0 2/0 2.46 KB 0.5 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.53 0.51 2 0.264 0 0 Sum 2/0 2.46 KB 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.53 0.51 2 0.264 0 0 Int 0/0 0.00 KB 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.00 0.00 0 0.000 0 0 If the number of L0 files is large, the read performance will be greatly affected and compaction can be triggered.","title":"\"Where are the logs related to Compaction stored?\""},{"location":"8.service-tuning/compaction/#can_i_do_full_compactions_for_multiple_graph_spaces_at_the_same_time","text":"Yes, you can. But the IO is much larger at this time and the efficiency may be affected.","title":"\"Can I do full compactions for multiple graph spaces at the same time?\""},{"location":"8.service-tuning/compaction/#how_much_time_does_it_take_for_full_compactions","text":"When rocksdb_rate_limit is set to 20 , you can estimate the full compaction time by dividing the hard disk usage by the rocksdb_rate_limit . If you do not set the rocksdb_rate_limit value, the empirical value is around 50 MB/s.","title":"\"How much time does it take for full compactions?\""},{"location":"8.service-tuning/compaction/#can_i_modify_--rocksdb_rate_limit_dynamically","text":"No, you cannot.","title":"\"Can I modify --rocksdb_rate_limit dynamically?\""},{"location":"8.service-tuning/compaction/#can_i_stop_a_full_compaction_after_it_starts","text":"No, you cannot. When you start a full compaction, you have to wait till it is done. This is the limitation of RocksDB.","title":"\"Can I stop a full compaction after it starts?\""},{"location":"8.service-tuning/load-balance/","text":"Storage load balance \u00b6 You can use the BALANCE statement to balance the distribution of partitions and Raft leaders, or clear some Storage servers for easy maintenance. For details, see BALANCE . Danger The BALANCE commands migrate data and balance the distribution of partitions by creating and executing a set of subtasks. DO NOT stop any machine in the cluster or change its IP address until all the subtasks finish. Otherwise, the follow-up subtasks fail. Balance partition distribution \u00b6 Enterpriseonly Only available for the Nebula Graph Enterprise Edition. Note If the current graph space already has a BALANCE DATA job in the FAILED status, you can restore the FAILED job, but cannot start a new BALANCE DATA job. If the job continues to fail, manually stop it, and then you can start a new one. The BALANCE DATA commands starts a job to balance the distribution of storage partitions in the current graph space by creating and executing a set of subtasks. Examples \u00b6 After you add new storage hosts into the cluster, no partition is deployed on the new hosts. Run SHOW HOSTS to check the partition distribution. nebual> SHOW HOSTS; +-----------------+------+-----------+----------+--------------+-----------------------+------------------------+-------------+ | Host | Port | HTTP port | Status | Leader count | Leader distribution | Partition distribution | Version | +-----------------+------+-----------+----------+--------------+-----------------------+------------------------+-------------+ | \"192.168.8.101\" | 9779 | 19669 | \"ONLINE\" | 0 | \"No valid partition\" | \"No valid partition\" | \"3.1.0-ent\" | | \"192.168.8.100\" | 9779 | 19669 | \"ONLINE\" | 15 | \"basketballplayer:15\" | \"basketballplayer:15\" | \"3.1.0-ent\" | +-----------------+------+-----------+----------+--------------+-----------------------+------------------------+-------------+ Enter the graph space basketballplayer , and execute the command BALANCE DATA to balance the distribution of storage partitions. nebula> USE basketballplayer; nebula> BALANCE DATA; +------------+ | New Job Id | +------------+ | 2 | +------------+ The job ID is returned after running BALANCE DATA . Run SHOW JOB <job_id> to check the status of the job. nebula> SHOW JOB 2; +------------------------+------------------------------------------+-------------+---------------------------------+---------------------------------+-------------+ | Job Id(spaceId:partId) | Command(src->dst) | Status | Start Time | Stop Time | Error Code | +------------------------+------------------------------------------+-------------+---------------------------------+---------------------------------+-------------+ | 2 | \"DATA_BALANCE\" | \"FINISHED\" | \"2022-04-12T03:41:43.000000000\" | \"2022-04-12T03:41:53.000000000\" | \"SUCCEEDED\" | | \"2, 1:1\" | \"192.168.8.100:9779->192.168.8.101:9779\" | \"SUCCEEDED\" | 2022-04-12T03:41:43.000000 | 2022-04-12T03:41:53.000000 | \"SUCCEEDED\" | | \"2, 1:2\" | \"192.168.8.100:9779->192.168.8.101:9779\" | \"SUCCEEDED\" | 2022-04-12T03:41:43.000000 | 2022-04-12T03:41:53.000000 | \"SUCCEEDED\" | | \"2, 1:3\" | \"192.168.8.100:9779->192.168.8.101:9779\" | \"SUCCEEDED\" | 2022-04-12T03:41:43.000000 | 2022-04-12T03:41:53.000000 | \"SUCCEEDED\" | | \"2, 1:4\" | \"192.168.8.100:9779->192.168.8.101:9779\" | \"SUCCEEDED\" | 2022-04-12T03:41:43.000000 | 2022-04-12T03:41:53.000000 | \"SUCCEEDED\" | | \"2, 1:5\" | \"192.168.8.100:9779->192.168.8.101:9779\" | \"SUCCEEDED\" | 2022-04-12T03:41:43.000000 | 2022-04-12T03:41:53.000000 | \"SUCCEEDED\" | | \"2, 1:6\" | \"192.168.8.100:9779->192.168.8.101:9779\" | \"SUCCEEDED\" | 2022-04-12T03:41:43.000000 | 2022-04-12T03:41:43.000000 | \"SUCCEEDED\" | | \"2, 1:7\" | \"192.168.8.100:9779->192.168.8.101:9779\" | \"SUCCEEDED\" | 2022-04-12T03:41:43.000000 | 2022-04-12T03:41:53.000000 | \"SUCCEEDED\" | | \"Total:7\" | \"Succeeded:7\" | \"Failed:0\" | \"In Progress:0\" | \"Invalid:0\" | \"\" | +------------------------+------------------------------------------+-------------+---------------------------------+---------------------------------+-------------+ When all the subtasks succeed, the load balancing process finishes. Run SHOW HOSTS again to make sure the partition distribution is balanced. Note BALANCE DATA does not balance the leader distribution. For more information, see Balance leader distribution . nebula> SHOW HOSTS; +-----------------+------+-----------+----------+--------------+----------------------+------------------------+-------------+ | Host | Port | HTTP port | Status | Leader count | Leader distribution | Partition distribution | Version | +-----------------+------+-----------+----------+--------------+----------------------+------------------------+-------------+ | \"192.168.8.101\" | 9779 | 19669 | \"ONLINE\" | 7 | \"basketballplayer:7\" | \"basketballplayer:7\" | \"3.1.0-ent\" | | \"192.168.8.100\" | 9779 | 19669 | \"ONLINE\" | 8 | \"basketballplayer:8\" | \"basketballplayer:8\" | \"3.1.0-ent\" | +-----------------+------+-----------+----------+--------------+----------------------+------------------------+-------------+ If any subtask fails, run RECOVER JOB <job_id> to recover the failed jobs. If redoing load balancing does not solve the problem, ask for help in the Nebula Graph community . Stop data balancing \u00b6 To stop a balance job, run STOP JOB <job_id> . If no balance job is running, an error is returned. If a balance job is running, Job stopped is returned. Note STOP JOB <job_id> does not stop the running subtasks but cancels all follow-up subtasks. The status of follow-up subtasks is set to INVALID . The status of ongoing subtasks is set to SUCCEEDED or FAILED based on the result. You can run the SHOW JOB <job_id> command to check the stopped job status. Once all the subtasks are finished or stopped, you can run RECOVER JOB <job_id> again to balance the partitions again, the subtasks continue to be executed in the original state. Restore a balance job \u00b6 To restore a balance job in the FAILED or STOPPED status, run RECOVER JOB <job_id> . Note For a STOPPED BALANCE DATA job, Nebula Graph detects whether the same type of FAILED jobs or FINISHED jobs have been created since the start time of the job. If so, the STOPPED job cannot be restored. For example, if chronologically there are STOPPED job1, FINISHED job2, and STOPPED Job3, only job3 can be restored, and job1 cannot. Migrate partition \u00b6 To migrate specified partitions and scale in the cluster, you can run BALANCE DATA REMOVE <ip:port> [,<ip>:<port> ...] . For example, to migrate the partitions in server 192.168.8.100:9779 , the command as following: nebula> BALANCE DATA REMOVE 192.168.8.100:9779; nebula> SHOW HOSTS; +-----------------+------+-----------+----------+--------------+-----------------------+------------------------+-------------+ | Host | Port | HTTP port | Status | Leader count | Leader distribution | Partition distribution | Version | +-----------------+------+-----------+----------+--------------+-----------------------+------------------------+-------------+ | \"192.168.8.101\" | 9779 | 19669 | \"ONLINE\" | 15 | \"basketballplayer:15\" | \"basketballplayer:15\" | \"3.1.0-ent\" | | \"192.168.8.100\" | 9779 | 19669 | \"ONLINE\" | 0 | \"No valid partition\" | \"No valid partition\" | \"3.1.0-ent\" | +-----------------+------+-----------+----------+--------------+-----------------------+------------------------+-------------+ Note This command migrates partitions to other storage hosts but does not delete the current storage host from the cluster. To delete the Storage hosts from cluster, see Manage Storage hosts . Balance leader distribution \u00b6 To balance the raft leaders, run BALANCE LEADER . Example \u00b6 nebula> BALANCE LEADER; Run SHOW HOSTS to check the balance result. nebula> SHOW HOSTS; +------------------+------+-----------+----------+--------------+-----------------------------------+------------------------+---------+ | Host | Port | HTTP port | Status | Leader count | Leader distribution | Partition distribution | Version | +------------------+------+-----------+----------+--------------+-----------------------------------+------------------------+---------+ | \"192.168.10.100\" | 9779 | 19669 | \"ONLINE\" | 4 | \"basketballplayer:3\" | \"basketballplayer:8\" | \"3.1.0\" | | \"192.168.10.101\" | 9779 | 19669 | \"ONLINE\" | 8 | \"basketballplayer:3\" | \"basketballplayer:8\" | \"3.1.0\" | | \"192.168.10.102\" | 9779 | 19669 | \"ONLINE\" | 3 | \"basketballplayer:3\" | \"basketballplayer:8\" | \"3.1.0\" | | \"192.168.10.103\" | 9779 | 19669 | \"ONLINE\" | 0 | \"basketballplayer:2\" | \"basketballplayer:7\" | \"3.1.0\" | | \"192.168.10.104\" | 9779 | 19669 | \"ONLINE\" | 0 | \"basketballplayer:2\" | \"basketballplayer:7\" | \"3.1.0\" | | \"192.168.10.105\" | 9779 | 19669 | \"ONLINE\" | 0 | \"basketballplayer:2\" | \"basketballplayer:7\" | \"3.1.0\" | +------------------+------+-----------+----------+--------------+-----------------------------------+------------------------+---------+ Caution In Nebula Graph 3.1.0, switching leaders will cause a large number of short-term request errors (Storage Error E_RPC_FAILURE ). For solutions, FAQ .","title":"Storage load balance"},{"location":"8.service-tuning/load-balance/#storage_load_balance","text":"You can use the BALANCE statement to balance the distribution of partitions and Raft leaders, or clear some Storage servers for easy maintenance. For details, see BALANCE . Danger The BALANCE commands migrate data and balance the distribution of partitions by creating and executing a set of subtasks. DO NOT stop any machine in the cluster or change its IP address until all the subtasks finish. Otherwise, the follow-up subtasks fail.","title":"Storage load balance"},{"location":"8.service-tuning/load-balance/#balance_partition_distribution","text":"Enterpriseonly Only available for the Nebula Graph Enterprise Edition. Note If the current graph space already has a BALANCE DATA job in the FAILED status, you can restore the FAILED job, but cannot start a new BALANCE DATA job. If the job continues to fail, manually stop it, and then you can start a new one. The BALANCE DATA commands starts a job to balance the distribution of storage partitions in the current graph space by creating and executing a set of subtasks.","title":"Balance partition distribution"},{"location":"8.service-tuning/load-balance/#examples","text":"After you add new storage hosts into the cluster, no partition is deployed on the new hosts. Run SHOW HOSTS to check the partition distribution. nebual> SHOW HOSTS; +-----------------+------+-----------+----------+--------------+-----------------------+------------------------+-------------+ | Host | Port | HTTP port | Status | Leader count | Leader distribution | Partition distribution | Version | +-----------------+------+-----------+----------+--------------+-----------------------+------------------------+-------------+ | \"192.168.8.101\" | 9779 | 19669 | \"ONLINE\" | 0 | \"No valid partition\" | \"No valid partition\" | \"3.1.0-ent\" | | \"192.168.8.100\" | 9779 | 19669 | \"ONLINE\" | 15 | \"basketballplayer:15\" | \"basketballplayer:15\" | \"3.1.0-ent\" | +-----------------+------+-----------+----------+--------------+-----------------------+------------------------+-------------+ Enter the graph space basketballplayer , and execute the command BALANCE DATA to balance the distribution of storage partitions. nebula> USE basketballplayer; nebula> BALANCE DATA; +------------+ | New Job Id | +------------+ | 2 | +------------+ The job ID is returned after running BALANCE DATA . Run SHOW JOB <job_id> to check the status of the job. nebula> SHOW JOB 2; +------------------------+------------------------------------------+-------------+---------------------------------+---------------------------------+-------------+ | Job Id(spaceId:partId) | Command(src->dst) | Status | Start Time | Stop Time | Error Code | +------------------------+------------------------------------------+-------------+---------------------------------+---------------------------------+-------------+ | 2 | \"DATA_BALANCE\" | \"FINISHED\" | \"2022-04-12T03:41:43.000000000\" | \"2022-04-12T03:41:53.000000000\" | \"SUCCEEDED\" | | \"2, 1:1\" | \"192.168.8.100:9779->192.168.8.101:9779\" | \"SUCCEEDED\" | 2022-04-12T03:41:43.000000 | 2022-04-12T03:41:53.000000 | \"SUCCEEDED\" | | \"2, 1:2\" | \"192.168.8.100:9779->192.168.8.101:9779\" | \"SUCCEEDED\" | 2022-04-12T03:41:43.000000 | 2022-04-12T03:41:53.000000 | \"SUCCEEDED\" | | \"2, 1:3\" | \"192.168.8.100:9779->192.168.8.101:9779\" | \"SUCCEEDED\" | 2022-04-12T03:41:43.000000 | 2022-04-12T03:41:53.000000 | \"SUCCEEDED\" | | \"2, 1:4\" | \"192.168.8.100:9779->192.168.8.101:9779\" | \"SUCCEEDED\" | 2022-04-12T03:41:43.000000 | 2022-04-12T03:41:53.000000 | \"SUCCEEDED\" | | \"2, 1:5\" | \"192.168.8.100:9779->192.168.8.101:9779\" | \"SUCCEEDED\" | 2022-04-12T03:41:43.000000 | 2022-04-12T03:41:53.000000 | \"SUCCEEDED\" | | \"2, 1:6\" | \"192.168.8.100:9779->192.168.8.101:9779\" | \"SUCCEEDED\" | 2022-04-12T03:41:43.000000 | 2022-04-12T03:41:43.000000 | \"SUCCEEDED\" | | \"2, 1:7\" | \"192.168.8.100:9779->192.168.8.101:9779\" | \"SUCCEEDED\" | 2022-04-12T03:41:43.000000 | 2022-04-12T03:41:53.000000 | \"SUCCEEDED\" | | \"Total:7\" | \"Succeeded:7\" | \"Failed:0\" | \"In Progress:0\" | \"Invalid:0\" | \"\" | +------------------------+------------------------------------------+-------------+---------------------------------+---------------------------------+-------------+ When all the subtasks succeed, the load balancing process finishes. Run SHOW HOSTS again to make sure the partition distribution is balanced. Note BALANCE DATA does not balance the leader distribution. For more information, see Balance leader distribution . nebula> SHOW HOSTS; +-----------------+------+-----------+----------+--------------+----------------------+------------------------+-------------+ | Host | Port | HTTP port | Status | Leader count | Leader distribution | Partition distribution | Version | +-----------------+------+-----------+----------+--------------+----------------------+------------------------+-------------+ | \"192.168.8.101\" | 9779 | 19669 | \"ONLINE\" | 7 | \"basketballplayer:7\" | \"basketballplayer:7\" | \"3.1.0-ent\" | | \"192.168.8.100\" | 9779 | 19669 | \"ONLINE\" | 8 | \"basketballplayer:8\" | \"basketballplayer:8\" | \"3.1.0-ent\" | +-----------------+------+-----------+----------+--------------+----------------------+------------------------+-------------+ If any subtask fails, run RECOVER JOB <job_id> to recover the failed jobs. If redoing load balancing does not solve the problem, ask for help in the Nebula Graph community .","title":"Examples"},{"location":"8.service-tuning/load-balance/#stop_data_balancing","text":"To stop a balance job, run STOP JOB <job_id> . If no balance job is running, an error is returned. If a balance job is running, Job stopped is returned. Note STOP JOB <job_id> does not stop the running subtasks but cancels all follow-up subtasks. The status of follow-up subtasks is set to INVALID . The status of ongoing subtasks is set to SUCCEEDED or FAILED based on the result. You can run the SHOW JOB <job_id> command to check the stopped job status. Once all the subtasks are finished or stopped, you can run RECOVER JOB <job_id> again to balance the partitions again, the subtasks continue to be executed in the original state.","title":"Stop data balancing"},{"location":"8.service-tuning/load-balance/#restore_a_balance_job","text":"To restore a balance job in the FAILED or STOPPED status, run RECOVER JOB <job_id> . Note For a STOPPED BALANCE DATA job, Nebula Graph detects whether the same type of FAILED jobs or FINISHED jobs have been created since the start time of the job. If so, the STOPPED job cannot be restored. For example, if chronologically there are STOPPED job1, FINISHED job2, and STOPPED Job3, only job3 can be restored, and job1 cannot.","title":"Restore a balance job"},{"location":"8.service-tuning/load-balance/#migrate_partition","text":"To migrate specified partitions and scale in the cluster, you can run BALANCE DATA REMOVE <ip:port> [,<ip>:<port> ...] . For example, to migrate the partitions in server 192.168.8.100:9779 , the command as following: nebula> BALANCE DATA REMOVE 192.168.8.100:9779; nebula> SHOW HOSTS; +-----------------+------+-----------+----------+--------------+-----------------------+------------------------+-------------+ | Host | Port | HTTP port | Status | Leader count | Leader distribution | Partition distribution | Version | +-----------------+------+-----------+----------+--------------+-----------------------+------------------------+-------------+ | \"192.168.8.101\" | 9779 | 19669 | \"ONLINE\" | 15 | \"basketballplayer:15\" | \"basketballplayer:15\" | \"3.1.0-ent\" | | \"192.168.8.100\" | 9779 | 19669 | \"ONLINE\" | 0 | \"No valid partition\" | \"No valid partition\" | \"3.1.0-ent\" | +-----------------+------+-----------+----------+--------------+-----------------------+------------------------+-------------+ Note This command migrates partitions to other storage hosts but does not delete the current storage host from the cluster. To delete the Storage hosts from cluster, see Manage Storage hosts .","title":"Migrate partition"},{"location":"8.service-tuning/load-balance/#balance_leader_distribution","text":"To balance the raft leaders, run BALANCE LEADER .","title":"Balance leader distribution"},{"location":"8.service-tuning/load-balance/#example","text":"nebula> BALANCE LEADER; Run SHOW HOSTS to check the balance result. nebula> SHOW HOSTS; +------------------+------+-----------+----------+--------------+-----------------------------------+------------------------+---------+ | Host | Port | HTTP port | Status | Leader count | Leader distribution | Partition distribution | Version | +------------------+------+-----------+----------+--------------+-----------------------------------+------------------------+---------+ | \"192.168.10.100\" | 9779 | 19669 | \"ONLINE\" | 4 | \"basketballplayer:3\" | \"basketballplayer:8\" | \"3.1.0\" | | \"192.168.10.101\" | 9779 | 19669 | \"ONLINE\" | 8 | \"basketballplayer:3\" | \"basketballplayer:8\" | \"3.1.0\" | | \"192.168.10.102\" | 9779 | 19669 | \"ONLINE\" | 3 | \"basketballplayer:3\" | \"basketballplayer:8\" | \"3.1.0\" | | \"192.168.10.103\" | 9779 | 19669 | \"ONLINE\" | 0 | \"basketballplayer:2\" | \"basketballplayer:7\" | \"3.1.0\" | | \"192.168.10.104\" | 9779 | 19669 | \"ONLINE\" | 0 | \"basketballplayer:2\" | \"basketballplayer:7\" | \"3.1.0\" | | \"192.168.10.105\" | 9779 | 19669 | \"ONLINE\" | 0 | \"basketballplayer:2\" | \"basketballplayer:7\" | \"3.1.0\" | +------------------+------+-----------+----------+--------------+-----------------------------------+------------------------+---------+ Caution In Nebula Graph 3.1.0, switching leaders will cause a large number of short-term request errors (Storage Error E_RPC_FAILURE ). For solutions, FAQ .","title":"Example"},{"location":"8.service-tuning/practice/","text":"Best practices \u00b6 Nebula Graph is used in a variety of industries. This topic presents a few best practices for using Nebula Graph. For more best practices, see Blog . Scenarios \u00b6 Use cases User review Performance Kernel \u00b6 Nebula Graph Source Code Explained: Variable-Length Pattern Matching Adding a Test Case for Nebula Graph BDD-Based Integration Testing Framework for Nebula Graph: Part \u2160 BDD-Based Integration Testing Framework for Nebula Graph: Part II Understanding Subgraph in Nebula Graph 2.0 Full-Text Indexing in Nebula Graph 2.0 Ecosystem tool \u00b6 Validating Import Performance of Nebula Importer Community Contribution | Nebula Graph 2.0 Performance Testing Ecosystem Tools: Nebula Graph Dashboard for Monitoring Visualizing Graph Data with Nebula Explorer","title":"Best practices"},{"location":"8.service-tuning/practice/#best_practices","text":"Nebula Graph is used in a variety of industries. This topic presents a few best practices for using Nebula Graph. For more best practices, see Blog .","title":"Best practices"},{"location":"8.service-tuning/practice/#scenarios","text":"Use cases User review Performance","title":"Scenarios"},{"location":"8.service-tuning/practice/#kernel","text":"Nebula Graph Source Code Explained: Variable-Length Pattern Matching Adding a Test Case for Nebula Graph BDD-Based Integration Testing Framework for Nebula Graph: Part \u2160 BDD-Based Integration Testing Framework for Nebula Graph: Part II Understanding Subgraph in Nebula Graph 2.0 Full-Text Indexing in Nebula Graph 2.0","title":"Kernel"},{"location":"8.service-tuning/practice/#ecosystem_tool","text":"Validating Import Performance of Nebula Importer Community Contribution | Nebula Graph 2.0 Performance Testing Ecosystem Tools: Nebula Graph Dashboard for Monitoring Visualizing Graph Data with Nebula Explorer","title":"Ecosystem tool"},{"location":"8.service-tuning/super-node/","text":"Processing super vertices \u00b6 Principle introduction \u00b6 In graph theory, a super vertex, also known as a dense vertex, is a vertex with an extremely high number of adjacent edges. The edges can be outgoing or incoming. Super vertices are very common because of the power-law distribution. For example, popular leaders in social networks (Internet celebrities), top stocks in the stock market, Big Four in the banking system, hubs in transportation networks, websites with high clicking rates on the Internet, and best sellers in E-commerce. In Nebula Graph 3.1.0, a vertex and its properties form a key-value pair , with its VID and other meta information as the key . Its Out-Edge Key-Value and In-Edge Key-Value are stored in the same partition in the form of LSM-trees in hard disks and caches. Therefore, directed traversals from this vertex and directed traversals ending at this vertex both involve either a large number of sequential IO scans (ideally, after Compaction or a large number of random IO (frequent writes to the vertex and its ingoing and outgoing edges ). As a rule of thumb, a vertex is considered dense when the number of its edges exceeds 10,000. Some special cases require additional consideration\u3002 Note In Nebula Graph 3.1.0, there is not any data structure to store the out/in degree for each vertex. Therefore, there is no direct method to know whether it is a super vertex or not. You can try to use Spark to count the degrees periodically. Indexes for duplicate properties \u00b6 In a property graph, there is another class of cases similar to super vertices: a property has a very high duplication rate , i.e., many vertices with the same tag but different VIDs have identical property and property values. Property indexes in Nebula Graph 3.1.0 are designed to reuse the functionality of RocksDB in the Storage Service, in which case indexes are modeled as keys with the same prefix . If the lookup of a property fails to hit the cache, it is processed as a random seek and a sequential prefix scan on the hard disk to find the corresponding VID. After that, the graph is usually traversed from this vertex, so that another random read and sequential scan for the corresponding key-value of this vertex will be triggered. The higher the duplication rate, the larger the scan range. For more information about property indexes, see How indexing works in Nebula Graph . Usually, special design and processing are required when the number of duplicate property values exceeds 10,000. Suggested solutions \u00b6 Solutions at the database end \u00b6 Truncation : Only return a certain number (a threshold) of edges, and do not return other edges exceeding this threshold. Compact : Reorganize the order of data in RocksDB to reduce random reads and increase sequential reads. Solutions at the application end \u00b6 Break up some of the super vertices according to their business significance: Delete multiple edges and merge them into one. For example, in the transfer scenario (Account_A)-[TRANSFER]->(Account_B) , each transfer record is modeled as an edge between account A and account B, then there may be tens of thousands of transfer records between (Account_A) and (Account_B) . In such scenarios, merge obsolete transfer details on a daily, weekly, or monthly basis. That is, batch-delete old edges and replace them with a small number of edges representing monthly total and times . And keep the transfer details of the latest month. Split an edge into multiple edges of different types. For example, in the (Airport)<-[DEPART]-(Flight) scenario, the departure of each flight is modeled as an edge between a flight and an airport. Departures from a big airport might be enormous. According to different airlines, divide the DEPART edge type into finer edge types, such as DEPART_CEAIR , DEPART_CSAIR , etc. Specify the departing airline in queries (graph traversal). Split vertices. For example, in the loan network (person)-[BORROW]->(bank) , large bank A will have a very large number of loans and borrowers. In such scenarios, you can split the large vertex A into connected sub-vertices A1, A2, and A3. (Person1)-[BORROW]->(BankA1), (Person2)-[BORROW]->(BankA2), (Person2)-[BORROW]->(BankA3); (BankA1)-[BELONGS_TO]->(BankA), (BankA2)-[BELONGS_TO]->(BankA), (BankA3)-[BELONGS_TO]->(BankA). A1, A2, and A3 can either be three real branches of bank A, such as Beijing branch, Shanghai branch, and Zhejiang branch, or three virtual branches set up according to certain rules, such as A1: 1-1000, A2: 1001-10000 and A3: 10000+ according to the number of loans. In this way, any operation on A is converted into three separate operations on A1, A2, and A3.","title":"Processing super vertices"},{"location":"8.service-tuning/super-node/#processing_super_vertices","text":"","title":"Processing super vertices"},{"location":"8.service-tuning/super-node/#principle_introduction","text":"In graph theory, a super vertex, also known as a dense vertex, is a vertex with an extremely high number of adjacent edges. The edges can be outgoing or incoming. Super vertices are very common because of the power-law distribution. For example, popular leaders in social networks (Internet celebrities), top stocks in the stock market, Big Four in the banking system, hubs in transportation networks, websites with high clicking rates on the Internet, and best sellers in E-commerce. In Nebula Graph 3.1.0, a vertex and its properties form a key-value pair , with its VID and other meta information as the key . Its Out-Edge Key-Value and In-Edge Key-Value are stored in the same partition in the form of LSM-trees in hard disks and caches. Therefore, directed traversals from this vertex and directed traversals ending at this vertex both involve either a large number of sequential IO scans (ideally, after Compaction or a large number of random IO (frequent writes to the vertex and its ingoing and outgoing edges ). As a rule of thumb, a vertex is considered dense when the number of its edges exceeds 10,000. Some special cases require additional consideration\u3002 Note In Nebula Graph 3.1.0, there is not any data structure to store the out/in degree for each vertex. Therefore, there is no direct method to know whether it is a super vertex or not. You can try to use Spark to count the degrees periodically.","title":"Principle introduction"},{"location":"8.service-tuning/super-node/#indexes_for_duplicate_properties","text":"In a property graph, there is another class of cases similar to super vertices: a property has a very high duplication rate , i.e., many vertices with the same tag but different VIDs have identical property and property values. Property indexes in Nebula Graph 3.1.0 are designed to reuse the functionality of RocksDB in the Storage Service, in which case indexes are modeled as keys with the same prefix . If the lookup of a property fails to hit the cache, it is processed as a random seek and a sequential prefix scan on the hard disk to find the corresponding VID. After that, the graph is usually traversed from this vertex, so that another random read and sequential scan for the corresponding key-value of this vertex will be triggered. The higher the duplication rate, the larger the scan range. For more information about property indexes, see How indexing works in Nebula Graph . Usually, special design and processing are required when the number of duplicate property values exceeds 10,000.","title":"Indexes for duplicate properties"},{"location":"8.service-tuning/super-node/#suggested_solutions","text":"","title":"Suggested solutions"},{"location":"8.service-tuning/super-node/#solutions_at_the_database_end","text":"Truncation : Only return a certain number (a threshold) of edges, and do not return other edges exceeding this threshold. Compact : Reorganize the order of data in RocksDB to reduce random reads and increase sequential reads.","title":"Solutions at the database end"},{"location":"8.service-tuning/super-node/#solutions_at_the_application_end","text":"Break up some of the super vertices according to their business significance: Delete multiple edges and merge them into one. For example, in the transfer scenario (Account_A)-[TRANSFER]->(Account_B) , each transfer record is modeled as an edge between account A and account B, then there may be tens of thousands of transfer records between (Account_A) and (Account_B) . In such scenarios, merge obsolete transfer details on a daily, weekly, or monthly basis. That is, batch-delete old edges and replace them with a small number of edges representing monthly total and times . And keep the transfer details of the latest month. Split an edge into multiple edges of different types. For example, in the (Airport)<-[DEPART]-(Flight) scenario, the departure of each flight is modeled as an edge between a flight and an airport. Departures from a big airport might be enormous. According to different airlines, divide the DEPART edge type into finer edge types, such as DEPART_CEAIR , DEPART_CSAIR , etc. Specify the departing airline in queries (graph traversal). Split vertices. For example, in the loan network (person)-[BORROW]->(bank) , large bank A will have a very large number of loans and borrowers. In such scenarios, you can split the large vertex A into connected sub-vertices A1, A2, and A3. (Person1)-[BORROW]->(BankA1), (Person2)-[BORROW]->(BankA2), (Person2)-[BORROW]->(BankA3); (BankA1)-[BELONGS_TO]->(BankA), (BankA2)-[BELONGS_TO]->(BankA), (BankA3)-[BELONGS_TO]->(BankA). A1, A2, and A3 can either be three real branches of bank A, such as Beijing branch, Shanghai branch, and Zhejiang branch, or three virtual branches set up according to certain rules, such as A1: 1-1000, A2: 1001-10000 and A3: 10000+ according to the number of loans. In this way, any operation on A is converted into three separate operations on A1, A2, and A3.","title":"Solutions at the application end"},{"location":"backup-and-restore/3.manage-snapshot/","text":"Backup and restore data with snapshots \u00b6 Nebula Graph supports using snapshots to back up and restore data. When data loss or misoperation occurs, the data will be restored through the snapshot. Prerequisites \u00b6 Nebula Graph authentication is disabled by default. In this case, all users can use the snapshot feature. If authentication is enabled, only the GOD role user can use the snapshot feature. For more information about roles, see Roles and privileges . Precautions \u00b6 To prevent data loss, create a snapshot as soon as the system structure changes, for example, after operations such as ADD HOST , DROP HOST , CREATE SPACE , DROP SPACE , and BALANCE are performed. Nebula Graph cannot automatically delete the invalid files created by a failed snapshot task. You have to manually delete them by using DROP SNAPSHOT . Customizing the storage path for snapshots is not supported for now. The default path is /usr/local/nebula/data . Snapshot form and path \u00b6 Nebula Graph snapshots are stored in the form of directories with names like SNAPSHOT_2021_03_09_08_43_12 . The suffix 2021_03_09_08_43_12 is generated automatically based on the creation time (UTC). When a snapshot is created, snapshot directories will be automatically created in the checkpoints directory on the leader Meta server and each Storage server. To fast locate the path where the snapshots are stored, you can use the Linux command find . For example: $ find | grep 'SNAPSHOT_2021_03_09_08_43_12' ./data/meta2/nebula/0/checkpoints/SNAPSHOT_2021_03_09_08_43_12 ./data/meta2/nebula/0/checkpoints/SNAPSHOT_2021_03_09_08_43_12/data ./data/meta2/nebula/0/checkpoints/SNAPSHOT_2021_03_09_08_43_12/data/000081.sst ... Create snapshots \u00b6 Run CREATE SNAPSHOT to create a snapshot for all the graph spaces based on the current time for Nebula Graph. Creating a snapshot for a specific graph space is not supported yet. Note If the creation fails, delete the snapshot and try again. nebula> CREATE SNAPSHOT; View snapshots \u00b6 To view all existing snapshots, run SHOW SNAPSHOTS . nebula> SHOW SNAPSHOTS; +--------------------------------+---------+------------------+ | Name | Status | Hosts | +--------------------------------+---------+------------------+ | \"SNAPSHOT_2021_03_09_08_43_12\" | \"VALID\" | \"127.0.0.1:9779\" | | \"SNAPSHOT_2021_03_09_09_10_52\" | \"VALID\" | \"127.0.0.1:9779\" | +--------------------------------+---------+------------------+ The parameters in the return information are described as follows. | Parameter | Description | |-----------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------| | Name | The name of the snapshot directory. The prefix SNAPSHOT indicates that the file is a snapshot file, and the suffix indicates the time the snapshot was created (UTC). | | Status | The status of the snapshot. VALID indicates that the creation succeeded, while INVALID indicates that it failed. | | Hosts | IP addresses and ports of all Storage servers at the time the snapshot was created. | Delete snapshots \u00b6 To delete a snapshot with the given name, run DROP SNAPSHOT . DROP SNAPSHOT <snapshot_name>; Example: nebula> DROP SNAPSHOT SNAPSHOT_2021_03_09_08_43_12; nebula> SHOW SNAPSHOTS; +--------------------------------+---------+------------------+ | Name | Status | Hosts | +--------------------------------+---------+------------------+ | \"SNAPSHOT_2021_03_09_09_10_52\" | \"VALID\" | \"127.0.0.1:9779\" | +--------------------------------+---------+------------------+ Restore data with snapshots \u00b6 Currently, there is no command to restore data with snapshots. You need to manually copy the snapshot file to the corresponding folder, or you can make it by using a shell script. The logic implements as follows: After the snapshot is created, the checkpoints directory is generated in the installation directory of the leader Meta server and all Storage servers, and saves the created snapshot. Taking this topic as an example, when there are two graph spaces, the snapshots created are saved in /usr/local/nebula/data/meta/nebula/0/checkpoints , /usr/local/nebula/data/storage/ nebula/3/checkpoints and /usr/local/nebula/data/storage/nebula/4/checkpoints . $ ls /usr/local/nebula/data/meta/nebula/0/checkpoints/ SNAPSHOT_2021_03_09_09_10_52 $ ls /usr/local/nebula/data/storage/nebula/3/checkpoints/ SNAPSHOT_2021_03_09_09_10_52 $ ls /usr/local/nebula/data/storage/nebula/4/checkpoints/ SNAPSHOT_2021_03_09_09_10_52 To restore the lost data through snapshots, you can take a snapshot at an appropriate time, copy the folders data and wal in the corresponding snapshot directory to its parent directory (at the same level with checkpoints ) to overwrite the previous data and wal , and then restart the cluster. Caution The data and wal directories of all Meta servers should be overwritten at the same time. Otherwise, the new leader Meta server will use the latest Meta data after a cluster is restarted.","title":"Manage snapshots"},{"location":"backup-and-restore/3.manage-snapshot/#backup_and_restore_data_with_snapshots","text":"Nebula Graph supports using snapshots to back up and restore data. When data loss or misoperation occurs, the data will be restored through the snapshot.","title":"Backup and restore data with snapshots"},{"location":"backup-and-restore/3.manage-snapshot/#prerequisites","text":"Nebula Graph authentication is disabled by default. In this case, all users can use the snapshot feature. If authentication is enabled, only the GOD role user can use the snapshot feature. For more information about roles, see Roles and privileges .","title":"Prerequisites"},{"location":"backup-and-restore/3.manage-snapshot/#precautions","text":"To prevent data loss, create a snapshot as soon as the system structure changes, for example, after operations such as ADD HOST , DROP HOST , CREATE SPACE , DROP SPACE , and BALANCE are performed. Nebula Graph cannot automatically delete the invalid files created by a failed snapshot task. You have to manually delete them by using DROP SNAPSHOT . Customizing the storage path for snapshots is not supported for now. The default path is /usr/local/nebula/data .","title":"Precautions"},{"location":"backup-and-restore/3.manage-snapshot/#snapshot_form_and_path","text":"Nebula Graph snapshots are stored in the form of directories with names like SNAPSHOT_2021_03_09_08_43_12 . The suffix 2021_03_09_08_43_12 is generated automatically based on the creation time (UTC). When a snapshot is created, snapshot directories will be automatically created in the checkpoints directory on the leader Meta server and each Storage server. To fast locate the path where the snapshots are stored, you can use the Linux command find . For example: $ find | grep 'SNAPSHOT_2021_03_09_08_43_12' ./data/meta2/nebula/0/checkpoints/SNAPSHOT_2021_03_09_08_43_12 ./data/meta2/nebula/0/checkpoints/SNAPSHOT_2021_03_09_08_43_12/data ./data/meta2/nebula/0/checkpoints/SNAPSHOT_2021_03_09_08_43_12/data/000081.sst ...","title":"Snapshot form and path"},{"location":"backup-and-restore/3.manage-snapshot/#create_snapshots","text":"Run CREATE SNAPSHOT to create a snapshot for all the graph spaces based on the current time for Nebula Graph. Creating a snapshot for a specific graph space is not supported yet. Note If the creation fails, delete the snapshot and try again. nebula> CREATE SNAPSHOT;","title":"Create snapshots"},{"location":"backup-and-restore/3.manage-snapshot/#view_snapshots","text":"To view all existing snapshots, run SHOW SNAPSHOTS . nebula> SHOW SNAPSHOTS; +--------------------------------+---------+------------------+ | Name | Status | Hosts | +--------------------------------+---------+------------------+ | \"SNAPSHOT_2021_03_09_08_43_12\" | \"VALID\" | \"127.0.0.1:9779\" | | \"SNAPSHOT_2021_03_09_09_10_52\" | \"VALID\" | \"127.0.0.1:9779\" | +--------------------------------+---------+------------------+ The parameters in the return information are described as follows. | Parameter | Description | |-----------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------| | Name | The name of the snapshot directory. The prefix SNAPSHOT indicates that the file is a snapshot file, and the suffix indicates the time the snapshot was created (UTC). | | Status | The status of the snapshot. VALID indicates that the creation succeeded, while INVALID indicates that it failed. | | Hosts | IP addresses and ports of all Storage servers at the time the snapshot was created. |","title":"View snapshots"},{"location":"backup-and-restore/3.manage-snapshot/#delete_snapshots","text":"To delete a snapshot with the given name, run DROP SNAPSHOT . DROP SNAPSHOT <snapshot_name>; Example: nebula> DROP SNAPSHOT SNAPSHOT_2021_03_09_08_43_12; nebula> SHOW SNAPSHOTS; +--------------------------------+---------+------------------+ | Name | Status | Hosts | +--------------------------------+---------+------------------+ | \"SNAPSHOT_2021_03_09_09_10_52\" | \"VALID\" | \"127.0.0.1:9779\" | +--------------------------------+---------+------------------+","title":"Delete snapshots"},{"location":"backup-and-restore/3.manage-snapshot/#restore_data_with_snapshots","text":"Currently, there is no command to restore data with snapshots. You need to manually copy the snapshot file to the corresponding folder, or you can make it by using a shell script. The logic implements as follows: After the snapshot is created, the checkpoints directory is generated in the installation directory of the leader Meta server and all Storage servers, and saves the created snapshot. Taking this topic as an example, when there are two graph spaces, the snapshots created are saved in /usr/local/nebula/data/meta/nebula/0/checkpoints , /usr/local/nebula/data/storage/ nebula/3/checkpoints and /usr/local/nebula/data/storage/nebula/4/checkpoints . $ ls /usr/local/nebula/data/meta/nebula/0/checkpoints/ SNAPSHOT_2021_03_09_09_10_52 $ ls /usr/local/nebula/data/storage/nebula/3/checkpoints/ SNAPSHOT_2021_03_09_09_10_52 $ ls /usr/local/nebula/data/storage/nebula/4/checkpoints/ SNAPSHOT_2021_03_09_09_10_52 To restore the lost data through snapshots, you can take a snapshot at an appropriate time, copy the folders data and wal in the corresponding snapshot directory to its parent directory (at the same level with checkpoints ) to overwrite the previous data and wal , and then restart the cluster. Caution The data and wal directories of all Meta servers should be overwritten at the same time. Otherwise, the new leader Meta server will use the latest Meta data after a cluster is restarted.","title":"Restore data with snapshots"},{"location":"backup-and-restore/nebula-br/1.what-is-br/","text":"What is Backup & Restore \u00b6 Backup & Restore (BR for short) is a Command-Line Interface (CLI) tool to back up data of graph spaces of Nebula Graph and to restore data from the backup files. Features \u00b6 The BR has the following features. It supports: Backing up and restoring data in a one-click operation. Restoring data in the following backup file types: Local Disk (SSD or HDD). It is recommend to use local disk in test environment only. Amazon S3 compatible interface, such as Alibaba Cloud OSS, MinIO\u3001Ceph RGW, etc. Backing up and restoring the entire Nebula Graph cluster. Backing up data of specified graph spaces (experimental). Limitations \u00b6 Supports Nebula Graph v3.1.0 only. Supports full backup, but not incremental backup. Currently, Nebula Listener and full-text indexes do not support backup. Backup and restore are supported when there is only one metad process configured in the local file. If you back up data to the local disk, the backup files will be saved in the local path of each server. You can also mount the NFS on your host to restore the backup data to a different host. The backup graph space can be restored to the original cluster only. Cross clusters restoration is not supported. During the backup process, both DDL and DML statements in the specified graph spaces are blocked. We recommend that you do the operation within the low peak period of the business, for example, from 2:00 AM to 5:00 AM. Supports restoration of data on clusters of the same topologies only, which means both clusters must have exactly the same number of hosts. We recommend that you restore the data OFFLINE because it requires removing the data and restart the service. If you backup data of a specified graph space in cluster A, the backup files CAN NOT be restored to cluster B. Restore specified graph space will delete all other graph space in the cluster. (experimental). How to use BR \u00b6 To use the BR, follow these steps: Compile BR . Use BR to back up data . Use BR to restore data from backup files .","title":"What is Backup & Restore"},{"location":"backup-and-restore/nebula-br/1.what-is-br/#what_is_backup_restore","text":"Backup & Restore (BR for short) is a Command-Line Interface (CLI) tool to back up data of graph spaces of Nebula Graph and to restore data from the backup files.","title":"What is Backup &amp; Restore"},{"location":"backup-and-restore/nebula-br/1.what-is-br/#features","text":"The BR has the following features. It supports: Backing up and restoring data in a one-click operation. Restoring data in the following backup file types: Local Disk (SSD or HDD). It is recommend to use local disk in test environment only. Amazon S3 compatible interface, such as Alibaba Cloud OSS, MinIO\u3001Ceph RGW, etc. Backing up and restoring the entire Nebula Graph cluster. Backing up data of specified graph spaces (experimental).","title":"Features"},{"location":"backup-and-restore/nebula-br/1.what-is-br/#limitations","text":"Supports Nebula Graph v3.1.0 only. Supports full backup, but not incremental backup. Currently, Nebula Listener and full-text indexes do not support backup. Backup and restore are supported when there is only one metad process configured in the local file. If you back up data to the local disk, the backup files will be saved in the local path of each server. You can also mount the NFS on your host to restore the backup data to a different host. The backup graph space can be restored to the original cluster only. Cross clusters restoration is not supported. During the backup process, both DDL and DML statements in the specified graph spaces are blocked. We recommend that you do the operation within the low peak period of the business, for example, from 2:00 AM to 5:00 AM. Supports restoration of data on clusters of the same topologies only, which means both clusters must have exactly the same number of hosts. We recommend that you restore the data OFFLINE because it requires removing the data and restart the service. If you backup data of a specified graph space in cluster A, the backup files CAN NOT be restored to cluster B. Restore specified graph space will delete all other graph space in the cluster. (experimental).","title":"Limitations"},{"location":"backup-and-restore/nebula-br/1.what-is-br/#how_to_use_br","text":"To use the BR, follow these steps: Compile BR . Use BR to back up data . Use BR to restore data from backup files .","title":"How to use BR"},{"location":"backup-and-restore/nebula-br/2.compile-br/","text":"Compile BR \u00b6 For now, the BR is not provided as a package. You need to compile the BR first. Prerequisites \u00b6 To compile the BR, do a check of these: Go 1.14.x or a later version is installed. make is installed. Procedures \u00b6 To compile the BR, follow these steps: Clone the nebula-br repository to your machine. git clone https://github.com/vesoft-inc/nebula-br.git Change to the br directory. cd nebula-br Compile the BR. make Users can enter bin/br version on the command line. If the following results are returned, the BR is compiled successfully. [ nebula-br ] $ bin/br version Nebula Backup And Restore Utility Tool,V-0.6.0","title":"Compile BR"},{"location":"backup-and-restore/nebula-br/2.compile-br/#compile_br","text":"For now, the BR is not provided as a package. You need to compile the BR first.","title":"Compile BR"},{"location":"backup-and-restore/nebula-br/2.compile-br/#prerequisites","text":"To compile the BR, do a check of these: Go 1.14.x or a later version is installed. make is installed.","title":"Prerequisites"},{"location":"backup-and-restore/nebula-br/2.compile-br/#procedures","text":"To compile the BR, follow these steps: Clone the nebula-br repository to your machine. git clone https://github.com/vesoft-inc/nebula-br.git Change to the br directory. cd nebula-br Compile the BR. make Users can enter bin/br version on the command line. If the following results are returned, the BR is compiled successfully. [ nebula-br ] $ bin/br version Nebula Backup And Restore Utility Tool,V-0.6.0","title":"Procedures"},{"location":"backup-and-restore/nebula-br/3.br-backup-data/","text":"Use BR to back up data \u00b6 After the BR is compiled, you can back up data of the entire graph space. This topic introduces how to use the BR to back up data. Prerequisites \u00b6 To back up data with the BR, do a check of these: The BR is compiled. For more information, see Compile BR . The Nebula Graph services are running. The nebula-agent has been downloaded and the nebula-agent service is running on each host in the cluster. If you store the backup files locally, create a directory with the same absolute path on the meta servers, the storage servers, and the BR machine for the backup files and get the absolute path. Make sure the account has write privileges for this directory. Note In the production environment, we recommend that you mount Network File System (NFS) storage to the meta servers, the storage servers, and the BR machine for local backup, or use Amazon S3 or Alibaba Cloud OSS for remote backup. When you restore the data from local files, you must manually move these backup files to a specified directory, which causes redundant data and troubles. For more information, see Restore data from backup files . Procedure \u00b6 Run the following command to perform a full backup for the entire cluster. Note Make sure that the local path where the backup file is stored exists. $ ./bin/br backup full --meta <ip_address> --storage <storage_path> For example: Run the following command to perform a full backup for the entire cluster whose meta service address is 127.0.0.1:9559 , and save the backup file to /home/nebula/backup/ . Caution If there are multiple metad addresses, you can use any one of them. $ ./bin/br backup full --meta \"127.0.0.1:9559\" --storage \"local:///home/nebula/backup/\" Run the following command to perform a full backup for the entire cluster whose meta service address is 127.0.0.1:9559 , and save the backup file to backup in the br-test bucket of the object storage service compatible with S3 protocol. $ ./bin/br backup full --meta \"127.0.0.1:9559\" --s3.endpoint \"http://127.0.0.1:9000\" --storage = \"s3://br-test/backup/\" --s3.access_key = minioadmin --s3.secret_key = minioadmin --s3.region = default The parameters are as follows. Parameter Data type Required Default value Description -h,-help - No None Checks help for restoration. -debug - No None Checks for more log information. -log string No \"br.log\" Specifies detailed log path for restoration and backup. -meta string Yes None The IP address and port of the meta service. -name string Yes None The name of backup. --storage string Yes None The target storage URL of BR backup data. The format is: \\<Schema>://\\<PATH>. Schema: Optional values are local and s3 . When selecting s3, you need to fill in s3.access_key , s3.endpoint , s3.region , and s3.secret_key . PATH: The path of the storage location. --s3.access_key string No None Sets AccessKey ID. --s3.endpoint string No None Sets the S3 endpoint URL, please specify the HTTP or HTTPS scheme explicitly. --s3.region string No None Sets the region or location to upload or download the backup. --s3.secret_key string No None Sets SecretKey for AccessKey ID. Next to do \u00b6 After the backup files are generated, you can use the BR to restore them for Nebula Graph. For more information, see Use BR to restore data .","title":"Use BR to back up data"},{"location":"backup-and-restore/nebula-br/3.br-backup-data/#use_br_to_back_up_data","text":"After the BR is compiled, you can back up data of the entire graph space. This topic introduces how to use the BR to back up data.","title":"Use BR to back up data"},{"location":"backup-and-restore/nebula-br/3.br-backup-data/#prerequisites","text":"To back up data with the BR, do a check of these: The BR is compiled. For more information, see Compile BR . The Nebula Graph services are running. The nebula-agent has been downloaded and the nebula-agent service is running on each host in the cluster. If you store the backup files locally, create a directory with the same absolute path on the meta servers, the storage servers, and the BR machine for the backup files and get the absolute path. Make sure the account has write privileges for this directory. Note In the production environment, we recommend that you mount Network File System (NFS) storage to the meta servers, the storage servers, and the BR machine for local backup, or use Amazon S3 or Alibaba Cloud OSS for remote backup. When you restore the data from local files, you must manually move these backup files to a specified directory, which causes redundant data and troubles. For more information, see Restore data from backup files .","title":"Prerequisites"},{"location":"backup-and-restore/nebula-br/3.br-backup-data/#procedure","text":"Run the following command to perform a full backup for the entire cluster. Note Make sure that the local path where the backup file is stored exists. $ ./bin/br backup full --meta <ip_address> --storage <storage_path> For example: Run the following command to perform a full backup for the entire cluster whose meta service address is 127.0.0.1:9559 , and save the backup file to /home/nebula/backup/ . Caution If there are multiple metad addresses, you can use any one of them. $ ./bin/br backup full --meta \"127.0.0.1:9559\" --storage \"local:///home/nebula/backup/\" Run the following command to perform a full backup for the entire cluster whose meta service address is 127.0.0.1:9559 , and save the backup file to backup in the br-test bucket of the object storage service compatible with S3 protocol. $ ./bin/br backup full --meta \"127.0.0.1:9559\" --s3.endpoint \"http://127.0.0.1:9000\" --storage = \"s3://br-test/backup/\" --s3.access_key = minioadmin --s3.secret_key = minioadmin --s3.region = default The parameters are as follows. Parameter Data type Required Default value Description -h,-help - No None Checks help for restoration. -debug - No None Checks for more log information. -log string No \"br.log\" Specifies detailed log path for restoration and backup. -meta string Yes None The IP address and port of the meta service. -name string Yes None The name of backup. --storage string Yes None The target storage URL of BR backup data. The format is: \\<Schema>://\\<PATH>. Schema: Optional values are local and s3 . When selecting s3, you need to fill in s3.access_key , s3.endpoint , s3.region , and s3.secret_key . PATH: The path of the storage location. --s3.access_key string No None Sets AccessKey ID. --s3.endpoint string No None Sets the S3 endpoint URL, please specify the HTTP or HTTPS scheme explicitly. --s3.region string No None Sets the region or location to upload or download the backup. --s3.secret_key string No None Sets SecretKey for AccessKey ID.","title":"Procedure"},{"location":"backup-and-restore/nebula-br/3.br-backup-data/#next_to_do","text":"After the backup files are generated, you can use the BR to restore them for Nebula Graph. For more information, see Use BR to restore data .","title":"Next to do"},{"location":"backup-and-restore/nebula-br/4.br-restore-data/","text":"Use BR to restore data \u00b6 If you use the BR to back up data, you can use it to restore the data to Nebula Graph. This topic introduces how to use the BR to restore data from backup files. Caution During the restoration process, the data on the target Nebula Graph cluster is removed and then is replaced with the data from the backup files. If necessary, back up the data on the target cluster. Caution The restoration process is performed OFFLINE. Prerequisites \u00b6 To restore data with the BR, do a check of these: The BR is compiled. For more information, see Compile BR . Download nebula-agent and start the agent service in each cluster(including metad, storaged, graphd) host. No application is connected to the target Nebula Graph cluster. Make sure that the target and the source Nebula Graph clusters have the same topology, which means that they have exactly the same number of hosts. The number of data folders for each host is consistently distributed. Procedures \u00b6 Users can use the following command to list the existing backup information: $ ./bin/br show --storage <ip_address> For example, run the following command to list the backup information in the local /home/nebula/backup path. $ ./bin/br show --storage \"local:///home/nebula/backup\" +----------------------------+---------------------+------------------------+-------------+------------+ | NAME | CREATE TIME | SPACES | FULL BACKUP | ALL SPACES | +----------------------------+---------------------+------------------------+-------------+------------+ | BACKUP_2022_02_10_07_40_41 | 2022 -02-10 07 :40:41 | basketballplayer | true | true | | BACKUP_2022_02_11_08_26_43 | 2022 -02-11 08 :26:47 | basketballplayer,foesa | true | true | +----------------------------+---------------------+------------------------+-------------+------------+ Or, you can run the following command to list the backup information stored in S3 URL s3://127.0.0.1:9000/br-test/backup . $ ./bin/br show --s3.endpoint \"http://127.0.0.1:9000\" --storage = \"s3://br-test/backup/\" --s3.access_key = minioadmin --s3.secret_key = minioadmin --s3.region = default Parameter Data type Required Default value Description -h,-help - No None Checks help for restoration. -debug - No None Checks for more log information. -log string No \"br.log\" Specifies detailed log path for restoration and backup. --storage string Yes None The target storage URL of BR backup data. The format is: <Schema>://<PATH>. Schema: Optional values are local and s3 . When selecting s3, you need to fill in s3.access_key , s3.endpoint , s3.region , and s3.secret_key . PATH: The path of the storage location. --s3.access_key string No None Sets AccessKey ID. --s3.endpoint string No None Sets the S3 endpoint URL, please specify the HTTP or HTTPS scheme explicitly. --s3.region string No None Sets the region or location to upload or download the backup. --s3.secret_key string No None Sets SecretKey for AccessKey ID. Run the following command to restore data. $ ./bin/br restore full --meta <ip_address> --storage <storage_path> --name <backup_name> For example, run the following command to upload the backup files from the local /home/nebula/backup/ to the cluster where the meta service's address is 127.0.0.1:9559 . $ ./bin/br restore full --meta \"127.0.0.1:9559\" --storage \"local:///home/nebula/backup/\" --name BACKUP_2021_12_08_18_38_08 Or, you can run the following command to upload the backup files from the S3 URL s3://127.0.0.1:9000/br-test/backup . $ ./bin/br restore full --meta \"127.0.0.1:9559\" --s3.endpoint \"http://127.0.0.1:9000\" --storage = \"s3://br-test/backup/\" --s3.access_key = minioadmin --s3.secret_key = minioadmin --s3.region = \"default\" --name BACKUP_2021_12_08_18_38_08 If the following information is returned, the data is restored successfully. Restore succeed. Caution If your new cluster hosts' IPs are not all the same as the backup cluster, after restoration, you should run add hosts to add the Storage host IPs in the new cluster one by one. The parameters are as follows. Parameter Data type Required Default value Description -h,-help - No None Checks help for restoration. -debug - No None Checks for more log information. -log string No \"br.log\" Specifies detailed log path for restoration and backup. -meta string Yes None The IP address and port of the meta service. -name string Yes None The name of backup. --storage string Yes None The target storage URL of BR backup data. The format is: \\<Schema>://\\<PATH>. Schema: Optional values are local and s3 . When selecting s3, you need to fill in s3.access_key , s3.endpoint , s3.region , and s3.secret_key . PATH: The path of the storage location. --s3.access_key string No None Sets AccessKey ID. --s3.endpoint string No None Sets the S3 endpoint URL, please specify the HTTP or HTTPS scheme explicitly. --s3.region string No None Sets the region or location to upload or download the backup. --s3.secret_key string No None Sets SecretKey for AccessKey ID. Run the following command to clean up temporary files if any error occurred during backup. It will clean the files in cluster and external storage. You could also use it to clean up old backups files in external storage. $ ./bin/br cleanup --meta <ip_address> --storage <storage_path> --name <backup_name> The parameters are as follows. Parameter Data type Required Default value Description -h,-help - No None Checks help for restoration. -debug - No None Checks for more log information. -log string No \"br.log\" Specifies detailed log path for restoration and backup. -meta string Yes None The IP address and port of the meta service. -name string Yes None The name of backup. --storage string Yes None The target storage URL of BR backup data. The format is: \\<Schema>://\\<PATH>. Schema: Optional values are local and s3 . When selecting s3, you need to fill in s3.access_key , s3.endpoint , s3.region , and s3.secret_key . PATH: The path of the storage location. --s3.access_key string No None Sets AccessKey ID. --s3.endpoint string No None Sets the S3 endpoint URL, please specify the HTTP or HTTPS scheme explicitly. --s3.region string No None Sets the region or location to upload or download the backup. --s3.secret_key string No None Sets SecretKey for AccessKey ID.","title":"Use BR to restore data"},{"location":"backup-and-restore/nebula-br/4.br-restore-data/#use_br_to_restore_data","text":"If you use the BR to back up data, you can use it to restore the data to Nebula Graph. This topic introduces how to use the BR to restore data from backup files. Caution During the restoration process, the data on the target Nebula Graph cluster is removed and then is replaced with the data from the backup files. If necessary, back up the data on the target cluster. Caution The restoration process is performed OFFLINE.","title":"Use BR to restore data"},{"location":"backup-and-restore/nebula-br/4.br-restore-data/#prerequisites","text":"To restore data with the BR, do a check of these: The BR is compiled. For more information, see Compile BR . Download nebula-agent and start the agent service in each cluster(including metad, storaged, graphd) host. No application is connected to the target Nebula Graph cluster. Make sure that the target and the source Nebula Graph clusters have the same topology, which means that they have exactly the same number of hosts. The number of data folders for each host is consistently distributed.","title":"Prerequisites"},{"location":"backup-and-restore/nebula-br/4.br-restore-data/#procedures","text":"Users can use the following command to list the existing backup information: $ ./bin/br show --storage <ip_address> For example, run the following command to list the backup information in the local /home/nebula/backup path. $ ./bin/br show --storage \"local:///home/nebula/backup\" +----------------------------+---------------------+------------------------+-------------+------------+ | NAME | CREATE TIME | SPACES | FULL BACKUP | ALL SPACES | +----------------------------+---------------------+------------------------+-------------+------------+ | BACKUP_2022_02_10_07_40_41 | 2022 -02-10 07 :40:41 | basketballplayer | true | true | | BACKUP_2022_02_11_08_26_43 | 2022 -02-11 08 :26:47 | basketballplayer,foesa | true | true | +----------------------------+---------------------+------------------------+-------------+------------+ Or, you can run the following command to list the backup information stored in S3 URL s3://127.0.0.1:9000/br-test/backup . $ ./bin/br show --s3.endpoint \"http://127.0.0.1:9000\" --storage = \"s3://br-test/backup/\" --s3.access_key = minioadmin --s3.secret_key = minioadmin --s3.region = default Parameter Data type Required Default value Description -h,-help - No None Checks help for restoration. -debug - No None Checks for more log information. -log string No \"br.log\" Specifies detailed log path for restoration and backup. --storage string Yes None The target storage URL of BR backup data. The format is: <Schema>://<PATH>. Schema: Optional values are local and s3 . When selecting s3, you need to fill in s3.access_key , s3.endpoint , s3.region , and s3.secret_key . PATH: The path of the storage location. --s3.access_key string No None Sets AccessKey ID. --s3.endpoint string No None Sets the S3 endpoint URL, please specify the HTTP or HTTPS scheme explicitly. --s3.region string No None Sets the region or location to upload or download the backup. --s3.secret_key string No None Sets SecretKey for AccessKey ID. Run the following command to restore data. $ ./bin/br restore full --meta <ip_address> --storage <storage_path> --name <backup_name> For example, run the following command to upload the backup files from the local /home/nebula/backup/ to the cluster where the meta service's address is 127.0.0.1:9559 . $ ./bin/br restore full --meta \"127.0.0.1:9559\" --storage \"local:///home/nebula/backup/\" --name BACKUP_2021_12_08_18_38_08 Or, you can run the following command to upload the backup files from the S3 URL s3://127.0.0.1:9000/br-test/backup . $ ./bin/br restore full --meta \"127.0.0.1:9559\" --s3.endpoint \"http://127.0.0.1:9000\" --storage = \"s3://br-test/backup/\" --s3.access_key = minioadmin --s3.secret_key = minioadmin --s3.region = \"default\" --name BACKUP_2021_12_08_18_38_08 If the following information is returned, the data is restored successfully. Restore succeed. Caution If your new cluster hosts' IPs are not all the same as the backup cluster, after restoration, you should run add hosts to add the Storage host IPs in the new cluster one by one. The parameters are as follows. Parameter Data type Required Default value Description -h,-help - No None Checks help for restoration. -debug - No None Checks for more log information. -log string No \"br.log\" Specifies detailed log path for restoration and backup. -meta string Yes None The IP address and port of the meta service. -name string Yes None The name of backup. --storage string Yes None The target storage URL of BR backup data. The format is: \\<Schema>://\\<PATH>. Schema: Optional values are local and s3 . When selecting s3, you need to fill in s3.access_key , s3.endpoint , s3.region , and s3.secret_key . PATH: The path of the storage location. --s3.access_key string No None Sets AccessKey ID. --s3.endpoint string No None Sets the S3 endpoint URL, please specify the HTTP or HTTPS scheme explicitly. --s3.region string No None Sets the region or location to upload or download the backup. --s3.secret_key string No None Sets SecretKey for AccessKey ID. Run the following command to clean up temporary files if any error occurred during backup. It will clean the files in cluster and external storage. You could also use it to clean up old backups files in external storage. $ ./bin/br cleanup --meta <ip_address> --storage <storage_path> --name <backup_name> The parameters are as follows. Parameter Data type Required Default value Description -h,-help - No None Checks help for restoration. -debug - No None Checks for more log information. -log string No \"br.log\" Specifies detailed log path for restoration and backup. -meta string Yes None The IP address and port of the meta service. -name string Yes None The name of backup. --storage string Yes None The target storage URL of BR backup data. The format is: \\<Schema>://\\<PATH>. Schema: Optional values are local and s3 . When selecting s3, you need to fill in s3.access_key , s3.endpoint , s3.region , and s3.secret_key . PATH: The path of the storage location. --s3.access_key string No None Sets AccessKey ID. --s3.endpoint string No None Sets the S3 endpoint URL, please specify the HTTP or HTTPS scheme explicitly. --s3.region string No None Sets the region or location to upload or download the backup. --s3.secret_key string No None Sets SecretKey for AccessKey ID.","title":"Procedures"},{"location":"nebula-dashboard/1.what-is-dashboard/","text":"What is Nebula Dashboard Community Edition \u00b6 Nebula Dashboard Community Edition (Dashboard for short) is a visualization tool that monitors the status of machines and services in Nebula Graph clusters. This topic introduces Dashboard Community Edition. For details of Dashboard Enterprise Edition, refer to What is Nebula Dashboard Enterprise Edition . Enterpriseonly Dashboard Enterprise Edition adds features such as visual cluster creation, batch import of clusters, fast scaling, etc. For more information, see Pricing . Features \u00b6 Dashboard monitors: The status of all the machines in clusters, including CPU, memory, load, disk, and network. The information of all the services in clusters, including the IP addresses, versions, and monitoring metrics (such as the number of queries, the latency of queries, the latency of heartbeats, and so on). The information of clusters, including the information of services, partitions, configurations, and long-term tasks. Features of the enterprise package (TODO: planning) Scenarios \u00b6 You can use Dashboard in one of the following scenarios: You want to monitor key metrics conveniently and quickly, and present multiple key information of the business to ensure the business operates normally. You want to monitor clusters from multiple dimensions (such as the time, aggregate rules, and metrics). After a failure occurs, you need to review it and confirm its occurrence time and unexpected phenomena. Precautions \u00b6 The monitoring data will be updated per 7 seconds by default. The monitoring data will be retained for 14 days by default, that is, only the monitoring data within the last 14 days can be queried. Note The monitoring service is supported by Prometheus. The update frequency and retention intervals can be modified. For details, see Prometheus . Version compatibility \u00b6 The version correspondence between Nebula Graph and Dashboard Community Edition is as follows. Nebula Graph version Dashboard version 2.5.x~3.1.x 1.1.0 2.0.1~2.5.1 1.0.2 2.0.1~2.5.1 1.0.1 Release note \u00b6 Release","title":"What is Nebula Dashboard"},{"location":"nebula-dashboard/1.what-is-dashboard/#what_is_nebula_dashboard_community_edition","text":"Nebula Dashboard Community Edition (Dashboard for short) is a visualization tool that monitors the status of machines and services in Nebula Graph clusters. This topic introduces Dashboard Community Edition. For details of Dashboard Enterprise Edition, refer to What is Nebula Dashboard Enterprise Edition . Enterpriseonly Dashboard Enterprise Edition adds features such as visual cluster creation, batch import of clusters, fast scaling, etc. For more information, see Pricing .","title":"What is Nebula Dashboard Community Edition"},{"location":"nebula-dashboard/1.what-is-dashboard/#features","text":"Dashboard monitors: The status of all the machines in clusters, including CPU, memory, load, disk, and network. The information of all the services in clusters, including the IP addresses, versions, and monitoring metrics (such as the number of queries, the latency of queries, the latency of heartbeats, and so on). The information of clusters, including the information of services, partitions, configurations, and long-term tasks. Features of the enterprise package (TODO: planning)","title":"Features"},{"location":"nebula-dashboard/1.what-is-dashboard/#scenarios","text":"You can use Dashboard in one of the following scenarios: You want to monitor key metrics conveniently and quickly, and present multiple key information of the business to ensure the business operates normally. You want to monitor clusters from multiple dimensions (such as the time, aggregate rules, and metrics). After a failure occurs, you need to review it and confirm its occurrence time and unexpected phenomena.","title":"Scenarios"},{"location":"nebula-dashboard/1.what-is-dashboard/#precautions","text":"The monitoring data will be updated per 7 seconds by default. The monitoring data will be retained for 14 days by default, that is, only the monitoring data within the last 14 days can be queried. Note The monitoring service is supported by Prometheus. The update frequency and retention intervals can be modified. For details, see Prometheus .","title":"Precautions"},{"location":"nebula-dashboard/1.what-is-dashboard/#version_compatibility","text":"The version correspondence between Nebula Graph and Dashboard Community Edition is as follows. Nebula Graph version Dashboard version 2.5.x~3.1.x 1.1.0 2.0.1~2.5.1 1.0.2 2.0.1~2.5.1 1.0.1","title":"Version compatibility"},{"location":"nebula-dashboard/1.what-is-dashboard/#release_note","text":"Release","title":"Release note"},{"location":"nebula-dashboard/2.deploy-dashboard/","text":"Deploy Dashboard \u00b6 The deployment of Dashboard involves five services. This topic will describe how to deploy Dashboard in detail. To download and compile the latest source code of Nebula Dashboard, follow the instructions on the nebula dashboard GitHub page . Prerequisites \u00b6 Before you deploy Dashboard, you must confirm that: The Nebula Graph services are deployed and started. For more information, see Nebula Graph Database Manual . Before the installation starts, the following ports are not occupied. 9200 9100 9090 8090 7003 The Linux distribution is CentOS, installed with Node.js of version above v10.12.0 and Go of version above 1.13. Download Dashboard \u00b6 Download the tar package as needed, and it is recommended to select the latest version. Dashboard package Nebula Graph version nebula-dashboard-3.0.0.x86_64.tar.gz 2.5.x~3.1.0 Service \u00b6 Run tar -xvf nebula-dashboard-3.0.0.x86_64.tar.gz to decompress the installation package. There are four services in the nebula-dashboard/vendors . The descriptions are as follows. |Name|Description||Port| |:---|:---|:---| |node-exporter | Collects the source information of machines in the cluster, including the CPU, memory, load, disk, and network. |9100| |nebula-stats-exporter | Collects the performance metrics in the cluster, including the IP addresses, versions, and monitoring metrics (such as the number of queries, the latency of queries, the latency of heartbeats, and so on). |9200| |prometheus | The time series database that stores monitoring data. |9090| |nebula-http-gateway | Provides HTTP ports for cluster services to execute nGQL statements to interact with the Nebula Graph database. |8090| The above four services should be deployed as follows. Procedure \u00b6 Deploy node-exporter \u00b6 Note You need to deploy the node-exporter service on each machine in the cluster. To start the service, run the following statement in node-exporter : $ nohup ./node-exporter --web.listen-address = \":9100\" & After the service is started, you can enter <IP>:9100 in the browser to check whether the service is started normally. Deploy nebula-stats-exporter \u00b6 Note You only need to deploy the nebula-stats-exporter service on the machine where the nebula-dashboard service is installed. Modify the config.yaml file in nebula-stats-exporter to deploy the HTTP ports of all the services. The example is as follows: clusters: - name: nebula instances: - name: metad0 endpointIP: 192 .168.8.157 endpointPort: 19559 componentType: metad - name: metad1 endpointIP: 192 .168.8.155 endpointPort: 19559 componentType: metad - name: metad2 endpointIP: 192 .168.8.154 endpointPort: 19559 componentType: metad - name: graphd0 endpointIP: 192 .168.8.157 endpointPort: 19669 componentType: graphd - name: graphd1 endpointIP: 192 .168.8.155 endpointPort: 19669 componentType: graphd - name: graphd2 endpointIP: 192 .168.8.154 endpointPort: 19669 componentType: graphd - name: storaged0 endpointIP: 192 .168.8.157 endpointPort: 19779 componentType: storaged - name: storaged1 endpointIP: 192 .168.8.155 endpointPort: 19779 componentType: storaged - name: storaged2 endpointIP: 192 .168.8.154 endpointPort: 19779 componentType: storaged Run the following statement to start the service: $ nohup ./nebula-stats-exporter --listen-address = \":9200\" --bare-metal --bare-metal-config = ./config.yaml & After the service is started, you can enter <IP>:9200 in the browser to check whether the service is started normally. Deploy prometheus \u00b6 Note You only need to deploy the prometheus service on the machine where the nebula-dashboard service is installed. Modify the prometheus.yaml file in prometheus to deploy the IP addresses and ports of the node-exporter service and the nebula-stats-exporter . The example is as follows: global: scrape_interval: 5s evaluation_interval: 5s scrape_configs: - job_name: 'nebula-stats-exporter' static_configs: - targets: [ '192.168.xx.100:9200' , # IP address and port of nebula-stats-exporter. ] - job_name: 'node-exporter' static_configs: - targets: [ '192.168.xx.100:9100' , # IP address and port of node-exporter. '192.168.xx.101:9100' ] scrape_interval: The interval for collecting the monitoring data, which is 1 minute by default. evaluation_interval: The interval for running alert rules, which is 1 minute by default. Run the following statement to start the service. $ nohup ./prometheus --config.file = ./prometheus.yaml & After the service is started, you can enter <IP>:9090 in the browser to check whether the service is started normally. Deploy nebula-http-gateway \u00b6 Note You only need to deploy the nebula-http-gateway service on the machine where the nebula-dashboard service is installed. To start the service, run the following statement in nebula-http-gateway : $ nohup ./nebula-httpd & After the service is started, you can enter <IP>:8090 in the browser to check whether the service is started normally. How to deploy the nebula-dashboard service \u00b6 Modify the config.json file in nebula-dashboard/ to deploy the IP address and port of the Graph Service and Proxy. The example is as follows: port: 7003 proxy: gateway: target: \"127.0.0.1:8090\" // The IP address and port of the gateway service. prometheus: target: \"127.0.0.1:9090\" // The IP address and port of the prometheus service. nebulaServer: ip: \"192.168.8.143\" // The IP of the machine where your Nebula Graph is deployed. port: 9669 // The port of the Nebula Graph. ... To start the service, run the following statement in nebula-dashboard : $ nohup ./dashboard & After the service is started, you can enter <IP>:7003 in the browser to check whether the service is started normally. Stop Dashboard \u00b6 You can enter kill <pid> to stop Dashboard. The examples are as follows: $ kill $( lsof -t -i :9100 ) # stop the node-exporter service $ kill $( lsof -t -i :9200 ) # stop the nebula-stats-exporter service $ kill $( lsof -t -i :9090 ) # stop the prometheus service $ kill $( lsof -t -i :8090 ) # stop the nebula-http-gateway service $ kill $( lsof -t -i :7003 ) # stop dashboard service","title":"Deploy Dashboard"},{"location":"nebula-dashboard/2.deploy-dashboard/#deploy_dashboard","text":"The deployment of Dashboard involves five services. This topic will describe how to deploy Dashboard in detail. To download and compile the latest source code of Nebula Dashboard, follow the instructions on the nebula dashboard GitHub page .","title":"Deploy Dashboard"},{"location":"nebula-dashboard/2.deploy-dashboard/#prerequisites","text":"Before you deploy Dashboard, you must confirm that: The Nebula Graph services are deployed and started. For more information, see Nebula Graph Database Manual . Before the installation starts, the following ports are not occupied. 9200 9100 9090 8090 7003 The Linux distribution is CentOS, installed with Node.js of version above v10.12.0 and Go of version above 1.13.","title":"Prerequisites"},{"location":"nebula-dashboard/2.deploy-dashboard/#download_dashboard","text":"Download the tar package as needed, and it is recommended to select the latest version. Dashboard package Nebula Graph version nebula-dashboard-3.0.0.x86_64.tar.gz 2.5.x~3.1.0","title":"Download Dashboard"},{"location":"nebula-dashboard/2.deploy-dashboard/#service","text":"Run tar -xvf nebula-dashboard-3.0.0.x86_64.tar.gz to decompress the installation package. There are four services in the nebula-dashboard/vendors . The descriptions are as follows. |Name|Description||Port| |:---|:---|:---| |node-exporter | Collects the source information of machines in the cluster, including the CPU, memory, load, disk, and network. |9100| |nebula-stats-exporter | Collects the performance metrics in the cluster, including the IP addresses, versions, and monitoring metrics (such as the number of queries, the latency of queries, the latency of heartbeats, and so on). |9200| |prometheus | The time series database that stores monitoring data. |9090| |nebula-http-gateway | Provides HTTP ports for cluster services to execute nGQL statements to interact with the Nebula Graph database. |8090| The above four services should be deployed as follows.","title":"Service"},{"location":"nebula-dashboard/2.deploy-dashboard/#procedure","text":"","title":"Procedure"},{"location":"nebula-dashboard/2.deploy-dashboard/#deploy_node-exporter","text":"Note You need to deploy the node-exporter service on each machine in the cluster. To start the service, run the following statement in node-exporter : $ nohup ./node-exporter --web.listen-address = \":9100\" & After the service is started, you can enter <IP>:9100 in the browser to check whether the service is started normally.","title":"Deploy node-exporter"},{"location":"nebula-dashboard/2.deploy-dashboard/#deploy_nebula-stats-exporter","text":"Note You only need to deploy the nebula-stats-exporter service on the machine where the nebula-dashboard service is installed. Modify the config.yaml file in nebula-stats-exporter to deploy the HTTP ports of all the services. The example is as follows: clusters: - name: nebula instances: - name: metad0 endpointIP: 192 .168.8.157 endpointPort: 19559 componentType: metad - name: metad1 endpointIP: 192 .168.8.155 endpointPort: 19559 componentType: metad - name: metad2 endpointIP: 192 .168.8.154 endpointPort: 19559 componentType: metad - name: graphd0 endpointIP: 192 .168.8.157 endpointPort: 19669 componentType: graphd - name: graphd1 endpointIP: 192 .168.8.155 endpointPort: 19669 componentType: graphd - name: graphd2 endpointIP: 192 .168.8.154 endpointPort: 19669 componentType: graphd - name: storaged0 endpointIP: 192 .168.8.157 endpointPort: 19779 componentType: storaged - name: storaged1 endpointIP: 192 .168.8.155 endpointPort: 19779 componentType: storaged - name: storaged2 endpointIP: 192 .168.8.154 endpointPort: 19779 componentType: storaged Run the following statement to start the service: $ nohup ./nebula-stats-exporter --listen-address = \":9200\" --bare-metal --bare-metal-config = ./config.yaml & After the service is started, you can enter <IP>:9200 in the browser to check whether the service is started normally.","title":"Deploy nebula-stats-exporter"},{"location":"nebula-dashboard/2.deploy-dashboard/#deploy_prometheus","text":"Note You only need to deploy the prometheus service on the machine where the nebula-dashboard service is installed. Modify the prometheus.yaml file in prometheus to deploy the IP addresses and ports of the node-exporter service and the nebula-stats-exporter . The example is as follows: global: scrape_interval: 5s evaluation_interval: 5s scrape_configs: - job_name: 'nebula-stats-exporter' static_configs: - targets: [ '192.168.xx.100:9200' , # IP address and port of nebula-stats-exporter. ] - job_name: 'node-exporter' static_configs: - targets: [ '192.168.xx.100:9100' , # IP address and port of node-exporter. '192.168.xx.101:9100' ] scrape_interval: The interval for collecting the monitoring data, which is 1 minute by default. evaluation_interval: The interval for running alert rules, which is 1 minute by default. Run the following statement to start the service. $ nohup ./prometheus --config.file = ./prometheus.yaml & After the service is started, you can enter <IP>:9090 in the browser to check whether the service is started normally.","title":"Deploy prometheus"},{"location":"nebula-dashboard/2.deploy-dashboard/#deploy_nebula-http-gateway","text":"Note You only need to deploy the nebula-http-gateway service on the machine where the nebula-dashboard service is installed. To start the service, run the following statement in nebula-http-gateway : $ nohup ./nebula-httpd & After the service is started, you can enter <IP>:8090 in the browser to check whether the service is started normally.","title":"Deploy nebula-http-gateway"},{"location":"nebula-dashboard/2.deploy-dashboard/#how_to_deploy_the_nebula-dashboard_service","text":"Modify the config.json file in nebula-dashboard/ to deploy the IP address and port of the Graph Service and Proxy. The example is as follows: port: 7003 proxy: gateway: target: \"127.0.0.1:8090\" // The IP address and port of the gateway service. prometheus: target: \"127.0.0.1:9090\" // The IP address and port of the prometheus service. nebulaServer: ip: \"192.168.8.143\" // The IP of the machine where your Nebula Graph is deployed. port: 9669 // The port of the Nebula Graph. ... To start the service, run the following statement in nebula-dashboard : $ nohup ./dashboard & After the service is started, you can enter <IP>:7003 in the browser to check whether the service is started normally.","title":"How to deploy the nebula-dashboard service"},{"location":"nebula-dashboard/2.deploy-dashboard/#stop_dashboard","text":"You can enter kill <pid> to stop Dashboard. The examples are as follows: $ kill $( lsof -t -i :9100 ) # stop the node-exporter service $ kill $( lsof -t -i :9200 ) # stop the nebula-stats-exporter service $ kill $( lsof -t -i :9090 ) # stop the prometheus service $ kill $( lsof -t -i :8090 ) # stop the nebula-http-gateway service $ kill $( lsof -t -i :7003 ) # stop dashboard service","title":"Stop Dashboard"},{"location":"nebula-dashboard/3.connect-dashboard/","text":"Connect Dashboard \u00b6 After Dashboard is deployed, you can log in and use Dashboard on the browser. Prerequisites \u00b6 The Dashboard services are started. For more information, see Deploy Dashboard . We recommend you to use the Chrome browser of the version above 58. Otherwise, there may be compatibility issues. Procedures \u00b6 Confirm the IP address of the machine where the nebula-dashboard service is installed. Enter <IP>:7003 in the browser to open the login page. Enter the username and the passwords of the Nebula Graph database and click the login button. Note Ensure that you have configured the IP of the machines where your Nebula Graph is deployed in the config.json file. For more information, see Deploy Dashboard . If authentication is enabled, you can log in with the created accounts. If authentication is not enabled, you can only log in using root as the username and random characters as the password. To enable authentication, see Authentication .","title":"Connect to Dashboard"},{"location":"nebula-dashboard/3.connect-dashboard/#connect_dashboard","text":"After Dashboard is deployed, you can log in and use Dashboard on the browser.","title":"Connect Dashboard"},{"location":"nebula-dashboard/3.connect-dashboard/#prerequisites","text":"The Dashboard services are started. For more information, see Deploy Dashboard . We recommend you to use the Chrome browser of the version above 58. Otherwise, there may be compatibility issues.","title":"Prerequisites"},{"location":"nebula-dashboard/3.connect-dashboard/#procedures","text":"Confirm the IP address of the machine where the nebula-dashboard service is installed. Enter <IP>:7003 in the browser to open the login page. Enter the username and the passwords of the Nebula Graph database and click the login button. Note Ensure that you have configured the IP of the machines where your Nebula Graph is deployed in the config.json file. For more information, see Deploy Dashboard . If authentication is enabled, you can log in with the created accounts. If authentication is not enabled, you can only log in using root as the username and random characters as the password. To enable authentication, see Authentication .","title":"Procedures"},{"location":"nebula-dashboard/4.use-dashboard/","text":"Dashboard \u00b6 Nebula Dashboard consists of three parts: Machine, Service, and Management. This topic will describe them in detail. Overview \u00b6 Machine \u00b6 Machine consists of the following parts: Overview You can check the fluctuations of CPU, Memory, Load, Disk, Network In, and Network Out in the past 24 hours. For details of certain monitoring metrics, you can click the symbol in the upper right corner, or click the monitoring metrics on the left. CPU, Memory, Load, Disk, Network It shows the detailed monitoring data of the machine from the above dimensions. By default, you can check the monitoring data up to 14 days before. The alternative can be 1 hour, 6 hours, 12 hours, 1 day, 3 days, 7 days, or 14 days in the past. You can choose the machine and monitoring metrics that you want to check. For more information, see monitor parameter . The Graph service supports a set of graph space metrics. For details, see Graph space . You can set a base line as a reference. Service \u00b6 Service consists of the following parts: Overview You can check the fluctuations of monitoring metrics of various services in the past 24 hours. You can also switch to the Version page to view the IP addresses and versions of all services. For details of certain monitoring metrics, you can click the symbol in the upper right corner, or click the services on the left. Note The overview page of the current Community Edition only supports setting two monitoring metrics for each service. You can adjust it by clicking the Set up button. Graph, Meta, Storage It shows the detailed monitoring data of the above services. By default, you can check the monitoring data up to 14 days before. The alternative can be 1 hour, 6 hours, 12 hours, 1 day, 3 days, 7 days, or 14 days in the past. You can choose the machine that you want to check the monitoring data, monitoring metrics, metric methods, and period. For more information, see monitor parameter . You can set a base line as a reference. You can check the status of the current service. Management \u00b6 Note Non-root users can view the service information and the partition information with spatial permissions, but cannot view the configuration and long-term tasks. Management consists of the following parts: Service Info It shows the basic information of the Storage Service, including the information of the host, the commit ID of versions, the number of leaders, the distribution of partitions, and the distribution of leaders. Partition Info You can check the information of partitions in different graph spaces. The descriptions are as follows. Parameter Description Partition ID The ID of the partition. Leader The IP address and the port of the leader. Peers The IP addresses and the ports of all the replicas. Losts The IP addresses and the ports of replicas at fault. Config It shows the configuration of each service. Dashboard does not support online modification of configurations for now. For details, see configurations . Long-term Task It shows the information of all jobs. Dashboard does not support online management of jobs for now. For details, see job statements . Others \u00b6 In the lower left corner of the page, you can: Sign out Switch between Chinese and English View the current Dashboard release View the user manual and forum Fold the sidebar","title":"Use Dashboard"},{"location":"nebula-dashboard/4.use-dashboard/#dashboard","text":"Nebula Dashboard consists of three parts: Machine, Service, and Management. This topic will describe them in detail.","title":"Dashboard"},{"location":"nebula-dashboard/4.use-dashboard/#overview","text":"","title":"Overview"},{"location":"nebula-dashboard/4.use-dashboard/#machine","text":"Machine consists of the following parts: Overview You can check the fluctuations of CPU, Memory, Load, Disk, Network In, and Network Out in the past 24 hours. For details of certain monitoring metrics, you can click the symbol in the upper right corner, or click the monitoring metrics on the left. CPU, Memory, Load, Disk, Network It shows the detailed monitoring data of the machine from the above dimensions. By default, you can check the monitoring data up to 14 days before. The alternative can be 1 hour, 6 hours, 12 hours, 1 day, 3 days, 7 days, or 14 days in the past. You can choose the machine and monitoring metrics that you want to check. For more information, see monitor parameter . The Graph service supports a set of graph space metrics. For details, see Graph space . You can set a base line as a reference.","title":"Machine"},{"location":"nebula-dashboard/4.use-dashboard/#service","text":"Service consists of the following parts: Overview You can check the fluctuations of monitoring metrics of various services in the past 24 hours. You can also switch to the Version page to view the IP addresses and versions of all services. For details of certain monitoring metrics, you can click the symbol in the upper right corner, or click the services on the left. Note The overview page of the current Community Edition only supports setting two monitoring metrics for each service. You can adjust it by clicking the Set up button. Graph, Meta, Storage It shows the detailed monitoring data of the above services. By default, you can check the monitoring data up to 14 days before. The alternative can be 1 hour, 6 hours, 12 hours, 1 day, 3 days, 7 days, or 14 days in the past. You can choose the machine that you want to check the monitoring data, monitoring metrics, metric methods, and period. For more information, see monitor parameter . You can set a base line as a reference. You can check the status of the current service.","title":"Service"},{"location":"nebula-dashboard/4.use-dashboard/#management","text":"Note Non-root users can view the service information and the partition information with spatial permissions, but cannot view the configuration and long-term tasks. Management consists of the following parts: Service Info It shows the basic information of the Storage Service, including the information of the host, the commit ID of versions, the number of leaders, the distribution of partitions, and the distribution of leaders. Partition Info You can check the information of partitions in different graph spaces. The descriptions are as follows. Parameter Description Partition ID The ID of the partition. Leader The IP address and the port of the leader. Peers The IP addresses and the ports of all the replicas. Losts The IP addresses and the ports of replicas at fault. Config It shows the configuration of each service. Dashboard does not support online modification of configurations for now. For details, see configurations . Long-term Task It shows the information of all jobs. Dashboard does not support online management of jobs for now. For details, see job statements .","title":"Management"},{"location":"nebula-dashboard/4.use-dashboard/#others","text":"In the lower left corner of the page, you can: Sign out Switch between Chinese and English View the current Dashboard release View the user manual and forum Fold the sidebar","title":"Others"},{"location":"nebula-dashboard/6.monitor-parameter/","text":"Metrics \u00b6 This topic will describe the monitoring metrics in Nebula Dashboard. Machine \u00b6 Note All the machine metrics listed below are for the Linux operating system. The default unit in Disk and Network is byte. The unit will change with the data magnitude as the page displays. For example, when the flow is less than 1 KB/s, the unit will be Bytes/s. For versions of Dashboard Community Edition greater than v1.0.2, the memory occupied by Buff and Cache will not be counted in the memory usage. CPU \u00b6 Parameter Description cpu_utilization The percentage of used CPU. cpu_idle The percentage of idled CPU. cpu_wait The percentage of CPU waiting for IO operations. cpu_user The percentage of CPU used by users. cpu_system The percentage of CPU used by the system. Memory \u00b6 Parameter Description memory_utilization The percentage of used memory. memory_used The memory space used (including caches). memory_actual_used The memory space used (not including caches). memory_free The memory space available. Load \u00b6 Parameter Description load_1m The average load of the system in the last 1 minute. load_5m The average load of the system in the last 5 minutes. load_15m The average load of the system in the last 15 minutes. Disk \u00b6 Parameter Description disk_used The disk space used. disk_free The disk space available. disk_readbytes The number of bytes that the system reads in the disk per second. disk_writebytes The number of bytes that the system writes in the disk per second. disk_readiops The number of read queries that the disk receives per second. disk_writeiops The number of write queries that the disk receives per second. inode_utilization The percentage of used inode. Network \u00b6 Parameter Description network_in_rate The number of bytes that the network card receives per second. network_out_rate The number of bytes that the network card sends out per second. network_in_errs The number of wrong bytes that the network card receives per second. network_out_errs The number of wrong bytes that the network card sends out per second. network_in_packets The number of data packages that the network card receives per second. network_out_packets The number of data packages that the network card sends out per second. Service \u00b6 Period \u00b6 The period is the time range of counting metrics. It currently supports 5 seconds, 60 seconds, 600 seconds, and 3600 seconds, which respectively represent the last 5 seconds, the last 1 minute, the last 10 minutes, and the last 1 hour. Metric methods \u00b6 Parameter Description rate The average rate of operations per second in a period. sum The sum of operations in the period. avg The average latency in the cycle. P75 The 75th percentile latency. P95 The 95th percentile latency. P99 The 99th percentile latency. P999 The 99.9th percentile latency. Graph \u00b6 Parameter Description num_active_queries The number of queries currently being executed. num_active_sessions The number of currently active sessions. num_aggregate_executors The number of executions for the Aggregation operator. num_auth_failed_sessions_bad_username_password The number of sessions where authentication failed due to incorrect username and password. num_auth_failed_sessions The number of sessions in which login authentication failed. num_indexscan_executors The number of executions for index scan operators. num_killed_queries The number of killed queries. num_opened_sessions The number of sessions connected to the server. num_queries The number of queries. num_query_errors_leader_changes The number of the raft leader changes due to query errors. num_query_errors The number of query errors. num_reclaimed_expired_sessions The number of expired sessions actively reclaimed by the server. num_rpc_sent_to_metad_failed The number of failed RPC requests that the Graphd service sends to the Metad service. num_rpc_sent_to_metad The number of RPC requests that the Graphd service sent to the Metad service. num_rpc_sent_to_storaged_failed The number of failed RPC requests that the Graphd service sent to the Storaged service. num_rpc_sent_to_storaged The number of RPC requests that the Graphd service sent to the Storaged service. num_sentences The number of statements received by the Graphd service. num_slow_queries The number of slow queries. num_sort_executors The number of executions for the Sort operator. optimizer_latency_us The latency of executing optimizer statements. query_latency_us The average latency of queries. slow_query_latency_us The average latency of slow queries. Meta \u00b6 Parameter Description commit_log_latency_us The latency of committing logs in Raft. commit_snapshot_latency_us The latency of committing snapshots in Raft. heartbeat_latency_us The latency of heartbeats. num_heartbeats The number of heartbeats. num_raft_votes The number of votes in Raft. transfer_leader_latency_us The latency of transferring the raft leader. Storage \u00b6 Parameter Description add_edges_latency_us The average latency of adding edges. add_vertices_latency_us The average latency of adding vertices. commit_log_latency_us The latency of committing logs in Raft. commit_snapshot_latency_us The latency of committing snapshots in Raft. delete_edges_latency_us The average latency of deleting edges. delete_vertices_latency_us The average latency of deleting vertices. get_neighbors_latency_us The average latency of querying neighbor vertices. num_edges_deleted The number of deleted edges. num_edges_inserted The number of inserted edges. num_raft_votes The number of votes in Raft. num_rpc_sent_to_metad_failed The number of failed RPC requests that the Storage service sent to the Meta service. num_rpc_sent_to_metad The number of RPC requests that the Storaged service sent to the Metad service. num_tags_deleted The number of deleted tags. num_vertices_deleted The number of deleted vertices. num_vertices_inserted The number of inserted vertices. transfer_leader_latency_us The latency of transferring the raft leader. Graph space \u00b6 Parameter Description num_active_queries The number of queries currently being executed. num_aggregate_executors The number of executions for the Aggregation operator. num_indexscan_executors The number of executions for index scan operators. num_killed_queries The number of killed queries. num_queries The number of queries. num_query_errors_leader_changes The number of raft leader changes due to query errors. num_query_errors The number of query errors. num_sentences The number of statements received by the Graphd service. num_slow_queries The number of slow queries. num_sort_executors The number of executions for the Sort operator. optimizer_latency_us The latency of executing optimizer statements. query_latency_us The average latency of queries.","title":"Monitoring metrics"},{"location":"nebula-dashboard/6.monitor-parameter/#metrics","text":"This topic will describe the monitoring metrics in Nebula Dashboard.","title":"Metrics"},{"location":"nebula-dashboard/6.monitor-parameter/#machine","text":"Note All the machine metrics listed below are for the Linux operating system. The default unit in Disk and Network is byte. The unit will change with the data magnitude as the page displays. For example, when the flow is less than 1 KB/s, the unit will be Bytes/s. For versions of Dashboard Community Edition greater than v1.0.2, the memory occupied by Buff and Cache will not be counted in the memory usage.","title":"Machine"},{"location":"nebula-dashboard/6.monitor-parameter/#cpu","text":"Parameter Description cpu_utilization The percentage of used CPU. cpu_idle The percentage of idled CPU. cpu_wait The percentage of CPU waiting for IO operations. cpu_user The percentage of CPU used by users. cpu_system The percentage of CPU used by the system.","title":"CPU"},{"location":"nebula-dashboard/6.monitor-parameter/#memory","text":"Parameter Description memory_utilization The percentage of used memory. memory_used The memory space used (including caches). memory_actual_used The memory space used (not including caches). memory_free The memory space available.","title":"Memory"},{"location":"nebula-dashboard/6.monitor-parameter/#load","text":"Parameter Description load_1m The average load of the system in the last 1 minute. load_5m The average load of the system in the last 5 minutes. load_15m The average load of the system in the last 15 minutes.","title":"Load"},{"location":"nebula-dashboard/6.monitor-parameter/#disk","text":"Parameter Description disk_used The disk space used. disk_free The disk space available. disk_readbytes The number of bytes that the system reads in the disk per second. disk_writebytes The number of bytes that the system writes in the disk per second. disk_readiops The number of read queries that the disk receives per second. disk_writeiops The number of write queries that the disk receives per second. inode_utilization The percentage of used inode.","title":"Disk"},{"location":"nebula-dashboard/6.monitor-parameter/#network","text":"Parameter Description network_in_rate The number of bytes that the network card receives per second. network_out_rate The number of bytes that the network card sends out per second. network_in_errs The number of wrong bytes that the network card receives per second. network_out_errs The number of wrong bytes that the network card sends out per second. network_in_packets The number of data packages that the network card receives per second. network_out_packets The number of data packages that the network card sends out per second.","title":"Network"},{"location":"nebula-dashboard/6.monitor-parameter/#service","text":"","title":"Service"},{"location":"nebula-dashboard/6.monitor-parameter/#period","text":"The period is the time range of counting metrics. It currently supports 5 seconds, 60 seconds, 600 seconds, and 3600 seconds, which respectively represent the last 5 seconds, the last 1 minute, the last 10 minutes, and the last 1 hour.","title":"Period"},{"location":"nebula-dashboard/6.monitor-parameter/#metric_methods","text":"Parameter Description rate The average rate of operations per second in a period. sum The sum of operations in the period. avg The average latency in the cycle. P75 The 75th percentile latency. P95 The 95th percentile latency. P99 The 99th percentile latency. P999 The 99.9th percentile latency.","title":"Metric methods"},{"location":"nebula-dashboard/6.monitor-parameter/#graph","text":"Parameter Description num_active_queries The number of queries currently being executed. num_active_sessions The number of currently active sessions. num_aggregate_executors The number of executions for the Aggregation operator. num_auth_failed_sessions_bad_username_password The number of sessions where authentication failed due to incorrect username and password. num_auth_failed_sessions The number of sessions in which login authentication failed. num_indexscan_executors The number of executions for index scan operators. num_killed_queries The number of killed queries. num_opened_sessions The number of sessions connected to the server. num_queries The number of queries. num_query_errors_leader_changes The number of the raft leader changes due to query errors. num_query_errors The number of query errors. num_reclaimed_expired_sessions The number of expired sessions actively reclaimed by the server. num_rpc_sent_to_metad_failed The number of failed RPC requests that the Graphd service sends to the Metad service. num_rpc_sent_to_metad The number of RPC requests that the Graphd service sent to the Metad service. num_rpc_sent_to_storaged_failed The number of failed RPC requests that the Graphd service sent to the Storaged service. num_rpc_sent_to_storaged The number of RPC requests that the Graphd service sent to the Storaged service. num_sentences The number of statements received by the Graphd service. num_slow_queries The number of slow queries. num_sort_executors The number of executions for the Sort operator. optimizer_latency_us The latency of executing optimizer statements. query_latency_us The average latency of queries. slow_query_latency_us The average latency of slow queries.","title":"Graph"},{"location":"nebula-dashboard/6.monitor-parameter/#meta","text":"Parameter Description commit_log_latency_us The latency of committing logs in Raft. commit_snapshot_latency_us The latency of committing snapshots in Raft. heartbeat_latency_us The latency of heartbeats. num_heartbeats The number of heartbeats. num_raft_votes The number of votes in Raft. transfer_leader_latency_us The latency of transferring the raft leader.","title":"Meta"},{"location":"nebula-dashboard/6.monitor-parameter/#storage","text":"Parameter Description add_edges_latency_us The average latency of adding edges. add_vertices_latency_us The average latency of adding vertices. commit_log_latency_us The latency of committing logs in Raft. commit_snapshot_latency_us The latency of committing snapshots in Raft. delete_edges_latency_us The average latency of deleting edges. delete_vertices_latency_us The average latency of deleting vertices. get_neighbors_latency_us The average latency of querying neighbor vertices. num_edges_deleted The number of deleted edges. num_edges_inserted The number of inserted edges. num_raft_votes The number of votes in Raft. num_rpc_sent_to_metad_failed The number of failed RPC requests that the Storage service sent to the Meta service. num_rpc_sent_to_metad The number of RPC requests that the Storaged service sent to the Metad service. num_tags_deleted The number of deleted tags. num_vertices_deleted The number of deleted vertices. num_vertices_inserted The number of inserted vertices. transfer_leader_latency_us The latency of transferring the raft leader.","title":"Storage"},{"location":"nebula-dashboard/6.monitor-parameter/#graph_space","text":"Parameter Description num_active_queries The number of queries currently being executed. num_aggregate_executors The number of executions for the Aggregation operator. num_indexscan_executors The number of executions for index scan operators. num_killed_queries The number of killed queries. num_queries The number of queries. num_query_errors_leader_changes The number of raft leader changes due to query errors. num_query_errors The number of query errors. num_sentences The number of statements received by the Graphd service. num_slow_queries The number of slow queries. num_sort_executors The number of executions for the Sort operator. optimizer_latency_us The latency of executing optimizer statements. query_latency_us The average latency of queries.","title":"Graph space"},{"location":"nebula-dashboard-ent/1.what-is-dashboard-ent/","text":"What is Nebula Dashboard Enterprise Edition \u00b6 Nebula Dashboard Enterprise Edition (Dashboard for short) is a visualization tool that monitors and manages the status of machines and services in Nebula Graph clusters. This topic introduces Dashboard Enterprise Edition. For more information, see What is Nebula Dashboard Community Edition . Features \u00b6 Create a Nebula Graph cluster of a specified version, import nodes in batches, scale out Nebula Graph services with one click Import clusters, balance data, scale out or in on the visualization interface. Manage clusters, and view the operation log of clusters within the last 14 days. Start, stop, and restart services on the visualization interface. Update the configuration of Storage services and Graph services in clusters quickly. Monitor the information of all the services in clusters, including the IP address, version, and monitoring metrics (such as the number of queries, the latency of queries, and the latency of heartbeats). Monitor the status of all the machines in clusters, including CPU, memory, load, disk, and network. Monitor the information of clusters, including the information of services, partitions, configurations, and long-term tasks. Scenarios \u00b6 You want a visualized operation and maintenance monitoring platform for large-scale clusters. You want to monitor key metrics conveniently and quickly, and present multiple key information of the business to ensure that the business can be operated normally. You want to monitor clusters from multiple dimensions (such as the time, aggregate rules, and metrics). You want to review the failure after it occurs, confirm when it happened, and view its associated phenomena. Precautions \u00b6 The monitoring data will be updated per 7 seconds by default. The monitoring data will be retained for 14 days by default, that is, only the monitoring data within the last 14 days can be queried. The version of Nebula Graph must be 2.5.0 or later. It is recommend to use the latest version of Chrome to access Dashboard. It is recommend to use the official installation package to create or import clusters. Note The monitoring feature is supported by Prometheus. The update frequency and retention intervals can be modified. For details, see Prometheus . Version compatibility \u00b6 The version correspondence between Nebula Graph and Dashboard Enterprise Edition is as follows. Nebula Graph version Dashboard version 2.5.x ~ 3.1.x 3.0.0 2.5.1 ~ 3.0.0 1.1.0 2.0.1 ~ 2.6.1 1.0.2 2.0.1 ~ 2.6.1 1.0.1 2.0.1 ~ 2.6.1 1.0.0","title":"What is Nebula Dashboard"},{"location":"nebula-dashboard-ent/1.what-is-dashboard-ent/#what_is_nebula_dashboard_enterprise_edition","text":"Nebula Dashboard Enterprise Edition (Dashboard for short) is a visualization tool that monitors and manages the status of machines and services in Nebula Graph clusters. This topic introduces Dashboard Enterprise Edition. For more information, see What is Nebula Dashboard Community Edition .","title":"What is Nebula Dashboard Enterprise Edition"},{"location":"nebula-dashboard-ent/1.what-is-dashboard-ent/#features","text":"Create a Nebula Graph cluster of a specified version, import nodes in batches, scale out Nebula Graph services with one click Import clusters, balance data, scale out or in on the visualization interface. Manage clusters, and view the operation log of clusters within the last 14 days. Start, stop, and restart services on the visualization interface. Update the configuration of Storage services and Graph services in clusters quickly. Monitor the information of all the services in clusters, including the IP address, version, and monitoring metrics (such as the number of queries, the latency of queries, and the latency of heartbeats). Monitor the status of all the machines in clusters, including CPU, memory, load, disk, and network. Monitor the information of clusters, including the information of services, partitions, configurations, and long-term tasks.","title":"Features"},{"location":"nebula-dashboard-ent/1.what-is-dashboard-ent/#scenarios","text":"You want a visualized operation and maintenance monitoring platform for large-scale clusters. You want to monitor key metrics conveniently and quickly, and present multiple key information of the business to ensure that the business can be operated normally. You want to monitor clusters from multiple dimensions (such as the time, aggregate rules, and metrics). You want to review the failure after it occurs, confirm when it happened, and view its associated phenomena.","title":"Scenarios"},{"location":"nebula-dashboard-ent/1.what-is-dashboard-ent/#precautions","text":"The monitoring data will be updated per 7 seconds by default. The monitoring data will be retained for 14 days by default, that is, only the monitoring data within the last 14 days can be queried. The version of Nebula Graph must be 2.5.0 or later. It is recommend to use the latest version of Chrome to access Dashboard. It is recommend to use the official installation package to create or import clusters. Note The monitoring feature is supported by Prometheus. The update frequency and retention intervals can be modified. For details, see Prometheus .","title":"Precautions"},{"location":"nebula-dashboard-ent/1.what-is-dashboard-ent/#version_compatibility","text":"The version correspondence between Nebula Graph and Dashboard Enterprise Edition is as follows. Nebula Graph version Dashboard version 2.5.x ~ 3.1.x 3.0.0 2.5.1 ~ 3.0.0 1.1.0 2.0.1 ~ 2.6.1 1.0.2 2.0.1 ~ 2.6.1 1.0.1 2.0.1 ~ 2.6.1 1.0.0","title":"Version compatibility"},{"location":"nebula-dashboard-ent/10.tasks/","text":"Task Center \u00b6 It takes a certain amount of time for you to make sure whether a cluster is created or scaled successfully in Nebula Dashboard Enterprise Edition. You can view the progress of such operations in Task Center. It displays the progress of ongoing operations and the history of complete operations. The operations on clusters are defined as tasks in Task Center. Currently, there are two task types in Task Center, cluster creation and cluster scaling. At the top navigation bar of the Dashboard Enterprise Edition page, click Task Center to view task information. Running tasks \u00b6 On the Task Center page, click Running Task to view the progress of tasks that clusters are being created or scaled. Click a task name to view the ID, node name, type, create time, and operator of the running task. Clink Task information to view task details. Task history \u00b6 On the Task Center page, click Task History to view all complete tasks. You can filter historical tasks by status, type, date, and time. On the right side of the target historical task, click Task information to view task details, and click Logs to view task execution logs. Delete tasks \u00b6 It is currently not possible to cancel running tasks or delete historical tasks.","title":"Task Center"},{"location":"nebula-dashboard-ent/10.tasks/#task_center","text":"It takes a certain amount of time for you to make sure whether a cluster is created or scaled successfully in Nebula Dashboard Enterprise Edition. You can view the progress of such operations in Task Center. It displays the progress of ongoing operations and the history of complete operations. The operations on clusters are defined as tasks in Task Center. Currently, there are two task types in Task Center, cluster creation and cluster scaling. At the top navigation bar of the Dashboard Enterprise Edition page, click Task Center to view task information.","title":"Task Center"},{"location":"nebula-dashboard-ent/10.tasks/#running_tasks","text":"On the Task Center page, click Running Task to view the progress of tasks that clusters are being created or scaled. Click a task name to view the ID, node name, type, create time, and operator of the running task. Clink Task information to view task details.","title":"Running tasks"},{"location":"nebula-dashboard-ent/10.tasks/#task_history","text":"On the Task Center page, click Task History to view all complete tasks. You can filter historical tasks by status, type, date, and time. On the right side of the target historical task, click Task information to view task details, and click Logs to view task execution logs.","title":"Task history"},{"location":"nebula-dashboard-ent/10.tasks/#delete_tasks","text":"It is currently not possible to cancel running tasks or delete historical tasks.","title":"Delete tasks"},{"location":"nebula-dashboard-ent/2.deploy-connect-dashboard-ent/","text":"Deploy Dashboard Enterprise Edition \u00b6 This topic will introduce how to deploy Dashboard Enterprise Edition in detail. Prerequisites \u00b6 Before deploying Dashboard Enterprise Edition, you must do a check of these: Select and download Dashboard Enterprise Edition of the correct version. For information about the version correspondence between Dashboard Enterprise Edition and Nebula Graph, see Version compatibility . The environment of MySQL is ready and a database named as dashboard is created. Before the installation starts, the following ports are not occupied. Port Description 7005 The port through which Dashboard Enterprise Edition provides the web service. 9090 The port of the prometheus service. 9200 The port of the nebula-stats-exporter service. 9093 The port of the Alertmanager service, used to receive Prometheus alerts and then send them to Dashboard. The license is ready. Enterpriseonly The license is only available in the Enterprise Edition. To obtain the license, apply for Nebula Dashboard Free Trial . Deploy Dashboard Enterprise Edition with TAR \u00b6 Installation \u00b6 Select and download the TAR package according to your needs. It is recommended to select the latest version. Enterpriseonly You can apply online for Dashboard Enterprise Edition free trial. To purchase, contact our sales team via email (inquiry@vesoft.com). For features of Dashboard Enterprise Edition, see Pricing . Use tar -xzvf to decompress the TAR package. tar -xzvf nebula-dashboard-ent-<version>.linux-amd64.tar.gz For example: tar -xzvf nebula-dashboard-ent-3.0.0.linux-amd64.tar.gz Edit vim config/config.yaml to modify the configuration. # Information about the database database: dialect: mysql # The type of database used, which currently only supports MySQL. host: 192 .168.8.157 # The IP address of the connected MySQL database. port: 3306 # The port of the connected MySQL database. username: root # The username to log in MySQL. password: nebula # The password to log in MySQL. name: dashboard # The name of the corresponding database. autoMigrate: true # Auto database tables creation, the default value of which is true. # Information about the exporter port exporter: nodePort: 9100 # The port of the node-exporter service. nebulaPort: 9200 # The port of the nebula-stats-exporter service. # Information of services proxy: prometheus: target: \"127.0.0.1:9090\" # The IP address and port of the prometheus service. alertmanager: target: \"127.0.0.1:9093\" # The IP address and port of the Alertmanager service. # Information of the sender's Email used to invite LDAP accounts. mail: host: smtp.office365.com # The SMTP server address. port: 587 # The port number of the SMTP server. username: \"\" # The SMTP server account name. password: \"\" # The SMTP server password. # System information system: webAddress: http://127.0.0.1:7005 # The address to access Dashboard for the invitee who is invited by mail. messageStore: 90 # It sets the number of days to keep alert messages, the value of which is 90 by default. # LDAP information ldap: server: ldap://127.0.0.1 # The LDAP server address. bindDN: cn = admin,dc = vesoft,dc = com # The LDAP login username. bindPassword: \"\" # The LDAP login password. baseDN: dc = vesoft,dc = com # Set the path to query user data. userFilter: \"&(objectClass=*)\" # Set a filter to LDAP search queries. emailKey: mail # Set the field name used to restore email in LDAP. Copy the license file to the nebula-dashboard-ent directory. cp -r <license> <dashboard_path> For example: cp -r nebula.license /usr/local/nebula-dashboard-ent Start Dashboard. You can use the following command to start the Dashboard with one click. cd scripts sudo ./dashboard.service start all Or execute the following commands to start prometheus, webserver, exporter and gateway services to start Dashboard. cd scripts sudo ./dashboard.service start prometheus # Start prometheus service sudo ./dashboard.service start webserver # Start webserver service sudo ./dashboard.service start exporter # Start exporter service sudo ./dashboard.service start gateway # Start gateway service Note If you change the configuration file after starting Dashboard, you can run dashboard.service restart all in the scripts directory to synchronize the changes to the Dashboard client page. Manage Dashboard Service \u00b6 You can use the dashboard.service script to start, stop, and check the Dashboard services. sudo <dashboard_path>/dashboard/scripts/dashboard.service [ -v ] [ -h ] <start | stop | status> <prometheus | webserver | exporter | gateway | all> Parameter Description dashboard_path Dashboard installation path. -v Display detailed debugging information. -h Display help information. start Start the target services. stop Stop the target services. status Check the status of the target services. prometheus Set the prometheus Service as the target service. webserver Set the webserver Service as the target service. exporter Set the exporter Service as the target service. gateway Set the gateway Service as the target service. all Set all the Dashboard services as the target services. Examples \u00b6 Dashboard is installed in the current directory, and you can use the following commands to manage services. sudo /dashboard/scripts/dashboard.service start all #Start Dashboard. sudo /dashboard/scripts/dashboard.service stop all #Stop Dashboard. sudo /dashboard/scripts/dashboard.service status all #Check Dashboard status. sudo /dashboard/scripts/dashboard.service restart all #Restart Dashboard. Deploy Dashboard Enterprise Edition with RPM \u00b6 Installation \u00b6 Download an RPM package. Enterpriseonly You can apply online for Dashboard Enterprise Edition free trial. To purchase, contact our sales team via email (inquiry@vesoft.com). For features of Dashboard Enterprise Edition, see Pricing . Run sudo rpm -i <rpm> to install the RPM package. For example, run the following command to install Dashboard Enterprise Edition. Installation path is /usr/local/nebula-dashboard-ent by default. sudo rpm -i nebula-dashboard-ent-<version>.x86_64.rpm You can also run the following command to specify the installation path. sudo rpm -i nebula-dashboard-ent-xxx.rpm --prefix = <path> During the installation process, you need to enter the path to the license and MySQL-related information, including the IP and port to connect to MySQL, the account and password to log into MySQL, and the MySQL database name. For example: Nebula Dashboard Enterprise version need license, please enter the license file path(~/nebula.license): /home/vesoft/nebula.license # The path to the license. Do you want to start the service now?[Y/N]: y Step1: set mysql database config Enter mysql host(127.0.0.1): 192.168.8.157 # The IP address of the connected MySQL database. Enter mysql service port(3306): # The port of the connected MySQL database. The default port is 3306. Enter mysql username(root): # The username to log in MySQL. Enter mysql password(nebula): # The password to log in MySQL. Enter mysql database name(dashboard): # The name of the corresponding database. (Optional) Run the following commands to view the status of and start all the services. sudo systemctl list-dependencies nebula-dashboard.target # View the status of all the services. sudo systemctl start nebula-dashboard.target # Start all the services. You can also view, start, and stop a single service. For example: sudo systemctl {status|stop|start} {nbd-prometheus.service|nbd-alert-manager.service|nbd-stats-exporter.service|nbd-webserver.service} (Optional) To configure recipients of cluster alert notifications and to configure LDAP accounts, run vim /usr/local/nebula-dashboard-ent/config/config.yaml and add the following settings. # Information of the sender's Email used to invite LDAP accounts. mail: host: smtp.office365.com # The SMTP server address. port: 587 # The port number of the SMTP server. username: \"\" # The SMTP server account name. password: \"\" # The SMTP server password. # System information system: webAddress: http://127.0.0.1:7005 # The address to access Dashboard for the invitee who is invited by mail. messageStore: 90 # It sets the number of days to keep alert messages, the value of which is 90 by default. # LDAP information ldap: server: ldap://127.0.0.1 # The LDAP server address. bindDN: cn=admin,dc=vesoft,dc=com # The LDAP login username. bindPassword: \"\" # The LDAP login password. baseDN: dc=vesoft,dc=com # Set the path to query user data. userFilter: \"&(objectClass=*)\" # Set a filter to LDAP search queries. emailKey: mail # Set the field name used to restore email in LDAP. View logs \u00b6 You can view the Dashboard Enterprise Edition operation logs in the /var/log/messages path. For example: sudo cat /var/log/messages Run the following command to view the logs of each service in Dashboard: journalctl -u {nbd-prometheus.service|nbd-alert-manager.service|nbd-stats-exporter.service|nbd-webserver.service} -b For example, to view the logs of the Prometheus service, run the following command: journalctl -u nbd-prometheus.service -b Uninstallation \u00b6 To uninstall Dashboard Enterprise Edition deployed with RPM, run the following command. sudo rpm -e <package_name> Deploy Dashboard Enterprise Edition with DEB \u00b6 Installation \u00b6 Download a DEB package. Enterpriseonly You can apply online for Dashboard Enterprise Edition free trial. To purchase, contact our sales team via email (inquiry@vesoft.com). For features of Dashboard Enterprise Edition, see Pricing . Install the package. sudo dpkg -i <package_name> Note Custom installation paths are not supported when installing Dashboard Enterprise Edition with DEB. The default installation path is /usr/local/nebula-dashboard-ent/ . For example, to install the DEB package of the 3.0.0 version: sudo dpkg -i nebula-dashboard-ent-3.0.0.ubuntu1804.amd64.deb During the installation process, you need to enter the path to the license and MySQL-related information, including the IP and port to connect to MySQL, the account and password to log into MySQL, and the MySQL database name. For example: Nebula Dashboard Enterprise version need license, please enter the license file path(~/nebula.license): /home/vesoft/nebula.license # The path to the license. Do you want to start the service now?[Y/N]: y Step1: set mysql database config Enter mysql host(127.0.0.1): 192.168.8.157 # The IP address of the connected MySQL database. Enter mysql service port(3306): # The port of the connected MySQL database. The default port is 3306. Enter mysql username(root): # The username to log in MySQL. Enter mysql password(nebula): # The password to log in MySQL. Enter mysql database name(dashboard): # The name of the corresponding database. (Optional) Run the following commands to view the status of and start all the services. sudo systemctl list-dependencies nebula-dashboard.target # View the status of all the services. sudo systemctl start nebula-dashboard.target # Start all the services. You can also view, start, and stop a single service. For example: sudo systemctl {status|stop|start} {nbd-prometheus.service|nbd-alert-manager.service|nbd-stats-exporter.service|nbd-webserver.service} (Optional) To configure recipients of cluster alert notifications and to configure LDAP accounts, run vim /usr/local/nebula-dashboard-ent/config/config.yaml and add the following settings. # Information of the sender's Email used to invite LDAP accounts. mail: host: smtp.office365.com # The SMTP server address. port: 587 # The port number of the SMTP server. username: \"\" # The SMTP server account name. password: \"\" # The SMTP server password. # System information system: webAddress: http://127.0.0.1:7005 # The address to access Dashboard for the invitee who is invited by mail. messageStore: 90 # It sets the number of days to keep alert messages, the value of which is 90 by default. # LDAP information ldap: server: ldap://127.0.0.1 # The LDAP server address. bindDN: cn=admin,dc=vesoft,dc=com # The LDAP login username. bindPassword: \"\" # The LDAP login password. baseDN: dc=vesoft,dc=com # Set the path to query user data. userFilter: \"&(objectClass=*)\" # Set a filter to LDAP search queries. emailKey: mail # Set the field name used to restore email in LDAP. View logs \u00b6 You can view the Dashboard Enterprise Edition operation logs in the /var/log/syslog path. For example: sudo cat /var/log/syslog Run the following command to view the logs of each service in Dashboard: journalctl -u {nbd-prometheus.service|nbd-alert-manager.service|nbd-stats-exporter.service|nbd-webserver.service} -b For example, to view the logs of the Prometheus service, run the following command: journalctl -u nbd-prometheus.service -b Uninstallation \u00b6 To uninstall Dashboard Enterprise Edition, run the following command. sudo dpkg -r <package_name> Connect to Dashboard \u00b6 After Dashboard is successfully started, you can enter http://<ip_address>:7005 in the address bar of a browser. If the following login interface is shown in the browser, then you have successfully deployed and started Dashboard. Note When logging into the Nebula Dashboard Enterprise Edition for the first time, the content of END USER LICENSE AGREEMENT is displayed on the login page. Please read it and then click I Agree . You can log into Dashboard with the initialization account name nebula and password nebula , and then create LDAP and general accounts. You can log into Dashboard with the accounts that you have created then. For more information about the Dashboard account, see Authority Management .","title":"Deploy Dashboard"},{"location":"nebula-dashboard-ent/2.deploy-connect-dashboard-ent/#deploy_dashboard_enterprise_edition","text":"This topic will introduce how to deploy Dashboard Enterprise Edition in detail.","title":"Deploy Dashboard Enterprise Edition"},{"location":"nebula-dashboard-ent/2.deploy-connect-dashboard-ent/#prerequisites","text":"Before deploying Dashboard Enterprise Edition, you must do a check of these: Select and download Dashboard Enterprise Edition of the correct version. For information about the version correspondence between Dashboard Enterprise Edition and Nebula Graph, see Version compatibility . The environment of MySQL is ready and a database named as dashboard is created. Before the installation starts, the following ports are not occupied. Port Description 7005 The port through which Dashboard Enterprise Edition provides the web service. 9090 The port of the prometheus service. 9200 The port of the nebula-stats-exporter service. 9093 The port of the Alertmanager service, used to receive Prometheus alerts and then send them to Dashboard. The license is ready. Enterpriseonly The license is only available in the Enterprise Edition. To obtain the license, apply for Nebula Dashboard Free Trial .","title":"Prerequisites"},{"location":"nebula-dashboard-ent/2.deploy-connect-dashboard-ent/#deploy_dashboard_enterprise_edition_with_tar","text":"","title":"Deploy Dashboard Enterprise Edition with TAR"},{"location":"nebula-dashboard-ent/2.deploy-connect-dashboard-ent/#installation","text":"Select and download the TAR package according to your needs. It is recommended to select the latest version. Enterpriseonly You can apply online for Dashboard Enterprise Edition free trial. To purchase, contact our sales team via email (inquiry@vesoft.com). For features of Dashboard Enterprise Edition, see Pricing . Use tar -xzvf to decompress the TAR package. tar -xzvf nebula-dashboard-ent-<version>.linux-amd64.tar.gz For example: tar -xzvf nebula-dashboard-ent-3.0.0.linux-amd64.tar.gz Edit vim config/config.yaml to modify the configuration. # Information about the database database: dialect: mysql # The type of database used, which currently only supports MySQL. host: 192 .168.8.157 # The IP address of the connected MySQL database. port: 3306 # The port of the connected MySQL database. username: root # The username to log in MySQL. password: nebula # The password to log in MySQL. name: dashboard # The name of the corresponding database. autoMigrate: true # Auto database tables creation, the default value of which is true. # Information about the exporter port exporter: nodePort: 9100 # The port of the node-exporter service. nebulaPort: 9200 # The port of the nebula-stats-exporter service. # Information of services proxy: prometheus: target: \"127.0.0.1:9090\" # The IP address and port of the prometheus service. alertmanager: target: \"127.0.0.1:9093\" # The IP address and port of the Alertmanager service. # Information of the sender's Email used to invite LDAP accounts. mail: host: smtp.office365.com # The SMTP server address. port: 587 # The port number of the SMTP server. username: \"\" # The SMTP server account name. password: \"\" # The SMTP server password. # System information system: webAddress: http://127.0.0.1:7005 # The address to access Dashboard for the invitee who is invited by mail. messageStore: 90 # It sets the number of days to keep alert messages, the value of which is 90 by default. # LDAP information ldap: server: ldap://127.0.0.1 # The LDAP server address. bindDN: cn = admin,dc = vesoft,dc = com # The LDAP login username. bindPassword: \"\" # The LDAP login password. baseDN: dc = vesoft,dc = com # Set the path to query user data. userFilter: \"&(objectClass=*)\" # Set a filter to LDAP search queries. emailKey: mail # Set the field name used to restore email in LDAP. Copy the license file to the nebula-dashboard-ent directory. cp -r <license> <dashboard_path> For example: cp -r nebula.license /usr/local/nebula-dashboard-ent Start Dashboard. You can use the following command to start the Dashboard with one click. cd scripts sudo ./dashboard.service start all Or execute the following commands to start prometheus, webserver, exporter and gateway services to start Dashboard. cd scripts sudo ./dashboard.service start prometheus # Start prometheus service sudo ./dashboard.service start webserver # Start webserver service sudo ./dashboard.service start exporter # Start exporter service sudo ./dashboard.service start gateway # Start gateway service Note If you change the configuration file after starting Dashboard, you can run dashboard.service restart all in the scripts directory to synchronize the changes to the Dashboard client page.","title":"Installation"},{"location":"nebula-dashboard-ent/2.deploy-connect-dashboard-ent/#manage_dashboard_service","text":"You can use the dashboard.service script to start, stop, and check the Dashboard services. sudo <dashboard_path>/dashboard/scripts/dashboard.service [ -v ] [ -h ] <start | stop | status> <prometheus | webserver | exporter | gateway | all> Parameter Description dashboard_path Dashboard installation path. -v Display detailed debugging information. -h Display help information. start Start the target services. stop Stop the target services. status Check the status of the target services. prometheus Set the prometheus Service as the target service. webserver Set the webserver Service as the target service. exporter Set the exporter Service as the target service. gateway Set the gateway Service as the target service. all Set all the Dashboard services as the target services.","title":"Manage Dashboard Service"},{"location":"nebula-dashboard-ent/2.deploy-connect-dashboard-ent/#examples","text":"Dashboard is installed in the current directory, and you can use the following commands to manage services. sudo /dashboard/scripts/dashboard.service start all #Start Dashboard. sudo /dashboard/scripts/dashboard.service stop all #Stop Dashboard. sudo /dashboard/scripts/dashboard.service status all #Check Dashboard status. sudo /dashboard/scripts/dashboard.service restart all #Restart Dashboard.","title":"Examples"},{"location":"nebula-dashboard-ent/2.deploy-connect-dashboard-ent/#deploy_dashboard_enterprise_edition_with_rpm","text":"","title":"Deploy Dashboard Enterprise Edition with RPM"},{"location":"nebula-dashboard-ent/2.deploy-connect-dashboard-ent/#installation_1","text":"Download an RPM package. Enterpriseonly You can apply online for Dashboard Enterprise Edition free trial. To purchase, contact our sales team via email (inquiry@vesoft.com). For features of Dashboard Enterprise Edition, see Pricing . Run sudo rpm -i <rpm> to install the RPM package. For example, run the following command to install Dashboard Enterprise Edition. Installation path is /usr/local/nebula-dashboard-ent by default. sudo rpm -i nebula-dashboard-ent-<version>.x86_64.rpm You can also run the following command to specify the installation path. sudo rpm -i nebula-dashboard-ent-xxx.rpm --prefix = <path> During the installation process, you need to enter the path to the license and MySQL-related information, including the IP and port to connect to MySQL, the account and password to log into MySQL, and the MySQL database name. For example: Nebula Dashboard Enterprise version need license, please enter the license file path(~/nebula.license): /home/vesoft/nebula.license # The path to the license. Do you want to start the service now?[Y/N]: y Step1: set mysql database config Enter mysql host(127.0.0.1): 192.168.8.157 # The IP address of the connected MySQL database. Enter mysql service port(3306): # The port of the connected MySQL database. The default port is 3306. Enter mysql username(root): # The username to log in MySQL. Enter mysql password(nebula): # The password to log in MySQL. Enter mysql database name(dashboard): # The name of the corresponding database. (Optional) Run the following commands to view the status of and start all the services. sudo systemctl list-dependencies nebula-dashboard.target # View the status of all the services. sudo systemctl start nebula-dashboard.target # Start all the services. You can also view, start, and stop a single service. For example: sudo systemctl {status|stop|start} {nbd-prometheus.service|nbd-alert-manager.service|nbd-stats-exporter.service|nbd-webserver.service} (Optional) To configure recipients of cluster alert notifications and to configure LDAP accounts, run vim /usr/local/nebula-dashboard-ent/config/config.yaml and add the following settings. # Information of the sender's Email used to invite LDAP accounts. mail: host: smtp.office365.com # The SMTP server address. port: 587 # The port number of the SMTP server. username: \"\" # The SMTP server account name. password: \"\" # The SMTP server password. # System information system: webAddress: http://127.0.0.1:7005 # The address to access Dashboard for the invitee who is invited by mail. messageStore: 90 # It sets the number of days to keep alert messages, the value of which is 90 by default. # LDAP information ldap: server: ldap://127.0.0.1 # The LDAP server address. bindDN: cn=admin,dc=vesoft,dc=com # The LDAP login username. bindPassword: \"\" # The LDAP login password. baseDN: dc=vesoft,dc=com # Set the path to query user data. userFilter: \"&(objectClass=*)\" # Set a filter to LDAP search queries. emailKey: mail # Set the field name used to restore email in LDAP.","title":"Installation"},{"location":"nebula-dashboard-ent/2.deploy-connect-dashboard-ent/#view_logs","text":"You can view the Dashboard Enterprise Edition operation logs in the /var/log/messages path. For example: sudo cat /var/log/messages Run the following command to view the logs of each service in Dashboard: journalctl -u {nbd-prometheus.service|nbd-alert-manager.service|nbd-stats-exporter.service|nbd-webserver.service} -b For example, to view the logs of the Prometheus service, run the following command: journalctl -u nbd-prometheus.service -b","title":"View logs"},{"location":"nebula-dashboard-ent/2.deploy-connect-dashboard-ent/#uninstallation","text":"To uninstall Dashboard Enterprise Edition deployed with RPM, run the following command. sudo rpm -e <package_name>","title":"Uninstallation"},{"location":"nebula-dashboard-ent/2.deploy-connect-dashboard-ent/#deploy_dashboard_enterprise_edition_with_deb","text":"","title":"Deploy Dashboard Enterprise Edition with DEB"},{"location":"nebula-dashboard-ent/2.deploy-connect-dashboard-ent/#installation_2","text":"Download a DEB package. Enterpriseonly You can apply online for Dashboard Enterprise Edition free trial. To purchase, contact our sales team via email (inquiry@vesoft.com). For features of Dashboard Enterprise Edition, see Pricing . Install the package. sudo dpkg -i <package_name> Note Custom installation paths are not supported when installing Dashboard Enterprise Edition with DEB. The default installation path is /usr/local/nebula-dashboard-ent/ . For example, to install the DEB package of the 3.0.0 version: sudo dpkg -i nebula-dashboard-ent-3.0.0.ubuntu1804.amd64.deb During the installation process, you need to enter the path to the license and MySQL-related information, including the IP and port to connect to MySQL, the account and password to log into MySQL, and the MySQL database name. For example: Nebula Dashboard Enterprise version need license, please enter the license file path(~/nebula.license): /home/vesoft/nebula.license # The path to the license. Do you want to start the service now?[Y/N]: y Step1: set mysql database config Enter mysql host(127.0.0.1): 192.168.8.157 # The IP address of the connected MySQL database. Enter mysql service port(3306): # The port of the connected MySQL database. The default port is 3306. Enter mysql username(root): # The username to log in MySQL. Enter mysql password(nebula): # The password to log in MySQL. Enter mysql database name(dashboard): # The name of the corresponding database. (Optional) Run the following commands to view the status of and start all the services. sudo systemctl list-dependencies nebula-dashboard.target # View the status of all the services. sudo systemctl start nebula-dashboard.target # Start all the services. You can also view, start, and stop a single service. For example: sudo systemctl {status|stop|start} {nbd-prometheus.service|nbd-alert-manager.service|nbd-stats-exporter.service|nbd-webserver.service} (Optional) To configure recipients of cluster alert notifications and to configure LDAP accounts, run vim /usr/local/nebula-dashboard-ent/config/config.yaml and add the following settings. # Information of the sender's Email used to invite LDAP accounts. mail: host: smtp.office365.com # The SMTP server address. port: 587 # The port number of the SMTP server. username: \"\" # The SMTP server account name. password: \"\" # The SMTP server password. # System information system: webAddress: http://127.0.0.1:7005 # The address to access Dashboard for the invitee who is invited by mail. messageStore: 90 # It sets the number of days to keep alert messages, the value of which is 90 by default. # LDAP information ldap: server: ldap://127.0.0.1 # The LDAP server address. bindDN: cn=admin,dc=vesoft,dc=com # The LDAP login username. bindPassword: \"\" # The LDAP login password. baseDN: dc=vesoft,dc=com # Set the path to query user data. userFilter: \"&(objectClass=*)\" # Set a filter to LDAP search queries. emailKey: mail # Set the field name used to restore email in LDAP.","title":"Installation"},{"location":"nebula-dashboard-ent/2.deploy-connect-dashboard-ent/#view_logs_1","text":"You can view the Dashboard Enterprise Edition operation logs in the /var/log/syslog path. For example: sudo cat /var/log/syslog Run the following command to view the logs of each service in Dashboard: journalctl -u {nbd-prometheus.service|nbd-alert-manager.service|nbd-stats-exporter.service|nbd-webserver.service} -b For example, to view the logs of the Prometheus service, run the following command: journalctl -u nbd-prometheus.service -b","title":"View logs"},{"location":"nebula-dashboard-ent/2.deploy-connect-dashboard-ent/#uninstallation_1","text":"To uninstall Dashboard Enterprise Edition, run the following command. sudo dpkg -r <package_name>","title":"Uninstallation"},{"location":"nebula-dashboard-ent/2.deploy-connect-dashboard-ent/#connect_to_dashboard","text":"After Dashboard is successfully started, you can enter http://<ip_address>:7005 in the address bar of a browser. If the following login interface is shown in the browser, then you have successfully deployed and started Dashboard. Note When logging into the Nebula Dashboard Enterprise Edition for the first time, the content of END USER LICENSE AGREEMENT is displayed on the login page. Please read it and then click I Agree . You can log into Dashboard with the initialization account name nebula and password nebula , and then create LDAP and general accounts. You can log into Dashboard with the accounts that you have created then. For more information about the Dashboard account, see Authority Management .","title":"Connect to Dashboard"},{"location":"nebula-dashboard-ent/5.account-management/","text":"Authority management \u00b6 You can log into Nebula Dashboard Enterprise Edition with different types of accounts. Different accounts have different permissions. This article introduces account types, roles, and permissions. Account types \u00b6 Once you log into Dashboard Enterprise Edition using the initialized account name nebula and password nebula , you can create different types of accounts: LDAP accounts and general accounts. LDAP accounts \u00b6 Dashboard Enterprise Edition enables you to log into it with your enterprise account by accessing LDAP\uff08Lightweight Directory Access Protocol\uff09 . Before using an LDAP account, LDAP configurations are required. In the Dashboard Enterprise Edition installation path, such as nebula-graph-dashboard-ent/nebula-dashboard-ent/config , find the configuration file config.yaml . In config.yaml , add the following content. ldap: server: ldap://127.0.0.1 bindDN: cn=admin,dc=vesoft,dc=com bindPassword: \"\" baseDN: dc=vesoft,dc=com userFilter: \"&(objectClass=*)\" emailKey: mail Parameter Description server The LDAP server address. bindDN The LDAP login username. bindPassword The LDAP login password. baseDN Set the path to query user data. userFilter Set a filter to LDAP search queries. emailKey Set the field name used to restore email in LDAP. Restart Dashboard Enterprise Edition to apply changes. After the LDAP is configured successfully, and use the initialized account name nebula and password nebula to log into Dashboard Enterprise Edition, you can create an LDAP account then. For information about how to create an account, see Create accounts . General accounts \u00b6 All accounts are general accounts except for LDAP accounts. Account roles \u00b6 You can set different roles for your accounts. Roles are different in permissions. There are two types of account roles in Dashboard Enterprise Edition: system roles ( admin and user ) and cluster roles ( owner and operator ). The relationship between system roles and cluster roles and their descriptions are as follows. System roles : Roles Permission Description admin 1. Create accounts. 2. Modify the role of an existing account. 3. Perform platform settings, system-level alert settings. 4. Delete accounts. 1. There can be multiple admin roles, i.e. system administrators. 2. An admin is the operator of all clusters by default, i.e. an admin can manage all clusters. 3. Displayed in the cluster member list by default. An owner cannot remove an admin unless the admin is converted to user , and the system will automatically remove the admin from the cluster member list. user 1. Has read-only permissions for the system dimension. 2. After an admin creates a new account with the user role, the user account cannot view any clusters if the corresponding cluster is not assigned to the account. 3. Can create clusters and become the owner of the clusters. 1. General role. 2. There can be multiple user roles. Cluster roles : Roles Permission Description operator 1. Scale clusters. 2. Set cluster alerts. 3. Manage cluster nodes. 4. Manage cluster services. 1. The cluster operator. 2. There can be multiple operator roles in a cluster. owner 1. Have all the permissions of operator . 2. Unbind and delete clusters. 3. Add and remove accounts with operator roles. 4. Transfer the owner role. 1. The cluster owner. 2. There can only be one owner in a cluster. Create accounts \u00b6 Accounts with admin roles can create other accounts. The steps are as follows: At the top navigation bar of the Dashboard Enterprise Edition page, click Authority , and click Create . Select one method and input information to create an account, and click OK . Invite: Set the invitee's enterprise email and role. After the invitee clicks the Accept button in the email to activate the account, the invitee needs to click Login to automatically jump to the Dashboard Enterprise Edition login page. The invitee can log into Dashboard with his/her enterprise email account and password. Note When selecting the Invite method to add accounts, ensure that the invitee's email has been added to the enterprise LDAP server. Create Account: Set the login name, password, and role for the new account. For information about roles, Account roles . View accounts \u00b6 The created accounts are displayed on the Authority page. You can view the username, account type, role, associated cluster, and create time of accounts. Account Type : Includes platform and ldap . platform is a general account and ldap is an LDAP account. Role : Displays the role of an account, including admin and user . For more information about roles, see the above content. Associated Clusters : Displays all the clusters that can be operated by an account. If the cluster was created by the account, the associated cluster has the owner tag. You can search for accounts in the search box, and filter accounts by selecting an associated cluster. Other operations \u00b6 In the Action column on the Authority page, click to edit account information. In the Action column on the Authority page, click to delete an account.","title":"Authority management"},{"location":"nebula-dashboard-ent/5.account-management/#authority_management","text":"You can log into Nebula Dashboard Enterprise Edition with different types of accounts. Different accounts have different permissions. This article introduces account types, roles, and permissions.","title":"Authority management"},{"location":"nebula-dashboard-ent/5.account-management/#account_types","text":"Once you log into Dashboard Enterprise Edition using the initialized account name nebula and password nebula , you can create different types of accounts: LDAP accounts and general accounts.","title":"Account types"},{"location":"nebula-dashboard-ent/5.account-management/#ldap_accounts","text":"Dashboard Enterprise Edition enables you to log into it with your enterprise account by accessing LDAP\uff08Lightweight Directory Access Protocol\uff09 . Before using an LDAP account, LDAP configurations are required. In the Dashboard Enterprise Edition installation path, such as nebula-graph-dashboard-ent/nebula-dashboard-ent/config , find the configuration file config.yaml . In config.yaml , add the following content. ldap: server: ldap://127.0.0.1 bindDN: cn=admin,dc=vesoft,dc=com bindPassword: \"\" baseDN: dc=vesoft,dc=com userFilter: \"&(objectClass=*)\" emailKey: mail Parameter Description server The LDAP server address. bindDN The LDAP login username. bindPassword The LDAP login password. baseDN Set the path to query user data. userFilter Set a filter to LDAP search queries. emailKey Set the field name used to restore email in LDAP. Restart Dashboard Enterprise Edition to apply changes. After the LDAP is configured successfully, and use the initialized account name nebula and password nebula to log into Dashboard Enterprise Edition, you can create an LDAP account then. For information about how to create an account, see Create accounts .","title":"LDAP accounts"},{"location":"nebula-dashboard-ent/5.account-management/#general_accounts","text":"All accounts are general accounts except for LDAP accounts.","title":"General accounts"},{"location":"nebula-dashboard-ent/5.account-management/#account_roles","text":"You can set different roles for your accounts. Roles are different in permissions. There are two types of account roles in Dashboard Enterprise Edition: system roles ( admin and user ) and cluster roles ( owner and operator ). The relationship between system roles and cluster roles and their descriptions are as follows. System roles : Roles Permission Description admin 1. Create accounts. 2. Modify the role of an existing account. 3. Perform platform settings, system-level alert settings. 4. Delete accounts. 1. There can be multiple admin roles, i.e. system administrators. 2. An admin is the operator of all clusters by default, i.e. an admin can manage all clusters. 3. Displayed in the cluster member list by default. An owner cannot remove an admin unless the admin is converted to user , and the system will automatically remove the admin from the cluster member list. user 1. Has read-only permissions for the system dimension. 2. After an admin creates a new account with the user role, the user account cannot view any clusters if the corresponding cluster is not assigned to the account. 3. Can create clusters and become the owner of the clusters. 1. General role. 2. There can be multiple user roles. Cluster roles : Roles Permission Description operator 1. Scale clusters. 2. Set cluster alerts. 3. Manage cluster nodes. 4. Manage cluster services. 1. The cluster operator. 2. There can be multiple operator roles in a cluster. owner 1. Have all the permissions of operator . 2. Unbind and delete clusters. 3. Add and remove accounts with operator roles. 4. Transfer the owner role. 1. The cluster owner. 2. There can only be one owner in a cluster.","title":"Account roles"},{"location":"nebula-dashboard-ent/5.account-management/#create_accounts","text":"Accounts with admin roles can create other accounts. The steps are as follows: At the top navigation bar of the Dashboard Enterprise Edition page, click Authority , and click Create . Select one method and input information to create an account, and click OK . Invite: Set the invitee's enterprise email and role. After the invitee clicks the Accept button in the email to activate the account, the invitee needs to click Login to automatically jump to the Dashboard Enterprise Edition login page. The invitee can log into Dashboard with his/her enterprise email account and password. Note When selecting the Invite method to add accounts, ensure that the invitee's email has been added to the enterprise LDAP server. Create Account: Set the login name, password, and role for the new account. For information about roles, Account roles .","title":"Create accounts"},{"location":"nebula-dashboard-ent/5.account-management/#view_accounts","text":"The created accounts are displayed on the Authority page. You can view the username, account type, role, associated cluster, and create time of accounts. Account Type : Includes platform and ldap . platform is a general account and ldap is an LDAP account. Role : Displays the role of an account, including admin and user . For more information about roles, see the above content. Associated Clusters : Displays all the clusters that can be operated by an account. If the cluster was created by the account, the associated cluster has the owner tag. You can search for accounts in the search box, and filter accounts by selecting an associated cluster.","title":"View accounts"},{"location":"nebula-dashboard-ent/5.account-management/#other_operations","text":"In the Action column on the Authority page, click to edit account information. In the Action column on the Authority page, click to delete an account.","title":"Other operations"},{"location":"nebula-dashboard-ent/6.global-config/","text":"Global settings \u00b6 This article describes the global settings of using Dashboard Enterprise Edition, including interface settings, help center, and user information. Interface settings \u00b6 At the top navigation bar of the Dashboard Enterprise Edition, click Interface settings to set system, notification, and other settings. System settings \u00b6 On the left-side navigation bar of the Interface Settings , click System Settings to modify the page title, logo image, and cover image. Notification Endpoints \u00b6 Mail \uff1aDashboard Enterprise Edition supports sending and receiving alert messages for all clusters via E-mail. On the left-side navigation bar of the Interface Settings page, click Notification Endpoints -> Mail : You need to set the following parameters to send alert messages. Parameter Description SMTP Server Address The SMTP server address corresponding to your mailbox. Port The port number of the SMTP server corresponding to your mailbox. Use SSL Check the box to enable SSL for encrypted data transmission. SMTP User Name The SMTP server account name. SMTP Password The SMTP server password. Sender Email The email address of the one who sent you the email. You need to set a receiver to receive alert messages. Parameter Description Receiver Set the email address to receive alert messages. This email address will receive alert messages for all clusters created on Dashboard. Webhook \uff1aSupports configuring Webhook to bring all cluster alert messages into third-party projects. On the left-side navigation bar of the Interface Settings page, click Notification Endpoints -> Webhook to input the Webhook URL used to receive alert messages. You can turn on or off the Webhook feature at the top right of the page. Other settings \u00b6 On the left-side navigation bar of the Interface Settings page, click Other Settings to have the following operations: Change the display language. Currently, only Chinese and English are supported. Turn on or off help tips. An example of tips is as follows. Help center \u00b6 At the top navigation bar of the Dashboard Enterprise Edition, click Help . On the Help page, you can jump to Dashboard Docs, Nebula Graph Docs, Nebula Graph Website, or Nebula Graph Forum. User information \u00b6 At the top right of the Dashboard Enterprise Edition page, hover mouse to your account name, such as nebula : Click Profile to view your account information and modify the account login password. Note For an LDAP account, the login password cannot be modified. For more information about accounts, see Authority management . Click Logout to log out of the current account.","title":"Global settings"},{"location":"nebula-dashboard-ent/6.global-config/#global_settings","text":"This article describes the global settings of using Dashboard Enterprise Edition, including interface settings, help center, and user information.","title":"Global settings"},{"location":"nebula-dashboard-ent/6.global-config/#interface_settings","text":"At the top navigation bar of the Dashboard Enterprise Edition, click Interface settings to set system, notification, and other settings.","title":"Interface settings"},{"location":"nebula-dashboard-ent/6.global-config/#system_settings","text":"On the left-side navigation bar of the Interface Settings , click System Settings to modify the page title, logo image, and cover image.","title":"System settings"},{"location":"nebula-dashboard-ent/6.global-config/#notification_endpoints","text":"Mail \uff1aDashboard Enterprise Edition supports sending and receiving alert messages for all clusters via E-mail. On the left-side navigation bar of the Interface Settings page, click Notification Endpoints -> Mail : You need to set the following parameters to send alert messages. Parameter Description SMTP Server Address The SMTP server address corresponding to your mailbox. Port The port number of the SMTP server corresponding to your mailbox. Use SSL Check the box to enable SSL for encrypted data transmission. SMTP User Name The SMTP server account name. SMTP Password The SMTP server password. Sender Email The email address of the one who sent you the email. You need to set a receiver to receive alert messages. Parameter Description Receiver Set the email address to receive alert messages. This email address will receive alert messages for all clusters created on Dashboard. Webhook \uff1aSupports configuring Webhook to bring all cluster alert messages into third-party projects. On the left-side navigation bar of the Interface Settings page, click Notification Endpoints -> Webhook to input the Webhook URL used to receive alert messages. You can turn on or off the Webhook feature at the top right of the page.","title":"Notification Endpoints"},{"location":"nebula-dashboard-ent/6.global-config/#other_settings","text":"On the left-side navigation bar of the Interface Settings page, click Other Settings to have the following operations: Change the display language. Currently, only Chinese and English are supported. Turn on or off help tips. An example of tips is as follows.","title":"Other settings"},{"location":"nebula-dashboard-ent/6.global-config/#help_center","text":"At the top navigation bar of the Dashboard Enterprise Edition, click Help . On the Help page, you can jump to Dashboard Docs, Nebula Graph Docs, Nebula Graph Website, or Nebula Graph Forum.","title":"Help center"},{"location":"nebula-dashboard-ent/6.global-config/#user_information","text":"At the top right of the Dashboard Enterprise Edition page, hover mouse to your account name, such as nebula : Click Profile to view your account information and modify the account login password. Note For an LDAP account, the login password cannot be modified. For more information about accounts, see Authority management . Click Logout to log out of the current account.","title":"User information"},{"location":"nebula-dashboard-ent/7.monitor-parameter/","text":"Metrics \u00b6 This topic will describe the monitoring metrics in Nebula Dashboard. Machine \u00b6 Note All the machine metrics listed below are for the Linux operating system. The default unit for Disk and Network is byte. The unit changes with the data magnitude as the page displays. For example, when the flow is less than 1 KB/s, the unit is Bytes/s. For all versions of Dashboard Enterprise Edition, the memory occupied by Buff and Cache will not be counted in the memory usage. CPU \u00b6 Parameter Description cpu_utilization The percentage of used CPU. cpu_idle The percentage of idled CPU. cpu_wait The percentage of CPU waiting for IO operations. cpu_user The percentage of CPU used by users. cpu_system The percentage of CPU used by the system. Memory \u00b6 Parameter Description memory_utilization The percentage of used memory. memory_used The memory space used (including caches). memory_actual_used The memory space used (not including caches). memory_free The memory space available. Load \u00b6 Parameter Description load_1m The average load of the system in the last 1 minute. load_5m The average load of the system in the last 5 minutes. load_15m The average load of the system in the last 15 minutes. Disk \u00b6 Parameter Description disk_used The disk space used. disk_free The disk space available. disk_readbytes The number of bytes that the system reads in the disk per second. disk_writebytes The number of bytes that the system writes in the disk per second. disk_readiops The number of read queries that the disk receives per second. disk_writeiops The number of write queries that the disk receives per second. inode_utilization The percentage of used inode. Network \u00b6 Parameter Description network_in_rate The number of bytes that the network card receives per second. network_out_rate The number of bytes that the network card sends out per second. network_in_errs The number of wrong bytes that the network card receives per second. network_out_errs The number of wrong bytes that the network card sends out per second. network_in_packets The number of data packages that the network card receives per second. network_out_packets The number of data packages that the network card sends out per second. Service \u00b6 Period \u00b6 The period is the time range of counting metrics. It currently supports 5 seconds, 60 seconds, 600 seconds, and 3600 seconds, which respectively represent the last 5 seconds, the last 1 minute, the last 10 minutes, and the last 1 hour. Metric methods \u00b6 Parameter Description rate The average rate of operations per second in a period. sum The sum of operations in the period. avg The average latency in the cycle. P75 The 75th percentile latency. P95 The 95th percentile latency. P99 The 99th percentile latency. P999 The 99.9th percentile latency. Graph \u00b6 Parameter Description num_active_queries The number of queries currently being executed. num_active_sessions The number of currently active sessions. num_aggregate_executors The number of executions for the Aggregation operator. num_auth_failed_sessions_bad_username_password The number of sessions where authentication failed due to incorrect username and password. num_auth_failed_sessions The number of sessions in which login authentication failed. num_indexscan_executors The number of executions for index scan operators. num_killed_queries The number of killed queries. num_opened_sessions The number of sessions connected to the server. num_queries The number of queries. num_query_errors_leader_changes The number of raft leader changes due to query errors. num_query_errors The number of query errors. num_reclaimed_expired_sessions The number of expired sessions actively reclaimed by the server. num_rpc_sent_to_metad_failed The number of failed RPC requests that the Graphd service sends to the Metad service. num_rpc_sent_to_metad The number of RPC requests that the Graphd service sent to the Metad service. num_rpc_sent_to_storaged_failed The number of failed RPC requests that the Graphd service sent to the Storaged service. num_rpc_sent_to_storaged The number of RPC requests that the Graphd service sent to the Storaged service. num_sentences The number of statements received by the Graphd service. num_slow_queries The number of slow queries. num_sort_executors The number of executions for the Sort operator. optimizer_latency_us The latency of executing optimizer statements. query_latency_us The average latency of queries. slow_query_latency_us The average latency of slow queries. Meta \u00b6 Parameter Description commit_log_latency_us The latency of committing logs in Raft. commit_snapshot_latency_us The latency of committing snapshots in Raft. heartbeat_latency_us The latency of heartbeats. num_heartbeats The number of heartbeats. num_raft_votes The number of votes in Raft. transfer_leader_latency_us The latency of transferring the raft leader. Storage \u00b6 Parameter Description add_edges_latency_us The average latency of adding edges. add_vertices_latency_us The average latency of adding vertices. commit_log_latency_us The latency of committing logs in Raft. commit_snapshot_latency_us The latency of committing snapshots in Raft. delete_edges_latency_us The average latency of deleting edges. delete_vertices_latency_us The average latency of deleting vertices. get_neighbors_latency_us The average latency of querying neighbor vertices. num_edges_deleted The number of deleted edges. num_edges_inserted The number of inserted edges. num_raft_votes The number of votes in Raft. num_rpc_sent_to_metad_failed The number of failed RPC requests that the Storaged service sent to the Metad service. num_rpc_sent_to_metad The number of RPC requests that the Storage service sent to the Meta service. num_tags_deleted The number of deleted tags. num_vertices_deleted The number of deleted vertices. num_vertices_inserted The number of inserted vertices. transfer_leader_latency_us The latency of transferring the raft leader. Graph space \u00b6 Parameter Description num_active_queries The number of queries currently being executed. num_aggregate_executors The number of executions for the Aggregation operator. num_indexscan_executors The number of executions for index scan operators. num_killed_queries The number of killed queries. num_queries The number of queries. num_query_errors_leader_changes The number of raft leader changes due to query errors. num_query_errors The number of query errors. num_sentences The number of statements received by the Graphd service. num_slow_queries The number of slow queries. num_sort_executors The number of executions for the Sort operator. optimizer_latency_us The latency of executing optimizer statements. query_latency_us The average latency of queries.","title":"Monitoring metrics"},{"location":"nebula-dashboard-ent/7.monitor-parameter/#metrics","text":"This topic will describe the monitoring metrics in Nebula Dashboard.","title":"Metrics"},{"location":"nebula-dashboard-ent/7.monitor-parameter/#machine","text":"Note All the machine metrics listed below are for the Linux operating system. The default unit for Disk and Network is byte. The unit changes with the data magnitude as the page displays. For example, when the flow is less than 1 KB/s, the unit is Bytes/s. For all versions of Dashboard Enterprise Edition, the memory occupied by Buff and Cache will not be counted in the memory usage.","title":"Machine"},{"location":"nebula-dashboard-ent/7.monitor-parameter/#cpu","text":"Parameter Description cpu_utilization The percentage of used CPU. cpu_idle The percentage of idled CPU. cpu_wait The percentage of CPU waiting for IO operations. cpu_user The percentage of CPU used by users. cpu_system The percentage of CPU used by the system.","title":"CPU"},{"location":"nebula-dashboard-ent/7.monitor-parameter/#memory","text":"Parameter Description memory_utilization The percentage of used memory. memory_used The memory space used (including caches). memory_actual_used The memory space used (not including caches). memory_free The memory space available.","title":"Memory"},{"location":"nebula-dashboard-ent/7.monitor-parameter/#load","text":"Parameter Description load_1m The average load of the system in the last 1 minute. load_5m The average load of the system in the last 5 minutes. load_15m The average load of the system in the last 15 minutes.","title":"Load"},{"location":"nebula-dashboard-ent/7.monitor-parameter/#disk","text":"Parameter Description disk_used The disk space used. disk_free The disk space available. disk_readbytes The number of bytes that the system reads in the disk per second. disk_writebytes The number of bytes that the system writes in the disk per second. disk_readiops The number of read queries that the disk receives per second. disk_writeiops The number of write queries that the disk receives per second. inode_utilization The percentage of used inode.","title":"Disk"},{"location":"nebula-dashboard-ent/7.monitor-parameter/#network","text":"Parameter Description network_in_rate The number of bytes that the network card receives per second. network_out_rate The number of bytes that the network card sends out per second. network_in_errs The number of wrong bytes that the network card receives per second. network_out_errs The number of wrong bytes that the network card sends out per second. network_in_packets The number of data packages that the network card receives per second. network_out_packets The number of data packages that the network card sends out per second.","title":"Network"},{"location":"nebula-dashboard-ent/7.monitor-parameter/#service","text":"","title":"Service"},{"location":"nebula-dashboard-ent/7.monitor-parameter/#period","text":"The period is the time range of counting metrics. It currently supports 5 seconds, 60 seconds, 600 seconds, and 3600 seconds, which respectively represent the last 5 seconds, the last 1 minute, the last 10 minutes, and the last 1 hour.","title":"Period"},{"location":"nebula-dashboard-ent/7.monitor-parameter/#metric_methods","text":"Parameter Description rate The average rate of operations per second in a period. sum The sum of operations in the period. avg The average latency in the cycle. P75 The 75th percentile latency. P95 The 95th percentile latency. P99 The 99th percentile latency. P999 The 99.9th percentile latency.","title":"Metric methods"},{"location":"nebula-dashboard-ent/7.monitor-parameter/#graph","text":"Parameter Description num_active_queries The number of queries currently being executed. num_active_sessions The number of currently active sessions. num_aggregate_executors The number of executions for the Aggregation operator. num_auth_failed_sessions_bad_username_password The number of sessions where authentication failed due to incorrect username and password. num_auth_failed_sessions The number of sessions in which login authentication failed. num_indexscan_executors The number of executions for index scan operators. num_killed_queries The number of killed queries. num_opened_sessions The number of sessions connected to the server. num_queries The number of queries. num_query_errors_leader_changes The number of raft leader changes due to query errors. num_query_errors The number of query errors. num_reclaimed_expired_sessions The number of expired sessions actively reclaimed by the server. num_rpc_sent_to_metad_failed The number of failed RPC requests that the Graphd service sends to the Metad service. num_rpc_sent_to_metad The number of RPC requests that the Graphd service sent to the Metad service. num_rpc_sent_to_storaged_failed The number of failed RPC requests that the Graphd service sent to the Storaged service. num_rpc_sent_to_storaged The number of RPC requests that the Graphd service sent to the Storaged service. num_sentences The number of statements received by the Graphd service. num_slow_queries The number of slow queries. num_sort_executors The number of executions for the Sort operator. optimizer_latency_us The latency of executing optimizer statements. query_latency_us The average latency of queries. slow_query_latency_us The average latency of slow queries.","title":"Graph"},{"location":"nebula-dashboard-ent/7.monitor-parameter/#meta","text":"Parameter Description commit_log_latency_us The latency of committing logs in Raft. commit_snapshot_latency_us The latency of committing snapshots in Raft. heartbeat_latency_us The latency of heartbeats. num_heartbeats The number of heartbeats. num_raft_votes The number of votes in Raft. transfer_leader_latency_us The latency of transferring the raft leader.","title":"Meta"},{"location":"nebula-dashboard-ent/7.monitor-parameter/#storage","text":"Parameter Description add_edges_latency_us The average latency of adding edges. add_vertices_latency_us The average latency of adding vertices. commit_log_latency_us The latency of committing logs in Raft. commit_snapshot_latency_us The latency of committing snapshots in Raft. delete_edges_latency_us The average latency of deleting edges. delete_vertices_latency_us The average latency of deleting vertices. get_neighbors_latency_us The average latency of querying neighbor vertices. num_edges_deleted The number of deleted edges. num_edges_inserted The number of inserted edges. num_raft_votes The number of votes in Raft. num_rpc_sent_to_metad_failed The number of failed RPC requests that the Storaged service sent to the Metad service. num_rpc_sent_to_metad The number of RPC requests that the Storage service sent to the Meta service. num_tags_deleted The number of deleted tags. num_vertices_deleted The number of deleted vertices. num_vertices_inserted The number of inserted vertices. transfer_leader_latency_us The latency of transferring the raft leader.","title":"Storage"},{"location":"nebula-dashboard-ent/7.monitor-parameter/#graph_space","text":"Parameter Description num_active_queries The number of queries currently being executed. num_aggregate_executors The number of executions for the Aggregation operator. num_indexscan_executors The number of executions for index scan operators. num_killed_queries The number of killed queries. num_queries The number of queries. num_query_errors_leader_changes The number of raft leader changes due to query errors. num_query_errors The number of query errors. num_sentences The number of statements received by the Graphd service. num_slow_queries The number of slow queries. num_sort_executors The number of executions for the Sort operator. optimizer_latency_us The latency of executing optimizer statements. query_latency_us The average latency of queries.","title":"Graph space"},{"location":"nebula-dashboard-ent/8.faq/","text":"FAQ \u00b6 This topic lists the frequently asked questions for using Nebula Dashboard. You can use the search box in the help center or the search function of the browser to match the questions you are looking for. \"What are Cluster, Node, and Service?\" \u00b6 Cluster: refers to a group of systems composed of nodes where multiple Nebula Graph services are located. Node: refers to the physical or virtual machine hosting Nebula Graph services. Service: refers to Nebula services, including Metad, Storaged, and Graphd services. \"What is the cluster status?\" \u00b6 The status of a cluster is as follows: installing: The cluster is being created. The process will take about 3 to 10 minutes. healthy: All services in the cluster are healthy. unhealthy: There is an unhealthy service in the cluster service. \"Why authorizing nodes?\" \u00b6 Managing clusters requires the SSH information of the corresponding node. Therefore, you need to have at least an SSH account and the corresponding password with executable permissions before performing operations on Dashboard. \"What is scaling?\" \u00b6 Nebula Graph is a distributed graph database that supports dynamic scaling services at runtime. Therefore, you can dynamically scale Storaged and Graphd services through Dashboard. The Metad service cannot be scaled. \"Why cannot operate on the Metad service?\" \u00b6 The Metad service stores the metadata of the Nebula Graph database. Once the Metad service fails to function, the entire cluster may break down. Besides, the amount of data processed by the Metad service is not much, so it is not recommended to scale the Metad service. And we directly disabled operating on the Metad service in Dashboard to prevent the cluster from being unavailable due to the misoperation of users. \"What impact will the scaling have on the data?\" \u00b6 Scale out the Storaged service: Dashboard will create and start the Storaged service on the specified machine, which will not affect the existing data. You can choose to perform Balance Data on the Service information page or Balance Leader on the Leader page according to your own needs. Scale in the Storaged service: Dashboard will automatically execute Balance Data Remove to ensure that the service is stopped after the data partition on the specified service is successfully migrated. Scaling the Graphd service will not affect the data. \"Why Dashboard Enterprise Edition cannot be started?\" \u00b6 Make sure that the license file is copied to the Dashboard directory and sudo ./dashboard.service start all is executed. Make sure that the license is not expired. You can also execute cat logs/webserver.log in the Dashboard directory to view the startup information of each module. If the above conditions are met but Dashboard still cannot be started, go to Nebula Graph Official Forum for consultation. \"Can I add the Nebula Graph installation package manually?\" \u00b6 You can add the installation package manually in Dashboard. To download the system and RPM/DEB package you need, see How to download Nebula Graph and add the package to nebula-dashboard-ent/download/nebula-graph . And you can select the added package for deployment when creating and scaling out a cluster. Why does it prompt \u201cSSH connection error\u201d when importing a cluster\uff1f \u00b6 If Service Host shows 127.0.0.1 , and your Dashboard and Nebula Graph are deployed on the same machine when authorizing service hosts, the system will prompt \"SSH connection error\u201d. You need to change the Host IP of each service to the real machine IP in the configuration files of all Nebula Graph services. For more information, see Configuration management . If you import a cluster deployed with Docker, it also prompts \"SSH connection error\". Dashboard does not support importing a cluster deployed with Docker.","title":"FAQ"},{"location":"nebula-dashboard-ent/8.faq/#faq","text":"This topic lists the frequently asked questions for using Nebula Dashboard. You can use the search box in the help center or the search function of the browser to match the questions you are looking for.","title":"FAQ"},{"location":"nebula-dashboard-ent/8.faq/#what_are_cluster_node_and_service","text":"Cluster: refers to a group of systems composed of nodes where multiple Nebula Graph services are located. Node: refers to the physical or virtual machine hosting Nebula Graph services. Service: refers to Nebula services, including Metad, Storaged, and Graphd services.","title":"\"What are Cluster, Node, and Service?\""},{"location":"nebula-dashboard-ent/8.faq/#what_is_the_cluster_status","text":"The status of a cluster is as follows: installing: The cluster is being created. The process will take about 3 to 10 minutes. healthy: All services in the cluster are healthy. unhealthy: There is an unhealthy service in the cluster service.","title":"\"What is the cluster status?\""},{"location":"nebula-dashboard-ent/8.faq/#why_authorizing_nodes","text":"Managing clusters requires the SSH information of the corresponding node. Therefore, you need to have at least an SSH account and the corresponding password with executable permissions before performing operations on Dashboard.","title":"\"Why authorizing nodes?\""},{"location":"nebula-dashboard-ent/8.faq/#what_is_scaling","text":"Nebula Graph is a distributed graph database that supports dynamic scaling services at runtime. Therefore, you can dynamically scale Storaged and Graphd services through Dashboard. The Metad service cannot be scaled.","title":"\"What is scaling?\""},{"location":"nebula-dashboard-ent/8.faq/#why_cannot_operate_on_the_metad_service","text":"The Metad service stores the metadata of the Nebula Graph database. Once the Metad service fails to function, the entire cluster may break down. Besides, the amount of data processed by the Metad service is not much, so it is not recommended to scale the Metad service. And we directly disabled operating on the Metad service in Dashboard to prevent the cluster from being unavailable due to the misoperation of users.","title":"\"Why cannot operate on the Metad service?\""},{"location":"nebula-dashboard-ent/8.faq/#what_impact_will_the_scaling_have_on_the_data","text":"Scale out the Storaged service: Dashboard will create and start the Storaged service on the specified machine, which will not affect the existing data. You can choose to perform Balance Data on the Service information page or Balance Leader on the Leader page according to your own needs. Scale in the Storaged service: Dashboard will automatically execute Balance Data Remove to ensure that the service is stopped after the data partition on the specified service is successfully migrated. Scaling the Graphd service will not affect the data.","title":"\"What impact will the scaling have on the data?\""},{"location":"nebula-dashboard-ent/8.faq/#why_dashboard_enterprise_edition_cannot_be_started","text":"Make sure that the license file is copied to the Dashboard directory and sudo ./dashboard.service start all is executed. Make sure that the license is not expired. You can also execute cat logs/webserver.log in the Dashboard directory to view the startup information of each module. If the above conditions are met but Dashboard still cannot be started, go to Nebula Graph Official Forum for consultation.","title":"\"Why Dashboard Enterprise Edition cannot be started?\""},{"location":"nebula-dashboard-ent/8.faq/#can_i_add_the_nebula_graph_installation_package_manually","text":"You can add the installation package manually in Dashboard. To download the system and RPM/DEB package you need, see How to download Nebula Graph and add the package to nebula-dashboard-ent/download/nebula-graph . And you can select the added package for deployment when creating and scaling out a cluster.","title":"\"Can I add the Nebula Graph installation package manually?\""},{"location":"nebula-dashboard-ent/8.faq/#why_does_it_prompt_ssh_connection_error_when_importing_a_cluster","text":"If Service Host shows 127.0.0.1 , and your Dashboard and Nebula Graph are deployed on the same machine when authorizing service hosts, the system will prompt \"SSH connection error\u201d. You need to change the Host IP of each service to the real machine IP in the configuration files of all Nebula Graph services. For more information, see Configuration management . If you import a cluster deployed with Docker, it also prompts \"SSH connection error\". Dashboard does not support importing a cluster deployed with Docker.","title":"Why does it prompt \u201cSSH connection error\u201d when importing a cluster\uff1f"},{"location":"nebula-dashboard-ent/9.alerts/","text":"Alerts \u00b6 Nebula Graph alerts on monitoring metrics. You can view alert messages, set alert rules, and set alert receivers. Alert messages \u00b6 On the left side navigation bar of the Cluster Management page, click Notification -> Alert Messages to view alert messages. You can search for alert messages by message name. You can filter alert messages by date and time, and period. Available periods are 1 hour, 6 hours, 12 hours, 1 day, 3 days, 7 days, and 14 days. You can filter alert messages by severity, type, and status. Click Reset to empty all filtering results. You can set the processing status of alert messages. The status is unsolved by default, and you can set the status to Dealing or Solved . Alert messages cannot be deleted. In the nebula-dashboard-ent/config/config.yaml file, messageStore sets the number of days to keep alert messages, the value of which is 90 by default. For more information about the configuration file, see Deploy Dashboard \u3002 Alert rules \u00b6 Before receiving alert messages, you need to set alert rules. Alert rules include custom rules and build-in rules. Create custom rules \u00b6 Follow the below steps to create a custom rule. At the top navigation bar of the Dashboard Enterprise Edition page, click Cluster Management , and then on the right side of the target cluster, click Detail . On the left side of the Cluster Management page, click Notification -> Alert Rules . On the Alert Rules page, click Custom Rules , and then click Create Rule at the top right of the page. Set alert rules. On the Basic Information tab, set alert name, severity, and frequency. Parameter Description Alert Name Set a name for an alert rule. The name can only contain lowercase letters, numbers, and hyphens ( - ), and must begin and end with a lowercase letter or number. The name contains up to 253 characters. Severity Set a severity level for an alert rule. The severity level includes emergency , critical , and warning . Alert Frequency Set how often an alert message is sent. Unit: Minute\uff08Min\uff09. On the Condition tab, set metric type, rule, and alert duration. Parameter Description Metric Type Set a metric type. Metric type includes the node metric type and the service type (graphd\u3001storaged\u3001metad). Metric Rule Set metric rules for a node or a service. For more information, see Monitoring metrics . Alert duration Set how long an alert lasts before the alert message is triggered. Unit: Minute (Min). On the Message Settings tab, you can see the rule summary and rule details, and then click Submit . Note DO NOT modify the rule details unless you are clear of the consequences. View custom rules \u00b6 On the Custom Rules , you can do the following operations. Search for alert rules and filter alert rules by severity, type, metric, and status. Click Reset to empty all filtering results. Turn on or off the alert rule you set. The status of an alert rule that has been turned on is active . The status of an alert rule that has been turned off is disable . Edit custom rules \u00b6 In the Custom Rules list, select the target rule, and then click the edit icon to edit the rule. Delete custom rules \u00b6 In the Custom Rules list, select the target rule, click the delete icon to delete the rule. Built-in Rules \u00b6 The built-in rules are the default rules in Dashboard Enterprise Edition. You can enable or disable the built-in rules. The status of a built-in rule that has been turned on is active . The status of a built-in rule that has been turned off is disable . Note Built-in rules cannot be edited or deleted. Receiver configuration \u00b6 Alerts can be configured to send notifications to receivers. You can set the email address of the receiver who receives alert notifications. You can also view your Webhook URL and whether the webhook is enabled or not. For more information about the Webhook, see Global settings . At the top navigation bar of the Dashboard Enterprise Edition page, Click Cluster Management , and on the right side of the target cluster, click Detail . On the left-side navigation bar of the Cluster Management page, click Notification -> Receivers . On the Receivers page, Click Mail and input the email of the receiver who receives alert notifications and then click Add . Click Webhook and see your Webhook URL and whether the webhook is enabled or not.","title":"Alerts"},{"location":"nebula-dashboard-ent/9.alerts/#alerts","text":"Nebula Graph alerts on monitoring metrics. You can view alert messages, set alert rules, and set alert receivers.","title":"Alerts"},{"location":"nebula-dashboard-ent/9.alerts/#alert_messages","text":"On the left side navigation bar of the Cluster Management page, click Notification -> Alert Messages to view alert messages. You can search for alert messages by message name. You can filter alert messages by date and time, and period. Available periods are 1 hour, 6 hours, 12 hours, 1 day, 3 days, 7 days, and 14 days. You can filter alert messages by severity, type, and status. Click Reset to empty all filtering results. You can set the processing status of alert messages. The status is unsolved by default, and you can set the status to Dealing or Solved . Alert messages cannot be deleted. In the nebula-dashboard-ent/config/config.yaml file, messageStore sets the number of days to keep alert messages, the value of which is 90 by default. For more information about the configuration file, see Deploy Dashboard \u3002","title":"Alert messages"},{"location":"nebula-dashboard-ent/9.alerts/#alert_rules","text":"Before receiving alert messages, you need to set alert rules. Alert rules include custom rules and build-in rules.","title":"Alert rules"},{"location":"nebula-dashboard-ent/9.alerts/#create_custom_rules","text":"Follow the below steps to create a custom rule. At the top navigation bar of the Dashboard Enterprise Edition page, click Cluster Management , and then on the right side of the target cluster, click Detail . On the left side of the Cluster Management page, click Notification -> Alert Rules . On the Alert Rules page, click Custom Rules , and then click Create Rule at the top right of the page. Set alert rules. On the Basic Information tab, set alert name, severity, and frequency. Parameter Description Alert Name Set a name for an alert rule. The name can only contain lowercase letters, numbers, and hyphens ( - ), and must begin and end with a lowercase letter or number. The name contains up to 253 characters. Severity Set a severity level for an alert rule. The severity level includes emergency , critical , and warning . Alert Frequency Set how often an alert message is sent. Unit: Minute\uff08Min\uff09. On the Condition tab, set metric type, rule, and alert duration. Parameter Description Metric Type Set a metric type. Metric type includes the node metric type and the service type (graphd\u3001storaged\u3001metad). Metric Rule Set metric rules for a node or a service. For more information, see Monitoring metrics . Alert duration Set how long an alert lasts before the alert message is triggered. Unit: Minute (Min). On the Message Settings tab, you can see the rule summary and rule details, and then click Submit . Note DO NOT modify the rule details unless you are clear of the consequences.","title":"Create custom rules"},{"location":"nebula-dashboard-ent/9.alerts/#view_custom_rules","text":"On the Custom Rules , you can do the following operations. Search for alert rules and filter alert rules by severity, type, metric, and status. Click Reset to empty all filtering results. Turn on or off the alert rule you set. The status of an alert rule that has been turned on is active . The status of an alert rule that has been turned off is disable .","title":"View custom rules"},{"location":"nebula-dashboard-ent/9.alerts/#edit_custom_rules","text":"In the Custom Rules list, select the target rule, and then click the edit icon to edit the rule.","title":"Edit custom rules"},{"location":"nebula-dashboard-ent/9.alerts/#delete_custom_rules","text":"In the Custom Rules list, select the target rule, click the delete icon to delete the rule.","title":"Delete custom rules"},{"location":"nebula-dashboard-ent/9.alerts/#built-in_rules","text":"The built-in rules are the default rules in Dashboard Enterprise Edition. You can enable or disable the built-in rules. The status of a built-in rule that has been turned on is active . The status of a built-in rule that has been turned off is disable . Note Built-in rules cannot be edited or deleted.","title":"Built-in Rules"},{"location":"nebula-dashboard-ent/9.alerts/#receiver_configuration","text":"Alerts can be configured to send notifications to receivers. You can set the email address of the receiver who receives alert notifications. You can also view your Webhook URL and whether the webhook is enabled or not. For more information about the Webhook, see Global settings . At the top navigation bar of the Dashboard Enterprise Edition page, Click Cluster Management , and on the right side of the target cluster, click Detail . On the left-side navigation bar of the Cluster Management page, click Notification -> Receivers . On the Receivers page, Click Mail and input the email of the receiver who receives alert notifications and then click Add . Click Webhook and see your Webhook URL and whether the webhook is enabled or not.","title":"Receiver configuration"},{"location":"nebula-dashboard-ent/3.create-import-dashboard/1.create-cluster/","text":"Create clusters \u00b6 This topic introduces how to create clusters using Dashboard. Steps \u00b6 You can create a cluster following these steps: At the top of the Dashboard page, click the Cluster Management button. On the Cluster management page, click Create cluster . On the Create cluster page, fill in the following: Enter a Cluster Name , up to 15 characters for each name. In this example, the cluster name is test . Choose a Nebula Graph version to install. In this example, the version is Enterprise v3.1.0 . Note Some Community versions of Nebula Graph are provided for you to choose from on the Create cluster page. To install an Enterprise Edition version, you need to manually add the corresponding installer package to the nebula-dashboard-ent/download/nebula-graph path before you can see the corresponding version. Click Upload License . Note For the creation of a Community version of Nebula Graph, skip this step to upload the License file. Add nodes . The information of each node is required. Enter the IP information of each host. In this example, it is 192.168.8.129 . Enter the SSH information. In this example, the SSH port is 22 , the SSH user is vesoft , and the SSH password is nebula . Choose the target Nebula Graph package. In this example, the package is nebula-graph-ent-3.1.0-ent.el7.x86_64.rpm . Customize the cluster installation path. In this example, the default path is .nebula/cluster . (Optional) Enter the node name to make a note on the node. In this example, the note is Node_1 . Import nodes in batches . The information of each node is required. To import nodes in batches, you need to choose the installation package and click download the CSV template . Fill in the template and upload it. Ensure that the node is correct, otherwise, upload failure may happen. Select the node and add the service you need in the upper right corner. To create a cluster, you need to add 3 types of services to the node. If not familiar with the Nebula Graph architecture, click Auto add service . (Optional) Edit the port and HTTP port of the meta, graph, and storage services, and then click OK . Click Create Cluster . Make sure the configuration is correct and there is no conflict between nodes, and then click Confirm . If a cluster with the status of installing appears in the list on the cluster management page, you need to wait for 3 to 10 minutes until the status changes to healthy , that is, the cluster is created successfully. If the service status is unhealthy , it means that there is an abnormal service in the cluster, click Detail for more information. Next to do \u00b6 After the cluster is successfully created, you can operate on the cluster. For details, see Cluster operations .","title":"Create clusters"},{"location":"nebula-dashboard-ent/3.create-import-dashboard/1.create-cluster/#create_clusters","text":"This topic introduces how to create clusters using Dashboard.","title":"Create clusters"},{"location":"nebula-dashboard-ent/3.create-import-dashboard/1.create-cluster/#steps","text":"You can create a cluster following these steps: At the top of the Dashboard page, click the Cluster Management button. On the Cluster management page, click Create cluster . On the Create cluster page, fill in the following: Enter a Cluster Name , up to 15 characters for each name. In this example, the cluster name is test . Choose a Nebula Graph version to install. In this example, the version is Enterprise v3.1.0 . Note Some Community versions of Nebula Graph are provided for you to choose from on the Create cluster page. To install an Enterprise Edition version, you need to manually add the corresponding installer package to the nebula-dashboard-ent/download/nebula-graph path before you can see the corresponding version. Click Upload License . Note For the creation of a Community version of Nebula Graph, skip this step to upload the License file. Add nodes . The information of each node is required. Enter the IP information of each host. In this example, it is 192.168.8.129 . Enter the SSH information. In this example, the SSH port is 22 , the SSH user is vesoft , and the SSH password is nebula . Choose the target Nebula Graph package. In this example, the package is nebula-graph-ent-3.1.0-ent.el7.x86_64.rpm . Customize the cluster installation path. In this example, the default path is .nebula/cluster . (Optional) Enter the node name to make a note on the node. In this example, the note is Node_1 . Import nodes in batches . The information of each node is required. To import nodes in batches, you need to choose the installation package and click download the CSV template . Fill in the template and upload it. Ensure that the node is correct, otherwise, upload failure may happen. Select the node and add the service you need in the upper right corner. To create a cluster, you need to add 3 types of services to the node. If not familiar with the Nebula Graph architecture, click Auto add service . (Optional) Edit the port and HTTP port of the meta, graph, and storage services, and then click OK . Click Create Cluster . Make sure the configuration is correct and there is no conflict between nodes, and then click Confirm . If a cluster with the status of installing appears in the list on the cluster management page, you need to wait for 3 to 10 minutes until the status changes to healthy , that is, the cluster is created successfully. If the service status is unhealthy , it means that there is an abnormal service in the cluster, click Detail for more information.","title":"Steps"},{"location":"nebula-dashboard-ent/3.create-import-dashboard/1.create-cluster/#next_to_do","text":"After the cluster is successfully created, you can operate on the cluster. For details, see Cluster operations .","title":"Next to do"},{"location":"nebula-dashboard-ent/3.create-import-dashboard/2.import-cluster/","text":"Import clusters \u00b6 This topic introduces how to import clusters using Dashboard. The current version only supports importing clusters deployed by the official DEB or RPM packages and clusters created by Dashboard. Currently, importing clusters deployed by Docker and Kubernetes is not supported. Steps \u00b6 Caution In the same cluster, the service versions need to be unified. Importing Nebula Graph examples from different versions in the same cluster is not supported. In the configuration files of each service, change the IP in <meta|graph|storage>_server_addrs and local_ip to the server's IP, and then start Nebula Graph. For details, see Configurations and Manage Nebula Graph services . On the Cluster management page, click Import cluster . On the Import cluster page, enter the information of Connect to Nebula Graph . Graphd Host: :n . In this example, the IP is 192.168.8.157:9669 . Username: The account to connect to Nebula Graph. In this example, the username is vesoft . Password: The password to connect to Nebula Graph. In this example, the password is nebula . Note By default, authentication is disabled in Nebula Graph. Therefore, you can use root as the username and any password to connect to Nebula Graph. When authentication is enabled in Nebula Graph, you need to use the specified username and password to connect to Nebula Graph. For details of authentication, see Nebula Graph manual . On the Nebula Graph connection panel, fill in the following: Enter the cluster name, 15 characters at most. In this example, the cluster name is create_1027 , and choose whether to use sudo to connect to the cluster. Notice If your SSH account does not have permission for the Nebula Graph cluster, you can use sudo to connect to it. Authorize the node. The SSH username and password of each node are required, and choose to run sudo or not. Notice If your SSH account has no permission to operate Nebula Graph, but can execute sudo commands without password, set use sudo to yes . Batch authorization requires uploading the CSV file. Edit the authentication information of each node according to the downloaded CSV file. Ensure that the node information is correct, otherwise upload failure may happen. If the node status on the page becomes authorized , the node authentication is successful. Ensure that all nodes are authorized successfully. Click Import cluster . Next to do \u00b6 After the cluster is successfully imported, you can operate the cluster. For details, see Overview .","title":"Import clusters"},{"location":"nebula-dashboard-ent/3.create-import-dashboard/2.import-cluster/#import_clusters","text":"This topic introduces how to import clusters using Dashboard. The current version only supports importing clusters deployed by the official DEB or RPM packages and clusters created by Dashboard. Currently, importing clusters deployed by Docker and Kubernetes is not supported.","title":"Import clusters"},{"location":"nebula-dashboard-ent/3.create-import-dashboard/2.import-cluster/#steps","text":"Caution In the same cluster, the service versions need to be unified. Importing Nebula Graph examples from different versions in the same cluster is not supported. In the configuration files of each service, change the IP in <meta|graph|storage>_server_addrs and local_ip to the server's IP, and then start Nebula Graph. For details, see Configurations and Manage Nebula Graph services . On the Cluster management page, click Import cluster . On the Import cluster page, enter the information of Connect to Nebula Graph . Graphd Host: :n . In this example, the IP is 192.168.8.157:9669 . Username: The account to connect to Nebula Graph. In this example, the username is vesoft . Password: The password to connect to Nebula Graph. In this example, the password is nebula . Note By default, authentication is disabled in Nebula Graph. Therefore, you can use root as the username and any password to connect to Nebula Graph. When authentication is enabled in Nebula Graph, you need to use the specified username and password to connect to Nebula Graph. For details of authentication, see Nebula Graph manual . On the Nebula Graph connection panel, fill in the following: Enter the cluster name, 15 characters at most. In this example, the cluster name is create_1027 , and choose whether to use sudo to connect to the cluster. Notice If your SSH account does not have permission for the Nebula Graph cluster, you can use sudo to connect to it. Authorize the node. The SSH username and password of each node are required, and choose to run sudo or not. Notice If your SSH account has no permission to operate Nebula Graph, but can execute sudo commands without password, set use sudo to yes . Batch authorization requires uploading the CSV file. Edit the authentication information of each node according to the downloaded CSV file. Ensure that the node information is correct, otherwise upload failure may happen. If the node status on the page becomes authorized , the node authentication is successful. Ensure that all nodes are authorized successfully. Click Import cluster .","title":"Steps"},{"location":"nebula-dashboard-ent/3.create-import-dashboard/2.import-cluster/#next_to_do","text":"After the cluster is successfully imported, you can operate the cluster. For details, see Overview .","title":"Next to do"},{"location":"nebula-dashboard-ent/4.cluster-operator/1.overview/","text":"Cluster overview \u00b6 This topic introduces the Overview page of Dashboard. At the top of the Dashboard page, click Cluster Management , and then click Detail on the right of the cluster management page to check the overview of a specified cluster. Overview \u00b6 The Overview page has five parts: Cluster survey Alert Information Node Status list Service Cluster survey \u00b6 In this part, you can view the number of nodes as well as the number of running and abnormal services of Graphd, Storaged, and Metad. You can click the View button to quickly check the abnormal service and node. Alert \u00b6 In the Alert section, the system displays the five most recently triggered alert messages according to their severity levels ( emergency > critical > warning ). In the right upper corner, click Alert Messages to view alert messages. For more information about alerts, see Alerts . Information \u00b6 In this part, you can view the information of Cluster Name , Creation Time , Expiration Time , Creator , and Version . Cluster Name \uff1aThe name of the cluster. Creation Time \uff1aThe time when the cluster is created. Expiration Time \uff1aThe time when the cluster license expires. Enterpriseonly The parameter Expiration Time is displayed only if the created or imported cluster is an Enterprise Edition cluster. Creator \uff1aThe Dashboard account that is used to create the cluster. Version \uff1aThe version of Nebula Graph installed in the cluster. In the upper right of the Information section, click to view the cluster details, including name, creation time, account name, version, and the role of the account name. Enterpriseonly For Enterprise Edition, there is a License section. Displays the license details of the cluster, including the usage status, the organization, the creation time, the expiration time, the cluster versions supported by the license, and the license type. Supports uploading new licenses. When the license expires, you cannot perform operations on the current cluster. Click Upload License to upload a new license. Node \u00b6 You can view the information of node monitoring quickly and change the displayed information. By default, the CPU information will be shown. You can click on the page to insert a base line. You can click to jump to the detailed node monitoring page. Status list \u00b6 This part uses pie charts to visually display the running status of nodes. Service \u00b6 By default, the information of query_latency_us and slow_query_latency_us will be shown. You can click Set up to insert a base line. You can click View to jump to the detailed service monitoring page.","title":"Cluster overview"},{"location":"nebula-dashboard-ent/4.cluster-operator/1.overview/#cluster_overview","text":"This topic introduces the Overview page of Dashboard. At the top of the Dashboard page, click Cluster Management , and then click Detail on the right of the cluster management page to check the overview of a specified cluster.","title":"Cluster overview"},{"location":"nebula-dashboard-ent/4.cluster-operator/1.overview/#overview","text":"The Overview page has five parts: Cluster survey Alert Information Node Status list Service","title":"Overview"},{"location":"nebula-dashboard-ent/4.cluster-operator/1.overview/#cluster_survey","text":"In this part, you can view the number of nodes as well as the number of running and abnormal services of Graphd, Storaged, and Metad. You can click the View button to quickly check the abnormal service and node.","title":"Cluster survey"},{"location":"nebula-dashboard-ent/4.cluster-operator/1.overview/#alert","text":"In the Alert section, the system displays the five most recently triggered alert messages according to their severity levels ( emergency > critical > warning ). In the right upper corner, click Alert Messages to view alert messages. For more information about alerts, see Alerts .","title":"Alert"},{"location":"nebula-dashboard-ent/4.cluster-operator/1.overview/#information","text":"In this part, you can view the information of Cluster Name , Creation Time , Expiration Time , Creator , and Version . Cluster Name \uff1aThe name of the cluster. Creation Time \uff1aThe time when the cluster is created. Expiration Time \uff1aThe time when the cluster license expires. Enterpriseonly The parameter Expiration Time is displayed only if the created or imported cluster is an Enterprise Edition cluster. Creator \uff1aThe Dashboard account that is used to create the cluster. Version \uff1aThe version of Nebula Graph installed in the cluster. In the upper right of the Information section, click to view the cluster details, including name, creation time, account name, version, and the role of the account name. Enterpriseonly For Enterprise Edition, there is a License section. Displays the license details of the cluster, including the usage status, the organization, the creation time, the expiration time, the cluster versions supported by the license, and the license type. Supports uploading new licenses. When the license expires, you cannot perform operations on the current cluster. Click Upload License to upload a new license.","title":"Information"},{"location":"nebula-dashboard-ent/4.cluster-operator/1.overview/#node","text":"You can view the information of node monitoring quickly and change the displayed information. By default, the CPU information will be shown. You can click on the page to insert a base line. You can click to jump to the detailed node monitoring page.","title":"Node"},{"location":"nebula-dashboard-ent/4.cluster-operator/1.overview/#status_list","text":"This part uses pie charts to visually display the running status of nodes.","title":"Status list"},{"location":"nebula-dashboard-ent/4.cluster-operator/1.overview/#service","text":"By default, the information of query_latency_us and slow_query_latency_us will be shown. You can click Set up to insert a base line. You can click View to jump to the detailed service monitoring page.","title":"Service"},{"location":"nebula-dashboard-ent/4.cluster-operator/2.monitor/","text":"Cluster monitoring \u00b6 This topic introduces node monitoring, service monitoring, graph space monitoring, and the Big Screen of Dashboard. Node \u00b6 At the top navigation bar of the Dashboard Enterprise Edition page, click Cluster Management , and click Monitoring -> Node to enter the node monitoring page. On this page, you can view the variation of CPU, Memory, Load, Disk, and Network In/Out quickly. To set a base line, click the button. To view the detailed monitoring information, click the button. In this example, select Load for details. The figure is as follows. By default, you can view the monitoring data of the latest 1 hour, 6 hours, 12 hours, 1 day, 3 days, 7days, or 14 days. You can select the machine and monitoring metrics that you want to view. For details of monitoring metrics, see Monitor parameter . You can set a base line as a reference standard. Service \u00b6 At the top navigation bar of the Dashboard Enterprise Edition page, click Cluster Management , and click Monitoring -> Service to enter the service monitoring page. On this page, you can view the information of Graph, Meta, and Storage services quickly. In the upper right corner, the number of normal services and abnormal services will be displayed. Note In the current Service page of the Enterprise Edition, only two monitoring metrics can be set for each service, which can be adjusted by clicking the Set up button. To view the detailed monitoring information, click the button. In this example, select Graph for details. The figure is as follows. By default, you can view the monitoring data of the latest 1 hour, 6 hours, 12 hours, 1 day, 3 days, 7days, or 14 days. You can select the machine and monitoring metrics that you want to view. For details of monitoring metrics, see Monitor parameter . The Graph service supports a set of space-level metrics. For more information, see the following section Graph space . You can set a base line as a reference standard. You can view the status of the current service. Graph space \u00b6 Note Before using graph space metrics, you need to set enable_space_level_metrics to true in the Graph service. For details, see Update config . Space graph metrics record the information of different graph spaces separately. Currently, only the Graph service supports a set of space-level metrics. Only when the behavior of a graph space metric is triggered, you can specify the graph space to view information about the corresponding graph space metric . For information about the space graph metrics, see Space graph . Big Screen \u00b6 The Big Screen feature helps users understand the health status of the cluster and the information of services and nodes at a glance. At the top navigation bar of the Dashboard Enterprise Edition page, click Cluster Management , and click Monitoring -> Big Screen to enter the Big Screen page. Screen area Information displayed Upper middle area 1. The health degree of your cluster. The system scores the health of your cluster. For more information, see the following note. 2. The information and number of running nodes, the number of running services and abnormal services in the cluster. 3. CPU and memory usage of the node at the current time. 4. Alert notifications. The system displays the 5 most recently triggered alert messages based on their severity level (emergency>critical>warning). For more information, Monitoring alerts . Lower middle area Monitoring information of 4 Graph service metrics at different periods. The 4 metrics are: 1. num_active_sessions 2. num_slow_queries 3. num_active_queries 4. num_query_errors Left side of the area 1. QPS (Query Per Second) of your cluster. 2. The monitoring information of 2 Storage service metrics at different periods. The two metrics are: add_edges_latency_us\u3001add_vertices_latency_us. Right side of the area The node-related metrics information at different periods. Metrics include: 1. cpu_utilization 2. memory_utilization 3. load_1m 4. disk_readbytes 5. disk_writebytes For more information about the monitoring metrics, see Metrics . Note Cluster scoring rules are as follows: The maximum score is 100; The minimum score is 13. When 100\u2265Health Degree\u226580, the score is blue; When 80\uff1eHealth Degree\u226560, the score is yellow; When Health Degree\uff1c60, the score is yellow. Algorithm: (1-number of abnormal services/total number of services)*100%\u3002 Except for the appearance of the first emergency level alert that deducts 40 points, 10 points are deducted for each of the other emergency level alerts and other levels of alerts.","title":"Cluster monitoring"},{"location":"nebula-dashboard-ent/4.cluster-operator/2.monitor/#cluster_monitoring","text":"This topic introduces node monitoring, service monitoring, graph space monitoring, and the Big Screen of Dashboard.","title":"Cluster monitoring"},{"location":"nebula-dashboard-ent/4.cluster-operator/2.monitor/#node","text":"At the top navigation bar of the Dashboard Enterprise Edition page, click Cluster Management , and click Monitoring -> Node to enter the node monitoring page. On this page, you can view the variation of CPU, Memory, Load, Disk, and Network In/Out quickly. To set a base line, click the button. To view the detailed monitoring information, click the button. In this example, select Load for details. The figure is as follows. By default, you can view the monitoring data of the latest 1 hour, 6 hours, 12 hours, 1 day, 3 days, 7days, or 14 days. You can select the machine and monitoring metrics that you want to view. For details of monitoring metrics, see Monitor parameter . You can set a base line as a reference standard.","title":"Node"},{"location":"nebula-dashboard-ent/4.cluster-operator/2.monitor/#service","text":"At the top navigation bar of the Dashboard Enterprise Edition page, click Cluster Management , and click Monitoring -> Service to enter the service monitoring page. On this page, you can view the information of Graph, Meta, and Storage services quickly. In the upper right corner, the number of normal services and abnormal services will be displayed. Note In the current Service page of the Enterprise Edition, only two monitoring metrics can be set for each service, which can be adjusted by clicking the Set up button. To view the detailed monitoring information, click the button. In this example, select Graph for details. The figure is as follows. By default, you can view the monitoring data of the latest 1 hour, 6 hours, 12 hours, 1 day, 3 days, 7days, or 14 days. You can select the machine and monitoring metrics that you want to view. For details of monitoring metrics, see Monitor parameter . The Graph service supports a set of space-level metrics. For more information, see the following section Graph space . You can set a base line as a reference standard. You can view the status of the current service.","title":"Service"},{"location":"nebula-dashboard-ent/4.cluster-operator/2.monitor/#graph_space","text":"Note Before using graph space metrics, you need to set enable_space_level_metrics to true in the Graph service. For details, see Update config . Space graph metrics record the information of different graph spaces separately. Currently, only the Graph service supports a set of space-level metrics. Only when the behavior of a graph space metric is triggered, you can specify the graph space to view information about the corresponding graph space metric . For information about the space graph metrics, see Space graph .","title":"Graph space"},{"location":"nebula-dashboard-ent/4.cluster-operator/2.monitor/#big_screen","text":"The Big Screen feature helps users understand the health status of the cluster and the information of services and nodes at a glance. At the top navigation bar of the Dashboard Enterprise Edition page, click Cluster Management , and click Monitoring -> Big Screen to enter the Big Screen page. Screen area Information displayed Upper middle area 1. The health degree of your cluster. The system scores the health of your cluster. For more information, see the following note. 2. The information and number of running nodes, the number of running services and abnormal services in the cluster. 3. CPU and memory usage of the node at the current time. 4. Alert notifications. The system displays the 5 most recently triggered alert messages based on their severity level (emergency>critical>warning). For more information, Monitoring alerts . Lower middle area Monitoring information of 4 Graph service metrics at different periods. The 4 metrics are: 1. num_active_sessions 2. num_slow_queries 3. num_active_queries 4. num_query_errors Left side of the area 1. QPS (Query Per Second) of your cluster. 2. The monitoring information of 2 Storage service metrics at different periods. The two metrics are: add_edges_latency_us\u3001add_vertices_latency_us. Right side of the area The node-related metrics information at different periods. Metrics include: 1. cpu_utilization 2. memory_utilization 3. load_1m 4. disk_readbytes 5. disk_writebytes For more information about the monitoring metrics, see Metrics . Note Cluster scoring rules are as follows: The maximum score is 100; The minimum score is 13. When 100\u2265Health Degree\u226580, the score is blue; When 80\uff1eHealth Degree\u226560, the score is yellow; When Health Degree\uff1c60, the score is yellow. Algorithm: (1-number of abnormal services/total number of services)*100%\u3002 Except for the appearance of the first emergency level alert that deducts 40 points, 10 points are deducted for each of the other emergency level alerts and other levels of alerts.","title":"Big Screen"},{"location":"nebula-dashboard-ent/4.cluster-operator/3.cluster-information/","text":"Cluster information \u00b6 This topic introduces the cluster information of Dashboard from two parts Overview Info and Cluster Diagnostics . The Overview Info section displays the overview information of the Nebula Graph cluster. The Cluster Diagnostics section displays the cluster Diagnostics information of the Nebula Graph cluster. Entry \u00b6 At the top navigation bar of the Dashboard Enterprise Edition page, click Cluster Management . On the right side of the target cluster, click Details . On the left-side navigation bar of the page, click Information . You will see the following parts. - Overview Info - Cluster Diagnostics Overview Info \u00b6 Note Before viewing the cluster information, you need to select any online Graph service address, enter the account to log in to Nebula Graph (not the Dashboard login account), and the corresponding password. Caution You need to ensure that Nebula Graph services have been deployed and started. For more information, see Nebula Graph installation and deployment . On the Overview Info page, you can see the information of the Nebula Graph cluster, including Storage leader distribution, Storage service details, versions and hosts information of each Nebula Graph service, and partition distribution and details. Storage Leader Distribution \u00b6 In this section, the number of Leaders and the Leader distribution will be shown. Click the Balance Leader button in the upper right corner to distribute Leaders evenly and quickly in the Nebula Graph cluster. For details about the Leader, see Storage Service . Click Detail in the upper right corner to view the details of the Leader distribution. Version \u00b6 In this section, the version and host information of each Nebula Graph service will be shown. Click Detail in the upper right corner to view the details of the version and host information. Service information \u00b6 In this section, the information on Storage services will be shown. The parameter description is as follows: Parameter Description Host The IP address of the host. Port The port of the host. Status The host status. Git Info Sha The commit ID of the current version. Leader Count The number of Leaders. Partition Distribution The distribution of partitions. Leader Distribution The distribution of Leaders. Click Detail in the upper right corner to view the details of the Storage service information. Partition Distribution \u00b6 Select the specified graph space in the upper left corner, and then you can perform the following operations: View the distribution of partitions in the specified graph space. You can see the IP addresses and ports of all Storage services in the cluster, and the number of partitions in each Storage service. Click Balance Data to evenly distribute the partitions in the specified graph space. Click Balance Data Remove to migrate the partitions in the specified Storage service and distribute them evenly to the other Storage services in the cluster. The system will guide you to select the host IP where the specified Storage service is located. Click Detail in the upper right corner to view more details. Partition information \u00b6 In this section, the information on partitions will be shown. Before viewing the partition information, you need to select a graph space in the upper left corner. The parameter description is as follows: Parameter Description Partition ID The ID of the partition. Leader The IP address and port of the leader. Peers The IP addresses and ports of all the replicas. Losts The IP addresses and ports of faulty replicas. Click Detail in the upper right corner to view details. You can also enter the partition ID into the input box in the upper right corner of the details page to filter the shown data. Cluster Diagnostics \u00b6 You can click Cluster Diagnostics under the Cluster Information menu bar to locate and analyze the problem that occurs in the cluster. For details, see Cluster Diagnostics .","title":"Cluster information"},{"location":"nebula-dashboard-ent/4.cluster-operator/3.cluster-information/#cluster_information","text":"This topic introduces the cluster information of Dashboard from two parts Overview Info and Cluster Diagnostics . The Overview Info section displays the overview information of the Nebula Graph cluster. The Cluster Diagnostics section displays the cluster Diagnostics information of the Nebula Graph cluster.","title":"Cluster information"},{"location":"nebula-dashboard-ent/4.cluster-operator/3.cluster-information/#entry","text":"At the top navigation bar of the Dashboard Enterprise Edition page, click Cluster Management . On the right side of the target cluster, click Details . On the left-side navigation bar of the page, click Information . You will see the following parts. - Overview Info - Cluster Diagnostics","title":"Entry"},{"location":"nebula-dashboard-ent/4.cluster-operator/3.cluster-information/#overview_info","text":"Note Before viewing the cluster information, you need to select any online Graph service address, enter the account to log in to Nebula Graph (not the Dashboard login account), and the corresponding password. Caution You need to ensure that Nebula Graph services have been deployed and started. For more information, see Nebula Graph installation and deployment . On the Overview Info page, you can see the information of the Nebula Graph cluster, including Storage leader distribution, Storage service details, versions and hosts information of each Nebula Graph service, and partition distribution and details.","title":"Overview Info"},{"location":"nebula-dashboard-ent/4.cluster-operator/3.cluster-information/#storage_leader_distribution","text":"In this section, the number of Leaders and the Leader distribution will be shown. Click the Balance Leader button in the upper right corner to distribute Leaders evenly and quickly in the Nebula Graph cluster. For details about the Leader, see Storage Service . Click Detail in the upper right corner to view the details of the Leader distribution.","title":"Storage Leader Distribution"},{"location":"nebula-dashboard-ent/4.cluster-operator/3.cluster-information/#version","text":"In this section, the version and host information of each Nebula Graph service will be shown. Click Detail in the upper right corner to view the details of the version and host information.","title":"Version"},{"location":"nebula-dashboard-ent/4.cluster-operator/3.cluster-information/#service_information","text":"In this section, the information on Storage services will be shown. The parameter description is as follows: Parameter Description Host The IP address of the host. Port The port of the host. Status The host status. Git Info Sha The commit ID of the current version. Leader Count The number of Leaders. Partition Distribution The distribution of partitions. Leader Distribution The distribution of Leaders. Click Detail in the upper right corner to view the details of the Storage service information.","title":"Service information"},{"location":"nebula-dashboard-ent/4.cluster-operator/3.cluster-information/#partition_distribution","text":"Select the specified graph space in the upper left corner, and then you can perform the following operations: View the distribution of partitions in the specified graph space. You can see the IP addresses and ports of all Storage services in the cluster, and the number of partitions in each Storage service. Click Balance Data to evenly distribute the partitions in the specified graph space. Click Balance Data Remove to migrate the partitions in the specified Storage service and distribute them evenly to the other Storage services in the cluster. The system will guide you to select the host IP where the specified Storage service is located. Click Detail in the upper right corner to view more details.","title":"Partition Distribution"},{"location":"nebula-dashboard-ent/4.cluster-operator/3.cluster-information/#partition_information","text":"In this section, the information on partitions will be shown. Before viewing the partition information, you need to select a graph space in the upper left corner. The parameter description is as follows: Parameter Description Partition ID The ID of the partition. Leader The IP address and port of the leader. Peers The IP addresses and ports of all the replicas. Losts The IP addresses and ports of faulty replicas. Click Detail in the upper right corner to view details. You can also enter the partition ID into the input box in the upper right corner of the details page to filter the shown data.","title":"Partition information"},{"location":"nebula-dashboard-ent/4.cluster-operator/3.cluster-information/#cluster_diagnostics","text":"You can click Cluster Diagnostics under the Cluster Information menu bar to locate and analyze the problem that occurs in the cluster. For details, see Cluster Diagnostics .","title":"Cluster Diagnostics"},{"location":"nebula-dashboard-ent/4.cluster-operator/4.manage/","text":"Cluster operation \u00b6 This topic introduces the cluster operation of Dashboard, including cluster node, cluster service, cluster scaling, service configuration, and member management. Node \u00b6 On this page, the information of all nodes will be shown, including the cluster name, Host(SSH_User), CPU (Core), etc. To add a node quickly, click Add Node and enter the following information, the Host, SSH port, SSH user, SSH password, and select a Nebula Graph package. Click the button to view the process name, service type, status, runtime directory of the corresponding node. Click Node Monitoring to jump to the detailed node monitoring page. For more information, see Cluster monitoring . Click Edit Node to modify the SSH port, SSH user, and SSH password. If a node has no service, you can Delete Node . Scale \u00b6 Enterpriseonly Only when the cluster you created or imported is the Enterprise Edition, this feature is available. On this page, you can add node and import node in batches quickly, and add Graph services and Storage services to the existing nodes. Click the Reset button to restore to the initial state. Caution Currently, you can dynamically scale Storaged and Graphd services through Dashboard. The Metad service cannot be scaled. When scaling a cluster, it is recommended to back up data in advance so that data can be rolled back when scaling fails. For more information, see FAQ . Make sure that services of the same type are not deployed on the same node, and at least one of each type of services is deployed in the cluster. In this example, storage services with nodes 192.168.8.143 and 192.168.8.167 are added, and Graph services with node 192.168.8.169 are deleted. If the box is dotted and the service name is greyed, it means the service is removed. If the box is solid, it means the service is newly added. In the Services section below, green indicates services that will be added soon, and red indicates services that will be removed. You can modify the port, HTTP port, and HTTP2 port of the newly added service. Service \u00b6 On this page, you can select the service type, service status, and host to filter the shown data, quickly select one or multiple services, and start/stop/restart the service with one click. Click the icon to quickly view the Service monitoring . Danger If you click Stop / Restart , the running task will be stopped instantly, which may cause data inconsistency. It is recommended to perform this operation during the low peak period of the business. Update Config \u00b6 On this page, you can modify configuration files of Storage and Graph services. For more information, see Storage service configuration and Graph service configuration . Updating configuration files is a batch operation, and each Storage/Graph configuration file will be modified. After clicking Save , the configuration will take effect after the next service restart. Click Save and Restart to directly restart the service to make the configuration take effect immediately. Danger If you click Save and Restart , the running task will be stopped and the cluster will be restarted instantly, which may cause data inconsistency. It is recommended to perform this operation during the low peak period of the business. Member Management \u00b6 Accounts with the role admin and cluster creators with the role user can add members to manage clusters. By default, the role of cluster creators is owner , and is displayed on the Member Management page. The role of added members is operator . For more information about accounts and roles, see Authority management . Add cluster members \u00b6 The accounts of cluster members must be included in Dashboard accounts. For information about how to create an account, see Authority management . You can follow the below steps to add cluster members. At the top navigation bar of the Dashboard Enterprise Edition page, click Cluster Management . On the right side of the target cluster, click Detail . On the left-side navigation bar of the page, click Operation -> Member Management . On the Member Management page, click the search box at the top left. In the drop-down list, select the target account that you want to add to be the administrator of the cluster, and then click Add . Other operations \u00b6 At the top right of the Member Management page, you can search for cluster members. Click to delete members.","title":"Cluster operations"},{"location":"nebula-dashboard-ent/4.cluster-operator/4.manage/#cluster_operation","text":"This topic introduces the cluster operation of Dashboard, including cluster node, cluster service, cluster scaling, service configuration, and member management.","title":"Cluster operation"},{"location":"nebula-dashboard-ent/4.cluster-operator/4.manage/#node","text":"On this page, the information of all nodes will be shown, including the cluster name, Host(SSH_User), CPU (Core), etc. To add a node quickly, click Add Node and enter the following information, the Host, SSH port, SSH user, SSH password, and select a Nebula Graph package. Click the button to view the process name, service type, status, runtime directory of the corresponding node. Click Node Monitoring to jump to the detailed node monitoring page. For more information, see Cluster monitoring . Click Edit Node to modify the SSH port, SSH user, and SSH password. If a node has no service, you can Delete Node .","title":"Node"},{"location":"nebula-dashboard-ent/4.cluster-operator/4.manage/#scale","text":"Enterpriseonly Only when the cluster you created or imported is the Enterprise Edition, this feature is available. On this page, you can add node and import node in batches quickly, and add Graph services and Storage services to the existing nodes. Click the Reset button to restore to the initial state. Caution Currently, you can dynamically scale Storaged and Graphd services through Dashboard. The Metad service cannot be scaled. When scaling a cluster, it is recommended to back up data in advance so that data can be rolled back when scaling fails. For more information, see FAQ . Make sure that services of the same type are not deployed on the same node, and at least one of each type of services is deployed in the cluster. In this example, storage services with nodes 192.168.8.143 and 192.168.8.167 are added, and Graph services with node 192.168.8.169 are deleted. If the box is dotted and the service name is greyed, it means the service is removed. If the box is solid, it means the service is newly added. In the Services section below, green indicates services that will be added soon, and red indicates services that will be removed. You can modify the port, HTTP port, and HTTP2 port of the newly added service.","title":"Scale"},{"location":"nebula-dashboard-ent/4.cluster-operator/4.manage/#service","text":"On this page, you can select the service type, service status, and host to filter the shown data, quickly select one or multiple services, and start/stop/restart the service with one click. Click the icon to quickly view the Service monitoring . Danger If you click Stop / Restart , the running task will be stopped instantly, which may cause data inconsistency. It is recommended to perform this operation during the low peak period of the business.","title":"Service"},{"location":"nebula-dashboard-ent/4.cluster-operator/4.manage/#update_config","text":"On this page, you can modify configuration files of Storage and Graph services. For more information, see Storage service configuration and Graph service configuration . Updating configuration files is a batch operation, and each Storage/Graph configuration file will be modified. After clicking Save , the configuration will take effect after the next service restart. Click Save and Restart to directly restart the service to make the configuration take effect immediately. Danger If you click Save and Restart , the running task will be stopped and the cluster will be restarted instantly, which may cause data inconsistency. It is recommended to perform this operation during the low peak period of the business.","title":"Update Config"},{"location":"nebula-dashboard-ent/4.cluster-operator/4.manage/#member_management","text":"Accounts with the role admin and cluster creators with the role user can add members to manage clusters. By default, the role of cluster creators is owner , and is displayed on the Member Management page. The role of added members is operator . For more information about accounts and roles, see Authority management .","title":"Member Management"},{"location":"nebula-dashboard-ent/4.cluster-operator/4.manage/#add_cluster_members","text":"The accounts of cluster members must be included in Dashboard accounts. For information about how to create an account, see Authority management . You can follow the below steps to add cluster members. At the top navigation bar of the Dashboard Enterprise Edition page, click Cluster Management . On the right side of the target cluster, click Detail . On the left-side navigation bar of the page, click Operation -> Member Management . On the Member Management page, click the search box at the top left. In the drop-down list, select the target account that you want to add to be the administrator of the cluster, and then click Add .","title":"Add cluster members"},{"location":"nebula-dashboard-ent/4.cluster-operator/4.manage/#other_operations","text":"At the top right of the Member Management page, you can search for cluster members. Click to delete members.","title":"Other operations"},{"location":"nebula-dashboard-ent/4.cluster-operator/5.operation-record/","text":"Operation record \u00b6 This topic shows how to use the operation record feature in Nebula Dashboard. At the top navigation bar of the Dashboard Enterprise Edition page, click Cluster Management , and on the left-side navigation bar, click Operation Record to enter the operation history page. On the Operation record page, you can check the operation records of the latest 1 hour, 6 hours, 1 day, 3 days, 7days, or 14 days. You can also view who runs what operation on which cluster at what time.","title":"Operation records"},{"location":"nebula-dashboard-ent/4.cluster-operator/5.operation-record/#operation_record","text":"This topic shows how to use the operation record feature in Nebula Dashboard. At the top navigation bar of the Dashboard Enterprise Edition page, click Cluster Management , and on the left-side navigation bar, click Operation Record to enter the operation history page. On the Operation record page, you can check the operation records of the latest 1 hour, 6 hours, 1 day, 3 days, 7days, or 14 days. You can also view who runs what operation on which cluster at what time.","title":"Operation record"},{"location":"nebula-dashboard-ent/4.cluster-operator/6.settings/","text":"Other settings \u00b6 The following shows other settings in Nebula Dashboard. At the top navigation bar of the Dashboard Enterprise Edition page, click Cluster Management , click Details on the right side of the target cluster, and on the left-side navigation bar, click Other Settings to enter the other settings page. Information: shows the cluster name, the creation time, the creator, and the owner of the current cluster. Unbind: Unbind a cluster and remove its information from the platform. The unbound cluster info will be removed and no operations will be done on cluster services or Nebula data. Note To unbind a cluster, enter the cluster name first. Delete: Delete a cluster and remove its information from the platform. Deleting the cluster will stop its service and unbind the cluster info, but retain its Nebula data. Be cautious when you delete a cluster. Note To delete a cluster, enter the cluster name first","title":"Other settings"},{"location":"nebula-dashboard-ent/4.cluster-operator/6.settings/#other_settings","text":"The following shows other settings in Nebula Dashboard. At the top navigation bar of the Dashboard Enterprise Edition page, click Cluster Management , click Details on the right side of the target cluster, and on the left-side navigation bar, click Other Settings to enter the other settings page. Information: shows the cluster name, the creation time, the creator, and the owner of the current cluster. Unbind: Unbind a cluster and remove its information from the platform. The unbound cluster info will be removed and no operations will be done on cluster services or Nebula data. Note To unbind a cluster, enter the cluster name first. Delete: Delete a cluster and remove its information from the platform. Deleting the cluster will stop its service and unbind the cluster info, but retain its Nebula data. Be cautious when you delete a cluster. Note To delete a cluster, enter the cluster name first","title":"Other settings"},{"location":"nebula-dashboard-ent/4.cluster-operator/7.cluster-diagnosis/","text":"Cluster Diagnositics \u00b6 The cluster diagnostics feature in Dashboard Enterprise Edition is to locate and analyze the current cluster problems within a specified time range and summarize the diagnostic results and cluster monitoring information to web-based diagnostic reports. Features \u00b6 Diagnostic reports allow you to troubleshoot the current cluster problems within a specified time range. Quickly understand the basic information of the nodes, services, service configurations, and query sessions in the cluster. Based on the diagnostic reports, you can make operation and maintenance recommendations and cluster alerts. Entry \u00b6 In the top navigation bar of the Dashboard Enterprise Edition page, click Cluster Management . On the right side of the target cluster, click Detail . In the left navigation bar, click Information -> Cluster Diagnostics . Create diagnostic reports \u00b6 Select a time range for diagnostics. You can customize the time range or set the range by selecting time intervals, including 1 Hour , 6 Hours , 12 Hours , 1 Day , 3 Days , 7 Days , and 14 Days . Caution Note that the end time of the diagnostic range you set cannot be longer than the current time. If the end time is longer than the current time, the end time will be set to the current time. On the Cluster Diagnostics page, click Start . Wait for the diagnostic report to be generated. When the diagnostic status is changed to success from generating , the diagnostic report is ready. View diagnostic reports \u00b6 In the diagnostic report list, you can view the diagnostic reports by clicking Detail on the right side of the target report. A diagnostic report contains the following information: Diagnosis Result Basic Info Load Info Network Session Service Info Configuration Info Diagnosis Result \u00b6 When the following parameters are abnormal, the corresponding information is displayed in the Diagnosis Result section, including the parameter name, type, severity, and details. Parameter Description num_queries_hit_memory_watermark The total number of nGQL statements that reach the memory high-water mark during execution. graphd_down Graph services stopped running. storaged_down Storage services stopped running. metad_down Meta services stopped running. node-exporter down The service used to collect data from the node stopped running. When no abnormality is diagnosed, no diagnostic information is displayed in the diagnostic result. Basic Info \u00b6 Report Time Range : Displays the time range of the diagnostic report. Node Info : Displays the basic information of the node, including the node IP, number of services, CPU, memory, and disk. Parameter Description HOST The IP address of the node. INSTANCE The number of Nebula Graph services deployed on this node. Such as: metad*1 graphd*1 storaged*1 . CPU The number of CPU cores. Unit: Core. MEMORY The memory size of the node. Unit: GB. DISK The disk size of the node. Unit: GB. Service Info : Displays the type, node IP, HTTP port, and operational status of each Nebula Graph service. Leader Distribution : Displays the distribution of Leaders in Storage services. Parameter Description Storage Service Displays the access addresses for Storage services. Number of Leaders Displays the number of Leaders in the corresponding Storage service. Leader Distribution Displays the number of Leader distributions for different space graphs in the corresponding Storage service. Load Info \u00b6 Displays the load information of the node, including the average value (AVG), maximum value (MAX), minimum value (MIN) of the following metrics of the node within the time range: Memory Utilization : Displays the node memory usage in %. CPU Utilization : Displays the node CPU usage in %. Disk Utilization : Displays the total disk utilization of the node and the utilization of each disk in the node in %. Network \u00b6 Displays the network traffic information of all nodes in the cluster, including the average (AVG), maximum (MAX), and minimum (MIN) values of the following metrics: NetworkOut : Displays the magnitude of network outflow speed for each node in the cluster, and the magnitude of outflow speed for each NIC in each node. Unit: Bytes/s. NetworkIn : Shows the magnitude of network inflow speed for each server node in the cluster and the magnitude of inflow speed for each NIC in each node. Unit: Bytes/s. Session \u00b6 Displays the session-related information for all Graph services in the cluster. Parameter Description num_opened_sessions The number of sessions connected to the server. num_auth_failed_sessions The number of sessions in which login authentication failed. num_active_sessions The number of currently active sessions. num_reclaimed_expired_sessions The number of expired sessions actively reclaimed by the server. Service Info \u00b6 Displays metrics related to the stability of each service in the cluster. Graph : Parameter Description METRIC_NAME query : The number of all queries. slow_queries : The number of slow queries. num_killed_queries : The number of killed queries. num_queries_hit_memory_watermark : The total number of nGQL statements that reach the memory high-water mark during execution. num_rpc_sent_to_metad : The number of RPC requests that the Graphd service sent to the Metad service. Meta : Parameter Description METRIC_NAME heartbeat : The number of heartbeats. Storage : Parameter Description METRIC_NAME delete_vertices : The number of deleted vertices. delete_edges : The number of deleted edges. delete_tags : The number of deleted tags. num_rpc_sent_to_metad : The number of RPC requests that the Storaged service sent to the Metad service. The descriptions of other parameters are as follows: Parameter Description TOTAL The total number of times this monitoring metric is executed. ERROR The number of errors that occurred. P75 The 75th percentile latency. P95 The 95th percentile latency. P99 The 99th percentile latency. P999 The 99.9th percentile latency. Configuration Info \u00b6 Lists all configuration information for Graph, Meta, and Storage services in the current cluster. For information about the configurations of each service in Nebula Graph, see Configurations .","title":"Cluster Diagnostics"},{"location":"nebula-dashboard-ent/4.cluster-operator/7.cluster-diagnosis/#cluster_diagnositics","text":"The cluster diagnostics feature in Dashboard Enterprise Edition is to locate and analyze the current cluster problems within a specified time range and summarize the diagnostic results and cluster monitoring information to web-based diagnostic reports.","title":"Cluster Diagnositics"},{"location":"nebula-dashboard-ent/4.cluster-operator/7.cluster-diagnosis/#features","text":"Diagnostic reports allow you to troubleshoot the current cluster problems within a specified time range. Quickly understand the basic information of the nodes, services, service configurations, and query sessions in the cluster. Based on the diagnostic reports, you can make operation and maintenance recommendations and cluster alerts.","title":"Features"},{"location":"nebula-dashboard-ent/4.cluster-operator/7.cluster-diagnosis/#entry","text":"In the top navigation bar of the Dashboard Enterprise Edition page, click Cluster Management . On the right side of the target cluster, click Detail . In the left navigation bar, click Information -> Cluster Diagnostics .","title":"Entry"},{"location":"nebula-dashboard-ent/4.cluster-operator/7.cluster-diagnosis/#create_diagnostic_reports","text":"Select a time range for diagnostics. You can customize the time range or set the range by selecting time intervals, including 1 Hour , 6 Hours , 12 Hours , 1 Day , 3 Days , 7 Days , and 14 Days . Caution Note that the end time of the diagnostic range you set cannot be longer than the current time. If the end time is longer than the current time, the end time will be set to the current time. On the Cluster Diagnostics page, click Start . Wait for the diagnostic report to be generated. When the diagnostic status is changed to success from generating , the diagnostic report is ready.","title":"Create diagnostic reports"},{"location":"nebula-dashboard-ent/4.cluster-operator/7.cluster-diagnosis/#view_diagnostic_reports","text":"In the diagnostic report list, you can view the diagnostic reports by clicking Detail on the right side of the target report. A diagnostic report contains the following information: Diagnosis Result Basic Info Load Info Network Session Service Info Configuration Info","title":"View diagnostic reports"},{"location":"nebula-dashboard-ent/4.cluster-operator/7.cluster-diagnosis/#diagnosis_result","text":"When the following parameters are abnormal, the corresponding information is displayed in the Diagnosis Result section, including the parameter name, type, severity, and details. Parameter Description num_queries_hit_memory_watermark The total number of nGQL statements that reach the memory high-water mark during execution. graphd_down Graph services stopped running. storaged_down Storage services stopped running. metad_down Meta services stopped running. node-exporter down The service used to collect data from the node stopped running. When no abnormality is diagnosed, no diagnostic information is displayed in the diagnostic result.","title":"Diagnosis Result"},{"location":"nebula-dashboard-ent/4.cluster-operator/7.cluster-diagnosis/#basic_info","text":"Report Time Range : Displays the time range of the diagnostic report. Node Info : Displays the basic information of the node, including the node IP, number of services, CPU, memory, and disk. Parameter Description HOST The IP address of the node. INSTANCE The number of Nebula Graph services deployed on this node. Such as: metad*1 graphd*1 storaged*1 . CPU The number of CPU cores. Unit: Core. MEMORY The memory size of the node. Unit: GB. DISK The disk size of the node. Unit: GB. Service Info : Displays the type, node IP, HTTP port, and operational status of each Nebula Graph service. Leader Distribution : Displays the distribution of Leaders in Storage services. Parameter Description Storage Service Displays the access addresses for Storage services. Number of Leaders Displays the number of Leaders in the corresponding Storage service. Leader Distribution Displays the number of Leader distributions for different space graphs in the corresponding Storage service.","title":"Basic Info"},{"location":"nebula-dashboard-ent/4.cluster-operator/7.cluster-diagnosis/#load_info","text":"Displays the load information of the node, including the average value (AVG), maximum value (MAX), minimum value (MIN) of the following metrics of the node within the time range: Memory Utilization : Displays the node memory usage in %. CPU Utilization : Displays the node CPU usage in %. Disk Utilization : Displays the total disk utilization of the node and the utilization of each disk in the node in %.","title":"Load Info"},{"location":"nebula-dashboard-ent/4.cluster-operator/7.cluster-diagnosis/#network","text":"Displays the network traffic information of all nodes in the cluster, including the average (AVG), maximum (MAX), and minimum (MIN) values of the following metrics: NetworkOut : Displays the magnitude of network outflow speed for each node in the cluster, and the magnitude of outflow speed for each NIC in each node. Unit: Bytes/s. NetworkIn : Shows the magnitude of network inflow speed for each server node in the cluster and the magnitude of inflow speed for each NIC in each node. Unit: Bytes/s.","title":"Network"},{"location":"nebula-dashboard-ent/4.cluster-operator/7.cluster-diagnosis/#session","text":"Displays the session-related information for all Graph services in the cluster. Parameter Description num_opened_sessions The number of sessions connected to the server. num_auth_failed_sessions The number of sessions in which login authentication failed. num_active_sessions The number of currently active sessions. num_reclaimed_expired_sessions The number of expired sessions actively reclaimed by the server.","title":"Session"},{"location":"nebula-dashboard-ent/4.cluster-operator/7.cluster-diagnosis/#service_info","text":"Displays metrics related to the stability of each service in the cluster. Graph : Parameter Description METRIC_NAME query : The number of all queries. slow_queries : The number of slow queries. num_killed_queries : The number of killed queries. num_queries_hit_memory_watermark : The total number of nGQL statements that reach the memory high-water mark during execution. num_rpc_sent_to_metad : The number of RPC requests that the Graphd service sent to the Metad service. Meta : Parameter Description METRIC_NAME heartbeat : The number of heartbeats. Storage : Parameter Description METRIC_NAME delete_vertices : The number of deleted vertices. delete_edges : The number of deleted edges. delete_tags : The number of deleted tags. num_rpc_sent_to_metad : The number of RPC requests that the Storaged service sent to the Metad service. The descriptions of other parameters are as follows: Parameter Description TOTAL The total number of times this monitoring metric is executed. ERROR The number of errors that occurred. P75 The 75th percentile latency. P95 The 95th percentile latency. P99 The 99th percentile latency. P999 The 99.9th percentile latency.","title":"Service Info"},{"location":"nebula-dashboard-ent/4.cluster-operator/7.cluster-diagnosis/#configuration_info","text":"Lists all configuration information for Graph, Meta, and Storage services in the current cluster. For information about the configurations of each service in Nebula Graph, see Configurations .","title":"Configuration Info"},{"location":"nebula-exchange/ex-ug-FAQ/","text":"Exchange FAQ \u00b6 Compilation \u00b6 Q: Some packages not in central repository failed to download, error: Could not resolve dependencies for project xxx \u00b6 Please check the mirror part of Maven installation directory libexec/conf/settings.xml : <mirror> <id>alimaven</id> <mirrorOf>central</mirrorOf> <name>aliyun maven</name> <url>http://maven.aliyun.com/nexus/content/repositories/central/</url> </mirror> Check whether the value of mirrorOf is configured to * . If it is, change it to central or *,!SparkPackagesRepo,!bintray-streamnative-maven . Reason : There are two dependency packages in Exchange's pom.xml that are not in Maven's central repository. pom.xml configures the repository address for these two dependencies. If the mirrorOf value for the mirror address configured in Maven is * , all dependencies will be downloaded from the Central repository, causing the download to fail. Q: Unable to download SNAPSHOT packages when compiling Exchange \u00b6 Problem description: The system reports Could not find artifact com.vesoft:client:jar:xxx-SNAPSHOT when compiling. Cause: There is no local Maven repository for storing or downloading SNAPSHOT packages. The default central repository in Maven only stores official releases, not development versions (SNAPSHOT). Solution: Add the following configuration in the profiles scope of Maven's setting.xml file: <profile> <activation> <activeByDefault> true </activeByDefault> </activation> <repositories> <repository> <id> snapshots </id> <url> https://oss.sonatype.org/content/repositories/snapshots/ </url> <snapshots> <enabled> true </enabled> </snapshots> </repository> </repositories> </profile> Execution \u00b6 Q: How to submit in Yarn-Cluster mode? \u00b6 To submit a task in Yarn-Cluster mode, run the following command: $SPARK_HOME /bin/spark-submit --class com.vesoft.nebula.exchange.Exchange \\ --master yarn-cluster \\ --files application.conf \\ --conf spark.driver.extraClassPath = ./ \\ --conf spark.executor.extraClassPath = ./ \\ nebula-exchange-3.0.0.jar \\ -c application.conf Q: Error: method name xxx not found \u00b6 Generally, the port configuration is incorrect. Check the port configuration of the Meta service, Graph service, and Storage service. Q: Error: NoSuchMethod, MethodNotFound ( Exception in thread \"main\" java.lang.NoSuchMethodError , etc) \u00b6 Most errors are caused by JAR package conflicts or version conflicts. Check whether the version of the error reporting service is the same as that used in Exchange, especially Spark, Scala, and Hive. Q: When Exchange imports Hive data, error: Exception in thread \"main\" org.apache.spark.sql.AnalysisException: Table or view not found \u00b6 Check whether the -h parameter is omitted in the command for submitting the Exchange task and whether the table and database are correct, and run the user-configured exec statement in spark-SQL to verify the correctness of the exec statement. Q: Run error: com.facebook.thrift.protocol.TProtocolException: Expected protocol id xxx \u00b6 Check that the Nebula Graph service port is configured correctly. For source, RPM, or DEB installations, configure the port number corresponding to --port in the configuration file for each service. For docker installation, configure the docker mapped port number as follows: Execute docker-compose ps in the nebula-docker-compose directory, for example: $ docker-compose ps Name Command State Ports --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- nebula-docker-compose_graphd_1 /usr/local/nebula/bin/nebu ... Up ( healthy ) 0 .0.0.0:33205->19669/tcp, 0 .0.0.0:33204->19670/tcp, 0 .0.0.0:9669->9669/tcp nebula-docker-compose_metad0_1 ./bin/nebula-metad --flagf ... Up ( healthy ) 0 .0.0.0:33165->19559/tcp, 0 .0.0.0:33162->19560/tcp, 0 .0.0.0:33167->9559/tcp, 9560 /tcp nebula-docker-compose_metad1_1 ./bin/nebula-metad --flagf ... Up ( healthy ) 0 .0.0.0:33166->19559/tcp, 0 .0.0.0:33163->19560/tcp, 0 .0.0.0:33168->9559/tcp, 9560 /tcp nebula-docker-compose_metad2_1 ./bin/nebula-metad --flagf ... Up ( healthy ) 0 .0.0.0:33161->19559/tcp, 0 .0.0.0:33160->19560/tcp, 0 .0.0.0:33164->9559/tcp, 9560 /tcp nebula-docker-compose_storaged0_1 ./bin/nebula-storaged --fl ... Up ( healthy ) 0 .0.0.0:33180->19779/tcp, 0 .0.0.0:33178->19780/tcp, 9777 /tcp, 9778 /tcp, 0 .0.0.0:33183->9779/tcp, 9780 /tcp nebula-docker-compose_storaged1_1 ./bin/nebula-storaged --fl ... Up ( healthy ) 0 .0.0.0:33175->19779/tcp, 0 .0.0.0:33172->19780/tcp, 9777 /tcp, 9778 /tcp, 0 .0.0.0:33177->9779/tcp, 9780 /tcp nebula-docker-compose_storaged2_1 ./bin/nebula-storaged --fl ... Up ( healthy ) 0 .0.0.0:33184->19779/tcp, 0 .0.0.0:33181->19780/tcp, 9777 /tcp, 9778 /tcp, 0 .0.0.0:33185->9779/tcp, 9780 /tcp Check the Ports column to find the docker mapped port number, for example: - The port number available for Graph service is 9669. - The port number for Meta service are 33167, 33168, 33164. - The port number for Storage service are 33183, 33177, 33185. Q: Error: Exception in thread \"main\" com.facebook.thrift.protocol.TProtocolException: The field 'code' has been assigned the invalid value -4 \u00b6 Check whether the version of Exchange is the same as that of Nebula Graph. For more information, see Limitations . Q: How to correct the messy code when importing Hive data into Nebula Graph? \u00b6 It may happen if the property value of the data in Hive contains Chinese characters. The solution is to add the following options before the JAR package path in the import command: --conf spark.driver.extraJavaOptions = -Dfile.encoding = utf-8 --conf spark.executor.extraJavaOptions = -Dfile.encoding = utf-8 Namely: <spark_install_path>/bin/spark-submit --master \"local\" \\ --conf spark.driver.extraJavaOptions = -Dfile.encoding = utf-8 \\ --conf spark.executor.extraJavaOptions = -Dfile.encoding = utf-8 \\ --class com.vesoft.nebula.exchange.Exchange \\ <nebula-exchange-3.x.y.jar_path> -c <application.conf_path> In YARN, use the following command: <spark_install_path>/bin/spark-submit \\ --class com.vesoft.nebula.exchange.Exchange \\ --master yarn-cluster \\ --files <application.conf_path> \\ --conf spark.driver.extraClassPath = ./ \\ --conf spark.executor.extraClassPath = ./ \\ --conf spark.driver.extraJavaOptions = -Dfile.encoding = utf-8 \\ --conf spark.executor.extraJavaOptions = -Dfile.encoding = utf-8 \\ <nebula-exchange-3.x.y.jar_path> \\ -c application.conf Q: org.rocksdb.RocksDBException: While open a file for appending: /path/sst/1-xxx.sst: No such file or directory \u00b6 Solution: Check if /path exists. If not, or if the path is set incorrectly, create or correct it. Check if Spark's current user on each machine has the operation permission on /path . If not, grant the permission. Configuration \u00b6 Q: Which configuration fields will affect import performance? \u00b6 batch: The number of data contained in each nGQL statement sent to the Nebula Graph service. partition: The number of Spark data partitions, indicating the number of concurrent data imports. nebula.rate: Get a token from the token bucket before sending a request to Nebula Graph. - limit: Represents the size of the token bucket. - timeout: Represents the timeout period for obtaining the token. The values of these four parameters can be adjusted appropriately according to the machine performance. If the leader of the Storage service changes during the import process, you can adjust the values of these four parameters to reduce the import speed. Others \u00b6 Q: Which versions of Nebula Graph are supported by Exchange? \u00b6 See Limitations . Q: What is the relationship between Exchange and Spark Writer? \u00b6 Exchange is the Spark application developed based on Spark Writer. Both are suitable for bulk migration of cluster data to Nebula Graph in a distributed environment, but later maintenance work will be focused on Exchange. Compared with Spark Writer, Exchange has the following improvements: It supports more abundant data sources, such as MySQL, Neo4j, Hive, HBase, Kafka, Pulsar, etc. It fixed some problems of Spark Writer. For example, when Spark reads data from HDFS, the default source data is String, which may be different from the Nebula Graph's Schema. So Exchange adds automatic data type matching and type conversion. When the data type in the Nebula Graph's Schema is non-String (e.g. double), Exchange converts the source data of String type to the corresponding type.","title":"Exchange FAQ"},{"location":"nebula-exchange/ex-ug-FAQ/#exchange_faq","text":"","title":"Exchange FAQ"},{"location":"nebula-exchange/ex-ug-FAQ/#compilation","text":"","title":"Compilation"},{"location":"nebula-exchange/ex-ug-FAQ/#q_some_packages_not_in_central_repository_failed_to_download_error_could_not_resolve_dependencies_for_project_xxx","text":"Please check the mirror part of Maven installation directory libexec/conf/settings.xml : <mirror> <id>alimaven</id> <mirrorOf>central</mirrorOf> <name>aliyun maven</name> <url>http://maven.aliyun.com/nexus/content/repositories/central/</url> </mirror> Check whether the value of mirrorOf is configured to * . If it is, change it to central or *,!SparkPackagesRepo,!bintray-streamnative-maven . Reason : There are two dependency packages in Exchange's pom.xml that are not in Maven's central repository. pom.xml configures the repository address for these two dependencies. If the mirrorOf value for the mirror address configured in Maven is * , all dependencies will be downloaded from the Central repository, causing the download to fail.","title":"Q: Some packages not in central repository failed to download, error: Could not resolve dependencies for project xxx"},{"location":"nebula-exchange/ex-ug-FAQ/#q_unable_to_download_snapshot_packages_when_compiling_exchange","text":"Problem description: The system reports Could not find artifact com.vesoft:client:jar:xxx-SNAPSHOT when compiling. Cause: There is no local Maven repository for storing or downloading SNAPSHOT packages. The default central repository in Maven only stores official releases, not development versions (SNAPSHOT). Solution: Add the following configuration in the profiles scope of Maven's setting.xml file: <profile> <activation> <activeByDefault> true </activeByDefault> </activation> <repositories> <repository> <id> snapshots </id> <url> https://oss.sonatype.org/content/repositories/snapshots/ </url> <snapshots> <enabled> true </enabled> </snapshots> </repository> </repositories> </profile>","title":"Q: Unable to download SNAPSHOT packages when compiling Exchange"},{"location":"nebula-exchange/ex-ug-FAQ/#execution","text":"","title":"Execution"},{"location":"nebula-exchange/ex-ug-FAQ/#q_how_to_submit_in_yarn-cluster_mode","text":"To submit a task in Yarn-Cluster mode, run the following command: $SPARK_HOME /bin/spark-submit --class com.vesoft.nebula.exchange.Exchange \\ --master yarn-cluster \\ --files application.conf \\ --conf spark.driver.extraClassPath = ./ \\ --conf spark.executor.extraClassPath = ./ \\ nebula-exchange-3.0.0.jar \\ -c application.conf","title":"Q: How to submit in Yarn-Cluster mode?"},{"location":"nebula-exchange/ex-ug-FAQ/#q_error_method_name_xxx_not_found","text":"Generally, the port configuration is incorrect. Check the port configuration of the Meta service, Graph service, and Storage service.","title":"Q: Error: method name xxx not found"},{"location":"nebula-exchange/ex-ug-FAQ/#q_error_nosuchmethod_methodnotfound_exception_in_thread_main_javalangnosuchmethoderror_etc","text":"Most errors are caused by JAR package conflicts or version conflicts. Check whether the version of the error reporting service is the same as that used in Exchange, especially Spark, Scala, and Hive.","title":"Q: Error: NoSuchMethod, MethodNotFound (Exception in thread \"main\" java.lang.NoSuchMethodError, etc)"},{"location":"nebula-exchange/ex-ug-FAQ/#q_when_exchange_imports_hive_data_error_exception_in_thread_main_orgapachesparksqlanalysisexception_table_or_view_not_found","text":"Check whether the -h parameter is omitted in the command for submitting the Exchange task and whether the table and database are correct, and run the user-configured exec statement in spark-SQL to verify the correctness of the exec statement.","title":"Q: When Exchange imports Hive data, error: Exception in thread \"main\" org.apache.spark.sql.AnalysisException: Table or view not found"},{"location":"nebula-exchange/ex-ug-FAQ/#q_run_error_comfacebookthriftprotocoltprotocolexception_expected_protocol_id_xxx","text":"Check that the Nebula Graph service port is configured correctly. For source, RPM, or DEB installations, configure the port number corresponding to --port in the configuration file for each service. For docker installation, configure the docker mapped port number as follows: Execute docker-compose ps in the nebula-docker-compose directory, for example: $ docker-compose ps Name Command State Ports --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- nebula-docker-compose_graphd_1 /usr/local/nebula/bin/nebu ... Up ( healthy ) 0 .0.0.0:33205->19669/tcp, 0 .0.0.0:33204->19670/tcp, 0 .0.0.0:9669->9669/tcp nebula-docker-compose_metad0_1 ./bin/nebula-metad --flagf ... Up ( healthy ) 0 .0.0.0:33165->19559/tcp, 0 .0.0.0:33162->19560/tcp, 0 .0.0.0:33167->9559/tcp, 9560 /tcp nebula-docker-compose_metad1_1 ./bin/nebula-metad --flagf ... Up ( healthy ) 0 .0.0.0:33166->19559/tcp, 0 .0.0.0:33163->19560/tcp, 0 .0.0.0:33168->9559/tcp, 9560 /tcp nebula-docker-compose_metad2_1 ./bin/nebula-metad --flagf ... Up ( healthy ) 0 .0.0.0:33161->19559/tcp, 0 .0.0.0:33160->19560/tcp, 0 .0.0.0:33164->9559/tcp, 9560 /tcp nebula-docker-compose_storaged0_1 ./bin/nebula-storaged --fl ... Up ( healthy ) 0 .0.0.0:33180->19779/tcp, 0 .0.0.0:33178->19780/tcp, 9777 /tcp, 9778 /tcp, 0 .0.0.0:33183->9779/tcp, 9780 /tcp nebula-docker-compose_storaged1_1 ./bin/nebula-storaged --fl ... Up ( healthy ) 0 .0.0.0:33175->19779/tcp, 0 .0.0.0:33172->19780/tcp, 9777 /tcp, 9778 /tcp, 0 .0.0.0:33177->9779/tcp, 9780 /tcp nebula-docker-compose_storaged2_1 ./bin/nebula-storaged --fl ... Up ( healthy ) 0 .0.0.0:33184->19779/tcp, 0 .0.0.0:33181->19780/tcp, 9777 /tcp, 9778 /tcp, 0 .0.0.0:33185->9779/tcp, 9780 /tcp Check the Ports column to find the docker mapped port number, for example: - The port number available for Graph service is 9669. - The port number for Meta service are 33167, 33168, 33164. - The port number for Storage service are 33183, 33177, 33185.","title":"Q: Run error: com.facebook.thrift.protocol.TProtocolException: Expected protocol id xxx"},{"location":"nebula-exchange/ex-ug-FAQ/#q_error_exception_in_thread_main_comfacebookthriftprotocoltprotocolexception_the_field_code_has_been_assigned_the_invalid_value_-4","text":"Check whether the version of Exchange is the same as that of Nebula Graph. For more information, see Limitations .","title":"Q: Error: Exception in thread \"main\" com.facebook.thrift.protocol.TProtocolException: The field 'code' has been assigned the invalid value -4"},{"location":"nebula-exchange/ex-ug-FAQ/#q_how_to_correct_the_messy_code_when_importing_hive_data_into_nebula_graph","text":"It may happen if the property value of the data in Hive contains Chinese characters. The solution is to add the following options before the JAR package path in the import command: --conf spark.driver.extraJavaOptions = -Dfile.encoding = utf-8 --conf spark.executor.extraJavaOptions = -Dfile.encoding = utf-8 Namely: <spark_install_path>/bin/spark-submit --master \"local\" \\ --conf spark.driver.extraJavaOptions = -Dfile.encoding = utf-8 \\ --conf spark.executor.extraJavaOptions = -Dfile.encoding = utf-8 \\ --class com.vesoft.nebula.exchange.Exchange \\ <nebula-exchange-3.x.y.jar_path> -c <application.conf_path> In YARN, use the following command: <spark_install_path>/bin/spark-submit \\ --class com.vesoft.nebula.exchange.Exchange \\ --master yarn-cluster \\ --files <application.conf_path> \\ --conf spark.driver.extraClassPath = ./ \\ --conf spark.executor.extraClassPath = ./ \\ --conf spark.driver.extraJavaOptions = -Dfile.encoding = utf-8 \\ --conf spark.executor.extraJavaOptions = -Dfile.encoding = utf-8 \\ <nebula-exchange-3.x.y.jar_path> \\ -c application.conf","title":"Q: How to correct the messy code when importing Hive data into Nebula Graph?"},{"location":"nebula-exchange/ex-ug-FAQ/#q_orgrocksdbrocksdbexception_while_open_a_file_for_appending_pathsst1-xxxsst_no_such_file_or_directory","text":"Solution: Check if /path exists. If not, or if the path is set incorrectly, create or correct it. Check if Spark's current user on each machine has the operation permission on /path . If not, grant the permission.","title":"Q: org.rocksdb.RocksDBException: While open a file for appending: /path/sst/1-xxx.sst: No such file or directory"},{"location":"nebula-exchange/ex-ug-FAQ/#configuration","text":"","title":"Configuration"},{"location":"nebula-exchange/ex-ug-FAQ/#q_which_configuration_fields_will_affect_import_performance","text":"batch: The number of data contained in each nGQL statement sent to the Nebula Graph service. partition: The number of Spark data partitions, indicating the number of concurrent data imports. nebula.rate: Get a token from the token bucket before sending a request to Nebula Graph. - limit: Represents the size of the token bucket. - timeout: Represents the timeout period for obtaining the token. The values of these four parameters can be adjusted appropriately according to the machine performance. If the leader of the Storage service changes during the import process, you can adjust the values of these four parameters to reduce the import speed.","title":"Q: Which configuration fields will affect import performance?"},{"location":"nebula-exchange/ex-ug-FAQ/#others","text":"","title":"Others"},{"location":"nebula-exchange/ex-ug-FAQ/#q_which_versions_of_nebula_graph_are_supported_by_exchange","text":"See Limitations .","title":"Q: Which versions of Nebula Graph are supported by Exchange?"},{"location":"nebula-exchange/ex-ug-FAQ/#q_what_is_the_relationship_between_exchange_and_spark_writer","text":"Exchange is the Spark application developed based on Spark Writer. Both are suitable for bulk migration of cluster data to Nebula Graph in a distributed environment, but later maintenance work will be focused on Exchange. Compared with Spark Writer, Exchange has the following improvements: It supports more abundant data sources, such as MySQL, Neo4j, Hive, HBase, Kafka, Pulsar, etc. It fixed some problems of Spark Writer. For example, when Spark reads data from HDFS, the default source data is String, which may be different from the Nebula Graph's Schema. So Exchange adds automatic data type matching and type conversion. When the data type in the Nebula Graph's Schema is non-String (e.g. double), Exchange converts the source data of String type to the corresponding type.","title":"Q: What is the relationship between Exchange and Spark Writer?"},{"location":"nebula-exchange/ex-ug-compile/","text":"Get Exchange \u00b6 This topic introduces how to get the JAR file of Nebula Exchange. Download the JAR file directly \u00b6 The JAR file of Exchange Community Edition can be downloaded directly. To download Exchange Enterprise Edition, get Nebula Graph Enterprise Edition Package first. Get the JAR file by compiling the source code \u00b6 You can get the JAR file of Exchange Community Edition by compiling the source code. The following introduces how to compile the source code of Exchange. Enterpriseonly You can get Exchange Enterprise Edition in Nebula Graph Enterprise Edition Package only. Prerequisites \u00b6 Install Maven . Install the correct version of Apache Spark. Exporting data from different sources requires different Spark versions. For more information, see Software dependencies . Steps \u00b6 Clone the repository nebula-exchange in the / directory. git clone -b v3.0.0 https://github.com/vesoft-inc/nebula-exchange.git Switch to the directory nebula-exchange . cd nebula-exchange Package Nebula Exchange. Run the following command based on the Spark version: For Spark 2.2\uff1a mvn clean package -Dmaven.test.skip = true -Dgpg.skip -Dmaven.javadoc.skip = true \\ -pl nebula-exchange_spark_2.2 -am -Pscala-2.11 -Pspark-2.2 For Spark 2.4\uff1a mvn clean package -Dmaven.test.skip = true -Dgpg.skip -Dmaven.javadoc.skip = true \\ -pl nebula-exchange_spark_2.4 -am -Pscala-2.11 -Pspark-2.4 For Spark 3.0\uff1a mvn clean package -Dmaven.test.skip = true -Dgpg.skip -Dmaven.javadoc.skip = true \\ -pl nebula-exchange_spark_3.0 -am -Pscala-2.12 -Pspark-3.0 After the compilation is successful, you can find the nebula-exchange_spark_x.x-v3.0.0.jar file in the nebula-exchange_spark_x.x/target/ directory. x.x indicates the Spark version, for example, 2.4 . Note The JAR file version changes with the release of the Nebula Java Client. Users can view the latest version on the Releases page . When migrating data, you can refer to configuration file target/classes/application.conf . Failed to download the dependency package \u00b6 If downloading dependencies fails when compiling: Check the network settings and ensure that the network is normal. Modify the mirror part of Maven installation directory libexec/conf/settings.xml : <mirror> <id>alimaven</id> <mirrorOf>central</mirrorOf> <name>aliyun maven</name> <url>http://maven.aliyun.com/nexus/content/repositories/central/</url> </mirror>","title":"Get Exchange"},{"location":"nebula-exchange/ex-ug-compile/#get_exchange","text":"This topic introduces how to get the JAR file of Nebula Exchange.","title":"Get Exchange"},{"location":"nebula-exchange/ex-ug-compile/#download_the_jar_file_directly","text":"The JAR file of Exchange Community Edition can be downloaded directly. To download Exchange Enterprise Edition, get Nebula Graph Enterprise Edition Package first.","title":"Download the JAR file directly"},{"location":"nebula-exchange/ex-ug-compile/#get_the_jar_file_by_compiling_the_source_code","text":"You can get the JAR file of Exchange Community Edition by compiling the source code. The following introduces how to compile the source code of Exchange. Enterpriseonly You can get Exchange Enterprise Edition in Nebula Graph Enterprise Edition Package only.","title":"Get the JAR file by compiling the source code"},{"location":"nebula-exchange/ex-ug-compile/#prerequisites","text":"Install Maven . Install the correct version of Apache Spark. Exporting data from different sources requires different Spark versions. For more information, see Software dependencies .","title":"Prerequisites"},{"location":"nebula-exchange/ex-ug-compile/#steps","text":"Clone the repository nebula-exchange in the / directory. git clone -b v3.0.0 https://github.com/vesoft-inc/nebula-exchange.git Switch to the directory nebula-exchange . cd nebula-exchange Package Nebula Exchange. Run the following command based on the Spark version: For Spark 2.2\uff1a mvn clean package -Dmaven.test.skip = true -Dgpg.skip -Dmaven.javadoc.skip = true \\ -pl nebula-exchange_spark_2.2 -am -Pscala-2.11 -Pspark-2.2 For Spark 2.4\uff1a mvn clean package -Dmaven.test.skip = true -Dgpg.skip -Dmaven.javadoc.skip = true \\ -pl nebula-exchange_spark_2.4 -am -Pscala-2.11 -Pspark-2.4 For Spark 3.0\uff1a mvn clean package -Dmaven.test.skip = true -Dgpg.skip -Dmaven.javadoc.skip = true \\ -pl nebula-exchange_spark_3.0 -am -Pscala-2.12 -Pspark-3.0 After the compilation is successful, you can find the nebula-exchange_spark_x.x-v3.0.0.jar file in the nebula-exchange_spark_x.x/target/ directory. x.x indicates the Spark version, for example, 2.4 . Note The JAR file version changes with the release of the Nebula Java Client. Users can view the latest version on the Releases page . When migrating data, you can refer to configuration file target/classes/application.conf .","title":"Steps"},{"location":"nebula-exchange/ex-ug-compile/#failed_to_download_the_dependency_package","text":"If downloading dependencies fails when compiling: Check the network settings and ensure that the network is normal. Modify the mirror part of Maven installation directory libexec/conf/settings.xml : <mirror> <id>alimaven</id> <mirrorOf>central</mirrorOf> <name>aliyun maven</name> <url>http://maven.aliyun.com/nexus/content/repositories/central/</url> </mirror>","title":"Failed to download the dependency package"},{"location":"nebula-exchange/about-exchange/ex-ug-limitations/","text":"Limitations \u00b6 This topic describes some of the limitations of using Exchange 3.x. Version compatibility \u00b6 The correspondence between the Nebula Exchange release (the JAR version) and the Nebula Graph core release is as follows. Exchange client Nebula Graph 3.0-SNAPSHOT nightly 3.0.0 3.1.0 2.6.x 2.6.x 2.5.x 2.5.x 2.1.0 2.0.0, 2.0.1 2.0.1 2.0.0, 2.0.1 2.0.0 2.0.0, 2.0.1 JAR packages are available in two ways: compile them yourself or download them from the Maven repository. If you are using Nebula Graph 1.x, use Nebula Exchange 1.x . Environment \u00b6 Exchange 3.x supports the following operating systems: CentOS 7 macOS Software dependencies \u00b6 To ensure the healthy operation of Exchange, ensure that the following software has been installed on the machine: Java version 1.8 Scala version 2.10.7, 2.11.12, or 2.12.10 Apache Spark. The requirements for Spark versions when using Exchange to export data from data sources are as follows. In the following table, Y means that the corresponding Spark version is supported, and N means not supported. Note Use the correct Exchange JAR file based on the Spark version. For example, for Spark version 2.4, use nebula-exchange_spark_2.4-3.0.0.jar. Data source Spark 2.2 Spark 2.4 Spark 3 CSV file Y N Y JSON file Y Y Y ORC file Y Y Y Parquet file Y Y Y HBase Y Y Y MySQL Y Y Y PostgreSQL Y Y Y ClickHouse Y Y Y Neo4j N Y N Hive Y Y Y MaxCompute N Y N Pulsar N Y Untested Kafka N Y Untested Nebula Graph N Y N Hadoop Distributed File System (HDFS) needs to be deployed in the following scenarios: Migrate HDFS data Generate SST files","title":"Limitations"},{"location":"nebula-exchange/about-exchange/ex-ug-limitations/#limitations","text":"This topic describes some of the limitations of using Exchange 3.x.","title":"Limitations"},{"location":"nebula-exchange/about-exchange/ex-ug-limitations/#version_compatibility","text":"The correspondence between the Nebula Exchange release (the JAR version) and the Nebula Graph core release is as follows. Exchange client Nebula Graph 3.0-SNAPSHOT nightly 3.0.0 3.1.0 2.6.x 2.6.x 2.5.x 2.5.x 2.1.0 2.0.0, 2.0.1 2.0.1 2.0.0, 2.0.1 2.0.0 2.0.0, 2.0.1 JAR packages are available in two ways: compile them yourself or download them from the Maven repository. If you are using Nebula Graph 1.x, use Nebula Exchange 1.x .","title":"Version compatibility"},{"location":"nebula-exchange/about-exchange/ex-ug-limitations/#environment","text":"Exchange 3.x supports the following operating systems: CentOS 7 macOS","title":"Environment"},{"location":"nebula-exchange/about-exchange/ex-ug-limitations/#software_dependencies","text":"To ensure the healthy operation of Exchange, ensure that the following software has been installed on the machine: Java version 1.8 Scala version 2.10.7, 2.11.12, or 2.12.10 Apache Spark. The requirements for Spark versions when using Exchange to export data from data sources are as follows. In the following table, Y means that the corresponding Spark version is supported, and N means not supported. Note Use the correct Exchange JAR file based on the Spark version. For example, for Spark version 2.4, use nebula-exchange_spark_2.4-3.0.0.jar. Data source Spark 2.2 Spark 2.4 Spark 3 CSV file Y N Y JSON file Y Y Y ORC file Y Y Y Parquet file Y Y Y HBase Y Y Y MySQL Y Y Y PostgreSQL Y Y Y ClickHouse Y Y Y Neo4j N Y N Hive Y Y Y MaxCompute N Y N Pulsar N Y Untested Kafka N Y Untested Nebula Graph N Y N Hadoop Distributed File System (HDFS) needs to be deployed in the following scenarios: Migrate HDFS data Generate SST files","title":"Software dependencies"},{"location":"nebula-exchange/about-exchange/ex-ug-what-is-exchange/","text":"What is Nebula Exchange \u00b6 Nebula Exchange (Exchange) is an Apache Spark\u2122 application for bulk migration of cluster data to Nebula Graph in a distributed environment, supporting batch and streaming data migration in a variety of formats. Exchange consists of Reader, Processor, and Writer. After Reader reads data from different sources and returns a DataFrame, the Processor iterates through each row of the DataFrame and obtains the corresponding value based on the mapping between fields in the configuration file. After iterating through the number of rows in the specified batch, Writer writes the captured data to the Nebula Graph at once. The following figure illustrates the process by which Exchange completes the data conversion and migration. Editions \u00b6 Exchange has two editions, the Community Edition and the Enterprise Edition. The Community Edition is open source developed on GitHub . The Enterprise Edition supports not only the functions of the Community Edition but also adds additional features. For details, see Comparisons . Scenarios \u00b6 Exchange applies to the following scenarios: Streaming data from Kafka and Pulsar platforms, such as log files, online shopping data, activities of game players, information on social websites, financial transactions or geospatial services, and telemetry data from connected devices or instruments in the data center, are required to be converted into the vertex or edge data of the property graph and import them into the Nebula Graph database. Batch data, such as data from a time period, needs to be read from a relational database (such as MySQL) or a distributed file system (such as HDFS), converted into vertex or edge data for a property graph, and imported into the Nebula Graph database. A large volume of data needs to be generated into SST files that Nebula Graph can recognize and then imported into the Nebula Graph database. The data saved in Nebula Graph needs to be exported. Enterpriseonly Exporting the data saved in Nebula Graph is supported by Exchange Enterprise Edition only. Advantages \u00b6 Exchange has the following advantages: High adaptability: It supports importing data into the Nebula Graph database in a variety of formats or from a variety of sources, making it easy to migrate data. SST import: It supports converting data from different sources into SST files for data import. SSL encryption: It supports establishing the SSL encryption between Exchange and Nebula Graph to ensure data security. Resumable data import: It supports resumable data import to save time and improve data import efficiency. Note Resumable data import is currently supported when migrating Neo4j data only. Asynchronous operation: An insert statement is generated in the source data and sent to the Graph service. Then the insert operation is performed. Great flexibility: It supports importing multiple Tags and Edge types at the same time. Different Tags and Edge types can be from different data sources or in different formats. Statistics: It uses the accumulator in Apache Spark\u2122 to count the number of successful and failed insert operations. Easy to use: It adopts the Human-Optimized Config Object Notation (HOCON) configuration file format and has an object-oriented style, which is easy to understand and operate. Data source \u00b6 Exchange 3.0.0 supports converting data from the following formats or sources into vertexes and edges that Nebula Graph can recognize, and then importing them into Nebula Graph in the form of nGQL statements: Data stored in HDFS or locally: Apache Parquet Apache ORC JSON CSV Apache HBase\u2122 Data repository: Hive MaxCompute Graph database: Neo4j (Client version 2.4.5-M1) Relational database: MySQL PostgreSQL Column database: ClickHouse Stream processing software platform: Apache Kafka\u00ae Publish/Subscribe messaging platform: Apache Pulsar 2.4.5 In addition to importing data as nGQL statements, Exchange supports generating SST files for data sources and then importing SST files via Console. In addition, Exchange Enterprise Edition also supports exporting data to a CSV file using Nebula Graph as data sources. Release note \u00b6 Release","title":"What is Nebula Exchange"},{"location":"nebula-exchange/about-exchange/ex-ug-what-is-exchange/#what_is_nebula_exchange","text":"Nebula Exchange (Exchange) is an Apache Spark\u2122 application for bulk migration of cluster data to Nebula Graph in a distributed environment, supporting batch and streaming data migration in a variety of formats. Exchange consists of Reader, Processor, and Writer. After Reader reads data from different sources and returns a DataFrame, the Processor iterates through each row of the DataFrame and obtains the corresponding value based on the mapping between fields in the configuration file. After iterating through the number of rows in the specified batch, Writer writes the captured data to the Nebula Graph at once. The following figure illustrates the process by which Exchange completes the data conversion and migration.","title":"What is Nebula Exchange"},{"location":"nebula-exchange/about-exchange/ex-ug-what-is-exchange/#editions","text":"Exchange has two editions, the Community Edition and the Enterprise Edition. The Community Edition is open source developed on GitHub . The Enterprise Edition supports not only the functions of the Community Edition but also adds additional features. For details, see Comparisons .","title":"Editions"},{"location":"nebula-exchange/about-exchange/ex-ug-what-is-exchange/#scenarios","text":"Exchange applies to the following scenarios: Streaming data from Kafka and Pulsar platforms, such as log files, online shopping data, activities of game players, information on social websites, financial transactions or geospatial services, and telemetry data from connected devices or instruments in the data center, are required to be converted into the vertex or edge data of the property graph and import them into the Nebula Graph database. Batch data, such as data from a time period, needs to be read from a relational database (such as MySQL) or a distributed file system (such as HDFS), converted into vertex or edge data for a property graph, and imported into the Nebula Graph database. A large volume of data needs to be generated into SST files that Nebula Graph can recognize and then imported into the Nebula Graph database. The data saved in Nebula Graph needs to be exported. Enterpriseonly Exporting the data saved in Nebula Graph is supported by Exchange Enterprise Edition only.","title":"Scenarios"},{"location":"nebula-exchange/about-exchange/ex-ug-what-is-exchange/#advantages","text":"Exchange has the following advantages: High adaptability: It supports importing data into the Nebula Graph database in a variety of formats or from a variety of sources, making it easy to migrate data. SST import: It supports converting data from different sources into SST files for data import. SSL encryption: It supports establishing the SSL encryption between Exchange and Nebula Graph to ensure data security. Resumable data import: It supports resumable data import to save time and improve data import efficiency. Note Resumable data import is currently supported when migrating Neo4j data only. Asynchronous operation: An insert statement is generated in the source data and sent to the Graph service. Then the insert operation is performed. Great flexibility: It supports importing multiple Tags and Edge types at the same time. Different Tags and Edge types can be from different data sources or in different formats. Statistics: It uses the accumulator in Apache Spark\u2122 to count the number of successful and failed insert operations. Easy to use: It adopts the Human-Optimized Config Object Notation (HOCON) configuration file format and has an object-oriented style, which is easy to understand and operate.","title":"Advantages"},{"location":"nebula-exchange/about-exchange/ex-ug-what-is-exchange/#data_source","text":"Exchange 3.0.0 supports converting data from the following formats or sources into vertexes and edges that Nebula Graph can recognize, and then importing them into Nebula Graph in the form of nGQL statements: Data stored in HDFS or locally: Apache Parquet Apache ORC JSON CSV Apache HBase\u2122 Data repository: Hive MaxCompute Graph database: Neo4j (Client version 2.4.5-M1) Relational database: MySQL PostgreSQL Column database: ClickHouse Stream processing software platform: Apache Kafka\u00ae Publish/Subscribe messaging platform: Apache Pulsar 2.4.5 In addition to importing data as nGQL statements, Exchange supports generating SST files for data sources and then importing SST files via Console. In addition, Exchange Enterprise Edition also supports exporting data to a CSV file using Nebula Graph as data sources.","title":"Data source"},{"location":"nebula-exchange/about-exchange/ex-ug-what-is-exchange/#release_note","text":"Release","title":"Release note"},{"location":"nebula-exchange/parameter-reference/ex-ug-para-import-command/","text":"Options for import \u00b6 After editing the configuration file, run the following commands to import specified source data into the Nebula Graph database. First import <spark_install_path>/bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange <nebula-exchange-2.x.y.jar_path> -c <application.conf_path> Import the reload file If some data fails to be imported during the first import, the failed data will be stored in the reload file. Use the parameter -r to import the reload file. <spark_install_path>/bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange <nebula-exchange-2.x.y.jar_path> -c <application.conf_path> -r \"<reload_file_path>\" Note The version number of a JAR file is subject to the name of the JAR file that is actually compiled. Note If users use the yarn-cluster mode to submit a job, see the following command: $SPARK_HOME /bin/spark-submit --master yarn-cluster \\ --class com.vesoft.nebula.exchange.Exchange \\ --files application.conf \\ --conf spark.driver.extraClassPath = ./ \\ --conf spark.executor.extraClassPath = ./ \\ nebula-exchange-3.0.0.jar \\ -c application.conf The following table lists command parameters. Parameter Required Default value Description --class Yes - Specify the main class of the driver. --master Yes - Specify the URL of the master process in a Spark cluster. For more information, see master-urls . -c / --config Yes - Specify the path of the configuration file. -h / --hive No false Indicate support for importing Hive data. -D / --dry No false Check whether the format of the configuration file meets the requirements, but it does not check whether the configuration items of tags and edges are correct. This parameter cannot be added when users import data. -r / --reload No - Specify the path of the reload file that needs to be reloaded. For more Spark parameter configurations, see Spark Configuration .","title":"Options for import"},{"location":"nebula-exchange/parameter-reference/ex-ug-para-import-command/#options_for_import","text":"After editing the configuration file, run the following commands to import specified source data into the Nebula Graph database. First import <spark_install_path>/bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange <nebula-exchange-2.x.y.jar_path> -c <application.conf_path> Import the reload file If some data fails to be imported during the first import, the failed data will be stored in the reload file. Use the parameter -r to import the reload file. <spark_install_path>/bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange <nebula-exchange-2.x.y.jar_path> -c <application.conf_path> -r \"<reload_file_path>\" Note The version number of a JAR file is subject to the name of the JAR file that is actually compiled. Note If users use the yarn-cluster mode to submit a job, see the following command: $SPARK_HOME /bin/spark-submit --master yarn-cluster \\ --class com.vesoft.nebula.exchange.Exchange \\ --files application.conf \\ --conf spark.driver.extraClassPath = ./ \\ --conf spark.executor.extraClassPath = ./ \\ nebula-exchange-3.0.0.jar \\ -c application.conf The following table lists command parameters. Parameter Required Default value Description --class Yes - Specify the main class of the driver. --master Yes - Specify the URL of the master process in a Spark cluster. For more information, see master-urls . -c / --config Yes - Specify the path of the configuration file. -h / --hive No false Indicate support for importing Hive data. -D / --dry No false Check whether the format of the configuration file meets the requirements, but it does not check whether the configuration items of tags and edges are correct. This parameter cannot be added when users import data. -r / --reload No - Specify the path of the reload file that needs to be reloaded. For more Spark parameter configurations, see Spark Configuration .","title":"Options for import"},{"location":"nebula-exchange/parameter-reference/ex-ug-parameter/","text":"Parameters in the configuration file \u00b6 This topic describes how to configure the file application.conf when users use Nebula Exchange. Before configuring the application.conf file, it is recommended to copy the file name application.conf and then edit the file name according to the file type of a data source. For example, change the file name to csv_application.conf if the file type of the data source is CSV. The application.conf file contains the following content types: Spark configurations Hive configurations (optional) Nebula Graph configurations Vertex configurations Edge configurations Spark configurations \u00b6 This topic lists only some Spark parameters. For more information, see Spark Configuration . Parameter Type Default value Required Description spark.app.name string - No The drive name in Spark. spark.driver.cores int 1 No The number of CPU cores used by a driver, only applicable to a cluster mode. spark.driver.maxResultSize string 1G No The total size limit (in bytes) of the serialized results of all partitions in a single Spark operation (such as collect). The minimum value is 1M, and 0 means unlimited. spark.executor.memory string 1G No The amount of memory used by a Spark driver which can be specified in units, such as 512M or 1G. spark.cores.max int 16 No The maximum number of CPU cores of applications requested across clusters (rather than from each node) when a driver runs in a coarse-grained sharing mode on a standalone cluster or a Mesos cluster. The default value is spark.deploy.defaultCores on a Spark standalone cluster manager or the value of the infinite parameter (all available cores) on Mesos. Hive configurations (optional) \u00b6 Users only need to configure parameters for connecting to Hive if Spark and Hive are deployed in different clusters. Otherwise, please ignore the following configurations. Parameter Type Default value Required Description hive.warehouse string - Yes The warehouse path in HDFS. Enclose the path in double quotes and start with hdfs:// . hive.connectionURL string - Yes The URL of a JDBC connection. For example, \"jdbc:mysql://127.0.0.1:3306/hive_spark?characterEncoding=UTF-8\" . hive.connectionDriverName string \"com.mysql.jdbc.Driver\" Yes The driver name. hive.connectionUserName list[string] - Yes The username for connections. hive.connectionPassword list[string] - Yes The account password. Nebula Graph configurations \u00b6 Parameter Type Default value Required Description nebula.address.graph list[string] [\"127.0.0.1:9669\"] Yes The addresses of all Graph services, including IPs and ports, separated by commas (,). Example: [\"ip1:port1\",\"ip2:port2\",\"ip3:port3\"] . nebula.address.meta list[string] [\"127.0.0.1:9559\"] Yes The addresses of all Meta services, including IPs and ports, separated by commas (,). Example: [\"ip1:port1\",\"ip2:port2\",\"ip3:port3\"] . nebula.user string - Yes The username with write permissions for Nebula Graph. nebula.pswd string - Yes The account password. nebula.space string - Yes The name of the graph space where data needs to be imported. nebula.ssl.enable.graph bool false Yes Enables the SSL encryption between Exchange and Graph services. If the value is true , the SSL encryption is enabled and the following SSL parameters take effect. If Exchange is run on a multi-machine cluster, you need to store the corresponding files in the same path on each machine when setting the following SSL-related paths. nebula.ssl.sign string ca Yes Specifies the SSL sign. Optional values are ca and self . nebula.ssl.ca.param.caCrtFilePath string Specifies the storage path of the CA certificate. It takes effect when the value of nebula.ssl.sign is ca . nebula.ssl.ca.param.crtFilePath string \"/path/crtFilePath\" Yes Specifies the storage path of the CRT certificate. It takes effect when the value of nebula.ssl.sign is ca . nebula.ssl.ca.param.keyFilePath string \"/path/keyFilePath\" Yes Specifies the storage path of the key file. It takes effect when the value of nebula.ssl.sign is ca . nebula.ssl.self.param.crtFilePath string \"/path/crtFilePath\" Yes Specifies the storage path of the CRT certificate. It takes effect when the value of nebula.ssl.sign is self . nebula.ssl.self.param.keyFilePath string \"/path/keyFilePath\" Yes Specifies the storage path of the key file. It takes effect when the value of nebula.ssl.sign is self . nebula.ssl.self.param.password string \"nebula\" Yes Specifies the storage path of the password. It takes effect when the value of nebula.ssl.sign is self . nebula.path.local string \"/tmp\" No The local SST file path which needs to be set when users import SST files. nebula.path.remote string \"/sst\" No The remote SST file path which needs to be set when users import SST files. nebula.path.hdfs.namenode string \"hdfs://name_node:9000\" No The NameNode path which needs to be set when users import SST files. nebula.connection.timeout int 3000 No The timeout set for Thrift connections. Unit: ms. nebula.connection.retry int 3 No Retries set for Thrift connections. nebula.execution.retry int 3 No Retries set for executing nGQL statements. nebula.error.max int 32 No The maximum number of failures during the import process. When the number of failures reaches the maximum, the Spark job submitted will stop automatically . nebula.error.output string /tmp/errors No The path to output error logs. Failed nGQL statement executions are saved in the error log. nebula.rate.limit int 1024 No The limit on the number of tokens in the token bucket when importing data. nebula.rate.timeout int 1000 No The timeout period for getting tokens from a token bucket. Unit: milliseconds. Vertex configurations \u00b6 For different data sources, the vertex configurations are different. There are many general parameters and some specific parameters. General parameters and specific parameters of different data sources need to be configured when users configure vertices. General parameters \u00b6 Parameter Type Default value Required Description tags.name string - Yes The tag name defined in Nebula Graph. tags.type.source string - Yes Specify a data source. For example, csv . tags.type.sink string client Yes Specify an import method. Optional values are client and SST . tags.fields list[string] - Yes The header or column name of the column corresponding to properties. If there is a header or a column name, please use that name directly. If a CSV file does not have a header, use the form of [_c0, _c1, _c2] to represent the first column, the second column, the third column, and so on. tags.nebula.fields list[string] - Yes Property names defined in Nebula Graph, the order of which must correspond to tags.fields . For example, [_c1, _c2] corresponds to [name, age] , which means that values in the second column are the values of the property name , and values in the third column are the values of the property age . tags.vertex.field string - Yes The column of vertex IDs. For example, when a CSV file has no header, users can use _c0 to indicate values in the first column are vertex IDs. tags.batch int 256 Yes The maximum number of vertices written into Nebula Graph in a single batch. tags.partition int 32 Yes The number of Spark partitions. Specific parameters of Parquet/JSON/ORC data sources \u00b6 Parameter Type Default value Required Description tags.path string - Yes The path of vertex data files in HDFS. Enclose the path in double quotes and start with hdfs:// . Specific parameters of CSV data sources \u00b6 Parameter Type Default value Required Description tags.path string - Yes The path of vertex data files in HDFS. Enclose the path in double quotes and start with hdfs:// . tags.separator string , Yes The separator. The default value is a comma (,). tags.header bool true Yes Whether the file has a header. Specific parameters of Hive data sources \u00b6 Parameter Type Default value Required Description tags.exec string - Yes The statement to query data sources. For example, select name,age from mooc.users . Specific parameters of MaxCompute data sources \u00b6 Parameter Type Default value Required Description tags.table string - Yes The table name of the MaxCompute. tags.project string - Yes The project name of the MaxCompute. tags.odpsUrl string - Yes The odpsUrl of the MaxCompute service. For more information about odpsUrl, see Endpoints . tags.tunnelUrl string - Yes The tunnelUrl of the MaxCompute service. For more information about tunnelUrl, see Endpoints . tags.accessKeyId string - Yes The accessKeyId of the MaxCompute service. tags.accessKeySecret string - Yes The accessKeySecret of the MaxCompute service. tags.partitionSpec string - No Partition descriptions of MaxCompute tables. tags.sentence string - No Statements to query data sources. The table name in the SQL statement is the same as the value of the table above. Specific parameters of Neo4j data sources \u00b6 Parameter Type Default value Required Description tags.exec string - Yes Statements to query data sources. For example: match (n:label) return n.neo4j-field-0 . tags.server string \"bolt://127.0.0.1:7687\" Yes The server address of Neo4j. tags.user string - Yes The Neo4j username with read permissions. tags.password string - Yes The account password. tags.database string - Yes The name of the database where source data is saved in Neo4j. tags.check_point_path string /tmp/test No The directory set to import progress information, which is used for resuming transfers. If not set, the resuming transfer is disabled. Specific parameters of MySQL/PostgreSQL data sources \u00b6 Parameter Type Default value Required Description tags.host string - Yes The MySQL/PostgreSQL server address. tags.port string - Yes The MySQL/PostgreSQL server port. tags.database string - Yes The database name. tags.table string - Yes The name of a table used as a data source. tags.user string - Yes The MySQL/PostgreSQL username with read permissions. tags.password string - Yes The account password. tags.sentence string - Yes Statements to query data sources. For example: \"select teamid, name from basketball.team order by teamid;\" . Specific parameters of ClickHouse data sources \u00b6 Parameter Type Default value Required Description tags.url string - Yes The JDBC URL of ClickHouse. tags.user string - Yes The ClickHouse username with read permissions. tags.password string - Yes The account password. tags.numPartition string - Yes The number of ClickHouse partitions. tags.sentence string - Yes Statements to query data sources. Specific parameters of Hbase data sources \u00b6 Parameter Type Default value Required Description tags.host string 127.0.0.1 Yes The Hbase server address. tags.port string 2181 Yes The Hbase server port. tags.table string - Yes The name of a table used as a data source. tags.columnFamily string - Yes The column family to which a table belongs. Specific parameters of Pulsar data sources \u00b6 Parameter Type Default value Required Description tags.service string \"pulsar://localhost:6650\" Yes The Pulsar server address. tags.admin string \"http://localhost:8081\" Yes The admin URL used to connect pulsar. tags.options.<topic\\|topics\\| topicsPattern> string - Yes Options offered by Pulsar, which can be configured by choosing one from topic , topics , and topicsPattern . tags.interval.seconds int 10 Yes The interval for reading messages. Unit: seconds. Specific parameters of Kafka data sources \u00b6 Parameter Type Default value Required Description tags.service string - Yes The Kafka server address. tags.topic string - Yes The message type. tags.interval.seconds int 10 Yes The interval for reading messages. Unit: seconds. Specific parameters for generating SST files \u00b6 Parameter Type Default value Required Description tags.path string - Yes The path of the source file specified to generate SST files. tags.repartitionWithNebula bool false No Whether to repartition data based on the number of partitions of graph spaces in Nebula Graph when generating the SST file. Enabling this function can reduce the time required to DOWNLOAD and INGEST SST files. Specific parameters of Nebula Graph \u00b6 Enterpriseonly Specific parameters of Nebula Graph are used for exporting Nebula Graph data, which is supported by Exchange Enterprise Edition only. Parameter Data type Default value Required Description tags.path string \"hdfs://namenode:9000/path/vertex\" Yes Specifies the storage path of the CSV file. You need to set a new path and Exchange will automatically create the path you set. If you store the data to the HDFS server, the path format is the same as the default value, such as \"hdfs://192.168.8.177:9000/vertex/player\" . If you store the data to the local, the path format is \"file:///path/vertex\" , such as \"file:///home/nebula/vertex/player\" . If there are multiple Tags, different directories must be set for each Tag. tags.noField bool false Yes If the value is true , only VIDs will be exported, not the property data. If the value is false , VIDs and the property data will be exported. tags.return.fields list [] Yes Specifies the properties to be exported. For example, to export the name and age , you need to set the parameter value to [\"name\",\"age\"] . This parameter only takes effect when the value of tags.noField is false . Edge configurations \u00b6 For different data sources, configurations of edges are also different. There are general parameters and some specific parameters. General parameters and specific parameters of different data sources need to be configured when users configure edges. For the specific parameters of different data sources for edge configurations, please refer to the introduction of specific parameters of different data sources above, and pay attention to distinguishing tags and edges. General parameters \u00b6 Parameter Type Default value Required Description edges.name string - Yes The edge type name defined in Nebula Graph. edges.type.source string - Yes The data source of edges. For example, csv . edges.type.sink string client Yes The method specified to import data. Optional values are client and SST . edges.fields list[string] - Yes The header or column name of the column corresponding to properties. If there is a header or column name, please use that name directly. If a CSV file does not have a header, use the form of [_c0, _c1, _c2] to represent the first column, the second column, the third column, and so on. edges.nebula.fields list[string] - Yes Edge names defined in Nebula Graph, the order of which must correspond to edges.fields . For example, [_c2, _c3] corresponds to [start_year, end_year] , which means that values in the third column are the values of the start year, and values in the fourth column are the values of the end year. edges.source.field string - Yes The column of source vertices of edges. For example, _c0 indicates a value in the first column that is used as the source vertex of an edge. edges.target.field string - Yes The column of destination vertices of edges. For example, _c0 indicates a value in the first column that is used as the destination vertex of an edge. edges.ranking int - No The column of rank values. If not specified, all rank values are 0 by default. edges.batch int 256 Yes The maximum number of edges written into Nebula Graph in a single batch. edges.partition int 32 Yes The number of Spark partitions. Specific parameters for generating SST files \u00b6 Parameter Type Default value Required Description edges.path string - Yes The path of the source file specified to generate SST files. edges.repartitionWithNebula bool false No Whether to repartition data based on the number of partitions of graph spaces in Nebula Graph when generating the SST file. Enabling this function can reduce the time required to DOWNLOAD and INGEST SST files. Specific parameters of Nebula Graph \u00b6 Parameter Type Default value Required Description edges.path string \"hdfs://namenode:9000/path/edge\" Yes Specifies the storage path of the CSV file. You need to set a new path and Exchange will automatically create the path you set. If you store the data to the HDFS server, the path format is the same as the default value, such as \"hdfs://192.168.8.177:9000/edge/follow\" . If you store the data to the local, the path format is \"file:///path/edge\" , such as \"file:///home/nebula/edge/follow\" . If there are multiple Edges, different directories must be set for each Edge. edges.noField bool false Yes If the value is true , source vertex IDs, destination vertex IDs, and ranks will be exported, not the property data. If the vaue is false , ranks, source vertex IDs, destination vertex IDs, ranks, and the property data will be exported. edges.return.fields list [] Yes Specifies the properties to be exported. For example, to export start_year and end_year , you need to set the parameter value to [\"start_year\",\"end_year\"] . This parameter only takes effect when the value of edges.noField is false .","title":"Parameters in the configuration file"},{"location":"nebula-exchange/parameter-reference/ex-ug-parameter/#parameters_in_the_configuration_file","text":"This topic describes how to configure the file application.conf when users use Nebula Exchange. Before configuring the application.conf file, it is recommended to copy the file name application.conf and then edit the file name according to the file type of a data source. For example, change the file name to csv_application.conf if the file type of the data source is CSV. The application.conf file contains the following content types: Spark configurations Hive configurations (optional) Nebula Graph configurations Vertex configurations Edge configurations","title":"Parameters in the configuration file"},{"location":"nebula-exchange/parameter-reference/ex-ug-parameter/#spark_configurations","text":"This topic lists only some Spark parameters. For more information, see Spark Configuration . Parameter Type Default value Required Description spark.app.name string - No The drive name in Spark. spark.driver.cores int 1 No The number of CPU cores used by a driver, only applicable to a cluster mode. spark.driver.maxResultSize string 1G No The total size limit (in bytes) of the serialized results of all partitions in a single Spark operation (such as collect). The minimum value is 1M, and 0 means unlimited. spark.executor.memory string 1G No The amount of memory used by a Spark driver which can be specified in units, such as 512M or 1G. spark.cores.max int 16 No The maximum number of CPU cores of applications requested across clusters (rather than from each node) when a driver runs in a coarse-grained sharing mode on a standalone cluster or a Mesos cluster. The default value is spark.deploy.defaultCores on a Spark standalone cluster manager or the value of the infinite parameter (all available cores) on Mesos.","title":"Spark configurations"},{"location":"nebula-exchange/parameter-reference/ex-ug-parameter/#hive_configurations_optional","text":"Users only need to configure parameters for connecting to Hive if Spark and Hive are deployed in different clusters. Otherwise, please ignore the following configurations. Parameter Type Default value Required Description hive.warehouse string - Yes The warehouse path in HDFS. Enclose the path in double quotes and start with hdfs:// . hive.connectionURL string - Yes The URL of a JDBC connection. For example, \"jdbc:mysql://127.0.0.1:3306/hive_spark?characterEncoding=UTF-8\" . hive.connectionDriverName string \"com.mysql.jdbc.Driver\" Yes The driver name. hive.connectionUserName list[string] - Yes The username for connections. hive.connectionPassword list[string] - Yes The account password.","title":"Hive configurations (optional)"},{"location":"nebula-exchange/parameter-reference/ex-ug-parameter/#nebula_graph_configurations","text":"Parameter Type Default value Required Description nebula.address.graph list[string] [\"127.0.0.1:9669\"] Yes The addresses of all Graph services, including IPs and ports, separated by commas (,). Example: [\"ip1:port1\",\"ip2:port2\",\"ip3:port3\"] . nebula.address.meta list[string] [\"127.0.0.1:9559\"] Yes The addresses of all Meta services, including IPs and ports, separated by commas (,). Example: [\"ip1:port1\",\"ip2:port2\",\"ip3:port3\"] . nebula.user string - Yes The username with write permissions for Nebula Graph. nebula.pswd string - Yes The account password. nebula.space string - Yes The name of the graph space where data needs to be imported. nebula.ssl.enable.graph bool false Yes Enables the SSL encryption between Exchange and Graph services. If the value is true , the SSL encryption is enabled and the following SSL parameters take effect. If Exchange is run on a multi-machine cluster, you need to store the corresponding files in the same path on each machine when setting the following SSL-related paths. nebula.ssl.sign string ca Yes Specifies the SSL sign. Optional values are ca and self . nebula.ssl.ca.param.caCrtFilePath string Specifies the storage path of the CA certificate. It takes effect when the value of nebula.ssl.sign is ca . nebula.ssl.ca.param.crtFilePath string \"/path/crtFilePath\" Yes Specifies the storage path of the CRT certificate. It takes effect when the value of nebula.ssl.sign is ca . nebula.ssl.ca.param.keyFilePath string \"/path/keyFilePath\" Yes Specifies the storage path of the key file. It takes effect when the value of nebula.ssl.sign is ca . nebula.ssl.self.param.crtFilePath string \"/path/crtFilePath\" Yes Specifies the storage path of the CRT certificate. It takes effect when the value of nebula.ssl.sign is self . nebula.ssl.self.param.keyFilePath string \"/path/keyFilePath\" Yes Specifies the storage path of the key file. It takes effect when the value of nebula.ssl.sign is self . nebula.ssl.self.param.password string \"nebula\" Yes Specifies the storage path of the password. It takes effect when the value of nebula.ssl.sign is self . nebula.path.local string \"/tmp\" No The local SST file path which needs to be set when users import SST files. nebula.path.remote string \"/sst\" No The remote SST file path which needs to be set when users import SST files. nebula.path.hdfs.namenode string \"hdfs://name_node:9000\" No The NameNode path which needs to be set when users import SST files. nebula.connection.timeout int 3000 No The timeout set for Thrift connections. Unit: ms. nebula.connection.retry int 3 No Retries set for Thrift connections. nebula.execution.retry int 3 No Retries set for executing nGQL statements. nebula.error.max int 32 No The maximum number of failures during the import process. When the number of failures reaches the maximum, the Spark job submitted will stop automatically . nebula.error.output string /tmp/errors No The path to output error logs. Failed nGQL statement executions are saved in the error log. nebula.rate.limit int 1024 No The limit on the number of tokens in the token bucket when importing data. nebula.rate.timeout int 1000 No The timeout period for getting tokens from a token bucket. Unit: milliseconds.","title":"Nebula Graph configurations"},{"location":"nebula-exchange/parameter-reference/ex-ug-parameter/#vertex_configurations","text":"For different data sources, the vertex configurations are different. There are many general parameters and some specific parameters. General parameters and specific parameters of different data sources need to be configured when users configure vertices.","title":"Vertex configurations"},{"location":"nebula-exchange/parameter-reference/ex-ug-parameter/#general_parameters","text":"Parameter Type Default value Required Description tags.name string - Yes The tag name defined in Nebula Graph. tags.type.source string - Yes Specify a data source. For example, csv . tags.type.sink string client Yes Specify an import method. Optional values are client and SST . tags.fields list[string] - Yes The header or column name of the column corresponding to properties. If there is a header or a column name, please use that name directly. If a CSV file does not have a header, use the form of [_c0, _c1, _c2] to represent the first column, the second column, the third column, and so on. tags.nebula.fields list[string] - Yes Property names defined in Nebula Graph, the order of which must correspond to tags.fields . For example, [_c1, _c2] corresponds to [name, age] , which means that values in the second column are the values of the property name , and values in the third column are the values of the property age . tags.vertex.field string - Yes The column of vertex IDs. For example, when a CSV file has no header, users can use _c0 to indicate values in the first column are vertex IDs. tags.batch int 256 Yes The maximum number of vertices written into Nebula Graph in a single batch. tags.partition int 32 Yes The number of Spark partitions.","title":"General parameters"},{"location":"nebula-exchange/parameter-reference/ex-ug-parameter/#specific_parameters_of_parquetjsonorc_data_sources","text":"Parameter Type Default value Required Description tags.path string - Yes The path of vertex data files in HDFS. Enclose the path in double quotes and start with hdfs:// .","title":"Specific parameters of Parquet/JSON/ORC data sources"},{"location":"nebula-exchange/parameter-reference/ex-ug-parameter/#specific_parameters_of_csv_data_sources","text":"Parameter Type Default value Required Description tags.path string - Yes The path of vertex data files in HDFS. Enclose the path in double quotes and start with hdfs:// . tags.separator string , Yes The separator. The default value is a comma (,). tags.header bool true Yes Whether the file has a header.","title":"Specific parameters of CSV data sources"},{"location":"nebula-exchange/parameter-reference/ex-ug-parameter/#specific_parameters_of_hive_data_sources","text":"Parameter Type Default value Required Description tags.exec string - Yes The statement to query data sources. For example, select name,age from mooc.users .","title":"Specific parameters of Hive data sources"},{"location":"nebula-exchange/parameter-reference/ex-ug-parameter/#specific_parameters_of_maxcompute_data_sources","text":"Parameter Type Default value Required Description tags.table string - Yes The table name of the MaxCompute. tags.project string - Yes The project name of the MaxCompute. tags.odpsUrl string - Yes The odpsUrl of the MaxCompute service. For more information about odpsUrl, see Endpoints . tags.tunnelUrl string - Yes The tunnelUrl of the MaxCompute service. For more information about tunnelUrl, see Endpoints . tags.accessKeyId string - Yes The accessKeyId of the MaxCompute service. tags.accessKeySecret string - Yes The accessKeySecret of the MaxCompute service. tags.partitionSpec string - No Partition descriptions of MaxCompute tables. tags.sentence string - No Statements to query data sources. The table name in the SQL statement is the same as the value of the table above.","title":"Specific parameters of MaxCompute data sources"},{"location":"nebula-exchange/parameter-reference/ex-ug-parameter/#specific_parameters_of_neo4j_data_sources","text":"Parameter Type Default value Required Description tags.exec string - Yes Statements to query data sources. For example: match (n:label) return n.neo4j-field-0 . tags.server string \"bolt://127.0.0.1:7687\" Yes The server address of Neo4j. tags.user string - Yes The Neo4j username with read permissions. tags.password string - Yes The account password. tags.database string - Yes The name of the database where source data is saved in Neo4j. tags.check_point_path string /tmp/test No The directory set to import progress information, which is used for resuming transfers. If not set, the resuming transfer is disabled.","title":"Specific parameters of Neo4j data sources"},{"location":"nebula-exchange/parameter-reference/ex-ug-parameter/#specific_parameters_of_mysqlpostgresql_data_sources","text":"Parameter Type Default value Required Description tags.host string - Yes The MySQL/PostgreSQL server address. tags.port string - Yes The MySQL/PostgreSQL server port. tags.database string - Yes The database name. tags.table string - Yes The name of a table used as a data source. tags.user string - Yes The MySQL/PostgreSQL username with read permissions. tags.password string - Yes The account password. tags.sentence string - Yes Statements to query data sources. For example: \"select teamid, name from basketball.team order by teamid;\" .","title":"Specific parameters of MySQL/PostgreSQL data sources"},{"location":"nebula-exchange/parameter-reference/ex-ug-parameter/#specific_parameters_of_clickhouse_data_sources","text":"Parameter Type Default value Required Description tags.url string - Yes The JDBC URL of ClickHouse. tags.user string - Yes The ClickHouse username with read permissions. tags.password string - Yes The account password. tags.numPartition string - Yes The number of ClickHouse partitions. tags.sentence string - Yes Statements to query data sources.","title":"Specific parameters of ClickHouse data sources"},{"location":"nebula-exchange/parameter-reference/ex-ug-parameter/#specific_parameters_of_hbase_data_sources","text":"Parameter Type Default value Required Description tags.host string 127.0.0.1 Yes The Hbase server address. tags.port string 2181 Yes The Hbase server port. tags.table string - Yes The name of a table used as a data source. tags.columnFamily string - Yes The column family to which a table belongs.","title":"Specific parameters of Hbase data sources"},{"location":"nebula-exchange/parameter-reference/ex-ug-parameter/#specific_parameters_of_pulsar_data_sources","text":"Parameter Type Default value Required Description tags.service string \"pulsar://localhost:6650\" Yes The Pulsar server address. tags.admin string \"http://localhost:8081\" Yes The admin URL used to connect pulsar. tags.options.<topic\\|topics\\| topicsPattern> string - Yes Options offered by Pulsar, which can be configured by choosing one from topic , topics , and topicsPattern . tags.interval.seconds int 10 Yes The interval for reading messages. Unit: seconds.","title":"Specific parameters of Pulsar data sources"},{"location":"nebula-exchange/parameter-reference/ex-ug-parameter/#specific_parameters_of_kafka_data_sources","text":"Parameter Type Default value Required Description tags.service string - Yes The Kafka server address. tags.topic string - Yes The message type. tags.interval.seconds int 10 Yes The interval for reading messages. Unit: seconds.","title":"Specific parameters of Kafka data sources"},{"location":"nebula-exchange/parameter-reference/ex-ug-parameter/#specific_parameters_for_generating_sst_files","text":"Parameter Type Default value Required Description tags.path string - Yes The path of the source file specified to generate SST files. tags.repartitionWithNebula bool false No Whether to repartition data based on the number of partitions of graph spaces in Nebula Graph when generating the SST file. Enabling this function can reduce the time required to DOWNLOAD and INGEST SST files.","title":"Specific parameters for generating SST files"},{"location":"nebula-exchange/parameter-reference/ex-ug-parameter/#specific_parameters_of_nebula_graph","text":"Enterpriseonly Specific parameters of Nebula Graph are used for exporting Nebula Graph data, which is supported by Exchange Enterprise Edition only. Parameter Data type Default value Required Description tags.path string \"hdfs://namenode:9000/path/vertex\" Yes Specifies the storage path of the CSV file. You need to set a new path and Exchange will automatically create the path you set. If you store the data to the HDFS server, the path format is the same as the default value, such as \"hdfs://192.168.8.177:9000/vertex/player\" . If you store the data to the local, the path format is \"file:///path/vertex\" , such as \"file:///home/nebula/vertex/player\" . If there are multiple Tags, different directories must be set for each Tag. tags.noField bool false Yes If the value is true , only VIDs will be exported, not the property data. If the value is false , VIDs and the property data will be exported. tags.return.fields list [] Yes Specifies the properties to be exported. For example, to export the name and age , you need to set the parameter value to [\"name\",\"age\"] . This parameter only takes effect when the value of tags.noField is false .","title":"Specific parameters of Nebula Graph"},{"location":"nebula-exchange/parameter-reference/ex-ug-parameter/#edge_configurations","text":"For different data sources, configurations of edges are also different. There are general parameters and some specific parameters. General parameters and specific parameters of different data sources need to be configured when users configure edges. For the specific parameters of different data sources for edge configurations, please refer to the introduction of specific parameters of different data sources above, and pay attention to distinguishing tags and edges.","title":"Edge configurations"},{"location":"nebula-exchange/parameter-reference/ex-ug-parameter/#general_parameters_1","text":"Parameter Type Default value Required Description edges.name string - Yes The edge type name defined in Nebula Graph. edges.type.source string - Yes The data source of edges. For example, csv . edges.type.sink string client Yes The method specified to import data. Optional values are client and SST . edges.fields list[string] - Yes The header or column name of the column corresponding to properties. If there is a header or column name, please use that name directly. If a CSV file does not have a header, use the form of [_c0, _c1, _c2] to represent the first column, the second column, the third column, and so on. edges.nebula.fields list[string] - Yes Edge names defined in Nebula Graph, the order of which must correspond to edges.fields . For example, [_c2, _c3] corresponds to [start_year, end_year] , which means that values in the third column are the values of the start year, and values in the fourth column are the values of the end year. edges.source.field string - Yes The column of source vertices of edges. For example, _c0 indicates a value in the first column that is used as the source vertex of an edge. edges.target.field string - Yes The column of destination vertices of edges. For example, _c0 indicates a value in the first column that is used as the destination vertex of an edge. edges.ranking int - No The column of rank values. If not specified, all rank values are 0 by default. edges.batch int 256 Yes The maximum number of edges written into Nebula Graph in a single batch. edges.partition int 32 Yes The number of Spark partitions.","title":"General parameters"},{"location":"nebula-exchange/parameter-reference/ex-ug-parameter/#specific_parameters_for_generating_sst_files_1","text":"Parameter Type Default value Required Description edges.path string - Yes The path of the source file specified to generate SST files. edges.repartitionWithNebula bool false No Whether to repartition data based on the number of partitions of graph spaces in Nebula Graph when generating the SST file. Enabling this function can reduce the time required to DOWNLOAD and INGEST SST files.","title":"Specific parameters for generating SST files"},{"location":"nebula-exchange/parameter-reference/ex-ug-parameter/#specific_parameters_of_nebula_graph_1","text":"Parameter Type Default value Required Description edges.path string \"hdfs://namenode:9000/path/edge\" Yes Specifies the storage path of the CSV file. You need to set a new path and Exchange will automatically create the path you set. If you store the data to the HDFS server, the path format is the same as the default value, such as \"hdfs://192.168.8.177:9000/edge/follow\" . If you store the data to the local, the path format is \"file:///path/edge\" , such as \"file:///home/nebula/edge/follow\" . If there are multiple Edges, different directories must be set for each Edge. edges.noField bool false Yes If the value is true , source vertex IDs, destination vertex IDs, and ranks will be exported, not the property data. If the vaue is false , ranks, source vertex IDs, destination vertex IDs, ranks, and the property data will be exported. edges.return.fields list [] Yes Specifies the properties to be exported. For example, to export start_year and end_year , you need to set the parameter value to [\"start_year\",\"end_year\"] . This parameter only takes effect when the value of edges.noField is false .","title":"Specific parameters of Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-export-from-nebula/","text":"Export data from Nebula Graph \u00b6 This topic uses an example to illustrate how to use Exchange to export data from Nebula Graph to a CSV file. Enterpriseonly Only Exchange Enterprise Edition supports exporting data from Nebula Graph to a CSV file. Note SSL encryption is not supported when exporting data from Nebula Graph. Preparation \u00b6 This example is completed on a virtual machine equipped with Linux. The hardware and software you need to prepare before exporting data are as follows. Hardware \u00b6 Type Information CPU 4 Intel(R) Xeon(R) Platinum 8260 CPU @ 2.30GHz Memory 16G Hard disk 50G System \u00b6 CentOS 7.9.2009 Software \u00b6 Name Version JDK 1.8.0 Hadoop 2.10.1 Scala 2.12.11 Spark 2.4.7 Nebula Graph 3.1.0 Dataset \u00b6 As the data source, Nebula Graph stores the basketballplayer dataset in this example, the Schema elements of which are shown as follows. Element Name Property Tag player name string, age int Tag team name string Edge type follow degree int Edge type serve start_year int, end_year int Steps \u00b6 Get the JAR file of Exchange Enterprise Edition from the Nebula Graph Enterprise Edition Package . Modify the configuration file. Exchange Enterprise Edition provides the configuration template export_application.conf for exporting Nebula Graph data. For details, see Exchange parameters . The core content of the configuration file used in this example is as follows: ... # Processing tags # There are tag config examples for different dataSources. tags: [ # export NebulaGraph tag data to csv, only support export to CSV for now. { name: player type: { source: Nebula sink: CSV } # the path to save the NebulaGrpah data, make sure the path doesn't exist. path:\"hdfs://192.168.8.177:9000/vertex/player\" # if no need to export any properties when export NebulaGraph tag data # if noField is configured true, just export vertexId noField:false # define properties to export from NebulaGraph tag data # if return.fields is configured as empty list, then export all properties return.fields:[] # nebula space partition number partition:10 } ... ] # Processing edges # There are edge config examples for different dataSources. edges: [ # export NebulaGraph tag data to csv, only support export to CSV for now. { name: follow type: { source: Nebula sink: CSV } # the path to save the NebulaGrpah data, make sure the path doesn't exist. path:\"hdfs://192.168.8.177:9000/edge/follow\" # if no need to export any properties when export NebulaGraph edge data # if noField is configured true, just export src,dst,rank noField:false # define properties to export from NebulaGraph edge data # if return.fields is configured as empty list, then export all properties return.fields:[] # nebula space partition number partition:10 } ... ] } Export data from Nebula Graph with the following command. <spark_install_path>/bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange nebula-exchange-x.y.z.jar_path> -c <export_application.conf_path> The command used in this example is as follows. $ ./spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange \\ ~/exchange-ent/nebula-exchange-ent-3.0.0.jar -c ~/exchange-ent/export_application.conf Check the exported data. Check whether the CSV file is successfully generated under the target path. $ hadoop fs -ls /vertex/player Found 11 items -rw-r--r-- 3 nebula supergroup 0 2021 -11-05 07 :36 /vertex/player/_SUCCESS -rw-r--r-- 3 nebula supergroup 160 2021 -11-05 07 :36 /vertex/player/ part-00000-17293020-ba2e-4243-b834-34495c0536b3-c000.csv -rw-r--r-- 3 nebula supergroup 163 2021 -11-05 07 :36 /vertex/player/ part-00001-17293020-ba2e-4243-b834-34495c0536b3-c000.csv -rw-r--r-- 3 nebula supergroup 172 2021 -11-05 07 :36 /vertex/player/ part-00002-17293020-ba2e-4243-b834-34495c0536b3-c000.csv -rw-r--r-- 3 nebula supergroup 172 2021 -11-05 07 :36 /vertex/player/ part-00003-17293020-ba2e-4243-b834-34495c0536b3-c000.csv -rw-r--r-- 3 nebula supergroup 144 2021 -11-05 07 :36 /vertex/player/ part-00004-17293020-ba2e-4243-b834-34495c0536b3-c000.csv -rw-r--r-- 3 nebula supergroup 173 2021 -11-05 07 :36 /vertex/player/ part-00005-17293020-ba2e-4243-b834-34495c0536b3-c000.csv -rw-r--r-- 3 nebula supergroup 160 2021 -11-05 07 :36 /vertex/player/ part-00006-17293020-ba2e-4243-b834-34495c0536b3-c000.csv -rw-r--r-- 3 nebula supergroup 148 2021 -11-05 07 :36 /vertex/player/ part-00007-17293020-ba2e-4243-b834-34495c0536b3-c000.csv -rw-r--r-- 3 nebula supergroup 125 2021 -11-05 07 :36 /vertex/player/ part-00008-17293020-ba2e-4243-b834-34495c0536b3-c000.csv -rw-r--r-- 3 nebula supergroup 119 2021 -11-05 07 :36 /vertex/player/ part-00009-17293020-ba2e-4243-b834-34495c0536b3-c000.csv Check the contents of the CSV file to ensure that the data export is successful.","title":"Export data from Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-export-from-nebula/#export_data_from_nebula_graph","text":"This topic uses an example to illustrate how to use Exchange to export data from Nebula Graph to a CSV file. Enterpriseonly Only Exchange Enterprise Edition supports exporting data from Nebula Graph to a CSV file. Note SSL encryption is not supported when exporting data from Nebula Graph.","title":"Export data from Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-export-from-nebula/#preparation","text":"This example is completed on a virtual machine equipped with Linux. The hardware and software you need to prepare before exporting data are as follows.","title":"Preparation"},{"location":"nebula-exchange/use-exchange/ex-ug-export-from-nebula/#hardware","text":"Type Information CPU 4 Intel(R) Xeon(R) Platinum 8260 CPU @ 2.30GHz Memory 16G Hard disk 50G","title":"Hardware"},{"location":"nebula-exchange/use-exchange/ex-ug-export-from-nebula/#system","text":"CentOS 7.9.2009","title":"System"},{"location":"nebula-exchange/use-exchange/ex-ug-export-from-nebula/#software","text":"Name Version JDK 1.8.0 Hadoop 2.10.1 Scala 2.12.11 Spark 2.4.7 Nebula Graph 3.1.0","title":"Software"},{"location":"nebula-exchange/use-exchange/ex-ug-export-from-nebula/#dataset","text":"As the data source, Nebula Graph stores the basketballplayer dataset in this example, the Schema elements of which are shown as follows. Element Name Property Tag player name string, age int Tag team name string Edge type follow degree int Edge type serve start_year int, end_year int","title":"Dataset"},{"location":"nebula-exchange/use-exchange/ex-ug-export-from-nebula/#steps","text":"Get the JAR file of Exchange Enterprise Edition from the Nebula Graph Enterprise Edition Package . Modify the configuration file. Exchange Enterprise Edition provides the configuration template export_application.conf for exporting Nebula Graph data. For details, see Exchange parameters . The core content of the configuration file used in this example is as follows: ... # Processing tags # There are tag config examples for different dataSources. tags: [ # export NebulaGraph tag data to csv, only support export to CSV for now. { name: player type: { source: Nebula sink: CSV } # the path to save the NebulaGrpah data, make sure the path doesn't exist. path:\"hdfs://192.168.8.177:9000/vertex/player\" # if no need to export any properties when export NebulaGraph tag data # if noField is configured true, just export vertexId noField:false # define properties to export from NebulaGraph tag data # if return.fields is configured as empty list, then export all properties return.fields:[] # nebula space partition number partition:10 } ... ] # Processing edges # There are edge config examples for different dataSources. edges: [ # export NebulaGraph tag data to csv, only support export to CSV for now. { name: follow type: { source: Nebula sink: CSV } # the path to save the NebulaGrpah data, make sure the path doesn't exist. path:\"hdfs://192.168.8.177:9000/edge/follow\" # if no need to export any properties when export NebulaGraph edge data # if noField is configured true, just export src,dst,rank noField:false # define properties to export from NebulaGraph edge data # if return.fields is configured as empty list, then export all properties return.fields:[] # nebula space partition number partition:10 } ... ] } Export data from Nebula Graph with the following command. <spark_install_path>/bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange nebula-exchange-x.y.z.jar_path> -c <export_application.conf_path> The command used in this example is as follows. $ ./spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange \\ ~/exchange-ent/nebula-exchange-ent-3.0.0.jar -c ~/exchange-ent/export_application.conf Check the exported data. Check whether the CSV file is successfully generated under the target path. $ hadoop fs -ls /vertex/player Found 11 items -rw-r--r-- 3 nebula supergroup 0 2021 -11-05 07 :36 /vertex/player/_SUCCESS -rw-r--r-- 3 nebula supergroup 160 2021 -11-05 07 :36 /vertex/player/ part-00000-17293020-ba2e-4243-b834-34495c0536b3-c000.csv -rw-r--r-- 3 nebula supergroup 163 2021 -11-05 07 :36 /vertex/player/ part-00001-17293020-ba2e-4243-b834-34495c0536b3-c000.csv -rw-r--r-- 3 nebula supergroup 172 2021 -11-05 07 :36 /vertex/player/ part-00002-17293020-ba2e-4243-b834-34495c0536b3-c000.csv -rw-r--r-- 3 nebula supergroup 172 2021 -11-05 07 :36 /vertex/player/ part-00003-17293020-ba2e-4243-b834-34495c0536b3-c000.csv -rw-r--r-- 3 nebula supergroup 144 2021 -11-05 07 :36 /vertex/player/ part-00004-17293020-ba2e-4243-b834-34495c0536b3-c000.csv -rw-r--r-- 3 nebula supergroup 173 2021 -11-05 07 :36 /vertex/player/ part-00005-17293020-ba2e-4243-b834-34495c0536b3-c000.csv -rw-r--r-- 3 nebula supergroup 160 2021 -11-05 07 :36 /vertex/player/ part-00006-17293020-ba2e-4243-b834-34495c0536b3-c000.csv -rw-r--r-- 3 nebula supergroup 148 2021 -11-05 07 :36 /vertex/player/ part-00007-17293020-ba2e-4243-b834-34495c0536b3-c000.csv -rw-r--r-- 3 nebula supergroup 125 2021 -11-05 07 :36 /vertex/player/ part-00008-17293020-ba2e-4243-b834-34495c0536b3-c000.csv -rw-r--r-- 3 nebula supergroup 119 2021 -11-05 07 :36 /vertex/player/ part-00009-17293020-ba2e-4243-b834-34495c0536b3-c000.csv Check the contents of the CSV file to ensure that the data export is successful.","title":"Steps"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-clickhouse/","text":"Import data from ClickHouse \u00b6 This topic provides an example of how to use Exchange to import data stored on ClickHouse into Nebula Graph. Data set \u00b6 This topic takes the basketballplayer dataset as an example. Environment \u00b6 This example is done on MacOS. Here is the environment configuration information: Hardware specifications: CPU: 1.7 GHz Quad-Core Intel Core i7 Memory: 16 GB Spark: 2.4.7, stand-alone Hadoop: 2.9.2, pseudo-distributed deployment ClickHouse: docker deployment yandex/clickhouse-server tag: latest(2021.07.01) Nebula Graph: 3.1.0. Deploy Nebula Graph with Docker Compose . Prerequisites \u00b6 Before importing data, you need to confirm the following information: Nebula Graph has been installed and deployed with the following information: IP addresses and ports of Graph and Meta services. The user name and password with write permission to Nebula Graph. Exchange has been compiled , or download the compiled .jar file directly. Spark has been installed. Learn about the Schema created in Nebula Graph, including names and properties of Tags and Edge types, and more. The Hadoop service has been installed and started. Steps \u00b6 Step 1: Create the Schema in Nebula Graph \u00b6 Analyze the data to create a Schema in Nebula Graph by following these steps: Identify the Schema elements. The Schema elements in the Nebula Graph are shown in the following table. Element Name Property Tag player name string, age int Tag team name string Edge Type follow degree int Edge Type serve start_year int, end_year int Create a graph space basketballplayer in the Nebula Graph and create a Schema as shown below. ## Create a graph space. nebula> CREATE SPACE basketballplayer \\ (partition_num = 10, \\ replica_factor = 1, \\ vid_type = FIXED_STRING(30)); ## Use the graph space basketballplayer. nebula> USE basketballplayer; ## Create the Tag player. nebula> CREATE TAG player(name string, age int); ## Create the Tag team. nebula> CREATE TAG team(name string); ## Create the Edge type follow. nebula> CREATE EDGE follow(degree int); ## Create the Edge type serve. nebula> CREATE EDGE serve(start_year int, end_year int); For more information, see Quick start workflow . Step 2: Modify configuration files \u00b6 After Exchange is compiled, copy the conf file target/classes/application.conf to set ClickHouse data source configuration. In this example, the copied file is called clickhouse_application.conf . For details on each configuration item, see Parameters in the configuration file . { # Spark configuration spark: { app: { name: Nebula Exchange 3.0.0 } driver: { cores: 1 maxResultSize: 1G } cores: { max: 16 } } # Nebula Graph configuration nebula: { address:{ # Specify the IP addresses and ports for Graph and Meta services. # If there are multiple addresses, the format is \"ip1:port\",\"ip2:port\",\"ip3:port\". # Addresses are separated by commas. graph:[\"127.0.0.1:9669\"] meta:[\"127.0.0.1:9559\"] } # The account entered must have write permission for the Nebula Graph space. user: root pswd: nebula # Fill in the name of the graph space you want to write data to in the Nebula Graph. space: basketballplayer connection: { timeout: 3000 retry: 3 } execution: { retry: 3 } error: { max: 32 output: /tmp/errors } rate: { limit: 1024 timeout: 1000 } } # Processing vertexes tags: [ # Set the information about the Tag player. { name: player type: { # Specify the data source file format to ClickHouse. source: clickhouse # Specify how to import the data of vertexes into Nebula Graph: Client or SST. sink: client } # JDBC URL of ClickHouse url:\"jdbc:clickhouse://192.168.*.*:8123/basketballplayer\" user:\"user\" password:\"123456\" # The number of ClickHouse partitions numPartition:\"5\" sentence:\"select * from player\" # Specify the column names in the player table in fields, and their corresponding values are specified as properties in the Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. # If multiple column names need to be specified, separate them by commas. fields: [name,age] nebula.fields: [name,age] # Specify a column of data in the table as the source of vertex VID in the Nebula Graph. vertex: { field:playerid # policy:hash } # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 } # Set the information about the Tag Team. { name: team type: { source: clickhouse sink: client } url:\"jdbc:clickhouse://192.168.*.*:8123/basketballplayer\" user:\"user\" password:\"123456\" numPartition:\"5\" sentence:\"select * from team\" fields: [name] nebula.fields: [name] vertex: { field:teamid } batch: 256 partition: 32 } ] # Processing edges edges: [ # Set the information about the Edge Type follow. { # The corresponding Edge Type name in Nebula Graph. name: follow type: { # Specify the data source file format to ClickHouse. source: clickhouse # Specify how to import the data into Nebula Graph: Client or SST. sink: client } # JDBC URL of ClickHouse url:\"jdbc:clickhouse://192.168.*.*:8123/basketballplayer\" user:\"user\" password:\"123456\" # The number of ClickHouse partitions. numPartition:\"5\" sentence:\"select * from follow\" # Specify the column names in the follow table in fields, and their corresponding values are specified as properties in the Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. # If multiple column names need to be specified, separate them by commas. fields: [degree] nebula.fields: [degree] # In source, use a column in the follow table as the source of the edge's source vertexes. source: { field:src_player } # In target, use a column in the follow table as the source of the edge's destination vertexes. target: { field:dst_player } # (Optional) Specify a column as the source of the rank. #ranking: rank # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 } # Set the information about the Edge Type serve. { name: serve type: { source: clickhouse sink: client } url:\"jdbc:clickhouse://192.168.*.*:8123/basketballplayer\" user:\"user\" password:\"123456\" numPartition:\"5\" sentence:\"select * from serve\" fields: [start_year,end_year] nebula.fields: [start_year,end_year] source: { field:playerid } target: { field:teamid } # (Optional) Specify a column as the source of the rank. #ranking: rank batch: 256 partition: 32 } ] } Step 3: Import data into Nebula Graph \u00b6 Run the following command to import ClickHouse data into Nebula Graph. For descriptions of the parameters, see Options for import . ${ SPARK_HOME } /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange <nebula-exchange-3.0.0.jar_path> -c <clickhouse_application.conf_path> Note JAR packages are available in two ways: compiled them yourself , or download the compiled .jar file directly. For example: ${ SPARK_HOME } /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange /root/nebula-exchange/nebula-exchange/target/nebula-exchange-3.0.0.jar -c /root/nebula-exchange/nebula-exchange/target/classes/clickhouse_application.conf You can search for batchSuccess.<tag_name/edge_name> in the command output to check the number of successes. For example, batchSuccess.follow: 300 . Step 4: (optional) Validate data \u00b6 Users can verify that data has been imported by executing a query in the Nebula Graph client (for example, Nebula Graph Studio). For example: GO FROM \"player100\" OVER follow; Users can also run the SHOW STATS command to view statistics. Step 5: (optional) Rebuild indexes in Nebula Graph \u00b6 With the data imported, users can recreate and rebuild indexes in Nebula Graph. For details, see Index overview .","title":"Import data from ClickHouse"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-clickhouse/#import_data_from_clickhouse","text":"This topic provides an example of how to use Exchange to import data stored on ClickHouse into Nebula Graph.","title":"Import data from ClickHouse"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-clickhouse/#data_set","text":"This topic takes the basketballplayer dataset as an example.","title":"Data set"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-clickhouse/#environment","text":"This example is done on MacOS. Here is the environment configuration information: Hardware specifications: CPU: 1.7 GHz Quad-Core Intel Core i7 Memory: 16 GB Spark: 2.4.7, stand-alone Hadoop: 2.9.2, pseudo-distributed deployment ClickHouse: docker deployment yandex/clickhouse-server tag: latest(2021.07.01) Nebula Graph: 3.1.0. Deploy Nebula Graph with Docker Compose .","title":"Environment"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-clickhouse/#prerequisites","text":"Before importing data, you need to confirm the following information: Nebula Graph has been installed and deployed with the following information: IP addresses and ports of Graph and Meta services. The user name and password with write permission to Nebula Graph. Exchange has been compiled , or download the compiled .jar file directly. Spark has been installed. Learn about the Schema created in Nebula Graph, including names and properties of Tags and Edge types, and more. The Hadoop service has been installed and started.","title":"Prerequisites"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-clickhouse/#steps","text":"","title":"Steps"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-clickhouse/#step_1_create_the_schema_in_nebula_graph","text":"Analyze the data to create a Schema in Nebula Graph by following these steps: Identify the Schema elements. The Schema elements in the Nebula Graph are shown in the following table. Element Name Property Tag player name string, age int Tag team name string Edge Type follow degree int Edge Type serve start_year int, end_year int Create a graph space basketballplayer in the Nebula Graph and create a Schema as shown below. ## Create a graph space. nebula> CREATE SPACE basketballplayer \\ (partition_num = 10, \\ replica_factor = 1, \\ vid_type = FIXED_STRING(30)); ## Use the graph space basketballplayer. nebula> USE basketballplayer; ## Create the Tag player. nebula> CREATE TAG player(name string, age int); ## Create the Tag team. nebula> CREATE TAG team(name string); ## Create the Edge type follow. nebula> CREATE EDGE follow(degree int); ## Create the Edge type serve. nebula> CREATE EDGE serve(start_year int, end_year int); For more information, see Quick start workflow .","title":"Step 1: Create the Schema in Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-clickhouse/#step_2_modify_configuration_files","text":"After Exchange is compiled, copy the conf file target/classes/application.conf to set ClickHouse data source configuration. In this example, the copied file is called clickhouse_application.conf . For details on each configuration item, see Parameters in the configuration file . { # Spark configuration spark: { app: { name: Nebula Exchange 3.0.0 } driver: { cores: 1 maxResultSize: 1G } cores: { max: 16 } } # Nebula Graph configuration nebula: { address:{ # Specify the IP addresses and ports for Graph and Meta services. # If there are multiple addresses, the format is \"ip1:port\",\"ip2:port\",\"ip3:port\". # Addresses are separated by commas. graph:[\"127.0.0.1:9669\"] meta:[\"127.0.0.1:9559\"] } # The account entered must have write permission for the Nebula Graph space. user: root pswd: nebula # Fill in the name of the graph space you want to write data to in the Nebula Graph. space: basketballplayer connection: { timeout: 3000 retry: 3 } execution: { retry: 3 } error: { max: 32 output: /tmp/errors } rate: { limit: 1024 timeout: 1000 } } # Processing vertexes tags: [ # Set the information about the Tag player. { name: player type: { # Specify the data source file format to ClickHouse. source: clickhouse # Specify how to import the data of vertexes into Nebula Graph: Client or SST. sink: client } # JDBC URL of ClickHouse url:\"jdbc:clickhouse://192.168.*.*:8123/basketballplayer\" user:\"user\" password:\"123456\" # The number of ClickHouse partitions numPartition:\"5\" sentence:\"select * from player\" # Specify the column names in the player table in fields, and their corresponding values are specified as properties in the Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. # If multiple column names need to be specified, separate them by commas. fields: [name,age] nebula.fields: [name,age] # Specify a column of data in the table as the source of vertex VID in the Nebula Graph. vertex: { field:playerid # policy:hash } # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 } # Set the information about the Tag Team. { name: team type: { source: clickhouse sink: client } url:\"jdbc:clickhouse://192.168.*.*:8123/basketballplayer\" user:\"user\" password:\"123456\" numPartition:\"5\" sentence:\"select * from team\" fields: [name] nebula.fields: [name] vertex: { field:teamid } batch: 256 partition: 32 } ] # Processing edges edges: [ # Set the information about the Edge Type follow. { # The corresponding Edge Type name in Nebula Graph. name: follow type: { # Specify the data source file format to ClickHouse. source: clickhouse # Specify how to import the data into Nebula Graph: Client or SST. sink: client } # JDBC URL of ClickHouse url:\"jdbc:clickhouse://192.168.*.*:8123/basketballplayer\" user:\"user\" password:\"123456\" # The number of ClickHouse partitions. numPartition:\"5\" sentence:\"select * from follow\" # Specify the column names in the follow table in fields, and their corresponding values are specified as properties in the Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. # If multiple column names need to be specified, separate them by commas. fields: [degree] nebula.fields: [degree] # In source, use a column in the follow table as the source of the edge's source vertexes. source: { field:src_player } # In target, use a column in the follow table as the source of the edge's destination vertexes. target: { field:dst_player } # (Optional) Specify a column as the source of the rank. #ranking: rank # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 } # Set the information about the Edge Type serve. { name: serve type: { source: clickhouse sink: client } url:\"jdbc:clickhouse://192.168.*.*:8123/basketballplayer\" user:\"user\" password:\"123456\" numPartition:\"5\" sentence:\"select * from serve\" fields: [start_year,end_year] nebula.fields: [start_year,end_year] source: { field:playerid } target: { field:teamid } # (Optional) Specify a column as the source of the rank. #ranking: rank batch: 256 partition: 32 } ] }","title":"Step 2: Modify configuration files"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-clickhouse/#step_3_import_data_into_nebula_graph","text":"Run the following command to import ClickHouse data into Nebula Graph. For descriptions of the parameters, see Options for import . ${ SPARK_HOME } /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange <nebula-exchange-3.0.0.jar_path> -c <clickhouse_application.conf_path> Note JAR packages are available in two ways: compiled them yourself , or download the compiled .jar file directly. For example: ${ SPARK_HOME } /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange /root/nebula-exchange/nebula-exchange/target/nebula-exchange-3.0.0.jar -c /root/nebula-exchange/nebula-exchange/target/classes/clickhouse_application.conf You can search for batchSuccess.<tag_name/edge_name> in the command output to check the number of successes. For example, batchSuccess.follow: 300 .","title":"Step 3: Import data into Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-clickhouse/#step_4_optional_validate_data","text":"Users can verify that data has been imported by executing a query in the Nebula Graph client (for example, Nebula Graph Studio). For example: GO FROM \"player100\" OVER follow; Users can also run the SHOW STATS command to view statistics.","title":"Step 4: (optional) Validate data"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-clickhouse/#step_5_optional_rebuild_indexes_in_nebula_graph","text":"With the data imported, users can recreate and rebuild indexes in Nebula Graph. For details, see Index overview .","title":"Step 5: (optional) Rebuild indexes in Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-csv/","text":"Import data from CSV files \u00b6 This topic provides an example of how to use Exchange to import Nebula Graph data stored in HDFS or local CSV files. To import a local CSV file to Nebula Graph, see Nebula Importer . Data set \u00b6 This topic takes the basketballplayer dataset as an example. Environment \u00b6 This example is done on MacOS. Here is the environment configuration information: Hardware specifications: CPU: 1.7 GHz Quad-Core Intel Core i7 Memory: 16 GB Spark: 2.4.7, stand-alone Hadoop: 2.9.2, pseudo-distributed deployment Nebula Graph: 3.1.0. Deploy Nebula Graph with Docker Compose . Prerequisites \u00b6 Before importing data, you need to confirm the following information: Nebula Graph has been installed and deployed with the following information: IP addresses and ports of Graph and Meta services. The user name and password with write permission to Nebula Graph. Exchange has been compiled , or download the compiled .jar file directly. Spark has been installed. Learn about the Schema created in Nebula Graph, including names and properties of Tags and Edge types, and more. If files are stored in HDFS, ensure that the Hadoop service is running normally. If files are stored locally and Nebula Graph is a cluster architecture, you need to place the files in the same directory locally on each machine in the cluster. Steps \u00b6 Step 1: Create the Schema in Nebula Graph \u00b6 Analyze the data to create a Schema in Nebula Graph by following these steps: Identify the Schema elements. The Schema elements in the Nebula Graph are shown in the following table. Element Name Property Tag player name string, age int Tag team name string Edge Type follow degree int Edge Type serve start_year int, end_year int Create a graph space basketballplayer in the Nebula Graph and create a Schema as shown below. ## Create a graph space. nebula> CREATE SPACE basketballplayer \\ (partition_num = 10, \\ replica_factor = 1, \\ vid_type = FIXED_STRING(30)); ## Use the graph space basketballplayer. nebula> USE basketballplayer; ## Create the Tag player. nebula> CREATE TAG player(name string, age int); ## Create the Tag team. nebula> CREATE TAG team(name string); ## Create the Edge type follow. nebula> CREATE EDGE follow(degree int); ## Create the Edge type serve. nebula> CREATE EDGE serve(start_year int, end_year int); For more information, see Quick start workflow . Step 2: Process CSV files \u00b6 Confirm the following information: Process CSV files to meet Schema requirements. Note Exchange supports uploading CSV files with or without headers. Obtain the CSV file storage path. Step 3: Modify configuration files \u00b6 After Exchange is compiled, copy the conf file target/classes/application.conf to set CSV data source configuration. In this example, the copied file is called csv_application.conf . For details on each configuration item, see Parameters in the configuration file . { # Spark configuration spark: { app: { name: Nebula Exchange 3.0.0 } driver: { cores: 1 maxResultSize: 1G } executor: { memory:1G } cores: { max: 16 } } # Nebula Graph configuration nebula: { address:{ # Specify the IP addresses and ports for Graph and Meta services. # If there are multiple addresses, the format is \"ip1:port\",\"ip2:port\",\"ip3:port\". # Addresses are separated by commas. graph:[\"127.0.0.1:9669\"] meta:[\"127.0.0.1:9559\"] } # The account entered must have write permission for the Nebula Graph space. user: root pswd: nebula # Fill in the name of the graph space you want to write data to in the Nebula Graph. space: basketballplayer connection: { timeout: 3000 retry: 3 } execution: { retry: 3 } error: { max: 32 output: /tmp/errors } rate: { limit: 1024 timeout: 1000 } } # Processing vertexes tags: [ # Set the information about the Tag player. { # Specify the Tag name defined in Nebula Graph. name: player type: { # Specify the data source file format to CSV. source: csv # Specify how to import the data into Nebula Graph: Client or SST. sink: client } # Specify the path to the CSV file. # If the file is stored in HDFS, use double quotation marks to enclose the file path, starting with hdfs://. For example: \"hdfs://ip:port/xx/xx\". # If the file is stored locally, use double quotation marks to enclose the file path, starting with file://. For example: \"file:///tmp/xx.csv\". path: \"hdfs://192.168.*.*:9000/data/vertex_player.csv\" # If the CSV file does not have a header, use [_c0, _c1, _c2, ..., _cn] to represent its header and indicate the columns as the source of the property values. # If the CSV file has headers, use the actual column names. fields: [_c1, _c2] # Specify the column names in the player table in fields, and their corresponding values are specified as properties in the Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. nebula.fields: [age, name] # Specify a column of data in the table as the source of vertex VID in the Nebula Graph. # The value of vertex must be the same as the column names in the above fields or csv.fields. # Currently, Nebula Graph 3.1.0 supports only strings or integers of VID. vertex: { field:_c0 # policy:hash } # The delimiter specified. The default value is comma. separator: \",\" # If the CSV file has a header, set the header to true. # If the CSV file does not have a header, set the header to false. The default value is false. header: false # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 } # Set the information about the Tag Team. { # Specify the Tag name defined in Nebula Graph. name: team type: { # Specify the data source file format to CSV. source: csv # Specify how to import the data into Nebula Graph: Client or SST. sink: client } # Specify the path to the CSV file. # If the file is stored in HDFS, use double quotation marks to enclose the file path, starting with hdfs://. For example: \"hdfs://ip:port/xx/xx\". # If the file is stored locally, use double quotation marks to enclose the file path, starting with file://. For example: \"file:///tmp/xx.csv\". path: \"hdfs://192.168.*.*:9000/data/vertex_team.csv\" # If the CSV file does not have a header, use [_c0, _c1, _c2, ..., _cn] to represent its header and indicate the columns as the source of the property values. # If the CSV file has headers, use the actual column names. fields: [_c1] # Specify the column names in the player table in fields, and their corresponding values are specified as properties in the Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. nebula.fields: [name] # Specify a column of data in the table as the source of VIDs in the Nebula Graph. # The value of vertex must be the same as the column names in the above fields or csv.fields. # Currently, Nebula Graph 3.1.0 supports only strings or integers of VID. vertex: { field:_c0 # policy:hash } # The delimiter specified. The default value is comma. separator: \",\" # If the CSV file has a header, set the header to true. # If the CSV file does not have a header, set the header to false. The default value is false. header: false # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 } # If more vertexes need to be added, refer to the previous configuration to add them. ] # Processing edges edges: [ # Set the information about the Edge Type follow. { # Specify the Edge Type name defined in Nebula Graph. name: follow type: { # Specify the data source file format to CSV. source: csv # Specify how to import the data into Nebula Graph: Client or SST. sink: client } # Specify the path to the CSV file. # If the file is stored in HDFS, use double quotation marks to enclose the file path, starting with hdfs://. For example: \"hdfs://ip:port/xx/xx\". # If the file is stored locally, use double quotation marks to enclose the file path, starting with file://. For example: \"file:///tmp/xx.csv\". path: \"hdfs://192.168.*.*:9000/data/edge_follow.csv\" # If the CSV file does not have a header, use [_c0, _c1, _c2, ..., _cn] to represent its header and indicate the columns as the source of the property values. # If the CSV file has headers, use the actual column names. fields: [_c2] # Specify the column names in the edge table in fields, and their corresponding values are specified as properties in the Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. nebula.fields: [degree] # Specify a column as the source for the source and destination vertexes. # The value of vertex must be the same as the column names in the above fields or csv.fields. # Currently, Nebula Graph 3.1.0 supports only strings or integers of VID. source: { field: _c0 } target: { field: _c1 } # The delimiter specified. The default value is comma. separator: \",\" # Specify a column as the source of the rank (optional). #ranking: rank # If the CSV file has a header, set the header to true. # If the CSV file does not have a header, set the header to false. The default value is false. header: false # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 } # Set the information about the Edge Type serve. { # Specify the Edge Type name defined in Nebula Graph. name: serve type: { # Specify the data source file format to CSV. source: csv # Specify how to import the data into Nebula Graph: Client or SST. sink: client } # Specify the path to the CSV file. # If the file is stored in HDFS, use double quotation marks to enclose the file path, starting with hdfs://. For example: \"hdfs://ip:port/xx/xx\". # If the file is stored locally, use double quotation marks to enclose the file path, starting with file://. For example: \"file:///tmp/xx.csv\". path: \"hdfs://192.168.*.*:9000/data/edge_serve.csv\" # If the CSV file does not have a header, use [_c0, _c1, _c2, ..., _cn] to represent its header and indicate the columns as the source of the property values. # If the CSV file has headers, use the actual column names. fields: [_c2,_c3] # Specify the column names in the edge table in fields, and their corresponding values are specified as properties in the Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. nebula.fields: [start_year, end_year] # Specify a column as the source for the source and destination vertexes. # The value of vertex must be the same as the column names in the above fields or csv.fields. # Currently, Nebula Graph 3.1.0 supports only strings or integers of VID. source: { field: _c0 } target: { field: _c1 } # The delimiter specified. The default value is comma. separator: \",\" # Specify a column as the source of the rank (optional). #ranking: _c5 # If the CSV file has a header, set the header to true. # If the CSV file does not have a header, set the header to false. The default value is false. header: false # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 } ] # If more edges need to be added, refer to the previous configuration to add them. } Step 4: Import data into Nebula Graph \u00b6 Run the following command to import CSV data into Nebula Graph. For descriptions of the parameters, see Options for import . ${ SPARK_HOME } /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange <nebula-exchange-3.0.0.jar_path> -c <csv_application.conf_path> Note JAR packages are available in two ways: compiled them yourself , or download the compiled .jar file directly. For example: ${ SPARK_HOME } /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange /root/nebula-exchange/nebula-exchange/target/nebula-exchange-3.0.0.jar -c /root/nebula-exchange/nebula-exchange/target/classes/csv_application.conf You can search for batchSuccess.<tag_name/edge_name> in the command output to check the number of successes. For example, batchSuccess.follow: 300 . Step 5: (optional) Validate data \u00b6 Users can verify that data has been imported by executing a query in the Nebula Graph client (for example, Nebula Graph Studio). For example: GO FROM \"player100\" OVER follow; Users can also run the SHOW STATS command to view statistics. Step 6: (optional) Rebuild indexes in Nebula Graph \u00b6 With the data imported, users can recreate and rebuild indexes in Nebula Graph. For details, see Index overview .","title":"Import data from CSV files"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-csv/#import_data_from_csv_files","text":"This topic provides an example of how to use Exchange to import Nebula Graph data stored in HDFS or local CSV files. To import a local CSV file to Nebula Graph, see Nebula Importer .","title":"Import data from CSV files"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-csv/#data_set","text":"This topic takes the basketballplayer dataset as an example.","title":"Data set"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-csv/#environment","text":"This example is done on MacOS. Here is the environment configuration information: Hardware specifications: CPU: 1.7 GHz Quad-Core Intel Core i7 Memory: 16 GB Spark: 2.4.7, stand-alone Hadoop: 2.9.2, pseudo-distributed deployment Nebula Graph: 3.1.0. Deploy Nebula Graph with Docker Compose .","title":"Environment"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-csv/#prerequisites","text":"Before importing data, you need to confirm the following information: Nebula Graph has been installed and deployed with the following information: IP addresses and ports of Graph and Meta services. The user name and password with write permission to Nebula Graph. Exchange has been compiled , or download the compiled .jar file directly. Spark has been installed. Learn about the Schema created in Nebula Graph, including names and properties of Tags and Edge types, and more. If files are stored in HDFS, ensure that the Hadoop service is running normally. If files are stored locally and Nebula Graph is a cluster architecture, you need to place the files in the same directory locally on each machine in the cluster.","title":"Prerequisites"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-csv/#steps","text":"","title":"Steps"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-csv/#step_1_create_the_schema_in_nebula_graph","text":"Analyze the data to create a Schema in Nebula Graph by following these steps: Identify the Schema elements. The Schema elements in the Nebula Graph are shown in the following table. Element Name Property Tag player name string, age int Tag team name string Edge Type follow degree int Edge Type serve start_year int, end_year int Create a graph space basketballplayer in the Nebula Graph and create a Schema as shown below. ## Create a graph space. nebula> CREATE SPACE basketballplayer \\ (partition_num = 10, \\ replica_factor = 1, \\ vid_type = FIXED_STRING(30)); ## Use the graph space basketballplayer. nebula> USE basketballplayer; ## Create the Tag player. nebula> CREATE TAG player(name string, age int); ## Create the Tag team. nebula> CREATE TAG team(name string); ## Create the Edge type follow. nebula> CREATE EDGE follow(degree int); ## Create the Edge type serve. nebula> CREATE EDGE serve(start_year int, end_year int); For more information, see Quick start workflow .","title":"Step 1: Create the Schema in Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-csv/#step_2_process_csv_files","text":"Confirm the following information: Process CSV files to meet Schema requirements. Note Exchange supports uploading CSV files with or without headers. Obtain the CSV file storage path.","title":"Step 2: Process CSV files"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-csv/#step_3_modify_configuration_files","text":"After Exchange is compiled, copy the conf file target/classes/application.conf to set CSV data source configuration. In this example, the copied file is called csv_application.conf . For details on each configuration item, see Parameters in the configuration file . { # Spark configuration spark: { app: { name: Nebula Exchange 3.0.0 } driver: { cores: 1 maxResultSize: 1G } executor: { memory:1G } cores: { max: 16 } } # Nebula Graph configuration nebula: { address:{ # Specify the IP addresses and ports for Graph and Meta services. # If there are multiple addresses, the format is \"ip1:port\",\"ip2:port\",\"ip3:port\". # Addresses are separated by commas. graph:[\"127.0.0.1:9669\"] meta:[\"127.0.0.1:9559\"] } # The account entered must have write permission for the Nebula Graph space. user: root pswd: nebula # Fill in the name of the graph space you want to write data to in the Nebula Graph. space: basketballplayer connection: { timeout: 3000 retry: 3 } execution: { retry: 3 } error: { max: 32 output: /tmp/errors } rate: { limit: 1024 timeout: 1000 } } # Processing vertexes tags: [ # Set the information about the Tag player. { # Specify the Tag name defined in Nebula Graph. name: player type: { # Specify the data source file format to CSV. source: csv # Specify how to import the data into Nebula Graph: Client or SST. sink: client } # Specify the path to the CSV file. # If the file is stored in HDFS, use double quotation marks to enclose the file path, starting with hdfs://. For example: \"hdfs://ip:port/xx/xx\". # If the file is stored locally, use double quotation marks to enclose the file path, starting with file://. For example: \"file:///tmp/xx.csv\". path: \"hdfs://192.168.*.*:9000/data/vertex_player.csv\" # If the CSV file does not have a header, use [_c0, _c1, _c2, ..., _cn] to represent its header and indicate the columns as the source of the property values. # If the CSV file has headers, use the actual column names. fields: [_c1, _c2] # Specify the column names in the player table in fields, and their corresponding values are specified as properties in the Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. nebula.fields: [age, name] # Specify a column of data in the table as the source of vertex VID in the Nebula Graph. # The value of vertex must be the same as the column names in the above fields or csv.fields. # Currently, Nebula Graph 3.1.0 supports only strings or integers of VID. vertex: { field:_c0 # policy:hash } # The delimiter specified. The default value is comma. separator: \",\" # If the CSV file has a header, set the header to true. # If the CSV file does not have a header, set the header to false. The default value is false. header: false # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 } # Set the information about the Tag Team. { # Specify the Tag name defined in Nebula Graph. name: team type: { # Specify the data source file format to CSV. source: csv # Specify how to import the data into Nebula Graph: Client or SST. sink: client } # Specify the path to the CSV file. # If the file is stored in HDFS, use double quotation marks to enclose the file path, starting with hdfs://. For example: \"hdfs://ip:port/xx/xx\". # If the file is stored locally, use double quotation marks to enclose the file path, starting with file://. For example: \"file:///tmp/xx.csv\". path: \"hdfs://192.168.*.*:9000/data/vertex_team.csv\" # If the CSV file does not have a header, use [_c0, _c1, _c2, ..., _cn] to represent its header and indicate the columns as the source of the property values. # If the CSV file has headers, use the actual column names. fields: [_c1] # Specify the column names in the player table in fields, and their corresponding values are specified as properties in the Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. nebula.fields: [name] # Specify a column of data in the table as the source of VIDs in the Nebula Graph. # The value of vertex must be the same as the column names in the above fields or csv.fields. # Currently, Nebula Graph 3.1.0 supports only strings or integers of VID. vertex: { field:_c0 # policy:hash } # The delimiter specified. The default value is comma. separator: \",\" # If the CSV file has a header, set the header to true. # If the CSV file does not have a header, set the header to false. The default value is false. header: false # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 } # If more vertexes need to be added, refer to the previous configuration to add them. ] # Processing edges edges: [ # Set the information about the Edge Type follow. { # Specify the Edge Type name defined in Nebula Graph. name: follow type: { # Specify the data source file format to CSV. source: csv # Specify how to import the data into Nebula Graph: Client or SST. sink: client } # Specify the path to the CSV file. # If the file is stored in HDFS, use double quotation marks to enclose the file path, starting with hdfs://. For example: \"hdfs://ip:port/xx/xx\". # If the file is stored locally, use double quotation marks to enclose the file path, starting with file://. For example: \"file:///tmp/xx.csv\". path: \"hdfs://192.168.*.*:9000/data/edge_follow.csv\" # If the CSV file does not have a header, use [_c0, _c1, _c2, ..., _cn] to represent its header and indicate the columns as the source of the property values. # If the CSV file has headers, use the actual column names. fields: [_c2] # Specify the column names in the edge table in fields, and their corresponding values are specified as properties in the Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. nebula.fields: [degree] # Specify a column as the source for the source and destination vertexes. # The value of vertex must be the same as the column names in the above fields or csv.fields. # Currently, Nebula Graph 3.1.0 supports only strings or integers of VID. source: { field: _c0 } target: { field: _c1 } # The delimiter specified. The default value is comma. separator: \",\" # Specify a column as the source of the rank (optional). #ranking: rank # If the CSV file has a header, set the header to true. # If the CSV file does not have a header, set the header to false. The default value is false. header: false # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 } # Set the information about the Edge Type serve. { # Specify the Edge Type name defined in Nebula Graph. name: serve type: { # Specify the data source file format to CSV. source: csv # Specify how to import the data into Nebula Graph: Client or SST. sink: client } # Specify the path to the CSV file. # If the file is stored in HDFS, use double quotation marks to enclose the file path, starting with hdfs://. For example: \"hdfs://ip:port/xx/xx\". # If the file is stored locally, use double quotation marks to enclose the file path, starting with file://. For example: \"file:///tmp/xx.csv\". path: \"hdfs://192.168.*.*:9000/data/edge_serve.csv\" # If the CSV file does not have a header, use [_c0, _c1, _c2, ..., _cn] to represent its header and indicate the columns as the source of the property values. # If the CSV file has headers, use the actual column names. fields: [_c2,_c3] # Specify the column names in the edge table in fields, and their corresponding values are specified as properties in the Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. nebula.fields: [start_year, end_year] # Specify a column as the source for the source and destination vertexes. # The value of vertex must be the same as the column names in the above fields or csv.fields. # Currently, Nebula Graph 3.1.0 supports only strings or integers of VID. source: { field: _c0 } target: { field: _c1 } # The delimiter specified. The default value is comma. separator: \",\" # Specify a column as the source of the rank (optional). #ranking: _c5 # If the CSV file has a header, set the header to true. # If the CSV file does not have a header, set the header to false. The default value is false. header: false # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 } ] # If more edges need to be added, refer to the previous configuration to add them. }","title":"Step 3: Modify configuration files"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-csv/#step_4_import_data_into_nebula_graph","text":"Run the following command to import CSV data into Nebula Graph. For descriptions of the parameters, see Options for import . ${ SPARK_HOME } /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange <nebula-exchange-3.0.0.jar_path> -c <csv_application.conf_path> Note JAR packages are available in two ways: compiled them yourself , or download the compiled .jar file directly. For example: ${ SPARK_HOME } /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange /root/nebula-exchange/nebula-exchange/target/nebula-exchange-3.0.0.jar -c /root/nebula-exchange/nebula-exchange/target/classes/csv_application.conf You can search for batchSuccess.<tag_name/edge_name> in the command output to check the number of successes. For example, batchSuccess.follow: 300 .","title":"Step 4: Import data into Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-csv/#step_5_optional_validate_data","text":"Users can verify that data has been imported by executing a query in the Nebula Graph client (for example, Nebula Graph Studio). For example: GO FROM \"player100\" OVER follow; Users can also run the SHOW STATS command to view statistics.","title":"Step 5: (optional) Validate data"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-csv/#step_6_optional_rebuild_indexes_in_nebula_graph","text":"With the data imported, users can recreate and rebuild indexes in Nebula Graph. For details, see Index overview .","title":"Step 6: (optional) Rebuild indexes in Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-hbase/","text":"Import data from HBase \u00b6 This topic provides an example of how to use Exchange to import Nebula Graph data stored in HBase. Data set \u00b6 This topic takes the basketballplayer dataset as an example. In this example, the data set has been stored in HBase. All vertexes and edges are stored in the player , team , follow , and serve tables. The following are some of the data for each table. hbase ( main ): 002 : 0 > scan \"player\" ROW COLUMN + CELL player100 column = cf : age , timestamp = 1618881347530 , value = 42 player100 column = cf : name , timestamp = 1618881354604 , value = Tim Duncan player101 column = cf : age , timestamp = 1618881369124 , value = 36 player101 column = cf : name , timestamp = 1618881379102 , value = Tony Parker player102 column = cf : age , timestamp = 1618881386987 , value = 33 player102 column = cf : name , timestamp = 1618881393370 , value = LaMarcus Aldridge player103 column = cf : age , timestamp = 1618881402002 , value = 32 player103 column = cf : name , timestamp = 1618881407882 , value = Rudy Gay ... hbase ( main ): 003 : 0 > scan \"team\" ROW COLUMN + CELL team200 column = cf : name , timestamp = 1618881445563 , value = Warriors team201 column = cf : name , timestamp = 1618881453636 , value = Nuggets ... hbase ( main ): 004 : 0 > scan \"follow\" ROW COLUMN + CELL player100 column = cf : degree , timestamp = 1618881804853 , value = 95 player100 column = cf : dst_player , timestamp = 1618881791522 , value = player101 player101 column = cf : degree , timestamp = 1618881824685 , value = 90 player101 column = cf : dst_player , timestamp = 1618881816042 , value = player102 ... hbase ( main ): 005 : 0 > scan \"serve\" ROW COLUMN + CELL player100 column = cf : end_year , timestamp = 1618881899333 , value = 2016 player100 column = cf : start_year , timestamp = 1618881890117 , value = 1997 player100 column = cf : teamid , timestamp = 1618881875739 , value = team204 ... Environment \u00b6 This example is done on MacOS. Here is the environment configuration information: Hardware specifications: CPU: 1.7 GHz Quad-Core Intel Core i7 Memory: 16 GB Spark: 2.4.7, stand-alone Hadoop: 2.9.2, pseudo-distributed deployment HBase: 2.2.7 Nebula Graph: 3.1.0. Deploy Nebula Graph with Docker Compose . Prerequisites \u00b6 Before importing data, you need to confirm the following information: Nebula Graph has been installed and deployed with the following information: IP addresses and ports of Graph and Meta services. The user name and password with write permission to Nebula Graph. Exchange has been compiled , or download the compiled .jar file directly. Spark has been installed. Learn about the Schema created in Nebula Graph, including names and properties of Tags and Edge types, and more. The Hadoop service has been installed and started. Steps \u00b6 Step 1: Create the Schema in Nebula Graph \u00b6 Analyze the data to create a Schema in Nebula Graph by following these steps: Identify the Schema elements. The Schema elements in the Nebula Graph are shown in the following table. Element Name Property Tag player name string, age int Tag team name string Edge Type follow degree int Edge Type serve start_year int, end_year int Create a graph space basketballplayer in the Nebula Graph and create a Schema as shown below. ## Create a graph space. nebula> CREATE SPACE basketballplayer \\ (partition_num = 10, \\ replica_factor = 1, \\ vid_type = FIXED_STRING(30)); ## Use the graph space basketballplayer. nebula> USE basketballplayer; ## Create the Tag player. nebula> CREATE TAG player(name string, age int); ## Create the Tag team. nebula> CREATE TAG team(name string); ## Create the Edge type follow. nebula> CREATE EDGE follow(degree int); ## Create the Edge type serve. nebula> CREATE EDGE serve(start_year int, end_year int); For more information, see Quick start workflow . Step 2: Modify configuration files \u00b6 After Exchange is compiled, copy the conf file target/classes/application.conf to set HBase data source configuration. In this example, the copied file is called hbase_application.conf . For details on each configuration item, see Parameters in the configuration file . { # Spark configuration spark: { app: { name: Nebula Exchange 3.0.0 } driver: { cores: 1 maxResultSize: 1G } cores: { max: 16 } } # Nebula Graph configuration nebula: { address:{ # Specify the IP addresses and ports for Graph and all Meta services. # If there are multiple addresses, the format is \"ip1:port\",\"ip2:port\",\"ip3:port\". # Addresses are separated by commas. graph:[\"127.0.0.1:9669\"] meta:[\"127.0.0.1:9559\"] } # The account entered must have write permission for the Nebula Graph space. user: root pswd: nebula # Fill in the name of the graph space you want to write data to in the Nebula Graph. space: basketballplayer connection: { timeout: 3000 retry: 3 } execution: { retry: 3 } error: { max: 32 output: /tmp/errors } rate: { limit: 1024 timeout: 1000 } } # Processing vertexes tags: [ # Set information about Tag player. # If you want to set RowKey as the data source, enter rowkey and the actual column name of the column family. { # The Tag name in Nebula Graph. name: player type: { # Specify the data source file format to HBase. source: hbase # Specify how to import the data into Nebula Graph: Client or SST. sink: client } host:192.168.*.* port:2181 table:\"player\" columnFamily:\"cf\" # Specify the column names in the player table in fields, and their corresponding values are specified as properties in the Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. # If multiple column names need to be specified, separate them by commas. fields: [age,name] nebula.fields: [age,name] # Specify a column of data in the table as the source of vertex VID in the Nebula Graph. # For example, if rowkey is the source of the VID, enter rowkey. vertex:{ field:rowkey } # Number of pieces of data written to Nebula Graph in a single batch. batch: 256 # Number of Spark partitions partition: 32 } # Set Tag Team information. { name: team type: { source: hbase sink: client } host:192.168.*.* port:2181 table:\"team\" columnFamily:\"cf\" fields: [name] nebula.fields: [name] vertex:{ field:rowkey } batch: 256 partition: 32 } ] # Processing edges edges: [ # Set the information about the Edge Type follow. { # The corresponding Edge Type name in Nebula Graph. name: follow type: { # Specify the data source file format to HBase. source: hbase # Specify how to import the Edge type data into Nebula Graph. # Specify how to import the data into Nebula Graph: Client or SST. sink: client } host:192.168.*.* port:2181 table:\"follow\" columnFamily:\"cf\" # Specify the column names in the follow table in fields, and their corresponding values are specified as properties in the Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. # If multiple column names need to be specified, separate them by commas. fields: [degree] nebula.fields: [degree] # In source, use a column in the follow table as the source of the edge's source vertex. # In target, use a column in the follow table as the source of the edge's destination vertex. source:{ field:rowkey } target:{ field:dst_player } # (Optional) Specify a column as the source of the rank. #ranking: rank # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 } # Set the information about the Edge Type serve. { name: serve type: { source: hbase sink: client } host:192.168.*.* port:2181 table:\"serve\" columnFamily:\"cf\" fields: [start_year,end_year] nebula.fields: [start_year,end_year] source:{ field:rowkey } target:{ field:teamid } # (Optional) Specify a column as the source of the rank. #ranking: rank batch: 256 partition: 32 } ] } Step 3: Import data into Nebula Graph \u00b6 Run the following command to import HBase data into Nebula Graph. For descriptions of the parameters, see Options for import . ${ SPARK_HOME } /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange <nebula-exchange-3.0.0.jar_path> -c <hbase_application.conf_path> Note JAR packages are available in two ways: compiled them yourself , or download the compiled .jar file directly. For example: ${ SPARK_HOME } /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange /root/nebula-exchange/nebula-exchange/target/nebula-exchange-3.0.0.jar -c /root/nebula-exchange/nebula-exchange/target/classes/hbase_application.conf You can search for batchSuccess.<tag_name/edge_name> in the command output to check the number of successes. For example, batchSuccess.follow: 300 . Step 4: (optional) Validate data \u00b6 Users can verify that data has been imported by executing a query in the Nebula Graph client (for example, Nebula Graph Studio). For example: GO FROM \"player100\" OVER follow; Users can also run the SHOW STATS command to view statistics. Step 5: (optional) Rebuild indexes in Nebula Graph \u00b6 With the data imported, users can recreate and rebuild indexes in Nebula Graph. For details, see Index overview .","title":"Import data from HBase"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-hbase/#import_data_from_hbase","text":"This topic provides an example of how to use Exchange to import Nebula Graph data stored in HBase.","title":"Import data from HBase"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-hbase/#data_set","text":"This topic takes the basketballplayer dataset as an example. In this example, the data set has been stored in HBase. All vertexes and edges are stored in the player , team , follow , and serve tables. The following are some of the data for each table. hbase ( main ): 002 : 0 > scan \"player\" ROW COLUMN + CELL player100 column = cf : age , timestamp = 1618881347530 , value = 42 player100 column = cf : name , timestamp = 1618881354604 , value = Tim Duncan player101 column = cf : age , timestamp = 1618881369124 , value = 36 player101 column = cf : name , timestamp = 1618881379102 , value = Tony Parker player102 column = cf : age , timestamp = 1618881386987 , value = 33 player102 column = cf : name , timestamp = 1618881393370 , value = LaMarcus Aldridge player103 column = cf : age , timestamp = 1618881402002 , value = 32 player103 column = cf : name , timestamp = 1618881407882 , value = Rudy Gay ... hbase ( main ): 003 : 0 > scan \"team\" ROW COLUMN + CELL team200 column = cf : name , timestamp = 1618881445563 , value = Warriors team201 column = cf : name , timestamp = 1618881453636 , value = Nuggets ... hbase ( main ): 004 : 0 > scan \"follow\" ROW COLUMN + CELL player100 column = cf : degree , timestamp = 1618881804853 , value = 95 player100 column = cf : dst_player , timestamp = 1618881791522 , value = player101 player101 column = cf : degree , timestamp = 1618881824685 , value = 90 player101 column = cf : dst_player , timestamp = 1618881816042 , value = player102 ... hbase ( main ): 005 : 0 > scan \"serve\" ROW COLUMN + CELL player100 column = cf : end_year , timestamp = 1618881899333 , value = 2016 player100 column = cf : start_year , timestamp = 1618881890117 , value = 1997 player100 column = cf : teamid , timestamp = 1618881875739 , value = team204 ...","title":"Data set"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-hbase/#environment","text":"This example is done on MacOS. Here is the environment configuration information: Hardware specifications: CPU: 1.7 GHz Quad-Core Intel Core i7 Memory: 16 GB Spark: 2.4.7, stand-alone Hadoop: 2.9.2, pseudo-distributed deployment HBase: 2.2.7 Nebula Graph: 3.1.0. Deploy Nebula Graph with Docker Compose .","title":"Environment"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-hbase/#prerequisites","text":"Before importing data, you need to confirm the following information: Nebula Graph has been installed and deployed with the following information: IP addresses and ports of Graph and Meta services. The user name and password with write permission to Nebula Graph. Exchange has been compiled , or download the compiled .jar file directly. Spark has been installed. Learn about the Schema created in Nebula Graph, including names and properties of Tags and Edge types, and more. The Hadoop service has been installed and started.","title":"Prerequisites"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-hbase/#steps","text":"","title":"Steps"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-hbase/#step_1_create_the_schema_in_nebula_graph","text":"Analyze the data to create a Schema in Nebula Graph by following these steps: Identify the Schema elements. The Schema elements in the Nebula Graph are shown in the following table. Element Name Property Tag player name string, age int Tag team name string Edge Type follow degree int Edge Type serve start_year int, end_year int Create a graph space basketballplayer in the Nebula Graph and create a Schema as shown below. ## Create a graph space. nebula> CREATE SPACE basketballplayer \\ (partition_num = 10, \\ replica_factor = 1, \\ vid_type = FIXED_STRING(30)); ## Use the graph space basketballplayer. nebula> USE basketballplayer; ## Create the Tag player. nebula> CREATE TAG player(name string, age int); ## Create the Tag team. nebula> CREATE TAG team(name string); ## Create the Edge type follow. nebula> CREATE EDGE follow(degree int); ## Create the Edge type serve. nebula> CREATE EDGE serve(start_year int, end_year int); For more information, see Quick start workflow .","title":"Step 1: Create the Schema in Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-hbase/#step_2_modify_configuration_files","text":"After Exchange is compiled, copy the conf file target/classes/application.conf to set HBase data source configuration. In this example, the copied file is called hbase_application.conf . For details on each configuration item, see Parameters in the configuration file . { # Spark configuration spark: { app: { name: Nebula Exchange 3.0.0 } driver: { cores: 1 maxResultSize: 1G } cores: { max: 16 } } # Nebula Graph configuration nebula: { address:{ # Specify the IP addresses and ports for Graph and all Meta services. # If there are multiple addresses, the format is \"ip1:port\",\"ip2:port\",\"ip3:port\". # Addresses are separated by commas. graph:[\"127.0.0.1:9669\"] meta:[\"127.0.0.1:9559\"] } # The account entered must have write permission for the Nebula Graph space. user: root pswd: nebula # Fill in the name of the graph space you want to write data to in the Nebula Graph. space: basketballplayer connection: { timeout: 3000 retry: 3 } execution: { retry: 3 } error: { max: 32 output: /tmp/errors } rate: { limit: 1024 timeout: 1000 } } # Processing vertexes tags: [ # Set information about Tag player. # If you want to set RowKey as the data source, enter rowkey and the actual column name of the column family. { # The Tag name in Nebula Graph. name: player type: { # Specify the data source file format to HBase. source: hbase # Specify how to import the data into Nebula Graph: Client or SST. sink: client } host:192.168.*.* port:2181 table:\"player\" columnFamily:\"cf\" # Specify the column names in the player table in fields, and their corresponding values are specified as properties in the Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. # If multiple column names need to be specified, separate them by commas. fields: [age,name] nebula.fields: [age,name] # Specify a column of data in the table as the source of vertex VID in the Nebula Graph. # For example, if rowkey is the source of the VID, enter rowkey. vertex:{ field:rowkey } # Number of pieces of data written to Nebula Graph in a single batch. batch: 256 # Number of Spark partitions partition: 32 } # Set Tag Team information. { name: team type: { source: hbase sink: client } host:192.168.*.* port:2181 table:\"team\" columnFamily:\"cf\" fields: [name] nebula.fields: [name] vertex:{ field:rowkey } batch: 256 partition: 32 } ] # Processing edges edges: [ # Set the information about the Edge Type follow. { # The corresponding Edge Type name in Nebula Graph. name: follow type: { # Specify the data source file format to HBase. source: hbase # Specify how to import the Edge type data into Nebula Graph. # Specify how to import the data into Nebula Graph: Client or SST. sink: client } host:192.168.*.* port:2181 table:\"follow\" columnFamily:\"cf\" # Specify the column names in the follow table in fields, and their corresponding values are specified as properties in the Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. # If multiple column names need to be specified, separate them by commas. fields: [degree] nebula.fields: [degree] # In source, use a column in the follow table as the source of the edge's source vertex. # In target, use a column in the follow table as the source of the edge's destination vertex. source:{ field:rowkey } target:{ field:dst_player } # (Optional) Specify a column as the source of the rank. #ranking: rank # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 } # Set the information about the Edge Type serve. { name: serve type: { source: hbase sink: client } host:192.168.*.* port:2181 table:\"serve\" columnFamily:\"cf\" fields: [start_year,end_year] nebula.fields: [start_year,end_year] source:{ field:rowkey } target:{ field:teamid } # (Optional) Specify a column as the source of the rank. #ranking: rank batch: 256 partition: 32 } ] }","title":"Step 2: Modify configuration files"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-hbase/#step_3_import_data_into_nebula_graph","text":"Run the following command to import HBase data into Nebula Graph. For descriptions of the parameters, see Options for import . ${ SPARK_HOME } /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange <nebula-exchange-3.0.0.jar_path> -c <hbase_application.conf_path> Note JAR packages are available in two ways: compiled them yourself , or download the compiled .jar file directly. For example: ${ SPARK_HOME } /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange /root/nebula-exchange/nebula-exchange/target/nebula-exchange-3.0.0.jar -c /root/nebula-exchange/nebula-exchange/target/classes/hbase_application.conf You can search for batchSuccess.<tag_name/edge_name> in the command output to check the number of successes. For example, batchSuccess.follow: 300 .","title":"Step 3: Import data into Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-hbase/#step_4_optional_validate_data","text":"Users can verify that data has been imported by executing a query in the Nebula Graph client (for example, Nebula Graph Studio). For example: GO FROM \"player100\" OVER follow; Users can also run the SHOW STATS command to view statistics.","title":"Step 4: (optional) Validate data"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-hbase/#step_5_optional_rebuild_indexes_in_nebula_graph","text":"With the data imported, users can recreate and rebuild indexes in Nebula Graph. For details, see Index overview .","title":"Step 5: (optional) Rebuild indexes in Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-hive/","text":"Import data from Hive \u00b6 This topic provides an example of how to use Exchange to import Nebula Graph data stored in Hive. Data set \u00b6 This topic takes the basketballplayer dataset as an example. In this example, the data set has been stored in Hive. All vertexes and edges are stored in the player , team , follow , and serve tables. The following are some of the data for each table. scala > spark . sql ( \"describe basketball.player\" ). show + --------+---------+-------+ | col_name | data_type | comment | + --------+---------+-------+ | playerid | string | null | | age | bigint | null | | name | string | null | + --------+---------+-------+ scala > spark . sql ( \"describe basketball.team\" ). show + ----------+---------+-------+ | col_name | data_type | comment | + ----------+---------+-------+ | teamid | string | null | | name | string | null | + ----------+---------+-------+ scala > spark . sql ( \"describe basketball.follow\" ). show + ----------+---------+-------+ | col_name | data_type | comment | + ----------+---------+-------+ | src_player | string | null | | dst_player | string | null | | degree | bigint | null | + ----------+---------+-------+ scala > spark . sql ( \"describe basketball.serve\" ). show + ----------+---------+-------+ | col_name | data_type | comment | + ----------+---------+-------+ | playerid | string | null | | teamid | string | null | | start_year | bigint | null | | end_year | bigint | null | + ----------+---------+-------+ Note The Hive data type bigint corresponds to the Nebula Graph int . Environment \u00b6 This example is done on MacOS. Here is the environment configuration information: Hardware specifications: CPU: 1.7 GHz Quad-Core Intel Core i7 Memory: 16 GB Spark: 2.4.7, stand-alone Hadoop: 2.9.2, pseudo-distributed deployment Hive: 2.3.7, Hive Metastore database is MySQL 8.0.22 Nebula Graph: 3.1.0. Deploy Nebula Graph with Docker Compose . Prerequisites \u00b6 Before importing data, you need to confirm the following information: Nebula Graph has been installed and deployed with the following information: IP addresses and ports of Graph and Meta services. The user name and password with write permission to Nebula Graph. Exchange has been compiled , or download the compiled .jar file directly. Spark has been installed. Learn about the Schema created in Nebula Graph, including names and properties of Tags and Edge types, and more. Hadoop has been installed and started, and the Hive Metastore database (MySQL in this example) has been started. Steps \u00b6 Step 1: Create the Schema in Nebula Graph \u00b6 Analyze the data to create a Schema in Nebula Graph by following these steps: Identify the Schema elements. The Schema elements in the Nebula Graph are shown in the following table. Element Name Property Tag player name string, age int Tag team name string Edge Type follow degree int Edge Type serve start_year int, end_year int Create a graph space basketballplayer in the Nebula Graph and create a Schema as shown below. ## Create a graph space nebula> CREATE SPACE basketballplayer \\ (partition_num = 10, \\ replica_factor = 1, \\ vid_type = FIXED_STRING(30)); ## Use the graph space basketballplayer nebula> USE basketballplayer; ## Create the Tag player nebula> CREATE TAG player(name string, age int); ## Create the Tag team nebula> CREATE TAG team(name string); ## Create the Edge type follow nebula> CREATE EDGE follow(degree int); ## Create the Edge type serve nebula> CREATE EDGE serve(start_year int, end_year int); For more information, see Quick start workflow . Step 2: Use Spark SQL to confirm Hive SQL statements \u00b6 After the Spark-shell environment is started, run the following statements to ensure that Spark can read data in Hive. scala > sql ( \"select playerid, age, name from basketball.player\" ). show scala > sql ( \"select teamid, name from basketball.team\" ). show scala > sql ( \"select src_player, dst_player, degree from basketball.follow\" ). show scala > sql ( \"select playerid, teamid, start_year, end_year from basketball.serve\" ). show The following is the result read from the table basketball.player . +---------+----+-----------------+ | playerid | age | name | +---------+----+-----------------+ | player100 | 42 | Tim Duncan | | player101 | 36 | Tony Parker | | player102 | 33 | LaMarcus Aldridge | | player103 | 32 | Rudy Gay | | player104 | 32 | Marco Belinelli | +---------+----+-----------------+ ... Step 3: Modify configuration file \u00b6 After Exchange is compiled, copy the conf file target/classes/application.conf to set Hive data source configuration. In this example, the copied file is called hive_application.conf . For details on each configuration item, see Parameters in the configuration file . { # Spark configuration spark: { app: { name: Nebula Exchange 3.0.0 } driver: { cores: 1 maxResultSize: 1G } cores: { max: 16 } } # If Spark and Hive are deployed in different clusters, you need to configure the parameters for connecting to Hive. Otherwise, skip these configurations. #hive: { # waredir: \"hdfs://NAMENODE_IP:9000/apps/svr/hive-xxx/warehouse/\" # connectionURL: \"jdbc:mysql://your_ip:3306/hive_spark?characterEncoding=UTF-8\" # connectionDriverName: \"com.mysql.jdbc.Driver\" # connectionUserName: \"user\" # connectionPassword: \"password\" #} # Nebula Graph configuration nebula: { address:{ # Specify the IP addresses and ports for Graph and all Meta services. # If there are multiple addresses, the format is \"ip1:port\",\"ip2:port\",\"ip3:port\". # Addresses are separated by commas. graph:[\"127.0.0.1:9669\"] meta:[\"127.0.0.1:9559\"] } # The account entered must have write permission for the Nebula Graph space. user: root pswd: nebula # Fill in the name of the graph space you want to write data to in the Nebula Graph. space: basketballplayer connection: { timeout: 3000 retry: 3 } execution: { retry: 3 } error: { max: 32 output: /tmp/errors } rate: { limit: 1024 timeout: 1000 } } # Processing vertexes tags: [ # Set the information about the Tag player. { # The Tag name in Nebula Graph. name: player type: { # Specify the data source file format to Hive. source: hive # Specify how to import the data into Nebula Graph: Client or SST. sink: client } # Set the SQL statement to read the data of player table in basketball database. exec: \"select playerid, age, name from basketball.player\" # Specify the column names in the player table in fields, and their corresponding values are specified as properties in the Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. # If multiple column names need to be specified, separate them by commas. fields: [age,name] nebula.fields: [age,name] # Specify a column of data in the table as the source of vertex VID in the Nebula Graph. vertex:{ field:playerid } # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 } # Set the information about the Tag Team. { name: team type: { source: hive sink: client } exec: \"select teamid, name from basketball.team\" fields: [name] nebula.fields: [name] vertex: { field: teamid } batch: 256 partition: 32 } ] # Processing edges edges: [ # Set the information about the Edge Type follow. { # The corresponding Edge Type name in Nebula Graph. name: follow type: { # Specify the data source file format to Hive. source: hive # Specify how to import the Edge type data into Nebula Graph. # Specify how to import the data into Nebula Graph: Client or SST. sink: client } # Set the SQL statement to read the data of follow table in the basketball database. exec: \"select src_player, dst_player, degree from basketball.follow\" # Specify the column names in the follow table in Fields, and their corresponding values are specified as properties in the Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. # If multiple column names need to be specified, separate them by commas. fields: [degree] nebula.fields: [degree] # In source, use a column in the follow table as the source of the edge's starting vertex. # In target, use a column in the follow table as the source of the edge's destination vertex. source: { field: src_player } target: { field: dst_player } # (Optional) Specify a column as the source of the rank. #ranking: rank # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 } # Set the information about the Edge Type serve. { name: serve type: { source: hive sink: client } exec: \"select playerid, teamid, start_year, end_year from basketball.serve\" fields: [start_year,end_year] nebula.fields: [start_year,end_year] source: { field: playerid } target: { field: teamid } # (Optional) Specify a column as the source of the rank. #ranking: rank batch: 256 partition: 32 } ] } Step 4: Import data into Nebula Graph \u00b6 Run the following command to import Hive data into Nebula Graph. For a description of the parameters, see Options for import . ${ SPARK_HOME } /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange <nebula-exchange-3.0.0.jar_path> -c <hive_application.conf_path> -h Note JAR packages are available in two ways: compiled them yourself , or download the compiled .jar file directly. For example: ${ SPARK_HOME } /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange /root/nebula-exchange/nebula-exchange/target/nebula-exchange-3.0.0.jar -c /root/nebula-exchange/nebula-exchange/target/classes/hive_application.conf -h You can search for batchSuccess.<tag_name/edge_name> in the command output to check the number of successes. For example, batchSuccess.follow: 300 . Step 5: (optional) Validate data \u00b6 Users can verify that data has been imported by executing a query in the Nebula Graph client (for example, Nebula Graph Studio). For example: GO FROM \"player100\" OVER follow; Users can also run the SHOW STATS command to view statistics. Step 6: (optional) Rebuild indexes in Nebula Graph \u00b6 With the data imported, users can recreate and rebuild indexes in Nebula Graph. For details, see Index overview .","title":"Import data from Hive"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-hive/#import_data_from_hive","text":"This topic provides an example of how to use Exchange to import Nebula Graph data stored in Hive.","title":"Import data from Hive"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-hive/#data_set","text":"This topic takes the basketballplayer dataset as an example. In this example, the data set has been stored in Hive. All vertexes and edges are stored in the player , team , follow , and serve tables. The following are some of the data for each table. scala > spark . sql ( \"describe basketball.player\" ). show + --------+---------+-------+ | col_name | data_type | comment | + --------+---------+-------+ | playerid | string | null | | age | bigint | null | | name | string | null | + --------+---------+-------+ scala > spark . sql ( \"describe basketball.team\" ). show + ----------+---------+-------+ | col_name | data_type | comment | + ----------+---------+-------+ | teamid | string | null | | name | string | null | + ----------+---------+-------+ scala > spark . sql ( \"describe basketball.follow\" ). show + ----------+---------+-------+ | col_name | data_type | comment | + ----------+---------+-------+ | src_player | string | null | | dst_player | string | null | | degree | bigint | null | + ----------+---------+-------+ scala > spark . sql ( \"describe basketball.serve\" ). show + ----------+---------+-------+ | col_name | data_type | comment | + ----------+---------+-------+ | playerid | string | null | | teamid | string | null | | start_year | bigint | null | | end_year | bigint | null | + ----------+---------+-------+ Note The Hive data type bigint corresponds to the Nebula Graph int .","title":"Data set"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-hive/#environment","text":"This example is done on MacOS. Here is the environment configuration information: Hardware specifications: CPU: 1.7 GHz Quad-Core Intel Core i7 Memory: 16 GB Spark: 2.4.7, stand-alone Hadoop: 2.9.2, pseudo-distributed deployment Hive: 2.3.7, Hive Metastore database is MySQL 8.0.22 Nebula Graph: 3.1.0. Deploy Nebula Graph with Docker Compose .","title":"Environment"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-hive/#prerequisites","text":"Before importing data, you need to confirm the following information: Nebula Graph has been installed and deployed with the following information: IP addresses and ports of Graph and Meta services. The user name and password with write permission to Nebula Graph. Exchange has been compiled , or download the compiled .jar file directly. Spark has been installed. Learn about the Schema created in Nebula Graph, including names and properties of Tags and Edge types, and more. Hadoop has been installed and started, and the Hive Metastore database (MySQL in this example) has been started.","title":"Prerequisites"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-hive/#steps","text":"","title":"Steps"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-hive/#step_1_create_the_schema_in_nebula_graph","text":"Analyze the data to create a Schema in Nebula Graph by following these steps: Identify the Schema elements. The Schema elements in the Nebula Graph are shown in the following table. Element Name Property Tag player name string, age int Tag team name string Edge Type follow degree int Edge Type serve start_year int, end_year int Create a graph space basketballplayer in the Nebula Graph and create a Schema as shown below. ## Create a graph space nebula> CREATE SPACE basketballplayer \\ (partition_num = 10, \\ replica_factor = 1, \\ vid_type = FIXED_STRING(30)); ## Use the graph space basketballplayer nebula> USE basketballplayer; ## Create the Tag player nebula> CREATE TAG player(name string, age int); ## Create the Tag team nebula> CREATE TAG team(name string); ## Create the Edge type follow nebula> CREATE EDGE follow(degree int); ## Create the Edge type serve nebula> CREATE EDGE serve(start_year int, end_year int); For more information, see Quick start workflow .","title":"Step 1: Create the Schema in Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-hive/#step_2_use_spark_sql_to_confirm_hive_sql_statements","text":"After the Spark-shell environment is started, run the following statements to ensure that Spark can read data in Hive. scala > sql ( \"select playerid, age, name from basketball.player\" ). show scala > sql ( \"select teamid, name from basketball.team\" ). show scala > sql ( \"select src_player, dst_player, degree from basketball.follow\" ). show scala > sql ( \"select playerid, teamid, start_year, end_year from basketball.serve\" ). show The following is the result read from the table basketball.player . +---------+----+-----------------+ | playerid | age | name | +---------+----+-----------------+ | player100 | 42 | Tim Duncan | | player101 | 36 | Tony Parker | | player102 | 33 | LaMarcus Aldridge | | player103 | 32 | Rudy Gay | | player104 | 32 | Marco Belinelli | +---------+----+-----------------+ ...","title":"Step 2: Use Spark SQL to confirm Hive SQL statements"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-hive/#step_3_modify_configuration_file","text":"After Exchange is compiled, copy the conf file target/classes/application.conf to set Hive data source configuration. In this example, the copied file is called hive_application.conf . For details on each configuration item, see Parameters in the configuration file . { # Spark configuration spark: { app: { name: Nebula Exchange 3.0.0 } driver: { cores: 1 maxResultSize: 1G } cores: { max: 16 } } # If Spark and Hive are deployed in different clusters, you need to configure the parameters for connecting to Hive. Otherwise, skip these configurations. #hive: { # waredir: \"hdfs://NAMENODE_IP:9000/apps/svr/hive-xxx/warehouse/\" # connectionURL: \"jdbc:mysql://your_ip:3306/hive_spark?characterEncoding=UTF-8\" # connectionDriverName: \"com.mysql.jdbc.Driver\" # connectionUserName: \"user\" # connectionPassword: \"password\" #} # Nebula Graph configuration nebula: { address:{ # Specify the IP addresses and ports for Graph and all Meta services. # If there are multiple addresses, the format is \"ip1:port\",\"ip2:port\",\"ip3:port\". # Addresses are separated by commas. graph:[\"127.0.0.1:9669\"] meta:[\"127.0.0.1:9559\"] } # The account entered must have write permission for the Nebula Graph space. user: root pswd: nebula # Fill in the name of the graph space you want to write data to in the Nebula Graph. space: basketballplayer connection: { timeout: 3000 retry: 3 } execution: { retry: 3 } error: { max: 32 output: /tmp/errors } rate: { limit: 1024 timeout: 1000 } } # Processing vertexes tags: [ # Set the information about the Tag player. { # The Tag name in Nebula Graph. name: player type: { # Specify the data source file format to Hive. source: hive # Specify how to import the data into Nebula Graph: Client or SST. sink: client } # Set the SQL statement to read the data of player table in basketball database. exec: \"select playerid, age, name from basketball.player\" # Specify the column names in the player table in fields, and their corresponding values are specified as properties in the Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. # If multiple column names need to be specified, separate them by commas. fields: [age,name] nebula.fields: [age,name] # Specify a column of data in the table as the source of vertex VID in the Nebula Graph. vertex:{ field:playerid } # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 } # Set the information about the Tag Team. { name: team type: { source: hive sink: client } exec: \"select teamid, name from basketball.team\" fields: [name] nebula.fields: [name] vertex: { field: teamid } batch: 256 partition: 32 } ] # Processing edges edges: [ # Set the information about the Edge Type follow. { # The corresponding Edge Type name in Nebula Graph. name: follow type: { # Specify the data source file format to Hive. source: hive # Specify how to import the Edge type data into Nebula Graph. # Specify how to import the data into Nebula Graph: Client or SST. sink: client } # Set the SQL statement to read the data of follow table in the basketball database. exec: \"select src_player, dst_player, degree from basketball.follow\" # Specify the column names in the follow table in Fields, and their corresponding values are specified as properties in the Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. # If multiple column names need to be specified, separate them by commas. fields: [degree] nebula.fields: [degree] # In source, use a column in the follow table as the source of the edge's starting vertex. # In target, use a column in the follow table as the source of the edge's destination vertex. source: { field: src_player } target: { field: dst_player } # (Optional) Specify a column as the source of the rank. #ranking: rank # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 } # Set the information about the Edge Type serve. { name: serve type: { source: hive sink: client } exec: \"select playerid, teamid, start_year, end_year from basketball.serve\" fields: [start_year,end_year] nebula.fields: [start_year,end_year] source: { field: playerid } target: { field: teamid } # (Optional) Specify a column as the source of the rank. #ranking: rank batch: 256 partition: 32 } ] }","title":"Step 3: Modify configuration file"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-hive/#step_4_import_data_into_nebula_graph","text":"Run the following command to import Hive data into Nebula Graph. For a description of the parameters, see Options for import . ${ SPARK_HOME } /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange <nebula-exchange-3.0.0.jar_path> -c <hive_application.conf_path> -h Note JAR packages are available in two ways: compiled them yourself , or download the compiled .jar file directly. For example: ${ SPARK_HOME } /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange /root/nebula-exchange/nebula-exchange/target/nebula-exchange-3.0.0.jar -c /root/nebula-exchange/nebula-exchange/target/classes/hive_application.conf -h You can search for batchSuccess.<tag_name/edge_name> in the command output to check the number of successes. For example, batchSuccess.follow: 300 .","title":"Step 4: Import data into Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-hive/#step_5_optional_validate_data","text":"Users can verify that data has been imported by executing a query in the Nebula Graph client (for example, Nebula Graph Studio). For example: GO FROM \"player100\" OVER follow; Users can also run the SHOW STATS command to view statistics.","title":"Step 5: (optional) Validate data"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-hive/#step_6_optional_rebuild_indexes_in_nebula_graph","text":"With the data imported, users can recreate and rebuild indexes in Nebula Graph. For details, see Index overview .","title":"Step 6: (optional) Rebuild indexes in Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-json/","text":"Import data from JSON files \u00b6 This topic provides an example of how to use Exchange to import Nebula Graph data stored in HDFS or local JSON files. Data set \u00b6 This topic takes the basketballplayer dataset as an example. Some sample data are as follows: player { \"id\" : \"player100\" , \"age\" : 42 , \"name\" : \"Tim Duncan\" } { \"id\" : \"player101\" , \"age\" : 36 , \"name\" : \"Tony Parker\" } { \"id\" : \"player102\" , \"age\" : 33 , \"name\" : \"LaMarcus Aldridge\" } { \"id\" : \"player103\" , \"age\" : 32 , \"name\" : \"Rudy Gay\" } ... team { \"id\" : \"team200\" , \"name\" : \"Warriors\" } { \"id\" : \"team201\" , \"name\" : \"Nuggets\" } ... follow { \"src\" : \"player100\" , \"dst\" : \"player101\" , \"degree\" : 95 } { \"src\" : \"player101\" , \"dst\" : \"player102\" , \"degree\" : 90 } ... serve { \"src\" : \"player100\" , \"dst\" : \"team204\" , \"start_year\" : \"1997\" , \"end_year\" : \"2016\" } { \"src\" : \"player101\" , \"dst\" : \"team204\" , \"start_year\" : \"1999\" , \"end_year\" : \"2018\" } ... Environment \u00b6 This example is done on MacOS. Here is the environment configuration information: Hardware specifications: CPU: 1.7 GHz Quad-Core Intel Core i7 Memory: 16 GB Spark: 2.4.7, stand-alone Hadoop: 2.9.2, pseudo-distributed deployment Nebula Graph: 3.1.0. Deploy Nebula Graph with Docker Compose . Prerequisites \u00b6 Before importing data, you need to confirm the following information: Nebula Graph has been installed and deployed with the following information: IP addresses and ports of Graph and Meta services. The user name and password with write permission to Nebula Graph. Exchange has been compiled , or download the compiled .jar file directly. Spark has been installed. Learn about the Schema created in Nebula Graph, including names and properties of Tags and Edge types, and more. If files are stored in HDFS, ensure that the Hadoop service is running properly. If files are stored locally and Nebula Graph is a cluster architecture, you need to place the files in the same directory locally on each machine in the cluster. Steps \u00b6 Step 1: Create the Schema in Nebula Graph \u00b6 Analyze the data to create a Schema in Nebula Graph by following these steps: Identify the Schema elements. The Schema elements in the Nebula Graph are shown in the following table. Element Name Property Tag player name string, age int Tag team name string Edge Type follow degree int Edge Type serve start_year int, end_year int Create a graph space basketballplayer in the Nebula Graph and create a Schema as shown below. ## Create a graph space. nebula> CREATE SPACE basketballplayer \\ (partition_num = 10, \\ replica_factor = 1, \\ vid_type = FIXED_STRING(30)); ## Use the graph space basketballplayer. nebula> USE basketballplayer; ## Create the Tag player. nebula> CREATE TAG player(name string, age int); ## Create the Tag team. nebula> CREATE TAG team(name string); ## Create the Edge type follow. nebula> CREATE EDGE follow(degree int); ## Create the Edge type serve. nebula> CREATE EDGE serve(start_year int, end_year int); For more information, see Quick start workflow . Step 2: Process JSON files \u00b6 Confirm the following information: Process JSON files to meet Schema requirements. Obtain the JSON file storage path. Step 3: Modify configuration files \u00b6 After Exchange is compiled, copy the conf file target/classes/application.conf to set JSON data source configuration. In this example, the copied file is called json_application.conf . For details on each configuration item, see Parameters in the configuration file . { # Spark configuration spark: { app: { name: Nebula Exchange 3.0.0 } driver: { cores: 1 maxResultSize: 1G } executor: { memory:1G } cores: { max: 16 } } # Nebula Graph configuration nebula: { address:{ # Specify the IP addresses and ports for Graph and all Meta services. # If there are multiple addresses, the format is \"ip1:port\",\"ip2:port\",\"ip3:port\". # Addresses are separated by commas. graph:[\"127.0.0.1:9669\"] meta:[\"127.0.0.1:9559\"] } # The account entered must have write permission for the Nebula Graph space. user: root pswd: nebula # Fill in the name of the graph space you want to write data to in the Nebula Graph. space: basketballplayer connection: { timeout: 3000 retry: 3 } execution: { retry: 3 } error: { max: 32 output: /tmp/errors } rate: { limit: 1024 timeout: 1000 } } # Processing vertexes tags: [ # Set the information about the Tag player. { # Specify the Tag name defined in Nebula Graph. name: player type: { # Specify the data source file format to JSON. source: json # Specify how to import the data into Nebula Graph: Client or SST. sink: client } # Specify the path to the JSON file. # If the file is stored in HDFS, use double quotation marks to enclose the file path, starting with hdfs://. For example, \"hdfs://ip:port/xx/xx\". # If the file is stored locally, use double quotation marks to enclose the file path, starting with file://. For example, \"file:///tmp/xx.json\". path: \"hdfs://192.168.*.*:9000/data/vertex_player.json\" # Specify the key name in the JSON file in fields, and its corresponding value will serve as the data source for the properties specified in the Nebula Graph. # If multiple column names need to be specified, separate them by commas. fields: [age,name] # Specify the column names in the player table in fields, and their corresponding values are specified as properties in the Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. nebula.fields: [age, name] # Specify a column of data in the table as the source of vertex VID in the Nebula Graph. # The value of vertex must be the same as that in the JSON file. # Currently, Nebula Graph 3.1.0 supports only strings or integers of VID. vertex: { field:id } # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 } # Set the information about the Tag Team. { # Specify the Tag name defined in Nebula Graph. name: team type: { # Specify the data source file format to JSON. source: json # Specify how to import the data into Nebula Graph: Client or SST. sink: client } # Specify the path to the JSON file. # If the file is stored in HDFS, use double quotation marks to enclose the file path, starting with hdfs://. For example, \"hdfs://ip:port/xx/xx\". # If the file is stored locally, use double quotation marks to enclose the file path, starting with file://. For example, \"file:///tmp/xx.json\". path: \"hdfs://192.168.*.*:9000/data/vertex_team.json\" # Specify the key name in the JSON file in fields, and its corresponding value will serve as the data source for the properties specified in the Nebula Graph. # If multiple column names need to be specified, separate them by commas. fields: [name] # Specify the column names in the player table in fields, and their corresponding values are specified as properties in the Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. nebula.fields: [name] # Specify a column of data in the table as the source of vertex VID in the Nebula Graph. # The value of vertex must be the same as that in the JSON file. # Currently, Nebula Graph 3.1.0 supports only strings or integers of VID. vertex: { field:id } # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 } # If more vertexes need to be added, refer to the previous configuration to add them. ] # Processing edges edges: [ # Set the information about the Edge Type follow. { # Specify the Edge Type name defined in Nebula Graph. name: follow type: { # Specify the data source file format to JSON. source: json # Specify how to import the data into Nebula Graph: Client or SST. sink: client } # Specify the path to the JSON file. # If the file is stored in HDFS, use double quotation marks to enclose the file path, starting with hdfs://. For example, \"hdfs://ip:port/xx/xx\". # If the file is stored locally, use double quotation marks to enclose the file path, starting with file://. For example, \"file:///tmp/xx.json\". path: \"hdfs://192.168.*.*:9000/data/edge_follow.json\" # Specify the key name in the JSON file in fields, and its corresponding value will serve as the data source for the properties specified in the Nebula Graph. # If multiple column names need to be specified, separate them by commas. fields: [degree] # Specify the column names in the edge table in fields, and their corresponding values are specified as properties in the Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. nebula.fields: [degree] # Specify a column as the source for the source and destination vertexes. # The value of vertex must be the same as that in the JSON file. # Currently, Nebula Graph 3.1.0 supports only strings or integers of VID. source: { field: src } target: { field: dst } # (Optional) Specify a column as the source of the rank. #ranking: rank # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 } # Set the information about the Edge Type serve. { # Specify the Edge type name defined in Nebula Graph. name: serve type: { # Specify the data source file format to JSON. source: json # Specify how to import the data into Nebula Graph: Client or SST. sink: client } # Specify the path to the JSON file. # If the file is stored in HDFS, use double quotation marks to enclose the file path, starting with hdfs://. For example, \"hdfs://ip:port/xx/xx\". # If the file is stored locally, use double quotation marks to enclose the file path, starting with file://. For example, \"file:///tmp/xx.json\". path: \"hdfs://192.168.*.*:9000/data/edge_serve.json\" # Specify the key name in the JSON file in fields, and its corresponding value will serve as the data source for the properties specified in the Nebula Graph. # If multiple column names need to be specified, separate them by commas. fields: [start_year,end_year] # Specify the column names in the edge table in fields, and their corresponding values are specified as properties in the Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. nebula.fields: [start_year, end_year] # Specify a column as the source for the source and destination vertexes. # The value of vertex must be the same as that in the JSON file. # Currently, Nebula Graph 3.1.0 supports only strings or integers of VID. source: { field: src } target: { field: dst } # (Optional) Specify a column as the source of the rank. #ranking: _c5 # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 } ] # If more edges need to be added, refer to the previous configuration to add them. } Step 4: Import data into Nebula Graph \u00b6 Run the following command to import JSON data into Nebula Graph. For a description of the parameters, see Options for import . ${ SPARK_HOME } /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange <nebula-exchange-3.0.0.jar_path> -c <json_application.conf_path> Note JAR packages are available in two ways: compiled them yourself , or download the compiled .jar file directly. For example: ${ SPARK_HOME } /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange /root/nebula-echange/nebula-exchange/target/nebula-exchange-3.0.0.jar -c /root/nebula-exchange/nebula-exchange/target/classes/json_application.conf You can search for batchSuccess.<tag_name/edge_name> in the command output to check the number of successes. For example, batchSuccess.follow: 300 . Step 5: (optional) Validate data \u00b6 Users can verify that data has been imported by executing a query in the Nebula Graph client (for example, Nebula Graph Studio). For example: GO FROM \"player100\" OVER follow; Users can also run the SHOW STATS command to view statistics. Step 6: (optional) Rebuild indexes in Nebula Graph \u00b6 With the data imported, users can recreate and rebuild indexes in Nebula Graph. For details, see Index overview .","title":"Import data from JSON files"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-json/#import_data_from_json_files","text":"This topic provides an example of how to use Exchange to import Nebula Graph data stored in HDFS or local JSON files.","title":"Import data from JSON files"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-json/#data_set","text":"This topic takes the basketballplayer dataset as an example. Some sample data are as follows: player { \"id\" : \"player100\" , \"age\" : 42 , \"name\" : \"Tim Duncan\" } { \"id\" : \"player101\" , \"age\" : 36 , \"name\" : \"Tony Parker\" } { \"id\" : \"player102\" , \"age\" : 33 , \"name\" : \"LaMarcus Aldridge\" } { \"id\" : \"player103\" , \"age\" : 32 , \"name\" : \"Rudy Gay\" } ... team { \"id\" : \"team200\" , \"name\" : \"Warriors\" } { \"id\" : \"team201\" , \"name\" : \"Nuggets\" } ... follow { \"src\" : \"player100\" , \"dst\" : \"player101\" , \"degree\" : 95 } { \"src\" : \"player101\" , \"dst\" : \"player102\" , \"degree\" : 90 } ... serve { \"src\" : \"player100\" , \"dst\" : \"team204\" , \"start_year\" : \"1997\" , \"end_year\" : \"2016\" } { \"src\" : \"player101\" , \"dst\" : \"team204\" , \"start_year\" : \"1999\" , \"end_year\" : \"2018\" } ...","title":"Data set"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-json/#environment","text":"This example is done on MacOS. Here is the environment configuration information: Hardware specifications: CPU: 1.7 GHz Quad-Core Intel Core i7 Memory: 16 GB Spark: 2.4.7, stand-alone Hadoop: 2.9.2, pseudo-distributed deployment Nebula Graph: 3.1.0. Deploy Nebula Graph with Docker Compose .","title":"Environment"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-json/#prerequisites","text":"Before importing data, you need to confirm the following information: Nebula Graph has been installed and deployed with the following information: IP addresses and ports of Graph and Meta services. The user name and password with write permission to Nebula Graph. Exchange has been compiled , or download the compiled .jar file directly. Spark has been installed. Learn about the Schema created in Nebula Graph, including names and properties of Tags and Edge types, and more. If files are stored in HDFS, ensure that the Hadoop service is running properly. If files are stored locally and Nebula Graph is a cluster architecture, you need to place the files in the same directory locally on each machine in the cluster.","title":"Prerequisites"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-json/#steps","text":"","title":"Steps"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-json/#step_1_create_the_schema_in_nebula_graph","text":"Analyze the data to create a Schema in Nebula Graph by following these steps: Identify the Schema elements. The Schema elements in the Nebula Graph are shown in the following table. Element Name Property Tag player name string, age int Tag team name string Edge Type follow degree int Edge Type serve start_year int, end_year int Create a graph space basketballplayer in the Nebula Graph and create a Schema as shown below. ## Create a graph space. nebula> CREATE SPACE basketballplayer \\ (partition_num = 10, \\ replica_factor = 1, \\ vid_type = FIXED_STRING(30)); ## Use the graph space basketballplayer. nebula> USE basketballplayer; ## Create the Tag player. nebula> CREATE TAG player(name string, age int); ## Create the Tag team. nebula> CREATE TAG team(name string); ## Create the Edge type follow. nebula> CREATE EDGE follow(degree int); ## Create the Edge type serve. nebula> CREATE EDGE serve(start_year int, end_year int); For more information, see Quick start workflow .","title":"Step 1: Create the Schema in Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-json/#step_2_process_json_files","text":"Confirm the following information: Process JSON files to meet Schema requirements. Obtain the JSON file storage path.","title":"Step 2: Process JSON files"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-json/#step_3_modify_configuration_files","text":"After Exchange is compiled, copy the conf file target/classes/application.conf to set JSON data source configuration. In this example, the copied file is called json_application.conf . For details on each configuration item, see Parameters in the configuration file . { # Spark configuration spark: { app: { name: Nebula Exchange 3.0.0 } driver: { cores: 1 maxResultSize: 1G } executor: { memory:1G } cores: { max: 16 } } # Nebula Graph configuration nebula: { address:{ # Specify the IP addresses and ports for Graph and all Meta services. # If there are multiple addresses, the format is \"ip1:port\",\"ip2:port\",\"ip3:port\". # Addresses are separated by commas. graph:[\"127.0.0.1:9669\"] meta:[\"127.0.0.1:9559\"] } # The account entered must have write permission for the Nebula Graph space. user: root pswd: nebula # Fill in the name of the graph space you want to write data to in the Nebula Graph. space: basketballplayer connection: { timeout: 3000 retry: 3 } execution: { retry: 3 } error: { max: 32 output: /tmp/errors } rate: { limit: 1024 timeout: 1000 } } # Processing vertexes tags: [ # Set the information about the Tag player. { # Specify the Tag name defined in Nebula Graph. name: player type: { # Specify the data source file format to JSON. source: json # Specify how to import the data into Nebula Graph: Client or SST. sink: client } # Specify the path to the JSON file. # If the file is stored in HDFS, use double quotation marks to enclose the file path, starting with hdfs://. For example, \"hdfs://ip:port/xx/xx\". # If the file is stored locally, use double quotation marks to enclose the file path, starting with file://. For example, \"file:///tmp/xx.json\". path: \"hdfs://192.168.*.*:9000/data/vertex_player.json\" # Specify the key name in the JSON file in fields, and its corresponding value will serve as the data source for the properties specified in the Nebula Graph. # If multiple column names need to be specified, separate them by commas. fields: [age,name] # Specify the column names in the player table in fields, and their corresponding values are specified as properties in the Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. nebula.fields: [age, name] # Specify a column of data in the table as the source of vertex VID in the Nebula Graph. # The value of vertex must be the same as that in the JSON file. # Currently, Nebula Graph 3.1.0 supports only strings or integers of VID. vertex: { field:id } # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 } # Set the information about the Tag Team. { # Specify the Tag name defined in Nebula Graph. name: team type: { # Specify the data source file format to JSON. source: json # Specify how to import the data into Nebula Graph: Client or SST. sink: client } # Specify the path to the JSON file. # If the file is stored in HDFS, use double quotation marks to enclose the file path, starting with hdfs://. For example, \"hdfs://ip:port/xx/xx\". # If the file is stored locally, use double quotation marks to enclose the file path, starting with file://. For example, \"file:///tmp/xx.json\". path: \"hdfs://192.168.*.*:9000/data/vertex_team.json\" # Specify the key name in the JSON file in fields, and its corresponding value will serve as the data source for the properties specified in the Nebula Graph. # If multiple column names need to be specified, separate them by commas. fields: [name] # Specify the column names in the player table in fields, and their corresponding values are specified as properties in the Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. nebula.fields: [name] # Specify a column of data in the table as the source of vertex VID in the Nebula Graph. # The value of vertex must be the same as that in the JSON file. # Currently, Nebula Graph 3.1.0 supports only strings or integers of VID. vertex: { field:id } # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 } # If more vertexes need to be added, refer to the previous configuration to add them. ] # Processing edges edges: [ # Set the information about the Edge Type follow. { # Specify the Edge Type name defined in Nebula Graph. name: follow type: { # Specify the data source file format to JSON. source: json # Specify how to import the data into Nebula Graph: Client or SST. sink: client } # Specify the path to the JSON file. # If the file is stored in HDFS, use double quotation marks to enclose the file path, starting with hdfs://. For example, \"hdfs://ip:port/xx/xx\". # If the file is stored locally, use double quotation marks to enclose the file path, starting with file://. For example, \"file:///tmp/xx.json\". path: \"hdfs://192.168.*.*:9000/data/edge_follow.json\" # Specify the key name in the JSON file in fields, and its corresponding value will serve as the data source for the properties specified in the Nebula Graph. # If multiple column names need to be specified, separate them by commas. fields: [degree] # Specify the column names in the edge table in fields, and their corresponding values are specified as properties in the Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. nebula.fields: [degree] # Specify a column as the source for the source and destination vertexes. # The value of vertex must be the same as that in the JSON file. # Currently, Nebula Graph 3.1.0 supports only strings or integers of VID. source: { field: src } target: { field: dst } # (Optional) Specify a column as the source of the rank. #ranking: rank # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 } # Set the information about the Edge Type serve. { # Specify the Edge type name defined in Nebula Graph. name: serve type: { # Specify the data source file format to JSON. source: json # Specify how to import the data into Nebula Graph: Client or SST. sink: client } # Specify the path to the JSON file. # If the file is stored in HDFS, use double quotation marks to enclose the file path, starting with hdfs://. For example, \"hdfs://ip:port/xx/xx\". # If the file is stored locally, use double quotation marks to enclose the file path, starting with file://. For example, \"file:///tmp/xx.json\". path: \"hdfs://192.168.*.*:9000/data/edge_serve.json\" # Specify the key name in the JSON file in fields, and its corresponding value will serve as the data source for the properties specified in the Nebula Graph. # If multiple column names need to be specified, separate them by commas. fields: [start_year,end_year] # Specify the column names in the edge table in fields, and their corresponding values are specified as properties in the Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. nebula.fields: [start_year, end_year] # Specify a column as the source for the source and destination vertexes. # The value of vertex must be the same as that in the JSON file. # Currently, Nebula Graph 3.1.0 supports only strings or integers of VID. source: { field: src } target: { field: dst } # (Optional) Specify a column as the source of the rank. #ranking: _c5 # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 } ] # If more edges need to be added, refer to the previous configuration to add them. }","title":"Step 3: Modify configuration files"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-json/#step_4_import_data_into_nebula_graph","text":"Run the following command to import JSON data into Nebula Graph. For a description of the parameters, see Options for import . ${ SPARK_HOME } /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange <nebula-exchange-3.0.0.jar_path> -c <json_application.conf_path> Note JAR packages are available in two ways: compiled them yourself , or download the compiled .jar file directly. For example: ${ SPARK_HOME } /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange /root/nebula-echange/nebula-exchange/target/nebula-exchange-3.0.0.jar -c /root/nebula-exchange/nebula-exchange/target/classes/json_application.conf You can search for batchSuccess.<tag_name/edge_name> in the command output to check the number of successes. For example, batchSuccess.follow: 300 .","title":"Step 4: Import data into Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-json/#step_5_optional_validate_data","text":"Users can verify that data has been imported by executing a query in the Nebula Graph client (for example, Nebula Graph Studio). For example: GO FROM \"player100\" OVER follow; Users can also run the SHOW STATS command to view statistics.","title":"Step 5: (optional) Validate data"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-json/#step_6_optional_rebuild_indexes_in_nebula_graph","text":"With the data imported, users can recreate and rebuild indexes in Nebula Graph. For details, see Index overview .","title":"Step 6: (optional) Rebuild indexes in Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-kafka/","text":"Import data from Kafka \u00b6 This topic provides a simple guide to importing Data stored on Kafka into Nebula Graph using Exchange. Environment \u00b6 This example is done on MacOS. Here is the environment configuration information: Hardware specifications: CPU: 1.7 GHz Quad-Core Intel Core i7 Memory: 16 GB Spark: 2.4.7, stand-alone Nebula Graph: 3.1.0. Deploy Nebula Graph with Docker Compose . Prerequisites \u00b6 Before importing data, you need to confirm the following information: Nebula Graph has been installed and deployed with the following information: IP addresses and ports of Graph and Meta services. The user name and password with write permission to Nebula Graph. Exchange has been compiled , or download the compiled .jar file directly. Spark has been installed. Learn about the Schema created in Nebula Graph, including names and properties of Tags and Edge types, and more. The Kafka service has been installed and started. Steps \u00b6 Step 1: Create the Schema in Nebula Graph \u00b6 Analyze the data to create a Schema in Nebula Graph by following these steps: Identify the Schema elements. The Schema elements in the Nebula Graph are shown in the following table. Element Name Property Tag player name string, age int Tag team name string Edge Type follow degree int Edge Type serve start_year int, end_year int Create a graph space basketballplayer in the Nebula Graph and create a Schema as shown below. ## Create a graph space. nebula> CREATE SPACE basketballplayer \\ (partition_num = 10, \\ replica_factor = 1, \\ vid_type = FIXED_STRING(30)); ## Use the graph space basketballplayer. nebula> USE basketballplayer; ## Create the Tag player. nebula> CREATE TAG player(name string, age int); ## Create the Tag team. nebula> CREATE TAG team(name string); ## Create the Edge type follow. nebula> CREATE EDGE follow(degree int); ## Create the Edge type serve. nebula> CREATE EDGE serve(start_year int, end_year int); For more information, see Quick start workflow . Step 2: Modify configuration files \u00b6 Note If some data is stored in Kafka's value field, you need to modify the source code, get the value from Kafka, parse the value through the from_JSON function, and return it as a Dataframe. After Exchange is compiled, copy the conf file target/classes/application.conf to set Kafka data source configuration. In this example, the copied file is called kafka_application.conf . For details on each configuration item, see Parameters in the configuration file . { # Spark configuration spark: { app: { name: Nebula Exchange 3.0.0 } driver: { cores: 1 maxResultSize: 1G } cores: { max: 16 } } # Nebula Graph configuration nebula: { address:{ # Specify the IP addresses and ports for Graph and all Meta services. # If there are multiple addresses, the format is \"ip1:port\",\"ip2:port\",\"ip3:port\". # Addresses are separated by commas. graph:[\"127.0.0.1:9669\"] meta:[\"127.0.0.1:9559\"] } # The account entered must have write permission for the Nebula Graph space. user: root pswd: nebula # Fill in the name of the graph space you want to write data to in the Nebula Graph. space: basketballplayer connection: { timeout: 3000 retry: 3 } execution: { retry: 3 } error: { max: 32 output: /tmp/errors } rate: { limit: 1024 timeout: 1000 } } # Processing vertexes tags: [ # Set the information about the Tag player. { # The corresponding Tag name in Nebula Graph. name: player type: { # Specify the data source file format to Kafka. source: kafka # Specify how to import the data into Nebula Graph: Client or SST. sink: client } # Kafka server address. service: \"127.0.0.1:9092\" # Message category. topic: \"topic_name1\" # Kafka data has a fixed domain name: key, value, topic, partition, offset, timestamp, timestampType. # If multiple fields need to be specified after Spark reads as DataFrame, separate them with commas. # Specify the field name in fields. For example, use key for name in Nebula and value for age in Nebula, as shown in the following. fields: [key,value] nebula.fields: [name,age] # Specify a column of data in the table as the source of vertex VID in the Nebula Graph. # The key is the same as the value above, indicating that key is used as both VID and property name. vertex:{ field:key } # The number of data written to Nebula Graph in a single batch. batch: 10 # The number of Spark partitions. partition: 10 # The interval for message reading. Unit: second. interval.seconds: 10 } # Set the information about the Tag Team. { name: team type: { source: kafka sink: client } service: \"127.0.0.1:9092\" topic: \"topic_name2\" fields: [key] nebula.fields: [name] vertex:{ field:key } batch: 10 partition: 10 interval.seconds: 10 } ] # Processing edges edges: [ # Set the information about the Edge Type follow. { # The corresponding Edge Type name in Nebula Graph. name: follow type: { # Specify the data source file format to Kafka. source: kafka # Specify how to import the Edge type data into Nebula Graph. # Specify how to import the data into Nebula Graph: Client or SST. sink: client } # Kafka server address. service: \"127.0.0.1:9092\" # Message category. topic: \"topic_name3\" # Kafka data has a fixed domain name: key, value, topic, partition, offset, timestamp, timestampType. # If multiple fields need to be specified after Spark reads as DataFrame, separate them with commas. # Specify the field name in fields. For example, use key for degree in Nebula, as shown in the following. fields: [key] nebula.fields: [degree] # In source, use a column in the topic as the source of the edge's source vertex. # In target, use a column in the topic as the source of the edge's destination vertex. source:{ field:timestamp } target:{ field:offset } # (Optional) Specify a column as the source of the rank. #ranking: rank # The number of data written to Nebula Graph in a single batch. batch: 10 # The number of Spark partitions. partition: 10 # The interval for message reading. Unit: second. interval.seconds: 10 } # Set the information about the Edge Type serve. { name: serve type: { source: kafka sink: client } service: \"127.0.0.1:9092\" topic: \"topic_name4\" fields: [timestamp,offset] nebula.fields: [start_year,end_year] source:{ field:key } target:{ field:value } # (Optional) Specify a column as the source of the rank. #ranking: rank batch: 10 partition: 10 interval.seconds: 10 } ] } Step 3: Import data into Nebula Graph \u00b6 Run the following command to import Kafka data into Nebula Graph. For a description of the parameters, see Options for import . ${ SPARK_HOME } /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange <nebula-exchange-3.0.0.jar_path> -c <kafka_application.conf_path> Note JAR packages are available in two ways: compiled them yourself , or download the compiled .jar file directly. For example: ${ SPARK_HOME } /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange /root/nebula-exchange/nebula-exchange/target/nebula-exchange-3.0.0.jar -c /root/nebula-exchange/nebula-exchange/target/classes/kafka_application.conf You can search for batchSuccess.<tag_name/edge_name> in the command output to check the number of successes. For example, batchSuccess.follow: 300 . Step 4: (optional) Validate data \u00b6 Users can verify that data has been imported by executing a query in the Nebula Graph client (for example, Nebula Graph Studio). For example: GO FROM \"player100\" OVER follow; Users can also run the SHOW STATS command to view statistics. Step 5: (optional) Rebuild indexes in Nebula Graph \u00b6 With the data imported, users can recreate and rebuild indexes in Nebula Graph. For details, see Index overview .","title":"Import data from Kafka"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-kafka/#import_data_from_kafka","text":"This topic provides a simple guide to importing Data stored on Kafka into Nebula Graph using Exchange.","title":"Import data from Kafka"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-kafka/#environment","text":"This example is done on MacOS. Here is the environment configuration information: Hardware specifications: CPU: 1.7 GHz Quad-Core Intel Core i7 Memory: 16 GB Spark: 2.4.7, stand-alone Nebula Graph: 3.1.0. Deploy Nebula Graph with Docker Compose .","title":"Environment"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-kafka/#prerequisites","text":"Before importing data, you need to confirm the following information: Nebula Graph has been installed and deployed with the following information: IP addresses and ports of Graph and Meta services. The user name and password with write permission to Nebula Graph. Exchange has been compiled , or download the compiled .jar file directly. Spark has been installed. Learn about the Schema created in Nebula Graph, including names and properties of Tags and Edge types, and more. The Kafka service has been installed and started.","title":"Prerequisites"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-kafka/#steps","text":"","title":"Steps"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-kafka/#step_1_create_the_schema_in_nebula_graph","text":"Analyze the data to create a Schema in Nebula Graph by following these steps: Identify the Schema elements. The Schema elements in the Nebula Graph are shown in the following table. Element Name Property Tag player name string, age int Tag team name string Edge Type follow degree int Edge Type serve start_year int, end_year int Create a graph space basketballplayer in the Nebula Graph and create a Schema as shown below. ## Create a graph space. nebula> CREATE SPACE basketballplayer \\ (partition_num = 10, \\ replica_factor = 1, \\ vid_type = FIXED_STRING(30)); ## Use the graph space basketballplayer. nebula> USE basketballplayer; ## Create the Tag player. nebula> CREATE TAG player(name string, age int); ## Create the Tag team. nebula> CREATE TAG team(name string); ## Create the Edge type follow. nebula> CREATE EDGE follow(degree int); ## Create the Edge type serve. nebula> CREATE EDGE serve(start_year int, end_year int); For more information, see Quick start workflow .","title":"Step 1: Create the Schema in Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-kafka/#step_2_modify_configuration_files","text":"Note If some data is stored in Kafka's value field, you need to modify the source code, get the value from Kafka, parse the value through the from_JSON function, and return it as a Dataframe. After Exchange is compiled, copy the conf file target/classes/application.conf to set Kafka data source configuration. In this example, the copied file is called kafka_application.conf . For details on each configuration item, see Parameters in the configuration file . { # Spark configuration spark: { app: { name: Nebula Exchange 3.0.0 } driver: { cores: 1 maxResultSize: 1G } cores: { max: 16 } } # Nebula Graph configuration nebula: { address:{ # Specify the IP addresses and ports for Graph and all Meta services. # If there are multiple addresses, the format is \"ip1:port\",\"ip2:port\",\"ip3:port\". # Addresses are separated by commas. graph:[\"127.0.0.1:9669\"] meta:[\"127.0.0.1:9559\"] } # The account entered must have write permission for the Nebula Graph space. user: root pswd: nebula # Fill in the name of the graph space you want to write data to in the Nebula Graph. space: basketballplayer connection: { timeout: 3000 retry: 3 } execution: { retry: 3 } error: { max: 32 output: /tmp/errors } rate: { limit: 1024 timeout: 1000 } } # Processing vertexes tags: [ # Set the information about the Tag player. { # The corresponding Tag name in Nebula Graph. name: player type: { # Specify the data source file format to Kafka. source: kafka # Specify how to import the data into Nebula Graph: Client or SST. sink: client } # Kafka server address. service: \"127.0.0.1:9092\" # Message category. topic: \"topic_name1\" # Kafka data has a fixed domain name: key, value, topic, partition, offset, timestamp, timestampType. # If multiple fields need to be specified after Spark reads as DataFrame, separate them with commas. # Specify the field name in fields. For example, use key for name in Nebula and value for age in Nebula, as shown in the following. fields: [key,value] nebula.fields: [name,age] # Specify a column of data in the table as the source of vertex VID in the Nebula Graph. # The key is the same as the value above, indicating that key is used as both VID and property name. vertex:{ field:key } # The number of data written to Nebula Graph in a single batch. batch: 10 # The number of Spark partitions. partition: 10 # The interval for message reading. Unit: second. interval.seconds: 10 } # Set the information about the Tag Team. { name: team type: { source: kafka sink: client } service: \"127.0.0.1:9092\" topic: \"topic_name2\" fields: [key] nebula.fields: [name] vertex:{ field:key } batch: 10 partition: 10 interval.seconds: 10 } ] # Processing edges edges: [ # Set the information about the Edge Type follow. { # The corresponding Edge Type name in Nebula Graph. name: follow type: { # Specify the data source file format to Kafka. source: kafka # Specify how to import the Edge type data into Nebula Graph. # Specify how to import the data into Nebula Graph: Client or SST. sink: client } # Kafka server address. service: \"127.0.0.1:9092\" # Message category. topic: \"topic_name3\" # Kafka data has a fixed domain name: key, value, topic, partition, offset, timestamp, timestampType. # If multiple fields need to be specified after Spark reads as DataFrame, separate them with commas. # Specify the field name in fields. For example, use key for degree in Nebula, as shown in the following. fields: [key] nebula.fields: [degree] # In source, use a column in the topic as the source of the edge's source vertex. # In target, use a column in the topic as the source of the edge's destination vertex. source:{ field:timestamp } target:{ field:offset } # (Optional) Specify a column as the source of the rank. #ranking: rank # The number of data written to Nebula Graph in a single batch. batch: 10 # The number of Spark partitions. partition: 10 # The interval for message reading. Unit: second. interval.seconds: 10 } # Set the information about the Edge Type serve. { name: serve type: { source: kafka sink: client } service: \"127.0.0.1:9092\" topic: \"topic_name4\" fields: [timestamp,offset] nebula.fields: [start_year,end_year] source:{ field:key } target:{ field:value } # (Optional) Specify a column as the source of the rank. #ranking: rank batch: 10 partition: 10 interval.seconds: 10 } ] }","title":"Step 2: Modify configuration files"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-kafka/#step_3_import_data_into_nebula_graph","text":"Run the following command to import Kafka data into Nebula Graph. For a description of the parameters, see Options for import . ${ SPARK_HOME } /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange <nebula-exchange-3.0.0.jar_path> -c <kafka_application.conf_path> Note JAR packages are available in two ways: compiled them yourself , or download the compiled .jar file directly. For example: ${ SPARK_HOME } /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange /root/nebula-exchange/nebula-exchange/target/nebula-exchange-3.0.0.jar -c /root/nebula-exchange/nebula-exchange/target/classes/kafka_application.conf You can search for batchSuccess.<tag_name/edge_name> in the command output to check the number of successes. For example, batchSuccess.follow: 300 .","title":"Step 3: Import data into Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-kafka/#step_4_optional_validate_data","text":"Users can verify that data has been imported by executing a query in the Nebula Graph client (for example, Nebula Graph Studio). For example: GO FROM \"player100\" OVER follow; Users can also run the SHOW STATS command to view statistics.","title":"Step 4: (optional) Validate data"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-kafka/#step_5_optional_rebuild_indexes_in_nebula_graph","text":"With the data imported, users can recreate and rebuild indexes in Nebula Graph. For details, see Index overview .","title":"Step 5: (optional) Rebuild indexes in Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-maxcompute/","text":"Import data from MaxCompute \u00b6 This topic provides an example of how to use Exchange to import Nebula Graph data stored in MaxCompute. Data set \u00b6 This topic takes the basketballplayer dataset as an example. Environment \u00b6 This example is done on MacOS. Here is the environment configuration information: Hardware specifications: CPU: 1.7 GHz Quad-Core Intel Core i7 Memory: 16 GB Spark: 2.4.7, stand-alone Hadoop: 2.9.2, pseudo-distributed deployment MaxCompute: Alibaba Cloud official version Nebula Graph: 3.1.0. Deploy Nebula Graph with Docker Compose . Prerequisites \u00b6 Before importing data, you need to confirm the following information: Nebula Graph has been installed and deployed with the following information: IP addresses and ports of Graph and Meta services. The user name and password with write permission to Nebula Graph. Exchange has been compiled , or download the compiled .jar file directly. Spark has been installed. Learn about the Schema created in Nebula Graph, including names and properties of Tags and Edge types, and more. The Hadoop service has been installed and started. Steps \u00b6 Step 1: Create the Schema in Nebula Graph \u00b6 Analyze the data to create a Schema in Nebula Graph by following these steps: Identify the Schema elements. The Schema elements in the Nebula Graph are shown in the following table. Element Name Property Tag player name string, age int Tag team name string Edge Type follow degree int Edge Type serve start_year int, end_year int Create a graph space basketballplayer in the Nebula Graph and create a Schema as shown below. ## Create a graph space. nebula> CREATE SPACE basketballplayer \\ (partition_num = 10, \\ replica_factor = 1, \\ vid_type = FIXED_STRING(30)); ## Use the graph space basketballplayer. nebula> USE basketballplayer; ## Create the Tag player. nebula> CREATE TAG player(name string, age int); ## Create the Tag team. nebula> CREATE TAG team(name string); ## Create the Edge type follow. nebula> CREATE EDGE follow(degree int); ## Create the Edge type serve. nebula> CREATE EDGE serve(start_year int, end_year int); For more information, see Quick start workflow . Step 2: Modify configuration files \u00b6 After Exchange is compiled, copy the conf file target/classes/application.conf to set MaxCompute data source configuration. In this example, the copied file is called maxcompute_application.conf . For details on each configuration item, see Parameters in the configuration file . { # Spark configuration spark: { app: { name: Nebula Exchange 3.0.0 } driver: { cores: 1 maxResultSize: 1G } cores: { max: 16 } } # Nebula Graph configuration nebula: { address:{ # Specify the IP addresses and ports for Graph and Meta services. # If there are multiple addresses, the format is \"ip1:port\",\"ip2:port\",\"ip3:port\". # Addresses are separated by commas. graph:[\"127.0.0.1:9669\"] meta:[\"127.0.0.1:9559\"] } # The account entered must have write permission for the Nebula Graph space. user: root pswd: nebula # Fill in the name of the graph space you want to write data to in the Nebula Graph. space: basketballplayer connection: { timeout: 3000 retry: 3 } execution: { retry: 3 } error: { max: 32 output: /tmp/errors } rate: { limit: 1024 timeout: 1000 } } # Processing vertexes tags: [ # Set the information about the Tag player. { name: player type: { # Specify the data source file format to MaxCompute. source: maxcompute # Specify how to import the data into Nebula Graph: Client or SST. sink: client } # Table name of MaxCompute. table:player # Project name of MaxCompute. project:project # OdpsUrl and tunnelUrl for the MaxCompute service. # The address is https://help.aliyun.com/document_detail/34951.html. odpsUrl:\"http://service.cn-hangzhou.maxcompute.aliyun.com/api\" tunnelUrl:\"http://dt.cn-hangzhou.maxcompute.aliyun.com\" # AccessKeyId and accessKeySecret of the MaxCompute service. accessKeyId:xxx accessKeySecret:xxx # Partition description of the MaxCompute table. This configuration is optional. partitionSpec:\"dt='partition1'\" # Ensure that the table name in the SQL statement is the same as the value of the table above. This configuration is optional. sentence:\"select id, name, age, playerid from player where id < 10\" # Specify the column names in the player table in fields, and their corresponding values are specified as properties in the Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. # If multiple column names need to be specified, separate them by commas. fields:[name, age] nebula.fields:[name, age] # Specify a column of data in the table as the source of vertex VID in the Nebula Graph. vertex:{ field: playerid } # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 } # Set the information about the Tag Team. { name: team type: { source: maxcompute sink: client } table:team project:project odpsUrl:\"http://service.cn-hangzhou.maxcompute.aliyun.com/api\" tunnelUrl:\"http://dt.cn-hangzhou.maxcompute.aliyun.com\" accessKeyId:xxx accessKeySecret:xxx partitionSpec:\"dt='partition1'\" sentence:\"select id, name, teamid from team where id < 10\" fields:[name] nebula.fields:[name] vertex:{ field: teamid } batch: 256 partition: 32 } ] # Processing edges edges: [ # Set the information about the Edge Type follow. { # The corresponding Edge Type name in Nebula Graph. name: follow type:{ # Specify the data source file format to MaxCompute. source:maxcompute # Specify how to import the Edge type data into Nebula Graph. # Specify how to import the data into Nebula Graph: Client or SST. sink:client } # Table name of MaxCompute. table:follow # Project name of MaxCompute. project:project # OdpsUrl and tunnelUrl for MaxCompute service. # The address is https://help.aliyun.com/document_detail/34951.html. odpsUrl:\"http://service.cn-hangzhou.maxcompute.aliyun.com/api\" tunnelUrl:\"http://dt.cn-hangzhou.maxcompute.aliyun.com\" # AccessKeyId and accessKeySecret of the MaxCompute service. accessKeyId:xxx accessKeySecret:xxx # Partition description of the MaxCompute table. This configuration is optional. partitionSpec:\"dt='partition1'\" # Ensure that the table name in the SQL statement is the same as the value of the table above. This configuration is optional. sentence:\"select * from follow\" # Specify the column names in the follow table in Fields, and their corresponding values are specified as properties in the Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. # If multiple column names need to be specified, separate them by commas. fields:[degree] nebula.fields:[degree] # In source, use a column in the follow table as the source of the edge's source vertex. source:{ field: src_player } # In target, use a column in the follow table as the source of the edge's destination vertex. target:{ field: dst_player } # (Optional) Specify a column as the source of the rank. #ranking: rank # The number of Spark partitions. partition:10 # The number of data written to Nebula Graph in a single batch. batch:10 } # Set the information about the Edge Type serve. { name: serve type:{ source:maxcompute sink:client } table:serve project:project odpsUrl:\"http://service.cn-hangzhou.maxcompute.aliyun.com/api\" tunnelUrl:\"http://dt.cn-hangzhou.maxcompute.aliyun.com\" accessKeyId:xxx accessKeySecret:xxx partitionSpec:\"dt='partition1'\" sentence:\"select * from serve\" fields:[start_year,end_year] nebula.fields:[start_year,end_year] source:{ field: playerid } target:{ field: teamid } # (Optional) Specify a column as the source of the rank. #ranking: rank partition:10 batch:10 } ] } Step 3: Import data into Nebula Graph \u00b6 Run the following command to import MaxCompute data into Nebula Graph. For a description of the parameters, see Options for import . ${ SPARK_HOME } /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange <nebula-exchange-3.0.0.jar_path> -c <maxcompute_application.conf_path> Note JAR packages are available in two ways: compiled them yourself , or download the compiled .jar file directly. For example: ${ SPARK_HOME } /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange /root/nebula-exchange/nebula-exchange/target/nebula-exchange-3.0.0.jar -c /root/nebula-exchange/nebula-exchange/target/classes/maxcompute_application.conf You can search for batchSuccess.<tag_name/edge_name> in the command output to check the number of successes. For example, batchSuccess.follow: 300 . Step 4: (optional) Validate data \u00b6 Users can verify that data has been imported by executing a query in the Nebula Graph client (for example, Nebula Graph Studio). For example: GO FROM \"player100\" OVER follow; Users can also run the SHOW STATS command to view statistics. Step 5: (optional) Rebuild indexes in Nebula Graph \u00b6 With the data imported, users can recreate and rebuild indexes in Nebula Graph. For details, see Index overview .","title":"Import data from MaxCompute"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-maxcompute/#import_data_from_maxcompute","text":"This topic provides an example of how to use Exchange to import Nebula Graph data stored in MaxCompute.","title":"Import data from MaxCompute"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-maxcompute/#data_set","text":"This topic takes the basketballplayer dataset as an example.","title":"Data set"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-maxcompute/#environment","text":"This example is done on MacOS. Here is the environment configuration information: Hardware specifications: CPU: 1.7 GHz Quad-Core Intel Core i7 Memory: 16 GB Spark: 2.4.7, stand-alone Hadoop: 2.9.2, pseudo-distributed deployment MaxCompute: Alibaba Cloud official version Nebula Graph: 3.1.0. Deploy Nebula Graph with Docker Compose .","title":"Environment"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-maxcompute/#prerequisites","text":"Before importing data, you need to confirm the following information: Nebula Graph has been installed and deployed with the following information: IP addresses and ports of Graph and Meta services. The user name and password with write permission to Nebula Graph. Exchange has been compiled , or download the compiled .jar file directly. Spark has been installed. Learn about the Schema created in Nebula Graph, including names and properties of Tags and Edge types, and more. The Hadoop service has been installed and started.","title":"Prerequisites"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-maxcompute/#steps","text":"","title":"Steps"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-maxcompute/#step_1_create_the_schema_in_nebula_graph","text":"Analyze the data to create a Schema in Nebula Graph by following these steps: Identify the Schema elements. The Schema elements in the Nebula Graph are shown in the following table. Element Name Property Tag player name string, age int Tag team name string Edge Type follow degree int Edge Type serve start_year int, end_year int Create a graph space basketballplayer in the Nebula Graph and create a Schema as shown below. ## Create a graph space. nebula> CREATE SPACE basketballplayer \\ (partition_num = 10, \\ replica_factor = 1, \\ vid_type = FIXED_STRING(30)); ## Use the graph space basketballplayer. nebula> USE basketballplayer; ## Create the Tag player. nebula> CREATE TAG player(name string, age int); ## Create the Tag team. nebula> CREATE TAG team(name string); ## Create the Edge type follow. nebula> CREATE EDGE follow(degree int); ## Create the Edge type serve. nebula> CREATE EDGE serve(start_year int, end_year int); For more information, see Quick start workflow .","title":"Step 1: Create the Schema in Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-maxcompute/#step_2_modify_configuration_files","text":"After Exchange is compiled, copy the conf file target/classes/application.conf to set MaxCompute data source configuration. In this example, the copied file is called maxcompute_application.conf . For details on each configuration item, see Parameters in the configuration file . { # Spark configuration spark: { app: { name: Nebula Exchange 3.0.0 } driver: { cores: 1 maxResultSize: 1G } cores: { max: 16 } } # Nebula Graph configuration nebula: { address:{ # Specify the IP addresses and ports for Graph and Meta services. # If there are multiple addresses, the format is \"ip1:port\",\"ip2:port\",\"ip3:port\". # Addresses are separated by commas. graph:[\"127.0.0.1:9669\"] meta:[\"127.0.0.1:9559\"] } # The account entered must have write permission for the Nebula Graph space. user: root pswd: nebula # Fill in the name of the graph space you want to write data to in the Nebula Graph. space: basketballplayer connection: { timeout: 3000 retry: 3 } execution: { retry: 3 } error: { max: 32 output: /tmp/errors } rate: { limit: 1024 timeout: 1000 } } # Processing vertexes tags: [ # Set the information about the Tag player. { name: player type: { # Specify the data source file format to MaxCompute. source: maxcompute # Specify how to import the data into Nebula Graph: Client or SST. sink: client } # Table name of MaxCompute. table:player # Project name of MaxCompute. project:project # OdpsUrl and tunnelUrl for the MaxCompute service. # The address is https://help.aliyun.com/document_detail/34951.html. odpsUrl:\"http://service.cn-hangzhou.maxcompute.aliyun.com/api\" tunnelUrl:\"http://dt.cn-hangzhou.maxcompute.aliyun.com\" # AccessKeyId and accessKeySecret of the MaxCompute service. accessKeyId:xxx accessKeySecret:xxx # Partition description of the MaxCompute table. This configuration is optional. partitionSpec:\"dt='partition1'\" # Ensure that the table name in the SQL statement is the same as the value of the table above. This configuration is optional. sentence:\"select id, name, age, playerid from player where id < 10\" # Specify the column names in the player table in fields, and their corresponding values are specified as properties in the Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. # If multiple column names need to be specified, separate them by commas. fields:[name, age] nebula.fields:[name, age] # Specify a column of data in the table as the source of vertex VID in the Nebula Graph. vertex:{ field: playerid } # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 } # Set the information about the Tag Team. { name: team type: { source: maxcompute sink: client } table:team project:project odpsUrl:\"http://service.cn-hangzhou.maxcompute.aliyun.com/api\" tunnelUrl:\"http://dt.cn-hangzhou.maxcompute.aliyun.com\" accessKeyId:xxx accessKeySecret:xxx partitionSpec:\"dt='partition1'\" sentence:\"select id, name, teamid from team where id < 10\" fields:[name] nebula.fields:[name] vertex:{ field: teamid } batch: 256 partition: 32 } ] # Processing edges edges: [ # Set the information about the Edge Type follow. { # The corresponding Edge Type name in Nebula Graph. name: follow type:{ # Specify the data source file format to MaxCompute. source:maxcompute # Specify how to import the Edge type data into Nebula Graph. # Specify how to import the data into Nebula Graph: Client or SST. sink:client } # Table name of MaxCompute. table:follow # Project name of MaxCompute. project:project # OdpsUrl and tunnelUrl for MaxCompute service. # The address is https://help.aliyun.com/document_detail/34951.html. odpsUrl:\"http://service.cn-hangzhou.maxcompute.aliyun.com/api\" tunnelUrl:\"http://dt.cn-hangzhou.maxcompute.aliyun.com\" # AccessKeyId and accessKeySecret of the MaxCompute service. accessKeyId:xxx accessKeySecret:xxx # Partition description of the MaxCompute table. This configuration is optional. partitionSpec:\"dt='partition1'\" # Ensure that the table name in the SQL statement is the same as the value of the table above. This configuration is optional. sentence:\"select * from follow\" # Specify the column names in the follow table in Fields, and their corresponding values are specified as properties in the Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. # If multiple column names need to be specified, separate them by commas. fields:[degree] nebula.fields:[degree] # In source, use a column in the follow table as the source of the edge's source vertex. source:{ field: src_player } # In target, use a column in the follow table as the source of the edge's destination vertex. target:{ field: dst_player } # (Optional) Specify a column as the source of the rank. #ranking: rank # The number of Spark partitions. partition:10 # The number of data written to Nebula Graph in a single batch. batch:10 } # Set the information about the Edge Type serve. { name: serve type:{ source:maxcompute sink:client } table:serve project:project odpsUrl:\"http://service.cn-hangzhou.maxcompute.aliyun.com/api\" tunnelUrl:\"http://dt.cn-hangzhou.maxcompute.aliyun.com\" accessKeyId:xxx accessKeySecret:xxx partitionSpec:\"dt='partition1'\" sentence:\"select * from serve\" fields:[start_year,end_year] nebula.fields:[start_year,end_year] source:{ field: playerid } target:{ field: teamid } # (Optional) Specify a column as the source of the rank. #ranking: rank partition:10 batch:10 } ] }","title":"Step 2: Modify configuration files"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-maxcompute/#step_3_import_data_into_nebula_graph","text":"Run the following command to import MaxCompute data into Nebula Graph. For a description of the parameters, see Options for import . ${ SPARK_HOME } /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange <nebula-exchange-3.0.0.jar_path> -c <maxcompute_application.conf_path> Note JAR packages are available in two ways: compiled them yourself , or download the compiled .jar file directly. For example: ${ SPARK_HOME } /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange /root/nebula-exchange/nebula-exchange/target/nebula-exchange-3.0.0.jar -c /root/nebula-exchange/nebula-exchange/target/classes/maxcompute_application.conf You can search for batchSuccess.<tag_name/edge_name> in the command output to check the number of successes. For example, batchSuccess.follow: 300 .","title":"Step 3: Import data into Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-maxcompute/#step_4_optional_validate_data","text":"Users can verify that data has been imported by executing a query in the Nebula Graph client (for example, Nebula Graph Studio). For example: GO FROM \"player100\" OVER follow; Users can also run the SHOW STATS command to view statistics.","title":"Step 4: (optional) Validate data"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-maxcompute/#step_5_optional_rebuild_indexes_in_nebula_graph","text":"With the data imported, users can recreate and rebuild indexes in Nebula Graph. For details, see Index overview .","title":"Step 5: (optional) Rebuild indexes in Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-mysql/","text":"Import data from MySQL/PostgreSQL \u00b6 This topic provides an example of how to use Exchange to export MySQL data and import to Nebula Graph. It also applies to exporting data from PostgreSQL into Nebula Graph. Data set \u00b6 This topic takes the basketballplayer dataset as an example. In this example, the data set has been stored in MySQL. All vertexes and edges are stored in the player , team , follow , and serve tables. The following are some of the data for each table. mysql > desc player ; + ----------+-------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | + ----------+-------------+------+-----+---------+-------+ | playerid | varchar ( 30 ) | YES | | NULL | | | age | int | YES | | NULL | | | name | varchar ( 30 ) | YES | | NULL | | + ----------+-------------+------+-----+---------+-------+ mysql > desc team ; + --------+-------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | + --------+-------------+------+-----+---------+-------+ | teamid | varchar ( 30 ) | YES | | NULL | | | name | varchar ( 30 ) | YES | | NULL | | + --------+-------------+------+-----+---------+-------+ mysql > desc follow ; + ------------+-------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | + ------------+-------------+------+-----+---------+-------+ | src_player | varchar ( 30 ) | YES | | NULL | | | dst_player | varchar ( 30 ) | YES | | NULL | | | degree | int | YES | | NULL | | + ------------+-------------+------+-----+---------+-------+ mysql > desc serve ; + ------------+-------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | + ------------+-------------+------+-----+---------+-------+ | playerid | varchar ( 30 ) | YES | | NULL | | | teamid | varchar ( 30 ) | YES | | NULL | | | start_year | int | YES | | NULL | | | end_year | int | YES | | NULL | | + ------------+-------------+------+-----+---------+-------+ Environment \u00b6 This example is done on MacOS. Here is the environment configuration information: Hardware specifications: CPU: 1.7 GHz Quad-Core Intel Core i7 Memory: 16 GB Spark: 2.4.7, stand-alone Hadoop: 2.9.2, pseudo-distributed deployment MySQL: 8.0.23 Nebula Graph: 3.1.0. Deploy Nebula Graph with Docker Compose . Prerequisites \u00b6 Before importing data, you need to confirm the following information: Nebula Graph has been installed and deployed with the following information: IP addresses and ports of Graph and Meta services. The user name and password with write permission to Nebula Graph. Exchange has been compiled , or download the compiled .jar file directly. Spark has been installed. Learn about the Schema created in Nebula Graph, including names and properties of Tags and Edge types, and more. The Hadoop service has been installed and started. Steps \u00b6 Step 1: Create the Schema in Nebula Graph \u00b6 Analyze the data to create a Schema in Nebula Graph by following these steps: Identify the Schema elements. The Schema elements in the Nebula Graph are shown in the following table. Element Name Property Tag player name string, age int Tag team name string Edge Type follow degree int Edge Type serve start_year int, end_year int Create a graph space basketballplayer in the Nebula Graph and create a Schema as shown below. ## Create a graph space. nebula> CREATE SPACE basketballplayer \\ (partition_num = 10, \\ replica_factor = 1, \\ vid_type = FIXED_STRING(30)); ## Use the graph space basketballplayer. nebula> USE basketballplayer; ## Create the Tag player. nebula> CREATE TAG player(name string, age int); ## Create the Tag team. nebula> CREATE TAG team(name string); ## Create the Edge type follow. nebula> CREATE EDGE follow(degree int); ## Create the Edge type serve. nebula> CREATE EDGE serve(start_year int, end_year int); For more information, see Quick start workflow . Step 2: Modify configuration files \u00b6 After Exchange is compiled, copy the conf file target/classes/application.conf to set MySQL data source configuration. In this case, the copied file is called mysql_application.conf . For details on each configuration item, see Parameters in the configuration file . { # Spark configuration spark: { app: { name: Nebula Exchange 3.0.0 } driver: { cores: 1 maxResultSize: 1G } cores: { max: 16 } } # Nebula Graph configuration nebula: { address:{ # Specify the IP addresses and ports for Graph and Meta services. # If there are multiple addresses, the format is \"ip1:port\",\"ip2:port\",\"ip3:port\". # Addresses are separated by commas. graph:[\"127.0.0.1:9669\"] meta:[\"127.0.0.1:9559\"] } # The account entered must have write permission for the Nebula Graph space. user: root pswd: nebula # Fill in the name of the graph space you want to write data to in the Nebula Graph. space: basketballplayer connection: { timeout: 3000 retry: 3 } execution: { retry: 3 } error: { max: 32 output: /tmp/errors } rate: { limit: 1024 timeout: 1000 } } # Processing vertexes tags: [ # Set the information about the Tag player. { # The Tag name in Nebula Graph. name: player type: { # Specify the data source file format to MySQL. source: mysql # Specify how to import the data into Nebula Graph: Client or SST. sink: client } host:192.168.*.* port:3306 database:\"basketball\" table:\"player\" user:\"test\" password:\"123456\" sentence:\"select playerid, age, name from basketball.player order by playerid;\" # Specify the column names in the player table in fields, and their corresponding values are specified as properties in the Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. # If multiple column names need to be specified, separate them by commas. fields: [age,name] nebula.fields: [age,name] # Specify a column of data in the table as the source of VIDs in the Nebula Graph. vertex: { field:playerid } # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 } # Set the information about the Tag Team. { name: team type: { source: mysql sink: client } host:192.168.*.* port:3306 database:\"basketball\" table:\"team\" user:\"test\" password:\"123456\" sentence:\"select teamid, name from basketball.team order by teamid;\" fields: [name] nebula.fields: [name] vertex: { field: teamid } batch: 256 partition: 32 } ] # Processing edges edges: [ # Set the information about the Edge Type follow. { # The corresponding Edge Type name in Nebula Graph. name: follow type: { # Specify the data source file format to MySQL. source: mysql # Specify how to import the Edge type data into Nebula Graph. # Specify how to import the data into Nebula Graph: Client or SST. sink: client } host:192.168.*.* port:3306 database:\"basketball\" table:\"follow\" user:\"test\" password:\"123456\" sentence:\"select src_player,dst_player,degree from basketball.follow order by src_player;\" # Specify the column names in the follow table in fields, and their corresponding values are specified as properties in the Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. # If multiple column names need to be specified, separate them by commas. fields: [degree] nebula.fields: [degree] # In source, use a column in the follow table as the source of the edge's source vertex. # In target, use a column in the follow table as the source of the edge's destination vertex. source: { field: src_player } target: { field: dst_player } # (Optional) Specify a column as the source of the rank. #ranking: rank # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 } # Set the information about the Edge Type serve. { name: serve type: { source: mysql sink: client } host:192.168.*.* port:3306 database:\"basketball\" table:\"serve\" user:\"test\" password:\"123456\" sentence:\"select playerid,teamid,start_year,end_year from basketball.serve order by playerid;\" fields: [start_year,end_year] nebula.fields: [start_year,end_year] source: { field: playerid } target: { field: teamid } batch: 256 partition: 32 } ] } Step 3: Import data into Nebula Graph \u00b6 Run the following command to import MySQL data into Nebula Graph. For a description of the parameters, see Options for import . ${ SPARK_HOME } /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange <nebula-exchange-3.0.0.jar_path> -c <mysql_application.conf_path> Note JAR packages are available in two ways: compiled them yourself , or download the compiled .jar file directly. For example: ${ SPARK_HOME } /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange /root/nebula-exchange/nebula-exchange/target/nebula-exchange-3.0.0.jar -c /root/nebula-exchange/nebula-exchange/target/classes/mysql_application.conf You can search for batchSuccess.<tag_name/edge_name> in the command output to check the number of successes. For example, batchSuccess.follow: 300 . Step 4: (optional) Validate data \u00b6 Users can verify that data has been imported by executing a query in the Nebula Graph client (for example, Nebula Graph Studio). For example: GO FROM \"player100\" OVER follow; Users can also run the SHOW STATS command to view statistics. Step 5: (optional) Rebuild indexes in Nebula Graph \u00b6 With the data imported, users can recreate and rebuild indexes in Nebula Graph. For details, see Index overview .","title":"Import data from MySQL/PostgreSQL"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-mysql/#import_data_from_mysqlpostgresql","text":"This topic provides an example of how to use Exchange to export MySQL data and import to Nebula Graph. It also applies to exporting data from PostgreSQL into Nebula Graph.","title":"Import data from MySQL/PostgreSQL"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-mysql/#data_set","text":"This topic takes the basketballplayer dataset as an example. In this example, the data set has been stored in MySQL. All vertexes and edges are stored in the player , team , follow , and serve tables. The following are some of the data for each table. mysql > desc player ; + ----------+-------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | + ----------+-------------+------+-----+---------+-------+ | playerid | varchar ( 30 ) | YES | | NULL | | | age | int | YES | | NULL | | | name | varchar ( 30 ) | YES | | NULL | | + ----------+-------------+------+-----+---------+-------+ mysql > desc team ; + --------+-------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | + --------+-------------+------+-----+---------+-------+ | teamid | varchar ( 30 ) | YES | | NULL | | | name | varchar ( 30 ) | YES | | NULL | | + --------+-------------+------+-----+---------+-------+ mysql > desc follow ; + ------------+-------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | + ------------+-------------+------+-----+---------+-------+ | src_player | varchar ( 30 ) | YES | | NULL | | | dst_player | varchar ( 30 ) | YES | | NULL | | | degree | int | YES | | NULL | | + ------------+-------------+------+-----+---------+-------+ mysql > desc serve ; + ------------+-------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | + ------------+-------------+------+-----+---------+-------+ | playerid | varchar ( 30 ) | YES | | NULL | | | teamid | varchar ( 30 ) | YES | | NULL | | | start_year | int | YES | | NULL | | | end_year | int | YES | | NULL | | + ------------+-------------+------+-----+---------+-------+","title":"Data set"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-mysql/#environment","text":"This example is done on MacOS. Here is the environment configuration information: Hardware specifications: CPU: 1.7 GHz Quad-Core Intel Core i7 Memory: 16 GB Spark: 2.4.7, stand-alone Hadoop: 2.9.2, pseudo-distributed deployment MySQL: 8.0.23 Nebula Graph: 3.1.0. Deploy Nebula Graph with Docker Compose .","title":"Environment"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-mysql/#prerequisites","text":"Before importing data, you need to confirm the following information: Nebula Graph has been installed and deployed with the following information: IP addresses and ports of Graph and Meta services. The user name and password with write permission to Nebula Graph. Exchange has been compiled , or download the compiled .jar file directly. Spark has been installed. Learn about the Schema created in Nebula Graph, including names and properties of Tags and Edge types, and more. The Hadoop service has been installed and started.","title":"Prerequisites"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-mysql/#steps","text":"","title":"Steps"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-mysql/#step_1_create_the_schema_in_nebula_graph","text":"Analyze the data to create a Schema in Nebula Graph by following these steps: Identify the Schema elements. The Schema elements in the Nebula Graph are shown in the following table. Element Name Property Tag player name string, age int Tag team name string Edge Type follow degree int Edge Type serve start_year int, end_year int Create a graph space basketballplayer in the Nebula Graph and create a Schema as shown below. ## Create a graph space. nebula> CREATE SPACE basketballplayer \\ (partition_num = 10, \\ replica_factor = 1, \\ vid_type = FIXED_STRING(30)); ## Use the graph space basketballplayer. nebula> USE basketballplayer; ## Create the Tag player. nebula> CREATE TAG player(name string, age int); ## Create the Tag team. nebula> CREATE TAG team(name string); ## Create the Edge type follow. nebula> CREATE EDGE follow(degree int); ## Create the Edge type serve. nebula> CREATE EDGE serve(start_year int, end_year int); For more information, see Quick start workflow .","title":"Step 1: Create the Schema in Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-mysql/#step_2_modify_configuration_files","text":"After Exchange is compiled, copy the conf file target/classes/application.conf to set MySQL data source configuration. In this case, the copied file is called mysql_application.conf . For details on each configuration item, see Parameters in the configuration file . { # Spark configuration spark: { app: { name: Nebula Exchange 3.0.0 } driver: { cores: 1 maxResultSize: 1G } cores: { max: 16 } } # Nebula Graph configuration nebula: { address:{ # Specify the IP addresses and ports for Graph and Meta services. # If there are multiple addresses, the format is \"ip1:port\",\"ip2:port\",\"ip3:port\". # Addresses are separated by commas. graph:[\"127.0.0.1:9669\"] meta:[\"127.0.0.1:9559\"] } # The account entered must have write permission for the Nebula Graph space. user: root pswd: nebula # Fill in the name of the graph space you want to write data to in the Nebula Graph. space: basketballplayer connection: { timeout: 3000 retry: 3 } execution: { retry: 3 } error: { max: 32 output: /tmp/errors } rate: { limit: 1024 timeout: 1000 } } # Processing vertexes tags: [ # Set the information about the Tag player. { # The Tag name in Nebula Graph. name: player type: { # Specify the data source file format to MySQL. source: mysql # Specify how to import the data into Nebula Graph: Client or SST. sink: client } host:192.168.*.* port:3306 database:\"basketball\" table:\"player\" user:\"test\" password:\"123456\" sentence:\"select playerid, age, name from basketball.player order by playerid;\" # Specify the column names in the player table in fields, and their corresponding values are specified as properties in the Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. # If multiple column names need to be specified, separate them by commas. fields: [age,name] nebula.fields: [age,name] # Specify a column of data in the table as the source of VIDs in the Nebula Graph. vertex: { field:playerid } # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 } # Set the information about the Tag Team. { name: team type: { source: mysql sink: client } host:192.168.*.* port:3306 database:\"basketball\" table:\"team\" user:\"test\" password:\"123456\" sentence:\"select teamid, name from basketball.team order by teamid;\" fields: [name] nebula.fields: [name] vertex: { field: teamid } batch: 256 partition: 32 } ] # Processing edges edges: [ # Set the information about the Edge Type follow. { # The corresponding Edge Type name in Nebula Graph. name: follow type: { # Specify the data source file format to MySQL. source: mysql # Specify how to import the Edge type data into Nebula Graph. # Specify how to import the data into Nebula Graph: Client or SST. sink: client } host:192.168.*.* port:3306 database:\"basketball\" table:\"follow\" user:\"test\" password:\"123456\" sentence:\"select src_player,dst_player,degree from basketball.follow order by src_player;\" # Specify the column names in the follow table in fields, and their corresponding values are specified as properties in the Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. # If multiple column names need to be specified, separate them by commas. fields: [degree] nebula.fields: [degree] # In source, use a column in the follow table as the source of the edge's source vertex. # In target, use a column in the follow table as the source of the edge's destination vertex. source: { field: src_player } target: { field: dst_player } # (Optional) Specify a column as the source of the rank. #ranking: rank # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 } # Set the information about the Edge Type serve. { name: serve type: { source: mysql sink: client } host:192.168.*.* port:3306 database:\"basketball\" table:\"serve\" user:\"test\" password:\"123456\" sentence:\"select playerid,teamid,start_year,end_year from basketball.serve order by playerid;\" fields: [start_year,end_year] nebula.fields: [start_year,end_year] source: { field: playerid } target: { field: teamid } batch: 256 partition: 32 } ] }","title":"Step 2: Modify configuration files"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-mysql/#step_3_import_data_into_nebula_graph","text":"Run the following command to import MySQL data into Nebula Graph. For a description of the parameters, see Options for import . ${ SPARK_HOME } /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange <nebula-exchange-3.0.0.jar_path> -c <mysql_application.conf_path> Note JAR packages are available in two ways: compiled them yourself , or download the compiled .jar file directly. For example: ${ SPARK_HOME } /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange /root/nebula-exchange/nebula-exchange/target/nebula-exchange-3.0.0.jar -c /root/nebula-exchange/nebula-exchange/target/classes/mysql_application.conf You can search for batchSuccess.<tag_name/edge_name> in the command output to check the number of successes. For example, batchSuccess.follow: 300 .","title":"Step 3: Import data into Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-mysql/#step_4_optional_validate_data","text":"Users can verify that data has been imported by executing a query in the Nebula Graph client (for example, Nebula Graph Studio). For example: GO FROM \"player100\" OVER follow; Users can also run the SHOW STATS command to view statistics.","title":"Step 4: (optional) Validate data"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-mysql/#step_5_optional_rebuild_indexes_in_nebula_graph","text":"With the data imported, users can recreate and rebuild indexes in Nebula Graph. For details, see Index overview .","title":"Step 5: (optional) Rebuild indexes in Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-neo4j/","text":"Import data from Neo4j \u00b6 This topic provides an example of how to use Exchange to import Nebula Graph data stored in Neo4j. Implementation method \u00b6 Exchange uses Neo4j Driver 4.0.1 to read Neo4j data. Before batch export, you need to write Cypher statements that are automatically executed based on labels and relationship types and the number of Spark partitions in the configuration file to improve data export performance. When Exchange reads Neo4j data, it needs to do the following: The Reader in Exchange replaces the statement following the Cypher RETURN statement in the exec part of the configuration file with COUNT(*) , and executes this statement to get the total amount of data, then calculates the starting offset and size of each partition based on the number of Spark partitions. (Optional) If the user has configured the check_point_path directory, Reader reads the files in the directory. In the transferring state, Reader calculates the offset and size that each Spark partition should have. In each Spark partition, the Reader in Exchange adds different SKIP and LIMIT statements to the Cypher statement and calls the Neo4j Driver for parallel execution to distribute data to different Spark partitions. The Reader finally processes the returned data into a DataFrame. At this point, Exchange has finished exporting the Neo4j data. The data is then written in parallel to the Nebula Graph database. The whole process is illustrated below. Data set \u00b6 This topic takes the basketballplayer dataset as an example. Environment \u00b6 This example is done on MacOS. Here is the environment configuration information: Hardware specifications: CPU\uff1aIntel(R) Xeon(R) CPU E5-2697 v3 @ 2.60GHz CPU cores: 14 Memory: 251 GB Spark: Stand-alone, 2.4.6 pre-build for Hadoop 2.7 Neo4j: 3.5.20 Community Edition Nebula Graph: 3.1.0. Deploy Nebula Graph with Docker Compose . Prerequisites \u00b6 Before importing data, you need to confirm the following information: Nebula Graph has been installed and deployed with the following information: IP addresses and ports of Graph and Meta services. The user name and password with Nebula Graph write permission. Exchange has been compiled , or download the compiled .jar file directly. Spark has been installed. Learn about the Schema created in Nebula Graph, including names and properties of Tags and Edge types, and more. Steps \u00b6 Step 1: Create the Schema in Nebula Graph \u00b6 Analyze the data to create a Schema in Nebula Graph by following these steps: Identify the Schema elements. The Schema elements in the Nebula Graph are shown in the following table. Element Name Property Tag player name string, age int Tag team name string Edge Type follow degree int Edge Type serve start_year int, end_year int Create a graph space basketballplayer in the Nebula Graph and create a Schema as shown below. ## Create a graph space nebula> CREATE SPACE basketballplayer \\ (partition_num = 10, \\ replica_factor = 1, \\ vid_type = FIXED_STRING(30)); ## Use the graph space basketballplayer nebula> USE basketballplayer; ## Create the Tag player nebula> CREATE TAG player(name string, age int); ## Create the Tag team nebula> CREATE TAG team(name string); ## Create the Edge type follow nebula> CREATE EDGE follow(degree int); ## Create the Edge type serve nebula> CREATE EDGE serve(start_year int, end_year int); For more information, see Quick start workflow . Step 2: Configuring source data \u00b6 To speed up the export of Neo4j data, create indexes for the corresponding properties in the Neo4j database. For more information, refer to the Neo4j manual . Step 3: Modify configuration files \u00b6 After Exchange is compiled, copy the conf file target/classes/application.conf to set Neo4j data source configuration. In this example, the copied file is called neo4j_application.conf . For details on each configuration item, see Parameters in the configuration file . { # Spark configuration spark: { app: { name: Nebula Exchange 3.0.0 } driver: { cores: 1 maxResultSize: 1G } executor: { memory:1G } cores: { max: 16 } } # Nebula Graph configuration nebula: { address:{ graph:[\"127.0.0.1:9669\"] meta:[\"127.0.0.1:9559\"] } user: root pswd: nebula space: basketballplayer connection: { timeout: 3000 retry: 3 } execution: { retry: 3 } error: { max: 32 output: /tmp/errors } rate: { limit: 1024 timeout: 1000 } } # Processing vertexes tags: [ # Set the information about the Tag player { name: player type: { source: neo4j sink: client } server: \"bolt://192.168.*.*:7687\" user: neo4j password:neo4j database:neo4j exec: \"match (n:player) return n.id as id, n.age as age, n.name as name\" fields: [age,name] nebula.fields: [age,name] vertex: { field:id } partition: 10 batch: 1000 check_point_path: /tmp/test } # Set the information about the Tag Team { name: team type: { source: neo4j sink: client } server: \"bolt://192.168.*.*:7687\" user: neo4j password:neo4j database:neo4j exec: \"match (n:team) return n.id as id,n.name as name\" fields: [name] nebula.fields: [name] vertex: { field:id } partition: 10 batch: 1000 check_point_path: /tmp/test } ] # Processing edges edges: [ # Set the information about the Edge Type follow { name: follow type: { source: neo4j sink: client } server: \"bolt://192.168.*.*:7687\" user: neo4j password:neo4j database:neo4j exec: \"match (a:player)-[r:follow]->(b:player) return a.id as src, b.id as dst, r.degree as degree order by id(r)\" fields: [degree] nebula.fields: [degree] source: { field: src } target: { field: dst } #ranking: rank partition: 10 batch: 1000 check_point_path: /tmp/test } # Set the information about the Edge Type serve { name: serve type: { source: neo4j sink: client } server: \"bolt://192.168.*.*:7687\" user: neo4j password:neo4j database:neo4j exec: \"match (a:player)-[r:serve]->(b:team) return a.id as src, b.id as dst, r.start_year as start_year, r.end_year as end_year order by id(r)\" fields: [start_year,end_year] nebula.fields: [start_year,end_year] source: { field: src } target: { field: dst } #ranking: rank partition: 10 batch: 1000 check_point_path: /tmp/test } ] } Exec configuration \u00b6 When configuring either the tags.exec or edges.exec parameters, you need to fill in the Cypher query. To prevent loss of data during import, it is strongly recommended to include ORDER BY clause in Cypher queries. Meanwhile, in order to improve data import efficiency, it is better to select indexed properties for ordering. If there is no index, users can also observe the default order and select the appropriate properties for ordering to improve efficiency. If the pattern of the default order cannot be found, users can order them by the ID of the vertex or relationship and set the partition to a small value to reduce the ordering pressure of Neo4j. Note Using the ORDER BY clause lengthens the data import time. Exchange needs to execute different SKIP and LIMIT Cypher statements on different Spark partitions, so SKIP and LIMIT clauses cannot be included in the Cypher statements corresponding to tags.exec and edges.exec . tags.vertex or edges.vertex configuration \u00b6 Nebula Graph uses ID as the unique primary key when creating vertexes and edges, overwriting the data in that primary key if it already exists. So, if a Neo4j property value is given as the Nebula Graph'S ID and the value is duplicated in Neo4j, duplicate IDs will be generated. One and only one of their corresponding data will be stored in the Nebula Graph, and the others will be overwritten. Because the data import process is concurrently writing data to Nebula Graph, the final saved data is not guaranteed to be the latest data in Neo4j. check_point_path configuration \u00b6 If breakpoint transfers are enabled, to avoid data loss, the state of the database should not change between the breakpoint and the transfer. For example, data cannot be added or deleted, and the partition quantity configuration should not be changed. Step 4: Import data into Nebula Graph \u00b6 Run the following command to import Neo4j data into Nebula Graph. For a description of the parameters, see Options for import . ${ SPARK_HOME } /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange <nebula-exchange-3.0.0.jar_path> -c <neo4j_application.conf_path> Note JAR packages are available in two ways: compiled them yourself , or download the compiled .jar file directly. For example: ${ SPARK_HOME } /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange /root/nebula-exchange/nebula-exchange/target/nebula-exchange-3.0.0.jar -c /root/nebula-exchange/nebula-exchange/target/classes/neo4j_application.conf You can search for batchSuccess.<tag_name/edge_name> in the command output to check the number of successes. For example, batchSuccess.follow: 300 . Step 5: (optional) Validate data \u00b6 Users can verify that data has been imported by executing a query in the Nebula Graph client (for example, Nebula Graph Studio). For example: GO FROM \"player100\" OVER follow; Users can also run the SHOW STATS command to view statistics. Step 6: (optional) Rebuild indexes in Nebula Graph \u00b6 With the data imported, users can recreate and rebuild indexes in Nebula Graph. For details, see Index overview .","title":"Import data from Neo4j"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-neo4j/#import_data_from_neo4j","text":"This topic provides an example of how to use Exchange to import Nebula Graph data stored in Neo4j.","title":"Import data from Neo4j"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-neo4j/#implementation_method","text":"Exchange uses Neo4j Driver 4.0.1 to read Neo4j data. Before batch export, you need to write Cypher statements that are automatically executed based on labels and relationship types and the number of Spark partitions in the configuration file to improve data export performance. When Exchange reads Neo4j data, it needs to do the following: The Reader in Exchange replaces the statement following the Cypher RETURN statement in the exec part of the configuration file with COUNT(*) , and executes this statement to get the total amount of data, then calculates the starting offset and size of each partition based on the number of Spark partitions. (Optional) If the user has configured the check_point_path directory, Reader reads the files in the directory. In the transferring state, Reader calculates the offset and size that each Spark partition should have. In each Spark partition, the Reader in Exchange adds different SKIP and LIMIT statements to the Cypher statement and calls the Neo4j Driver for parallel execution to distribute data to different Spark partitions. The Reader finally processes the returned data into a DataFrame. At this point, Exchange has finished exporting the Neo4j data. The data is then written in parallel to the Nebula Graph database. The whole process is illustrated below.","title":"Implementation method"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-neo4j/#data_set","text":"This topic takes the basketballplayer dataset as an example.","title":"Data set"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-neo4j/#environment","text":"This example is done on MacOS. Here is the environment configuration information: Hardware specifications: CPU\uff1aIntel(R) Xeon(R) CPU E5-2697 v3 @ 2.60GHz CPU cores: 14 Memory: 251 GB Spark: Stand-alone, 2.4.6 pre-build for Hadoop 2.7 Neo4j: 3.5.20 Community Edition Nebula Graph: 3.1.0. Deploy Nebula Graph with Docker Compose .","title":"Environment"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-neo4j/#prerequisites","text":"Before importing data, you need to confirm the following information: Nebula Graph has been installed and deployed with the following information: IP addresses and ports of Graph and Meta services. The user name and password with Nebula Graph write permission. Exchange has been compiled , or download the compiled .jar file directly. Spark has been installed. Learn about the Schema created in Nebula Graph, including names and properties of Tags and Edge types, and more.","title":"Prerequisites"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-neo4j/#steps","text":"","title":"Steps"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-neo4j/#step_1_create_the_schema_in_nebula_graph","text":"Analyze the data to create a Schema in Nebula Graph by following these steps: Identify the Schema elements. The Schema elements in the Nebula Graph are shown in the following table. Element Name Property Tag player name string, age int Tag team name string Edge Type follow degree int Edge Type serve start_year int, end_year int Create a graph space basketballplayer in the Nebula Graph and create a Schema as shown below. ## Create a graph space nebula> CREATE SPACE basketballplayer \\ (partition_num = 10, \\ replica_factor = 1, \\ vid_type = FIXED_STRING(30)); ## Use the graph space basketballplayer nebula> USE basketballplayer; ## Create the Tag player nebula> CREATE TAG player(name string, age int); ## Create the Tag team nebula> CREATE TAG team(name string); ## Create the Edge type follow nebula> CREATE EDGE follow(degree int); ## Create the Edge type serve nebula> CREATE EDGE serve(start_year int, end_year int); For more information, see Quick start workflow .","title":"Step 1: Create the Schema in Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-neo4j/#step_2_configuring_source_data","text":"To speed up the export of Neo4j data, create indexes for the corresponding properties in the Neo4j database. For more information, refer to the Neo4j manual .","title":"Step 2: Configuring source data"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-neo4j/#step_3_modify_configuration_files","text":"After Exchange is compiled, copy the conf file target/classes/application.conf to set Neo4j data source configuration. In this example, the copied file is called neo4j_application.conf . For details on each configuration item, see Parameters in the configuration file . { # Spark configuration spark: { app: { name: Nebula Exchange 3.0.0 } driver: { cores: 1 maxResultSize: 1G } executor: { memory:1G } cores: { max: 16 } } # Nebula Graph configuration nebula: { address:{ graph:[\"127.0.0.1:9669\"] meta:[\"127.0.0.1:9559\"] } user: root pswd: nebula space: basketballplayer connection: { timeout: 3000 retry: 3 } execution: { retry: 3 } error: { max: 32 output: /tmp/errors } rate: { limit: 1024 timeout: 1000 } } # Processing vertexes tags: [ # Set the information about the Tag player { name: player type: { source: neo4j sink: client } server: \"bolt://192.168.*.*:7687\" user: neo4j password:neo4j database:neo4j exec: \"match (n:player) return n.id as id, n.age as age, n.name as name\" fields: [age,name] nebula.fields: [age,name] vertex: { field:id } partition: 10 batch: 1000 check_point_path: /tmp/test } # Set the information about the Tag Team { name: team type: { source: neo4j sink: client } server: \"bolt://192.168.*.*:7687\" user: neo4j password:neo4j database:neo4j exec: \"match (n:team) return n.id as id,n.name as name\" fields: [name] nebula.fields: [name] vertex: { field:id } partition: 10 batch: 1000 check_point_path: /tmp/test } ] # Processing edges edges: [ # Set the information about the Edge Type follow { name: follow type: { source: neo4j sink: client } server: \"bolt://192.168.*.*:7687\" user: neo4j password:neo4j database:neo4j exec: \"match (a:player)-[r:follow]->(b:player) return a.id as src, b.id as dst, r.degree as degree order by id(r)\" fields: [degree] nebula.fields: [degree] source: { field: src } target: { field: dst } #ranking: rank partition: 10 batch: 1000 check_point_path: /tmp/test } # Set the information about the Edge Type serve { name: serve type: { source: neo4j sink: client } server: \"bolt://192.168.*.*:7687\" user: neo4j password:neo4j database:neo4j exec: \"match (a:player)-[r:serve]->(b:team) return a.id as src, b.id as dst, r.start_year as start_year, r.end_year as end_year order by id(r)\" fields: [start_year,end_year] nebula.fields: [start_year,end_year] source: { field: src } target: { field: dst } #ranking: rank partition: 10 batch: 1000 check_point_path: /tmp/test } ] }","title":"Step 3: Modify configuration files"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-neo4j/#exec_configuration","text":"When configuring either the tags.exec or edges.exec parameters, you need to fill in the Cypher query. To prevent loss of data during import, it is strongly recommended to include ORDER BY clause in Cypher queries. Meanwhile, in order to improve data import efficiency, it is better to select indexed properties for ordering. If there is no index, users can also observe the default order and select the appropriate properties for ordering to improve efficiency. If the pattern of the default order cannot be found, users can order them by the ID of the vertex or relationship and set the partition to a small value to reduce the ordering pressure of Neo4j. Note Using the ORDER BY clause lengthens the data import time. Exchange needs to execute different SKIP and LIMIT Cypher statements on different Spark partitions, so SKIP and LIMIT clauses cannot be included in the Cypher statements corresponding to tags.exec and edges.exec .","title":"Exec configuration"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-neo4j/#tagsvertex_or_edgesvertex_configuration","text":"Nebula Graph uses ID as the unique primary key when creating vertexes and edges, overwriting the data in that primary key if it already exists. So, if a Neo4j property value is given as the Nebula Graph'S ID and the value is duplicated in Neo4j, duplicate IDs will be generated. One and only one of their corresponding data will be stored in the Nebula Graph, and the others will be overwritten. Because the data import process is concurrently writing data to Nebula Graph, the final saved data is not guaranteed to be the latest data in Neo4j.","title":"tags.vertex or edges.vertex configuration"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-neo4j/#check_point_path_configuration","text":"If breakpoint transfers are enabled, to avoid data loss, the state of the database should not change between the breakpoint and the transfer. For example, data cannot be added or deleted, and the partition quantity configuration should not be changed.","title":"check_point_path configuration"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-neo4j/#step_4_import_data_into_nebula_graph","text":"Run the following command to import Neo4j data into Nebula Graph. For a description of the parameters, see Options for import . ${ SPARK_HOME } /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange <nebula-exchange-3.0.0.jar_path> -c <neo4j_application.conf_path> Note JAR packages are available in two ways: compiled them yourself , or download the compiled .jar file directly. For example: ${ SPARK_HOME } /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange /root/nebula-exchange/nebula-exchange/target/nebula-exchange-3.0.0.jar -c /root/nebula-exchange/nebula-exchange/target/classes/neo4j_application.conf You can search for batchSuccess.<tag_name/edge_name> in the command output to check the number of successes. For example, batchSuccess.follow: 300 .","title":"Step 4: Import data into Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-neo4j/#step_5_optional_validate_data","text":"Users can verify that data has been imported by executing a query in the Nebula Graph client (for example, Nebula Graph Studio). For example: GO FROM \"player100\" OVER follow; Users can also run the SHOW STATS command to view statistics.","title":"Step 5: (optional) Validate data"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-neo4j/#step_6_optional_rebuild_indexes_in_nebula_graph","text":"With the data imported, users can recreate and rebuild indexes in Nebula Graph. For details, see Index overview .","title":"Step 6: (optional) Rebuild indexes in Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-orc/","text":"Import data from ORC files \u00b6 This topic provides an example of how to use Exchange to import Nebula Graph data stored in HDFS or local ORC files. To import a local ORC file to Nebula Graph, see Nebula Importer . Data set \u00b6 This topic takes the basketballplayer dataset as an example. Environment \u00b6 This example is done on MacOS. Here is the environment configuration information: Hardware specifications: CPU: 1.7 GHz Quad-Core Intel Core i7 Memory: 16 GB Spark: 2.4.7, stand-alone Hadoop: 2.9.2, pseudo-distributed deployment Nebula Graph: 3.1.0. Deploy Nebula Graph with Docker Compose . Prerequisites \u00b6 Before importing data, you need to confirm the following information: Nebula Graph has been installed and deployed with the following information: IP addresses and ports of Graph and Meta services. The user name and password with write permission to Nebula Graph. Exchange has been compiled , or download the compiled .jar file directly. Spark has been installed. Learn about the Schema created in Nebula Graph, including names and properties of Tags and Edge types, and more. If files are stored in HDFS, ensure that the Hadoop service is running properly. If files are stored locally and Nebula Graph is a cluster architecture, you need to place the files in the same directory locally on each machine in the cluster. Steps \u00b6 Step 1: Create the Schema in Nebula Graph \u00b6 Analyze the data to create a Schema in Nebula Graph by following these steps: Identify the Schema elements. The Schema elements in the Nebula Graph are shown in the following table. Element Name Property Tag player name string, age int Tag team name string Edge Type follow degree int Edge Type serve start_year int, end_year int Create a graph space basketballplayer in the Nebula Graph and create a Schema as shown below. ## Create a graph space. nebula> CREATE SPACE basketballplayer \\ (partition_num = 10, \\ replica_factor = 1, \\ vid_type = FIXED_STRING(30)); ## Use the graph space basketballplayer. nebula> USE basketballplayer; ## Create the Tag player. nebula> CREATE TAG player(name string, age int); ## Create the Tag team. nebula> CREATE TAG team(name string); ## Create the Edge type follow. nebula> CREATE EDGE follow(degree int); ## Create the Edge type serve. nebula> CREATE EDGE serve(start_year int, end_year int); For more information, see Quick start workflow . Step 2: Process ORC files \u00b6 Confirm the following information: Process ORC files to meet Schema requirements. Obtain the ORC file storage path. Step 3: Modify configuration files \u00b6 After Exchange is compiled, copy the conf file target/classes/application.conf to set ORC data source configuration. In this example, the copied file is called orc_application.conf . For details on each configuration item, see Parameters in the configuration file . { # Spark configuration spark: { app: { name: Nebula Exchange 3.0.0 } driver: { cores: 1 maxResultSize: 1G } executor: { memory:1G } cores: { max: 16 } } # Nebula Graph configuration nebula: { address:{ # Specify the IP addresses and ports for Graph and all Meta services. # If there are multiple addresses, the format is \"ip1:port\",\"ip2:port\",\"ip3:port\". # Addresses are separated by commas. graph:[\"127.0.0.1:9669\"] meta:[\"127.0.0.1:9559\"] } # The account entered must have write permission for the Nebula Graph space. user: root pswd: nebula # Fill in the name of the graph space you want to write data to in the Nebula Graph. space: basketballplayer connection: { timeout: 3000 retry: 3 } execution: { retry: 3 } error: { max: 32 output: /tmp/errors } rate: { limit: 1024 timeout: 1000 } } # Processing vertexes tags: [ # Set the information about the Tag player. { name: player type: { # Specify the data source file format to ORC. source: orc # Specify how to import the data into Nebula Graph: Client or SST. sink: client } # Specify the path to the ORC file. # If the file is stored in HDFS, use double quotation marks to enclose the file path, starting with hdfs://. For example, \"hdfs://ip:port/xx/xx\". # If the file is stored locally, use double quotation marks to enclose the file path, starting with file://. For example, \"file:///tmp/xx.orc\". path: \"hdfs://192.168.*.*:9000/data/vertex_player.orc\" # Specify the key name in the ORC file in fields, and its corresponding value will serve as the data source for the properties specified in the Nebula Graph. # If multiple values need to be specified, separate them with commas. fields: [age,name] # Specify the property names defined in Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. nebula.fields: [age, name] # Specify a column of data in the table as the source of VIDs in the Nebula Graph. # The value of vertex must be consistent with the field in the ORC file. # Currently, Nebula Graph 3.1.0 supports only strings or integers of VID. vertex: { field:id } # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 } # Set the information about the Tag team. { # Specify the Tag name defined in Nebula Graph. name: team type: { # Specify the data source file format to ORC. source: orc # Specify how to import the data into Nebula Graph: Client or SST. sink: client } # Specify the path to the ORC file. # If the file is stored in HDFS, use double quotation marks to enclose the file path, starting with hdfs://. For example, \"hdfs://ip:port/xx/xx\". # If the file is stored locally, use double quotation marks to enclose the file path, starting with file://. For example, \"file:///tmp/xx.orc\". path: \"hdfs://192.168.*.*:9000/data/vertex_team.orc\" # Specify the key name in the ORC file in fields, and its corresponding value will serve as the data source for the properties specified in the Nebula Graph. # If multiple values need to be specified, separate them with commas. fields: [name] # Specify the property names defined in Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. nebula.fields: [name] # Specify a column of data in the table as the source of VIDs in the Nebula Graph. # The value of vertex must be consistent with the field in the ORC file. # Currently, Nebula Graph 3.1.0 supports only strings or integers of VID. vertex: { field:id } # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 } # If more vertexes need to be added, refer to the previous configuration to add them. ] # Processing edges edges: [ # Set the information about the Edge Type follow. { # Specify the Edge Type name defined in Nebula Graph. name: follow type: { # Specify the data source file format to ORC. source: orc # Specify how to import the data into Nebula Graph: Client or SST. sink: client } # Specify the path to the ORC file. # If the file is stored in HDFS, use double quotation marks to enclose the file path, starting with hdfs://. For example, \"hdfs://ip:port/xx/xx\". # If the file is stored locally, use double quotation marks to enclose the file path, starting with file://. For example, \"file:///tmp/xx.orc\". path: \"hdfs://192.168.*.*:9000/data/edge_follow.orc\" # Specify the key name in the ORC file in fields, and its corresponding value will serve as the data source for the properties specified in the Nebula Graph. # If multiple values need to be specified, separate them with commas. fields: [degree] # Specify the property names defined in Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. nebula.fields: [degree] # Specify a column as the source for the source and destination vertexes. # The value of vertex must be consistent with the field in the ORC file. # Currently, Nebula Graph 3.1.0 supports only strings or integers of VID. source: { field: src } target: { field: dst } # (Optional) Specify a column as the source of the rank. #ranking: rank # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 } # Set the information about the Edge type serve. { # Specify the Edge type name defined in Nebula Graph. name: serve type: { # Specify the data source file format to ORC. source: orc # Specify how to import the data into Nebula Graph: Client or SST. sink: client } # Specify the path to the ORC file. # If the file is stored in HDFS, use double quotation marks to enclose the file path, starting with hdfs://. For example, \"hdfs://ip:port/xx/xx\". # If the file is stored locally, use double quotation marks to enclose the file path, starting with file://. For example, \"file:///tmp/xx.orc\". path: \"hdfs://192.168.*.*:9000/data/edge_serve.orc\" # Specify the key name in the ORC file in fields, and its corresponding value will serve as the data source for the properties specified in the Nebula Graph. # If multiple values need to be specified, separate them with commas. fields: [start_year,end_year] # Specify the property names defined in Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. nebula.fields: [start_year, end_year] # Specify a column as the source for the source and destination vertexes. # The value of vertex must be consistent with the field in the ORC file. # Currently, Nebula Graph 3.1.0 supports only strings or integers of VID. source: { field: src } target: { field: dst } # (Optional) Specify a column as the source of the rank. #ranking: _c5 # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 } # If more edges need to be added, refer to the previous configuration to add them. } Step 4: Import data into Nebula Graph \u00b6 Run the following command to import ORC data into Nebula Graph. For a description of the parameters, see Options for import . ${ SPARK_HOME } /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange <nebula-exchange-3.0.0.jar_path> -c <orc_application.conf_path> Note JAR packages are available in two ways: compiled them yourself , or download the compiled .jar file directly. For example: ${ SPARK_HOME } /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange /root/nebula-exchange/nebula-exchange/target/nebula-exchange-3.0.0.jar -c /root/nebula-exchange/nebula-exchange/target/classes/orc_application.conf You can search for batchSuccess.<tag_name/edge_name> in the command output to check the number of successes. For example, batchSuccess.follow: 300 . Step 5: (optional) Validate data \u00b6 Users can verify that data has been imported by executing a query in the Nebula Graph client (for example, Nebula Graph Studio). For example: GO FROM \"player100\" OVER follow; Users can also run the SHOW STATS command to view statistics. Step 6: (optional) Rebuild indexes in Nebula Graph \u00b6 With the data imported, users can recreate and rebuild indexes in Nebula Graph. For details, see Index overview .","title":"Import data from ORC files"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-orc/#import_data_from_orc_files","text":"This topic provides an example of how to use Exchange to import Nebula Graph data stored in HDFS or local ORC files. To import a local ORC file to Nebula Graph, see Nebula Importer .","title":"Import data from ORC files"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-orc/#data_set","text":"This topic takes the basketballplayer dataset as an example.","title":"Data set"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-orc/#environment","text":"This example is done on MacOS. Here is the environment configuration information: Hardware specifications: CPU: 1.7 GHz Quad-Core Intel Core i7 Memory: 16 GB Spark: 2.4.7, stand-alone Hadoop: 2.9.2, pseudo-distributed deployment Nebula Graph: 3.1.0. Deploy Nebula Graph with Docker Compose .","title":"Environment"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-orc/#prerequisites","text":"Before importing data, you need to confirm the following information: Nebula Graph has been installed and deployed with the following information: IP addresses and ports of Graph and Meta services. The user name and password with write permission to Nebula Graph. Exchange has been compiled , or download the compiled .jar file directly. Spark has been installed. Learn about the Schema created in Nebula Graph, including names and properties of Tags and Edge types, and more. If files are stored in HDFS, ensure that the Hadoop service is running properly. If files are stored locally and Nebula Graph is a cluster architecture, you need to place the files in the same directory locally on each machine in the cluster.","title":"Prerequisites"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-orc/#steps","text":"","title":"Steps"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-orc/#step_1_create_the_schema_in_nebula_graph","text":"Analyze the data to create a Schema in Nebula Graph by following these steps: Identify the Schema elements. The Schema elements in the Nebula Graph are shown in the following table. Element Name Property Tag player name string, age int Tag team name string Edge Type follow degree int Edge Type serve start_year int, end_year int Create a graph space basketballplayer in the Nebula Graph and create a Schema as shown below. ## Create a graph space. nebula> CREATE SPACE basketballplayer \\ (partition_num = 10, \\ replica_factor = 1, \\ vid_type = FIXED_STRING(30)); ## Use the graph space basketballplayer. nebula> USE basketballplayer; ## Create the Tag player. nebula> CREATE TAG player(name string, age int); ## Create the Tag team. nebula> CREATE TAG team(name string); ## Create the Edge type follow. nebula> CREATE EDGE follow(degree int); ## Create the Edge type serve. nebula> CREATE EDGE serve(start_year int, end_year int); For more information, see Quick start workflow .","title":"Step 1: Create the Schema in Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-orc/#step_2_process_orc_files","text":"Confirm the following information: Process ORC files to meet Schema requirements. Obtain the ORC file storage path.","title":"Step 2: Process ORC files"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-orc/#step_3_modify_configuration_files","text":"After Exchange is compiled, copy the conf file target/classes/application.conf to set ORC data source configuration. In this example, the copied file is called orc_application.conf . For details on each configuration item, see Parameters in the configuration file . { # Spark configuration spark: { app: { name: Nebula Exchange 3.0.0 } driver: { cores: 1 maxResultSize: 1G } executor: { memory:1G } cores: { max: 16 } } # Nebula Graph configuration nebula: { address:{ # Specify the IP addresses and ports for Graph and all Meta services. # If there are multiple addresses, the format is \"ip1:port\",\"ip2:port\",\"ip3:port\". # Addresses are separated by commas. graph:[\"127.0.0.1:9669\"] meta:[\"127.0.0.1:9559\"] } # The account entered must have write permission for the Nebula Graph space. user: root pswd: nebula # Fill in the name of the graph space you want to write data to in the Nebula Graph. space: basketballplayer connection: { timeout: 3000 retry: 3 } execution: { retry: 3 } error: { max: 32 output: /tmp/errors } rate: { limit: 1024 timeout: 1000 } } # Processing vertexes tags: [ # Set the information about the Tag player. { name: player type: { # Specify the data source file format to ORC. source: orc # Specify how to import the data into Nebula Graph: Client or SST. sink: client } # Specify the path to the ORC file. # If the file is stored in HDFS, use double quotation marks to enclose the file path, starting with hdfs://. For example, \"hdfs://ip:port/xx/xx\". # If the file is stored locally, use double quotation marks to enclose the file path, starting with file://. For example, \"file:///tmp/xx.orc\". path: \"hdfs://192.168.*.*:9000/data/vertex_player.orc\" # Specify the key name in the ORC file in fields, and its corresponding value will serve as the data source for the properties specified in the Nebula Graph. # If multiple values need to be specified, separate them with commas. fields: [age,name] # Specify the property names defined in Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. nebula.fields: [age, name] # Specify a column of data in the table as the source of VIDs in the Nebula Graph. # The value of vertex must be consistent with the field in the ORC file. # Currently, Nebula Graph 3.1.0 supports only strings or integers of VID. vertex: { field:id } # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 } # Set the information about the Tag team. { # Specify the Tag name defined in Nebula Graph. name: team type: { # Specify the data source file format to ORC. source: orc # Specify how to import the data into Nebula Graph: Client or SST. sink: client } # Specify the path to the ORC file. # If the file is stored in HDFS, use double quotation marks to enclose the file path, starting with hdfs://. For example, \"hdfs://ip:port/xx/xx\". # If the file is stored locally, use double quotation marks to enclose the file path, starting with file://. For example, \"file:///tmp/xx.orc\". path: \"hdfs://192.168.*.*:9000/data/vertex_team.orc\" # Specify the key name in the ORC file in fields, and its corresponding value will serve as the data source for the properties specified in the Nebula Graph. # If multiple values need to be specified, separate them with commas. fields: [name] # Specify the property names defined in Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. nebula.fields: [name] # Specify a column of data in the table as the source of VIDs in the Nebula Graph. # The value of vertex must be consistent with the field in the ORC file. # Currently, Nebula Graph 3.1.0 supports only strings or integers of VID. vertex: { field:id } # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 } # If more vertexes need to be added, refer to the previous configuration to add them. ] # Processing edges edges: [ # Set the information about the Edge Type follow. { # Specify the Edge Type name defined in Nebula Graph. name: follow type: { # Specify the data source file format to ORC. source: orc # Specify how to import the data into Nebula Graph: Client or SST. sink: client } # Specify the path to the ORC file. # If the file is stored in HDFS, use double quotation marks to enclose the file path, starting with hdfs://. For example, \"hdfs://ip:port/xx/xx\". # If the file is stored locally, use double quotation marks to enclose the file path, starting with file://. For example, \"file:///tmp/xx.orc\". path: \"hdfs://192.168.*.*:9000/data/edge_follow.orc\" # Specify the key name in the ORC file in fields, and its corresponding value will serve as the data source for the properties specified in the Nebula Graph. # If multiple values need to be specified, separate them with commas. fields: [degree] # Specify the property names defined in Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. nebula.fields: [degree] # Specify a column as the source for the source and destination vertexes. # The value of vertex must be consistent with the field in the ORC file. # Currently, Nebula Graph 3.1.0 supports only strings or integers of VID. source: { field: src } target: { field: dst } # (Optional) Specify a column as the source of the rank. #ranking: rank # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 } # Set the information about the Edge type serve. { # Specify the Edge type name defined in Nebula Graph. name: serve type: { # Specify the data source file format to ORC. source: orc # Specify how to import the data into Nebula Graph: Client or SST. sink: client } # Specify the path to the ORC file. # If the file is stored in HDFS, use double quotation marks to enclose the file path, starting with hdfs://. For example, \"hdfs://ip:port/xx/xx\". # If the file is stored locally, use double quotation marks to enclose the file path, starting with file://. For example, \"file:///tmp/xx.orc\". path: \"hdfs://192.168.*.*:9000/data/edge_serve.orc\" # Specify the key name in the ORC file in fields, and its corresponding value will serve as the data source for the properties specified in the Nebula Graph. # If multiple values need to be specified, separate them with commas. fields: [start_year,end_year] # Specify the property names defined in Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. nebula.fields: [start_year, end_year] # Specify a column as the source for the source and destination vertexes. # The value of vertex must be consistent with the field in the ORC file. # Currently, Nebula Graph 3.1.0 supports only strings or integers of VID. source: { field: src } target: { field: dst } # (Optional) Specify a column as the source of the rank. #ranking: _c5 # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 } # If more edges need to be added, refer to the previous configuration to add them. }","title":"Step 3: Modify configuration files"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-orc/#step_4_import_data_into_nebula_graph","text":"Run the following command to import ORC data into Nebula Graph. For a description of the parameters, see Options for import . ${ SPARK_HOME } /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange <nebula-exchange-3.0.0.jar_path> -c <orc_application.conf_path> Note JAR packages are available in two ways: compiled them yourself , or download the compiled .jar file directly. For example: ${ SPARK_HOME } /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange /root/nebula-exchange/nebula-exchange/target/nebula-exchange-3.0.0.jar -c /root/nebula-exchange/nebula-exchange/target/classes/orc_application.conf You can search for batchSuccess.<tag_name/edge_name> in the command output to check the number of successes. For example, batchSuccess.follow: 300 .","title":"Step 4: Import data into Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-orc/#step_5_optional_validate_data","text":"Users can verify that data has been imported by executing a query in the Nebula Graph client (for example, Nebula Graph Studio). For example: GO FROM \"player100\" OVER follow; Users can also run the SHOW STATS command to view statistics.","title":"Step 5: (optional) Validate data"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-orc/#step_6_optional_rebuild_indexes_in_nebula_graph","text":"With the data imported, users can recreate and rebuild indexes in Nebula Graph. For details, see Index overview .","title":"Step 6: (optional) Rebuild indexes in Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-parquet/","text":"Import data from Parquet files \u00b6 This topic provides an example of how to use Exchange to import Nebula Graph data stored in HDFS or local Parquet files. To import a local Parquet file to Nebula Graph, see Nebula Importer . Data set \u00b6 This topic takes the basketballplayer dataset as an example. Environment \u00b6 This example is done on MacOS. Here is the environment configuration information: Hardware specifications: CPU: 1.7 GHz Quad-Core Intel Core i7 Memory: 16 GB Spark: 2.4.7, stand-alone Hadoop: 2.9.2, pseudo-distributed deployment Nebula Graph: 3.1.0. Deploy Nebula Graph with Docker Compose . Prerequisites \u00b6 Before importing data, you need to confirm the following information: Nebula Graph has been installed and deployed with the following information: IP addresses and ports of Graph and Meta services. The user name and password with write permission to Nebula Graph. Exchange has been compiled , or download the compiled .jar file directly. Spark has been installed. Learn about the Schema created in Nebula Graph, including names and properties of Tags and Edge types, and more. If files are stored in HDFS, ensure that the Hadoop service is running properly. If files are stored locally and Nebula Graph is a cluster architecture, you need to place the files in the same directory locally on each machine in the cluster. Steps \u00b6 Step 1: Create the Schema in Nebula Graph \u00b6 Analyze the data to create a Schema in Nebula Graph by following these steps: Identify the Schema elements. The Schema elements in the Nebula Graph are shown in the following table. Element Name Property Tag player name string, age int Tag team name string Edge Type follow degree int Edge Type serve start_year int, end_year int Create a graph space basketballplayer in the Nebula Graph and create a Schema as shown below. ## Create a graph space. nebula> CREATE SPACE basketballplayer \\ (partition_num = 10, \\ replica_factor = 1, \\ vid_type = FIXED_STRING(30)); ## Use the graph space basketballplayer. nebula> USE basketballplayer; ## Create the Tag player. nebula> CREATE TAG player(name string, age int); ## Create the Tag team. nebula> CREATE TAG team(name string); ## Create the Edge type follow. nebula> CREATE EDGE follow(degree int); ## Create the Edge type serve. nebula> CREATE EDGE serve(start_year int, end_year int); For more information, see Quick start workflow . Step 2: Process Parquet files \u00b6 Confirm the following information: Process Parquet files to meet Schema requirements. Obtain the Parquet file storage path. Step 3: Modify configuration files \u00b6 After Exchange is compiled, copy the conf file target/classes/application.conf to set Parquet data source configuration. In this example, the copied file is called parquet_application.conf . For details on each configuration item, see Parameters in the configuration file . { # Spark configuration spark: { app: { name: Nebula Exchange 3.0.0 } driver: { cores: 1 maxResultSize: 1G } executor: { memory:1G } cores: { max: 16 } } # Nebula Graph configuration nebula: { address:{ # Specify the IP addresses and ports for Graph and all Meta services. # If there are multiple addresses, the format is \"ip1:port\",\"ip2:port\",\"ip3:port\". # Addresses are separated by commas. graph:[\"127.0.0.1:9669\"] meta:[\"127.0.0.1:9559\"] } # The account entered must have write permission for the Nebula Graph space. user: root pswd: nebula # Fill in the name of the graph space you want to write data to in the Nebula Graph. space: basketballplayer connection: { timeout: 3000 retry: 3 } execution: { retry: 3 } error: { max: 32 output: /tmp/errors } rate: { limit: 1024 timeout: 1000 } } # Processing vertexes tags: [ # Set the information about the Tag player. { # Specify the Tag name defined in Nebula Graph. name: player type: { # Specify the data source file format to Parquet. source: parquet # Specifies how to import the data into Nebula Graph: Client or SST. sink: client } # Specify the path to the Parquet file. # If the file is stored in HDFS, use double quotation marks to enclose the file path, starting with hdfs://. For example, \"hdfs://ip:port/xx/xx\". # If the file is stored locally, use double quotation marks to enclose the file path, starting with file://. For example, \"file:///tmp/xx.parquet\". path: \"hdfs://192.168.*.13:9000/data/vertex_player.parquet\" # Specify the key name in the Parquet file in fields, and its corresponding value will serve as the data source for the properties specified in the Nebula Graph. # If multiple values need to be specified, separate them with commas. fields: [age,name] # Specify the property name defined in Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. nebula.fields: [age, name] # Specify a column of data in the table as the source of VIDs in the Nebula Graph. # The value of vertex must be consistent with the field in the Parquet file. # Currently, Nebula Graph 3.1.0 supports only strings or integers of VID. vertex: { field:id } # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 } # Set the information about the Tag team. { # Specify the Tag name defined in Nebula Graph. name: team type: { # Specify the data source file format to Parquet. source: parquet # Specifies how to import the data into Nebula Graph: Client or SST. sink: client } # Specify the path to the Parquet file. # If the file is stored in HDFS, use double quotation marks to enclose the file path, starting with hdfs://. For example, \"hdfs://ip:port/xx/xx\". # If the file is stored locally, use double quotation marks to enclose the file path, starting with file://. For example, \"file:///tmp/xx.parquet\". path: \"hdfs://192.168.11.13:9000/data/vertex_team.parquet\" # Specify the key name in the Parquet file in fields, and its corresponding value will serve as the data source for the properties specified in the Nebula Graph. # If multiple values need to be specified, separate them with commas. fields: [name] # Specify the property name defined in Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. nebula.fields: [name] # Specify a column of data in the table as the source of VIDs in the Nebula Graph. # The value of vertex must be consistent with the field in the Parquet file. # Currently, Nebula Graph 3.1.0 supports only strings or integers of VID. vertex: { field:id } # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 } # If more vertexes need to be added, refer to the previous configuration to add them. ] # Processing edges edges: [ # Set the information about the Edge Type follow. { # Specify the Edge Type name defined in Nebula Graph. name: follow type: { # Specify the data source file format to Parquet. source: parquet # Specifies how to import the data into Nebula Graph: Client or SST. sink: client } # Specify the path to the Parquet file. # If the file is stored in HDFS, use double quotation marks to enclose the file path, starting with hdfs://. For example, \"hdfs://ip:port/xx/xx\". # If the file is stored locally, use double quotation marks to enclose the file path, starting with file://. For example, \"file:///tmp/xx.parquet\". path: \"hdfs://192.168.11.13:9000/data/edge_follow.parquet\" # Specify the key name in the Parquet file in fields, and its corresponding value will serve as the data source for the properties specified in the Nebula Graph. # If multiple values need to be specified, separate them with commas. fields: [degree] # Specify the property name defined in Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. nebula.fields: [degree] # Specify a column as the source for the source and destination vertexes. # The values of vertex must be consistent with the fields in the Parquet file. # Currently, Nebula Graph 3.1.0 supports only strings or integers of VID. source: { field: src } target: { field: dst } # (Optional) Specify a column as the source of the rank. #ranking: rank # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 } # Set the information about the Edge type serve. { # Specify the Edge type name defined in Nebula Graph. name: serve type: { # Specify the data source file format to Parquet. source: parquet # Specifies how to import the data into Nebula Graph: Client or SST. sink: client } # Specify the path to the Parquet file. # If the file is stored in HDFS, use double quotation marks to enclose the file path, starting with hdfs://. For example, \"hdfs://ip:port/xx/xx\". # If the file is stored locally, use double quotation marks to enclose the file path, starting with file://. For example, \"file:///tmp/xx.parquet\". path: \"hdfs://192.168.11.13:9000/data/edge_serve.parquet\" # Specify the key name in the Parquet file in fields, and its corresponding value will serve as the data source for the properties specified in the Nebula Graph. # If multiple values need to be specified, separate them with commas. fields: [start_year,end_year] # Specify the property name defined in Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. nebula.fields: [start_year, end_year] # Specify a column as the source for the source and destination vertexes. # The values of vertex must be consistent with the fields in the Parquet file. # Currently, Nebula Graph 3.1.0 supports only strings or integers of VID. source: { field: src } target: { field: dst } # (Optional) Specify a column as the source of the rank. #ranking: _c5 # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 } ] # If more edges need to be added, refer to the previous configuration to add them. } Step 4: Import data into Nebula Graph \u00b6 Run the following command to import Parquet data into Nebula Graph. For a description of the parameters, see Options for import . ${ SPARK_HOME } /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange <nebula-exchange-3.0.0.jar_path> -c <parquet_application.conf_path> Note JAR packages are available in two ways: compiled them yourself , or download the compiled .jar file directly. For example: ${ SPARK_HOME } /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange /root/nebula-exchange/nebula-exchange/target/nebula-exchange-3.0.0.jar -c /root/nebula-exchange/nebula-exchange/target/classes/parquet_application.conf You can search for batchSuccess.<tag_name/edge_name> in the command output to check the number of successes. For example, batchSuccess.follow: 300 . Step 5: (optional) Validate data \u00b6 Users can verify that data has been imported by executing a query in the Nebula Graph client (for example, Nebula Graph Studio). For example: GO FROM \"player100\" OVER follow; Users can also run the SHOW STATS command to view statistics. Step 6: (optional) Rebuild indexes in Nebula Graph \u00b6 With the data imported, users can recreate and rebuild indexes in Nebula Graph. For details, see Index overview .","title":"Import data from Parquet files"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-parquet/#import_data_from_parquet_files","text":"This topic provides an example of how to use Exchange to import Nebula Graph data stored in HDFS or local Parquet files. To import a local Parquet file to Nebula Graph, see Nebula Importer .","title":"Import data from Parquet files"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-parquet/#data_set","text":"This topic takes the basketballplayer dataset as an example.","title":"Data set"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-parquet/#environment","text":"This example is done on MacOS. Here is the environment configuration information: Hardware specifications: CPU: 1.7 GHz Quad-Core Intel Core i7 Memory: 16 GB Spark: 2.4.7, stand-alone Hadoop: 2.9.2, pseudo-distributed deployment Nebula Graph: 3.1.0. Deploy Nebula Graph with Docker Compose .","title":"Environment"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-parquet/#prerequisites","text":"Before importing data, you need to confirm the following information: Nebula Graph has been installed and deployed with the following information: IP addresses and ports of Graph and Meta services. The user name and password with write permission to Nebula Graph. Exchange has been compiled , or download the compiled .jar file directly. Spark has been installed. Learn about the Schema created in Nebula Graph, including names and properties of Tags and Edge types, and more. If files are stored in HDFS, ensure that the Hadoop service is running properly. If files are stored locally and Nebula Graph is a cluster architecture, you need to place the files in the same directory locally on each machine in the cluster.","title":"Prerequisites"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-parquet/#steps","text":"","title":"Steps"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-parquet/#step_1_create_the_schema_in_nebula_graph","text":"Analyze the data to create a Schema in Nebula Graph by following these steps: Identify the Schema elements. The Schema elements in the Nebula Graph are shown in the following table. Element Name Property Tag player name string, age int Tag team name string Edge Type follow degree int Edge Type serve start_year int, end_year int Create a graph space basketballplayer in the Nebula Graph and create a Schema as shown below. ## Create a graph space. nebula> CREATE SPACE basketballplayer \\ (partition_num = 10, \\ replica_factor = 1, \\ vid_type = FIXED_STRING(30)); ## Use the graph space basketballplayer. nebula> USE basketballplayer; ## Create the Tag player. nebula> CREATE TAG player(name string, age int); ## Create the Tag team. nebula> CREATE TAG team(name string); ## Create the Edge type follow. nebula> CREATE EDGE follow(degree int); ## Create the Edge type serve. nebula> CREATE EDGE serve(start_year int, end_year int); For more information, see Quick start workflow .","title":"Step 1: Create the Schema in Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-parquet/#step_2_process_parquet_files","text":"Confirm the following information: Process Parquet files to meet Schema requirements. Obtain the Parquet file storage path.","title":"Step 2: Process Parquet files"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-parquet/#step_3_modify_configuration_files","text":"After Exchange is compiled, copy the conf file target/classes/application.conf to set Parquet data source configuration. In this example, the copied file is called parquet_application.conf . For details on each configuration item, see Parameters in the configuration file . { # Spark configuration spark: { app: { name: Nebula Exchange 3.0.0 } driver: { cores: 1 maxResultSize: 1G } executor: { memory:1G } cores: { max: 16 } } # Nebula Graph configuration nebula: { address:{ # Specify the IP addresses and ports for Graph and all Meta services. # If there are multiple addresses, the format is \"ip1:port\",\"ip2:port\",\"ip3:port\". # Addresses are separated by commas. graph:[\"127.0.0.1:9669\"] meta:[\"127.0.0.1:9559\"] } # The account entered must have write permission for the Nebula Graph space. user: root pswd: nebula # Fill in the name of the graph space you want to write data to in the Nebula Graph. space: basketballplayer connection: { timeout: 3000 retry: 3 } execution: { retry: 3 } error: { max: 32 output: /tmp/errors } rate: { limit: 1024 timeout: 1000 } } # Processing vertexes tags: [ # Set the information about the Tag player. { # Specify the Tag name defined in Nebula Graph. name: player type: { # Specify the data source file format to Parquet. source: parquet # Specifies how to import the data into Nebula Graph: Client or SST. sink: client } # Specify the path to the Parquet file. # If the file is stored in HDFS, use double quotation marks to enclose the file path, starting with hdfs://. For example, \"hdfs://ip:port/xx/xx\". # If the file is stored locally, use double quotation marks to enclose the file path, starting with file://. For example, \"file:///tmp/xx.parquet\". path: \"hdfs://192.168.*.13:9000/data/vertex_player.parquet\" # Specify the key name in the Parquet file in fields, and its corresponding value will serve as the data source for the properties specified in the Nebula Graph. # If multiple values need to be specified, separate them with commas. fields: [age,name] # Specify the property name defined in Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. nebula.fields: [age, name] # Specify a column of data in the table as the source of VIDs in the Nebula Graph. # The value of vertex must be consistent with the field in the Parquet file. # Currently, Nebula Graph 3.1.0 supports only strings or integers of VID. vertex: { field:id } # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 } # Set the information about the Tag team. { # Specify the Tag name defined in Nebula Graph. name: team type: { # Specify the data source file format to Parquet. source: parquet # Specifies how to import the data into Nebula Graph: Client or SST. sink: client } # Specify the path to the Parquet file. # If the file is stored in HDFS, use double quotation marks to enclose the file path, starting with hdfs://. For example, \"hdfs://ip:port/xx/xx\". # If the file is stored locally, use double quotation marks to enclose the file path, starting with file://. For example, \"file:///tmp/xx.parquet\". path: \"hdfs://192.168.11.13:9000/data/vertex_team.parquet\" # Specify the key name in the Parquet file in fields, and its corresponding value will serve as the data source for the properties specified in the Nebula Graph. # If multiple values need to be specified, separate them with commas. fields: [name] # Specify the property name defined in Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. nebula.fields: [name] # Specify a column of data in the table as the source of VIDs in the Nebula Graph. # The value of vertex must be consistent with the field in the Parquet file. # Currently, Nebula Graph 3.1.0 supports only strings or integers of VID. vertex: { field:id } # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 } # If more vertexes need to be added, refer to the previous configuration to add them. ] # Processing edges edges: [ # Set the information about the Edge Type follow. { # Specify the Edge Type name defined in Nebula Graph. name: follow type: { # Specify the data source file format to Parquet. source: parquet # Specifies how to import the data into Nebula Graph: Client or SST. sink: client } # Specify the path to the Parquet file. # If the file is stored in HDFS, use double quotation marks to enclose the file path, starting with hdfs://. For example, \"hdfs://ip:port/xx/xx\". # If the file is stored locally, use double quotation marks to enclose the file path, starting with file://. For example, \"file:///tmp/xx.parquet\". path: \"hdfs://192.168.11.13:9000/data/edge_follow.parquet\" # Specify the key name in the Parquet file in fields, and its corresponding value will serve as the data source for the properties specified in the Nebula Graph. # If multiple values need to be specified, separate them with commas. fields: [degree] # Specify the property name defined in Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. nebula.fields: [degree] # Specify a column as the source for the source and destination vertexes. # The values of vertex must be consistent with the fields in the Parquet file. # Currently, Nebula Graph 3.1.0 supports only strings or integers of VID. source: { field: src } target: { field: dst } # (Optional) Specify a column as the source of the rank. #ranking: rank # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 } # Set the information about the Edge type serve. { # Specify the Edge type name defined in Nebula Graph. name: serve type: { # Specify the data source file format to Parquet. source: parquet # Specifies how to import the data into Nebula Graph: Client or SST. sink: client } # Specify the path to the Parquet file. # If the file is stored in HDFS, use double quotation marks to enclose the file path, starting with hdfs://. For example, \"hdfs://ip:port/xx/xx\". # If the file is stored locally, use double quotation marks to enclose the file path, starting with file://. For example, \"file:///tmp/xx.parquet\". path: \"hdfs://192.168.11.13:9000/data/edge_serve.parquet\" # Specify the key name in the Parquet file in fields, and its corresponding value will serve as the data source for the properties specified in the Nebula Graph. # If multiple values need to be specified, separate them with commas. fields: [start_year,end_year] # Specify the property name defined in Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. nebula.fields: [start_year, end_year] # Specify a column as the source for the source and destination vertexes. # The values of vertex must be consistent with the fields in the Parquet file. # Currently, Nebula Graph 3.1.0 supports only strings or integers of VID. source: { field: src } target: { field: dst } # (Optional) Specify a column as the source of the rank. #ranking: _c5 # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 } ] # If more edges need to be added, refer to the previous configuration to add them. }","title":"Step 3: Modify configuration files"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-parquet/#step_4_import_data_into_nebula_graph","text":"Run the following command to import Parquet data into Nebula Graph. For a description of the parameters, see Options for import . ${ SPARK_HOME } /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange <nebula-exchange-3.0.0.jar_path> -c <parquet_application.conf_path> Note JAR packages are available in two ways: compiled them yourself , or download the compiled .jar file directly. For example: ${ SPARK_HOME } /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange /root/nebula-exchange/nebula-exchange/target/nebula-exchange-3.0.0.jar -c /root/nebula-exchange/nebula-exchange/target/classes/parquet_application.conf You can search for batchSuccess.<tag_name/edge_name> in the command output to check the number of successes. For example, batchSuccess.follow: 300 .","title":"Step 4: Import data into Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-parquet/#step_5_optional_validate_data","text":"Users can verify that data has been imported by executing a query in the Nebula Graph client (for example, Nebula Graph Studio). For example: GO FROM \"player100\" OVER follow; Users can also run the SHOW STATS command to view statistics.","title":"Step 5: (optional) Validate data"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-parquet/#step_6_optional_rebuild_indexes_in_nebula_graph","text":"With the data imported, users can recreate and rebuild indexes in Nebula Graph. For details, see Index overview .","title":"Step 6: (optional) Rebuild indexes in Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-pulsar/","text":"Import data from Pulsar \u00b6 This topic provides an example of how to use Exchange to import Nebula Graph data stored in Pulsar. Environment \u00b6 This example is done on MacOS. Here is the environment configuration information: Hardware specifications: CPU: 1.7 GHz Quad-Core Intel Core i7 Memory: 16 GB Spark: 2.4.7, stand-alone Nebula Graph: 3.1.0. Deploy Nebula Graph with Docker Compose . Prerequisites \u00b6 Before importing data, you need to confirm the following information: Nebula Graph has been installed and deployed with the following information: IP addresses and ports of Graph and Meta services. The user name and password with write permission to Nebula Graph. Exchange has been compiled , or download the compiled .jar file directly. Spark has been installed. Learn about the Schema created in Nebula Graph, including names and properties of Tags and Edge types, and more. The Pulsar service has been installed and started. Steps \u00b6 Step 1: Create the Schema in Nebula Graph \u00b6 Analyze the data to create a Schema in Nebula Graph by following these steps: Identify the Schema elements. The Schema elements in the Nebula Graph are shown in the following table. Element Name Property Tag player name string, age int Tag team name string Edge Type follow degree int Edge Type serve start_year int, end_year int Create a graph space basketballplayer in the Nebula Graph and create a Schema as shown below. ## Create a graph space nebula> CREATE SPACE basketballplayer \\ (partition_num = 10, \\ replica_factor = 1, \\ vid_type = FIXED_STRING(30)); ## Use the graph space basketballplayer nebula> USE basketballplayer; ## Create the Tag player nebula> CREATE TAG player(name string, age int); ## Create the Tag team nebula> CREATE TAG team(name string); ## Create the Edge type follow nebula> CREATE EDGE follow(degree int); ## Create the Edge type serve nebula> CREATE EDGE serve(start_year int, end_year int); For more information, see Quick start workflow . Step 2: Modify configuration files \u00b6 After Exchange is compiled, copy the conf file target/classes/application.conf to set Pulsar data source configuration. In this example, the copied file is called pulsar_application.conf . For details on each configuration item, see Parameters in the configuration file . { # Spark configuration spark: { app: { name: Nebula Exchange 3.0.0 } driver: { cores: 1 maxResultSize: 1G } cores: { max: 16 } } # Nebula Graph configuration nebula: { address:{ # Specify the IP addresses and ports for Graph and all Meta services. # If there are multiple addresses, the format is \"ip1:port\",\"ip2:port\",\"ip3:port\". # Addresses are separated by commas. graph:[\"127.0.0.1:9669\"] meta:[\"127.0.0.1:9559\"] } # The account entered must have write permission for the Nebula Graph space. user: root pswd: nebula # Fill in the name of the graph space you want to write data to in the Nebula Graph. space: basketballplayer connection: { timeout: 3000 retry: 3 } execution: { retry: 3 } error: { max: 32 output: /tmp/errors } rate: { limit: 1024 timeout: 1000 } } # Processing vertices tags: [ # Set the information about the Tag player. { # The corresponding Tag name in Nebula Graph. name: player type: { # Specify the data source file format to Pulsar. source: pulsar # Specify how to import the data into Nebula Graph: Client or SST. sink: client } # The address of the Pulsar server. service: \"pulsar://127.0.0.1:6650\" # admin.url of pulsar. admin: \"http://127.0.0.1:8081\" # The Pulsar option can be configured from topic, topics or topicsPattern. options: { topics: \"topic1,topic2\" } # Specify the column names in the player table in fields, and their corresponding values are specified as properties in the Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. # If multiple column names need to be specified, separate them by commas. fields: [age,name] nebula.fields: [age,name] # Specify a column of data in the table as the source of VIDs in the Nebula Graph. vertex:{ field:playerid } # The number of data written to Nebula Graph in a single batch. batch: 10 # The number of Spark partitions. partition: 10 # The interval for message reading. Unit: second. interval.seconds: 10 } # Set the information about the Tag Team. { name: team type: { source: pulsar sink: client } service: \"pulsar://127.0.0.1:6650\" admin: \"http://127.0.0.1:8081\" options: { topics: \"topic1,topic2\" } fields: [name] nebula.fields: [name] vertex:{ field:teamid } batch: 10 partition: 10 interval.seconds: 10 } ] # Processing edges edges: [ # Set the information about Edge Type follow { # The corresponding Edge Type name in Nebula Graph. name: follow type: { # Specify the data source file format to Pulsar. source: pulsar # Specify how to import the Edge type data into Nebula Graph. # Specify how to import the data into Nebula Graph: Client or SST. sink: client } # The address of the Pulsar server. service: \"pulsar://127.0.0.1:6650\" # admin.url of pulsar. admin: \"http://127.0.0.1:8081\" # The Pulsar option can be configured from topic, topics or topicsPattern. options: { topics: \"topic1,topic2\" } # Specify the column names in the follow table in fields, and their corresponding values are specified as properties in the Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. # If multiple column names need to be specified, separate them by commas. fields: [degree] nebula.fields: [degree] # In source, use a column in the follow table as the source of the edge's source vertex. # In target, use a column in the follow table as the source of the edge's destination vertex. source:{ field:src_player } target:{ field:dst_player } # (Optional) Specify a column as the source of the rank. #ranking: rank # The number of data written to Nebula Graph in a single batch. batch: 10 # The number of Spark partitions. partition: 10 # The interval for message reading. Unit: second. interval.seconds: 10 } # Set the information about the Edge Type serve { name: serve type: { source: Pulsar sink: client } service: \"pulsar://127.0.0.1:6650\" admin: \"http://127.0.0.1:8081\" options: { topics: \"topic1,topic2\" } fields: [start_year,end_year] nebula.fields: [start_year,end_year] source:{ field:playerid } target:{ field:teamid } # (Optional) Specify a column as the source of the rank. #ranking: rank batch: 10 partition: 10 interval.seconds: 10 } ] } Step 3: Import data into Nebula Graph \u00b6 Run the following command to import Pulsar data into Nebula Graph. For a description of the parameters, see Options for import . ${ SPARK_HOME } /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange <nebula-exchange-3.0.0.jar_path> -c <pulsar_application.conf_path> Note JAR packages are available in two ways: compiled them yourself , or download the compiled .jar file directly. For example: ${ SPARK_HOME } /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange /root/nebula-exchange/nebula-exchange/target/nebula-exchange-3.0.0.jar -c /root/nebula-exchange/nebula-exchange/target/classes/pulsar_application.conf You can search for batchSuccess.<tag_name/edge_name> in the command output to check the number of successes. For example, batchSuccess.follow: 300 . Step 4: (optional) Validate data \u00b6 Users can verify that data has been imported by executing a query in the Nebula Graph client (for example, Nebula Graph Studio). For example: GO FROM \"player100\" OVER follow; Users can also run the SHOW STATS command to view statistics. Step 5: (optional) Rebuild indexes in Nebula Graph \u00b6 With the data imported, users can recreate and rebuild indexes in Nebula Graph. For details, see Index overview .","title":"Import data from Pulsar"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-pulsar/#import_data_from_pulsar","text":"This topic provides an example of how to use Exchange to import Nebula Graph data stored in Pulsar.","title":"Import data from Pulsar"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-pulsar/#environment","text":"This example is done on MacOS. Here is the environment configuration information: Hardware specifications: CPU: 1.7 GHz Quad-Core Intel Core i7 Memory: 16 GB Spark: 2.4.7, stand-alone Nebula Graph: 3.1.0. Deploy Nebula Graph with Docker Compose .","title":"Environment"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-pulsar/#prerequisites","text":"Before importing data, you need to confirm the following information: Nebula Graph has been installed and deployed with the following information: IP addresses and ports of Graph and Meta services. The user name and password with write permission to Nebula Graph. Exchange has been compiled , or download the compiled .jar file directly. Spark has been installed. Learn about the Schema created in Nebula Graph, including names and properties of Tags and Edge types, and more. The Pulsar service has been installed and started.","title":"Prerequisites"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-pulsar/#steps","text":"","title":"Steps"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-pulsar/#step_1_create_the_schema_in_nebula_graph","text":"Analyze the data to create a Schema in Nebula Graph by following these steps: Identify the Schema elements. The Schema elements in the Nebula Graph are shown in the following table. Element Name Property Tag player name string, age int Tag team name string Edge Type follow degree int Edge Type serve start_year int, end_year int Create a graph space basketballplayer in the Nebula Graph and create a Schema as shown below. ## Create a graph space nebula> CREATE SPACE basketballplayer \\ (partition_num = 10, \\ replica_factor = 1, \\ vid_type = FIXED_STRING(30)); ## Use the graph space basketballplayer nebula> USE basketballplayer; ## Create the Tag player nebula> CREATE TAG player(name string, age int); ## Create the Tag team nebula> CREATE TAG team(name string); ## Create the Edge type follow nebula> CREATE EDGE follow(degree int); ## Create the Edge type serve nebula> CREATE EDGE serve(start_year int, end_year int); For more information, see Quick start workflow .","title":"Step 1: Create the Schema in Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-pulsar/#step_2_modify_configuration_files","text":"After Exchange is compiled, copy the conf file target/classes/application.conf to set Pulsar data source configuration. In this example, the copied file is called pulsar_application.conf . For details on each configuration item, see Parameters in the configuration file . { # Spark configuration spark: { app: { name: Nebula Exchange 3.0.0 } driver: { cores: 1 maxResultSize: 1G } cores: { max: 16 } } # Nebula Graph configuration nebula: { address:{ # Specify the IP addresses and ports for Graph and all Meta services. # If there are multiple addresses, the format is \"ip1:port\",\"ip2:port\",\"ip3:port\". # Addresses are separated by commas. graph:[\"127.0.0.1:9669\"] meta:[\"127.0.0.1:9559\"] } # The account entered must have write permission for the Nebula Graph space. user: root pswd: nebula # Fill in the name of the graph space you want to write data to in the Nebula Graph. space: basketballplayer connection: { timeout: 3000 retry: 3 } execution: { retry: 3 } error: { max: 32 output: /tmp/errors } rate: { limit: 1024 timeout: 1000 } } # Processing vertices tags: [ # Set the information about the Tag player. { # The corresponding Tag name in Nebula Graph. name: player type: { # Specify the data source file format to Pulsar. source: pulsar # Specify how to import the data into Nebula Graph: Client or SST. sink: client } # The address of the Pulsar server. service: \"pulsar://127.0.0.1:6650\" # admin.url of pulsar. admin: \"http://127.0.0.1:8081\" # The Pulsar option can be configured from topic, topics or topicsPattern. options: { topics: \"topic1,topic2\" } # Specify the column names in the player table in fields, and their corresponding values are specified as properties in the Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. # If multiple column names need to be specified, separate them by commas. fields: [age,name] nebula.fields: [age,name] # Specify a column of data in the table as the source of VIDs in the Nebula Graph. vertex:{ field:playerid } # The number of data written to Nebula Graph in a single batch. batch: 10 # The number of Spark partitions. partition: 10 # The interval for message reading. Unit: second. interval.seconds: 10 } # Set the information about the Tag Team. { name: team type: { source: pulsar sink: client } service: \"pulsar://127.0.0.1:6650\" admin: \"http://127.0.0.1:8081\" options: { topics: \"topic1,topic2\" } fields: [name] nebula.fields: [name] vertex:{ field:teamid } batch: 10 partition: 10 interval.seconds: 10 } ] # Processing edges edges: [ # Set the information about Edge Type follow { # The corresponding Edge Type name in Nebula Graph. name: follow type: { # Specify the data source file format to Pulsar. source: pulsar # Specify how to import the Edge type data into Nebula Graph. # Specify how to import the data into Nebula Graph: Client or SST. sink: client } # The address of the Pulsar server. service: \"pulsar://127.0.0.1:6650\" # admin.url of pulsar. admin: \"http://127.0.0.1:8081\" # The Pulsar option can be configured from topic, topics or topicsPattern. options: { topics: \"topic1,topic2\" } # Specify the column names in the follow table in fields, and their corresponding values are specified as properties in the Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. # If multiple column names need to be specified, separate them by commas. fields: [degree] nebula.fields: [degree] # In source, use a column in the follow table as the source of the edge's source vertex. # In target, use a column in the follow table as the source of the edge's destination vertex. source:{ field:src_player } target:{ field:dst_player } # (Optional) Specify a column as the source of the rank. #ranking: rank # The number of data written to Nebula Graph in a single batch. batch: 10 # The number of Spark partitions. partition: 10 # The interval for message reading. Unit: second. interval.seconds: 10 } # Set the information about the Edge Type serve { name: serve type: { source: Pulsar sink: client } service: \"pulsar://127.0.0.1:6650\" admin: \"http://127.0.0.1:8081\" options: { topics: \"topic1,topic2\" } fields: [start_year,end_year] nebula.fields: [start_year,end_year] source:{ field:playerid } target:{ field:teamid } # (Optional) Specify a column as the source of the rank. #ranking: rank batch: 10 partition: 10 interval.seconds: 10 } ] }","title":"Step 2: Modify configuration files"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-pulsar/#step_3_import_data_into_nebula_graph","text":"Run the following command to import Pulsar data into Nebula Graph. For a description of the parameters, see Options for import . ${ SPARK_HOME } /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange <nebula-exchange-3.0.0.jar_path> -c <pulsar_application.conf_path> Note JAR packages are available in two ways: compiled them yourself , or download the compiled .jar file directly. For example: ${ SPARK_HOME } /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange /root/nebula-exchange/nebula-exchange/target/nebula-exchange-3.0.0.jar -c /root/nebula-exchange/nebula-exchange/target/classes/pulsar_application.conf You can search for batchSuccess.<tag_name/edge_name> in the command output to check the number of successes. For example, batchSuccess.follow: 300 .","title":"Step 3: Import data into Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-pulsar/#step_4_optional_validate_data","text":"Users can verify that data has been imported by executing a query in the Nebula Graph client (for example, Nebula Graph Studio). For example: GO FROM \"player100\" OVER follow; Users can also run the SHOW STATS command to view statistics.","title":"Step 4: (optional) Validate data"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-pulsar/#step_5_optional_rebuild_indexes_in_nebula_graph","text":"With the data imported, users can recreate and rebuild indexes in Nebula Graph. For details, see Index overview .","title":"Step 5: (optional) Rebuild indexes in Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-sst/","text":"Import data from SST files \u00b6 This topic provides an example of how to generate the data from the data source into an SST (Sorted String Table) file and save it on HDFS, and then import it into Nebula Graph. The sample data source is a CSV file. Precautions \u00b6 The SST file can be imported only in Linux. The default value of the property is not supported. Background information \u00b6 Exchange supports two data import modes: Import the data from the data source directly into Nebula Graph as nGQL statements. Generate the SST file from the data source, and use Console to import the SST file into Nebula Graph. The following describes the scenarios, implementation methods, prerequisites, and steps for generating an SST file and importing data. Scenarios \u00b6 Suitable for online services, because the generation almost does not affect services (just reads the Schema), and the import speed is fast. Caution Although the import speed is fast, write operations in the corresponding space are blocked during the import period (about 10 seconds). Therefore, you are advised to import data in off-peak hours. Suitable for scenarios with a large amount of data from data sources for its fast import speed. Implementation methods \u00b6 The underlying code in Nebula Graph uses RocksDB as the key-value storage engine. RocksDB is a storage engine based on the hard disk, providing a series of APIs for creating and importing SST files to help quickly import massive data. The SST file is an internal file containing an arbitrarily long set of ordered key-value pairs for efficient storage of large amounts of key-value data. The entire process of generating SST files is mainly done by Exchange Reader, sstProcessor, and sstWriter. The whole data processing steps are as follows: Reader reads data from the data source. sstProcessor generates the SST file from the Nebula Graph's Schema information and uploads it to the HDFS. For details about the format of the SST file, see Data Storage Format . sstWriter opens a file and inserts data. When generating SST files, keys must be written in sequence. After the SST file is generated, RocksDB imports the SST file into Nebula Graph using the IngestExternalFile() method. For example: IngestExternalFileOptions ifo; # Import two SST files Status s = db_->IngestExternalFile({\"/home/usr/file1.sst\", \"/home/usr/file2.sst\"}, ifo); if (!s.ok()) { printf(\"Error while adding file %s and %s, Error %s\\n\", file_path1.c_str(), file_path2.c_str(), s.ToString().c_str()); return 1; } When the IngestExternalFile() method is called, RocksDB copies the file to the data directory by default and blocks the RocksDB write operation. If the key range in the SST file overwrites the Memtable key range, flush the Memtable to the hard disk. After placing the SST file in an optimal location in the LSM tree, assign a global serial number to the file and turn on the write operation. Data set \u00b6 This topic takes the basketballplayer dataset as an example. Environment \u00b6 This example is done on MacOS. Here is the environment configuration information: Hardware specifications: CPU: 1.7 GHz Quad-Core Intel Core i7 Memory: 16 GB Spark: 2.4.7, stand-alone Hadoop: 2.9.2, pseudo-distributed deployment Nebula Graph: 3.1.0. Prerequisites \u00b6 Before importing data, you need to confirm the following information: Nebula Graph has been installed and deployed with the following information: IP addresses and ports of Graph and Meta services. The user name and password with write permission to Nebula Graph. --ws_storage_http_port in the Meta service configuration file is the same as --ws_http_port in the Storage service configuration file. For example, 19779 . --ws_meta_http_port in the Graph service configuration file is the same as --ws_http_port in the Meta service configuration file. For example, 19559 . The information about the Schema, including names and properties of Tags and Edge types, and more. Exchange has been compiled , or download the compiled .jar file directly. Spark has been installed. JDK 1.8 or the later version has been installed and the environment variable JAVA_HOME has been configured. The Hadoop service has been installed and started. Note To generate SST files of other data sources, see documents of the corresponding data source and check the prerequisites. To generate SST files only, users do not need to install the Hadoop service on the machine where the Storage service is deployed. To delete the SST file after the ingest (data import) operation, add the configuration -- move_Files =true to the Storage Service configuration file. Steps \u00b6 Step 1: Create the Schema in Nebula Graph \u00b6 Analyze the data to create a Schema in Nebula Graph by following these steps: Identify the Schema elements. The Schema elements in the Nebula Graph are shown in the following table. Element Name Property Tag player name string, age int Tag team name string Edge Type follow degree int Edge Type serve start_year int, end_year int Create a graph space basketballplayer in the Nebula Graph and create a Schema as shown below. ## Create a graph space nebula> CREATE SPACE basketballplayer \\ (partition_num = 10, \\ replica_factor = 1, \\ vid_type = FIXED_STRING(30)); ## Use the graph space basketballplayer nebula> USE basketballplayer; ## Create the Tag player nebula> CREATE TAG player(name string, age int); ## Create the Tag team nebula> CREATE TAG team(name string); ## Create the Edge type follow nebula> CREATE EDGE follow(degree int); ## Create the Edge type serve nebula> CREATE EDGE serve(start_year int, end_year int); For more information, see Quick start workflow . Step 2: Process CSV files \u00b6 Confirm the following information: Process CSV files to meet Schema requirements. Note Exchange supports uploading CSV files with or without headers. Obtain the CSV file storage path. Step 3: Modify configuration files \u00b6 After Exchange is compiled, copy the conf file target/classes/application.conf to set SST data source configuration. In this example, the copied file is called sst_application.conf . For details on each configuration item, see Parameters in the configuration file . { # Spark configuration spark: { app: { name: Nebula Exchange 3.0.0 } master:local driver: { cores: 1 maxResultSize: 1G } executor: { memory:1G } cores:{ max: 16 } } # Nebula Graph configuration nebula: { address:{ graph:[\"127.0.0.1:9669\"] meta:[\"127.0.0.1:9559\"] } user: root pswd: nebula space: basketballplayer # SST file configuration path:{ # The local directory that temporarily stores generated SST files local:\"/tmp\" # The path for storing the SST file in the HDFS remote:\"/sst\" # The NameNode address of HDFS hdfs.namenode: \"hdfs://*.*.*.*:9000\" } # The connection parameters of clients connection: { # The timeout duration of socket connection and execution. Unit: milliseconds. timeout: 30000 } error: { # The maximum number of failures that will exit the application. max: 32 # Failed import jobs are logged in the output path. output: /tmp/errors } # Use Google's RateLimiter to limit requests to NebulaGraph. rate: { # Steady throughput of RateLimiter. limit: 1024 # Get the allowed timeout duration from RateLimiter. Unit: milliseconds. timeout: 1000 } } # Processing vertices tags: [ # Set the information about the Tag player. { # Specify the Tag name defined in Nebula Graph. name: player type: { # Specify the data source file format to CSV. source: csv # Specify how to import the data into Nebula Graph: Client or SST. sink: sst } # Specify the path to the CSV file. # If the file is stored in HDFS, use double quotation marks to enclose the file path, starting with hdfs://. For example, \"hdfs://ip:port/xx/xx.csv\". path: \"hdfs://*.*.*.*:9000/dataset/vertex_player.csv\" # If the CSV file does not have a header, use [_c0, _c1, _c2, ..., _cn] to represent its header and indicate the columns as the source of the property values. # If the CSV file has a header, use the actual column name. fields: [_c1, _c2] # Specify the property name defined in Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. nebula.fields: [age, name] # Specify a column of data in the table as the source of VIDs in Nebula Graph. # The value of vertex must be consistent with the column name in the above fields or csv.fields. # Currently, Nebula Graph 3.1.0 supports only strings or integers of VID. vertex: { field:_c0 } # The delimiter specified. The default value is comma. separator: \",\" # If the CSV file has a header, set the header to true. # If the CSV file does not have a header, set the header to false. The default value is false. header: false # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 # Whether to repartition data based on the number of partitions of graph spaces in Nebula Graph when generating the SST file. repartitionWithNebula: false } # Set the information about the Tag Team. { # Specify the Tag name defined in Nebula Graph. name: team type: { # Specify the data source file format to CSV. source: csv # Specify how to import the data into Nebula Graph: Client or SST. sink: sst } # Specify the path to the CSV file. # If the file is stored in HDFS, use double quotation marks to enclose the file path, starting with hdfs://. For example, \"hdfs://ip:port/xx/xx.csv\". path: \"hdfs://*.*.*.*:9000/dataset/vertex_team.csv\" # If the CSV file does not have a header, use [_c0, _c1, _c2, ..., _cn] to represent its header and indicate the columns as the source of the property values. # If the CSV file has a header, use the actual column name. fields: [_c1] # Specify the property name defined in Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. nebula.fields: [name] # Specify a column of data in the table as the source of VIDs in Nebula Graph. # The value of vertex must be consistent with the column name in the above fields or csv.fields. # Currently, Nebula Graph 3.1.0 supports only strings or integers of VID. vertex: { field:_c0 } # The delimiter specified. The default value is comma. separator: \",\" # If the CSV file has a header, set the header to true. # If the CSV file does not have a header, set the header to false. The default value is false. header: false # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 # Whether to repartition data based on the number of partitions of graph spaces in Nebula Graph when generating the SST file. repartitionWithNebula: false } # If more vertices need to be added, refer to the previous configuration to add them. ] # Processing edges edges: [ # Set the information about the Edge Type follow. { # The Edge Type name defined in Nebula Graph. name: follow type: { # Specify the data source file format to CSV. source: csv # Specify how to import the data into Nebula Graph: Client or SST. sink: sst } # Specify the path to the CSV file. # If the file is stored in HDFS, use double quotation marks to enclose the file path, starting with hdfs://. For example, \"hdfs://ip:port/xx/xx.csv\". path: \"hdfs://*.*.*.*:9000/dataset/edge_follow.csv\" # If the CSV file does not have a header, use [_c0, _c1, _c2, ..., _cn] to represent its header and indicate the columns as the source of the property values. # If the CSV file has a header, use the actual column name. fields: [_c2] # Specify the property name defined in Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. nebula.fields: [degree] # Specify a column as the source for the source and destination vertices. # The value of vertex must be consistent with the column name in the above fields or csv.fields. # Currently, Nebula Graph 3.1.0 supports only strings or integers of VID. source: { field: _c0 } target: { field: _c1 } # The delimiter specified. The default value is comma. separator: \",\" # (Optional) Specify a column as the source of the rank. #ranking: rank # If the CSV file has a header, set the header to true. # If the CSV file does not have a header, set the header to false. The default value is false. header: false # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 # Whether to repartition data based on the number of partitions of graph spaces in Nebula Graph when generating the SST file. repartitionWithNebula: false } # Set the information about the Edge Type serve. { # Specify the Edge type name defined in Nebula Graph. name: serve type: { # Specify the data source file format to CSV. source: csv # Specify how to import the data into Nebula Graph: Client or SST. sink: sst } # Specify the path to the CSV file. # If the file is stored in HDFS, use double quotation marks to enclose the file path, starting with hdfs://. For example, \"hdfs://ip:port/xx/xx.csv\". path: \"hdfs://*.*.*.*:9000/dataset/edge_serve.csv\" # If the CSV file does not have a header, use [_c0, _c1, _c2, ..., _cn] to represent its header and indicate the columns as the source of the property values. # If the CSV file has a header, use the actual column name. fields: [_c2,_c3] # Specify the property name defined in Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. nebula.fields: [start_year, end_year] # Specify a column as the source for the source and destination vertices. # The value of vertex must be consistent with the column name in the above fields or csv.fields. # Currently, Nebula Graph 3.1.0 supports only strings or integers of VID. source: { field: _c0 } target: { field: _c1 } # The delimiter specified. The default value is comma. separator: \",\" # (Optional) Specify a column as the source of the rank. #ranking: _c5 # If the CSV file has a header, set the header to true. # If the CSV file does not have a header, set the header to false. The default value is false. header: false # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 # Whether to repartition data based on the number of partitions of graph spaces in Nebula Graph when generating the SST file. repartitionWithNebula: false } ] # If more edges need to be added, refer to the previous configuration to add them. } Step 4: Generate the SST file \u00b6 Run the following command to generate the SST file from the CSV source file. For a description of the parameters, see Options for import . ${ SPARK_HOME } /bin/spark-submit --master \"local\" --conf spark.sql.shuffle.partition = <shuffle_concurrency> --class com.vesoft.nebula.exchange.Exchange <nebula-exchange-3.0.0.jar_path> -c <sst_application.conf_path> Note When generating SST files, the shuffle operation of Spark will be involved. Note that the configuration of spark.sql.shuffle.partition should be added when you submit the command. Note JAR packages are available in two ways: compiled them yourself , or download the compiled .jar file directly. For example: ${ SPARK_HOME } /bin/spark-submit --master \"local\" --conf spark.sql.shuffle.partition = 200 --class com.vesoft.nebula.exchange.Exchange /root/nebula-exchange/nebula-exchange/target/nebula-exchange-3.0.0.jar -c /root/nebula-exchange/nebula-exchange/target/classes/sst_application.conf After the task is complete, you can view the generated SST file in the /sst directory (specified by the nebula.path.remote parameter) on HDFS. Note If you modify the Schema, such as rebuilding the graph space, modifying the Tag, or modifying the Edge type, you need to regenerate the SST file because the SST file verifies the space ID, Tag ID, and Edge ID. Step 5: Import the SST file \u00b6 Note Confirm the following information before importing: Confirm that the Hadoop service has been deployed on all the machines where the Storage service is deployed, and configure HADOOP_HOME and JAVA_HOME . The --ws_storage_http_port in the Meta service configuration file (add it manually if it does not exist) is the same as the --ws_http_port in the Storage service configuration file. For example, both are 19779 . The --ws_meta_http_port in the Graph service configuration file (add it manually if it does not exist) is the same as the --ws_http_port in the Meta service configuration file. For example, both are 19559 . Connect to the Nebula Graph database using the client tool and import the SST file as follows: Run the following command to select the graph space you created earlier. nebula> USE basketballplayer; Run the following command to download the SST file: nebula> SUBMIT JOB DOWNLOAD HDFS \"hdfs://<hadoop_address>:<hadoop_port>/<sst_file_path>\"; For example: nebula> SUBMIT JOB DOWNLOAD HDFS \"hdfs://*.*.*.*:9000/sst\"; Run the following command to import the SST file: nebula> SUBMIT JOB INGEST; Note To download the SST file again, delete the download folder in the space ID in the data/storage/nebula directory in the Nebula Graph installation path, and then download the SST file again. If the space has multiple copies, the download folder needs to be deleted on all machines where the copies are saved. If there is a problem with the import and re-importing is required, re-execute SUBMIT JOB INGEST; . Step 6: (optional) Validate data \u00b6 Users can verify that data has been imported by executing a query in the Nebula Graph client (for example, Nebula Graph Studio). For example: GO FROM \"player100\" OVER follow; Users can also run the SHOW STATS command to view statistics. Step 7: (optional) Rebuild indexes in Nebula Graph \u00b6 With the data imported, users can recreate and rebuild indexes in Nebula Graph. For details, see Index overview .","title":"Import data from SST files"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-sst/#import_data_from_sst_files","text":"This topic provides an example of how to generate the data from the data source into an SST (Sorted String Table) file and save it on HDFS, and then import it into Nebula Graph. The sample data source is a CSV file.","title":"Import data from SST files"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-sst/#precautions","text":"The SST file can be imported only in Linux. The default value of the property is not supported.","title":"Precautions"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-sst/#background_information","text":"Exchange supports two data import modes: Import the data from the data source directly into Nebula Graph as nGQL statements. Generate the SST file from the data source, and use Console to import the SST file into Nebula Graph. The following describes the scenarios, implementation methods, prerequisites, and steps for generating an SST file and importing data.","title":"Background information"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-sst/#scenarios","text":"Suitable for online services, because the generation almost does not affect services (just reads the Schema), and the import speed is fast. Caution Although the import speed is fast, write operations in the corresponding space are blocked during the import period (about 10 seconds). Therefore, you are advised to import data in off-peak hours. Suitable for scenarios with a large amount of data from data sources for its fast import speed.","title":"Scenarios"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-sst/#implementation_methods","text":"The underlying code in Nebula Graph uses RocksDB as the key-value storage engine. RocksDB is a storage engine based on the hard disk, providing a series of APIs for creating and importing SST files to help quickly import massive data. The SST file is an internal file containing an arbitrarily long set of ordered key-value pairs for efficient storage of large amounts of key-value data. The entire process of generating SST files is mainly done by Exchange Reader, sstProcessor, and sstWriter. The whole data processing steps are as follows: Reader reads data from the data source. sstProcessor generates the SST file from the Nebula Graph's Schema information and uploads it to the HDFS. For details about the format of the SST file, see Data Storage Format . sstWriter opens a file and inserts data. When generating SST files, keys must be written in sequence. After the SST file is generated, RocksDB imports the SST file into Nebula Graph using the IngestExternalFile() method. For example: IngestExternalFileOptions ifo; # Import two SST files Status s = db_->IngestExternalFile({\"/home/usr/file1.sst\", \"/home/usr/file2.sst\"}, ifo); if (!s.ok()) { printf(\"Error while adding file %s and %s, Error %s\\n\", file_path1.c_str(), file_path2.c_str(), s.ToString().c_str()); return 1; } When the IngestExternalFile() method is called, RocksDB copies the file to the data directory by default and blocks the RocksDB write operation. If the key range in the SST file overwrites the Memtable key range, flush the Memtable to the hard disk. After placing the SST file in an optimal location in the LSM tree, assign a global serial number to the file and turn on the write operation.","title":"Implementation methods"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-sst/#data_set","text":"This topic takes the basketballplayer dataset as an example.","title":"Data set"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-sst/#environment","text":"This example is done on MacOS. Here is the environment configuration information: Hardware specifications: CPU: 1.7 GHz Quad-Core Intel Core i7 Memory: 16 GB Spark: 2.4.7, stand-alone Hadoop: 2.9.2, pseudo-distributed deployment Nebula Graph: 3.1.0.","title":"Environment"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-sst/#prerequisites","text":"Before importing data, you need to confirm the following information: Nebula Graph has been installed and deployed with the following information: IP addresses and ports of Graph and Meta services. The user name and password with write permission to Nebula Graph. --ws_storage_http_port in the Meta service configuration file is the same as --ws_http_port in the Storage service configuration file. For example, 19779 . --ws_meta_http_port in the Graph service configuration file is the same as --ws_http_port in the Meta service configuration file. For example, 19559 . The information about the Schema, including names and properties of Tags and Edge types, and more. Exchange has been compiled , or download the compiled .jar file directly. Spark has been installed. JDK 1.8 or the later version has been installed and the environment variable JAVA_HOME has been configured. The Hadoop service has been installed and started. Note To generate SST files of other data sources, see documents of the corresponding data source and check the prerequisites. To generate SST files only, users do not need to install the Hadoop service on the machine where the Storage service is deployed. To delete the SST file after the ingest (data import) operation, add the configuration -- move_Files =true to the Storage Service configuration file.","title":"Prerequisites"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-sst/#steps","text":"","title":"Steps"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-sst/#step_1_create_the_schema_in_nebula_graph","text":"Analyze the data to create a Schema in Nebula Graph by following these steps: Identify the Schema elements. The Schema elements in the Nebula Graph are shown in the following table. Element Name Property Tag player name string, age int Tag team name string Edge Type follow degree int Edge Type serve start_year int, end_year int Create a graph space basketballplayer in the Nebula Graph and create a Schema as shown below. ## Create a graph space nebula> CREATE SPACE basketballplayer \\ (partition_num = 10, \\ replica_factor = 1, \\ vid_type = FIXED_STRING(30)); ## Use the graph space basketballplayer nebula> USE basketballplayer; ## Create the Tag player nebula> CREATE TAG player(name string, age int); ## Create the Tag team nebula> CREATE TAG team(name string); ## Create the Edge type follow nebula> CREATE EDGE follow(degree int); ## Create the Edge type serve nebula> CREATE EDGE serve(start_year int, end_year int); For more information, see Quick start workflow .","title":"Step 1: Create the Schema in Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-sst/#step_2_process_csv_files","text":"Confirm the following information: Process CSV files to meet Schema requirements. Note Exchange supports uploading CSV files with or without headers. Obtain the CSV file storage path.","title":"Step 2: Process CSV files"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-sst/#step_3_modify_configuration_files","text":"After Exchange is compiled, copy the conf file target/classes/application.conf to set SST data source configuration. In this example, the copied file is called sst_application.conf . For details on each configuration item, see Parameters in the configuration file . { # Spark configuration spark: { app: { name: Nebula Exchange 3.0.0 } master:local driver: { cores: 1 maxResultSize: 1G } executor: { memory:1G } cores:{ max: 16 } } # Nebula Graph configuration nebula: { address:{ graph:[\"127.0.0.1:9669\"] meta:[\"127.0.0.1:9559\"] } user: root pswd: nebula space: basketballplayer # SST file configuration path:{ # The local directory that temporarily stores generated SST files local:\"/tmp\" # The path for storing the SST file in the HDFS remote:\"/sst\" # The NameNode address of HDFS hdfs.namenode: \"hdfs://*.*.*.*:9000\" } # The connection parameters of clients connection: { # The timeout duration of socket connection and execution. Unit: milliseconds. timeout: 30000 } error: { # The maximum number of failures that will exit the application. max: 32 # Failed import jobs are logged in the output path. output: /tmp/errors } # Use Google's RateLimiter to limit requests to NebulaGraph. rate: { # Steady throughput of RateLimiter. limit: 1024 # Get the allowed timeout duration from RateLimiter. Unit: milliseconds. timeout: 1000 } } # Processing vertices tags: [ # Set the information about the Tag player. { # Specify the Tag name defined in Nebula Graph. name: player type: { # Specify the data source file format to CSV. source: csv # Specify how to import the data into Nebula Graph: Client or SST. sink: sst } # Specify the path to the CSV file. # If the file is stored in HDFS, use double quotation marks to enclose the file path, starting with hdfs://. For example, \"hdfs://ip:port/xx/xx.csv\". path: \"hdfs://*.*.*.*:9000/dataset/vertex_player.csv\" # If the CSV file does not have a header, use [_c0, _c1, _c2, ..., _cn] to represent its header and indicate the columns as the source of the property values. # If the CSV file has a header, use the actual column name. fields: [_c1, _c2] # Specify the property name defined in Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. nebula.fields: [age, name] # Specify a column of data in the table as the source of VIDs in Nebula Graph. # The value of vertex must be consistent with the column name in the above fields or csv.fields. # Currently, Nebula Graph 3.1.0 supports only strings or integers of VID. vertex: { field:_c0 } # The delimiter specified. The default value is comma. separator: \",\" # If the CSV file has a header, set the header to true. # If the CSV file does not have a header, set the header to false. The default value is false. header: false # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 # Whether to repartition data based on the number of partitions of graph spaces in Nebula Graph when generating the SST file. repartitionWithNebula: false } # Set the information about the Tag Team. { # Specify the Tag name defined in Nebula Graph. name: team type: { # Specify the data source file format to CSV. source: csv # Specify how to import the data into Nebula Graph: Client or SST. sink: sst } # Specify the path to the CSV file. # If the file is stored in HDFS, use double quotation marks to enclose the file path, starting with hdfs://. For example, \"hdfs://ip:port/xx/xx.csv\". path: \"hdfs://*.*.*.*:9000/dataset/vertex_team.csv\" # If the CSV file does not have a header, use [_c0, _c1, _c2, ..., _cn] to represent its header and indicate the columns as the source of the property values. # If the CSV file has a header, use the actual column name. fields: [_c1] # Specify the property name defined in Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. nebula.fields: [name] # Specify a column of data in the table as the source of VIDs in Nebula Graph. # The value of vertex must be consistent with the column name in the above fields or csv.fields. # Currently, Nebula Graph 3.1.0 supports only strings or integers of VID. vertex: { field:_c0 } # The delimiter specified. The default value is comma. separator: \",\" # If the CSV file has a header, set the header to true. # If the CSV file does not have a header, set the header to false. The default value is false. header: false # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 # Whether to repartition data based on the number of partitions of graph spaces in Nebula Graph when generating the SST file. repartitionWithNebula: false } # If more vertices need to be added, refer to the previous configuration to add them. ] # Processing edges edges: [ # Set the information about the Edge Type follow. { # The Edge Type name defined in Nebula Graph. name: follow type: { # Specify the data source file format to CSV. source: csv # Specify how to import the data into Nebula Graph: Client or SST. sink: sst } # Specify the path to the CSV file. # If the file is stored in HDFS, use double quotation marks to enclose the file path, starting with hdfs://. For example, \"hdfs://ip:port/xx/xx.csv\". path: \"hdfs://*.*.*.*:9000/dataset/edge_follow.csv\" # If the CSV file does not have a header, use [_c0, _c1, _c2, ..., _cn] to represent its header and indicate the columns as the source of the property values. # If the CSV file has a header, use the actual column name. fields: [_c2] # Specify the property name defined in Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. nebula.fields: [degree] # Specify a column as the source for the source and destination vertices. # The value of vertex must be consistent with the column name in the above fields or csv.fields. # Currently, Nebula Graph 3.1.0 supports only strings or integers of VID. source: { field: _c0 } target: { field: _c1 } # The delimiter specified. The default value is comma. separator: \",\" # (Optional) Specify a column as the source of the rank. #ranking: rank # If the CSV file has a header, set the header to true. # If the CSV file does not have a header, set the header to false. The default value is false. header: false # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 # Whether to repartition data based on the number of partitions of graph spaces in Nebula Graph when generating the SST file. repartitionWithNebula: false } # Set the information about the Edge Type serve. { # Specify the Edge type name defined in Nebula Graph. name: serve type: { # Specify the data source file format to CSV. source: csv # Specify how to import the data into Nebula Graph: Client or SST. sink: sst } # Specify the path to the CSV file. # If the file is stored in HDFS, use double quotation marks to enclose the file path, starting with hdfs://. For example, \"hdfs://ip:port/xx/xx.csv\". path: \"hdfs://*.*.*.*:9000/dataset/edge_serve.csv\" # If the CSV file does not have a header, use [_c0, _c1, _c2, ..., _cn] to represent its header and indicate the columns as the source of the property values. # If the CSV file has a header, use the actual column name. fields: [_c2,_c3] # Specify the property name defined in Nebula Graph. # The sequence of fields and nebula.fields must correspond to each other. nebula.fields: [start_year, end_year] # Specify a column as the source for the source and destination vertices. # The value of vertex must be consistent with the column name in the above fields or csv.fields. # Currently, Nebula Graph 3.1.0 supports only strings or integers of VID. source: { field: _c0 } target: { field: _c1 } # The delimiter specified. The default value is comma. separator: \",\" # (Optional) Specify a column as the source of the rank. #ranking: _c5 # If the CSV file has a header, set the header to true. # If the CSV file does not have a header, set the header to false. The default value is false. header: false # The number of data written to Nebula Graph in a single batch. batch: 256 # The number of Spark partitions. partition: 32 # Whether to repartition data based on the number of partitions of graph spaces in Nebula Graph when generating the SST file. repartitionWithNebula: false } ] # If more edges need to be added, refer to the previous configuration to add them. }","title":"Step 3: Modify configuration files"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-sst/#step_4_generate_the_sst_file","text":"Run the following command to generate the SST file from the CSV source file. For a description of the parameters, see Options for import . ${ SPARK_HOME } /bin/spark-submit --master \"local\" --conf spark.sql.shuffle.partition = <shuffle_concurrency> --class com.vesoft.nebula.exchange.Exchange <nebula-exchange-3.0.0.jar_path> -c <sst_application.conf_path> Note When generating SST files, the shuffle operation of Spark will be involved. Note that the configuration of spark.sql.shuffle.partition should be added when you submit the command. Note JAR packages are available in two ways: compiled them yourself , or download the compiled .jar file directly. For example: ${ SPARK_HOME } /bin/spark-submit --master \"local\" --conf spark.sql.shuffle.partition = 200 --class com.vesoft.nebula.exchange.Exchange /root/nebula-exchange/nebula-exchange/target/nebula-exchange-3.0.0.jar -c /root/nebula-exchange/nebula-exchange/target/classes/sst_application.conf After the task is complete, you can view the generated SST file in the /sst directory (specified by the nebula.path.remote parameter) on HDFS. Note If you modify the Schema, such as rebuilding the graph space, modifying the Tag, or modifying the Edge type, you need to regenerate the SST file because the SST file verifies the space ID, Tag ID, and Edge ID.","title":"Step 4: Generate the SST file"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-sst/#step_5_import_the_sst_file","text":"Note Confirm the following information before importing: Confirm that the Hadoop service has been deployed on all the machines where the Storage service is deployed, and configure HADOOP_HOME and JAVA_HOME . The --ws_storage_http_port in the Meta service configuration file (add it manually if it does not exist) is the same as the --ws_http_port in the Storage service configuration file. For example, both are 19779 . The --ws_meta_http_port in the Graph service configuration file (add it manually if it does not exist) is the same as the --ws_http_port in the Meta service configuration file. For example, both are 19559 . Connect to the Nebula Graph database using the client tool and import the SST file as follows: Run the following command to select the graph space you created earlier. nebula> USE basketballplayer; Run the following command to download the SST file: nebula> SUBMIT JOB DOWNLOAD HDFS \"hdfs://<hadoop_address>:<hadoop_port>/<sst_file_path>\"; For example: nebula> SUBMIT JOB DOWNLOAD HDFS \"hdfs://*.*.*.*:9000/sst\"; Run the following command to import the SST file: nebula> SUBMIT JOB INGEST; Note To download the SST file again, delete the download folder in the space ID in the data/storage/nebula directory in the Nebula Graph installation path, and then download the SST file again. If the space has multiple copies, the download folder needs to be deleted on all machines where the copies are saved. If there is a problem with the import and re-importing is required, re-execute SUBMIT JOB INGEST; .","title":"Step 5: Import the SST file"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-sst/#step_6_optional_validate_data","text":"Users can verify that data has been imported by executing a query in the Nebula Graph client (for example, Nebula Graph Studio). For example: GO FROM \"player100\" OVER follow; Users can also run the SHOW STATS command to view statistics.","title":"Step 6: (optional) Validate data"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-sst/#step_7_optional_rebuild_indexes_in_nebula_graph","text":"With the data imported, users can recreate and rebuild indexes in Nebula Graph. For details, see Index overview .","title":"Step 7: (optional) Rebuild indexes in Nebula Graph"},{"location":"nebula-explorer/10.create-schema/","text":"Create a schema \u00b6 Before using Nebula Graph to query data, you must have a schema. Explorer allows you to create a schema both using GUI and using commands. At the upper right corner of the page, click to enter the schema creation page. The Explorer's schema feature is the same as Studio's. For more information, see Create a schema .","title":"Create a schema"},{"location":"nebula-explorer/10.create-schema/#create_a_schema","text":"Before using Nebula Graph to query data, you must have a schema. Explorer allows you to create a schema both using GUI and using commands. At the upper right corner of the page, click to enter the schema creation page. The Explorer's schema feature is the same as Studio's. For more information, see Create a schema .","title":"Create a schema"},{"location":"nebula-explorer/11.import-data/","text":"Import data \u00b6 Explorer allows you to import data into Nebula Graph using GUI. At the upper right corner of the page, click to enter the data import page. The Explorer's data import feature is the same as Studio's. For more information, see Import data .","title":"Import data"},{"location":"nebula-explorer/11.import-data/#import_data","text":"Explorer allows you to import data into Nebula Graph using GUI. At the upper right corner of the page, click to enter the data import page. The Explorer's data import feature is the same as Studio's. For more information, see Import data .","title":"Import data"},{"location":"nebula-explorer/12.query-visually/","text":"Visual Query \u00b6 The Visual Query feature uses a visual representation to express related requests. It allows you to create query scenarios to look up the desired data and view the corresponding statements. You can construct visual query statements by simply dragging and dropping, and then the system displays the query results on the query panel. Compatibility The Visual Query feature is not compatible with Nebula Graph versions below 3.0.0. Note Currently, the Visual Query feature is still in beta. Prerequisite \u00b6 You have choosen a graph space. For details, see Choose graph spaces . You have created indexes for particular queries. For details, see Note and CREATE INDEX . Steps \u00b6 At the top of the Explorer page, click Visual Query to enter the visual query page. Page elements \u00b6 On the left side of the Visual Query page, all the Tag(s) corresponding to the graph space (e.g.player and team) and the Tag named Any Tag are displayed. You can query vertices without tags by the Tag named Any Tag . On the page, the descriptions of other icons are as follows. Icon/Element Description The selected vertices and edges are the results to be queried. Double-click on the query pattern frame to limit the number of queries (with a priority higher than the value of the maximum number of returns in the global settings). Only querying edges is not supported. Zoom in on the query page. Zoom out on the query page. Save the current query graph. The saved graph is cached in the browser. View all stored query graphs. Up to 10 recently saved visual graphs are displayed. Click any of the stored graphs to display them on the visual query page. nGQL Click nGQL to view the statement corresponding to the query pattern. Run Query Click Run Query to display the query results visually on the canvas. Configurations \u00b6 Vertex configurations \u00b6 Tag Name : Set zero, one, or multiple tags. Note One vertex can have zero or multiple tags: When 0 tag is set, query the vertex without tags. When 1 tag is set, query the vertex with that tag. When multiple tags are set, query the vertex that has all the tags you set. Filter : Add one or more sets of filter conditions, including vertex properties, operators, and property values. Enterpriseonly When setting multiple tags in the Tag Name dialog box, only the Nebula Graph Enterprise Edition supports setting filter conditions to query data. Edge configurations \u00b6 Edge Type : Set one or multiple edge types. Note One edge have one and only one edge type: When one edge type is set, query the edge with that edge type. When multiple edge types are set, query the edge that has any of the edge types you set. Direction : Set the edge direction between two vertices, including Outgoing, Incoming, and Bidirect. Single : Set a fixed-length path. Range : Set a variable-length. Filter : Add one or more sets of filter conditions, including edge properties, operators, and property values. Enterpriseonly When setting multiple edge types in the Edge Type dialog box, only the Nebula Graph Enterprise Edition supports setting filter conditions to query data. Examples \u00b6 Example 1 \u00b6 Find out players who follow each other with Yao Ming and older than 35, and which teams these players are loyal to, and limit the number of the query patterns of the players and teams to 6. Create a query pattern by dragging and dropping Tags to the panel (2 players and 1 team). Configure filter conditions. Set the filter condition for the first player to player.name == Yao Ming . Set the edge type of the edge between the first and second players to follow , set the direction to Bidirect , and the steps to 1 . Set the filter condition for the second player to player.age > 35 . Set the edge type of the edge between the second player and the team to serve , the direction to Outgoing , and the steps to 1 . Click to select the second player, the team, and the serve edge between them. Click the Query Pattern frame, and set the Limit Number to 6 . Click Run Query , and the system displays 6 query patterns on the canvas. Example 2 \u00b6 Find out what teams two mutually-following players are loyal to and query for all players on that team who are older than 30. Create a query pattern by dragging and dropping Tags to the panel (3 players and 1 team). Configure filter conditions. Set the edge type of the edge between the first and second players to follow , set the direction to Bidirect , and the steps to 1 . Set the edge type of the edge between the first player and the team to serve , the direction to Outgoing , and the steps to 1 . Set the edge type of the edge between the second player and the team to serve , the direction to Outgoing , and the steps to 1 . Set the filter conditions for the third player to player.age > 30 . Set the edge type of the edge between the third player and the team to serve , the direction to Outgoing , and the steps to 1 . Click to select the third player, the team, and the serve edge between them. Click Run Query .","title":"Visual Query"},{"location":"nebula-explorer/12.query-visually/#visual_query","text":"The Visual Query feature uses a visual representation to express related requests. It allows you to create query scenarios to look up the desired data and view the corresponding statements. You can construct visual query statements by simply dragging and dropping, and then the system displays the query results on the query panel. Compatibility The Visual Query feature is not compatible with Nebula Graph versions below 3.0.0. Note Currently, the Visual Query feature is still in beta.","title":"Visual Query"},{"location":"nebula-explorer/12.query-visually/#prerequisite","text":"You have choosen a graph space. For details, see Choose graph spaces . You have created indexes for particular queries. For details, see Note and CREATE INDEX .","title":"Prerequisite"},{"location":"nebula-explorer/12.query-visually/#steps","text":"At the top of the Explorer page, click Visual Query to enter the visual query page.","title":"Steps"},{"location":"nebula-explorer/12.query-visually/#page_elements","text":"On the left side of the Visual Query page, all the Tag(s) corresponding to the graph space (e.g.player and team) and the Tag named Any Tag are displayed. You can query vertices without tags by the Tag named Any Tag . On the page, the descriptions of other icons are as follows. Icon/Element Description The selected vertices and edges are the results to be queried. Double-click on the query pattern frame to limit the number of queries (with a priority higher than the value of the maximum number of returns in the global settings). Only querying edges is not supported. Zoom in on the query page. Zoom out on the query page. Save the current query graph. The saved graph is cached in the browser. View all stored query graphs. Up to 10 recently saved visual graphs are displayed. Click any of the stored graphs to display them on the visual query page. nGQL Click nGQL to view the statement corresponding to the query pattern. Run Query Click Run Query to display the query results visually on the canvas.","title":"Page elements"},{"location":"nebula-explorer/12.query-visually/#configurations","text":"","title":"Configurations"},{"location":"nebula-explorer/12.query-visually/#vertex_configurations","text":"Tag Name : Set zero, one, or multiple tags. Note One vertex can have zero or multiple tags: When 0 tag is set, query the vertex without tags. When 1 tag is set, query the vertex with that tag. When multiple tags are set, query the vertex that has all the tags you set. Filter : Add one or more sets of filter conditions, including vertex properties, operators, and property values. Enterpriseonly When setting multiple tags in the Tag Name dialog box, only the Nebula Graph Enterprise Edition supports setting filter conditions to query data.","title":"Vertex configurations"},{"location":"nebula-explorer/12.query-visually/#edge_configurations","text":"Edge Type : Set one or multiple edge types. Note One edge have one and only one edge type: When one edge type is set, query the edge with that edge type. When multiple edge types are set, query the edge that has any of the edge types you set. Direction : Set the edge direction between two vertices, including Outgoing, Incoming, and Bidirect. Single : Set a fixed-length path. Range : Set a variable-length. Filter : Add one or more sets of filter conditions, including edge properties, operators, and property values. Enterpriseonly When setting multiple edge types in the Edge Type dialog box, only the Nebula Graph Enterprise Edition supports setting filter conditions to query data.","title":"Edge configurations"},{"location":"nebula-explorer/12.query-visually/#examples","text":"","title":"Examples"},{"location":"nebula-explorer/12.query-visually/#example_1","text":"Find out players who follow each other with Yao Ming and older than 35, and which teams these players are loyal to, and limit the number of the query patterns of the players and teams to 6. Create a query pattern by dragging and dropping Tags to the panel (2 players and 1 team). Configure filter conditions. Set the filter condition for the first player to player.name == Yao Ming . Set the edge type of the edge between the first and second players to follow , set the direction to Bidirect , and the steps to 1 . Set the filter condition for the second player to player.age > 35 . Set the edge type of the edge between the second player and the team to serve , the direction to Outgoing , and the steps to 1 . Click to select the second player, the team, and the serve edge between them. Click the Query Pattern frame, and set the Limit Number to 6 . Click Run Query , and the system displays 6 query patterns on the canvas.","title":"Example 1"},{"location":"nebula-explorer/12.query-visually/#example_2","text":"Find out what teams two mutually-following players are loyal to and query for all players on that team who are older than 30. Create a query pattern by dragging and dropping Tags to the panel (3 players and 1 team). Configure filter conditions. Set the edge type of the edge between the first and second players to follow , set the direction to Bidirect , and the steps to 1 . Set the edge type of the edge between the first player and the team to serve , the direction to Outgoing , and the steps to 1 . Set the edge type of the edge between the second player and the team to serve , the direction to Outgoing , and the steps to 1 . Set the filter conditions for the third player to player.age > 30 . Set the edge type of the edge between the third player and the team to serve , the direction to Outgoing , and the steps to 1 . Click to select the third player, the team, and the serve edge between them. Click Run Query .","title":"Example 2"},{"location":"nebula-explorer/13.choose-graphspace/","text":"Choose graph spaces \u00b6 You must first choose a graph space and then query and analyze data with Explorer. This topic introduces how to choose a graph space. Prerequisite \u00b6 You have connected to Explorer. For details, see Connect to Explorer . Steps \u00b6 After connecting to Explorer, the system automatically displays the graph space selection page. You only need to select the target graph space. If you do not choose a graph space in time after connecting to Explorer, follow the below steps to choose one. In the navigation bar on the left side of the Explorer page, click the graph space icon . Choose the target graph space. Note You can select the same or different graph spaces multiple times, and each selection creates a canvas for the corresponding graph space.","title":"Choose graph spaces"},{"location":"nebula-explorer/13.choose-graphspace/#choose_graph_spaces","text":"You must first choose a graph space and then query and analyze data with Explorer. This topic introduces how to choose a graph space.","title":"Choose graph spaces"},{"location":"nebula-explorer/13.choose-graphspace/#prerequisite","text":"You have connected to Explorer. For details, see Connect to Explorer .","title":"Prerequisite"},{"location":"nebula-explorer/13.choose-graphspace/#steps","text":"After connecting to Explorer, the system automatically displays the graph space selection page. You only need to select the target graph space. If you do not choose a graph space in time after connecting to Explorer, follow the below steps to choose one. In the navigation bar on the left side of the Explorer page, click the graph space icon . Choose the target graph space. Note You can select the same or different graph spaces multiple times, and each selection creates a canvas for the corresponding graph space.","title":"Steps"},{"location":"nebula-explorer/ex-ug-graph-exploration/","text":"Graph exploration \u00b6 The graph exploration can be performed from the following four aspects: Expand Common Neighbor Search for Path Inspect Property Prerequisite \u00b6 Make sure that there are vertices on the canvas. For more information, see Start querying . Expand \u00b6 In the navigation bar on the left side of the page, click to open the Expand panel. You can double-click on a vertex or right-click to select multiple vertices for expansion. On the panel, you can edit edge types, select the direction of edges, change the color of vertices, custom steps, and add filtering conditions. Parameter Description Edge type All edges in the graph space are displayed and selected by default. Direction Define the edge direction for the selected vertices, including Outgoing , Incoming , and Bidirect . Vertex Style Group by vertex tag : The target vertices are displayed in the same color as the corresponding tag. Custom Style : You can customize the color of the target vertices. Steps Single : Customize the number of steps from the selected vertex to the target vertex. Range : Customize the step range from the selected vertex to the target vertex. Filter Query target vertices by filtering conditions. Note The system saves the current configurations on the panel. When you double-click or right-click on a vertex for exploration, the exploration will be performed based on the saved configurations. Common Neighbor \u00b6 In the navigation bar on the left side of the page, click to open the Common Neighbor panel. You can select two or more vertices on the canvas and query their common neighbors. When the selected vertices have no common neighbor, the default returns **There is no data . For information about selecting two or more vertices, see Basic operations . Search for Path \u00b6 In the navigation bar on the left side of the page, click to open the Search Path panel. You can select two vertices on the canvas. The first selected vertex is the source and the second is the destination vertex by default . You can also customize the type and direction of edges, specify the number of exploration steps, and select the query path type. Parameter Description Edge Type All edges in the graph space are displayed and selected by default. Direction Define the edge direction for the selected vertices, including Outgoing , Incoming , and Bidirect . Query Type All path : Request for vertices and edges in all paths from the source vertex to the destination vertex. Shortest Path : Request for vertices and edges in the shortest path from the source vertex to the destination vertex. NoLoop Path : Request for vertices and edges in non-loop paths from the source vertex to the destination vertex. Steps Customize the number of steps from the source vertex to the destination vertex. Filter Query target vertices by filtering conditions. Inspect Property \u00b6 In the navigation bar on the left side of the page, click to open the Inspect Property panel. Properties of vertices or edges can be hidden or displayed on the canvas. Note Vertex properties are displayed on the canvas only when the zoom ratio is greater than 90%, and properties are automatically hidden when the zoom ratio is less than 90%. Edge properties are displayed on the canvas only when the zoom ratio is greater than 100%, and properties are automatically hidden when the zoom ratio is less than 100%.","title":"Graph exploration"},{"location":"nebula-explorer/ex-ug-graph-exploration/#graph_exploration","text":"The graph exploration can be performed from the following four aspects: Expand Common Neighbor Search for Path Inspect Property","title":"Graph exploration"},{"location":"nebula-explorer/ex-ug-graph-exploration/#prerequisite","text":"Make sure that there are vertices on the canvas. For more information, see Start querying .","title":"Prerequisite"},{"location":"nebula-explorer/ex-ug-graph-exploration/#expand","text":"In the navigation bar on the left side of the page, click to open the Expand panel. You can double-click on a vertex or right-click to select multiple vertices for expansion. On the panel, you can edit edge types, select the direction of edges, change the color of vertices, custom steps, and add filtering conditions. Parameter Description Edge type All edges in the graph space are displayed and selected by default. Direction Define the edge direction for the selected vertices, including Outgoing , Incoming , and Bidirect . Vertex Style Group by vertex tag : The target vertices are displayed in the same color as the corresponding tag. Custom Style : You can customize the color of the target vertices. Steps Single : Customize the number of steps from the selected vertex to the target vertex. Range : Customize the step range from the selected vertex to the target vertex. Filter Query target vertices by filtering conditions. Note The system saves the current configurations on the panel. When you double-click or right-click on a vertex for exploration, the exploration will be performed based on the saved configurations.","title":"Expand"},{"location":"nebula-explorer/ex-ug-graph-exploration/#common_neighbor","text":"In the navigation bar on the left side of the page, click to open the Common Neighbor panel. You can select two or more vertices on the canvas and query their common neighbors. When the selected vertices have no common neighbor, the default returns **There is no data . For information about selecting two or more vertices, see Basic operations .","title":"Common Neighbor"},{"location":"nebula-explorer/ex-ug-graph-exploration/#search_for_path","text":"In the navigation bar on the left side of the page, click to open the Search Path panel. You can select two vertices on the canvas. The first selected vertex is the source and the second is the destination vertex by default . You can also customize the type and direction of edges, specify the number of exploration steps, and select the query path type. Parameter Description Edge Type All edges in the graph space are displayed and selected by default. Direction Define the edge direction for the selected vertices, including Outgoing , Incoming , and Bidirect . Query Type All path : Request for vertices and edges in all paths from the source vertex to the destination vertex. Shortest Path : Request for vertices and edges in the shortest path from the source vertex to the destination vertex. NoLoop Path : Request for vertices and edges in non-loop paths from the source vertex to the destination vertex. Steps Customize the number of steps from the source vertex to the destination vertex. Filter Query target vertices by filtering conditions.","title":"Search for Path"},{"location":"nebula-explorer/ex-ug-graph-exploration/#inspect_property","text":"In the navigation bar on the left side of the page, click to open the Inspect Property panel. Properties of vertices or edges can be hidden or displayed on the canvas. Note Vertex properties are displayed on the canvas only when the zoom ratio is greater than 90%, and properties are automatically hidden when the zoom ratio is less than 90%. Edge properties are displayed on the canvas only when the zoom ratio is greater than 100%, and properties are automatically hidden when the zoom ratio is less than 100%.","title":"Inspect Property"},{"location":"nebula-explorer/ex-ug-page-overview/","text":"Page overview \u00b6 This topic introduces the Nebula Explorer page to help you learn more about Nebula Explorer's functions. The Nebula Explorer page consists of three modules top navigation bar, left-side navigation bar, and canvas. Top navigation bar \u00b6 Icon/Element Description Explorer Visually explore and analyze data. For more information, see Start querying , Vertex Filter , and Graph exploration . Visual Query Visually construct scenarios for data queries. For more information, see Visual Query . Manage Nebula Graph database graph spaces. For more information, see Create a schema . Bulk import of data into Nebula Graph. For more information, see Import data . Query the Nebula Graph data with nGQL statements. For more information, see Console . Select the language of Nebula Explorer page. Chinese and English are supported. Guide and help you in using Nebula Graph. Show the Nebula Graph version and allow you to disconnect from Nebula Explorer. Left-side navigation bar \u00b6 Note After logging into Explorer, select a graph space and click on it to unlock query and exploration functions in the left-side navigation bar. For more information, see Choose graph spaces . Click the icons in the left-side navigation bar to import, analyze, and explore graph data. The descriptions of the icons are as follows: Icon Description Enter VIDs or tags to query data. For more information, see Ways to query data . Search for target vertexes displayed on the canvas. For more information, see Filter vertices . Perform explorations on the vertices on the canvas by setting edge directions, steps, and filtering conditions. For more information, see Graph exploration . Select at least two vertices on the canvas to search for their common neighbors. For more information, see Graph exploration . Find all paths, the shortest path, and the non-loop paths from the source to the destination vertex. For more information, see Graph exploration . Choose whether to display the properties of vertices or edges on the canvas. For more information, see Graph exploration . View historical snapshots. For more information, see Canvas snapshots . View all graph spaces. Click a graph space to create a canvas corresponding to it. For more information, see Choose graph spaces . View Explorer documents and Nebula Graph forum. View your account and shortcuts, edit languages, limit returned results, and clear connection. Canvas \u00b6 Note After logging into Explorer, select a graph space and click on it to enter the canvas page. For more information, see Choose graph spaces . Graph data can be displayed visually on a canvas. The canvas consists of the following parts: Tabs on the Top Visualization modes Data storage Search box Layouts Minimap Data overview For more information, see Canvas overview .","title":"Page overview"},{"location":"nebula-explorer/ex-ug-page-overview/#page_overview","text":"This topic introduces the Nebula Explorer page to help you learn more about Nebula Explorer's functions. The Nebula Explorer page consists of three modules top navigation bar, left-side navigation bar, and canvas.","title":"Page overview"},{"location":"nebula-explorer/ex-ug-page-overview/#top_navigation_bar","text":"Icon/Element Description Explorer Visually explore and analyze data. For more information, see Start querying , Vertex Filter , and Graph exploration . Visual Query Visually construct scenarios for data queries. For more information, see Visual Query . Manage Nebula Graph database graph spaces. For more information, see Create a schema . Bulk import of data into Nebula Graph. For more information, see Import data . Query the Nebula Graph data with nGQL statements. For more information, see Console . Select the language of Nebula Explorer page. Chinese and English are supported. Guide and help you in using Nebula Graph. Show the Nebula Graph version and allow you to disconnect from Nebula Explorer.","title":"Top navigation bar"},{"location":"nebula-explorer/ex-ug-page-overview/#left-side_navigation_bar","text":"Note After logging into Explorer, select a graph space and click on it to unlock query and exploration functions in the left-side navigation bar. For more information, see Choose graph spaces . Click the icons in the left-side navigation bar to import, analyze, and explore graph data. The descriptions of the icons are as follows: Icon Description Enter VIDs or tags to query data. For more information, see Ways to query data . Search for target vertexes displayed on the canvas. For more information, see Filter vertices . Perform explorations on the vertices on the canvas by setting edge directions, steps, and filtering conditions. For more information, see Graph exploration . Select at least two vertices on the canvas to search for their common neighbors. For more information, see Graph exploration . Find all paths, the shortest path, and the non-loop paths from the source to the destination vertex. For more information, see Graph exploration . Choose whether to display the properties of vertices or edges on the canvas. For more information, see Graph exploration . View historical snapshots. For more information, see Canvas snapshots . View all graph spaces. Click a graph space to create a canvas corresponding to it. For more information, see Choose graph spaces . View Explorer documents and Nebula Graph forum. View your account and shortcuts, edit languages, limit returned results, and clear connection.","title":"Left-side navigation bar"},{"location":"nebula-explorer/ex-ug-page-overview/#canvas","text":"Note After logging into Explorer, select a graph space and click on it to enter the canvas page. For more information, see Choose graph spaces . Graph data can be displayed visually on a canvas. The canvas consists of the following parts: Tabs on the Top Visualization modes Data storage Search box Layouts Minimap Data overview For more information, see Canvas overview .","title":"Canvas"},{"location":"nebula-explorer/ex-ug-query-exploration/","text":"Start querying \u00b6 Note Select and click a target graph space before querying data. For more information, see Choose graph spaces . Legacy version compatibility For versions of Nebula Graph below 3.0.0, you need to create an index before querying data. For more information, see Create an index . Click the Start icon to query target data on the Explorer page. The queried data will be displayed on the canvas. You have the following ways to query data: Query by VID Query by Tag Query Subgraph Query by VID \u00b6 You can enter VIDs to query the target vertices and then start data exploration and analysis based on the vertices. There are three ways to generate VIDs: Manual input, Random import, and File import. Note The VIDs entered or imported must exist in the graph space you have selected. Only one VID per row is supported in the querying area. The following GIF shows how to query data using the basketballplayer graph space and related data. Query by Tag \u00b6 Note Make sure that the corresponding tags and indexes exist in the graph space when querying by tag. For more information, Create tags and Create indexes . You can limit the number of results and filter the results. The following example queries 10 players whose age is greater than 30 years old and not equal to 40 years old. Query Subgraph \u00b6 When querying subgraphs, you must enter one or more VIDs. You can specify the number of steps, edge types, and the direction of inflow and outflow of the subgraph. The following is an example of VIDs Kings and Suns , step number 2 , and incoming edge types with a VID value of 101, the number of steps of 4, and edge types of server and like . Note When multiple VIDs are entered, the VIDs are separated by the Enter key.","title":"Start querying"},{"location":"nebula-explorer/ex-ug-query-exploration/#start_querying","text":"Note Select and click a target graph space before querying data. For more information, see Choose graph spaces . Legacy version compatibility For versions of Nebula Graph below 3.0.0, you need to create an index before querying data. For more information, see Create an index . Click the Start icon to query target data on the Explorer page. The queried data will be displayed on the canvas. You have the following ways to query data: Query by VID Query by Tag Query Subgraph","title":"Start querying"},{"location":"nebula-explorer/ex-ug-query-exploration/#query_by_vid","text":"You can enter VIDs to query the target vertices and then start data exploration and analysis based on the vertices. There are three ways to generate VIDs: Manual input, Random import, and File import. Note The VIDs entered or imported must exist in the graph space you have selected. Only one VID per row is supported in the querying area. The following GIF shows how to query data using the basketballplayer graph space and related data.","title":"Query by VID"},{"location":"nebula-explorer/ex-ug-query-exploration/#query_by_tag","text":"Note Make sure that the corresponding tags and indexes exist in the graph space when querying by tag. For more information, Create tags and Create indexes . You can limit the number of results and filter the results. The following example queries 10 players whose age is greater than 30 years old and not equal to 40 years old.","title":"Query by Tag"},{"location":"nebula-explorer/ex-ug-query-exploration/#query_subgraph","text":"When querying subgraphs, you must enter one or more VIDs. You can specify the number of steps, edge types, and the direction of inflow and outflow of the subgraph. The following is an example of VIDs Kings and Suns , step number 2 , and incoming edge types with a VID value of 101, the number of steps of 4, and edge types of server and like . Note When multiple VIDs are entered, the VIDs are separated by the Enter key.","title":"Query Subgraph"},{"location":"nebula-explorer/ex-ug-shortcuts/","text":"Basic operations and shortcuts \u00b6 This topic lists the basic operations and shortcuts supported in Explorer. Basic operations \u00b6 Operation Description Move a canvas Hold down left click and drag the canvas. Zoom in or out the canvas Mouse wheel scrolls down to zoom in and up to zoom out. Select one single vertex or edge Left-click a vertex or an edge. Select multiple vertices and edges Hold Shift and left-click vertices and edges. Batch selection Hold down right click and frame vertices and edges; Or Hold Shift and hold down left click, and then frame vertices and edges. Move selected vertices Left-click the selected vertices and then move them. Shortcuts \u00b6 Operation Description Enter Expand Shift + '-' Zoom out Shift + '+' Zoom in Shift + 'l' Display Ctrl/Cmd + 'z' Undo Ctrl/Cmd + Shift + 'z' Redo Ctrl/Cmd + 'a' Select all vertices. Selected + 'del' Hide the selected elements. Selected + Shift + 'del' Hide the unselected elements.","title":"Basic operations and shortcuts"},{"location":"nebula-explorer/ex-ug-shortcuts/#basic_operations_and_shortcuts","text":"This topic lists the basic operations and shortcuts supported in Explorer.","title":"Basic operations and shortcuts"},{"location":"nebula-explorer/ex-ug-shortcuts/#basic_operations","text":"Operation Description Move a canvas Hold down left click and drag the canvas. Zoom in or out the canvas Mouse wheel scrolls down to zoom in and up to zoom out. Select one single vertex or edge Left-click a vertex or an edge. Select multiple vertices and edges Hold Shift and left-click vertices and edges. Batch selection Hold down right click and frame vertices and edges; Or Hold Shift and hold down left click, and then frame vertices and edges. Move selected vertices Left-click the selected vertices and then move them.","title":"Basic operations"},{"location":"nebula-explorer/ex-ug-shortcuts/#shortcuts","text":"Operation Description Enter Expand Shift + '-' Zoom out Shift + '+' Zoom in Shift + 'l' Display Ctrl/Cmd + 'z' Undo Ctrl/Cmd + Shift + 'z' Redo Ctrl/Cmd + 'a' Select all vertices. Selected + 'del' Hide the selected elements. Selected + Shift + 'del' Hide the unselected elements.","title":"Shortcuts"},{"location":"nebula-explorer/explorer-console/","text":"Explorer console \u00b6 Explorer console allows you to enter nGQL statements and visualize the query results. At the upper right corner of the page, click to enter the console page. The Explorer's console feature is the same as Studio's. For more information, see Console .","title":"Explorer console"},{"location":"nebula-explorer/explorer-console/#explorer_console","text":"Explorer console allows you to enter nGQL statements and visualize the query results. At the upper right corner of the page, click to enter the console page. The Explorer's console feature is the same as Studio's. For more information, see Console .","title":"Explorer console"},{"location":"nebula-explorer/node-filtering/","text":"Vertex Filter \u00b6 The Vertex Filter helps you filter the vertices and edges displayed on the canvas. You can filter data by tag only or by one or more sets of filter conditions. Prerequisite \u00b6 Make sure that there are vertices on the canvas. For more information, see Start query . Notes \u00b6 When filtering vertices and associated edges by Tag : All the tags in the graph space are displayed on the Filters panel. The selected tag turns gray, and the vertices and associated edges of the corresponding tag are hidden. For multi-tag vertices, if any of its tags is selected, the vertices are hidden. You can enter a tag name in the search box to search for tags. When filtering vertices and associated edges by filter conditions . Each set of filter conditions is only for the data with the target tag. The filtering conditions include Tag, Property, Operator, and Value. If the conditions are met, and the corresponding vertices will be automatically selected. If the conditions are not met, the corresponding vertices can be set to be hidden or turning gray . The vertices with other tags are not affected. If the filter conditions include a selected tag (in gray), the corresponding data will not be displayed on the canvas. Each time you perform Vertex Filter , only one tag can be selected. If you want to filter data based on more tags, conduct Add New Filter multiple times. Example \u00b6 Example 1 Filter vertices on the canvas with the tag player \u00b6 In the left navigation bar, click Vertex Filter . On the Filters panel, click player . Only vertices with the tag team are displayed on the canvas. The orange vertices filtered out in the above figure are the vertices with the tag team. Example 2 Filter players older than 33 years old \u00b6 In the left navigation bar, click Vertex Filter . Click Add New Filter , and set filter conditions (The values in the example are player , age , > , and 33 ). (Optional) Repeat the second step to add more sets of filtering conditions (This example adds only one set of filter conditions). Click Hide to hide the vertices or click Grayscale to gray the vertices that do not meet the filter conditions ( Grayscale is set in this example). Click Add New Filter and then fill in the following values as shown below. (Optional) Repeat the second step to add multiple filtering conditions. Turn on the Apply Filter button.","title":"Vertex Filter"},{"location":"nebula-explorer/node-filtering/#vertex_filter","text":"The Vertex Filter helps you filter the vertices and edges displayed on the canvas. You can filter data by tag only or by one or more sets of filter conditions.","title":"Vertex Filter"},{"location":"nebula-explorer/node-filtering/#prerequisite","text":"Make sure that there are vertices on the canvas. For more information, see Start query .","title":"Prerequisite"},{"location":"nebula-explorer/node-filtering/#notes","text":"When filtering vertices and associated edges by Tag : All the tags in the graph space are displayed on the Filters panel. The selected tag turns gray, and the vertices and associated edges of the corresponding tag are hidden. For multi-tag vertices, if any of its tags is selected, the vertices are hidden. You can enter a tag name in the search box to search for tags. When filtering vertices and associated edges by filter conditions . Each set of filter conditions is only for the data with the target tag. The filtering conditions include Tag, Property, Operator, and Value. If the conditions are met, and the corresponding vertices will be automatically selected. If the conditions are not met, the corresponding vertices can be set to be hidden or turning gray . The vertices with other tags are not affected. If the filter conditions include a selected tag (in gray), the corresponding data will not be displayed on the canvas. Each time you perform Vertex Filter , only one tag can be selected. If you want to filter data based on more tags, conduct Add New Filter multiple times.","title":"Notes"},{"location":"nebula-explorer/node-filtering/#example","text":"","title":"Example"},{"location":"nebula-explorer/node-filtering/#example_1_filter_vertices_on_the_canvas_with_the_tag_player","text":"In the left navigation bar, click Vertex Filter . On the Filters panel, click player . Only vertices with the tag team are displayed on the canvas. The orange vertices filtered out in the above figure are the vertices with the tag team.","title":"Example 1 Filter vertices on the canvas with the tag player"},{"location":"nebula-explorer/node-filtering/#example_2_filter_players_older_than_33_years_old","text":"In the left navigation bar, click Vertex Filter . Click Add New Filter , and set filter conditions (The values in the example are player , age , > , and 33 ). (Optional) Repeat the second step to add more sets of filtering conditions (This example adds only one set of filter conditions). Click Hide to hide the vertices or click Grayscale to gray the vertices that do not meet the filter conditions ( Grayscale is set in this example). Click Add New Filter and then fill in the following values as shown below. (Optional) Repeat the second step to add multiple filtering conditions. Turn on the Apply Filter button.","title":"Example 2 Filter players older than 33 years old"},{"location":"nebula-explorer/about-explorer/ex-ug-what-is-explorer/","text":"What is Nebula Explorer \u00b6 Nebula Explorer (Explorer in short) is a browser-based visualization tool. It is used with the Nebula Graph core to visualize interaction with graph data. Even if there is no experience in graph database, you can quickly become a graph exploration expert. Enterpriseonly Explorer is only available in the enterprise version. Note You can also try some functions online in Explorer . Scenarios \u00b6 You can use Explorer in one of these scenarios: You need to quickly find neighbor relationships from complex relationships, analyze suspicious targets, and display graph data in a visual manner. For large-scale data sets, the data needs to be filtered, analyzed, and explored in a visual manner. Features \u00b6 Explorer has these features: Easy to use : Explorer can be deployed in simple steps. And User-friendly : Explorer uses simple visual interaction, no need to conceive nGQL sentences, easy to realize graph exploration. Flexible : Explorer supports querying data through VID, Tag, and Subgraph. Exploration operations : Explorer supports exploration operations on multiple vertices, querying the common neighbors of multiple vertices, and querying the path between the source vertex and the destination vertex. Various display : Explorer supports modifying the color and icon of the vertex in the canvas to highlight key nodes. Data can also be displayed in different modes. Data storage : Data on a canvas can be stored and exported. Authentication \u00b6 Authentication is not enabled in Nebula Graph by default. Users can log into Studio with the root account and any password. When Nebula Graph enables authentication, users can only sign into Studio with the specified account. For more information, see Authentication .","title":"What is Nebula Explorer"},{"location":"nebula-explorer/about-explorer/ex-ug-what-is-explorer/#what_is_nebula_explorer","text":"Nebula Explorer (Explorer in short) is a browser-based visualization tool. It is used with the Nebula Graph core to visualize interaction with graph data. Even if there is no experience in graph database, you can quickly become a graph exploration expert. Enterpriseonly Explorer is only available in the enterprise version. Note You can also try some functions online in Explorer .","title":"What is Nebula Explorer"},{"location":"nebula-explorer/about-explorer/ex-ug-what-is-explorer/#scenarios","text":"You can use Explorer in one of these scenarios: You need to quickly find neighbor relationships from complex relationships, analyze suspicious targets, and display graph data in a visual manner. For large-scale data sets, the data needs to be filtered, analyzed, and explored in a visual manner.","title":"Scenarios"},{"location":"nebula-explorer/about-explorer/ex-ug-what-is-explorer/#features","text":"Explorer has these features: Easy to use : Explorer can be deployed in simple steps. And User-friendly : Explorer uses simple visual interaction, no need to conceive nGQL sentences, easy to realize graph exploration. Flexible : Explorer supports querying data through VID, Tag, and Subgraph. Exploration operations : Explorer supports exploration operations on multiple vertices, querying the common neighbors of multiple vertices, and querying the path between the source vertex and the destination vertex. Various display : Explorer supports modifying the color and icon of the vertex in the canvas to highlight key nodes. Data can also be displayed in different modes. Data storage : Data on a canvas can be stored and exported.","title":"Features"},{"location":"nebula-explorer/about-explorer/ex-ug-what-is-explorer/#authentication","text":"Authentication is not enabled in Nebula Graph by default. Users can log into Studio with the root account and any password. When Nebula Graph enables authentication, users can only sign into Studio with the specified account. For more information, see Authentication .","title":"Authentication"},{"location":"nebula-explorer/canvas-operations/canvas-overview/","text":"Canvas overview \u00b6 You can visually explore data on a canvas. This topic introduces the composition of a canvas and its related functions. Canvas overview diagram: Tabs on the Top \u00b6 Click the plus sign to add a new canvas. You can have operations on multiple canvases simultaneously. Canvas data on different canvases can come from the same graph space or from different graph spaces. You can customize the name of a canvas except for the canvas in the left-most tab. Visualization modes \u00b6 Graph data can be visually explored in 2D mode and 3D mode . For more information, Visualization modes . Data storage \u00b6 Graph data on the current canvas can be stored by creating snapshots or exporting canvas data as images or CSV files. At the top right of the page, you can: Click to create a snapshot. For more information, see Canvas snapshots . Click and then click Export CSV File to store canvas data as CSV files. Click and then click Export PNG File to store canvas data as images. Search box \u00b6 In the search box at the top left of the page, click and enter a VID or the property values of tags to locate target vertices. Layouts \u00b6 Explorer provides 6 layouts to show the relationship between the data on a canvas. Force Dagre Circular Grid Neural Network Radial Minimap \u00b6 You can display the vertices on a canvas on full screen. You can also collapse the minimap, zoom in or zoom out the canvass, etc. The percentage of a canvas graph to the total is displayed in the lower-left corner of the minimap. Data overview \u00b6 On the right side of the page, click to expand the data overview panel. On the data overview panel, you are enabled to: See the number of tags and edge types, and the number of the corresponding vertices and edges on a canvas. Click the tag color icon to customize the color, size, and icon of the vertices with the same tag. Note Vertices with the same tag have the same color. Right-click a single vertex on a canvas to manually modify the style of the vertex. Upload images to personalize the style of the vertices in the canvas, and the uploaded images are stored in the browser. To store uploaded images permanently, save the canvas data as a snapshot. For details, see Manage snapshots . Select vertices and edges on the canvas, and then click Selected Vertices {number} Selected Edges {number} in the lower left corner to view the detailed information of the vertices and edges. You can export the data as a CSV file.","title":"Overview"},{"location":"nebula-explorer/canvas-operations/canvas-overview/#canvas_overview","text":"You can visually explore data on a canvas. This topic introduces the composition of a canvas and its related functions. Canvas overview diagram:","title":"Canvas overview"},{"location":"nebula-explorer/canvas-operations/canvas-overview/#tabs_on_the_top","text":"Click the plus sign to add a new canvas. You can have operations on multiple canvases simultaneously. Canvas data on different canvases can come from the same graph space or from different graph spaces. You can customize the name of a canvas except for the canvas in the left-most tab.","title":"Tabs on the Top"},{"location":"nebula-explorer/canvas-operations/canvas-overview/#visualization_modes","text":"Graph data can be visually explored in 2D mode and 3D mode . For more information, Visualization modes .","title":"Visualization modes"},{"location":"nebula-explorer/canvas-operations/canvas-overview/#data_storage","text":"Graph data on the current canvas can be stored by creating snapshots or exporting canvas data as images or CSV files. At the top right of the page, you can: Click to create a snapshot. For more information, see Canvas snapshots . Click and then click Export CSV File to store canvas data as CSV files. Click and then click Export PNG File to store canvas data as images.","title":"Data storage"},{"location":"nebula-explorer/canvas-operations/canvas-overview/#search_box","text":"In the search box at the top left of the page, click and enter a VID or the property values of tags to locate target vertices.","title":"Search box"},{"location":"nebula-explorer/canvas-operations/canvas-overview/#layouts","text":"Explorer provides 6 layouts to show the relationship between the data on a canvas. Force Dagre Circular Grid Neural Network Radial","title":"Layouts"},{"location":"nebula-explorer/canvas-operations/canvas-overview/#minimap","text":"You can display the vertices on a canvas on full screen. You can also collapse the minimap, zoom in or zoom out the canvass, etc. The percentage of a canvas graph to the total is displayed in the lower-left corner of the minimap.","title":"Minimap"},{"location":"nebula-explorer/canvas-operations/canvas-overview/#data_overview","text":"On the right side of the page, click to expand the data overview panel. On the data overview panel, you are enabled to: See the number of tags and edge types, and the number of the corresponding vertices and edges on a canvas. Click the tag color icon to customize the color, size, and icon of the vertices with the same tag. Note Vertices with the same tag have the same color. Right-click a single vertex on a canvas to manually modify the style of the vertex. Upload images to personalize the style of the vertices in the canvas, and the uploaded images are stored in the browser. To store uploaded images permanently, save the canvas data as a snapshot. For details, see Manage snapshots . Select vertices and edges on the canvas, and then click Selected Vertices {number} Selected Edges {number} in the lower left corner to view the detailed information of the vertices and edges. You can export the data as a CSV file.","title":"Data overview"},{"location":"nebula-explorer/canvas-operations/canvas-snapshot/","text":"Manage snapshots \u00b6 Explorer provides a snapshot feature that lets you store the visualized canvas data so that the data can be restored when your browser is opened again. Create snapshots \u00b6 In the upper right corner of a canvas page, click the camera icon . Fill in the snapshot name and notes (optional). Click submit . Note Created snapshots are stored on the snapshot list page. For more information, see below. Historical snapshots \u00b6 In the left navigation bar of the Explorer page, click to enter the Snapshot page. You can switch graph spaces and view the historical snapshots of the corresponding graph space. You can also import snapshots to a canvas, download canvas snapshots to your local drive, and delete snapshots. Under the Operation column to the right of the target snapshot, you are enabled to: Click to import a historical snapshot to a new canvas. Click to download a snapshot in JSON format locally. Click to delete a snapshot. At the top left of the Snapshot page, click Import Snapshot to import previously downloaded files in JSON format to the Snapshot page for sharing the snapshot data offline. The system automatically places the imported snapshots in the corresponding graph space based on the graph space information recorded in the JSON file. Note Up to 50 snapshots can be stored in the snapshot list currently.","title":"Snapshots"},{"location":"nebula-explorer/canvas-operations/canvas-snapshot/#manage_snapshots","text":"Explorer provides a snapshot feature that lets you store the visualized canvas data so that the data can be restored when your browser is opened again.","title":"Manage snapshots"},{"location":"nebula-explorer/canvas-operations/canvas-snapshot/#create_snapshots","text":"In the upper right corner of a canvas page, click the camera icon . Fill in the snapshot name and notes (optional). Click submit . Note Created snapshots are stored on the snapshot list page. For more information, see below.","title":"Create snapshots"},{"location":"nebula-explorer/canvas-operations/canvas-snapshot/#historical_snapshots","text":"In the left navigation bar of the Explorer page, click to enter the Snapshot page. You can switch graph spaces and view the historical snapshots of the corresponding graph space. You can also import snapshots to a canvas, download canvas snapshots to your local drive, and delete snapshots. Under the Operation column to the right of the target snapshot, you are enabled to: Click to import a historical snapshot to a new canvas. Click to download a snapshot in JSON format locally. Click to delete a snapshot. At the top left of the Snapshot page, click Import Snapshot to import previously downloaded files in JSON format to the Snapshot page for sharing the snapshot data offline. The system automatically places the imported snapshots in the corresponding graph space based on the graph space information recorded in the JSON file. Note Up to 50 snapshots can be stored in the snapshot list currently.","title":"Historical snapshots"},{"location":"nebula-explorer/canvas-operations/visualization-mode/","text":"Visualization modes \u00b6 Explorer provides 2D and 3D visualization modes for you to explore data. 2D enables you to operate on graph data and view data information. 3D lets you explore graph data from a different perspective. The 3D is suitable for cases with a large amount of data or situations requiring presentations. Note In 3D mode, operations on graph data are unavailable. 2D mode \u00b6 Exploration of the data on a canvas is possible in 2D mode. Parameter Description Weight Degree Weight Degree\uff1aAutomatically resizes vertices according to the number of outgoing and incoming edges of all the vertices on the canvas. Reset Degree\uff1aResets the vertices on the canvas to their original size. Detection Outlier: Detects the vertices that connect no edges on a canvas. Dangling Edges: Detects edges associated with vertices of one degree in the canvas (associated vertices are included). Loop Detection: Detects the paths that connect a vertex to itself. Edit Dismiss: Hide the selected vertices and edges on the canvas. Dismiss Others: Hide the unselected vertices and edges on the canvas. Undo: Undo the action in the previous step. Redo: Restore the action that was previously undone. For more information about the operations available in 2D mode, see Canvas . 3D mode \u00b6 At the top left of the page, toggle the view button to switch to 3D mode. 3D mode allows you to switch back to 2D mode and does not influence operations in 2D. Parameter Description Bird View Shows a bird view of all the data in the current graph space (Displays data for up to 20,000 vertices and 2,000 edges in the current graph space). Image Quality High: Vertices are displayed in the form of balls with better light and shadow effects. Normal: Vertices are represented in a circle format and support a large amount of data. Reheat Disperses the distance between vertices when the vertices overlap. Legacy version compatibility For versions of Nebula Graph below 3.0.0, you need to create an index before using the Bird View feature. For more information, see Create an index .","title":"Visualization modes"},{"location":"nebula-explorer/canvas-operations/visualization-mode/#visualization_modes","text":"Explorer provides 2D and 3D visualization modes for you to explore data. 2D enables you to operate on graph data and view data information. 3D lets you explore graph data from a different perspective. The 3D is suitable for cases with a large amount of data or situations requiring presentations. Note In 3D mode, operations on graph data are unavailable.","title":"Visualization modes"},{"location":"nebula-explorer/canvas-operations/visualization-mode/#2d_mode","text":"Exploration of the data on a canvas is possible in 2D mode. Parameter Description Weight Degree Weight Degree\uff1aAutomatically resizes vertices according to the number of outgoing and incoming edges of all the vertices on the canvas. Reset Degree\uff1aResets the vertices on the canvas to their original size. Detection Outlier: Detects the vertices that connect no edges on a canvas. Dangling Edges: Detects edges associated with vertices of one degree in the canvas (associated vertices are included). Loop Detection: Detects the paths that connect a vertex to itself. Edit Dismiss: Hide the selected vertices and edges on the canvas. Dismiss Others: Hide the unselected vertices and edges on the canvas. Undo: Undo the action in the previous step. Redo: Restore the action that was previously undone. For more information about the operations available in 2D mode, see Canvas .","title":"2D mode"},{"location":"nebula-explorer/canvas-operations/visualization-mode/#3d_mode","text":"At the top left of the page, toggle the view button to switch to 3D mode. 3D mode allows you to switch back to 2D mode and does not influence operations in 2D. Parameter Description Bird View Shows a bird view of all the data in the current graph space (Displays data for up to 20,000 vertices and 2,000 edges in the current graph space). Image Quality High: Vertices are displayed in the form of balls with better light and shadow effects. Normal: Vertices are represented in a circle format and support a large amount of data. Reheat Disperses the distance between vertices when the vertices overlap. Legacy version compatibility For versions of Nebula Graph below 3.0.0, you need to create an index before using the Bird View feature. For more information, see Create an index .","title":"3D mode"},{"location":"nebula-explorer/deploy-connect/ex-ug-connect/","text":"Connect to Nebula Graph \u00b6 After successfully launching Explorer, you need to configure to connect to Nebula Graph. This topic describes how Explorer connects to the Nebula Graph database. Prerequisites \u00b6 Before connecting to the Nebula Graph database, you need to confirm the following information: The Nebula Graph services and Explorer are started. For more information, see Deploy Explorer . You have the local IP address and the port used by the Graph service of Nebula Graph. The default port is 9669 . You have a Nebula Graph account and its password. Note If authentication is enabled in Nebula Graph and different role-based accounts are created, you must use the assigned account to connect to Nebula Graph. If authentication is disabled, you can use the root and any password to connect to Nebula Graph. For more information, see Nebula Graph Database Manual . Procedure \u00b6 To connect Explorer to Nebula Graph, follow these steps: On the Config Server page of Explorer, configure these fields: Host : Enter the IP address and the port of the Graph service of Nebula Graph. The valid format is IP:port . The default port is 9669 . Note When Nebula Graph and Explorer are deployed on the same machine, you must enter the IP address of the machine, but not 127.0.0.1 or localhost , in the Host field. Username and Password : Fill in the log in account according to the authentication settings of Nebula Graph. If authentication is not enabled, you can use root and any password as the username and its password. If authentication is enabled and no account information has been created, you can only log in as GOD role and use root and nebula as the username and its password. If authentication is enabled and different users are created and assigned roles, users in different roles log in with their accounts and passwords. After the configuration, click the Login button. Note One session continues for up to 30 minutes. If you do not operate Explorer within 30 minutes, the active session will time out and you must connect to Nebula Graph again. Clear connection \u00b6 When Explorer is still connected to a Nebula Graph database, in the navigation bar on the left side of the page, select Settings > Clear Connect . After that, if the configuration database page is displayed on the browser, it means that Explorer has successfully disconnected from the Nebula Graph.","title":"Connect to Nebula Graph"},{"location":"nebula-explorer/deploy-connect/ex-ug-connect/#connect_to_nebula_graph","text":"After successfully launching Explorer, you need to configure to connect to Nebula Graph. This topic describes how Explorer connects to the Nebula Graph database.","title":"Connect to Nebula Graph"},{"location":"nebula-explorer/deploy-connect/ex-ug-connect/#prerequisites","text":"Before connecting to the Nebula Graph database, you need to confirm the following information: The Nebula Graph services and Explorer are started. For more information, see Deploy Explorer . You have the local IP address and the port used by the Graph service of Nebula Graph. The default port is 9669 . You have a Nebula Graph account and its password. Note If authentication is enabled in Nebula Graph and different role-based accounts are created, you must use the assigned account to connect to Nebula Graph. If authentication is disabled, you can use the root and any password to connect to Nebula Graph. For more information, see Nebula Graph Database Manual .","title":"Prerequisites"},{"location":"nebula-explorer/deploy-connect/ex-ug-connect/#procedure","text":"To connect Explorer to Nebula Graph, follow these steps: On the Config Server page of Explorer, configure these fields: Host : Enter the IP address and the port of the Graph service of Nebula Graph. The valid format is IP:port . The default port is 9669 . Note When Nebula Graph and Explorer are deployed on the same machine, you must enter the IP address of the machine, but not 127.0.0.1 or localhost , in the Host field. Username and Password : Fill in the log in account according to the authentication settings of Nebula Graph. If authentication is not enabled, you can use root and any password as the username and its password. If authentication is enabled and no account information has been created, you can only log in as GOD role and use root and nebula as the username and its password. If authentication is enabled and different users are created and assigned roles, users in different roles log in with their accounts and passwords. After the configuration, click the Login button. Note One session continues for up to 30 minutes. If you do not operate Explorer within 30 minutes, the active session will time out and you must connect to Nebula Graph again.","title":"Procedure"},{"location":"nebula-explorer/deploy-connect/ex-ug-connect/#clear_connection","text":"When Explorer is still connected to a Nebula Graph database, in the navigation bar on the left side of the page, select Settings > Clear Connect . After that, if the configuration database page is displayed on the browser, it means that Explorer has successfully disconnected from the Nebula Graph.","title":"Clear connection"},{"location":"nebula-explorer/deploy-connect/ex-ug-deploy/","text":"Deploy Explorer \u00b6 This topic describes how to deploy Explorer locally by RPM and tar packages. Nebula Graph version \u00b6 Note Explorer is released separately, not synchronized with Nebula Graph. And the version naming of Explorer is different from that of Nebula Graph. The version correspondence between Nebula Graph and Explorer is as follows. Nebula Graph version Explorer version 3.0.0 ~ 3.1.x 3.0.0 2.5.x ~ 3.0.0 2.2.0 2.6.x 2.1.0 2.5.x 2.0.0 Prerequisites \u00b6 Before deploying Explorer, you must check the following information: The Nebula Graph services are deployed and started. For more information, see Nebula Graph Database Manual . Before the installation starts, the following ports are not occupied. Port Description 7002 Web service provided by Explorer Caution By default, Explorer uses the port 7002 . You can modify the httpport in the conf/app.conf file in the installation directory and restart the service. The Linux distribution is CentOS. GO of version above 1.13 is installed. The license is ready. Enterpriseonly License is only available in the Enterprise Edition. To obtain the license, apply for Nebula Explorer Free Trial . RPM-based deployment \u00b6 Installation \u00b6 Select and download the RPM package according to your needs. It is recommended to select the latest version. Enterpriseonly You can apply online for Explorer free trial. To purchase, contact our sales team via email (inquiry@vesoft.com). For features of Explorer, see Pricing . Use sudo rpm -i <rpm> to install RPM package. For example, use the following command to install Explorer. The default installation path is /usr/local/nebula-explorer . sudo rpm -i nebula-explorer-<version>.x86_64.rpm You can also install it to the specified path using the following command: sudo rpm -i nebula-explorer-xxx.rpm --prefix = <path> Copy the license to the installation path. cp -r <license> <explorer_path> For example: cp -r nebula.license /usr/local/nebula-explorer After adding the license, you need to stop and restart the service using the following command. systemctl stop nebula-explorer #Stop the service systemctl start nebula-explorer #Start the service Start and stop \u00b6 You can use SystemCTL to start and stop the service. systemctl status nebula-explorer #Check the status systemctl stop nebula-explorer #Stop the service systemctl start nebula-explorer #Start the service You can also start or stop the service manually using the following command in the installation directory. cd ./scripts/rpm bash ./start.sh #Start the service bash ./stop.sh #Stop the service Uninstallation \u00b6 You can uninstall Explorer using the following command: sudo rpm -e nebula-graph-explorer-<version>.x86_64 DEB-based deployment \u00b6 Installation \u00b6 Select and download the RPM package according to your needs. It is recommended to select the latest version. Common links are as follows: Enterpriseonly You can apply online for Explorer free trial. To purchase, contact our sales team via email (inquiry@vesoft.com). For features of Explorer, see Pricing . Run sudo dpkg -i <package_name> to unpack the DEB package. For example, run the following command to install Explorer (The default installation path is /usr/local/nebula-explorer ). sudo dpkg -i nebula-explorer-3.0.0.x86_64.deb Note You cannot customize the installation path of Explorer when installing a DEB package. Copy the license to the Explorer installation path. Sudo cp -r <license> <explorer_path> For example: Sudo cp -r nebula.license /usr/local/nebula-explorer Run the following command to start the service. sudo systemctl start nebula-explorer.service You can also start the service manually using the following command in the nebula-explorer/lib directory. sudo bash ./start.sh View the status \u00b6 sudo systemctl status nebula-explorer.service Stop the service \u00b6 sudo systemctl stop nebula-explorer.service Uninstallation \u00b6 Run the following command to uninstall Explorer: sudo dpkg -r nebula-explorer TAR-based deployment \u00b6 Installation \u00b6 Select and download the TAR package according to your needs. It is recommended to select the latest version. Common links are as follows: Enterpriseonly Explorer is only available in the Enterprise Edition. Click Pricing to see more. Use tar -xvf to decompress the TAR package. tar -xvf nebula-graph-explorer-<version>.tar.gz Copy the license to the nebula-explorer directory. cp -r <license> <explorer_path> For example: cp -r nebula.license /usr/local/nebula-explorer Enter the nebula-explorer folder to start Explorer. cd nebula-explorer ./nebula-httpd & Stop Service \u00b6 You can use kill pid to stop the service. kill $( lsof -t -i :7002 ) Next to do \u00b6 When Explorer is started, use http://<ip_address>:7002 to get access to Explorer. The following login page shows that Explorer is successfully connected to Nebula Graph. Note When logging into Nebula Explorer for the first time, the content of END USER LICENSE AGREEMENT is displayed on the login page. Please read it and then click I agree . After entering the Explorer login interface, you need to connect to Nebula Graph. For more information, refer to Connecting to the Nebula Graph .","title":"Deploy Explorer"},{"location":"nebula-explorer/deploy-connect/ex-ug-deploy/#deploy_explorer","text":"This topic describes how to deploy Explorer locally by RPM and tar packages.","title":"Deploy Explorer"},{"location":"nebula-explorer/deploy-connect/ex-ug-deploy/#nebula_graph_version","text":"Note Explorer is released separately, not synchronized with Nebula Graph. And the version naming of Explorer is different from that of Nebula Graph. The version correspondence between Nebula Graph and Explorer is as follows. Nebula Graph version Explorer version 3.0.0 ~ 3.1.x 3.0.0 2.5.x ~ 3.0.0 2.2.0 2.6.x 2.1.0 2.5.x 2.0.0","title":"Nebula Graph version"},{"location":"nebula-explorer/deploy-connect/ex-ug-deploy/#prerequisites","text":"Before deploying Explorer, you must check the following information: The Nebula Graph services are deployed and started. For more information, see Nebula Graph Database Manual . Before the installation starts, the following ports are not occupied. Port Description 7002 Web service provided by Explorer Caution By default, Explorer uses the port 7002 . You can modify the httpport in the conf/app.conf file in the installation directory and restart the service. The Linux distribution is CentOS. GO of version above 1.13 is installed. The license is ready. Enterpriseonly License is only available in the Enterprise Edition. To obtain the license, apply for Nebula Explorer Free Trial .","title":"Prerequisites"},{"location":"nebula-explorer/deploy-connect/ex-ug-deploy/#rpm-based_deployment","text":"","title":"RPM-based deployment"},{"location":"nebula-explorer/deploy-connect/ex-ug-deploy/#installation","text":"Select and download the RPM package according to your needs. It is recommended to select the latest version. Enterpriseonly You can apply online for Explorer free trial. To purchase, contact our sales team via email (inquiry@vesoft.com). For features of Explorer, see Pricing . Use sudo rpm -i <rpm> to install RPM package. For example, use the following command to install Explorer. The default installation path is /usr/local/nebula-explorer . sudo rpm -i nebula-explorer-<version>.x86_64.rpm You can also install it to the specified path using the following command: sudo rpm -i nebula-explorer-xxx.rpm --prefix = <path> Copy the license to the installation path. cp -r <license> <explorer_path> For example: cp -r nebula.license /usr/local/nebula-explorer After adding the license, you need to stop and restart the service using the following command. systemctl stop nebula-explorer #Stop the service systemctl start nebula-explorer #Start the service","title":"Installation"},{"location":"nebula-explorer/deploy-connect/ex-ug-deploy/#start_and_stop","text":"You can use SystemCTL to start and stop the service. systemctl status nebula-explorer #Check the status systemctl stop nebula-explorer #Stop the service systemctl start nebula-explorer #Start the service You can also start or stop the service manually using the following command in the installation directory. cd ./scripts/rpm bash ./start.sh #Start the service bash ./stop.sh #Stop the service","title":"Start and stop"},{"location":"nebula-explorer/deploy-connect/ex-ug-deploy/#uninstallation","text":"You can uninstall Explorer using the following command: sudo rpm -e nebula-graph-explorer-<version>.x86_64","title":"Uninstallation"},{"location":"nebula-explorer/deploy-connect/ex-ug-deploy/#deb-based_deployment","text":"","title":"DEB-based deployment"},{"location":"nebula-explorer/deploy-connect/ex-ug-deploy/#installation_1","text":"Select and download the RPM package according to your needs. It is recommended to select the latest version. Common links are as follows: Enterpriseonly You can apply online for Explorer free trial. To purchase, contact our sales team via email (inquiry@vesoft.com). For features of Explorer, see Pricing . Run sudo dpkg -i <package_name> to unpack the DEB package. For example, run the following command to install Explorer (The default installation path is /usr/local/nebula-explorer ). sudo dpkg -i nebula-explorer-3.0.0.x86_64.deb Note You cannot customize the installation path of Explorer when installing a DEB package. Copy the license to the Explorer installation path. Sudo cp -r <license> <explorer_path> For example: Sudo cp -r nebula.license /usr/local/nebula-explorer Run the following command to start the service. sudo systemctl start nebula-explorer.service You can also start the service manually using the following command in the nebula-explorer/lib directory. sudo bash ./start.sh","title":"Installation"},{"location":"nebula-explorer/deploy-connect/ex-ug-deploy/#view_the_status","text":"sudo systemctl status nebula-explorer.service","title":"View the status"},{"location":"nebula-explorer/deploy-connect/ex-ug-deploy/#stop_the_service","text":"sudo systemctl stop nebula-explorer.service","title":"Stop the service"},{"location":"nebula-explorer/deploy-connect/ex-ug-deploy/#uninstallation_1","text":"Run the following command to uninstall Explorer: sudo dpkg -r nebula-explorer","title":"Uninstallation"},{"location":"nebula-explorer/deploy-connect/ex-ug-deploy/#tar-based_deployment","text":"","title":"TAR-based deployment"},{"location":"nebula-explorer/deploy-connect/ex-ug-deploy/#installation_2","text":"Select and download the TAR package according to your needs. It is recommended to select the latest version. Common links are as follows: Enterpriseonly Explorer is only available in the Enterprise Edition. Click Pricing to see more. Use tar -xvf to decompress the TAR package. tar -xvf nebula-graph-explorer-<version>.tar.gz Copy the license to the nebula-explorer directory. cp -r <license> <explorer_path> For example: cp -r nebula.license /usr/local/nebula-explorer Enter the nebula-explorer folder to start Explorer. cd nebula-explorer ./nebula-httpd &","title":"Installation"},{"location":"nebula-explorer/deploy-connect/ex-ug-deploy/#stop_service","text":"You can use kill pid to stop the service. kill $( lsof -t -i :7002 )","title":"Stop Service"},{"location":"nebula-explorer/deploy-connect/ex-ug-deploy/#next_to_do","text":"When Explorer is started, use http://<ip_address>:7002 to get access to Explorer. The following login page shows that Explorer is successfully connected to Nebula Graph. Note When logging into Nebula Explorer for the first time, the content of END USER LICENSE AGREEMENT is displayed on the login page. Please read it and then click I agree . After entering the Explorer login interface, you need to connect to Nebula Graph. For more information, refer to Connecting to the Nebula Graph .","title":"Next to do"},{"location":"nebula-importer/config-with-header/","text":"Configuration with Header \u00b6 For a CSV file with header, you need to set withHeader to true in the configuration file, indicating that the first behavior in the CSV file is the header. The header content has special meanings. Caution If the CSV file contains headers, the Importer will parse the Schema of each row of data according to the headers and ignore the vertex or edge settings in the YAML file. Sample files \u00b6 The following is an example of a CSV file with header: sample of vertex Example data for student_with_header.csv : :VID(string),student.name:string,student.age:int,student.gender:string student100,Monica,16,female student101,Mike,18,male student102,Jane,17,female The first column is the vertex ID, followed by the properties name , age , and gender . sample of edge Example data for follow_with_header.csv : :SRC_VID(string),:DST_VID(string),:RANK,follow.degree:double student100,student101,0,92.5 student101,student100,1,85.6 student101,student102,2,93.2 student100,student102,1,96.2 The first two columns are the start vertex ID and destination vertex ID, respectively. The third column is rank, and the fourth column is property degree . Header format description \u00b6 The header defines the start vertex, the destination vertex, the rank, and some special functions by keywords as follows: :VID (mandatory): Vertex ID. Need to use :VID(type) form to set data type, for example :VID(string) or :VID(int) . :SRC_VID (mandatory): The start vertex ID of the edge. The data type needs to be set in the form :SRC_VID(type) . :DST_VID (mandatory): The destination vertex ID of the edge. The data type needs to be set in the form :DST_VID(type) . :RANK (optional): The rank value of the edge. :IGNORE (optional): Ignore this column when inserting data. :LABEL (optional): Insert (+) or delete (-) the row. Must be column 1. For example: :LABEL, +, -, Note All columns except the :LABEL column can be sorted in any order, so for larger CSV files, the user has the flexibility to set the header to select the desired column. For Tag or Edge type properties, the format is <tag_name/edge_name>.<prop_name>:<prop_type> , described as follows: <tag_name/edge_name> : Tag or Edge type name. <prop_name> : property name. <prop_type> : property type. Support bool , int , float , double , timestamp and string , default string . Such as student.name:string , follow.degree:double . Sample configuration \u00b6 # Connected to the Nebula Graph version, set to v3 when connected to 3.x. version : v3 description : example # Whether to delete temporarily generated logs and error data files. removeTempFiles : false clientSettings : # Retry times of nGQL statement execution failures. retry : 3 # Number of Nebula Graph client concurrency. concurrency : 10 # Cache queue size per Nebula Graph client. channelBufferSize : 128 # Specifies the Nebula Graph space to import the data into. space : student # Connection information. connection : user : root password : nebula address : 192.168.*.13:9669 postStart : # Configure some of the operations to perform after connecting to the Nebula Graph server, and before inserting data. commands : | DROP SPACE IF EXISTS student; CREATE SPACE IF NOT EXISTS student(partition_num=5, replica_factor=1, vid_type=FIXED_STRING(20)); USE student; CREATE TAG student(name string, age int,gender string); CREATE EDGE follow(degree int); # The interval between the execution of the above command and the execution of the insert data command. afterPeriod : 15s preStop : # Configure some of the actions you performed before disconnecting from the Nebula Graph server. commands : | # Path of the error log file. logPath : ./err/test.log # CSV file Settings. files : # Path for storing data files. If a relative path is used, the path is merged with the current configuration file directory. The first data file in this example is vertex data. - path : ./student_with_header.csv # Insert the failed data file storage path, so that data can be written later. failDataPath : ./err/studenterr.csv # The number of statements inserting data in a batch. batchSize : 10 # Limit on the number of rows of read data. limit : 10 # Whether to insert rows in the file in order. If the value is set to false, the import rate decreases due to data skew. inOrder : true # File type. Currently, only CSV files are supported. type : csv csv : # Whether there is a header. withHeader : true # Whether there is a LABEL. withLabel : false # Specifies the delimiter for the CSV file. A string delimiter that supports only one character. delimiter : \",\" schema : # Schema type. Possible values are vertex and edge. type : vertex # The second data file in this example is edge data. - path : ./follow_with_header.csv failDataPath : ./err/followerr.csv batchSize : 10 limit : 10 inOrder : true type : csv csv : withHeader : true withLabel : false schema : # The type of Schema is edge. type : edge edge : # Edge type name. name : follow # Whether to include rank. withRanking : true Note The data type of the vertex ID must be the same as the data type of the statement in clientSettings.postStart.commands that creates the graph space.","title":"Configuration with Header"},{"location":"nebula-importer/config-with-header/#configuration_with_header","text":"For a CSV file with header, you need to set withHeader to true in the configuration file, indicating that the first behavior in the CSV file is the header. The header content has special meanings. Caution If the CSV file contains headers, the Importer will parse the Schema of each row of data according to the headers and ignore the vertex or edge settings in the YAML file.","title":"Configuration with Header"},{"location":"nebula-importer/config-with-header/#sample_files","text":"The following is an example of a CSV file with header: sample of vertex Example data for student_with_header.csv : :VID(string),student.name:string,student.age:int,student.gender:string student100,Monica,16,female student101,Mike,18,male student102,Jane,17,female The first column is the vertex ID, followed by the properties name , age , and gender . sample of edge Example data for follow_with_header.csv : :SRC_VID(string),:DST_VID(string),:RANK,follow.degree:double student100,student101,0,92.5 student101,student100,1,85.6 student101,student102,2,93.2 student100,student102,1,96.2 The first two columns are the start vertex ID and destination vertex ID, respectively. The third column is rank, and the fourth column is property degree .","title":"Sample files"},{"location":"nebula-importer/config-with-header/#header_format_description","text":"The header defines the start vertex, the destination vertex, the rank, and some special functions by keywords as follows: :VID (mandatory): Vertex ID. Need to use :VID(type) form to set data type, for example :VID(string) or :VID(int) . :SRC_VID (mandatory): The start vertex ID of the edge. The data type needs to be set in the form :SRC_VID(type) . :DST_VID (mandatory): The destination vertex ID of the edge. The data type needs to be set in the form :DST_VID(type) . :RANK (optional): The rank value of the edge. :IGNORE (optional): Ignore this column when inserting data. :LABEL (optional): Insert (+) or delete (-) the row. Must be column 1. For example: :LABEL, +, -, Note All columns except the :LABEL column can be sorted in any order, so for larger CSV files, the user has the flexibility to set the header to select the desired column. For Tag or Edge type properties, the format is <tag_name/edge_name>.<prop_name>:<prop_type> , described as follows: <tag_name/edge_name> : Tag or Edge type name. <prop_name> : property name. <prop_type> : property type. Support bool , int , float , double , timestamp and string , default string . Such as student.name:string , follow.degree:double .","title":"Header format description"},{"location":"nebula-importer/config-with-header/#sample_configuration","text":"# Connected to the Nebula Graph version, set to v3 when connected to 3.x. version : v3 description : example # Whether to delete temporarily generated logs and error data files. removeTempFiles : false clientSettings : # Retry times of nGQL statement execution failures. retry : 3 # Number of Nebula Graph client concurrency. concurrency : 10 # Cache queue size per Nebula Graph client. channelBufferSize : 128 # Specifies the Nebula Graph space to import the data into. space : student # Connection information. connection : user : root password : nebula address : 192.168.*.13:9669 postStart : # Configure some of the operations to perform after connecting to the Nebula Graph server, and before inserting data. commands : | DROP SPACE IF EXISTS student; CREATE SPACE IF NOT EXISTS student(partition_num=5, replica_factor=1, vid_type=FIXED_STRING(20)); USE student; CREATE TAG student(name string, age int,gender string); CREATE EDGE follow(degree int); # The interval between the execution of the above command and the execution of the insert data command. afterPeriod : 15s preStop : # Configure some of the actions you performed before disconnecting from the Nebula Graph server. commands : | # Path of the error log file. logPath : ./err/test.log # CSV file Settings. files : # Path for storing data files. If a relative path is used, the path is merged with the current configuration file directory. The first data file in this example is vertex data. - path : ./student_with_header.csv # Insert the failed data file storage path, so that data can be written later. failDataPath : ./err/studenterr.csv # The number of statements inserting data in a batch. batchSize : 10 # Limit on the number of rows of read data. limit : 10 # Whether to insert rows in the file in order. If the value is set to false, the import rate decreases due to data skew. inOrder : true # File type. Currently, only CSV files are supported. type : csv csv : # Whether there is a header. withHeader : true # Whether there is a LABEL. withLabel : false # Specifies the delimiter for the CSV file. A string delimiter that supports only one character. delimiter : \",\" schema : # Schema type. Possible values are vertex and edge. type : vertex # The second data file in this example is edge data. - path : ./follow_with_header.csv failDataPath : ./err/followerr.csv batchSize : 10 limit : 10 inOrder : true type : csv csv : withHeader : true withLabel : false schema : # The type of Schema is edge. type : edge edge : # Edge type name. name : follow # Whether to include rank. withRanking : true Note The data type of the vertex ID must be the same as the data type of the statement in clientSettings.postStart.commands that creates the graph space.","title":"Sample configuration"},{"location":"nebula-importer/config-without-header/","text":"Configuration without Header \u00b6 For CSV files without header, you need to set withHeader to false in the configuration file, indicating that the CSV file contains only data (excluding the header of the first row). You may also need to set the data type and corresponding columns. Sample files \u00b6 The following is an example of a CSV file without header: sample of vertex Example data for student_without_header.csv : student100,Monica,16,female student101,Mike,18,male student102,Jane,17,female The first column is the vertex ID, followed by the properties name , age , and gender . sample of edge Example data for follow_without_header.csv : student100,student101,0,92.5 student101,student100,1,85.6 student101,student102,2,93.2 student100,student102,1,96.2 The first two columns are the start vertex ID and destination vertex ID, respectively. The third column is rank, and the fourth column is property degree . Sample configuration \u00b6 # Connected to the Nebula Graph version, set to v3 when connected to 3.x. version : v3 description : example # Whether to delete temporarily generated logs and error data files. removeTempFiles : false clientSettings : # Retry times of nGQL statement execution failures. retry : 3 # Number of Nebula Graph client concurrency. concurrency : 10 # Cache queue size per Nebula Graph client. channelBufferSize : 128 # Specifies the Nebula Graph space to import the data into. space : student # Connection information. connection : user : root password : nebula address : 192.168.*.13:9669 postStart : # Configure some of the operations to perform after connecting to the Nebula Graph server, and before inserting data. commands : | DROP SPACE IF EXISTS student; CREATE SPACE IF NOT EXISTS student(partition_num=5, replica_factor=1, vid_type=FIXED_STRING(20)); USE student; CREATE TAG student(name string, age int,gender string); CREATE EDGE follow(degree int); # The interval between the execution of the above command and the execution of the insert data command. afterPeriod : 15s preStop : # Configure some of the actions you performed before disconnecting from the Nebula Graph server. commands : | # Path of the error log file. logPath : ./err/test.log # CSV file Settings. files : # Path for storing data files. If a relative path is used, the path is merged with the current configuration file directory. The first data file in this example is vertex data. - path : ./student_without_header.csv # Insert the failed data file storage path, so that data can be written later. failDataPath : ./err/studenterr.csv # The number of statements inserting data in a batch. batchSize : 10 # Limit on the number of rows of read data. limit : 10 # Whether to insert rows in the file in order. If the value is set to false, the import rate decreases due to data skew. inOrder : true # File type. Currently, only CSV files are supported. type : csv csv : # Whether there is a header. withHeader : false # Whether there is a LABEL. withLabel : false # Specifies the delimiter for the CSV file. A string delimiter that supports only one character. delimiter : \",\" schema : # Schema type. Possible values are vertex and edge. type : vertex vertex : # Vertex ID Settings. vid : # The vertex ID corresponds to the column number in the CSV file. Columns in the CSV file are numbered from 0. index : 0 # The data type of the vertex ID. The optional values are int and string, corresponding to INT64 and FIXED_STRING in the Nebula Graph, respectively. type : string # Tag Settings. # Tag name. - name : student # property Settings in the Tag. props : # property name. - name : name # Property data type. type : string # Property corresponds to the sequence number of the column in the CSV file. index : 1 - name : age type : int index : 2 - name : gender type : string index : 3 # The second data file in this example is edge data. - path : ./follow_without_header.csv failDataPath : ./err/followerr.csv batchSize : 10 limit : 10 inOrder : true type : csv csv : withHeader : false withLabel : false schema : # The type of Schema is edge. type : edge edge : # Edge type name. name : follow # Whether to include rank. withRanking : true # Start vertex ID setting. srcVID : # Data type. type : string # The start vertex ID corresponds to the sequence number of a column in the CSV file. index : 0 # Destination vertex ID. dstVID : type : string index : 1 # rank setting. rank : # Rank Indicates the rank number of a column in the CSV file. If index is not set, be sure to set the rank value in the third column. Subsequent columns set each property in turn. index : 2 # Edge Type property Settings. props : # property name. - name : degree # Data type. type : double # Property corresponds to the sequence number of the column in the CSV file. index : 3 Note The sequence numbers of the columns in the CSV file start from 0, that is, the sequence numbers of the first column are 0, and the sequence numbers of the second column are 1. The data type of the vertex ID must be the same as the data type of the statement in clientSettings.postStart.commands that creates the graph space. If the index field is not specified, the CSV file must comply with the following rules: In the vertex data file, the first column must be the vertex ID, followed by the properties, and must correspond to the order in the configuration file. In the side data file, the first column must be the start vertex ID, the second column must be the destination vertex ID, if withRanking is true , the third column must be the rank value, and the following columns must be properties, and must correspond to the order in the configuration file.","title":"Configuration without Header"},{"location":"nebula-importer/config-without-header/#configuration_without_header","text":"For CSV files without header, you need to set withHeader to false in the configuration file, indicating that the CSV file contains only data (excluding the header of the first row). You may also need to set the data type and corresponding columns.","title":"Configuration without Header"},{"location":"nebula-importer/config-without-header/#sample_files","text":"The following is an example of a CSV file without header: sample of vertex Example data for student_without_header.csv : student100,Monica,16,female student101,Mike,18,male student102,Jane,17,female The first column is the vertex ID, followed by the properties name , age , and gender . sample of edge Example data for follow_without_header.csv : student100,student101,0,92.5 student101,student100,1,85.6 student101,student102,2,93.2 student100,student102,1,96.2 The first two columns are the start vertex ID and destination vertex ID, respectively. The third column is rank, and the fourth column is property degree .","title":"Sample files"},{"location":"nebula-importer/config-without-header/#sample_configuration","text":"# Connected to the Nebula Graph version, set to v3 when connected to 3.x. version : v3 description : example # Whether to delete temporarily generated logs and error data files. removeTempFiles : false clientSettings : # Retry times of nGQL statement execution failures. retry : 3 # Number of Nebula Graph client concurrency. concurrency : 10 # Cache queue size per Nebula Graph client. channelBufferSize : 128 # Specifies the Nebula Graph space to import the data into. space : student # Connection information. connection : user : root password : nebula address : 192.168.*.13:9669 postStart : # Configure some of the operations to perform after connecting to the Nebula Graph server, and before inserting data. commands : | DROP SPACE IF EXISTS student; CREATE SPACE IF NOT EXISTS student(partition_num=5, replica_factor=1, vid_type=FIXED_STRING(20)); USE student; CREATE TAG student(name string, age int,gender string); CREATE EDGE follow(degree int); # The interval between the execution of the above command and the execution of the insert data command. afterPeriod : 15s preStop : # Configure some of the actions you performed before disconnecting from the Nebula Graph server. commands : | # Path of the error log file. logPath : ./err/test.log # CSV file Settings. files : # Path for storing data files. If a relative path is used, the path is merged with the current configuration file directory. The first data file in this example is vertex data. - path : ./student_without_header.csv # Insert the failed data file storage path, so that data can be written later. failDataPath : ./err/studenterr.csv # The number of statements inserting data in a batch. batchSize : 10 # Limit on the number of rows of read data. limit : 10 # Whether to insert rows in the file in order. If the value is set to false, the import rate decreases due to data skew. inOrder : true # File type. Currently, only CSV files are supported. type : csv csv : # Whether there is a header. withHeader : false # Whether there is a LABEL. withLabel : false # Specifies the delimiter for the CSV file. A string delimiter that supports only one character. delimiter : \",\" schema : # Schema type. Possible values are vertex and edge. type : vertex vertex : # Vertex ID Settings. vid : # The vertex ID corresponds to the column number in the CSV file. Columns in the CSV file are numbered from 0. index : 0 # The data type of the vertex ID. The optional values are int and string, corresponding to INT64 and FIXED_STRING in the Nebula Graph, respectively. type : string # Tag Settings. # Tag name. - name : student # property Settings in the Tag. props : # property name. - name : name # Property data type. type : string # Property corresponds to the sequence number of the column in the CSV file. index : 1 - name : age type : int index : 2 - name : gender type : string index : 3 # The second data file in this example is edge data. - path : ./follow_without_header.csv failDataPath : ./err/followerr.csv batchSize : 10 limit : 10 inOrder : true type : csv csv : withHeader : false withLabel : false schema : # The type of Schema is edge. type : edge edge : # Edge type name. name : follow # Whether to include rank. withRanking : true # Start vertex ID setting. srcVID : # Data type. type : string # The start vertex ID corresponds to the sequence number of a column in the CSV file. index : 0 # Destination vertex ID. dstVID : type : string index : 1 # rank setting. rank : # Rank Indicates the rank number of a column in the CSV file. If index is not set, be sure to set the rank value in the third column. Subsequent columns set each property in turn. index : 2 # Edge Type property Settings. props : # property name. - name : degree # Data type. type : double # Property corresponds to the sequence number of the column in the CSV file. index : 3 Note The sequence numbers of the columns in the CSV file start from 0, that is, the sequence numbers of the first column are 0, and the sequence numbers of the second column are 1. The data type of the vertex ID must be the same as the data type of the statement in clientSettings.postStart.commands that creates the graph space. If the index field is not specified, the CSV file must comply with the following rules: In the vertex data file, the first column must be the vertex ID, followed by the properties, and must correspond to the order in the configuration file. In the side data file, the first column must be the start vertex ID, the second column must be the destination vertex ID, if withRanking is true , the third column must be the rank value, and the following columns must be properties, and must correspond to the order in the configuration file.","title":"Sample configuration"},{"location":"nebula-importer/use-importer/","text":"Nebula Importer \u00b6 Nebula Importer (Importer) is a standalone import tool for CSV files with Nebula Graph . Importer can read the local CSV file and then import the data into the Nebula Graph database. Scenario \u00b6 Importer is used to import the contents of a local CSV file into the Nebula Graph. Advantage \u00b6 Lightweight and fast: no complex environment can be used, fast data import. Flexible filtering: You can flexibly filter CSV data through configuration files. Release note \u00b6 Release Prerequisites \u00b6 Before using Nebula Importer, make sure: Nebula Graph service has been deployed. There are currently three deployment modes: Deploy Nebula Graph with Docker Compose Install Nebula Graph with RPM or DEB package Install Nebula Graph by compiling the source code Schema is created in Nebula Graph, including space, Tag and Edge type, or set by parameter clientSettings.postStart.commands . Golang environment has been deployed on the machine running the Importer. For details, see Build Go environment . Steps \u00b6 Configure the YAML file and prepare the CSV file to be imported to use the tool to batch write data to Nebula Graph. Download binary package and run \u00b6 Download the binary package directly and add execute permission to it. Start the service. $ ./<binary_package_name> --config <yaml_config_file_path> Source code compile and run \u00b6 Clone repository. $ git clone -b release-3.1 https://github.com/vesoft-inc/nebula-importer.git Note Use the correct branch. Nebula Graph 2.x and 3.x have different RPC protocols. Access the directory nebula-importer . $ cd nebula-importer Compile the source code. $ make build Start the service. $ ./nebula-importer --config <yaml_config_file_path> Note For details about the YAML configuration file, see configuration file description at the end of topic. No network compilation mode \u00b6 If the server cannot be connected to the Internet, it is recommended to upload the source code and various dependency packages to the corresponding server for compilation on the machine that can be connected to the Internet. The operation steps are as follows: Clone repository. $ git clone -b 3 .1.0 https://github.com/vesoft-inc/nebula-importer.git Use the following command to download and package the dependent source code. $ cd nebula-importer $ go mod vendor $ cd .. && tar -zcvf nebula-importer.tar.gz nebula-importer Upload the compressed package to a server that cannot be connected to the Internet. Unzip and compile. $ tar -zxvf nebula-importer.tar.gz $ cd nebula-importer $ go build -mod vendor cmd/importer.go Run in Docker mode \u00b6 Instead of installing the Go locale locally, you can use Docker to pull the image of the Nebula Importer and mount the local configuration file and CSV data file into the container. The command is as follows: $ docker run --rm -ti \\ --network = host \\ -v <config_file>:<config_file> \\ -v <csv_data_dir>:<csv_data_dir> \\ vesoft/nebula-importer:<version> --config <config_file> <config_file> : The absolute path to the local YAML configuration file. <csv_data_dir> : The absolute path to the local CSV data file. <version> : Nebula Graph 2.x Please fill in 'v3'. Note A relative path is recommended. If you use a local absolute path, check that the path maps to the path in the Docker. Configuration File Description \u00b6 Nebula Importer uses configuration( nebula-importer/examples/v2/example.yaml ) files to describe information about the files to be imported, the Nebula Graph server, and more. You can refer to the example configuration file: Configuration without Header / Configuration with Header . This section describes the fields in the configuration file by category. Note If users download a binary package, create the configuration file manually. Basic configuration \u00b6 The example configuration is as follows: version : v3 description : example removeTempFiles : false Parameter Default value Required Description version v2 Yes Target version of Nebula Graph. description example No Description of the configuration file. removeTempFiles false No Whether to delete temporarily generated logs and error data files. Client configuration \u00b6 The client configuration stores the configurations associated with Nebula Graph. The example configuration is as follows: clientSettings : retry : 3 concurrency : 10 channelBufferSize : 128 space : test connection : user : user password : password address : 192.168.*.13:9669,192.168.*.14:9669 postStart : commands : | UPDATE CONFIGS storage:wal_ttl=3600; UPDATE CONFIGS storage:rocksdb_column_family_options = { disable_auto_compactions = true }; afterPeriod : 8s preStop : commands : | UPDATE CONFIGS storage:wal_ttl=86400; UPDATE CONFIGS storage:rocksdb_column_family_options = { disable_auto_compactions = false }; Parameter Default value Required Description clientSettings.retry 3 No Retry times of nGQL statement execution failures. clientSettings.concurrency 10 No Number of Nebula Graph client concurrency. clientSettings.channelBufferSize 128 No Cache queue size per Nebula Graph client. clientSettings.space - Yes Specifies the Nebula Graph space to import the data into. Do not import multiple spaces at the same time to avoid performance impact. clientSettings.connection.user - Yes Nebula Graph user name. clientSettings.connection.password - Yes The password for the Nebula Graph user name. clientSettings.connection.address - Yes Addresses and ports for all Graph services. clientSettings.postStart.commands - No Configure some of the operations to perform after connecting to the Nebula Graph server, and before inserting data. clientSettings.postStart.afterPeriod - No The interval, between executing the above commands and executing the insert data command, such as 8s . clientSettings.preStop.commands - No Configure some of the actions you performed before disconnecting from the Nebula Graph server. File configuration \u00b6 File configuration Stores the configuration of data files and logs, and details about the Schema. File and log configuration \u00b6 The example configuration is as follows: logPath : ./err/test.log files : - path : ./student_without_header.csv failDataPath : ./err/studenterr.csv batchSize : 128 limit : 10 inOrder : false type : csv csv : withHeader : false withLabel : false delimiter : \",\" Parameter Default value Required Description logPath - No Path for exporting log information, such as errors during import. files.path - Yes Path for storing data files. If a relative path is used, the path is merged with the current configuration file directory. You can use an asterisk (*) for fuzzy matching to import multiple files with similar names, but the files need to be the same structure. files.failDataPath - Yes Insert the failed data file storage path, so that data can be written later. files.batchSize 128 No The number of statements inserting data in a batch. files.limit - No Limit on the number of rows of read data. files.inOrder - No Whether to insert rows in the file in order. If the value is set to false , the import rate decreases due to data skew. files.type - Yes The file type. files.csv.withHeader false Yes Whether there is a header. files.csv.withLabel false Yes Whether there is a label. files.csv.delimiter \",\" Yes Specifies the delimiter for the CSV file. A string delimiter that supports only one character. Schema configuration \u00b6 Schema configuration describes the Meta information of the current data file. Schema types are vertex and edge. Multiple vertexes or edges can be configured at the same time. vertex configuration The example configuration is as follows: schema : type : vertex vertex : vid : type : string index : 0 tags : - name : student props : - name : name type : string index : 1 - name : age type : int index : 2 - name : gender type : string index : 3 Parameter Default value Required Description files.schema.type - Yes Schema type. Possible values are vertex and edge . files.schema.vertex.vid.type - No The data type of the vertex ID. Possible values are int and string . files.schema.vertex.vid.index - No The vertex ID corresponds to the column number in the CSV file. files.schema.vertex.tags.name - Yes Tag name. files.schema.vertex.tags.props.name - Yes Tag property name, which must match the Tag property in the Nebula Graph. files.schema.vertex.tags.props.type - Yes Property data type, supporting bool , int , float , double , timestamp and string . files.schema.vertex.tags.props.index - No Property corresponds to the sequence number of the column in the CSV file. Note The sequence numbers of the columns in the CSV file start from 0, that is, the sequence numbers of the first column are 0, and the sequence numbers of the second column are 1. edge configuration The example configuration is as follows: schema : type : edge edge : name : follow withRanking : true srcVID : type : string index : 0 dstVID : type : string index : 1 rank : index : 2 props : - name : degree type : double index : 3 Parameter Default value Required Description files.schema.type - Yes Schema type. Possible values are vertex and edge . files.schema.edge.name - Yes Edge type name. files.schema.edge.srcVID.type - No \u8fb9\u7684\u8d77\u59cb\u70b9ID\u7684\u6570\u636e\u7c7b\u578b. files.schema.edge.srcVID.index - No The data type of the starting vertex ID of the edge. files.schema.edge.dstVID.type - No The data type of the destination vertex ID of the edge. files.schema.edge.dstVID.index - No The destination vertex ID of the edge corresponds to the column number in the CSV file. files.schema.edge.rank.index - No The rank value of the edge corresponds to the column number in the CSV file. files.schema.edge.props.name - Yes The Edge Type property name must match the Edge Type property in the Nebula Graph. files.schema.edge.props.type - Yes Property data type, supporting bool , int , float , double , timestamp and string . files.schema.edge.props.index - No Property corresponds to the sequence number of the column in the CSV file. About the CSV file header \u00b6 According to whether the CSV file has a header or not, the Importer needs to make different Settings on the configuration file. For relevant examples and explanations, please refer to: Configuration without Header Configuration with Header","title":"Use Nebula Importer"},{"location":"nebula-importer/use-importer/#nebula_importer","text":"Nebula Importer (Importer) is a standalone import tool for CSV files with Nebula Graph . Importer can read the local CSV file and then import the data into the Nebula Graph database.","title":"Nebula Importer"},{"location":"nebula-importer/use-importer/#scenario","text":"Importer is used to import the contents of a local CSV file into the Nebula Graph.","title":"Scenario"},{"location":"nebula-importer/use-importer/#advantage","text":"Lightweight and fast: no complex environment can be used, fast data import. Flexible filtering: You can flexibly filter CSV data through configuration files.","title":"Advantage"},{"location":"nebula-importer/use-importer/#release_note","text":"Release","title":"Release note"},{"location":"nebula-importer/use-importer/#prerequisites","text":"Before using Nebula Importer, make sure: Nebula Graph service has been deployed. There are currently three deployment modes: Deploy Nebula Graph with Docker Compose Install Nebula Graph with RPM or DEB package Install Nebula Graph by compiling the source code Schema is created in Nebula Graph, including space, Tag and Edge type, or set by parameter clientSettings.postStart.commands . Golang environment has been deployed on the machine running the Importer. For details, see Build Go environment .","title":"Prerequisites"},{"location":"nebula-importer/use-importer/#steps","text":"Configure the YAML file and prepare the CSV file to be imported to use the tool to batch write data to Nebula Graph.","title":"Steps"},{"location":"nebula-importer/use-importer/#download_binary_package_and_run","text":"Download the binary package directly and add execute permission to it. Start the service. $ ./<binary_package_name> --config <yaml_config_file_path>","title":"Download binary package and run"},{"location":"nebula-importer/use-importer/#source_code_compile_and_run","text":"Clone repository. $ git clone -b release-3.1 https://github.com/vesoft-inc/nebula-importer.git Note Use the correct branch. Nebula Graph 2.x and 3.x have different RPC protocols. Access the directory nebula-importer . $ cd nebula-importer Compile the source code. $ make build Start the service. $ ./nebula-importer --config <yaml_config_file_path> Note For details about the YAML configuration file, see configuration file description at the end of topic.","title":"Source code compile and run"},{"location":"nebula-importer/use-importer/#no_network_compilation_mode","text":"If the server cannot be connected to the Internet, it is recommended to upload the source code and various dependency packages to the corresponding server for compilation on the machine that can be connected to the Internet. The operation steps are as follows: Clone repository. $ git clone -b 3 .1.0 https://github.com/vesoft-inc/nebula-importer.git Use the following command to download and package the dependent source code. $ cd nebula-importer $ go mod vendor $ cd .. && tar -zcvf nebula-importer.tar.gz nebula-importer Upload the compressed package to a server that cannot be connected to the Internet. Unzip and compile. $ tar -zxvf nebula-importer.tar.gz $ cd nebula-importer $ go build -mod vendor cmd/importer.go","title":"No network compilation mode"},{"location":"nebula-importer/use-importer/#run_in_docker_mode","text":"Instead of installing the Go locale locally, you can use Docker to pull the image of the Nebula Importer and mount the local configuration file and CSV data file into the container. The command is as follows: $ docker run --rm -ti \\ --network = host \\ -v <config_file>:<config_file> \\ -v <csv_data_dir>:<csv_data_dir> \\ vesoft/nebula-importer:<version> --config <config_file> <config_file> : The absolute path to the local YAML configuration file. <csv_data_dir> : The absolute path to the local CSV data file. <version> : Nebula Graph 2.x Please fill in 'v3'. Note A relative path is recommended. If you use a local absolute path, check that the path maps to the path in the Docker.","title":"Run in Docker mode"},{"location":"nebula-importer/use-importer/#configuration_file_description","text":"Nebula Importer uses configuration( nebula-importer/examples/v2/example.yaml ) files to describe information about the files to be imported, the Nebula Graph server, and more. You can refer to the example configuration file: Configuration without Header / Configuration with Header . This section describes the fields in the configuration file by category. Note If users download a binary package, create the configuration file manually.","title":"Configuration File Description"},{"location":"nebula-importer/use-importer/#basic_configuration","text":"The example configuration is as follows: version : v3 description : example removeTempFiles : false Parameter Default value Required Description version v2 Yes Target version of Nebula Graph. description example No Description of the configuration file. removeTempFiles false No Whether to delete temporarily generated logs and error data files.","title":"Basic configuration"},{"location":"nebula-importer/use-importer/#client_configuration","text":"The client configuration stores the configurations associated with Nebula Graph. The example configuration is as follows: clientSettings : retry : 3 concurrency : 10 channelBufferSize : 128 space : test connection : user : user password : password address : 192.168.*.13:9669,192.168.*.14:9669 postStart : commands : | UPDATE CONFIGS storage:wal_ttl=3600; UPDATE CONFIGS storage:rocksdb_column_family_options = { disable_auto_compactions = true }; afterPeriod : 8s preStop : commands : | UPDATE CONFIGS storage:wal_ttl=86400; UPDATE CONFIGS storage:rocksdb_column_family_options = { disable_auto_compactions = false }; Parameter Default value Required Description clientSettings.retry 3 No Retry times of nGQL statement execution failures. clientSettings.concurrency 10 No Number of Nebula Graph client concurrency. clientSettings.channelBufferSize 128 No Cache queue size per Nebula Graph client. clientSettings.space - Yes Specifies the Nebula Graph space to import the data into. Do not import multiple spaces at the same time to avoid performance impact. clientSettings.connection.user - Yes Nebula Graph user name. clientSettings.connection.password - Yes The password for the Nebula Graph user name. clientSettings.connection.address - Yes Addresses and ports for all Graph services. clientSettings.postStart.commands - No Configure some of the operations to perform after connecting to the Nebula Graph server, and before inserting data. clientSettings.postStart.afterPeriod - No The interval, between executing the above commands and executing the insert data command, such as 8s . clientSettings.preStop.commands - No Configure some of the actions you performed before disconnecting from the Nebula Graph server.","title":"Client configuration"},{"location":"nebula-importer/use-importer/#file_configuration","text":"File configuration Stores the configuration of data files and logs, and details about the Schema.","title":"File configuration"},{"location":"nebula-importer/use-importer/#file_and_log_configuration","text":"The example configuration is as follows: logPath : ./err/test.log files : - path : ./student_without_header.csv failDataPath : ./err/studenterr.csv batchSize : 128 limit : 10 inOrder : false type : csv csv : withHeader : false withLabel : false delimiter : \",\" Parameter Default value Required Description logPath - No Path for exporting log information, such as errors during import. files.path - Yes Path for storing data files. If a relative path is used, the path is merged with the current configuration file directory. You can use an asterisk (*) for fuzzy matching to import multiple files with similar names, but the files need to be the same structure. files.failDataPath - Yes Insert the failed data file storage path, so that data can be written later. files.batchSize 128 No The number of statements inserting data in a batch. files.limit - No Limit on the number of rows of read data. files.inOrder - No Whether to insert rows in the file in order. If the value is set to false , the import rate decreases due to data skew. files.type - Yes The file type. files.csv.withHeader false Yes Whether there is a header. files.csv.withLabel false Yes Whether there is a label. files.csv.delimiter \",\" Yes Specifies the delimiter for the CSV file. A string delimiter that supports only one character.","title":"File and log configuration"},{"location":"nebula-importer/use-importer/#schema_configuration","text":"Schema configuration describes the Meta information of the current data file. Schema types are vertex and edge. Multiple vertexes or edges can be configured at the same time. vertex configuration The example configuration is as follows: schema : type : vertex vertex : vid : type : string index : 0 tags : - name : student props : - name : name type : string index : 1 - name : age type : int index : 2 - name : gender type : string index : 3 Parameter Default value Required Description files.schema.type - Yes Schema type. Possible values are vertex and edge . files.schema.vertex.vid.type - No The data type of the vertex ID. Possible values are int and string . files.schema.vertex.vid.index - No The vertex ID corresponds to the column number in the CSV file. files.schema.vertex.tags.name - Yes Tag name. files.schema.vertex.tags.props.name - Yes Tag property name, which must match the Tag property in the Nebula Graph. files.schema.vertex.tags.props.type - Yes Property data type, supporting bool , int , float , double , timestamp and string . files.schema.vertex.tags.props.index - No Property corresponds to the sequence number of the column in the CSV file. Note The sequence numbers of the columns in the CSV file start from 0, that is, the sequence numbers of the first column are 0, and the sequence numbers of the second column are 1. edge configuration The example configuration is as follows: schema : type : edge edge : name : follow withRanking : true srcVID : type : string index : 0 dstVID : type : string index : 1 rank : index : 2 props : - name : degree type : double index : 3 Parameter Default value Required Description files.schema.type - Yes Schema type. Possible values are vertex and edge . files.schema.edge.name - Yes Edge type name. files.schema.edge.srcVID.type - No \u8fb9\u7684\u8d77\u59cb\u70b9ID\u7684\u6570\u636e\u7c7b\u578b. files.schema.edge.srcVID.index - No The data type of the starting vertex ID of the edge. files.schema.edge.dstVID.type - No The data type of the destination vertex ID of the edge. files.schema.edge.dstVID.index - No The destination vertex ID of the edge corresponds to the column number in the CSV file. files.schema.edge.rank.index - No The rank value of the edge corresponds to the column number in the CSV file. files.schema.edge.props.name - Yes The Edge Type property name must match the Edge Type property in the Nebula Graph. files.schema.edge.props.type - Yes Property data type, supporting bool , int , float , double , timestamp and string . files.schema.edge.props.index - No Property corresponds to the sequence number of the column in the CSV file.","title":"Schema configuration"},{"location":"nebula-importer/use-importer/#about_the_csv_file_header","text":"According to whether the CSV file has a header or not, the Importer needs to make different Settings on the configuration file. For relevant examples and explanations, please refer to: Configuration without Header Configuration with Header","title":"About the CSV file header"},{"location":"nebula-operator/1.introduction-to-nebula-operator/","text":"What is Nebula Operator \u00b6 Concept of Nebula Operator \u00b6 Nebula Operator is a tool to automate the deployment, operation, and maintenance of Nebula Graph clusters on Kubernetes . Building upon the excellent scalability mechanism of Kubernetes, Nebula Graph introduced its operation and maintenance knowledge into the Kubernetes system, which makes Nebula Graph a real cloud-native graph database . How it works \u00b6 For resource types that do not exist within Kubernetes\uff0cyou can register them by adding custom API objects. The common way is to use the CustomResourceDefinition . Nebula Operator abstracts the deployment management of Nebula Graph clusters as a CRD. By combining multiple built-in API objects including StatefulSet, Service, and ConfigMap, the routine management and maintenance of a Nebula Graph cluster are coded as a control loop in the Kubernetes system. When a CR instance is submitted, Nebula Operator drives database clusters to the final state according to the control process. Features of Nebula Operator \u00b6 The following features are already available in Nebula Operator: Deploy and uninstall clusters : Nebula Operator simplifies the process of deploying and uninstalling clusters for users. Nebula Operator allows you to quickly create, update, or delete a Nebula Graph cluster by simply providing the corresponding CR file. For more information, see Deploy Nebula Graph Clusters with Kubectl or Deploy Nebula Graph Clusters with Helm . Scale clusters : Nebula Operator calls Nebula Graph's native scaling interfaces in a control loop to implement the scaling logic. You can simply perform scaling operations with YAML configurations and ensure the stability of data. For more information, see Scale clusters with Kubectl or Scale clusters with Helm . Cluster Upgrade : Nebula Operator supports cluster upgrading from version 3.0.0 to version 3.1.x. Self-Healing : Nebula Operator calls interfaces provided by Nebula Graph clusters to dynamically sense cluster service status. Once an exception is detected, Nebula Operator performs fault tolerance. For more information, see Self-Healing . Balance Scheduling : Based on the scheduler extension interface, the scheduler provided by Nebula Operator evenly distributes Pods in a Nebula Graph cluster across all nodes. Limitations \u00b6 Version limitations \u00b6 Nebula Operator does not support the v1.x version of Nebula Graph. Nebula Operator version and the corresponding Nebula Graph version are as follows: Nebula Operator version Nebula Graph version 1.1.0 3.0.0 ~ 3.1.x 1.0.0 3.0.0 ~ 3.1.x 0.9.0 2.5.x ~ 2.6.x 0.8.0 2.5.x Legacy version compatibility The 1.x version Nebula Operator is not compatible with Nebula Graph of version below v3.x. Starting from Nebula Operator 0.9.0, logs and data are stored separately. Using Nebula Operator 0.9.0 or later versions to manage a Nebula Graph 2.5.x cluster created with Operator 0.8.0 can cause compatibility issues. You can backup the data of the Nebula Graph 2.5.x cluster and then create a 2.6.x cluster with Operator 0.9.0. Feature limitations \u00b6 The Nebula Operator scaling feature is only available for the Enterprise Edition of Nebula Graph clusters and does not support scaling the Community Edition version of Nebula Graph clusters. Release note \u00b6 Release","title":"What is Nebula Operator"},{"location":"nebula-operator/1.introduction-to-nebula-operator/#what_is_nebula_operator","text":"","title":"What is Nebula Operator"},{"location":"nebula-operator/1.introduction-to-nebula-operator/#concept_of_nebula_operator","text":"Nebula Operator is a tool to automate the deployment, operation, and maintenance of Nebula Graph clusters on Kubernetes . Building upon the excellent scalability mechanism of Kubernetes, Nebula Graph introduced its operation and maintenance knowledge into the Kubernetes system, which makes Nebula Graph a real cloud-native graph database .","title":"Concept of Nebula Operator"},{"location":"nebula-operator/1.introduction-to-nebula-operator/#how_it_works","text":"For resource types that do not exist within Kubernetes\uff0cyou can register them by adding custom API objects. The common way is to use the CustomResourceDefinition . Nebula Operator abstracts the deployment management of Nebula Graph clusters as a CRD. By combining multiple built-in API objects including StatefulSet, Service, and ConfigMap, the routine management and maintenance of a Nebula Graph cluster are coded as a control loop in the Kubernetes system. When a CR instance is submitted, Nebula Operator drives database clusters to the final state according to the control process.","title":"How it works"},{"location":"nebula-operator/1.introduction-to-nebula-operator/#features_of_nebula_operator","text":"The following features are already available in Nebula Operator: Deploy and uninstall clusters : Nebula Operator simplifies the process of deploying and uninstalling clusters for users. Nebula Operator allows you to quickly create, update, or delete a Nebula Graph cluster by simply providing the corresponding CR file. For more information, see Deploy Nebula Graph Clusters with Kubectl or Deploy Nebula Graph Clusters with Helm . Scale clusters : Nebula Operator calls Nebula Graph's native scaling interfaces in a control loop to implement the scaling logic. You can simply perform scaling operations with YAML configurations and ensure the stability of data. For more information, see Scale clusters with Kubectl or Scale clusters with Helm . Cluster Upgrade : Nebula Operator supports cluster upgrading from version 3.0.0 to version 3.1.x. Self-Healing : Nebula Operator calls interfaces provided by Nebula Graph clusters to dynamically sense cluster service status. Once an exception is detected, Nebula Operator performs fault tolerance. For more information, see Self-Healing . Balance Scheduling : Based on the scheduler extension interface, the scheduler provided by Nebula Operator evenly distributes Pods in a Nebula Graph cluster across all nodes.","title":"Features of Nebula Operator"},{"location":"nebula-operator/1.introduction-to-nebula-operator/#limitations","text":"","title":"Limitations"},{"location":"nebula-operator/1.introduction-to-nebula-operator/#version_limitations","text":"Nebula Operator does not support the v1.x version of Nebula Graph. Nebula Operator version and the corresponding Nebula Graph version are as follows: Nebula Operator version Nebula Graph version 1.1.0 3.0.0 ~ 3.1.x 1.0.0 3.0.0 ~ 3.1.x 0.9.0 2.5.x ~ 2.6.x 0.8.0 2.5.x Legacy version compatibility The 1.x version Nebula Operator is not compatible with Nebula Graph of version below v3.x. Starting from Nebula Operator 0.9.0, logs and data are stored separately. Using Nebula Operator 0.9.0 or later versions to manage a Nebula Graph 2.5.x cluster created with Operator 0.8.0 can cause compatibility issues. You can backup the data of the Nebula Graph 2.5.x cluster and then create a 2.6.x cluster with Operator 0.9.0.","title":"Version limitations"},{"location":"nebula-operator/1.introduction-to-nebula-operator/#feature_limitations","text":"The Nebula Operator scaling feature is only available for the Enterprise Edition of Nebula Graph clusters and does not support scaling the Community Edition version of Nebula Graph clusters.","title":"Feature limitations"},{"location":"nebula-operator/1.introduction-to-nebula-operator/#release_note","text":"Release","title":"Release note"},{"location":"nebula-operator/2.deploy-nebula-operator/","text":"Deploy Nebula Operator \u00b6 You can deploy Nebula Operator with Helm . Background \u00b6 Nebula Operator automates the management of Nebula Graph clusters, and eliminates the need for you to install, scale, upgrade, and uninstall Nebula Graph clusters, which lightens the burden on managing different application versions. Prerequisites \u00b6 Install software \u00b6 Before installing Nebula Operator, you need to install the following software and ensure the correct version of the software: Software Requirement Kubernetes >= 1.16 Helm >= 3.2.0 CoreDNS >= 1.6.0 CertManager >= 1.2.0 OpenKruise >= 0.8.0 If using a role-based access control policy, you need to enable RBAC (optional). Description of software \u00b6 Note The following software used by Nebula Operator is from the third party. Nebula Operator is not responsible for any problems that may arise during the software installation. CoreDNS CoreDNS is a flexible and scalable DNS server that is installed for Pods in Nebula Graph clusters. Components in a Nebula Graph cluster communicate with each other via DNS resolutions for domain names, like x.default.svc.cluster.local . cert-manager Note If you have set the value of the Nebula Operator configuration item admissionWebhook.create to false , there is no need to install cert-manager. For details about Nebula Operator configuration items, see the Customize Helm charts section in Install Nebula Operator below. cert-manager is a tool that automates the management of certificates. It leverages extensions of the Kubernetes API and uses the Webhook server to provide dynamic access control to cert-manager resources. For more information about installation, see cert-manager installation documentation . cert-manager is used to validate the numeric value of replicas for each component in a Nebula Graph cluster. If you run it in a production environment and care about the high availability of Nebula Graph clusters, it is recommended to set the value of admissionWebhook.create to true before installing cert-manager. OpenKruise OpenKruise is a full set of standard extensions for Kubernetes. It works well with original Kubernetes and provides more powerful and efficient features for managing Pods, sidecar containers, and even container images in clusters. OpenKruise is required to enable advanced features for StatefulSets when Nebula Operator starts. For information about installation, see openkruise installation documentation . Steps \u00b6 Install Nebula Operator \u00b6 Add the Nebula Operator chart repository to Helm. helm repo add nebula-operator https://vesoft-inc.github.io/nebula-operator/charts Update information of available charts locally from chart repositories. helm repo update For more information about helm repo , see Helm Repo . Install Nebula Operator. helm install nebula-operator nebula-operator/nebula-operator --namespace = <namespace_name> --version = ${ chart_version } For example, the command to install Nebula Operator of version 1.1.0 is as follows. helm install nebula-operator nebula-operator/nebula-operator --namespace = nebula-operator-system --version = 1 .1.0 nebula-operator-system is a user-created namespace name. If you have not created this namespace, run kubectl create namespace nebula-operator-system to create one. You can also use a different name. 1.1.0 is the version of the Nebula Operator chart. It can be unspecified when there is only one chart version in the Nebula Operator chart repository. Run helm search repo -l nebula-operator to see chart versions. You can customize the configuration items of the Nebula Operator chart before running the installation command. For more information, see Customize Helm charts below. Customize Helm charts \u00b6 Run helm show values [CHART] [flags] to see configurable options. For example: [ k8s@master ~ ] $ helm show values nebula-operator/nebula-operator image : nebulaOperator : image : vesoft/nebula-operator:v1.1.0 imagePullPolicy : Always kubeRBACProxy : image : gcr.io/kubebuilder/kube-rbac-proxy:v0.8.0 imagePullPolicy : Always kubeScheduler : image : k8s.gcr.io/kube-scheduler:v1.18.8 imagePullPolicy : Always imagePullSecrets : [] kubernetesClusterDomain : \"\" controllerManager : create : true replicas : 2 env : [] resources : limits : cpu : 200m memory : 200Mi requests : cpu : 100m memory : 100Mi admissionWebhook : create : true scheduler : create : true schedulerName : nebula-scheduler replicas : 2 env : [] resources : limits : cpu : 200m memory : 20Mi requests : cpu : 100m memory : 100Mi Part of the above parameters are described as follows: Parameter Default value Description image.nebulaOperator.image vesoft/nebula-operator:v1.1.0 The image of Nebula Operator, version of which is 1.1.0. image.nebulaOperator.imagePullPolicy IfNotPresent The image pull policy in Kubernetes. imagePullSecrets - The image pull secret in Kubernetes. kubernetesClusterDomain cluster.local The cluster domain. controllerManager.create true Whether to enable the controller-manager component. controllerManager.replicas 2 The numeric value of controller-manager replicas. admissionWebhook.create true Whether to enable Admission Webhook. shceduler.create true Whether to enable Scheduler. shceduler.schedulerName nebula-scheduler The Scheduler name. shceduler.replicas 2 The numeric value of nebula-scheduler replicas. You can run helm install [NAME] [CHART] [flags] to specify chart configurations when installing a chart. For more information, see Customizing the Chart Before Installing . The following example shows how to specify the Nebula Operator's AdmissionWebhook mechanism to be turned off when you install Nebula Operator (AdmissionWebhook is enabled by default): helm install nebula-operator nebula-operator/nebula-operator --namespace = <nebula-operator-system> --set admissionWebhook.create = false For more information about helm install , see Helm Install . Update Nebula Operator \u00b6 Update the information of available charts locally from chart repositories. helm repo update Update Nebula Operator by passing configuration parameters via -set or -values flag. --set \uff1aOverrides values using the command line. --values (or -f )\uff1aOverrides values using YAML files. For configurable items, see the above-mentioned section Customize Helm charts . For example, to disable the AdmissionWebhook ( AdmissionWebhook is enabled by default), run the following command: helm upgrade nebula-operator nebula-operator/nebula-operator --namespace = nebula-operator-system --version = 1 .1.0 --set admissionWebhook.create = false For more information, see Helm upgrade . Upgrade Nebula Operator \u00b6 Legacy version compatibility The 1.x version Nebula Operator is not compatible with Nebula Graph of version below v3.x. Update the information of available charts locally from chart repositories. helm repo update Upgrade Operator to v1.1.0. helm upgrade nebula-operator nebula-operator/nebula-operator --namespace = <namespace_name> --version = 1 .1.0 For example: helm upgrade nebula-operator nebula-operator/nebula-operator --namespace = nebula-operator-system --version = 1 .1.0 Output: Release \"nebula-operator\" has been upgraded. Happy Helming! NAME: nebula-operator LAST DEPLOYED: Tue Apr 16 02 :21:08 2022 NAMESPACE: nebula-operator-system STATUS: deployed REVISION: 3 TEST SUITE: None NOTES: Nebula Operator installed! Pull the latest CRD configuration file. Note You need to upgrade the corresponding CRD configurations after Nebula Operator is upgraded. Otherwise, the creation of Nebula Graph clusters will fail. For information about the CRD configurations, see apps.nebula-graph.io_nebulaclusters.yaml . Pull the Nebula Operator chart package. helm pull nebula-operator/nebula-operator --version = 1 .1.0 --version : The Nebula Operator version you want to upgrade to. If not specified, the latest version will be pulled. Run tar -zxvf to unpack the charts. For example: To unpack v1.1.0 chart to the /tmp path, run the following command: tar -zxvf nebula-operator-1.1.0.tgz -C /tmp -C /tmp : If not specified, the chart files will be unpacked to the current directory. Upgrade the CRD configuration file in the nebula-operator directory. kubectl apply -f crds/nebulacluster.yaml Output: customresourcedefinition.apiextensions.k8s.io/nebulaclusters.apps.nebula-graph.io configured Uninstall Nebula Operator \u00b6 Uninstall the Nebula Operator chart. helm uninstall nebula-operator --namespace = <nebula-operator-system> Delete CRD. kubectl delete crd nebulaclusters.apps.nebula-graph.io What's next \u00b6 Automate the deployment of Nebula Graph clusters with Nebula Operator. For more information, see Deploy Nebula Graph Clusters with Kubectl or Deploy Nebula Graph Clusters with Helm .","title":"Deploy Nebula Operator"},{"location":"nebula-operator/2.deploy-nebula-operator/#deploy_nebula_operator","text":"You can deploy Nebula Operator with Helm .","title":"Deploy Nebula Operator"},{"location":"nebula-operator/2.deploy-nebula-operator/#background","text":"Nebula Operator automates the management of Nebula Graph clusters, and eliminates the need for you to install, scale, upgrade, and uninstall Nebula Graph clusters, which lightens the burden on managing different application versions.","title":"Background"},{"location":"nebula-operator/2.deploy-nebula-operator/#prerequisites","text":"","title":"Prerequisites"},{"location":"nebula-operator/2.deploy-nebula-operator/#install_software","text":"Before installing Nebula Operator, you need to install the following software and ensure the correct version of the software: Software Requirement Kubernetes >= 1.16 Helm >= 3.2.0 CoreDNS >= 1.6.0 CertManager >= 1.2.0 OpenKruise >= 0.8.0 If using a role-based access control policy, you need to enable RBAC (optional).","title":"Install software"},{"location":"nebula-operator/2.deploy-nebula-operator/#description_of_software","text":"Note The following software used by Nebula Operator is from the third party. Nebula Operator is not responsible for any problems that may arise during the software installation. CoreDNS CoreDNS is a flexible and scalable DNS server that is installed for Pods in Nebula Graph clusters. Components in a Nebula Graph cluster communicate with each other via DNS resolutions for domain names, like x.default.svc.cluster.local . cert-manager Note If you have set the value of the Nebula Operator configuration item admissionWebhook.create to false , there is no need to install cert-manager. For details about Nebula Operator configuration items, see the Customize Helm charts section in Install Nebula Operator below. cert-manager is a tool that automates the management of certificates. It leverages extensions of the Kubernetes API and uses the Webhook server to provide dynamic access control to cert-manager resources. For more information about installation, see cert-manager installation documentation . cert-manager is used to validate the numeric value of replicas for each component in a Nebula Graph cluster. If you run it in a production environment and care about the high availability of Nebula Graph clusters, it is recommended to set the value of admissionWebhook.create to true before installing cert-manager. OpenKruise OpenKruise is a full set of standard extensions for Kubernetes. It works well with original Kubernetes and provides more powerful and efficient features for managing Pods, sidecar containers, and even container images in clusters. OpenKruise is required to enable advanced features for StatefulSets when Nebula Operator starts. For information about installation, see openkruise installation documentation .","title":"Description of software"},{"location":"nebula-operator/2.deploy-nebula-operator/#steps","text":"","title":"Steps"},{"location":"nebula-operator/2.deploy-nebula-operator/#install_nebula_operator","text":"Add the Nebula Operator chart repository to Helm. helm repo add nebula-operator https://vesoft-inc.github.io/nebula-operator/charts Update information of available charts locally from chart repositories. helm repo update For more information about helm repo , see Helm Repo . Install Nebula Operator. helm install nebula-operator nebula-operator/nebula-operator --namespace = <namespace_name> --version = ${ chart_version } For example, the command to install Nebula Operator of version 1.1.0 is as follows. helm install nebula-operator nebula-operator/nebula-operator --namespace = nebula-operator-system --version = 1 .1.0 nebula-operator-system is a user-created namespace name. If you have not created this namespace, run kubectl create namespace nebula-operator-system to create one. You can also use a different name. 1.1.0 is the version of the Nebula Operator chart. It can be unspecified when there is only one chart version in the Nebula Operator chart repository. Run helm search repo -l nebula-operator to see chart versions. You can customize the configuration items of the Nebula Operator chart before running the installation command. For more information, see Customize Helm charts below.","title":"Install Nebula Operator"},{"location":"nebula-operator/2.deploy-nebula-operator/#customize_helm_charts","text":"Run helm show values [CHART] [flags] to see configurable options. For example: [ k8s@master ~ ] $ helm show values nebula-operator/nebula-operator image : nebulaOperator : image : vesoft/nebula-operator:v1.1.0 imagePullPolicy : Always kubeRBACProxy : image : gcr.io/kubebuilder/kube-rbac-proxy:v0.8.0 imagePullPolicy : Always kubeScheduler : image : k8s.gcr.io/kube-scheduler:v1.18.8 imagePullPolicy : Always imagePullSecrets : [] kubernetesClusterDomain : \"\" controllerManager : create : true replicas : 2 env : [] resources : limits : cpu : 200m memory : 200Mi requests : cpu : 100m memory : 100Mi admissionWebhook : create : true scheduler : create : true schedulerName : nebula-scheduler replicas : 2 env : [] resources : limits : cpu : 200m memory : 20Mi requests : cpu : 100m memory : 100Mi Part of the above parameters are described as follows: Parameter Default value Description image.nebulaOperator.image vesoft/nebula-operator:v1.1.0 The image of Nebula Operator, version of which is 1.1.0. image.nebulaOperator.imagePullPolicy IfNotPresent The image pull policy in Kubernetes. imagePullSecrets - The image pull secret in Kubernetes. kubernetesClusterDomain cluster.local The cluster domain. controllerManager.create true Whether to enable the controller-manager component. controllerManager.replicas 2 The numeric value of controller-manager replicas. admissionWebhook.create true Whether to enable Admission Webhook. shceduler.create true Whether to enable Scheduler. shceduler.schedulerName nebula-scheduler The Scheduler name. shceduler.replicas 2 The numeric value of nebula-scheduler replicas. You can run helm install [NAME] [CHART] [flags] to specify chart configurations when installing a chart. For more information, see Customizing the Chart Before Installing . The following example shows how to specify the Nebula Operator's AdmissionWebhook mechanism to be turned off when you install Nebula Operator (AdmissionWebhook is enabled by default): helm install nebula-operator nebula-operator/nebula-operator --namespace = <nebula-operator-system> --set admissionWebhook.create = false For more information about helm install , see Helm Install .","title":"Customize Helm charts"},{"location":"nebula-operator/2.deploy-nebula-operator/#update_nebula_operator","text":"Update the information of available charts locally from chart repositories. helm repo update Update Nebula Operator by passing configuration parameters via -set or -values flag. --set \uff1aOverrides values using the command line. --values (or -f )\uff1aOverrides values using YAML files. For configurable items, see the above-mentioned section Customize Helm charts . For example, to disable the AdmissionWebhook ( AdmissionWebhook is enabled by default), run the following command: helm upgrade nebula-operator nebula-operator/nebula-operator --namespace = nebula-operator-system --version = 1 .1.0 --set admissionWebhook.create = false For more information, see Helm upgrade .","title":"Update Nebula Operator"},{"location":"nebula-operator/2.deploy-nebula-operator/#upgrade_nebula_operator","text":"Legacy version compatibility The 1.x version Nebula Operator is not compatible with Nebula Graph of version below v3.x. Update the information of available charts locally from chart repositories. helm repo update Upgrade Operator to v1.1.0. helm upgrade nebula-operator nebula-operator/nebula-operator --namespace = <namespace_name> --version = 1 .1.0 For example: helm upgrade nebula-operator nebula-operator/nebula-operator --namespace = nebula-operator-system --version = 1 .1.0 Output: Release \"nebula-operator\" has been upgraded. Happy Helming! NAME: nebula-operator LAST DEPLOYED: Tue Apr 16 02 :21:08 2022 NAMESPACE: nebula-operator-system STATUS: deployed REVISION: 3 TEST SUITE: None NOTES: Nebula Operator installed! Pull the latest CRD configuration file. Note You need to upgrade the corresponding CRD configurations after Nebula Operator is upgraded. Otherwise, the creation of Nebula Graph clusters will fail. For information about the CRD configurations, see apps.nebula-graph.io_nebulaclusters.yaml . Pull the Nebula Operator chart package. helm pull nebula-operator/nebula-operator --version = 1 .1.0 --version : The Nebula Operator version you want to upgrade to. If not specified, the latest version will be pulled. Run tar -zxvf to unpack the charts. For example: To unpack v1.1.0 chart to the /tmp path, run the following command: tar -zxvf nebula-operator-1.1.0.tgz -C /tmp -C /tmp : If not specified, the chart files will be unpacked to the current directory. Upgrade the CRD configuration file in the nebula-operator directory. kubectl apply -f crds/nebulacluster.yaml Output: customresourcedefinition.apiextensions.k8s.io/nebulaclusters.apps.nebula-graph.io configured","title":"Upgrade Nebula Operator"},{"location":"nebula-operator/2.deploy-nebula-operator/#uninstall_nebula_operator","text":"Uninstall the Nebula Operator chart. helm uninstall nebula-operator --namespace = <nebula-operator-system> Delete CRD. kubectl delete crd nebulaclusters.apps.nebula-graph.io","title":"Uninstall Nebula Operator"},{"location":"nebula-operator/2.deploy-nebula-operator/#whats_next","text":"Automate the deployment of Nebula Graph clusters with Nebula Operator. For more information, see Deploy Nebula Graph Clusters with Kubectl or Deploy Nebula Graph Clusters with Helm .","title":"What's next"},{"location":"nebula-operator/4.connect-to-nebula-graph-service/","text":"Connect to Nebula Graph databases with Nebular Operator \u00b6 After creating a Nebula Graph cluster with Nebula Operator on Kubernetes, you can connect to Nebula Graph databases from within the cluster and outside the cluster. Prerequisites \u00b6 Create a Nebula Graph cluster with Nebula Operator on Kubernetes. For more information, see Deploy Nebula Graph clusters with Kubectl or Deploy Nebula Graph clusters with Helm . Connect to Nebula Graph databases from within a Nebula Graph cluster \u00b6 When a Nebula Graph cluster is created, Nebula Operator automatically creates a Service named <cluster-name>-graphd-svc with the type ClusterIP under the same namespace. With the IP of the Service and the port number of the Nebula Graph database, you can connect to the Nebula Graph database. Run the following command to check the IP of the Service: $ kubectl get service -l app.kubernetes.io/cluster = <nebula> #<nebula> is a variable value. Replace it with the desired name. NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE nebula-graphd-svc ClusterIP 10 .98.213.34 <none> 9669 /TCP,19669/TCP,19670/TCP 23h nebula-metad-headless ClusterIP None <none> 9559 /TCP,19559/TCP,19560/TCP 23h nebula-storaged-headless ClusterIP None <none> 9779 /TCP,19779/TCP,19780/TCP,9778/TCP 23h Services of the ClusterIP type only can be accessed by other applications in a cluster. For more information, see ClusterIP . Run the following command to connect to the Nebula Graph database using the IP of the <cluster-name>-graphd-svc Service above: kubectl run -ti --image vesoft/nebula-console:v3.0.0 --restart = Never -- <nebula_console_name> -addr <cluster_ip> -port <service_port> -u <username> -p <password> For example: kubectl run -ti --image vesoft/nebula-console:v3.0.0 --restart = Never -- nebula-console -addr 10 .98.213.34 -port 9669 -u root -p vesoft - ` --image ` : The image for the tool Nebula Console used to connect to Nebula Graph databases. - ` <nebula-console> ` : The custom Pod name. - ` -addr ` : The IP of the ` ClusterIP ` Service, used to connect to Graphd services. - ` -port ` : The port to connect to Graphd services, the default port of which is 9669 . - ` -u ` : The username of your Nebula Graph account. Before enabling authentication, you can use any existing username. The default username is root. - ` -p ` : The password of your Nebula Graph account. Before enabling authentication, you can use any characters as the password. A successful connection to the database is indicated if the following is returned: ``` bash If you don ' t see a command prompt, try pressing enter. ( root@nebula ) [( none )] > You can also connect to Nebula Graph databases with Fully Qualified Domain Name (FQDN) . The domain format is <cluster-name>-graphd.<cluster-namespace>.svc.<CLUSTER_DOMAIN> : kubectl run -ti --image vesoft/nebula-console:v3.0.0 --restart = Never -- <nebula_console_name> -addr <cluster_name>-graphd-svc.default.svc.cluster.local -port <service_port> -u <username> -p <password> The default value of CLUSTER_DOMAIN is cluster.local . Connect to Nebula Graph databases from outside a Nebula Graph cluster via NodePort \u00b6 You can create a Service of type NodePort to connect to Nebula Graph databases from outside a Nebula Graph cluster with a node IP and an exposed node port. You can also use load balancing software provided by cloud providers (such as Azure, AWS, etc.) and set the Service of type LoadBalancer . The Service of type NodePort forwards the front-end requests via the label selector spec.selector to Graphd pods with labels app.kubernetes.io/cluster: <cluster-name> and app.kubernetes.io/component: graphd . Steps: Create a YAML file named graphd-nodeport-service.yaml . The file contents are as follows: apiVersion : v1 kind : Service metadata : labels : app.kubernetes.io/cluster : nebula app.kubernetes.io/component : graphd app.kubernetes.io/managed-by : nebula-operator app.kubernetes.io/name : nebula-graph name : nebula-graphd-svc-nodeport namespace : default spec : externalTrafficPolicy : Local ports : - name : thrift port : 9669 protocol : TCP targetPort : 9669 - name : http port : 19669 protocol : TCP targetPort : 19669 selector : app.kubernetes.io/cluster : nebula app.kubernetes.io/component : graphd app.kubernetes.io/managed-by : nebula-operator app.kubernetes.io/name : nebula-graph type : NodePort Nebula Graph uses port 9669 by default. 19669 is the port of the Graph service in a Nebula Graph cluster. The value of targetPort is the port mapped to the database Pods, which can be customized. Run the following command to create a NodePort Service. kubectl create -f graphd-nodeport-service.yaml Check the port mapped on all of your cluster nodes. kubectl get services Output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE nebula-graphd-svc ClusterIP 10 .98.213.34 <none> 9669 /TCP,19669/TCP,19670/TCP 23h nebula-graphd-svc-nodeport NodePort 10 .107.153.129 <none> 9669 :32236/TCP,19669:31674/TCP,19670:31057/TCP 24h nebula-metad-headless ClusterIP None <none> 9559 /TCP,19559/TCP,19560/TCP 23h nebula-storaged-headless ClusterIP None <none> 9779 /TCP,19779/TCP,19780/TCP,9778/TCP 23h As you see, the mapped port of Nebula Graph databases on all cluster nodes is 32236 . Connect to Nebula Graph databases with your node IP and the node port above. kubectl run -ti --image vesoft/nebula-console:v3.0.0 --restart = Never -- <nebula_console_name> -addr <node_ip> -port <node_port> -u <username> -p <password> For example: kubectl run -ti --image vesoft/nebula-console:v3.0.0 --restart = Never -- nebula-console2 -addr 192 .168.8.24 -port 32236 -u root -p vesoft If you don ' t see a command prompt, try pressing enter. ( root@nebula ) [( none )] > --image : The image for the tool Nebula Console used to connect to Nebula Graph databases. <nebula-console> : The custom Pod name. The above example uses nebula-console2 . -addr : The IP of any node in a Nebula Graph cluster. The above example uses 192.168.8.24 . -port : The mapped port of Nebula Graph databases on all cluster nodes. The above example uses 32236 . -u : The username of your Nebula Graph account. Before enabling authentication, you can use any existing username. The default username is root. -p : The password of your Nebula Graph account. Before enabling authentication, you can use any characters as the password. Connect to Nebula Graph databases from outside a Nebula Graph cluster via Ingress \u00b6 Nginx Ingress is an implementation of Kubernetes Ingress. Nginx Ingress watches the Ingress resource of a Kubernetes cluster and generates the Ingress rules into Nginx configurations that enable Nginx to forward 7 layers of traffic. You can use Nginx Ingress to connect to a Nebula Graph cluster from outside the cluster using a combination of the HostNetwork and DaemonSet pattern. As HostNetwork is used, the Nginx Ingress pod cannot be scheduled to the same node. To avoid listening port conflicts, some nodes can be selected and labeled as edge nodes in advance, which are specially used for the Nginx Ingress deployment. Nginx Ingress is then deployed on these nodes in a DaemonSet mode. Ingress does not support TCP or UDP services. For this reason, the nginx-ingress-controller pod uses the flags --tcp-services-configmap and --udp-services-configmap to point to an existing ConfigMap where the key refers to the external port to be used and the value refers to the format of the service to be exposed. The format of the value is <namespace/service_name>:<service_port> . For example, the configurations of the ConfigMap named as tcp-services is as follows: apiVersion : v1 kind : ConfigMap metadata : name : tcp-services namespace : nginx-ingress data : # update 9769 : \"default/nebula-graphd-svc:9669\" Steps are as follows. Create a file named nginx-ingress-daemonset-hostnetwork.yaml . Click on nginx-ingress-daemonset-hostnetwork.yaml to view the complete content of the example YAML file. Note The resource objects in the YAML file above use the namespace nginx-ingress . You can run kubectl create namespace nginx-ingress to create this namespace, or you can customize the namespace. Label a node where the DaemonSet named nginx-ingress-controller in the above YAML file (The node used in this example is named worker2 with an IP of 192.168.8.160 ) runs. kubectl label node worker2 nginx-ingress = true Run the following command to enable Nginx Ingress in the cluster you created. kubectl create -f nginx-ingress-daemonset-hostnetwork.yaml Output: configmap/nginx-ingress-controller created configmap/tcp-services created serviceaccount/nginx-ingress created serviceaccount/nginx-ingress-backend created clusterrole.rbac.authorization.k8s.io/nginx-ingress created clusterrolebinding.rbac.authorization.k8s.io/nginx-ingress created role.rbac.authorization.k8s.io/nginx-ingress created rolebinding.rbac.authorization.k8s.io/nginx-ingress created service/nginx-ingress-controller-metrics created service/nginx-ingress-default-backend created service/nginx-ingress-proxy-tcp created daemonset.apps/nginx-ingress-controller created Since the network type that is configured in Nginx Ingress is hostNetwork , after successfully deploying Nginx Ingress, with the IP ( 192.168.8.160 ) of the node where Nginx Ingress is deployed and with the external port ( 9769 ) you define, you can access Nebula Graph. Use the IP address and the port configured in the preceding steps. You can connect to Nebula Graph with Nebula Console. kubectl run -ti --image vesoft/nebula-console:v3.0.0 --restart = Never -- <nebula_console_name> -addr <host_ip> -port <external_port> -u <username> -p <password> Output: kubectl run -ti --image vesoft/nebula-console:v3.0.0 --restart = Never -- nebula-console -addr 192 .168.8.160 -port 9769 -u root -p vesoft --image : The image for the tool Nebula Console used to connect to Nebula Graph databases. <nebula-console> The custom Pod name. The above example uses nebula-console . -addr : The IP of the node where Nginx Ingress is deployed. The above example uses 192.168.8.160 . -port : The port used for external network access. The above example uses 9769 . -u : The username of your Nebula Graph account. Before enabling authentication, you can use any existing username. The default username is root. -p : The password of your Nebula Graph account. Before enabling authentication, you can use any characters as the password. A successful connection to the database is indicated if the following is returned: If you don ' t see a command prompt, try pressing enter. ( root@nebula ) [( none )] >","title":"Connect to Nebula Graph databases"},{"location":"nebula-operator/4.connect-to-nebula-graph-service/#connect_to_nebula_graph_databases_with_nebular_operator","text":"After creating a Nebula Graph cluster with Nebula Operator on Kubernetes, you can connect to Nebula Graph databases from within the cluster and outside the cluster.","title":"Connect to Nebula Graph databases with Nebular Operator"},{"location":"nebula-operator/4.connect-to-nebula-graph-service/#prerequisites","text":"Create a Nebula Graph cluster with Nebula Operator on Kubernetes. For more information, see Deploy Nebula Graph clusters with Kubectl or Deploy Nebula Graph clusters with Helm .","title":"Prerequisites"},{"location":"nebula-operator/4.connect-to-nebula-graph-service/#connect_to_nebula_graph_databases_from_within_a_nebula_graph_cluster","text":"When a Nebula Graph cluster is created, Nebula Operator automatically creates a Service named <cluster-name>-graphd-svc with the type ClusterIP under the same namespace. With the IP of the Service and the port number of the Nebula Graph database, you can connect to the Nebula Graph database. Run the following command to check the IP of the Service: $ kubectl get service -l app.kubernetes.io/cluster = <nebula> #<nebula> is a variable value. Replace it with the desired name. NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE nebula-graphd-svc ClusterIP 10 .98.213.34 <none> 9669 /TCP,19669/TCP,19670/TCP 23h nebula-metad-headless ClusterIP None <none> 9559 /TCP,19559/TCP,19560/TCP 23h nebula-storaged-headless ClusterIP None <none> 9779 /TCP,19779/TCP,19780/TCP,9778/TCP 23h Services of the ClusterIP type only can be accessed by other applications in a cluster. For more information, see ClusterIP . Run the following command to connect to the Nebula Graph database using the IP of the <cluster-name>-graphd-svc Service above: kubectl run -ti --image vesoft/nebula-console:v3.0.0 --restart = Never -- <nebula_console_name> -addr <cluster_ip> -port <service_port> -u <username> -p <password> For example: kubectl run -ti --image vesoft/nebula-console:v3.0.0 --restart = Never -- nebula-console -addr 10 .98.213.34 -port 9669 -u root -p vesoft - ` --image ` : The image for the tool Nebula Console used to connect to Nebula Graph databases. - ` <nebula-console> ` : The custom Pod name. - ` -addr ` : The IP of the ` ClusterIP ` Service, used to connect to Graphd services. - ` -port ` : The port to connect to Graphd services, the default port of which is 9669 . - ` -u ` : The username of your Nebula Graph account. Before enabling authentication, you can use any existing username. The default username is root. - ` -p ` : The password of your Nebula Graph account. Before enabling authentication, you can use any characters as the password. A successful connection to the database is indicated if the following is returned: ``` bash If you don ' t see a command prompt, try pressing enter. ( root@nebula ) [( none )] > You can also connect to Nebula Graph databases with Fully Qualified Domain Name (FQDN) . The domain format is <cluster-name>-graphd.<cluster-namespace>.svc.<CLUSTER_DOMAIN> : kubectl run -ti --image vesoft/nebula-console:v3.0.0 --restart = Never -- <nebula_console_name> -addr <cluster_name>-graphd-svc.default.svc.cluster.local -port <service_port> -u <username> -p <password> The default value of CLUSTER_DOMAIN is cluster.local .","title":"Connect to Nebula Graph databases from within a Nebula Graph cluster"},{"location":"nebula-operator/4.connect-to-nebula-graph-service/#connect_to_nebula_graph_databases_from_outside_a_nebula_graph_cluster_via_nodeport","text":"You can create a Service of type NodePort to connect to Nebula Graph databases from outside a Nebula Graph cluster with a node IP and an exposed node port. You can also use load balancing software provided by cloud providers (such as Azure, AWS, etc.) and set the Service of type LoadBalancer . The Service of type NodePort forwards the front-end requests via the label selector spec.selector to Graphd pods with labels app.kubernetes.io/cluster: <cluster-name> and app.kubernetes.io/component: graphd . Steps: Create a YAML file named graphd-nodeport-service.yaml . The file contents are as follows: apiVersion : v1 kind : Service metadata : labels : app.kubernetes.io/cluster : nebula app.kubernetes.io/component : graphd app.kubernetes.io/managed-by : nebula-operator app.kubernetes.io/name : nebula-graph name : nebula-graphd-svc-nodeport namespace : default spec : externalTrafficPolicy : Local ports : - name : thrift port : 9669 protocol : TCP targetPort : 9669 - name : http port : 19669 protocol : TCP targetPort : 19669 selector : app.kubernetes.io/cluster : nebula app.kubernetes.io/component : graphd app.kubernetes.io/managed-by : nebula-operator app.kubernetes.io/name : nebula-graph type : NodePort Nebula Graph uses port 9669 by default. 19669 is the port of the Graph service in a Nebula Graph cluster. The value of targetPort is the port mapped to the database Pods, which can be customized. Run the following command to create a NodePort Service. kubectl create -f graphd-nodeport-service.yaml Check the port mapped on all of your cluster nodes. kubectl get services Output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE nebula-graphd-svc ClusterIP 10 .98.213.34 <none> 9669 /TCP,19669/TCP,19670/TCP 23h nebula-graphd-svc-nodeport NodePort 10 .107.153.129 <none> 9669 :32236/TCP,19669:31674/TCP,19670:31057/TCP 24h nebula-metad-headless ClusterIP None <none> 9559 /TCP,19559/TCP,19560/TCP 23h nebula-storaged-headless ClusterIP None <none> 9779 /TCP,19779/TCP,19780/TCP,9778/TCP 23h As you see, the mapped port of Nebula Graph databases on all cluster nodes is 32236 . Connect to Nebula Graph databases with your node IP and the node port above. kubectl run -ti --image vesoft/nebula-console:v3.0.0 --restart = Never -- <nebula_console_name> -addr <node_ip> -port <node_port> -u <username> -p <password> For example: kubectl run -ti --image vesoft/nebula-console:v3.0.0 --restart = Never -- nebula-console2 -addr 192 .168.8.24 -port 32236 -u root -p vesoft If you don ' t see a command prompt, try pressing enter. ( root@nebula ) [( none )] > --image : The image for the tool Nebula Console used to connect to Nebula Graph databases. <nebula-console> : The custom Pod name. The above example uses nebula-console2 . -addr : The IP of any node in a Nebula Graph cluster. The above example uses 192.168.8.24 . -port : The mapped port of Nebula Graph databases on all cluster nodes. The above example uses 32236 . -u : The username of your Nebula Graph account. Before enabling authentication, you can use any existing username. The default username is root. -p : The password of your Nebula Graph account. Before enabling authentication, you can use any characters as the password.","title":"Connect to Nebula Graph databases from outside a Nebula Graph cluster via NodePort"},{"location":"nebula-operator/4.connect-to-nebula-graph-service/#connect_to_nebula_graph_databases_from_outside_a_nebula_graph_cluster_via_ingress","text":"Nginx Ingress is an implementation of Kubernetes Ingress. Nginx Ingress watches the Ingress resource of a Kubernetes cluster and generates the Ingress rules into Nginx configurations that enable Nginx to forward 7 layers of traffic. You can use Nginx Ingress to connect to a Nebula Graph cluster from outside the cluster using a combination of the HostNetwork and DaemonSet pattern. As HostNetwork is used, the Nginx Ingress pod cannot be scheduled to the same node. To avoid listening port conflicts, some nodes can be selected and labeled as edge nodes in advance, which are specially used for the Nginx Ingress deployment. Nginx Ingress is then deployed on these nodes in a DaemonSet mode. Ingress does not support TCP or UDP services. For this reason, the nginx-ingress-controller pod uses the flags --tcp-services-configmap and --udp-services-configmap to point to an existing ConfigMap where the key refers to the external port to be used and the value refers to the format of the service to be exposed. The format of the value is <namespace/service_name>:<service_port> . For example, the configurations of the ConfigMap named as tcp-services is as follows: apiVersion : v1 kind : ConfigMap metadata : name : tcp-services namespace : nginx-ingress data : # update 9769 : \"default/nebula-graphd-svc:9669\" Steps are as follows. Create a file named nginx-ingress-daemonset-hostnetwork.yaml . Click on nginx-ingress-daemonset-hostnetwork.yaml to view the complete content of the example YAML file. Note The resource objects in the YAML file above use the namespace nginx-ingress . You can run kubectl create namespace nginx-ingress to create this namespace, or you can customize the namespace. Label a node where the DaemonSet named nginx-ingress-controller in the above YAML file (The node used in this example is named worker2 with an IP of 192.168.8.160 ) runs. kubectl label node worker2 nginx-ingress = true Run the following command to enable Nginx Ingress in the cluster you created. kubectl create -f nginx-ingress-daemonset-hostnetwork.yaml Output: configmap/nginx-ingress-controller created configmap/tcp-services created serviceaccount/nginx-ingress created serviceaccount/nginx-ingress-backend created clusterrole.rbac.authorization.k8s.io/nginx-ingress created clusterrolebinding.rbac.authorization.k8s.io/nginx-ingress created role.rbac.authorization.k8s.io/nginx-ingress created rolebinding.rbac.authorization.k8s.io/nginx-ingress created service/nginx-ingress-controller-metrics created service/nginx-ingress-default-backend created service/nginx-ingress-proxy-tcp created daemonset.apps/nginx-ingress-controller created Since the network type that is configured in Nginx Ingress is hostNetwork , after successfully deploying Nginx Ingress, with the IP ( 192.168.8.160 ) of the node where Nginx Ingress is deployed and with the external port ( 9769 ) you define, you can access Nebula Graph. Use the IP address and the port configured in the preceding steps. You can connect to Nebula Graph with Nebula Console. kubectl run -ti --image vesoft/nebula-console:v3.0.0 --restart = Never -- <nebula_console_name> -addr <host_ip> -port <external_port> -u <username> -p <password> Output: kubectl run -ti --image vesoft/nebula-console:v3.0.0 --restart = Never -- nebula-console -addr 192 .168.8.160 -port 9769 -u root -p vesoft --image : The image for the tool Nebula Console used to connect to Nebula Graph databases. <nebula-console> The custom Pod name. The above example uses nebula-console . -addr : The IP of the node where Nginx Ingress is deployed. The above example uses 192.168.8.160 . -port : The port used for external network access. The above example uses 9769 . -u : The username of your Nebula Graph account. Before enabling authentication, you can use any existing username. The default username is root. -p : The password of your Nebula Graph account. Before enabling authentication, you can use any characters as the password. A successful connection to the database is indicated if the following is returned: If you don ' t see a command prompt, try pressing enter. ( root@nebula ) [( none )] >","title":"Connect to Nebula Graph databases from outside a Nebula Graph cluster via Ingress"},{"location":"nebula-operator/5.operator-failover/","text":"Self-healing \u00b6 Nebula Operator calls the interface provided by Nebula Graph clusters to dynamically sense cluster service status. Once an exception is detected (for example, a component in a Nebula Graph cluster stops running), Nebula Operator automatically performs fault tolerance. This topic shows how Nebular Operator performs self-healing by simulating cluster failure of deleting one Storage service Pod in a Nebula Graph cluster. Prerequisites \u00b6 Install Nebula Operator Steps \u00b6 Create a Nebula Graph cluster. For more information, see Deploy Nebula Graph clusters with Kubectl or Deploy Nebula Graph clusters with Helm . Delete the Pod named <cluster_name>-storaged-2 after all pods are in the Running status. kubectl delete pod <cluster-name>-storaged-2 --now <cluster_name> is the name of your Nebula Graph cluster. Nebula Operator automates the creation of the Pod named <cluster-name>-storaged-2 to perform self-healing. Run the kubectl get pods command to check the status of the Pod <cluster-name>-storaged-2 . ... nebula-cluster-storaged-1 1 /1 Running 0 5d23h nebula-cluster-storaged-2 0 /1 ContainerCreating 0 1s ... ... nebula-cluster-storaged-1 1 /1 Running 0 5d23h nebula-cluster-storaged-2 1 /1 Running 0 4m2s ... When the status of <cluster-name>-storaged-2 is changed from ContainerCreating to Running , the self-healing is performed successfully.","title":"Self-healing"},{"location":"nebula-operator/5.operator-failover/#self-healing","text":"Nebula Operator calls the interface provided by Nebula Graph clusters to dynamically sense cluster service status. Once an exception is detected (for example, a component in a Nebula Graph cluster stops running), Nebula Operator automatically performs fault tolerance. This topic shows how Nebular Operator performs self-healing by simulating cluster failure of deleting one Storage service Pod in a Nebula Graph cluster.","title":"Self-healing"},{"location":"nebula-operator/5.operator-failover/#prerequisites","text":"Install Nebula Operator","title":"Prerequisites"},{"location":"nebula-operator/5.operator-failover/#steps","text":"Create a Nebula Graph cluster. For more information, see Deploy Nebula Graph clusters with Kubectl or Deploy Nebula Graph clusters with Helm . Delete the Pod named <cluster_name>-storaged-2 after all pods are in the Running status. kubectl delete pod <cluster-name>-storaged-2 --now <cluster_name> is the name of your Nebula Graph cluster. Nebula Operator automates the creation of the Pod named <cluster-name>-storaged-2 to perform self-healing. Run the kubectl get pods command to check the status of the Pod <cluster-name>-storaged-2 . ... nebula-cluster-storaged-1 1 /1 Running 0 5d23h nebula-cluster-storaged-2 0 /1 ContainerCreating 0 1s ... ... nebula-cluster-storaged-1 1 /1 Running 0 5d23h nebula-cluster-storaged-2 1 /1 Running 0 4m2s ... When the status of <cluster-name>-storaged-2 is changed from ContainerCreating to Running , the self-healing is performed successfully.","title":"Steps"},{"location":"nebula-operator/6.get-started-with-operator/","text":"Overview of using Nebula Operator \u00b6 To use Nebula Operator to connect to Nebula Graph databases, see steps as follows: Install Nebula Operator . Create a Nebula Graph cluster. For more information, see Deploy Nebula Graph clusters with Kubectl or Deploy Nebula Graph clusters with Helm . Connect to a Nebula Graph database .","title":"Overview of using Nebula Operator"},{"location":"nebula-operator/6.get-started-with-operator/#overview_of_using_nebula_operator","text":"To use Nebula Operator to connect to Nebula Graph databases, see steps as follows: Install Nebula Operator . Create a Nebula Graph cluster. For more information, see Deploy Nebula Graph clusters with Kubectl or Deploy Nebula Graph clusters with Helm . Connect to a Nebula Graph database .","title":"Overview of using Nebula Operator"},{"location":"nebula-operator/7.operator-faq/","text":"FAQ \u00b6 Does Nebula Operator support the v1.x version of Nebula Graph? \u00b6 No, because the v1.x version of Nebula Graph does not support DNS, and Nebula Operator requires the use of DNS. Does Nebula Operator support the rolling upgrade feature for Nebula Graph clusters? \u00b6 Nebula Operator currently supports cluster upgrading from version 2.5.x to version 2.6.x. Is cluster stability guaranteed if using local storage? \u00b6 There is no guarantee. Using local storage means that the Pod is bound to a specific node, and Nebula Operator does not currently support failover in the event of a failure of the bound node. How to ensure the stability of a cluster when scaling the cluster? \u00b6 It is suggested to back up data in advance so that you can roll back data in case of failure.","title":"FAQ"},{"location":"nebula-operator/7.operator-faq/#faq","text":"","title":"FAQ"},{"location":"nebula-operator/7.operator-faq/#does_nebula_operator_support_the_v1x_version_of_nebula_graph","text":"No, because the v1.x version of Nebula Graph does not support DNS, and Nebula Operator requires the use of DNS.","title":"Does Nebula Operator support the v1.x version of Nebula Graph?"},{"location":"nebula-operator/7.operator-faq/#does_nebula_operator_support_the_rolling_upgrade_feature_for_nebula_graph_clusters","text":"Nebula Operator currently supports cluster upgrading from version 2.5.x to version 2.6.x.","title":"Does Nebula Operator support the rolling upgrade feature for Nebula Graph clusters?"},{"location":"nebula-operator/7.operator-faq/#is_cluster_stability_guaranteed_if_using_local_storage","text":"There is no guarantee. Using local storage means that the Pod is bound to a specific node, and Nebula Operator does not currently support failover in the event of a failure of the bound node.","title":"Is cluster stability guaranteed if using local storage?"},{"location":"nebula-operator/7.operator-faq/#how_to_ensure_the_stability_of_a_cluster_when_scaling_the_cluster","text":"It is suggested to back up data in advance so that you can roll back data in case of failure.","title":"How to ensure the stability of a cluster when scaling the cluster?"},{"location":"nebula-operator/9.upgrade-nebula-cluster/","text":"Upgrade Nebula Graph clusters created with Nebula Operator \u00b6 This topic introduces how to upgrade a Nebula Graph cluster created with Nebula Operator. Legacy version compatibility The 1.x version Nebula Operator is not compatible with Nebula Graph of version below v3.x. Limits \u00b6 Only for Nebula Graph clusters that have been created with Nebula Operator. Only support upgrading the Nebula Graph version from 3.0.0 to 3.1.x. Upgrade a Nebula Graph cluster with Kubectl \u00b6 Prerequisites \u00b6 You have created a Nebula Graph cluster with Kubectl. For details, see Create a Nebula Graph cluster with Kubectl . The version of the Nebula Graph cluster to be upgraded in this topic is 3.0.0 , and its YAML file name is apps_v1alpha1_nebulacluster.yaml . Steps \u00b6 Check the image version of the services in the cluster. kubectl get pods -l app.kubernetes.io/cluster = nebula -o jsonpath = \"{.items[*].spec.containers[*].image}\" | tr -s '[[:space:]]' '\\n' | sort | uniq -c Output: 1 vesoft/nebula-graphd:3.0.0 1 vesoft/nebula-metad:3.0.0 3 vesoft/nebula-storaged:3.0.0 Edit the apps_v1alpha1_nebulacluster.yaml file by changing the values of all the version parameters from 3.0.0 to v3.1.0. The modified YAML file reads as follows: apiVersion : apps.nebula-graph.io/v1alpha1 kind : NebulaCluster metadata : name : nebula spec : graphd : resources : requests : cpu : \"500m\" memory : \"500Mi\" limits : cpu : \"1\" memory : \"1Gi\" replicas : 1 image : vesoft/nebula-graphd version : v3.1.0 //Change the value from 3.0.0 to v3.1.0. service : type : NodePort externalTrafficPolicy : Local logVolumeClaim : resources : requests : storage : 2Gi storageClassName : gp2 metad : resources : requests : cpu : \"500m\" memory : \"500Mi\" limits : cpu : \"1\" memory : \"1Gi\" replicas : 1 image : vesoft/nebula-metad version : v3.1.0 //Change the value from 3.0.0 to v3.1.0. dataVolumeClaim : resources : requests : storage : 2Gi storageClassName : gp2 logVolumeClaim : resources : requests : storage : 2Gi storageClassName : gp2 storaged : resources : requests : cpu : \"500m\" memory : \"500Mi\" limits : cpu : \"1\" memory : \"1Gi\" replicas : 3 image : vesoft/nebula-storaged version : v3.1.0 //Change the value from 3.0.0 to v3.1.0. dataVolumeClaim : resources : requests : storage : 2Gi storageClassName : gp2 logVolumeClaim : resources : requests : storage : 2Gi storageClassName : gp2 reference : name : statefulsets.apps version : v1 schedulerName : default-scheduler imagePullPolicy : Always Run the following command to apply the version update to the cluster CR. kubectl apply -f apps_v1alpha1_nebulacluster.yaml After waiting for about 2 minutes, run the following command to see if the image versions of the services in the cluster have been changed to v3.1.0. kubectl get pods -l app.kubernetes.io/cluster = nebula -o jsonpath = \"{.items[*].spec.containers[*].image}\" | tr -s '[[:space:]]' '\\n' | sort | uniq -c Output: 1 vesoft/nebula-graphd:v3.1.0 1 vesoft/nebula-metad:v3.1.0 3 vesoft/nebula-storaged:v3.1.0 Upgrade a Nebula Graph cluster with Helm \u00b6 Prerequisites \u00b6 You have created a Nebula Graph cluster with Helm. For details, see Create a Nebula Graph cluster with Helm . Steps \u00b6 Update the information of available charts locally from chart repositories. helm repo update Set environment variables to your desired values. export NEBULA_CLUSTER_NAME = nebula # The desired Nebula Graph cluster name. export NEBULA_CLUSTER_NAMESPACE = nebula # The desired namespace where your Nebula Graph cluster locates. Upgrade a Nebula Graph cluster. For example, upgrade a cluster to v3.1.0. helm upgrade \" ${ NEBULA_CLUSTER_NAME } \" nebula-operator/nebula-cluster \\ --namespace = \" ${ NEBULA_CLUSTER_NAMESPACE } \" \\ --set nameOverride = ${ NEBULA_CLUSTER_NAME } \\ --set nebula.version = v3.1.0 The value of --set nebula.version specifies the version of the cluster you want to upgrade to. Run the following command to check the status and version of the upgraded cluster. Check cluster status: $ kubectl -n \" ${ NEBULA_CLUSTER_NAMESPACE } \" get pod -l \"app.kubernetes.io/cluster= ${ NEBULA_CLUSTER_NAME } \" NAME READY STATUS RESTARTS AGE nebula-graphd-0 1 /1 Running 0 2m nebula-graphd-1 1 /1 Running 0 2m nebula-metad-0 1 /1 Running 0 2m nebula-metad-1 1 /1 Running 0 2m nebula-metad-2 1 /1 Running 0 2m nebula-storaged-0 1 /1 Running 0 2m nebula-storaged-1 1 /1 Running 0 2m nebula-storaged-2 1 /1 Running 0 2m Check cluster version: $ kubectl get pods -l app.kubernetes.io/cluster = nebula -o jsonpath = \"{.items[*].spec.containers[*].image}\" | tr -s '[[:space:]]' '\\n' | sort | uniq -c 1 vesoft/nebula-graphd:v3.1.0 1 vesoft/nebula-metad:v3.1.0 3 vesoft/nebula-storaged:v3.1.0","title":"Upgrade Nebula Graph clusters"},{"location":"nebula-operator/9.upgrade-nebula-cluster/#upgrade_nebula_graph_clusters_created_with_nebula_operator","text":"This topic introduces how to upgrade a Nebula Graph cluster created with Nebula Operator. Legacy version compatibility The 1.x version Nebula Operator is not compatible with Nebula Graph of version below v3.x.","title":"Upgrade Nebula Graph clusters created with Nebula Operator"},{"location":"nebula-operator/9.upgrade-nebula-cluster/#limits","text":"Only for Nebula Graph clusters that have been created with Nebula Operator. Only support upgrading the Nebula Graph version from 3.0.0 to 3.1.x.","title":"Limits"},{"location":"nebula-operator/9.upgrade-nebula-cluster/#upgrade_a_nebula_graph_cluster_with_kubectl","text":"","title":"Upgrade a Nebula Graph cluster with Kubectl"},{"location":"nebula-operator/9.upgrade-nebula-cluster/#prerequisites","text":"You have created a Nebula Graph cluster with Kubectl. For details, see Create a Nebula Graph cluster with Kubectl . The version of the Nebula Graph cluster to be upgraded in this topic is 3.0.0 , and its YAML file name is apps_v1alpha1_nebulacluster.yaml .","title":"Prerequisites"},{"location":"nebula-operator/9.upgrade-nebula-cluster/#steps","text":"Check the image version of the services in the cluster. kubectl get pods -l app.kubernetes.io/cluster = nebula -o jsonpath = \"{.items[*].spec.containers[*].image}\" | tr -s '[[:space:]]' '\\n' | sort | uniq -c Output: 1 vesoft/nebula-graphd:3.0.0 1 vesoft/nebula-metad:3.0.0 3 vesoft/nebula-storaged:3.0.0 Edit the apps_v1alpha1_nebulacluster.yaml file by changing the values of all the version parameters from 3.0.0 to v3.1.0. The modified YAML file reads as follows: apiVersion : apps.nebula-graph.io/v1alpha1 kind : NebulaCluster metadata : name : nebula spec : graphd : resources : requests : cpu : \"500m\" memory : \"500Mi\" limits : cpu : \"1\" memory : \"1Gi\" replicas : 1 image : vesoft/nebula-graphd version : v3.1.0 //Change the value from 3.0.0 to v3.1.0. service : type : NodePort externalTrafficPolicy : Local logVolumeClaim : resources : requests : storage : 2Gi storageClassName : gp2 metad : resources : requests : cpu : \"500m\" memory : \"500Mi\" limits : cpu : \"1\" memory : \"1Gi\" replicas : 1 image : vesoft/nebula-metad version : v3.1.0 //Change the value from 3.0.0 to v3.1.0. dataVolumeClaim : resources : requests : storage : 2Gi storageClassName : gp2 logVolumeClaim : resources : requests : storage : 2Gi storageClassName : gp2 storaged : resources : requests : cpu : \"500m\" memory : \"500Mi\" limits : cpu : \"1\" memory : \"1Gi\" replicas : 3 image : vesoft/nebula-storaged version : v3.1.0 //Change the value from 3.0.0 to v3.1.0. dataVolumeClaim : resources : requests : storage : 2Gi storageClassName : gp2 logVolumeClaim : resources : requests : storage : 2Gi storageClassName : gp2 reference : name : statefulsets.apps version : v1 schedulerName : default-scheduler imagePullPolicy : Always Run the following command to apply the version update to the cluster CR. kubectl apply -f apps_v1alpha1_nebulacluster.yaml After waiting for about 2 minutes, run the following command to see if the image versions of the services in the cluster have been changed to v3.1.0. kubectl get pods -l app.kubernetes.io/cluster = nebula -o jsonpath = \"{.items[*].spec.containers[*].image}\" | tr -s '[[:space:]]' '\\n' | sort | uniq -c Output: 1 vesoft/nebula-graphd:v3.1.0 1 vesoft/nebula-metad:v3.1.0 3 vesoft/nebula-storaged:v3.1.0","title":"Steps"},{"location":"nebula-operator/9.upgrade-nebula-cluster/#upgrade_a_nebula_graph_cluster_with_helm","text":"","title":"Upgrade a Nebula Graph cluster with Helm"},{"location":"nebula-operator/9.upgrade-nebula-cluster/#prerequisites_1","text":"You have created a Nebula Graph cluster with Helm. For details, see Create a Nebula Graph cluster with Helm .","title":"Prerequisites"},{"location":"nebula-operator/9.upgrade-nebula-cluster/#steps_1","text":"Update the information of available charts locally from chart repositories. helm repo update Set environment variables to your desired values. export NEBULA_CLUSTER_NAME = nebula # The desired Nebula Graph cluster name. export NEBULA_CLUSTER_NAMESPACE = nebula # The desired namespace where your Nebula Graph cluster locates. Upgrade a Nebula Graph cluster. For example, upgrade a cluster to v3.1.0. helm upgrade \" ${ NEBULA_CLUSTER_NAME } \" nebula-operator/nebula-cluster \\ --namespace = \" ${ NEBULA_CLUSTER_NAMESPACE } \" \\ --set nameOverride = ${ NEBULA_CLUSTER_NAME } \\ --set nebula.version = v3.1.0 The value of --set nebula.version specifies the version of the cluster you want to upgrade to. Run the following command to check the status and version of the upgraded cluster. Check cluster status: $ kubectl -n \" ${ NEBULA_CLUSTER_NAMESPACE } \" get pod -l \"app.kubernetes.io/cluster= ${ NEBULA_CLUSTER_NAME } \" NAME READY STATUS RESTARTS AGE nebula-graphd-0 1 /1 Running 0 2m nebula-graphd-1 1 /1 Running 0 2m nebula-metad-0 1 /1 Running 0 2m nebula-metad-1 1 /1 Running 0 2m nebula-metad-2 1 /1 Running 0 2m nebula-storaged-0 1 /1 Running 0 2m nebula-storaged-1 1 /1 Running 0 2m nebula-storaged-2 1 /1 Running 0 2m Check cluster version: $ kubectl get pods -l app.kubernetes.io/cluster = nebula -o jsonpath = \"{.items[*].spec.containers[*].image}\" | tr -s '[[:space:]]' '\\n' | sort | uniq -c 1 vesoft/nebula-graphd:v3.1.0 1 vesoft/nebula-metad:v3.1.0 3 vesoft/nebula-storaged:v3.1.0","title":"Steps"},{"location":"nebula-operator/3.deploy-nebula-graph-cluster/3.1create-cluster-with-kubectl/","text":"Deploy Nebula Graph clusters with Kubectl \u00b6 Legacy version compatibility The 1.x version Nebula Operator is not compatible with Nebula Graph of version below v3.x. Prerequisites \u00b6 Install Nebula Operator Create clusters \u00b6 The following example shows how to create a Nebula Graph cluster by creating a cluster named nebula . Create a file named apps_v1alpha1_nebulacluster.yaml . The file contents are as follows: apiVersion : apps.nebula-graph.io/v1alpha1 kind : NebulaCluster metadata : name : nebula spec : graphd : resources : requests : cpu : \"500m\" memory : \"500Mi\" limits : cpu : \"1\" memory : \"1Gi\" replicas : 1 image : vesoft/nebula-graphd version : v3.1.0 service : type : NodePort externalTrafficPolicy : Local logVolumeClaim : resources : requests : storage : 2Gi storageClassName : gp2 metad : resources : requests : cpu : \"500m\" memory : \"500Mi\" limits : cpu : \"1\" memory : \"1Gi\" replicas : 1 image : vesoft/nebula-metad version : v3.1.0 dataVolumeClaim : resources : requests : storage : 2Gi storageClassName : gp2 logVolumeClaim : resources : requests : storage : 2Gi storageClassName : gp2 storaged : resources : requests : cpu : \"500m\" memory : \"500Mi\" limits : cpu : \"1\" memory : \"1Gi\" replicas : 3 image : vesoft/nebula-storaged version : v3.1.0 dataVolumeClaim : resources : requests : storage : 2Gi storageClassName : gp2 logVolumeClaim : resources : requests : storage : 2Gi storageClassName : gp2 reference : name : statefulsets.apps version : v1 schedulerName : default-scheduler imagePullPolicy : Always The parameters in the file are described as follows: Parameter Default value Description metadata.name - The name of the created Nebula Graph cluster. spec.graphd.replicas 1 The numeric value of replicas of the Graphd service. spec.graphd.images vesoft/nebula-graphd The container image of the Graphd service. spec.graphd.version v3.1.0 The version of the Graphd service. spec.graphd.service - The Service configurations for the Graphd service. spec.graphd.logVolumeClaim.storageClassName - The log disk storage configurations for the Graphd service. spec.metad.replicas 1 The numeric value of replicas of the Metad service. spec.metad.images vesoft/nebula-metad The container image of the Metad service. spec.metad.version v3.1.0 The version of the Metad service. spec.metad.dataVolumeClaim.storageClassName - The data disk storage configurations for the Metad service. spec.metad.logVolumeClaim.storageClassName - The log disk storage configurations for the Metad service. spec.storaged.replicas 3 The numeric value of replicas of the Storaged service. spec.storaged.images vesoft/nebula-storaged The container image of the Storaged service. spec.storaged.version v3.1.0 The version of the Storaged service. spec.storaged.dataVolumeClaim.storageClassName - The data disk storage configurations for the Storaged service. spec.storaged.logVolumeClaim.storageClassName - The log disk storage configurations for the Storaged service. spec.reference.name - The name of the dependent controller. spec.schedulerName - The scheduler name. spec.imagePullPolicy The image policy to pull the Nebula Graph image. For details, see Image pull policy . The image pull policy in Kubernetes. Create a Nebula Graph cluster. kubectl create -f apps_v1alpha1_nebulacluster.yaml Output: nebulacluster.apps.nebula-graph.io/nebula created Check the status of the Nebula Graph cluster. kubectl get nebulaclusters.apps.nebula-graph.io nebula Output: NAME GRAPHD-DESIRED GRAPHD-READY METAD-DESIRED METAD-READY STORAGED-DESIRED STORAGED-READY AGE nebula 1 1 1 1 3 3 86s Scaling clusters \u00b6 Enterpriseonly The cluster scaling feature is for Nebula Graph Enterprise Edition only. Scaling a Nebula Graph cluster for Enterprise Edition is supported only with Nebula Operator version 1.1.0 or later. You can modify the value of replicas in apps_v1alpha1_nebulacluster.yaml to scale a Nebula Graph cluster. Scale out clusters \u00b6 The following shows how to scale out a Nebula Graph cluster by changing the number of Storage services to 5: Change the value of the storaged.replicas from 3 to 5 in apps_v1alpha1_nebulacluster.yaml . storaged : resources : requests : cpu : \"500m\" memory : \"500Mi\" limits : cpu : \"1\" memory : \"1Gi\" replicas : 5 image : vesoft/nebula-storaged version : v3.1.0 dataVolumeClaim : resources : requests : storage : 2Gi storageClassName : gp2 logVolumeClaim : resources : requests : storage : 2Gi storageClassName : gp2 reference : name : statefulsets.apps version : v1 schedulerName : default-scheduler Run the following command to update the Nebula Graph cluster CR. kubectl apply -f apps_v1alpha1_nebulacluster.yaml Check the number of Storage services. kubectl get pods -l app.kubernetes.io/cluster = nebula Output: NAME READY STATUS RESTARTS AGE nebula-graphd-0 1 /1 Running 0 2m nebula-metad-0 1 /1 Running 0 2m nebula-storaged-0 1 /1 Running 0 2m nebula-storaged-1 1 /1 Running 0 2m nebula-storaged-2 1 /1 Running 0 2m nebula-storaged-3 1 /1 Running 0 5m nebula-storaged-4 1 /1 Running 0 5m As you can see above, the number of Storage services is scaled up to 5. Scale in clusters \u00b6 The principle of scaling in a cluster is the same as scaling out a cluster. You scale in a cluster if the numeric value of the replicas in apps_v1alpha1_nebulacluster.yaml is changed smaller than the current number. For more information, see the Scale out clusters section above. Caution Nebula Operator currently only supports scaling Graph and Storage services and does not support scale Meta services. Delete clusters \u00b6 Run the following command to delete a Nebula Graph cluster with Kubectl: kubectl delete -f apps_v1alpha1_nebulacluster.yaml What's next \u00b6 Connect to Nebula Graph databases","title":"Deploy clusters with Kubectl"},{"location":"nebula-operator/3.deploy-nebula-graph-cluster/3.1create-cluster-with-kubectl/#deploy_nebula_graph_clusters_with_kubectl","text":"Legacy version compatibility The 1.x version Nebula Operator is not compatible with Nebula Graph of version below v3.x.","title":"Deploy Nebula Graph clusters with Kubectl"},{"location":"nebula-operator/3.deploy-nebula-graph-cluster/3.1create-cluster-with-kubectl/#prerequisites","text":"Install Nebula Operator","title":"Prerequisites"},{"location":"nebula-operator/3.deploy-nebula-graph-cluster/3.1create-cluster-with-kubectl/#create_clusters","text":"The following example shows how to create a Nebula Graph cluster by creating a cluster named nebula . Create a file named apps_v1alpha1_nebulacluster.yaml . The file contents are as follows: apiVersion : apps.nebula-graph.io/v1alpha1 kind : NebulaCluster metadata : name : nebula spec : graphd : resources : requests : cpu : \"500m\" memory : \"500Mi\" limits : cpu : \"1\" memory : \"1Gi\" replicas : 1 image : vesoft/nebula-graphd version : v3.1.0 service : type : NodePort externalTrafficPolicy : Local logVolumeClaim : resources : requests : storage : 2Gi storageClassName : gp2 metad : resources : requests : cpu : \"500m\" memory : \"500Mi\" limits : cpu : \"1\" memory : \"1Gi\" replicas : 1 image : vesoft/nebula-metad version : v3.1.0 dataVolumeClaim : resources : requests : storage : 2Gi storageClassName : gp2 logVolumeClaim : resources : requests : storage : 2Gi storageClassName : gp2 storaged : resources : requests : cpu : \"500m\" memory : \"500Mi\" limits : cpu : \"1\" memory : \"1Gi\" replicas : 3 image : vesoft/nebula-storaged version : v3.1.0 dataVolumeClaim : resources : requests : storage : 2Gi storageClassName : gp2 logVolumeClaim : resources : requests : storage : 2Gi storageClassName : gp2 reference : name : statefulsets.apps version : v1 schedulerName : default-scheduler imagePullPolicy : Always The parameters in the file are described as follows: Parameter Default value Description metadata.name - The name of the created Nebula Graph cluster. spec.graphd.replicas 1 The numeric value of replicas of the Graphd service. spec.graphd.images vesoft/nebula-graphd The container image of the Graphd service. spec.graphd.version v3.1.0 The version of the Graphd service. spec.graphd.service - The Service configurations for the Graphd service. spec.graphd.logVolumeClaim.storageClassName - The log disk storage configurations for the Graphd service. spec.metad.replicas 1 The numeric value of replicas of the Metad service. spec.metad.images vesoft/nebula-metad The container image of the Metad service. spec.metad.version v3.1.0 The version of the Metad service. spec.metad.dataVolumeClaim.storageClassName - The data disk storage configurations for the Metad service. spec.metad.logVolumeClaim.storageClassName - The log disk storage configurations for the Metad service. spec.storaged.replicas 3 The numeric value of replicas of the Storaged service. spec.storaged.images vesoft/nebula-storaged The container image of the Storaged service. spec.storaged.version v3.1.0 The version of the Storaged service. spec.storaged.dataVolumeClaim.storageClassName - The data disk storage configurations for the Storaged service. spec.storaged.logVolumeClaim.storageClassName - The log disk storage configurations for the Storaged service. spec.reference.name - The name of the dependent controller. spec.schedulerName - The scheduler name. spec.imagePullPolicy The image policy to pull the Nebula Graph image. For details, see Image pull policy . The image pull policy in Kubernetes. Create a Nebula Graph cluster. kubectl create -f apps_v1alpha1_nebulacluster.yaml Output: nebulacluster.apps.nebula-graph.io/nebula created Check the status of the Nebula Graph cluster. kubectl get nebulaclusters.apps.nebula-graph.io nebula Output: NAME GRAPHD-DESIRED GRAPHD-READY METAD-DESIRED METAD-READY STORAGED-DESIRED STORAGED-READY AGE nebula 1 1 1 1 3 3 86s","title":"Create clusters"},{"location":"nebula-operator/3.deploy-nebula-graph-cluster/3.1create-cluster-with-kubectl/#scaling_clusters","text":"Enterpriseonly The cluster scaling feature is for Nebula Graph Enterprise Edition only. Scaling a Nebula Graph cluster for Enterprise Edition is supported only with Nebula Operator version 1.1.0 or later. You can modify the value of replicas in apps_v1alpha1_nebulacluster.yaml to scale a Nebula Graph cluster.","title":"Scaling clusters"},{"location":"nebula-operator/3.deploy-nebula-graph-cluster/3.1create-cluster-with-kubectl/#scale_out_clusters","text":"The following shows how to scale out a Nebula Graph cluster by changing the number of Storage services to 5: Change the value of the storaged.replicas from 3 to 5 in apps_v1alpha1_nebulacluster.yaml . storaged : resources : requests : cpu : \"500m\" memory : \"500Mi\" limits : cpu : \"1\" memory : \"1Gi\" replicas : 5 image : vesoft/nebula-storaged version : v3.1.0 dataVolumeClaim : resources : requests : storage : 2Gi storageClassName : gp2 logVolumeClaim : resources : requests : storage : 2Gi storageClassName : gp2 reference : name : statefulsets.apps version : v1 schedulerName : default-scheduler Run the following command to update the Nebula Graph cluster CR. kubectl apply -f apps_v1alpha1_nebulacluster.yaml Check the number of Storage services. kubectl get pods -l app.kubernetes.io/cluster = nebula Output: NAME READY STATUS RESTARTS AGE nebula-graphd-0 1 /1 Running 0 2m nebula-metad-0 1 /1 Running 0 2m nebula-storaged-0 1 /1 Running 0 2m nebula-storaged-1 1 /1 Running 0 2m nebula-storaged-2 1 /1 Running 0 2m nebula-storaged-3 1 /1 Running 0 5m nebula-storaged-4 1 /1 Running 0 5m As you can see above, the number of Storage services is scaled up to 5.","title":"Scale out clusters"},{"location":"nebula-operator/3.deploy-nebula-graph-cluster/3.1create-cluster-with-kubectl/#scale_in_clusters","text":"The principle of scaling in a cluster is the same as scaling out a cluster. You scale in a cluster if the numeric value of the replicas in apps_v1alpha1_nebulacluster.yaml is changed smaller than the current number. For more information, see the Scale out clusters section above. Caution Nebula Operator currently only supports scaling Graph and Storage services and does not support scale Meta services.","title":"Scale in clusters"},{"location":"nebula-operator/3.deploy-nebula-graph-cluster/3.1create-cluster-with-kubectl/#delete_clusters","text":"Run the following command to delete a Nebula Graph cluster with Kubectl: kubectl delete -f apps_v1alpha1_nebulacluster.yaml","title":"Delete clusters"},{"location":"nebula-operator/3.deploy-nebula-graph-cluster/3.1create-cluster-with-kubectl/#whats_next","text":"Connect to Nebula Graph databases","title":"What's next"},{"location":"nebula-operator/3.deploy-nebula-graph-cluster/3.2create-cluster-with-helm/","text":"Deploy Nebula Graph clusters with Helm \u00b6 Legacy version compatibility The 1.x version Nebula Operator is not compatible with Nebula Graph of version below v3.x. Prerequisite \u00b6 Install Nebula Operator Create clusters \u00b6 Add the Nebula Operator chart repository to Helm\uff08If you have already added the chart, skip the 1-2 steps and start from step 3). helm repo add nebula-operator https://vesoft-inc.github.io/nebula-operator/charts Update information of available charts locally from chart repositories. helm repo update Set environment variables to your desired values. export NEBULA_CLUSTER_NAME = nebula # The desired Nebula Graph cluster name. export NEBULA_CLUSTER_NAMESPACE = nebula # The desired namespace where your Nebula Graph cluster locates. export STORAGE_CLASS_NAME = gp2 # The desired StorageClass name in your Nebula Graph cluster. Create a namespace for your Nebula Graph cluster\uff08If you have created one, skip this step\uff09. kubectl create namespace \" ${ NEBULA_CLUSTER_NAMESPACE } \" Apply the variables to the Helm chart to create a Nebula Graph cluster. helm install \" ${ NEBULA_CLUSTER_NAME } \" nebula-operator/nebula-cluster \\ --namespace = \" ${ NEBULA_CLUSTER_NAMESPACE } \" \\ --set nameOverride = ${ NEBULA_CLUSTER_NAME } \\ --set nebula.storageClassName = \" ${ STORAGE_CLASS_NAME } \" Check the status of the Nebula Graph cluster you created. kubectl -n \" ${ NEBULA_CLUSTER_NAMESPACE } \" get pod -l \"app.kubernetes.io/cluster= ${ NEBULA_CLUSTER_NAME } \" Output: NAME READY STATUS RESTARTS AGE nebula-graphd-0 1 /1 Running 0 5m34s nebula-graphd-1 1 /1 Running 0 5m34s nebula-metad-0 1 /1 Running 0 5m34s nebula-metad-1 1 /1 Running 0 5m34s nebula-metad-2 1 /1 Running 0 5m34s nebula-storaged-0 1 /1 Running 0 5m34s nebula-storaged-1 1 /1 Running 0 5m34s nebula-storaged-2 1 /1 Running 0 5m34s Scaling clusters \u00b6 Enterpriseonly The cluster scaling feature is for Nebula Graph Enterprise Edition only. Scaling a Nebula Graph cluster for Enterprise Edition is supported only with Nebula Operator version 1.1.0 or later. You can scale a Nebula Graph cluster by defining the value of the replicas corresponding to the different services in the cluster. For example, run the following command to scale out a Nebula Graph cluster by changing the number of Storage services from 2 (the original value) to 5: helm upgrade \" ${ NEBULA_CLUSTER_NAME } \" nebula-operator/nebula-cluster \\ --namespace = \" ${ NEBULA_CLUSTER_NAMESPACE } \" \\ --set nameOverride = ${ NEBULA_CLUSTER_NAME } \\ --set nebula.storageClassName = \" ${ STORAGE_CLASS_NAME } \" \\ --set nebula.storaged.replicas = 5 Similarly, you can scale in a Nebula Graph cluster by setting the value of the replicas corresponding to the different services in the cluster smaller than the original value. Caution Nebula Operator currently only supports scaling Graph and Storage services and does not support scale Meta services. You can click on nebula-cluster/values.yaml to see more configurable parameters of the nebula-cluster chart. For more information about the descriptions of configurable parameters, see Configuration parameters of the nebula-cluster Helm chart below. Delete clusters \u00b6 Run the following command to delete a Nebula Graph cluster with Helm: helm uninstall \" ${ NEBULA_CLUSTER_NAME } \" --namespace = \" ${ NEBULA_CLUSTER_NAMESPACE } \" What's next \u00b6 Connect to Nebula Graph Databases Configuration parameters of the nebula-cluster Helm chart \u00b6 Parameter Default value Description nameOverride nil Replaces the name of the chart in the Chart.yaml file. nebula.version v3.1.0 The version of Nebula Graph. nebula.imagePullPolicy IfNotPresent The Nebula Graph image pull policy. For details, see Image pull policy . nebula.storageClassName nil The StorageClass name. StorageClass is the default persistent volume type. nebula.schedulerName default-scheduler The scheduler name of a Nebula Graph cluster. nebula.reference {\"name\": \"statefulsets.apps\", \"version\": \"v1\"} The workload referenced for a Nebula Graph cluster. nebula.graphd.image vesoft/nebula-graphd The image name for a Graphd service. Uses the value of nebula.version as its version. nebula.graphd.replicas 2 The number of the Graphd service. nebula.graphd.env [] The environment variables for the Graphd service. nebula.graphd.resources {\"resources\":{\"requests\":{\"cpu\":\"500m\",\"memory\":\"500Mi\"},\"limits\":{\"cpu\":\"1\",\"memory\":\"1Gi\"}}} The resource configurations for the Graphd service. nebula.graphd.logStorage 500Mi The log disk storage capacity for the Graphd service. nebula.graphd.podLabels {} Labels for the Graphd pod in a Nebula Graph cluster. nebula.graphd.podAnnotations {} Pod annotations for the Graphd pod in a Nebula Graph cluster. nebula.graphd.nodeSelector {} Labels for the Graphd pod to be scheduled to the specified node. nebula.graphd.tolerations {} Tolerations for the Graphd pod. nebula.graphd.affinity {} Affinity for the Graphd pod. nebula.graphd.readinessProbe {} ReadinessProbe for the Graphd pod. nebula.graphd.sidecarContainers {} Sidecar containers for the Graphd pod. nebula.graphd.sidecarVolumes {} Sidecar volumes for the Graphd pod. nebula.metad.image vesoft/nebula-metad The image name for a Metad service. Uses the value of nebula.version as its version. nebula.metad.replicas 3 The number of the Metad service. nebula.metad.env [] The environment variables for the Metad service. nebula.metad.resources {\"resources\":{\"requests\":{\"cpu\":\"500m\",\"memory\":\"500Mi\"},\"limits\":{\"cpu\":\"1\",\"memory\":\"1Gi\"}}} The resource configurations for the Metad service. nebula.metad.logStorage 500Mi The log disk capacity for the Metad service. nebula.metad.dataStorage 1Gi The data disk capacity for the Metad service. nebula.metad.podLabels {} Labels for the Metad pod in a Nebula Graph cluster. nebula.metad.podAnnotations {} Pod annotations for the Metad pod in a Nebula Graph cluster. nebula.metad.nodeSelector {} Labels for the Metad pod to be scheduled to the specified node. nebula.metad.tolerations {} Tolerations for the Metad pod. nebula.metad.affinity {} Affinity for the Metad pod. nebula.metad.readinessProbe {} ReadinessProbe for the Metad pod. nebula.metad.sidecarContainers {} Sidecar containers for the Metad pod. nebula.metad.sidecarVolumes {} Sidecar volumes for the Metad pod. nebula.storaged.image vesoft/nebula-storaged The image name for a Storaged service. Uses the value of nebula.version as its version. nebula.storaged.replicas 3 The number of Storaged services. nebula.storaged.env [] The environment variables for Storaged services. nebula.storaged.resources {\"resources\":{\"requests\":{\"cpu\":\"500m\",\"memory\":\"500Mi\"},\"limits\":{\"cpu\":\"1\",\"memory\":\"1Gi\"}}} The resource configurations for Storagedss services. nebula.storaged.logStorage 500Mi The log disk capacity for the Metad service. nebula.storaged.dataStorage 1Gi The data disk capacity for the Metad service. nebula.storaged.podLabels {} Labels for the Metad pod in a Nebula Graph cluster. nebula.storaged.podAnnotations {} Pod annotations for the Metad pod in a Nebula Graph cluster. nebula.storaged.nodeSelector {} Labels for the Metad pod to be scheduled to the specified node. nebula.storaged.tolerations {} Tolerations for the Metad pod. nebula.storaged.affinity {} Affinity for the Metad pod. nebula.storaged.readinessProbe {} ReadinessProbe for the Metad pod. nebula.storaged.sidecarContainers {} Sidecar containers for the Metad pod. nebula.storaged.sidecarVolumes {} Sidecar volumes for the Metad pod. imagePullSecrets [] The Secret to pull the Nebula Graph cluster image.","title":"Deploy clusters with Helm"},{"location":"nebula-operator/3.deploy-nebula-graph-cluster/3.2create-cluster-with-helm/#deploy_nebula_graph_clusters_with_helm","text":"Legacy version compatibility The 1.x version Nebula Operator is not compatible with Nebula Graph of version below v3.x.","title":"Deploy Nebula Graph clusters with Helm"},{"location":"nebula-operator/3.deploy-nebula-graph-cluster/3.2create-cluster-with-helm/#prerequisite","text":"Install Nebula Operator","title":"Prerequisite"},{"location":"nebula-operator/3.deploy-nebula-graph-cluster/3.2create-cluster-with-helm/#create_clusters","text":"Add the Nebula Operator chart repository to Helm\uff08If you have already added the chart, skip the 1-2 steps and start from step 3). helm repo add nebula-operator https://vesoft-inc.github.io/nebula-operator/charts Update information of available charts locally from chart repositories. helm repo update Set environment variables to your desired values. export NEBULA_CLUSTER_NAME = nebula # The desired Nebula Graph cluster name. export NEBULA_CLUSTER_NAMESPACE = nebula # The desired namespace where your Nebula Graph cluster locates. export STORAGE_CLASS_NAME = gp2 # The desired StorageClass name in your Nebula Graph cluster. Create a namespace for your Nebula Graph cluster\uff08If you have created one, skip this step\uff09. kubectl create namespace \" ${ NEBULA_CLUSTER_NAMESPACE } \" Apply the variables to the Helm chart to create a Nebula Graph cluster. helm install \" ${ NEBULA_CLUSTER_NAME } \" nebula-operator/nebula-cluster \\ --namespace = \" ${ NEBULA_CLUSTER_NAMESPACE } \" \\ --set nameOverride = ${ NEBULA_CLUSTER_NAME } \\ --set nebula.storageClassName = \" ${ STORAGE_CLASS_NAME } \" Check the status of the Nebula Graph cluster you created. kubectl -n \" ${ NEBULA_CLUSTER_NAMESPACE } \" get pod -l \"app.kubernetes.io/cluster= ${ NEBULA_CLUSTER_NAME } \" Output: NAME READY STATUS RESTARTS AGE nebula-graphd-0 1 /1 Running 0 5m34s nebula-graphd-1 1 /1 Running 0 5m34s nebula-metad-0 1 /1 Running 0 5m34s nebula-metad-1 1 /1 Running 0 5m34s nebula-metad-2 1 /1 Running 0 5m34s nebula-storaged-0 1 /1 Running 0 5m34s nebula-storaged-1 1 /1 Running 0 5m34s nebula-storaged-2 1 /1 Running 0 5m34s","title":"Create clusters"},{"location":"nebula-operator/3.deploy-nebula-graph-cluster/3.2create-cluster-with-helm/#scaling_clusters","text":"Enterpriseonly The cluster scaling feature is for Nebula Graph Enterprise Edition only. Scaling a Nebula Graph cluster for Enterprise Edition is supported only with Nebula Operator version 1.1.0 or later. You can scale a Nebula Graph cluster by defining the value of the replicas corresponding to the different services in the cluster. For example, run the following command to scale out a Nebula Graph cluster by changing the number of Storage services from 2 (the original value) to 5: helm upgrade \" ${ NEBULA_CLUSTER_NAME } \" nebula-operator/nebula-cluster \\ --namespace = \" ${ NEBULA_CLUSTER_NAMESPACE } \" \\ --set nameOverride = ${ NEBULA_CLUSTER_NAME } \\ --set nebula.storageClassName = \" ${ STORAGE_CLASS_NAME } \" \\ --set nebula.storaged.replicas = 5 Similarly, you can scale in a Nebula Graph cluster by setting the value of the replicas corresponding to the different services in the cluster smaller than the original value. Caution Nebula Operator currently only supports scaling Graph and Storage services and does not support scale Meta services. You can click on nebula-cluster/values.yaml to see more configurable parameters of the nebula-cluster chart. For more information about the descriptions of configurable parameters, see Configuration parameters of the nebula-cluster Helm chart below.","title":"Scaling clusters"},{"location":"nebula-operator/3.deploy-nebula-graph-cluster/3.2create-cluster-with-helm/#delete_clusters","text":"Run the following command to delete a Nebula Graph cluster with Helm: helm uninstall \" ${ NEBULA_CLUSTER_NAME } \" --namespace = \" ${ NEBULA_CLUSTER_NAMESPACE } \"","title":"Delete clusters"},{"location":"nebula-operator/3.deploy-nebula-graph-cluster/3.2create-cluster-with-helm/#whats_next","text":"Connect to Nebula Graph Databases","title":"What's next"},{"location":"nebula-operator/3.deploy-nebula-graph-cluster/3.2create-cluster-with-helm/#configuration_parameters_of_the_nebula-cluster_helm_chart","text":"Parameter Default value Description nameOverride nil Replaces the name of the chart in the Chart.yaml file. nebula.version v3.1.0 The version of Nebula Graph. nebula.imagePullPolicy IfNotPresent The Nebula Graph image pull policy. For details, see Image pull policy . nebula.storageClassName nil The StorageClass name. StorageClass is the default persistent volume type. nebula.schedulerName default-scheduler The scheduler name of a Nebula Graph cluster. nebula.reference {\"name\": \"statefulsets.apps\", \"version\": \"v1\"} The workload referenced for a Nebula Graph cluster. nebula.graphd.image vesoft/nebula-graphd The image name for a Graphd service. Uses the value of nebula.version as its version. nebula.graphd.replicas 2 The number of the Graphd service. nebula.graphd.env [] The environment variables for the Graphd service. nebula.graphd.resources {\"resources\":{\"requests\":{\"cpu\":\"500m\",\"memory\":\"500Mi\"},\"limits\":{\"cpu\":\"1\",\"memory\":\"1Gi\"}}} The resource configurations for the Graphd service. nebula.graphd.logStorage 500Mi The log disk storage capacity for the Graphd service. nebula.graphd.podLabels {} Labels for the Graphd pod in a Nebula Graph cluster. nebula.graphd.podAnnotations {} Pod annotations for the Graphd pod in a Nebula Graph cluster. nebula.graphd.nodeSelector {} Labels for the Graphd pod to be scheduled to the specified node. nebula.graphd.tolerations {} Tolerations for the Graphd pod. nebula.graphd.affinity {} Affinity for the Graphd pod. nebula.graphd.readinessProbe {} ReadinessProbe for the Graphd pod. nebula.graphd.sidecarContainers {} Sidecar containers for the Graphd pod. nebula.graphd.sidecarVolumes {} Sidecar volumes for the Graphd pod. nebula.metad.image vesoft/nebula-metad The image name for a Metad service. Uses the value of nebula.version as its version. nebula.metad.replicas 3 The number of the Metad service. nebula.metad.env [] The environment variables for the Metad service. nebula.metad.resources {\"resources\":{\"requests\":{\"cpu\":\"500m\",\"memory\":\"500Mi\"},\"limits\":{\"cpu\":\"1\",\"memory\":\"1Gi\"}}} The resource configurations for the Metad service. nebula.metad.logStorage 500Mi The log disk capacity for the Metad service. nebula.metad.dataStorage 1Gi The data disk capacity for the Metad service. nebula.metad.podLabels {} Labels for the Metad pod in a Nebula Graph cluster. nebula.metad.podAnnotations {} Pod annotations for the Metad pod in a Nebula Graph cluster. nebula.metad.nodeSelector {} Labels for the Metad pod to be scheduled to the specified node. nebula.metad.tolerations {} Tolerations for the Metad pod. nebula.metad.affinity {} Affinity for the Metad pod. nebula.metad.readinessProbe {} ReadinessProbe for the Metad pod. nebula.metad.sidecarContainers {} Sidecar containers for the Metad pod. nebula.metad.sidecarVolumes {} Sidecar volumes for the Metad pod. nebula.storaged.image vesoft/nebula-storaged The image name for a Storaged service. Uses the value of nebula.version as its version. nebula.storaged.replicas 3 The number of Storaged services. nebula.storaged.env [] The environment variables for Storaged services. nebula.storaged.resources {\"resources\":{\"requests\":{\"cpu\":\"500m\",\"memory\":\"500Mi\"},\"limits\":{\"cpu\":\"1\",\"memory\":\"1Gi\"}}} The resource configurations for Storagedss services. nebula.storaged.logStorage 500Mi The log disk capacity for the Metad service. nebula.storaged.dataStorage 1Gi The data disk capacity for the Metad service. nebula.storaged.podLabels {} Labels for the Metad pod in a Nebula Graph cluster. nebula.storaged.podAnnotations {} Pod annotations for the Metad pod in a Nebula Graph cluster. nebula.storaged.nodeSelector {} Labels for the Metad pod to be scheduled to the specified node. nebula.storaged.tolerations {} Tolerations for the Metad pod. nebula.storaged.affinity {} Affinity for the Metad pod. nebula.storaged.readinessProbe {} ReadinessProbe for the Metad pod. nebula.storaged.sidecarContainers {} Sidecar containers for the Metad pod. nebula.storaged.sidecarVolumes {} Sidecar volumes for the Metad pod. imagePullSecrets [] The Secret to pull the Nebula Graph cluster image.","title":"Configuration parameters of the nebula-cluster Helm chart"},{"location":"nebula-operator/8.custom-cluster-configurations/8.1.custom-conf-parameter/","text":"Customize configuration parameters for a Nebula Graph cluster \u00b6 Meta, Storage, and Graph services in a Nebula Cluster have their configurations, which are defined as config in the YAML file of the CR instance (Nebula Graph cluster) you created. The settings in config are mapped and loaded into the ConfigMap of the corresponding service in Kubernetes. Note It is not available to customize configuration parameters for Nebula Clusters deployed with Helm. The structure of config is as follows. Config map[string]string `json:\"config,omitempty\"` Prerequisites \u00b6 You have created a Nebula Graph cluster. For how to create a cluster with Kubectl, see Create a cluster with Kubectl . Steps \u00b6 The following example uses a cluster named nebula and the cluster's configuration file named nebula_cluster.yaml to show how to set config for the Graph service in a Nebula Graph cluster. Run the following command to access the edit page of the nebula cluster. kubectl edit nebulaclusters.apps.nebula-graph.io nebula Add enable_authorize and auth_type under spec.graphd.config . apiVersion : apps.nebula-graph.io/v1alpha1 kind : NebulaCluster metadata : name : nebula namespace : default spec : graphd : resources : requests : cpu : \"500m\" memory : \"500Mi\" limits : cpu : \"1\" memory : \"1Gi\" replicas : 1 image : vesoft/nebula-graphd version : v3.1.0 storageClaim : resources : requests : storage : 2Gi storageClassName : gp2 config : // Custom configuration parameters for the Graph service in a cluster. \"enable_authorize\" : \"true\" \"auth_type\" : \"password\" ... Run kubectl apply -f nebula_cluster.yaml to push your configuration changes to the cluster. After customizing the parameters enable_authorize and auth_type , the configurations in the corresponding ConfigMap ( nebula-graphd ) of the Graph service will be overwritten. Learn more \u00b6 For more information on the configuration parameters of Meta, Storage, and Graph services, see Configurations .","title":"Custom configuration parameters for a Nebula Graph cluster"},{"location":"nebula-operator/8.custom-cluster-configurations/8.1.custom-conf-parameter/#customize_configuration_parameters_for_a_nebula_graph_cluster","text":"Meta, Storage, and Graph services in a Nebula Cluster have their configurations, which are defined as config in the YAML file of the CR instance (Nebula Graph cluster) you created. The settings in config are mapped and loaded into the ConfigMap of the corresponding service in Kubernetes. Note It is not available to customize configuration parameters for Nebula Clusters deployed with Helm. The structure of config is as follows. Config map[string]string `json:\"config,omitempty\"`","title":"Customize configuration parameters for a Nebula Graph cluster"},{"location":"nebula-operator/8.custom-cluster-configurations/8.1.custom-conf-parameter/#prerequisites","text":"You have created a Nebula Graph cluster. For how to create a cluster with Kubectl, see Create a cluster with Kubectl .","title":"Prerequisites"},{"location":"nebula-operator/8.custom-cluster-configurations/8.1.custom-conf-parameter/#steps","text":"The following example uses a cluster named nebula and the cluster's configuration file named nebula_cluster.yaml to show how to set config for the Graph service in a Nebula Graph cluster. Run the following command to access the edit page of the nebula cluster. kubectl edit nebulaclusters.apps.nebula-graph.io nebula Add enable_authorize and auth_type under spec.graphd.config . apiVersion : apps.nebula-graph.io/v1alpha1 kind : NebulaCluster metadata : name : nebula namespace : default spec : graphd : resources : requests : cpu : \"500m\" memory : \"500Mi\" limits : cpu : \"1\" memory : \"1Gi\" replicas : 1 image : vesoft/nebula-graphd version : v3.1.0 storageClaim : resources : requests : storage : 2Gi storageClassName : gp2 config : // Custom configuration parameters for the Graph service in a cluster. \"enable_authorize\" : \"true\" \"auth_type\" : \"password\" ... Run kubectl apply -f nebula_cluster.yaml to push your configuration changes to the cluster. After customizing the parameters enable_authorize and auth_type , the configurations in the corresponding ConfigMap ( nebula-graphd ) of the Graph service will be overwritten.","title":"Steps"},{"location":"nebula-operator/8.custom-cluster-configurations/8.1.custom-conf-parameter/#learn_more","text":"For more information on the configuration parameters of Meta, Storage, and Graph services, see Configurations .","title":"Learn more"},{"location":"nebula-operator/8.custom-cluster-configurations/8.2.pv-reclaim/","text":"Reclaim PVs \u00b6 Nebula Operator uses PVs (Persistent Volumes) and PVCs (Persistent Volume Claims) to store persistent data. If you accidentally deletes a Nebula Graph cluster, PV and PVC objects and the relevant data will be retained to ensure data security. You can define whether to reclaim PVs or not in the configuration file of the cluster's CR instance with the parameter enablePVReclaim . If you need to release a graph space and retain the relevant data, update your nebula cluster by setting the parameter enablePVReclaim to true . Prerequisites \u00b6 You have created a cluster. For how to create a cluster with Kubectl, see Create a cluster with Kubectl . Steps \u00b6 The following example uses a cluster named nebula and the cluster's configuration file named nebula_cluster.yaml to show how to set enablePVReclaim : Run the following command to access the edit page of the nebula cluster. kubectl edit nebulaclusters.apps.nebula-graph.io nebula Add enablePVReclaim and set its value to true under spec . apiVersion : apps.nebula-graph.io/v1alpha1 kind : NebulaCluster metadata : name : nebula spec : enablePVReclaim : true //Set its value to true. graphd : image : vesoft/nebula-graphd logVolumeClaim : resources : requests : storage : 2Gi storageClassName : fast-disks replicas : 1 resources : limits : cpu : \"1\" memory : 1Gi requests : cpu : 500m memory : 500Mi version : v3.1.0 imagePullPolicy : IfNotPresent metad : dataVolumeClaim : resources : requests : storage : 2Gi storageClassName : fast-disks image : vesoft/nebula-metad logVolumeClaim : resources : requests : storage : 2Gi storageClassName : fast-disks replicas : 1 resources : limits : cpu : \"1\" memory : 1Gi requests : cpu : 500m memory : 500Mi version : v3.1.0 nodeSelector : nebula : cloud reference : name : statefulsets.apps version : v1 schedulerName : default-scheduler storaged : dataVolumeClaim : resources : requests : storage : 2Gi storageClassName : fast-disks image : vesoft/nebula-storaged logVolumeClaim : resources : requests : storage : 2Gi storageClassName : fast-disks replicas : 3 resources : limits : cpu : \"1\" memory : 1Gi requests : cpu : 500m memory : 500Mi version : v3.1.0 ... Run kubectl apply -f nebula_cluster.yaml to push your configuration changes to the cluster.","title":"Reclaim PVs"},{"location":"nebula-operator/8.custom-cluster-configurations/8.2.pv-reclaim/#reclaim_pvs","text":"Nebula Operator uses PVs (Persistent Volumes) and PVCs (Persistent Volume Claims) to store persistent data. If you accidentally deletes a Nebula Graph cluster, PV and PVC objects and the relevant data will be retained to ensure data security. You can define whether to reclaim PVs or not in the configuration file of the cluster's CR instance with the parameter enablePVReclaim . If you need to release a graph space and retain the relevant data, update your nebula cluster by setting the parameter enablePVReclaim to true .","title":"Reclaim PVs"},{"location":"nebula-operator/8.custom-cluster-configurations/8.2.pv-reclaim/#prerequisites","text":"You have created a cluster. For how to create a cluster with Kubectl, see Create a cluster with Kubectl .","title":"Prerequisites"},{"location":"nebula-operator/8.custom-cluster-configurations/8.2.pv-reclaim/#steps","text":"The following example uses a cluster named nebula and the cluster's configuration file named nebula_cluster.yaml to show how to set enablePVReclaim : Run the following command to access the edit page of the nebula cluster. kubectl edit nebulaclusters.apps.nebula-graph.io nebula Add enablePVReclaim and set its value to true under spec . apiVersion : apps.nebula-graph.io/v1alpha1 kind : NebulaCluster metadata : name : nebula spec : enablePVReclaim : true //Set its value to true. graphd : image : vesoft/nebula-graphd logVolumeClaim : resources : requests : storage : 2Gi storageClassName : fast-disks replicas : 1 resources : limits : cpu : \"1\" memory : 1Gi requests : cpu : 500m memory : 500Mi version : v3.1.0 imagePullPolicy : IfNotPresent metad : dataVolumeClaim : resources : requests : storage : 2Gi storageClassName : fast-disks image : vesoft/nebula-metad logVolumeClaim : resources : requests : storage : 2Gi storageClassName : fast-disks replicas : 1 resources : limits : cpu : \"1\" memory : 1Gi requests : cpu : 500m memory : 500Mi version : v3.1.0 nodeSelector : nebula : cloud reference : name : statefulsets.apps version : v1 schedulerName : default-scheduler storaged : dataVolumeClaim : resources : requests : storage : 2Gi storageClassName : fast-disks image : vesoft/nebula-storaged logVolumeClaim : resources : requests : storage : 2Gi storageClassName : fast-disks replicas : 3 resources : limits : cpu : \"1\" memory : 1Gi requests : cpu : 500m memory : 500Mi version : v3.1.0 ... Run kubectl apply -f nebula_cluster.yaml to push your configuration changes to the cluster.","title":"Steps"},{"location":"nebula-operator/8.custom-cluster-configurations/8.3.balance-data-when-scaling-storage/","text":"Balance storage data after scaling out \u00b6 Enterpriseonly This feature is for Nebula Graph Enterprise Edition only. After the Storage service is scaled out, you can decide whether to balance the data in the Storage service. The scaling out of the Nebula Graph's Storage service is divided into two stages. In the first stage, the status of all pods is changed to Ready . In the second stage, the commands of BALANCE DATA \u548c BALANCE LEADER are executed to balance data. These two stages decouple the scaling out process of the controller replica from the balancing data process, so that you can choose to perform the data balancing operation during low traffic period. The decoupling of the scaling out process from the balancing process can effectively reduce the impact on online services during data migration. You can define whether to balance data automatically or not with the parameter enableAutoBalance in the configuration file of the CR instance of the cluster you created. Prerequisites \u00b6 You have created a Nebula Graph cluster. For how to create a cluster with Kubectl, see Create a cluster with Kubectl . Steps \u00b6 The following example uses a cluster named nebula and the cluster's configuration file named nebula_cluster.yaml to show how to set enableAutoBalance . Run the following command to access the edit page of the nebula cluster. kubectl edit nebulaclusters.apps.nebula-graph.io nebula Add enableAutoBalance and set its value to true under spec.storaged . apiVersion : apps.nebula-graph.io/v1alpha1 kind : NebulaCluster metadata : name : nebula spec : graphd : image : vesoft/nebula-graphd logVolumeClaim : resources : requests : storage : 2Gi storageClassName : fast-disks replicas : 1 resources : limits : cpu : \"1\" memory : 1Gi requests : cpu : 500m memory : 500Mi version : v3.1.0 imagePullPolicy : IfNotPresent metad : dataVolumeClaim : resources : requests : storage : 2Gi storageClassName : fast-disks image : vesoft/nebula-metad logVolumeClaim : resources : requests : storage : 2Gi storageClassName : fast-disks replicas : 1 resources : limits : cpu : \"1\" memory : 1Gi requests : cpu : 500m memory : 500Mi version : v3.1.0 nodeSelector : nebula : cloud reference : name : statefulsets.apps version : v1 schedulerName : default-scheduler storaged : enableAutoBalance : true //Set its value to true which means storage data will be balanced after the Storage service is scaled out. dataVolumeClaim : resources : requests : storage : 2Gi storageClassName : fast-disks image : vesoft/nebula-storaged logVolumeClaim : resources : requests : storage : 2Gi storageClassName : fast-disks replicas : 3 resources : limits : cpu : \"1\" memory : 1Gi requests : cpu : 500m memory : 500Mi version : v3.1.0 ... When the value of enableAutoBalance is set to true , the Storage data will be automatically balanced after the Storage service is scaled out. When the value of enableAutoBalance is set to false , the Storage data will not be automatically balanced after the Storage service is scaled out. When the enableAutoBalance parameter is not set, the system will not automatically balance Storage data by default after the Storage service is scaled out. Run kubectl apply -f nebula_cluster.yaml to push your configuration changes to the cluster.","title":"Balance storage data after scaling out"},{"location":"nebula-operator/8.custom-cluster-configurations/8.3.balance-data-when-scaling-storage/#balance_storage_data_after_scaling_out","text":"Enterpriseonly This feature is for Nebula Graph Enterprise Edition only. After the Storage service is scaled out, you can decide whether to balance the data in the Storage service. The scaling out of the Nebula Graph's Storage service is divided into two stages. In the first stage, the status of all pods is changed to Ready . In the second stage, the commands of BALANCE DATA \u548c BALANCE LEADER are executed to balance data. These two stages decouple the scaling out process of the controller replica from the balancing data process, so that you can choose to perform the data balancing operation during low traffic period. The decoupling of the scaling out process from the balancing process can effectively reduce the impact on online services during data migration. You can define whether to balance data automatically or not with the parameter enableAutoBalance in the configuration file of the CR instance of the cluster you created.","title":"Balance storage data after scaling out"},{"location":"nebula-operator/8.custom-cluster-configurations/8.3.balance-data-when-scaling-storage/#prerequisites","text":"You have created a Nebula Graph cluster. For how to create a cluster with Kubectl, see Create a cluster with Kubectl .","title":"Prerequisites"},{"location":"nebula-operator/8.custom-cluster-configurations/8.3.balance-data-when-scaling-storage/#steps","text":"The following example uses a cluster named nebula and the cluster's configuration file named nebula_cluster.yaml to show how to set enableAutoBalance . Run the following command to access the edit page of the nebula cluster. kubectl edit nebulaclusters.apps.nebula-graph.io nebula Add enableAutoBalance and set its value to true under spec.storaged . apiVersion : apps.nebula-graph.io/v1alpha1 kind : NebulaCluster metadata : name : nebula spec : graphd : image : vesoft/nebula-graphd logVolumeClaim : resources : requests : storage : 2Gi storageClassName : fast-disks replicas : 1 resources : limits : cpu : \"1\" memory : 1Gi requests : cpu : 500m memory : 500Mi version : v3.1.0 imagePullPolicy : IfNotPresent metad : dataVolumeClaim : resources : requests : storage : 2Gi storageClassName : fast-disks image : vesoft/nebula-metad logVolumeClaim : resources : requests : storage : 2Gi storageClassName : fast-disks replicas : 1 resources : limits : cpu : \"1\" memory : 1Gi requests : cpu : 500m memory : 500Mi version : v3.1.0 nodeSelector : nebula : cloud reference : name : statefulsets.apps version : v1 schedulerName : default-scheduler storaged : enableAutoBalance : true //Set its value to true which means storage data will be balanced after the Storage service is scaled out. dataVolumeClaim : resources : requests : storage : 2Gi storageClassName : fast-disks image : vesoft/nebula-storaged logVolumeClaim : resources : requests : storage : 2Gi storageClassName : fast-disks replicas : 3 resources : limits : cpu : \"1\" memory : 1Gi requests : cpu : 500m memory : 500Mi version : v3.1.0 ... When the value of enableAutoBalance is set to true , the Storage data will be automatically balanced after the Storage service is scaled out. When the value of enableAutoBalance is set to false , the Storage data will not be automatically balanced after the Storage service is scaled out. When the enableAutoBalance parameter is not set, the system will not automatically balance Storage data by default after the Storage service is scaled out. Run kubectl apply -f nebula_cluster.yaml to push your configuration changes to the cluster.","title":"Steps"},{"location":"nebula-studio/about-studio/st-ug-limitations/","text":"Limitations \u00b6 This topic introduces the limitations of Studio. Nebula Graph versions \u00b6 Note The Studio version is released independently of the Nebula Graph core. The correspondence between the versions of Studio and the Nebula Graph core, as shown in the table below. Nebula Graph version Studio version 1.x 1.x 2.0 & 2.0.1 2.x 2.5.x 3.0.0 2.6.x 3.1.x 3.0.0 3.2.x 3.1.0 3.3.0 Architecture \u00b6 For now, Studio v3.x supports x86_64 architecture only. Upload data \u00b6 Only CSV files without headers can be uploaded, but no limitations are applied to the size and store period for a single file. The maximum data volume depends on the storage capacity of your machine. nGQL statements \u00b6 On the Console page of Docker-based and RPM-based Studio v3.x, all the nGQL syntaxes except these are supported: USE <space_name> : You cannot run such a statement on the Console page to choose a graph space. As an alternative, you can click a graph space name in the drop-down list of Current Graph Space . You cannot use line breaks (\\). As an alternative, you can use the Enter key to split a line. For Studio on Cloud, besides the preceding syntax, you cannot run these account and role management statements on the Console page: CREATE USER ALTER USER CHANGE PASSWORD DROP USER GRANT ROLE REVOKE ROLE For more information about the preceding statements, see User management Browser \u00b6 We recommend that you use the latest version of Chrome to get access to Studio.","title":"Limitations"},{"location":"nebula-studio/about-studio/st-ug-limitations/#limitations","text":"This topic introduces the limitations of Studio.","title":"Limitations"},{"location":"nebula-studio/about-studio/st-ug-limitations/#nebula_graph_versions","text":"Note The Studio version is released independently of the Nebula Graph core. The correspondence between the versions of Studio and the Nebula Graph core, as shown in the table below. Nebula Graph version Studio version 1.x 1.x 2.0 & 2.0.1 2.x 2.5.x 3.0.0 2.6.x 3.1.x 3.0.0 3.2.x 3.1.0 3.3.0","title":"Nebula Graph versions"},{"location":"nebula-studio/about-studio/st-ug-limitations/#architecture","text":"For now, Studio v3.x supports x86_64 architecture only.","title":"Architecture"},{"location":"nebula-studio/about-studio/st-ug-limitations/#upload_data","text":"Only CSV files without headers can be uploaded, but no limitations are applied to the size and store period for a single file. The maximum data volume depends on the storage capacity of your machine.","title":"Upload data"},{"location":"nebula-studio/about-studio/st-ug-limitations/#ngql_statements","text":"On the Console page of Docker-based and RPM-based Studio v3.x, all the nGQL syntaxes except these are supported: USE <space_name> : You cannot run such a statement on the Console page to choose a graph space. As an alternative, you can click a graph space name in the drop-down list of Current Graph Space . You cannot use line breaks (\\). As an alternative, you can use the Enter key to split a line. For Studio on Cloud, besides the preceding syntax, you cannot run these account and role management statements on the Console page: CREATE USER ALTER USER CHANGE PASSWORD DROP USER GRANT ROLE REVOKE ROLE For more information about the preceding statements, see User management","title":"nGQL statements"},{"location":"nebula-studio/about-studio/st-ug-limitations/#browser","text":"We recommend that you use the latest version of Chrome to get access to Studio.","title":"Browser"},{"location":"nebula-studio/about-studio/st-ug-release-note/","text":"Change Log \u00b6 v3.3.0(2022.04.25) \u00b6 Feature enhancements: Optimize UI style. Schema Support clone graph space. Support rebuild indexes. Support statistics for data in graph space. Import Support multi-task asynchronous import, you can view progress, logs, etc. Support quick import from Nebula Importer templates. Console Support favorite historical statements. Support the display of historical statements results. Add graph exploration to the console.","title":"Change Log"},{"location":"nebula-studio/about-studio/st-ug-release-note/#change_log","text":"","title":"Change Log"},{"location":"nebula-studio/about-studio/st-ug-release-note/#v33020220425","text":"Feature enhancements: Optimize UI style. Schema Support clone graph space. Support rebuild indexes. Support statistics for data in graph space. Import Support multi-task asynchronous import, you can view progress, logs, etc. Support quick import from Nebula Importer templates. Console Support favorite historical statements. Support the display of historical statements results. Add graph exploration to the console.","title":"v3.3.0(2022.04.25)"},{"location":"nebula-studio/about-studio/st-ug-what-is-graph-studio/","text":"What is Nebula Graph Studio \u00b6 Nebula Graph Studio (Studio in short) is a browser-based visualization tool to manage Nebula Graph. It provides you with a graphical user interface to manipulate graph schemas, import data, and run nGQL statements to retrieve data. With Studio, you can quickly become a graph exploration expert from scratch. You can view the latest source code in the Nebula Graph GitHub repository, see nebula-studio for details. Released versions \u00b6 You can deploy Studio using the following methods: You can deploy Studio with Docker, RPM-based, Tar-based or DEB-based and connect it to Nebula Graph. For more information, see Deploy Studio . Helm-based. You can deploy Studio with Helm in the Kubernetes cluster and connect it to Nebula Graph. For more information, see Helm-based Studio . The functions of the above four deployment methods are the same and may be restricted when using Studio. For more information, see Limitations . Features \u00b6 Studio can easily manage Nebula Graph data, with the following functions: On the Schema page, you can use the graphical user interface to create the space, Tag, Edge Type, Index, and view the statistics on the graph. It helps you quickly get started with Nebula Graph. On the Import page, you can operate batch import of vertex and edge data with clicks, and view a real-time import log. On the Console page, you can run nGQL statements and read the results in a human-friendly way. Scenarios \u00b6 You can use Studio in one of these scenarios: You have a dataset, and you want to explore and analyze data in a visualized way. You can use Docker Compose to deploy Nebula Graph and then use Studio to explore or analyze data in a visualized way. You have deployed Nebula Graph and imported a dataset. You want to use a GUI to run nGQL statements or explore and analyze graph data in a visualized way. You are a beginner of nGQL (Nebula Graph Query Language) and you prefer to use a GUI rather than a command-line interface (CLI) to learn the language. Authentication \u00b6 Authentication is not enabled in Nebula Graph by default. Users can log into Studio with the root account and any password. When Nebula Graph enables authentication, users can only sign into Studio with the specified account. For more information, see Authentication . Check updates \u00b6 Studio is in development. Users can view the latest releases features through Changelog . To view the Changelog, on the upper-right corner of the page, click the version and then New version .","title":"What is Nebula Graph Studio"},{"location":"nebula-studio/about-studio/st-ug-what-is-graph-studio/#what_is_nebula_graph_studio","text":"Nebula Graph Studio (Studio in short) is a browser-based visualization tool to manage Nebula Graph. It provides you with a graphical user interface to manipulate graph schemas, import data, and run nGQL statements to retrieve data. With Studio, you can quickly become a graph exploration expert from scratch. You can view the latest source code in the Nebula Graph GitHub repository, see nebula-studio for details.","title":"What is Nebula Graph Studio"},{"location":"nebula-studio/about-studio/st-ug-what-is-graph-studio/#released_versions","text":"You can deploy Studio using the following methods: You can deploy Studio with Docker, RPM-based, Tar-based or DEB-based and connect it to Nebula Graph. For more information, see Deploy Studio . Helm-based. You can deploy Studio with Helm in the Kubernetes cluster and connect it to Nebula Graph. For more information, see Helm-based Studio . The functions of the above four deployment methods are the same and may be restricted when using Studio. For more information, see Limitations .","title":"Released versions"},{"location":"nebula-studio/about-studio/st-ug-what-is-graph-studio/#features","text":"Studio can easily manage Nebula Graph data, with the following functions: On the Schema page, you can use the graphical user interface to create the space, Tag, Edge Type, Index, and view the statistics on the graph. It helps you quickly get started with Nebula Graph. On the Import page, you can operate batch import of vertex and edge data with clicks, and view a real-time import log. On the Console page, you can run nGQL statements and read the results in a human-friendly way.","title":"Features"},{"location":"nebula-studio/about-studio/st-ug-what-is-graph-studio/#scenarios","text":"You can use Studio in one of these scenarios: You have a dataset, and you want to explore and analyze data in a visualized way. You can use Docker Compose to deploy Nebula Graph and then use Studio to explore or analyze data in a visualized way. You have deployed Nebula Graph and imported a dataset. You want to use a GUI to run nGQL statements or explore and analyze graph data in a visualized way. You are a beginner of nGQL (Nebula Graph Query Language) and you prefer to use a GUI rather than a command-line interface (CLI) to learn the language.","title":"Scenarios"},{"location":"nebula-studio/about-studio/st-ug-what-is-graph-studio/#authentication","text":"Authentication is not enabled in Nebula Graph by default. Users can log into Studio with the root account and any password. When Nebula Graph enables authentication, users can only sign into Studio with the specified account. For more information, see Authentication .","title":"Authentication"},{"location":"nebula-studio/about-studio/st-ug-what-is-graph-studio/#check_updates","text":"Studio is in development. Users can view the latest releases features through Changelog . To view the Changelog, on the upper-right corner of the page, click the version and then New version .","title":"Check updates"},{"location":"nebula-studio/deploy-connect/st-ug-connect/","text":"Connect to Nebula Graph \u00b6 After successfully launching Studio, you need to configure to connect to Nebula Graph. This topic describes how Studio connects to the Nebula Graph database. Prerequisites \u00b6 Before connecting to the Nebula Graph database, you need to confirm the following information: The Nebula Graph services and Studio are started. For more information, see Deploy Studio . You have the local IP address and the port used by the Graph service of Nebula Graph. The default port is 9669 . Note Run ifconfig or ipconfig on the machine to get the IP address. You have a Nebula Graph account and its password. Note If authentication is enabled in Nebula Graph and different role-based accounts are created, you must use the assigned account to connect to Nebula Graph. If authentication is disabled, you can use the root and any password to connect to Nebula Graph. For more information, see Nebula Graph Database Manual . Procedure \u00b6 To connect Studio to Nebula Graph, follow these steps: On the Config Server page of Studio, configure these fields: Host : Enter the IP address and the port of the Graph service of Nebula Graph. The valid format is IP:port . The default port is 9669 . Note When Nebula Graph and Studio are deployed on the same machine, you must enter the IP address of the machine, but not 127.0.0.1 or localhost , in the Host field. Username and Password : Fill in the log in account according to the authentication settings of Nebula Graph. If authentication is not enabled, you can use root and any password as the username and its password. If authentication is enabled and no account information has been created, you can only log in as GOD role and use root and nebula as the username and its password. If authentication is enabled and different users are created and assigned roles, users in different roles log in with their accounts and passwords. After the configuration, click the Connect button. If you can see the Explore page, Studio is successfully connected to Nebula Graph. One session continues for up to 30 minutes. If you do not operate Studio within 30 minutes, the active session will time out and you must connect to Nebula Graph again. Next to do \u00b6 When Studio is successfully connected to Nebula Graph, you can do these operations: If your account has GOD or ADMIN privilege, you can create a schema on the Console page or on the Schema page, batch import data on the Import page, and execute nGQL statements on the Console page. If your account has DBA or USER privilege, you can batch import data on the Import page or execute nGQL statements on the Console page. If your account has GUEST privilege, you can retrieve data with nGQL statements on the Console page or explore and analyze data on the Explore page. Log out \u00b6 If you want to reset Nebula Graph, you can log out and reconfigure the database. When the Studio is still connected to a Nebula Graph database, you can click the user profile picture in the upper right corner, and choose Log out . If the Config Server page is displayed on the browser, it means that Studio has successfully disconnected from the Nebula Graph database.","title":"Connect to Nebula Graph"},{"location":"nebula-studio/deploy-connect/st-ug-connect/#connect_to_nebula_graph","text":"After successfully launching Studio, you need to configure to connect to Nebula Graph. This topic describes how Studio connects to the Nebula Graph database.","title":"Connect to Nebula Graph"},{"location":"nebula-studio/deploy-connect/st-ug-connect/#prerequisites","text":"Before connecting to the Nebula Graph database, you need to confirm the following information: The Nebula Graph services and Studio are started. For more information, see Deploy Studio . You have the local IP address and the port used by the Graph service of Nebula Graph. The default port is 9669 . Note Run ifconfig or ipconfig on the machine to get the IP address. You have a Nebula Graph account and its password. Note If authentication is enabled in Nebula Graph and different role-based accounts are created, you must use the assigned account to connect to Nebula Graph. If authentication is disabled, you can use the root and any password to connect to Nebula Graph. For more information, see Nebula Graph Database Manual .","title":"Prerequisites"},{"location":"nebula-studio/deploy-connect/st-ug-connect/#procedure","text":"To connect Studio to Nebula Graph, follow these steps: On the Config Server page of Studio, configure these fields: Host : Enter the IP address and the port of the Graph service of Nebula Graph. The valid format is IP:port . The default port is 9669 . Note When Nebula Graph and Studio are deployed on the same machine, you must enter the IP address of the machine, but not 127.0.0.1 or localhost , in the Host field. Username and Password : Fill in the log in account according to the authentication settings of Nebula Graph. If authentication is not enabled, you can use root and any password as the username and its password. If authentication is enabled and no account information has been created, you can only log in as GOD role and use root and nebula as the username and its password. If authentication is enabled and different users are created and assigned roles, users in different roles log in with their accounts and passwords. After the configuration, click the Connect button. If you can see the Explore page, Studio is successfully connected to Nebula Graph. One session continues for up to 30 minutes. If you do not operate Studio within 30 minutes, the active session will time out and you must connect to Nebula Graph again.","title":"Procedure"},{"location":"nebula-studio/deploy-connect/st-ug-connect/#next_to_do","text":"When Studio is successfully connected to Nebula Graph, you can do these operations: If your account has GOD or ADMIN privilege, you can create a schema on the Console page or on the Schema page, batch import data on the Import page, and execute nGQL statements on the Console page. If your account has DBA or USER privilege, you can batch import data on the Import page or execute nGQL statements on the Console page. If your account has GUEST privilege, you can retrieve data with nGQL statements on the Console page or explore and analyze data on the Explore page.","title":"Next to do"},{"location":"nebula-studio/deploy-connect/st-ug-connect/#log_out","text":"If you want to reset Nebula Graph, you can log out and reconfigure the database. When the Studio is still connected to a Nebula Graph database, you can click the user profile picture in the upper right corner, and choose Log out . If the Config Server page is displayed on the browser, it means that Studio has successfully disconnected from the Nebula Graph database.","title":"Log out"},{"location":"nebula-studio/deploy-connect/st-ug-deploy-by-helm/","text":"Deploy Studio with Helm \u00b6 This topic describes how to deploy Studio with Helm. Prerequisites \u00b6 Before installing Studio, you need to install the following software and ensure the correct version of the software: Software Requirement Kubernetes >= 1.14 Helm >= 3.2.0 Install \u00b6 Use Git to clone the source code of Studio to the host. $ git clone https://github.com/vesoft-inc/nebula-studio.git Make the nebula-studio directory the current working directory. bash $ cd nebula-studio Assume using release name: my-studio , installed Studio in Helm Chart. $ helm upgrade --install my-studio --set service.type = NodePort --set service.port = 30070 deployment/helm When Studio is started, use http://<node_address>:30070/ to get access to Studio. If you can see the Config Server page on the browser, Studio is started successfully. Uninstall \u00b6 $ helm uninstall my-studio Next to do \u00b6 On the Config Server page, connect Docker-based Studio to Nebula Graph. For more information, see Connect to Nebula Graph . Configuration \u00b6 Parameter Default value Description replicaCount 0 The number of replicas for Deployment. image.nebulaStudio.name vesoft/nebula-graph-studio The image name of nebula-graph-studio. image.nebulaStudio.version v3.2.0 The image version of nebula-graph-studio. service.type ClusterIP The service type, which should be one of 'NodePort', 'ClusterIP', and 'LoadBalancer'. service.port 7001 The expose port for nebula-graph-studio's web. service.nodePort 32701 The proxy port for accessing nebula-studio outside kubernetes cluster. resources.nebulaStudio {} The resource limits/requests for nebula-studio. persistent.storageClassName \"\" The name of storageClass. The default value will be used if not specified. persistent.size 5Gi The persistent volume size.","title":"Deploy Studio with Helm"},{"location":"nebula-studio/deploy-connect/st-ug-deploy-by-helm/#deploy_studio_with_helm","text":"This topic describes how to deploy Studio with Helm.","title":"Deploy Studio with Helm"},{"location":"nebula-studio/deploy-connect/st-ug-deploy-by-helm/#prerequisites","text":"Before installing Studio, you need to install the following software and ensure the correct version of the software: Software Requirement Kubernetes >= 1.14 Helm >= 3.2.0","title":"Prerequisites"},{"location":"nebula-studio/deploy-connect/st-ug-deploy-by-helm/#install","text":"Use Git to clone the source code of Studio to the host. $ git clone https://github.com/vesoft-inc/nebula-studio.git Make the nebula-studio directory the current working directory. bash $ cd nebula-studio Assume using release name: my-studio , installed Studio in Helm Chart. $ helm upgrade --install my-studio --set service.type = NodePort --set service.port = 30070 deployment/helm When Studio is started, use http://<node_address>:30070/ to get access to Studio. If you can see the Config Server page on the browser, Studio is started successfully.","title":"Install"},{"location":"nebula-studio/deploy-connect/st-ug-deploy-by-helm/#uninstall","text":"$ helm uninstall my-studio","title":"Uninstall"},{"location":"nebula-studio/deploy-connect/st-ug-deploy-by-helm/#next_to_do","text":"On the Config Server page, connect Docker-based Studio to Nebula Graph. For more information, see Connect to Nebula Graph .","title":"Next to do"},{"location":"nebula-studio/deploy-connect/st-ug-deploy-by-helm/#configuration","text":"Parameter Default value Description replicaCount 0 The number of replicas for Deployment. image.nebulaStudio.name vesoft/nebula-graph-studio The image name of nebula-graph-studio. image.nebulaStudio.version v3.2.0 The image version of nebula-graph-studio. service.type ClusterIP The service type, which should be one of 'NodePort', 'ClusterIP', and 'LoadBalancer'. service.port 7001 The expose port for nebula-graph-studio's web. service.nodePort 32701 The proxy port for accessing nebula-studio outside kubernetes cluster. resources.nebulaStudio {} The resource limits/requests for nebula-studio. persistent.storageClassName \"\" The name of storageClass. The default value will be used if not specified. persistent.size 5Gi The persistent volume size.","title":"Configuration"},{"location":"nebula-studio/deploy-connect/st-ug-deploy/","text":"Deploy Studio \u00b6 This topic describes how to deploy Studio locally by RPM, DEB, tar package and Docker. Note You can also try some functions online in Studio. RPM-based Studio \u00b6 Prerequisites \u00b6 Before you deploy RPM-based Studio, you must confirm that: The Nebula Graph services are deployed and started. For more information, see Nebula Graph Database Manual . The Linux distribution is CentOS, install lsof . Before the installation starts, the following ports are not occupied. Port Description 7001 Web service provided by Studio. Install \u00b6 Select and download the RPM package according to your needs. It is recommended to select the latest version. Common links are as follows: Installation package Checksum Nebula version nebula-graph-studio-3.3.0.x86_64.rpm nebula-graph-studio-3.3.0.x86_64.rpm.sha256 3.1.0 Use sudo rpm -i <rpm_name> to install RPM package. For example, install Studio 3.3.0, use the following command. The default installation path is /usr/local/nebula-graph-studio . $ sudo rpm -i nebula-graph-studio-3.3.0.x86_64.rpm You can also install it to the specified path using the following command: $ sudo rpm -i nebula-graph-studio-3.3.0.x86_64.rpm --prefix = <path> When the screen returns the following message, it means that the PRM-based Studio has been successfully started. Start installing Nebula Studio now... Nebula Studio has been installed. Nebula Studio started automatically. When Studio is started, use http://<ip address>:7001 to get access to Studio. If you can see the Config Server page on the browser, Studio is started successfully. Uninstall \u00b6 You can uninstall Studio using the following command: $ sudo rpm -e nebula-graph-studio-3.3.0.x86_64 If these lines are returned, PRM-based Studio has been uninstalled. Nebula Studio removed, bye~ Exception handling \u00b6 If the automatic start fails during the installation process or you want to manually start or stop the service, use the following command: Start the service manually $ bash /usr/local/nebula-graph-studio/scripts/rpm/start.sh Stop the service manually $ bash /usr/local/nebula-graph-studio/scripts/rpm/stop.sh If you encounter an error bind EADDRINUSE 0.0.0.0:7001 when starting the service, you can use the following command to check port 7001 usage. $ lsof -i:7001 If the port is occupied and the process on that port cannot be terminated, you can use the following command to change Studio service port and restart the service. //Open the configuration file $ vi config/config.default.js //Change the port web: # task_id_path: # upload_dir: # tasks_dir: # sqlitedb_file_path: # ip: port: 7001 // Modify this port number and change it to any //Restart service $ systemctl restart nebula-graph-studio.service DEB-based Studio \u00b6 Prerequisites \u00b6 Before you deploy DEB-based Studio, you must do a check of these: The Nebula Graph services are deployed and started. For more information, see Nebula Graph Database Manual . The Linux distribution is Ubuntu. Before the installation starts, the following ports are not occupied. Port Description 7001 Web service provided by Studio The path /usr/lib/systemd/system exists in the system. If not, create it manually. Install \u00b6 Select and download the DEB package according to your needs. It is recommended to select the latest version. Common links are as follows: Installation package Checksum Nebula version nebula-graph-studio-3.3.0.x86_64.deb nebula-graph-studio-3.3.0.x86_64.deb.sha256 3.1.0 Use sudo dpkg -i <deb_name> to install DEB package. For example, install Studio 3.3.0, use the following command: $ sudo dpkg -i nebula-graph-studio-3.3.0.x86_64.deb When Studio is started, use http://<ip address>:7001 to get access to Studio. If you can see the Config Server page on the browser, Studio is started successfully. Uninstall \u00b6 You can uninstall Studio using the following command: $ sudo dpkg -r nebula-graph-studio-3.3.0.x86_64 tar-based Studio \u00b6 Prerequisites \u00b6 Before you deploy tar-based Studio, you must do a check of these: The Nebula Graph services are deployed and started. For more information, see Nebula Graph Database Manual . Before the installation starts, the following ports are not occupied. Port Description 7001 Web service provided by Studio Install and deploy \u00b6 Select and download the tar package according to your needs. It is recommended to select the latest version. Common links are as follows: Installation package Studio version nebula-graph-studio-3.3.0.x86_64.tar.gz 3.3.0 Use tar -xvf to decompress the tar package. $ tar -xvf nebula-graph-studio-3.3.0.x86_64.tar.gz Deploy and start nebula-graph-studio. $ cd nebula-graph-studio $ ./server Caution Studio 3.1.0 version is not dependent on nebula-importer and nebula-http-gateway, so the installation and deployment procedure is different from Studio v3.1.0. When Studio is started, use http://<ip address>:7001 to get access to Studio. If you can see the Config Server page on the browser, Studio is started successfully. Stop Service \u00b6 You can use kill pid to stop the service: $ kill $( lsof -t -i :7001 ) #stop nebula-graph-studio Docker-based Studio \u00b6 Prerequisites \u00b6 Before you deploy Docker-based Studio, you must do a check of these: The Nebula Graph services are deployed and started. For more information, see Nebula Graph Database Manual . On the machine where Studio will run, Docker Compose is installed and started. For more information, see Docker Compose Documentation . Before the installation starts, the following ports are not occupied. Port Description 7001 Web service provided by Studio Procedure \u00b6 To deploy and start Docker-based Studio, run the following commands. Here we use Nebula Graph v3.1.0 for demonstration: Download the configuration files for the deployment. Installation package Nebula Graph version nebula-graph-studio-v3.3.0.tar.gz 3.1.0 Create the nebula-graph-studio-v3.3.0 directory and decompress the installation package to the directory. $ mkdir nebula-graph-studio-v3.3.0 -zxvf nebula-graph-studio-v3.3.0.gz -C nebula-graph-studio-v3.3.0 Change to the nebula-graph-studio-v3.3.0 directory. $ cd nebula-graph-studio-v3.3.0 Pull the Docker image of Studio. $ docker-compose pull Build and start Docker-based Studio. In this command, -d is to run the containers in the background. $ docker-compose up -d If these lines are returned, Docker-based Studio v3.x is deployed and started. Creating docker_web_1 ... done When Docker-based Studio is started, use http://<ip address>:7001 to get access to Studio. Note Run ifconfig or ipconfig to get the IP address of the machine where Docker-based Studio is running. On the machine running Docker-based Studio, you can use http://localhost:7001 to get access to Studio. If you can see the Config Server page on the browser, Docker-based Studio is started successfully. Next to do \u00b6 On the Config Server page, connect Docker-based Studio to Nebula Graph. For more information, see Connect to Nebula Graph .","title":"Deploy Studio"},{"location":"nebula-studio/deploy-connect/st-ug-deploy/#deploy_studio","text":"This topic describes how to deploy Studio locally by RPM, DEB, tar package and Docker. Note You can also try some functions online in Studio.","title":"Deploy Studio"},{"location":"nebula-studio/deploy-connect/st-ug-deploy/#rpm-based_studio","text":"","title":"RPM-based Studio"},{"location":"nebula-studio/deploy-connect/st-ug-deploy/#prerequisites","text":"Before you deploy RPM-based Studio, you must confirm that: The Nebula Graph services are deployed and started. For more information, see Nebula Graph Database Manual . The Linux distribution is CentOS, install lsof . Before the installation starts, the following ports are not occupied. Port Description 7001 Web service provided by Studio.","title":"Prerequisites"},{"location":"nebula-studio/deploy-connect/st-ug-deploy/#install","text":"Select and download the RPM package according to your needs. It is recommended to select the latest version. Common links are as follows: Installation package Checksum Nebula version nebula-graph-studio-3.3.0.x86_64.rpm nebula-graph-studio-3.3.0.x86_64.rpm.sha256 3.1.0 Use sudo rpm -i <rpm_name> to install RPM package. For example, install Studio 3.3.0, use the following command. The default installation path is /usr/local/nebula-graph-studio . $ sudo rpm -i nebula-graph-studio-3.3.0.x86_64.rpm You can also install it to the specified path using the following command: $ sudo rpm -i nebula-graph-studio-3.3.0.x86_64.rpm --prefix = <path> When the screen returns the following message, it means that the PRM-based Studio has been successfully started. Start installing Nebula Studio now... Nebula Studio has been installed. Nebula Studio started automatically. When Studio is started, use http://<ip address>:7001 to get access to Studio. If you can see the Config Server page on the browser, Studio is started successfully.","title":"Install"},{"location":"nebula-studio/deploy-connect/st-ug-deploy/#uninstall","text":"You can uninstall Studio using the following command: $ sudo rpm -e nebula-graph-studio-3.3.0.x86_64 If these lines are returned, PRM-based Studio has been uninstalled. Nebula Studio removed, bye~","title":"Uninstall"},{"location":"nebula-studio/deploy-connect/st-ug-deploy/#exception_handling","text":"If the automatic start fails during the installation process or you want to manually start or stop the service, use the following command: Start the service manually $ bash /usr/local/nebula-graph-studio/scripts/rpm/start.sh Stop the service manually $ bash /usr/local/nebula-graph-studio/scripts/rpm/stop.sh If you encounter an error bind EADDRINUSE 0.0.0.0:7001 when starting the service, you can use the following command to check port 7001 usage. $ lsof -i:7001 If the port is occupied and the process on that port cannot be terminated, you can use the following command to change Studio service port and restart the service. //Open the configuration file $ vi config/config.default.js //Change the port web: # task_id_path: # upload_dir: # tasks_dir: # sqlitedb_file_path: # ip: port: 7001 // Modify this port number and change it to any //Restart service $ systemctl restart nebula-graph-studio.service","title":"Exception handling"},{"location":"nebula-studio/deploy-connect/st-ug-deploy/#deb-based_studio","text":"","title":"DEB-based Studio"},{"location":"nebula-studio/deploy-connect/st-ug-deploy/#prerequisites_1","text":"Before you deploy DEB-based Studio, you must do a check of these: The Nebula Graph services are deployed and started. For more information, see Nebula Graph Database Manual . The Linux distribution is Ubuntu. Before the installation starts, the following ports are not occupied. Port Description 7001 Web service provided by Studio The path /usr/lib/systemd/system exists in the system. If not, create it manually.","title":"Prerequisites"},{"location":"nebula-studio/deploy-connect/st-ug-deploy/#install_1","text":"Select and download the DEB package according to your needs. It is recommended to select the latest version. Common links are as follows: Installation package Checksum Nebula version nebula-graph-studio-3.3.0.x86_64.deb nebula-graph-studio-3.3.0.x86_64.deb.sha256 3.1.0 Use sudo dpkg -i <deb_name> to install DEB package. For example, install Studio 3.3.0, use the following command: $ sudo dpkg -i nebula-graph-studio-3.3.0.x86_64.deb When Studio is started, use http://<ip address>:7001 to get access to Studio. If you can see the Config Server page on the browser, Studio is started successfully.","title":"Install"},{"location":"nebula-studio/deploy-connect/st-ug-deploy/#uninstall_1","text":"You can uninstall Studio using the following command: $ sudo dpkg -r nebula-graph-studio-3.3.0.x86_64","title":"Uninstall"},{"location":"nebula-studio/deploy-connect/st-ug-deploy/#tar-based_studio","text":"","title":"tar-based Studio"},{"location":"nebula-studio/deploy-connect/st-ug-deploy/#prerequisites_2","text":"Before you deploy tar-based Studio, you must do a check of these: The Nebula Graph services are deployed and started. For more information, see Nebula Graph Database Manual . Before the installation starts, the following ports are not occupied. Port Description 7001 Web service provided by Studio","title":"Prerequisites"},{"location":"nebula-studio/deploy-connect/st-ug-deploy/#install_and_deploy","text":"Select and download the tar package according to your needs. It is recommended to select the latest version. Common links are as follows: Installation package Studio version nebula-graph-studio-3.3.0.x86_64.tar.gz 3.3.0 Use tar -xvf to decompress the tar package. $ tar -xvf nebula-graph-studio-3.3.0.x86_64.tar.gz Deploy and start nebula-graph-studio. $ cd nebula-graph-studio $ ./server Caution Studio 3.1.0 version is not dependent on nebula-importer and nebula-http-gateway, so the installation and deployment procedure is different from Studio v3.1.0. When Studio is started, use http://<ip address>:7001 to get access to Studio. If you can see the Config Server page on the browser, Studio is started successfully.","title":"Install and deploy"},{"location":"nebula-studio/deploy-connect/st-ug-deploy/#stop_service","text":"You can use kill pid to stop the service: $ kill $( lsof -t -i :7001 ) #stop nebula-graph-studio","title":"Stop Service"},{"location":"nebula-studio/deploy-connect/st-ug-deploy/#docker-based_studio","text":"","title":"Docker-based Studio"},{"location":"nebula-studio/deploy-connect/st-ug-deploy/#prerequisites_3","text":"Before you deploy Docker-based Studio, you must do a check of these: The Nebula Graph services are deployed and started. For more information, see Nebula Graph Database Manual . On the machine where Studio will run, Docker Compose is installed and started. For more information, see Docker Compose Documentation . Before the installation starts, the following ports are not occupied. Port Description 7001 Web service provided by Studio","title":"Prerequisites"},{"location":"nebula-studio/deploy-connect/st-ug-deploy/#procedure","text":"To deploy and start Docker-based Studio, run the following commands. Here we use Nebula Graph v3.1.0 for demonstration: Download the configuration files for the deployment. Installation package Nebula Graph version nebula-graph-studio-v3.3.0.tar.gz 3.1.0 Create the nebula-graph-studio-v3.3.0 directory and decompress the installation package to the directory. $ mkdir nebula-graph-studio-v3.3.0 -zxvf nebula-graph-studio-v3.3.0.gz -C nebula-graph-studio-v3.3.0 Change to the nebula-graph-studio-v3.3.0 directory. $ cd nebula-graph-studio-v3.3.0 Pull the Docker image of Studio. $ docker-compose pull Build and start Docker-based Studio. In this command, -d is to run the containers in the background. $ docker-compose up -d If these lines are returned, Docker-based Studio v3.x is deployed and started. Creating docker_web_1 ... done When Docker-based Studio is started, use http://<ip address>:7001 to get access to Studio. Note Run ifconfig or ipconfig to get the IP address of the machine where Docker-based Studio is running. On the machine running Docker-based Studio, you can use http://localhost:7001 to get access to Studio. If you can see the Config Server page on the browser, Docker-based Studio is started successfully.","title":"Procedure"},{"location":"nebula-studio/deploy-connect/st-ug-deploy/#next_to_do","text":"On the Config Server page, connect Docker-based Studio to Nebula Graph. For more information, see Connect to Nebula Graph .","title":"Next to do"},{"location":"nebula-studio/manage-schema/st-ug-crud-edge-type/","text":"Operate edge types \u00b6 After a graph space is created in Nebula Graph, you can create edge types. With Studio, you can choose to use the Console page or the Schema page to create, retrieve, update, or delete edge types. This topic introduces how to use the Schema page to operate edge types in a graph space only. Prerequisites \u00b6 To operate an edge type on the Schema page of Studio, you must do a check of these: Studio is connected to Nebula Graph. A graph space is created. Your account has the authority of GOD, ADMIN, or DBA. Create an edge type \u00b6 In the toolbar, click the Schema tab. In the Graph Space List page, find a graph space and then click its name or click Schema in the Operations column. In the Current Graph Space field, confirm the name of the graph space. If necessary, you can choose another name to change the graph space. Click the Edge Type tab and click the + Create button. On the Create Edge Type page, do these settings: Name : Specify an appropriate name for the edge type. In this example, serve is used. Comment (Optional): Enter the description for edge type. Define Properties (Optional): If necessary, click + Add Property to do these settings: Enter a property name. Select a data type. Select whether to allow null values.. (Optional) Enter the default value. (Optional) Enter the description. Set TTL (Time To Live) (Optional): If no index is set for the edge type, you can set the TTL configuration: In the upper left corner of the Set TTL panel, click the check box to expand the panel, and configure TTL_COL and TTL_ DURATION (in seconds). For more information about both parameters, see TTL configuration . When the preceding settings are completed, in the Equivalent to the following nGQL statement panel, you can see the nGQL statement equivalent to these settings. Confirm the settings and then click the + Create button. When the edge type is created successfully, the Define Properties panel shows all its properties on the list. Edit an edge type \u00b6 In the toolbar, click the Schema tab. In the Graph Space List page, find a graph space and then click its name or click Schema in the Operations column. In the Current Graph Space field, confirm the name of the graph space. If necessary, you can choose another name to change the graph space. Click the Edge Type tab, find an edge type and then click the button in the Operations column. On the Edit page, do these operations: To edit a comment: Click Edit on the right of Comment . To edit a property: On the Define Properties panel, find a property, click Edit , and then change the data type or the default value. To delete a property: On the Define Properties panel, find a property, click Delete . To add more properties: On the Define Properties panel, click the Add Property button to add a new property. To set the TTL configuration: In the upper left corner of the Set TTL panel, click the check box and then set TTL. To delete the TTL configuration: When the Set TTL panel is expanded, in the upper left corner of the panel, click the check box to delete the configuration. To edit the TTL configuration: On the Set TTL panel, click Edit and then change the configuration of TTL_COL and TTL_DURATION (in seconds). Note For information about the coexistence problem of TTL and index, see [TTL]((../../3.ngql-guide/8.clauses-and-options/ttl-options.md). Delete an Edge type \u00b6 Danger Confirm the impact before deleting the Edge type. The deleted data cannot be restored if it is not backup . In the toolbar, click the Schema tab. In the Graph Space List page, find a graph space and then click its name or click Schema in the Operations column. In the Current Graph Space field, confirm the name of the graph space. If necessary, you can choose another name to change the graph space. Click the Edge Type tab, find an edge type and then click the button in the Operations column. Click OK to confirm in the pop-up dialog box. Next to do \u00b6 After the edge type is created, you can use the Console page to insert edge data one by one manually or use the Import page to bulk import edge data.","title":"Operate Edge types"},{"location":"nebula-studio/manage-schema/st-ug-crud-edge-type/#operate_edge_types","text":"After a graph space is created in Nebula Graph, you can create edge types. With Studio, you can choose to use the Console page or the Schema page to create, retrieve, update, or delete edge types. This topic introduces how to use the Schema page to operate edge types in a graph space only.","title":"Operate edge types"},{"location":"nebula-studio/manage-schema/st-ug-crud-edge-type/#prerequisites","text":"To operate an edge type on the Schema page of Studio, you must do a check of these: Studio is connected to Nebula Graph. A graph space is created. Your account has the authority of GOD, ADMIN, or DBA.","title":"Prerequisites"},{"location":"nebula-studio/manage-schema/st-ug-crud-edge-type/#create_an_edge_type","text":"In the toolbar, click the Schema tab. In the Graph Space List page, find a graph space and then click its name or click Schema in the Operations column. In the Current Graph Space field, confirm the name of the graph space. If necessary, you can choose another name to change the graph space. Click the Edge Type tab and click the + Create button. On the Create Edge Type page, do these settings: Name : Specify an appropriate name for the edge type. In this example, serve is used. Comment (Optional): Enter the description for edge type. Define Properties (Optional): If necessary, click + Add Property to do these settings: Enter a property name. Select a data type. Select whether to allow null values.. (Optional) Enter the default value. (Optional) Enter the description. Set TTL (Time To Live) (Optional): If no index is set for the edge type, you can set the TTL configuration: In the upper left corner of the Set TTL panel, click the check box to expand the panel, and configure TTL_COL and TTL_ DURATION (in seconds). For more information about both parameters, see TTL configuration . When the preceding settings are completed, in the Equivalent to the following nGQL statement panel, you can see the nGQL statement equivalent to these settings. Confirm the settings and then click the + Create button. When the edge type is created successfully, the Define Properties panel shows all its properties on the list.","title":"Create an edge type"},{"location":"nebula-studio/manage-schema/st-ug-crud-edge-type/#edit_an_edge_type","text":"In the toolbar, click the Schema tab. In the Graph Space List page, find a graph space and then click its name or click Schema in the Operations column. In the Current Graph Space field, confirm the name of the graph space. If necessary, you can choose another name to change the graph space. Click the Edge Type tab, find an edge type and then click the button in the Operations column. On the Edit page, do these operations: To edit a comment: Click Edit on the right of Comment . To edit a property: On the Define Properties panel, find a property, click Edit , and then change the data type or the default value. To delete a property: On the Define Properties panel, find a property, click Delete . To add more properties: On the Define Properties panel, click the Add Property button to add a new property. To set the TTL configuration: In the upper left corner of the Set TTL panel, click the check box and then set TTL. To delete the TTL configuration: When the Set TTL panel is expanded, in the upper left corner of the panel, click the check box to delete the configuration. To edit the TTL configuration: On the Set TTL panel, click Edit and then change the configuration of TTL_COL and TTL_DURATION (in seconds). Note For information about the coexistence problem of TTL and index, see [TTL]((../../3.ngql-guide/8.clauses-and-options/ttl-options.md).","title":"Edit an edge type"},{"location":"nebula-studio/manage-schema/st-ug-crud-edge-type/#delete_an_edge_type","text":"Danger Confirm the impact before deleting the Edge type. The deleted data cannot be restored if it is not backup . In the toolbar, click the Schema tab. In the Graph Space List page, find a graph space and then click its name or click Schema in the Operations column. In the Current Graph Space field, confirm the name of the graph space. If necessary, you can choose another name to change the graph space. Click the Edge Type tab, find an edge type and then click the button in the Operations column. Click OK to confirm in the pop-up dialog box.","title":"Delete an Edge type"},{"location":"nebula-studio/manage-schema/st-ug-crud-edge-type/#next_to_do","text":"After the edge type is created, you can use the Console page to insert edge data one by one manually or use the Import page to bulk import edge data.","title":"Next to do"},{"location":"nebula-studio/manage-schema/st-ug-crud-index/","text":"Operate Indexes \u00b6 You can create an index for a Tag and/or an Edge type. An index lets traversal start from vertices or edges with the same property and it can make a query more efficient. With Studio, you can use the Console page or the Schema page to create, retrieve, and delete indexes. This topic introduces how to use the Schema page to operate an index only. Note You can create an index when a Tag or an Edge Type is created. But an index can decrease the write speed during data import. We recommend that you import data firstly and then create and rebuild an index. For more information, see Index overview . Prerequisites \u00b6 To operate an index on the Schema page of Studio, you must do a check of these: Studio is connected to Nebula Graph. A graph Space, Tags, and Edge Types are created. Your account has the authority of GOD, ADMIN, or DBA. Create an index \u00b6 In the toolbar, click the Schema tab. In the Graph Space List page, find a graph space and then click its name or click Schema in the Operations column. In the Current Graph Space field, confirm the name of the graph space. If necessary, you can choose another name to change the graph space. Click the Index tab and then click the + Create button. On the Create page, do these settings: Index Type : Choose to create an index for a tag or for an edge type. In this example, Edge Type is chosen. Associated tag name : Choose a tag name or an edge type name. In this example, follow is chosen. Index Name : Specify a name for the new index. In this example, follow_index is used. Comment (Optional): Enter the description for index. Indexed Properties (Optional): Click Add property , and then, in the dialog box, choose a property. If necessary, repeat this step to choose more properties. You can drag the properties to sort them. In this example, degree is chosen. Note The order of the indexed properties has an effect on the result of the LOOKUP statement. For more information, see nGQL Manual . When the settings are done, the Equivalent to the following nGQL statement panel shows the statement equivalent to the settings. Confirm the settings and then click the + Create button. When an index is created, the index list shows the new index. View indexes \u00b6 In the toolbar, click the Schema tab. In the Graph Space List page, find a graph space and then click its name or click Schema in the Operations column. In the Current Graph Space field, confirm the name of the graph space. If necessary, you can choose another name to change the graph space. Click the Index tab, in the upper left corner, choose an index type, Tag or Edge Type . In the list, find an index and click its row. All its details are shown in the expanded row. Rebuild indexes \u00b6 In the toolbar, click the Schema tab. In the Graph Space List page, find a graph space and then click its name or click Schema in the Operations column. In the Current Graph Space field, confirm the name of the graph space. If necessary, you can choose another name to change the graph space. Click the Index tab, in the upper left corner, choose an index type, Tag or Edge Type . Click the Index tab, find an index and then click the button Rebuild in the Operations column. Note For more Information, see REBUILD INDEX . Delete an index \u00b6 To delete an index on Schema , follow these steps: In the toolbar, click the Schema tab. In the Graph Space List page, find a graph space and then click its name or click Schema in the Operations column. In the Current Graph Space field, confirm the name of the graph space. If necessary, you can choose another name to change the graph space. Click the Index tab, find an index and then click the button in the Operations column. Click OK to confirm in the pop-up dialog box.","title":"Operate Indexes"},{"location":"nebula-studio/manage-schema/st-ug-crud-index/#operate_indexes","text":"You can create an index for a Tag and/or an Edge type. An index lets traversal start from vertices or edges with the same property and it can make a query more efficient. With Studio, you can use the Console page or the Schema page to create, retrieve, and delete indexes. This topic introduces how to use the Schema page to operate an index only. Note You can create an index when a Tag or an Edge Type is created. But an index can decrease the write speed during data import. We recommend that you import data firstly and then create and rebuild an index. For more information, see Index overview .","title":"Operate Indexes"},{"location":"nebula-studio/manage-schema/st-ug-crud-index/#prerequisites","text":"To operate an index on the Schema page of Studio, you must do a check of these: Studio is connected to Nebula Graph. A graph Space, Tags, and Edge Types are created. Your account has the authority of GOD, ADMIN, or DBA.","title":"Prerequisites"},{"location":"nebula-studio/manage-schema/st-ug-crud-index/#create_an_index","text":"In the toolbar, click the Schema tab. In the Graph Space List page, find a graph space and then click its name or click Schema in the Operations column. In the Current Graph Space field, confirm the name of the graph space. If necessary, you can choose another name to change the graph space. Click the Index tab and then click the + Create button. On the Create page, do these settings: Index Type : Choose to create an index for a tag or for an edge type. In this example, Edge Type is chosen. Associated tag name : Choose a tag name or an edge type name. In this example, follow is chosen. Index Name : Specify a name for the new index. In this example, follow_index is used. Comment (Optional): Enter the description for index. Indexed Properties (Optional): Click Add property , and then, in the dialog box, choose a property. If necessary, repeat this step to choose more properties. You can drag the properties to sort them. In this example, degree is chosen. Note The order of the indexed properties has an effect on the result of the LOOKUP statement. For more information, see nGQL Manual . When the settings are done, the Equivalent to the following nGQL statement panel shows the statement equivalent to the settings. Confirm the settings and then click the + Create button. When an index is created, the index list shows the new index.","title":"Create an index"},{"location":"nebula-studio/manage-schema/st-ug-crud-index/#view_indexes","text":"In the toolbar, click the Schema tab. In the Graph Space List page, find a graph space and then click its name or click Schema in the Operations column. In the Current Graph Space field, confirm the name of the graph space. If necessary, you can choose another name to change the graph space. Click the Index tab, in the upper left corner, choose an index type, Tag or Edge Type . In the list, find an index and click its row. All its details are shown in the expanded row.","title":"View indexes"},{"location":"nebula-studio/manage-schema/st-ug-crud-index/#rebuild_indexes","text":"In the toolbar, click the Schema tab. In the Graph Space List page, find a graph space and then click its name or click Schema in the Operations column. In the Current Graph Space field, confirm the name of the graph space. If necessary, you can choose another name to change the graph space. Click the Index tab, in the upper left corner, choose an index type, Tag or Edge Type . Click the Index tab, find an index and then click the button Rebuild in the Operations column. Note For more Information, see REBUILD INDEX .","title":"Rebuild indexes"},{"location":"nebula-studio/manage-schema/st-ug-crud-index/#delete_an_index","text":"To delete an index on Schema , follow these steps: In the toolbar, click the Schema tab. In the Graph Space List page, find a graph space and then click its name or click Schema in the Operations column. In the Current Graph Space field, confirm the name of the graph space. If necessary, you can choose another name to change the graph space. Click the Index tab, find an index and then click the button in the Operations column. Click OK to confirm in the pop-up dialog box.","title":"Delete an index"},{"location":"nebula-studio/manage-schema/st-ug-crud-space/","text":"Operate graph spaces \u00b6 When Studio is connected to Nebula Graph, you can create or delete a graph space. You can use the Console page or the Schema page to do these operations. This article only introduces how to use the Schema page to operate graph spaces in Nebula Graph. Prerequisites \u00b6 To operate a graph space on the Schema page of Studio, you must do a check of these: Studio is connected to Nebula Graph. Your account has the authority of GOD. It means that: If the authentication is enabled in Nebula Graph, you can use root and any password to sign in to Studio. If the authentication is disabled in Nebula Graph, you must use root and its password to sign in to Studio. Create a graph space \u00b6 In the toolbar, click the Schema tab. In the Graph Space List page, click Create Space , do these settings: Name : Specify a name to the new graph space. In this example, basketballplayer is used. The name must be distinct in the database. Vid Type : The data types of VIDs are restricted to FIXED_STRING(<N>) or INT64 . A graph space can only select one VID type. In this example, FIXED_STRING(32) is used. For more information, see VID . Comment : Enter the description for graph space. The maximum length is 256 bytes. By default, there will be no comments on a space. But in this example, Statistics of basketball players is used. Optional Parameters : Set the values of partition_num and replica_factor respectively. In this example, these parameters are set to 100 and 1 respectively. For more information, see CREATE SPACE syntax . In the Equivalent to the following nGQL statement panel, you can see the statement equivalent to the preceding settings. CREATE SPACE basketballplayer ( partition_num = 100 , replica_factor = 1 , vid_type = FIXED_STRING ( 32 )) COMMENT = \"Statistics of basketball players\" Confirm the settings and then click the + Create button. If the graph space is created successfully, you can see it on the graph space list. Delete a graph space \u00b6 Danger Deleting the space will delete all the data in it, and the deleted data cannot be restored if it is not backed up . In the toolbar, click the Schema tab. In the Graph Space List , find the space you want to be deleted, and click Delete Graph Space in the Operation column. On the dialog box, confirm the information and then click OK . Next to do \u00b6 After a graph space is created, you can create or edit a schema, including: Operate tags Operate edge types Operate indexes","title":"Operate graph spaces"},{"location":"nebula-studio/manage-schema/st-ug-crud-space/#operate_graph_spaces","text":"When Studio is connected to Nebula Graph, you can create or delete a graph space. You can use the Console page or the Schema page to do these operations. This article only introduces how to use the Schema page to operate graph spaces in Nebula Graph.","title":"Operate graph spaces"},{"location":"nebula-studio/manage-schema/st-ug-crud-space/#prerequisites","text":"To operate a graph space on the Schema page of Studio, you must do a check of these: Studio is connected to Nebula Graph. Your account has the authority of GOD. It means that: If the authentication is enabled in Nebula Graph, you can use root and any password to sign in to Studio. If the authentication is disabled in Nebula Graph, you must use root and its password to sign in to Studio.","title":"Prerequisites"},{"location":"nebula-studio/manage-schema/st-ug-crud-space/#create_a_graph_space","text":"In the toolbar, click the Schema tab. In the Graph Space List page, click Create Space , do these settings: Name : Specify a name to the new graph space. In this example, basketballplayer is used. The name must be distinct in the database. Vid Type : The data types of VIDs are restricted to FIXED_STRING(<N>) or INT64 . A graph space can only select one VID type. In this example, FIXED_STRING(32) is used. For more information, see VID . Comment : Enter the description for graph space. The maximum length is 256 bytes. By default, there will be no comments on a space. But in this example, Statistics of basketball players is used. Optional Parameters : Set the values of partition_num and replica_factor respectively. In this example, these parameters are set to 100 and 1 respectively. For more information, see CREATE SPACE syntax . In the Equivalent to the following nGQL statement panel, you can see the statement equivalent to the preceding settings. CREATE SPACE basketballplayer ( partition_num = 100 , replica_factor = 1 , vid_type = FIXED_STRING ( 32 )) COMMENT = \"Statistics of basketball players\" Confirm the settings and then click the + Create button. If the graph space is created successfully, you can see it on the graph space list.","title":"Create a graph space"},{"location":"nebula-studio/manage-schema/st-ug-crud-space/#delete_a_graph_space","text":"Danger Deleting the space will delete all the data in it, and the deleted data cannot be restored if it is not backed up . In the toolbar, click the Schema tab. In the Graph Space List , find the space you want to be deleted, and click Delete Graph Space in the Operation column. On the dialog box, confirm the information and then click OK .","title":"Delete a graph space"},{"location":"nebula-studio/manage-schema/st-ug-crud-space/#next_to_do","text":"After a graph space is created, you can create or edit a schema, including: Operate tags Operate edge types Operate indexes","title":"Next to do"},{"location":"nebula-studio/manage-schema/st-ug-crud-tag/","text":"Operate tags \u00b6 After a graph space is created in Nebula Graph, you can create tags. With Studio, you can use the Console page or the Schema page to create, retrieve, update, or delete tags. This topic introduces how to use the Schema page to operate tags in a graph space only. Prerequisites \u00b6 To operate a tag on the Schema page of Studio, you must do a check of these: Studio is connected to Nebula Graph. A graph space is created. Your account has the authority of GOD, ADMIN, or DBA. Create a tag \u00b6 In the toolbar, click the Schema tab. In the Graph Space List page, find a graph space and then click its name or click Schema in the Operations column. In the Current Graph Space field, confirm the name of the graph space. If necessary, you can choose another name to change the graph space. Click the Tag tab and click the + Create button. On the Create page, do these settings: Name : Specify an appropriate name for the tag. In this example, course is specified. Comment (Optional): Enter the description for tag. Define Properties (Optional): If necessary, click + Add Property to do these settings: Enter a property name. Select a data type. Select whether to allow null values.. (Optional) Enter the default value. (Optional) Enter the description. Set TTL (Time To Live) (Optional): If no index is set for the tag, you can set the TTL configuration: In the upper left corner of the Set TTL panel, click the check box to expand the panel, and configure TTL_COL and TTL_ DURATION (in seconds). For more information about both parameters, see TTL configuration . When the preceding settings are completed, in the Equivalent to the following nGQL statement panel, you can see the nGQL statement equivalent to these settings. Confirm the settings and then click the + Create button. When the tag is created successfully, the Define Properties panel shows all its properties on the list. Edit a tag \u00b6 In the toolbar, click the Schema tab. In the Graph Space List page, find a graph space and then click its name or click Schema in the Operations column. In the Current Graph Space field, confirm the name of the graph space. If necessary, you can choose another name to change the graph space. Click the Tag tab, find a tag and then click the button in the Operations column. On the Edit page, do these operations: To edit a Comment: Click Edit on the right of Comment . To edit a property: On the Define Properties panel, find a property, click Edit , and then change the data type or the default value. To delete a property: On the Define Properties panel, find a property, click Delete . To add more properties: On the Define Properties panel, click the Add Property button to add a new property. To set the TTL configuration: In the upper left corner of the Set TTL panel, click the check box and then set TTL. To delete the TTL configuration: When the Set TTL panel is expanded, in the upper left corner of the panel, click the check box to delete the configuration. To edit the TTL configuration: On the Set TTL panel, click Edit and then change the configuration of TTL_COL and TTL_DURATION (in seconds). Note The problem of coexistence of TTL and index, see [TTL]((../../3.ngql-guide/8.clauses-and-options/ttl-options.md). Delete a tag \u00b6 Danger Confirm the impact before deleting the tag. The deleted data cannot be restored if it is not backup . In the toolbar, click the Schema tab. In the Graph Space List page, find a graph space and then click its name or click Schema in the Operations column. In the Current Graph Space field, confirm the name of the graph space. If necessary, you can choose another name to change the graph space. Click the Tag tab, find an tag and then click the button in the Operations column. Click OK to confirm delete a tag in the pop-up dialog box. Next to do \u00b6 After the tag is created, you can use the Console page to insert vertex data one by one manually or use the Import page to bulk import vertex data.","title":"Operate Tags"},{"location":"nebula-studio/manage-schema/st-ug-crud-tag/#operate_tags","text":"After a graph space is created in Nebula Graph, you can create tags. With Studio, you can use the Console page or the Schema page to create, retrieve, update, or delete tags. This topic introduces how to use the Schema page to operate tags in a graph space only.","title":"Operate tags"},{"location":"nebula-studio/manage-schema/st-ug-crud-tag/#prerequisites","text":"To operate a tag on the Schema page of Studio, you must do a check of these: Studio is connected to Nebula Graph. A graph space is created. Your account has the authority of GOD, ADMIN, or DBA.","title":"Prerequisites"},{"location":"nebula-studio/manage-schema/st-ug-crud-tag/#create_a_tag","text":"In the toolbar, click the Schema tab. In the Graph Space List page, find a graph space and then click its name or click Schema in the Operations column. In the Current Graph Space field, confirm the name of the graph space. If necessary, you can choose another name to change the graph space. Click the Tag tab and click the + Create button. On the Create page, do these settings: Name : Specify an appropriate name for the tag. In this example, course is specified. Comment (Optional): Enter the description for tag. Define Properties (Optional): If necessary, click + Add Property to do these settings: Enter a property name. Select a data type. Select whether to allow null values.. (Optional) Enter the default value. (Optional) Enter the description. Set TTL (Time To Live) (Optional): If no index is set for the tag, you can set the TTL configuration: In the upper left corner of the Set TTL panel, click the check box to expand the panel, and configure TTL_COL and TTL_ DURATION (in seconds). For more information about both parameters, see TTL configuration . When the preceding settings are completed, in the Equivalent to the following nGQL statement panel, you can see the nGQL statement equivalent to these settings. Confirm the settings and then click the + Create button. When the tag is created successfully, the Define Properties panel shows all its properties on the list.","title":"Create a tag"},{"location":"nebula-studio/manage-schema/st-ug-crud-tag/#edit_a_tag","text":"In the toolbar, click the Schema tab. In the Graph Space List page, find a graph space and then click its name or click Schema in the Operations column. In the Current Graph Space field, confirm the name of the graph space. If necessary, you can choose another name to change the graph space. Click the Tag tab, find a tag and then click the button in the Operations column. On the Edit page, do these operations: To edit a Comment: Click Edit on the right of Comment . To edit a property: On the Define Properties panel, find a property, click Edit , and then change the data type or the default value. To delete a property: On the Define Properties panel, find a property, click Delete . To add more properties: On the Define Properties panel, click the Add Property button to add a new property. To set the TTL configuration: In the upper left corner of the Set TTL panel, click the check box and then set TTL. To delete the TTL configuration: When the Set TTL panel is expanded, in the upper left corner of the panel, click the check box to delete the configuration. To edit the TTL configuration: On the Set TTL panel, click Edit and then change the configuration of TTL_COL and TTL_DURATION (in seconds). Note The problem of coexistence of TTL and index, see [TTL]((../../3.ngql-guide/8.clauses-and-options/ttl-options.md).","title":"Edit a tag"},{"location":"nebula-studio/manage-schema/st-ug-crud-tag/#delete_a_tag","text":"Danger Confirm the impact before deleting the tag. The deleted data cannot be restored if it is not backup . In the toolbar, click the Schema tab. In the Graph Space List page, find a graph space and then click its name or click Schema in the Operations column. In the Current Graph Space field, confirm the name of the graph space. If necessary, you can choose another name to change the graph space. Click the Tag tab, find an tag and then click the button in the Operations column. Click OK to confirm delete a tag in the pop-up dialog box.","title":"Delete a tag"},{"location":"nebula-studio/manage-schema/st-ug-crud-tag/#next_to_do","text":"After the tag is created, you can use the Console page to insert vertex data one by one manually or use the Import page to bulk import vertex data.","title":"Next to do"},{"location":"nebula-studio/quick-start/st-ug-console/","text":"Console \u00b6 Studio console interface is shown as follows. The following table lists various functions on the console interface. number function descriptions 1 toolbar Click the Console tab to enter the console page. 2 select a space Select a space in the Current Graph Space list. descriptions : Studio does not support running the USE <space_name> statements directly in the input box. 3 favorites Click the button to expand the favorites, click one of the statements, and the input box will automatically enter the statement. 4 history list Click button representing the statement record. In the statement running record list, click one of the statements, and the statement will be automatically entered in the input box. The list provides the record of the last 15 statements. 5 clean input box Click button to clear the content entered in the input box. 6 run After inputting the nGQL statement in the input box, click button to indicate the operation to start running the statement. 7 custom parameters display Click the button to expand the custom parameters for parameterized query. For details, see Manage parameters . 8 input box After inputting the nGQL statements, click the button to run the statement. You can input multiple statements and run them at the same time, separated by ; . 9 statement running status After running the nGQL statement, the statement running status is displayed. If the statement runs successfully, the statement is displayed in green. If the statement fails, the statement is displayed in red. 10 add to favorites Click the button to save the statement as a favorite, the button for the favorite statement is colored in yellow exhibit. 11 export CSV file or PNG file After running the nGQL statement to return the result, when the result is in Table window, click the button to export as a CSV file. Switch to the Graph window and click the button to save the results as a CSV file or PNG image export. 12 expand/hide execution results Click the button to hide the result or click button to expand the result. 13 close execution results Click the button to close the result returned by this nGQL statement. 14 Table window Display the result from running nGQL statement. If the statement returns results, the window displays the results in a table. 15 Graph window Display the result from running nGQL statement. If the statement returns the complete vertex-edge result, the window displays the result as a graph . Click the button on the right to view the overview panel.","title":"Use Console"},{"location":"nebula-studio/quick-start/st-ug-console/#console","text":"Studio console interface is shown as follows. The following table lists various functions on the console interface. number function descriptions 1 toolbar Click the Console tab to enter the console page. 2 select a space Select a space in the Current Graph Space list. descriptions : Studio does not support running the USE <space_name> statements directly in the input box. 3 favorites Click the button to expand the favorites, click one of the statements, and the input box will automatically enter the statement. 4 history list Click button representing the statement record. In the statement running record list, click one of the statements, and the statement will be automatically entered in the input box. The list provides the record of the last 15 statements. 5 clean input box Click button to clear the content entered in the input box. 6 run After inputting the nGQL statement in the input box, click button to indicate the operation to start running the statement. 7 custom parameters display Click the button to expand the custom parameters for parameterized query. For details, see Manage parameters . 8 input box After inputting the nGQL statements, click the button to run the statement. You can input multiple statements and run them at the same time, separated by ; . 9 statement running status After running the nGQL statement, the statement running status is displayed. If the statement runs successfully, the statement is displayed in green. If the statement fails, the statement is displayed in red. 10 add to favorites Click the button to save the statement as a favorite, the button for the favorite statement is colored in yellow exhibit. 11 export CSV file or PNG file After running the nGQL statement to return the result, when the result is in Table window, click the button to export as a CSV file. Switch to the Graph window and click the button to save the results as a CSV file or PNG image export. 12 expand/hide execution results Click the button to hide the result or click button to expand the result. 13 close execution results Click the button to close the result returned by this nGQL statement. 14 Table window Display the result from running nGQL statement. If the statement returns results, the window displays the results in a table. 15 Graph window Display the result from running nGQL statement. If the statement returns the complete vertex-edge result, the window displays the result as a graph . Click the button on the right to view the overview panel.","title":"Console"},{"location":"nebula-studio/quick-start/st-ug-create-schema/","text":"Create a schema \u00b6 To batch import data into Nebula Graph, you must have a graph schema. You can create a schema on the Console page or on the Schema page of Studio. Note You can use nebula-console to create a schema. For more information, see Nebula Graph Manual and Get started with Nebula Graph . Prerequisites \u00b6 To create a graph schema on Studio, you must do a check of these: Studio is connected to Nebula Graph. Your account has the privilege of GOD, ADMIN, or DBA. The schema is designed. A graph space is created. Note If no graph space exists and your account has the GOD privilege, you can create a graph space on the Console page. For more information, see CREATE SPACE . Create a schema with Schema \u00b6 Create tags. For more information, see Operate tags . Create edge types. For more information, see Operate edge types . Create a schema with Console \u00b6 In the toolbar, click the Console tab. In the Current Graph Space field, choose a graph space name. In this example, basketballplayer is used. In the input box, enter these statements one by one and click the button Run . // To create a tag named \"player\", with two property nebula> CREATE TAG player(name string, age int); // To create a tag named \"team\", with one property nebula> CREATE TAG team(name string); // To create an edge type named \"follow\", with one properties nebula> CREATE EDGE follow(degree int); // To create an edge type named \"serve\", with two properties nebula> CREATE EDGE serve(start_year int, end_year int); If the preceding statements are executed successfully, the schema is created. You can run the statements as follows to view the schema. // To list all the tags in the current graph space nebula> SHOW TAGS; // To list all the edge types in the current graph space nebula> SHOW EDGES; // To view the definition of the tags and edge types DESCRIBE TAG player; DESCRIBE TAG team; DESCRIBE EDGE follow; DESCRIBE EDGE serve; If the schema is created successfully, in the result window, you can see the definition of the tags and edge types. Next to do \u00b6 When a schema is created, you can import data .","title":"Create a schema"},{"location":"nebula-studio/quick-start/st-ug-create-schema/#create_a_schema","text":"To batch import data into Nebula Graph, you must have a graph schema. You can create a schema on the Console page or on the Schema page of Studio. Note You can use nebula-console to create a schema. For more information, see Nebula Graph Manual and Get started with Nebula Graph .","title":"Create a schema"},{"location":"nebula-studio/quick-start/st-ug-create-schema/#prerequisites","text":"To create a graph schema on Studio, you must do a check of these: Studio is connected to Nebula Graph. Your account has the privilege of GOD, ADMIN, or DBA. The schema is designed. A graph space is created. Note If no graph space exists and your account has the GOD privilege, you can create a graph space on the Console page. For more information, see CREATE SPACE .","title":"Prerequisites"},{"location":"nebula-studio/quick-start/st-ug-create-schema/#create_a_schema_with_schema","text":"Create tags. For more information, see Operate tags . Create edge types. For more information, see Operate edge types .","title":"Create a schema with Schema"},{"location":"nebula-studio/quick-start/st-ug-create-schema/#create_a_schema_with_console","text":"In the toolbar, click the Console tab. In the Current Graph Space field, choose a graph space name. In this example, basketballplayer is used. In the input box, enter these statements one by one and click the button Run . // To create a tag named \"player\", with two property nebula> CREATE TAG player(name string, age int); // To create a tag named \"team\", with one property nebula> CREATE TAG team(name string); // To create an edge type named \"follow\", with one properties nebula> CREATE EDGE follow(degree int); // To create an edge type named \"serve\", with two properties nebula> CREATE EDGE serve(start_year int, end_year int); If the preceding statements are executed successfully, the schema is created. You can run the statements as follows to view the schema. // To list all the tags in the current graph space nebula> SHOW TAGS; // To list all the edge types in the current graph space nebula> SHOW EDGES; // To view the definition of the tags and edge types DESCRIBE TAG player; DESCRIBE TAG team; DESCRIBE EDGE follow; DESCRIBE EDGE serve; If the schema is created successfully, in the result window, you can see the definition of the tags and edge types.","title":"Create a schema with Console"},{"location":"nebula-studio/quick-start/st-ug-create-schema/#next_to_do","text":"When a schema is created, you can import data .","title":"Next to do"},{"location":"nebula-studio/quick-start/st-ug-import-data/","text":"Import data \u00b6 After CSV files of data and a schema are created, you can use the Import page to batch import vertex and edge data into Nebula Graph for graph exploration and data analysis. Prerequisites \u00b6 To batch import data, do a check of these: Studio is connected to Nebula Graph. A schema is created. CSV files meet the demands of the Schema. Your account has privilege of GOD, ADMIN, DBA, or USER. Procedure \u00b6 Before importing data, you need to upload the file first and then create the import task. Upload files \u00b6 To upload files, follow these steps: In the toolbar, click the Import tab. On the Upload Files page, click the Upload Files button and then choose CSV files. In this example, edge_serve.csv , edge_follow.csv , vertex_player.csv , and vertex_team.csv are chosen. Note You can choose multiple CSV files at the same time. The CSV file used in this article can be downloaded in the Design a schema . After uploading, you can click the button in the Operations column to preview the file content, or click the button to delete the uploaded file. Import Data \u00b6 To batch import data, follow these steps: In the toolbar, click the Import tab. In Import tab, click the Import Data . On the Import Data page, click + New Import button to complete these operations: Caution users can click Import Template to download the example configuration file example.yaml , and upload the configuration file after configuration. The configuration mode is similar to that of Nebula Importer , but all file paths for configuration files in the template retain the filename only. And make sure all CSV data files are uploaded before importing the YAML file. Select a graph space. Fill in the task name. (Optional) Fill in the batch size. In the Map Vertices section, click the + Bind Datasource button, select bind source file in the dialog box, and click the Confirm button, the vertex_player.csv file is chosen. In the vertices 1 drop-down list, click Select CSV Index , and select the column where vertexID is located in the pop-up dialog box. Click the + Add Tag button and click the icon on the right. In the displayed property list, bind the source data for the tag property. In this example, player is used for the vertex_player.csv file. For the player tag, choose Column 1 for the age property, and choose Column 2 for the name property. In the Map Edges section, click the + Bind Datasource button, select bind source file in the dialog box, and click the Confirm button, the edge_follow.csv file is chosen. In the vertices 1 drop-down list, click Select Edge Type . In this example, follow is chosen. Based on the edge type property, select the corresponding data column from the edge_follow.csv file. srcId and dstId are the VIDs of the source vertex and destination vertex of an edge. In this example, srcId must be set to the VIDs of the player and dstId must be set to the VIDs of another player. Rank is optional. After completing the settings, click the Import button. You need to enter the password of your Nebula account before importing data. After importing data, you can view logs, download logs, download configuration files, and delete tasks on the Import Data tab.","title":"Import data"},{"location":"nebula-studio/quick-start/st-ug-import-data/#import_data","text":"After CSV files of data and a schema are created, you can use the Import page to batch import vertex and edge data into Nebula Graph for graph exploration and data analysis.","title":"Import data"},{"location":"nebula-studio/quick-start/st-ug-import-data/#prerequisites","text":"To batch import data, do a check of these: Studio is connected to Nebula Graph. A schema is created. CSV files meet the demands of the Schema. Your account has privilege of GOD, ADMIN, DBA, or USER.","title":"Prerequisites"},{"location":"nebula-studio/quick-start/st-ug-import-data/#procedure","text":"Before importing data, you need to upload the file first and then create the import task.","title":"Procedure"},{"location":"nebula-studio/quick-start/st-ug-import-data/#upload_files","text":"To upload files, follow these steps: In the toolbar, click the Import tab. On the Upload Files page, click the Upload Files button and then choose CSV files. In this example, edge_serve.csv , edge_follow.csv , vertex_player.csv , and vertex_team.csv are chosen. Note You can choose multiple CSV files at the same time. The CSV file used in this article can be downloaded in the Design a schema . After uploading, you can click the button in the Operations column to preview the file content, or click the button to delete the uploaded file.","title":"Upload files"},{"location":"nebula-studio/quick-start/st-ug-import-data/#import_data_1","text":"To batch import data, follow these steps: In the toolbar, click the Import tab. In Import tab, click the Import Data . On the Import Data page, click + New Import button to complete these operations: Caution users can click Import Template to download the example configuration file example.yaml , and upload the configuration file after configuration. The configuration mode is similar to that of Nebula Importer , but all file paths for configuration files in the template retain the filename only. And make sure all CSV data files are uploaded before importing the YAML file. Select a graph space. Fill in the task name. (Optional) Fill in the batch size. In the Map Vertices section, click the + Bind Datasource button, select bind source file in the dialog box, and click the Confirm button, the vertex_player.csv file is chosen. In the vertices 1 drop-down list, click Select CSV Index , and select the column where vertexID is located in the pop-up dialog box. Click the + Add Tag button and click the icon on the right. In the displayed property list, bind the source data for the tag property. In this example, player is used for the vertex_player.csv file. For the player tag, choose Column 1 for the age property, and choose Column 2 for the name property. In the Map Edges section, click the + Bind Datasource button, select bind source file in the dialog box, and click the Confirm button, the edge_follow.csv file is chosen. In the vertices 1 drop-down list, click Select Edge Type . In this example, follow is chosen. Based on the edge type property, select the corresponding data column from the edge_follow.csv file. srcId and dstId are the VIDs of the source vertex and destination vertex of an edge. In this example, srcId must be set to the VIDs of the player and dstId must be set to the VIDs of another player. Rank is optional. After completing the settings, click the Import button. You need to enter the password of your Nebula account before importing data. After importing data, you can view logs, download logs, download configuration files, and delete tasks on the Import Data tab.","title":"Import Data"},{"location":"nebula-studio/quick-start/st-ug-plan-schema/","text":"Design a schema \u00b6 To manipulate graph data in Nebula Graph with Studio, you must have a graph schema. This article introduces how to design a graph schema for Nebula Graph. A graph schema for Nebula Graph must have these essential elements: Tags (namely vertex types) and their properties. Edge types and their properties. In this article, you can install the sample data set basketballplayer and use it to explore a pre-designed schema. This table gives all the essential elements of the schema. Element Name Property name (Data type) Description Tag player - name ( string ) - age ( int ) Represents the player. Tag team - name ( string ) Represents the team. Edge type serve - start_year ( int ) - end_year ( int ) Represent the players behavior. This behavior connects the player to the team, and the direction is from player to team. Edge type follow - degree ( int ) Represent the players behavior. This behavior connects the player to the player, and the direction is from a player to a player. This figure shows the relationship ( serve / follow ) between a player and a team .","title":"Design a schema"},{"location":"nebula-studio/quick-start/st-ug-plan-schema/#design_a_schema","text":"To manipulate graph data in Nebula Graph with Studio, you must have a graph schema. This article introduces how to design a graph schema for Nebula Graph. A graph schema for Nebula Graph must have these essential elements: Tags (namely vertex types) and their properties. Edge types and their properties. In this article, you can install the sample data set basketballplayer and use it to explore a pre-designed schema. This table gives all the essential elements of the schema. Element Name Property name (Data type) Description Tag player - name ( string ) - age ( int ) Represents the player. Tag team - name ( string ) Represents the team. Edge type serve - start_year ( int ) - end_year ( int ) Represent the players behavior. This behavior connects the player to the team, and the direction is from player to team. Edge type follow - degree ( int ) Represent the players behavior. This behavior connects the player to the player, and the direction is from a player to a player. This figure shows the relationship ( serve / follow ) between a player and a team .","title":"Design a schema"},{"location":"nebula-studio/troubleshooting/st-ug-config-server-errors/","text":"Connecting to the database error \u00b6 Problem description \u00b6 According to the connect Studio operation, it prompts failed . Possible causes and solutions \u00b6 You can troubleshoot the problem by following the steps below. Step1: Confirm that the format of the Host field is correct \u00b6 You must fill in the IP address ( graph_server_ip ) and port of the Nebula Graph database Graph service. If no changes are made, the port defaults to 9669 . Even if Nebula Graph and Studio are deployed on the current machine, you must use the local IP address instead of 127.0.0.1 , localhost or 0.0.0.0 . Step2: Confirm that the username and password are correct \u00b6 If authentication is not enabled, you can use root and any password as the username and its password. If authentication is enabled and different users are created and assigned roles, users in different roles log in with their accounts and passwords. Step3: Confirm that Nebula Graph service is normal \u00b6 Check Nebula Graph service status. Regarding the operation of viewing services: If you compile and deploy Nebula Graph on a Linux server, refer to the Nebula Graph service . If you use Nebula Graph deployed by Docker Compose and RPM, refer to the Nebula Graph service status and ports . If the Nebula Graph service is normal, proceed to Step 4 to continue troubleshooting. Otherwise, please restart Nebula Graph service. Note If you used docker-compose up -d to satrt Nebula Graph before, you must run the docker-compose down to stop Nebula Graph. Step4: Confirm the network connection of the Graph service is normal \u00b6 Run a command (for example, telnet 9669) on the Studio machine to confirm whether Nebula Graph's Graph service network connection is normal. If the connection fails, check according to the following steps: If Studio and Nebula Graph are on the same machine, check if the port is exposed. If Studio and Nebula Graph are not on the same machine, check the network configuration of the Nebula Graph server, such as firewall, gateway, and port. If you cannot connect to the Nebula Graph service after troubleshooting with the above steps, please go to the Nebula Graph forum for consultation.","title":"Database connection error"},{"location":"nebula-studio/troubleshooting/st-ug-config-server-errors/#connecting_to_the_database_error","text":"","title":"Connecting to the database error"},{"location":"nebula-studio/troubleshooting/st-ug-config-server-errors/#problem_description","text":"According to the connect Studio operation, it prompts failed .","title":"Problem description"},{"location":"nebula-studio/troubleshooting/st-ug-config-server-errors/#possible_causes_and_solutions","text":"You can troubleshoot the problem by following the steps below.","title":"Possible causes and solutions"},{"location":"nebula-studio/troubleshooting/st-ug-config-server-errors/#step1_confirm_that_the_format_of_the_host_field_is_correct","text":"You must fill in the IP address ( graph_server_ip ) and port of the Nebula Graph database Graph service. If no changes are made, the port defaults to 9669 . Even if Nebula Graph and Studio are deployed on the current machine, you must use the local IP address instead of 127.0.0.1 , localhost or 0.0.0.0 .","title":"Step1: Confirm that the format of the Host field is correct"},{"location":"nebula-studio/troubleshooting/st-ug-config-server-errors/#step2_confirm_that_the_username_and_password_are_correct","text":"If authentication is not enabled, you can use root and any password as the username and its password. If authentication is enabled and different users are created and assigned roles, users in different roles log in with their accounts and passwords.","title":"Step2: Confirm that the username and password are correct"},{"location":"nebula-studio/troubleshooting/st-ug-config-server-errors/#step3_confirm_that_nebula_graph_service_is_normal","text":"Check Nebula Graph service status. Regarding the operation of viewing services: If you compile and deploy Nebula Graph on a Linux server, refer to the Nebula Graph service . If you use Nebula Graph deployed by Docker Compose and RPM, refer to the Nebula Graph service status and ports . If the Nebula Graph service is normal, proceed to Step 4 to continue troubleshooting. Otherwise, please restart Nebula Graph service. Note If you used docker-compose up -d to satrt Nebula Graph before, you must run the docker-compose down to stop Nebula Graph.","title":"Step3: Confirm that Nebula Graph service is normal"},{"location":"nebula-studio/troubleshooting/st-ug-config-server-errors/#step4_confirm_the_network_connection_of_the_graph_service_is_normal","text":"Run a command (for example, telnet 9669) on the Studio machine to confirm whether Nebula Graph's Graph service network connection is normal. If the connection fails, check according to the following steps: If Studio and Nebula Graph are on the same machine, check if the port is exposed. If Studio and Nebula Graph are not on the same machine, check the network configuration of the Nebula Graph server, such as firewall, gateway, and port. If you cannot connect to the Nebula Graph service after troubleshooting with the above steps, please go to the Nebula Graph forum for consultation.","title":"Step4: Confirm the network connection of the Graph service is normal"},{"location":"nebula-studio/troubleshooting/st-ug-connection-errors/","text":"Cannot access to Studio \u00b6 Problem description \u00b6 I follow the document description and visit 127.0.0.1:7001 or 0.0.0.0:7001 after starting Studio, why can\u2019t I open the page? Possible causes and solutions \u00b6 You can troubleshoot the problem by following the steps below. Step1: Confirm system architecture \u00b6 It is necessary to confirm whether the machine where the Studio service is deployed is of x86_64 architecture. Currently, Studio only supports x86_64 architecture. Step2: Check if the Studio service starts normally \u00b6 For Studio deployed with RPM or DEB packages, use systemctl status nebula-graph-studio to see the running status. For Studio deployed with tar package, use sudo lsof -i:7001 to check port status. For Studio deployed with docker, use docker-compose ps to see the running status. Run docker-compose ps to check if the service has started normally. If the service is normal, the return result is as follows. Among them, the State column should all be displayed as Up . Name Command State Ports ------------------------------------------------------------------------------------------------------ nebula-web-docker_client_1 ./nebula-go-api Up 0 .0.0.0:32782->8080/tcp nebula-web-docker_importer_1 nebula-importer --port = 569 ... Up 0 .0.0.0:32783->5699/tcp nebula-web-docker_nginx_1 /docker-entrypoint.sh ngin ... Up 0 .0.0.0:7001->7001/tcp, 80 /tcp nebula-web-docker_web_1 docker-entrypoint.sh npm r ... Up 0 .0.0.0:32784->7001/tcp If the above result is not returned, stop Studio and restart it first. For details, refer to Deploy Studio . !!! note If you used `docker-compose up -d` to satrt Nebula Graph before, you must run the `docker-compose down` to stop Nebula Graph. Step3: Confirm address \u00b6 If Studio and the browser are on the same machine, users can use localhost:7001 , 127.0.0.1:7001 or 0.0.0.0:7001 in the browser to access Studio. If Studio and the browser are not on the same machine, you must enter <studio_server_ip>:7001 in the browser. Among them, studio_server_ip refers to the IP address of the machine where the Studio service is deployed. Step4: Confirm network connection \u00b6 Run curl <studio_server_ip>:7001 -I to confirm if it is normal. If it returns HTTP/1.1 200 OK , it means that the network is connected normally. If the connection is refused, check according to the following steps: If the connection fails, check according to the following steps: If Studio and Nebula Graph are on the same machine, check if the port is exposed. If Studio and Nebula Graph are not on the same machine, check the network configuration of the Nebula Graph server, such as firewall, gateway, and port. If you cannot connect to the Nebula Graph service after troubleshooting with the above steps, please go to the Nebula Graph forum for consultation.","title":"Unable to access Studio"},{"location":"nebula-studio/troubleshooting/st-ug-connection-errors/#cannot_access_to_studio","text":"","title":"Cannot access to Studio"},{"location":"nebula-studio/troubleshooting/st-ug-connection-errors/#problem_description","text":"I follow the document description and visit 127.0.0.1:7001 or 0.0.0.0:7001 after starting Studio, why can\u2019t I open the page?","title":"Problem description"},{"location":"nebula-studio/troubleshooting/st-ug-connection-errors/#possible_causes_and_solutions","text":"You can troubleshoot the problem by following the steps below.","title":"Possible causes and solutions"},{"location":"nebula-studio/troubleshooting/st-ug-connection-errors/#step1_confirm_system_architecture","text":"It is necessary to confirm whether the machine where the Studio service is deployed is of x86_64 architecture. Currently, Studio only supports x86_64 architecture.","title":"Step1: Confirm system architecture"},{"location":"nebula-studio/troubleshooting/st-ug-connection-errors/#step2_check_if_the_studio_service_starts_normally","text":"For Studio deployed with RPM or DEB packages, use systemctl status nebula-graph-studio to see the running status. For Studio deployed with tar package, use sudo lsof -i:7001 to check port status. For Studio deployed with docker, use docker-compose ps to see the running status. Run docker-compose ps to check if the service has started normally. If the service is normal, the return result is as follows. Among them, the State column should all be displayed as Up . Name Command State Ports ------------------------------------------------------------------------------------------------------ nebula-web-docker_client_1 ./nebula-go-api Up 0 .0.0.0:32782->8080/tcp nebula-web-docker_importer_1 nebula-importer --port = 569 ... Up 0 .0.0.0:32783->5699/tcp nebula-web-docker_nginx_1 /docker-entrypoint.sh ngin ... Up 0 .0.0.0:7001->7001/tcp, 80 /tcp nebula-web-docker_web_1 docker-entrypoint.sh npm r ... Up 0 .0.0.0:32784->7001/tcp If the above result is not returned, stop Studio and restart it first. For details, refer to Deploy Studio . !!! note If you used `docker-compose up -d` to satrt Nebula Graph before, you must run the `docker-compose down` to stop Nebula Graph.","title":"Step2: Check if the Studio service starts normally"},{"location":"nebula-studio/troubleshooting/st-ug-connection-errors/#step3_confirm_address","text":"If Studio and the browser are on the same machine, users can use localhost:7001 , 127.0.0.1:7001 or 0.0.0.0:7001 in the browser to access Studio. If Studio and the browser are not on the same machine, you must enter <studio_server_ip>:7001 in the browser. Among them, studio_server_ip refers to the IP address of the machine where the Studio service is deployed.","title":"Step3: Confirm address"},{"location":"nebula-studio/troubleshooting/st-ug-connection-errors/#step4_confirm_network_connection","text":"Run curl <studio_server_ip>:7001 -I to confirm if it is normal. If it returns HTTP/1.1 200 OK , it means that the network is connected normally. If the connection is refused, check according to the following steps: If the connection fails, check according to the following steps: If Studio and Nebula Graph are on the same machine, check if the port is exposed. If Studio and Nebula Graph are not on the same machine, check the network configuration of the Nebula Graph server, such as firewall, gateway, and port. If you cannot connect to the Nebula Graph service after troubleshooting with the above steps, please go to the Nebula Graph forum for consultation.","title":"Step4: Confirm network connection"},{"location":"nebula-studio/troubleshooting/st-ug-faq/","text":"FAQ \u00b6 Why can't I use a function? If you find that a function cannot be used, it is recommended to troubleshoot the problem according to the following steps: Confirm that Nebula Graph is the latest version. If you use Docker Compose to deploy the Nebula Graph database, it is recommended to run docker-compose pull && docker-compose up -d to pull the latest Docker image and start the container. Confirm that Studio is the latest version. For more information, refer to check updates . Search the nebula forum , nebula and nebula-studio projects on the GitHub to confirm if there are already similar problems. If none of the above steps solve the problem, you can submit a problem on the forum.","title":"FAQ"},{"location":"nebula-studio/troubleshooting/st-ug-faq/#faq","text":"Why can't I use a function? If you find that a function cannot be used, it is recommended to troubleshoot the problem according to the following steps: Confirm that Nebula Graph is the latest version. If you use Docker Compose to deploy the Nebula Graph database, it is recommended to run docker-compose pull && docker-compose up -d to pull the latest Docker image and start the container. Confirm that Studio is the latest version. For more information, refer to check updates . Search the nebula forum , nebula and nebula-studio projects on the GitHub to confirm if there are already similar problems. If none of the above steps solve the problem, you can submit a problem on the forum.","title":"FAQ"},{"location":"reuse/source_connect-to-nebula-graph/","text":"This topic provides basic instruction on how to use the native CLI client Nebula Console to connect to Nebula Graph. Caution When connecting to Nebula Graph for the first time, you must register the Storage Service before querying data. Nebula Graph supports multiple types of clients, including a CLI client, a GUI client, and clients developed in popular programming languages. For more information, see the client list . Prerequisites \u00b6 You have started Nebula Graph services . The machine on which you plan to run Nebula Console has network access to the Graph Service of Nebula Graph. The Nebula Console version is compatible with the Nebula Graph version. Note Nebula Console and Nebula Graph of the same version number are the most compatible. There may be compatibility issues when connecting to Nebula Graph with a different version of Nebula Console. The error message incompatible version between client and server is displayed when there is such an issue. Steps \u00b6 On the Nebula Console releases page , select a Nebula Console version and click Assets . In the Assets area, find the correct binary file for the machine where you want to run Nebula Console and download the file to the machine. (Optional) Rename the binary file to nebula-console for convenience. Note For Windows, rename the file to nebula-console.exe . On the machine to run Nebula Console, grant the execute permission of the nebula-console binary file to the user. Note For Windows, skip this step. $ chmod 111 nebula-console In the command line interface, change the working directory to the one where the nebula-console binary file is stored. Run the following command to connect to Nebula Graph. For Linux or macOS: $ ./nebula-console -addr <ip> -port <port> -u <username> -p <password> [ -t 120 ] [ -e \"nGQL_statement\" | -f filename.nGQL ] For Windows: > nebula-console . exe -addr < ip > -port < port > -u < username > -p < password > [ -t 120 ] [ -e \"nGQL_statement\" | -f filename . nGQL ] Parameter descriptions are as follows: Parameter Description -h/-help Shows the help menu. -addr/-address Sets the IP address of the Graph service. The default address is 127.0.0.1. If Nebula Graph is deployed on Nebula Cloud , you need to create a Private Link and set the IP address of the Private Endpoint as the parameter value. -P/-port Sets the port number of the graphd service. The default port number is 9669. -u/-user Sets the username of your Nebula Graph account. Before enabling authentication, you can use any existing username. The default username is root . -p/-password Sets the password of your Nebula Graph account. Before enabling authentication, you can use any characters as the password. -t/-timeout Sets an integer-type timeout threshold of the connection. The unit is second. The default value is 120. -e/-eval Sets a string-type nGQL statement. The nGQL statement is executed once the connection succeeds. The connection stops after the result is returned. -f/-file Sets the path of an nGQL file. The nGQL statements in the file are executed once the connection succeeds. The result will be returned and the connection stops then. -enable_ssl Enables SSL encryption when connecting to Nebula Graph. -ssl_root_ca_path Sets the storage path of the certification authority file. -ssl_cert_path Sets the storage path of the certificate file. -ssl_private_key_path Sets the storage path of the private key file. For information on more parameters, see the project repository .","title":"Source connect to nebula graph"},{"location":"reuse/source_connect-to-nebula-graph/#prerequisites","text":"You have started Nebula Graph services . The machine on which you plan to run Nebula Console has network access to the Graph Service of Nebula Graph. The Nebula Console version is compatible with the Nebula Graph version. Note Nebula Console and Nebula Graph of the same version number are the most compatible. There may be compatibility issues when connecting to Nebula Graph with a different version of Nebula Console. The error message incompatible version between client and server is displayed when there is such an issue.","title":"Prerequisites"},{"location":"reuse/source_connect-to-nebula-graph/#steps","text":"On the Nebula Console releases page , select a Nebula Console version and click Assets . In the Assets area, find the correct binary file for the machine where you want to run Nebula Console and download the file to the machine. (Optional) Rename the binary file to nebula-console for convenience. Note For Windows, rename the file to nebula-console.exe . On the machine to run Nebula Console, grant the execute permission of the nebula-console binary file to the user. Note For Windows, skip this step. $ chmod 111 nebula-console In the command line interface, change the working directory to the one where the nebula-console binary file is stored. Run the following command to connect to Nebula Graph. For Linux or macOS: $ ./nebula-console -addr <ip> -port <port> -u <username> -p <password> [ -t 120 ] [ -e \"nGQL_statement\" | -f filename.nGQL ] For Windows: > nebula-console . exe -addr < ip > -port < port > -u < username > -p < password > [ -t 120 ] [ -e \"nGQL_statement\" | -f filename . nGQL ] Parameter descriptions are as follows: Parameter Description -h/-help Shows the help menu. -addr/-address Sets the IP address of the Graph service. The default address is 127.0.0.1. If Nebula Graph is deployed on Nebula Cloud , you need to create a Private Link and set the IP address of the Private Endpoint as the parameter value. -P/-port Sets the port number of the graphd service. The default port number is 9669. -u/-user Sets the username of your Nebula Graph account. Before enabling authentication, you can use any existing username. The default username is root . -p/-password Sets the password of your Nebula Graph account. Before enabling authentication, you can use any characters as the password. -t/-timeout Sets an integer-type timeout threshold of the connection. The unit is second. The default value is 120. -e/-eval Sets a string-type nGQL statement. The nGQL statement is executed once the connection succeeds. The connection stops after the result is returned. -f/-file Sets the path of an nGQL file. The nGQL statements in the file are executed once the connection succeeds. The result will be returned and the connection stops then. -enable_ssl Enables SSL encryption when connecting to Nebula Graph. -ssl_root_ca_path Sets the storage path of the certification authority file. -ssl_cert_path Sets the storage path of the certificate file. -ssl_private_key_path Sets the storage path of the private key file. For information on more parameters, see the project repository .","title":"Steps"},{"location":"reuse/source_install-nebula-graph-by-rpm-or-deb/","text":"RPM and DEB are common package formats on Linux systems. This topic shows how to quickly install Nebula Graph with the RPM or DEB package. Prerequisites \u00b6 Prepare the right resources . Note The console is not complied or packaged with Nebula Graph server binaries. You can install nebula-console by yourself. Enterpriseonly For the Enterprise Edition, please send email to inquiry@vesoft.com. Download the package from cloud service \u00b6 Download the released version. URL: //Centos 6 https://oss-cdn.nebula-graph.io/package/<release_version>/nebula-graph-<release_version>.el6.x86_64.rpm //Centos 7 https://oss-cdn.nebula-graph.io/package/<release_version>/nebula-graph-<release_version>.el7.x86_64.rpm //Centos 8 https://oss-cdn.nebula-graph.io/package/<release_version>/nebula-graph-<release_version>.el8.x86_64.rpm //Ubuntu 1604 https://oss-cdn.nebula-graph.io/package/<release_version>/nebula-graph-<release_version>.ubuntu1604.amd64.deb //Ubuntu 1804 https://oss-cdn.nebula-graph.io/package/<release_version>/nebula-graph-<release_version>.ubuntu1804.amd64.deb //Ubuntu 2004 https://oss-cdn.nebula-graph.io/package/<release_version>/nebula-graph-<release_version>.ubuntu2004.amd64.deb For example, download the release package 3.1.0 for Centos 7.5 : wget https://oss-cdn.nebula-graph.io/package/3.1.0/nebula-graph-3.1.0.el7.x86_64.rpm wget https://oss-cdn.nebula-graph.io/package/3.1.0/nebula-graph-3.1.0.el7.x86_64.rpm.sha256sum.txt Download the release package 3.1.0 for Ubuntu 1804 : wget https://oss-cdn.nebula-graph.io/package/3.1.0/nebula-graph-3.1.0.ubuntu1804.amd64.deb wget https://oss-cdn.nebula-graph.io/package/3.1.0/nebula-graph-3.1.0.ubuntu1804.amd64.deb.sha256sum.txt Download the nightly version. Danger Nightly versions are usually used to test new features. Do not use it in a production environment. Nightly versions may not be built successfully every night. And the names may change from day to day. URL: //Centos 6 https://oss-cdn.nebula-graph.io/package/nightly/<yyyy.mm.dd>/nebula-graph-<yyyy.mm.dd>-nightly.el6.x86_64.rpm //Centos 7 https://oss-cdn.nebula-graph.io/package/nightly/<yyyy.mm.dd>/nebula-graph-<yyyy.mm.dd>-nightly.el7.x86_64.rpm //Centos 8 https://oss-cdn.nebula-graph.io/package/nightly/<yyyy.mm.dd>/nebula-graph-<yyyy.mm.dd>-nightly.el8.x86_64.rpm //Ubuntu 1604 https://oss-cdn.nebula-graph.io/package/nightly/<yyyy.mm.dd>/nebula-graph-<yyyy.mm.dd>-nightly.ubuntu1604.amd64.deb //Ubuntu 1804 https://oss-cdn.nebula-graph.io/package/nightly/<yyyy.mm.dd>/nebula-graph-<yyyy.mm.dd>-nightly.ubuntu1804.amd64.deb //Ubuntu 2004 https://oss-cdn.nebula-graph.io/package/nightly/<yyyy.mm.dd>/nebula-graph-<yyyy.mm.dd>-nightly.ubuntu2004.amd64.deb For example, download the Centos 7.5 package developed and built in 2021.11.28 : wget https://oss-cdn.nebula-graph.io/package/nightly/2021.11.28/nebula-graph-2021.11.28-nightly.el7.x86_64.rpm wget https://oss-cdn.nebula-graph.io/package/nightly/2021.11.28/nebula-graph-2021.11.28-nightly.el7.x86_64.rpm.sha256sum.txt For example, download the Ubuntu 1804 package developed and built in 2021.11.28 : wget https://oss-cdn.nebula-graph.io/package/nightly/2021.11.28/nebula-graph-2021.11.28-nightly.ubuntu1804.amd64.deb wget https://oss-cdn.nebula-graph.io/package/nightly/2021.11.28/nebula-graph-2021.11.28-nightly.ubuntu1804.amd64.deb.sha256sum.txt Install Nebula Graph \u00b6 Use the following syntax to install with an RPM package. $ sudo rpm -ivh --prefix = <installation_path> <package_name> The option --prefix indicates the installation path. The default path is /usr/local/nebula/ . For example, to install an RPM package in the default path for the 3.1.0 version, run the following command. sudo rpm -ivh nebula-graph-3.1.0.el7.x86_64.rpm Use the following syntax to install with a DEB package. $ sudo dpkg -i <package_name> Note Customizing the installation path is not supported when installing Nebula Graph with a DEB package. The default installation path is /usr/local/nebula/ . For example, to install a DEB package for the 3.1.0 version, run the following command. sudo dpkg -i nebula-graph-3.1.0.ubuntu1804.amd64.deb Note The default installation path is /usr/local/nebula/ . Next to do \u00b6 (Enterprise Edition) Deploy license Start Nebula Graph Connect to Nebula Graph","title":"Source install nebula graph by rpm or deb"},{"location":"reuse/source_install-nebula-graph-by-rpm-or-deb/#prerequisites","text":"Prepare the right resources . Note The console is not complied or packaged with Nebula Graph server binaries. You can install nebula-console by yourself. Enterpriseonly For the Enterprise Edition, please send email to inquiry@vesoft.com.","title":"Prerequisites"},{"location":"reuse/source_install-nebula-graph-by-rpm-or-deb/#download_the_package_from_cloud_service","text":"Download the released version. URL: //Centos 6 https://oss-cdn.nebula-graph.io/package/<release_version>/nebula-graph-<release_version>.el6.x86_64.rpm //Centos 7 https://oss-cdn.nebula-graph.io/package/<release_version>/nebula-graph-<release_version>.el7.x86_64.rpm //Centos 8 https://oss-cdn.nebula-graph.io/package/<release_version>/nebula-graph-<release_version>.el8.x86_64.rpm //Ubuntu 1604 https://oss-cdn.nebula-graph.io/package/<release_version>/nebula-graph-<release_version>.ubuntu1604.amd64.deb //Ubuntu 1804 https://oss-cdn.nebula-graph.io/package/<release_version>/nebula-graph-<release_version>.ubuntu1804.amd64.deb //Ubuntu 2004 https://oss-cdn.nebula-graph.io/package/<release_version>/nebula-graph-<release_version>.ubuntu2004.amd64.deb For example, download the release package 3.1.0 for Centos 7.5 : wget https://oss-cdn.nebula-graph.io/package/3.1.0/nebula-graph-3.1.0.el7.x86_64.rpm wget https://oss-cdn.nebula-graph.io/package/3.1.0/nebula-graph-3.1.0.el7.x86_64.rpm.sha256sum.txt Download the release package 3.1.0 for Ubuntu 1804 : wget https://oss-cdn.nebula-graph.io/package/3.1.0/nebula-graph-3.1.0.ubuntu1804.amd64.deb wget https://oss-cdn.nebula-graph.io/package/3.1.0/nebula-graph-3.1.0.ubuntu1804.amd64.deb.sha256sum.txt Download the nightly version. Danger Nightly versions are usually used to test new features. Do not use it in a production environment. Nightly versions may not be built successfully every night. And the names may change from day to day. URL: //Centos 6 https://oss-cdn.nebula-graph.io/package/nightly/<yyyy.mm.dd>/nebula-graph-<yyyy.mm.dd>-nightly.el6.x86_64.rpm //Centos 7 https://oss-cdn.nebula-graph.io/package/nightly/<yyyy.mm.dd>/nebula-graph-<yyyy.mm.dd>-nightly.el7.x86_64.rpm //Centos 8 https://oss-cdn.nebula-graph.io/package/nightly/<yyyy.mm.dd>/nebula-graph-<yyyy.mm.dd>-nightly.el8.x86_64.rpm //Ubuntu 1604 https://oss-cdn.nebula-graph.io/package/nightly/<yyyy.mm.dd>/nebula-graph-<yyyy.mm.dd>-nightly.ubuntu1604.amd64.deb //Ubuntu 1804 https://oss-cdn.nebula-graph.io/package/nightly/<yyyy.mm.dd>/nebula-graph-<yyyy.mm.dd>-nightly.ubuntu1804.amd64.deb //Ubuntu 2004 https://oss-cdn.nebula-graph.io/package/nightly/<yyyy.mm.dd>/nebula-graph-<yyyy.mm.dd>-nightly.ubuntu2004.amd64.deb For example, download the Centos 7.5 package developed and built in 2021.11.28 : wget https://oss-cdn.nebula-graph.io/package/nightly/2021.11.28/nebula-graph-2021.11.28-nightly.el7.x86_64.rpm wget https://oss-cdn.nebula-graph.io/package/nightly/2021.11.28/nebula-graph-2021.11.28-nightly.el7.x86_64.rpm.sha256sum.txt For example, download the Ubuntu 1804 package developed and built in 2021.11.28 : wget https://oss-cdn.nebula-graph.io/package/nightly/2021.11.28/nebula-graph-2021.11.28-nightly.ubuntu1804.amd64.deb wget https://oss-cdn.nebula-graph.io/package/nightly/2021.11.28/nebula-graph-2021.11.28-nightly.ubuntu1804.amd64.deb.sha256sum.txt","title":"Download the package from cloud service"},{"location":"reuse/source_install-nebula-graph-by-rpm-or-deb/#install_nebula_graph","text":"Use the following syntax to install with an RPM package. $ sudo rpm -ivh --prefix = <installation_path> <package_name> The option --prefix indicates the installation path. The default path is /usr/local/nebula/ . For example, to install an RPM package in the default path for the 3.1.0 version, run the following command. sudo rpm -ivh nebula-graph-3.1.0.el7.x86_64.rpm Use the following syntax to install with a DEB package. $ sudo dpkg -i <package_name> Note Customizing the installation path is not supported when installing Nebula Graph with a DEB package. The default installation path is /usr/local/nebula/ . For example, to install a DEB package for the 3.1.0 version, run the following command. sudo dpkg -i nebula-graph-3.1.0.ubuntu1804.amd64.deb Note The default installation path is /usr/local/nebula/ .","title":"Install Nebula Graph"},{"location":"reuse/source_install-nebula-graph-by-rpm-or-deb/#next_to_do","text":"(Enterprise Edition) Deploy license Start Nebula Graph Connect to Nebula Graph","title":"Next to do"},{"location":"reuse/source_manage-service/","text":"Nebula Graph supports managing services with scripts or systemd. This topic will describe the two methods in detail. Enterpriseonly Managing Nebula Graph with systemd is only available in the Nebula Graph Enterprise Edition. Danger The two methods are incompatible. It is recommended to use only one method in a cluster. Manage services with script \u00b6 You can use the nebula.service script to start, stop, restart, terminate, and check the Nebula Graph services. Note nebula.service is stored in the /usr/local/nebula/scripts directory by default. If you have customized the path, use the actual path in your environment. Syntax \u00b6 $ sudo /usr/local/nebula/scripts/nebula.service [ -v ] [ -c <config_file_path> ] <start | stop | restart | kill | status> <metad | graphd | storaged | all> Parameter Description -v Display detailed debugging information. -c Specify the configuration file path. The default path is /usr/local/nebula/etc/ . start Start the target services. stop Stop the target services. restart Restart the target services. kill Terminate the target services. status Check the status of the target services. metad Set the Meta Service as the target service. graphd Set the Graph Service as the target service. storaged Set the Storage Service as the target service. all Set all the Nebula Graph services as the target services. Manage services with systemd \u00b6 For easy maintenance, Nebula Graph supports managing services with systemd. You can start, stop, restart, and check services with systemctl commands. Note After installing Nebula Graph, the .service files required by systemd are located in the etc/unit path in the installation directory. Nebula Graph installed with the RPM/DEB package automatically places the .service files into the path /usr/lib/systemd/system and the parameter ExecStart is generated based on the specified Nebula Graph installation path, so you can use systemctl commands directly. Otherwise, users need to move the .service files manually into the directory /usr/lib/systemd/system , and modify the file path of the parameter ExecStart in the .service files. Syntax \u00b6 $ systemctl <start | stop | restart | status > <nebula | nebula-metad | nebula-graphd | nebula-storaged> Parameter Description start Start the target services. stop Stop the target services. restart Restart the target services. status Check the status of the target services. nebula Set all the Nebula Graph services as the target services. nebula-metad Set the Meta Service as the target service. nebula-graphd Set the Graph Service as the target service. nebula-storaged Set the Storage Service as the target service. Start Nebula Graph \u00b6 In non-container environment \u00b6 Run the following command to start Nebula Graph. $ sudo /usr/local/nebula/scripts/nebula.service start all [ INFO ] Starting nebula-metad... [ INFO ] Done [ INFO ] Starting nebula-graphd... [ INFO ] Done [ INFO ] Starting nebula-storaged... [ INFO ] Done Users can also run the following command: $ systemctl start nebula If users want to automatically start Nebula Graph when the machine starts, run the following command: $ systemctl enable nebula In docker container (deployed with docker-compose) \u00b6 Run the following command in the nebula-docker-compose/ directory to start Nebula Graph. [ nebula-docker-compose ] $ docker-compose up -d Building with native build. Learn about native build in Compose here: https://docs.docker.com/go/compose-native-build/ Creating network \"nebula-docker-compose_nebula-net\" with the default driver Creating nebula-docker-compose_metad0_1 ... done Creating nebula-docker-compose_metad2_1 ... done Creating nebula-docker-compose_metad1_1 ... done Creating nebula-docker-compose_storaged2_1 ... done Creating nebula-docker-compose_graphd1_1 ... done Creating nebula-docker-compose_storaged1_1 ... done Creating nebula-docker-compose_storaged0_1 ... done Creating nebula-docker-compose_graphd2_1 ... done Creating nebula-docker-compose_graphd_1 ... done Stop Nebula Graph \u00b6 Danger Do not run kill -9 to forcibly terminate the processes. Otherwise, there is a low probability of data loss. In non-container environment \u00b6 Run the following command to stop Nebula Graph. $ sudo /usr/local/nebula/scripts/nebula.service stop all [ INFO ] Stopping nebula-metad... [ INFO ] Done [ INFO ] Stopping nebula-graphd... [ INFO ] Done [ INFO ] Stopping nebula-storaged... [ INFO ] Done Users can also run the following command: $ systemctl stop nebula In docker container (deployed with docker-compose) \u00b6 Run the following command in the nebula-docker-compose/ directory to stop Nebula Graph. nebula-docker-compose ] $ docker-compose down Stopping nebula-docker-compose_graphd_1 ... done Stopping nebula-docker-compose_graphd2_1 ... done Stopping nebula-docker-compose_storaged0_1 ... done Stopping nebula-docker-compose_storaged1_1 ... done Stopping nebula-docker-compose_graphd1_1 ... done Stopping nebula-docker-compose_storaged2_1 ... done Stopping nebula-docker-compose_metad1_1 ... done Stopping nebula-docker-compose_metad2_1 ... done Stopping nebula-docker-compose_metad0_1 ... done Removing nebula-docker-compose_graphd_1 ... done Removing nebula-docker-compose_graphd2_1 ... done Removing nebula-docker-compose_storaged0_1 ... done Removing nebula-docker-compose_storaged1_1 ... done Removing nebula-docker-compose_graphd1_1 ... done Removing nebula-docker-compose_storaged2_1 ... done Removing nebula-docker-compose_metad1_1 ... done Removing nebula-docker-compose_metad2_1 ... done Removing nebula-docker-compose_metad0_1 ... done Removing network nebula-docker-compose_nebula-net Note If you are using a developing or nightly version for testing and have compatibility issues, try to run docker-compose down -v to DELETE all data stored in Nebula Graph and import data again. Check the service status \u00b6 In non-container environment \u00b6 Run the following command to check the service status of Nebula Graph. $ sudo /usr/local/nebula/scripts/nebula.service status all Nebula Graph is running normally if the following information is returned. [ INFO ] nebula-metad ( 02b2091 ) : Running as 26601 , Listening on 9559 [ INFO ] nebula-graphd ( 02b2091 ) : Running as 26644 , Listening on 9669 [ INFO ] nebula-storaged ( 02b2091 ) : Running as 26709 , Listening on 9779 Note After starting Nebula Graph, the port of the nebula-storaged process is shown in red. Because the nebula-storaged process waits for the nebula-metad to add the current Storage service during the startup process. The Storage works after it receives the ready signal. Starting from Nebula Graph 3.0.0, the Meta service cannot directly read or write data in the Storage service that you add in the configuration file. The configuration file only registers the Storage service to the Meta service. You must run the ADD HOSTS command to enable the Meta to read and write data in the Storage service. For more information, see Manage Storage hosts . If the returned result is similar to the following one, there is a problem. You may also go to the Nebula Graph community for help. [ INFO ] nebula-metad: Running as 25600 , Listening on 9559 [ INFO ] nebula-graphd: Exited [ INFO ] nebula-storaged: Running as 25646 , Listening on 9779 Users can also run the following command: $ systemctl status nebula \u25cf nebula.service Loaded: loaded ( /usr/lib/systemd/system/nebula.service ; disabled ; vendor preset: disabled ) Active: active ( exited ) since \u4e00 2022 -03-28 04 :13:24 UTC ; 1h 47min ago Process: 21772 ExecStart = /usr/local/ent-nightly/scripts/nebula.service start all ( code = exited, status = 0 /SUCCESS ) Main PID: 21772 ( code = exited, status = 0 /SUCCESS ) Tasks: 325 Memory: 424 .5M CGroup: /system.slice/nebula.service \u251c\u250021789 /usr/local/ent-nightly/bin/nebula-metad --flagfile /usr/local/ent-nightly/etc/nebula-metad.conf \u251c\u250021827 /usr/local/ent-nightly/bin/nebula-graphd --flagfile /usr/local/ent-nightly/etc/nebula-graphd.conf \u2514\u250021900 /usr/local/ent-nightly/bin/nebula-storaged --flagfile /usr/local/ent-nightly/etc/nebula-storaged.conf 3\u6708 28 04 :13:24 xxxxxx systemd [ 1 ] : Started nebula.service. ... The Nebula Graph services consist of the Meta Service, Graph Service, and Storage Service. The configuration files for all three services are stored in the /usr/local/nebula/etc/ directory by default. You can check the configuration files according to the returned result to troubleshoot problems. In docker container (deployed with docker-compose) \u00b6 Run the following command in the nebula-docker-compose/ directory to check the service status of Nebula Graph. nebula-docker-compose ] $ docker-compose ps Name Command State Ports --------------------------------------------------------------------------------------------------------------------------------------------------------------------- nebula-docker-compose_graphd1_1 /usr/local/nebula/bin/nebu ... Up ( healthy ) 0 .0.0.0:49223->19669/tcp, 0 .0.0.0:49222->19670/tcp, 0 .0.0.0:49224->9669/tcp nebula-docker-compose_graphd2_1 /usr/local/nebula/bin/nebu ... Up ( healthy ) 0 .0.0.0:49229->19669/tcp, 0 .0.0.0:49228->19670/tcp, 0 .0.0.0:49230->9669/tcp nebula-docker-compose_graphd_1 /usr/local/nebula/bin/nebu ... Up ( healthy ) 0 .0.0.0:49221->19669/tcp, 0 .0.0.0:49220->19670/tcp, 0 .0.0.0:9669->9669/tcp nebula-docker-compose_metad0_1 ./bin/nebula-metad --flagf ... Up ( healthy ) 0 .0.0.0:49212->19559/tcp, 0 .0.0.0:49211->19560/tcp, 0 .0.0.0:49213->9559/tcp, 9560 /tcp nebula-docker-compose_metad1_1 ./bin/nebula-metad --flagf ... Up ( healthy ) 0 .0.0.0:49209->19559/tcp, 0 .0.0.0:49208->19560/tcp, 0 .0.0.0:49210->9559/tcp, 9560 /tcp nebula-docker-compose_metad2_1 ./bin/nebula-metad --flagf ... Up ( healthy ) 0 .0.0.0:49206->19559/tcp, 0 .0.0.0:49205->19560/tcp, 0 .0.0.0:49207->9559/tcp, 9560 /tcp nebula-docker-compose_storaged0_1 ./bin/nebula-storaged --fl ... Up ( healthy ) 0 .0.0.0:49218->19779/tcp, 0 .0.0.0:49217->19780/tcp, 9777 /tcp, 9778 /tcp, 0 .0.0.0:49219->9779/tcp, 9780 /tcp nebula-docker-compose_storaged1_1 ./bin/nebula-storaged --fl ... Up ( healthy ) 0 .0.0.0:49215->19779/tcp, 0 .0.0.0:49214->19780/tcp, 9777 /tcp, 9778 /tcp, 0 .0.0.0:49216->9779/tcp, 9780 /tcp nebula-docker-compose_storaged2_1 ./bin/nebula-storaged --fl ... Up ( healthy ) 0 .0.0.0:49226->19779/tcp, 0 .0.0.0:49225->19780/tcp, 9777 /tcp, 9778 /tcp, 0 .0.0.0:49227->9779/tcp, 9780 /tcp If the service is abnormal, you can first confirm the abnormal container name (such as nebula-docker-compose_graphd2_1 ). Then you can execute docker ps to view the corresponding CONTAINER ID (such as 2a6c56c405f5 ). [ nebula-docker-compose ] $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 2a6c56c405f5 vesoft/nebula-graphd:nightly \"/usr/local/nebula/b\u2026\" 36 minutes ago Up 36 minutes ( healthy ) 0 .0.0.0:49230->9669/tcp, 0 .0.0.0:49229->19669/tcp, 0 .0.0.0:49228->19670/tcp nebula-docker-compose_graphd2_1 7042e0a8e83d vesoft/nebula-storaged:nightly \"./bin/nebula-storag\u2026\" 36 minutes ago Up 36 minutes ( healthy ) 9777 -9778/tcp, 9780 /tcp, 0 .0.0.0:49227->9779/tcp, 0 .0.0.0:49226->19779/tcp, 0 .0.0.0:49225->19780/tcp nebula-docker-compose_storaged2_1 18e3ea63ad65 vesoft/nebula-storaged:nightly \"./bin/nebula-storag\u2026\" 36 minutes ago Up 36 minutes ( healthy ) 9777 -9778/tcp, 9780 /tcp, 0 .0.0.0:49219->9779/tcp, 0 .0.0.0:49218->19779/tcp, 0 .0.0.0:49217->19780/tcp nebula-docker-compose_storaged0_1 4dcabfe8677a vesoft/nebula-graphd:nightly \"/usr/local/nebula/b\u2026\" 36 minutes ago Up 36 minutes ( healthy ) 0 .0.0.0:49224->9669/tcp, 0 .0.0.0:49223->19669/tcp, 0 .0.0.0:49222->19670/tcp nebula-docker-compose_graphd1_1 a74054c6ae25 vesoft/nebula-graphd:nightly \"/usr/local/nebula/b\u2026\" 36 minutes ago Up 36 minutes ( healthy ) 0 .0.0.0:9669->9669/tcp, 0 .0.0.0:49221->19669/tcp, 0 .0.0.0:49220->19670/tcp nebula-docker-compose_graphd_1 880025a3858c vesoft/nebula-storaged:nightly \"./bin/nebula-storag\u2026\" 36 minutes ago Up 36 minutes ( healthy ) 9777 -9778/tcp, 9780 /tcp, 0 .0.0.0:49216->9779/tcp, 0 .0.0.0:49215->19779/tcp, 0 .0.0.0:49214->19780/tcp nebula-docker-compose_storaged1_1 45736a32a23a vesoft/nebula-metad:nightly \"./bin/nebula-metad \u2026\" 36 minutes ago Up 36 minutes ( healthy ) 9560 /tcp, 0 .0.0.0:49213->9559/tcp, 0 .0.0.0:49212->19559/tcp, 0 .0.0.0:49211->19560/tcp nebula-docker-compose_metad0_1 3b2c90eb073e vesoft/nebula-metad:nightly \"./bin/nebula-metad \u2026\" 36 minutes ago Up 36 minutes ( healthy ) 9560 /tcp, 0 .0.0.0:49207->9559/tcp, 0 .0.0.0:49206->19559/tcp, 0 .0.0.0:49205->19560/tcp nebula-docker-compose_metad2_1 7bb31b7a5b3f vesoft/nebula-metad:nightly \"./bin/nebula-metad \u2026\" 36 minutes ago Up 36 minutes ( healthy ) 9560 /tcp, 0 .0.0.0:49210->9559/tcp, 0 .0.0.0:49209->19559/tcp, 0 .0.0.0:49208->19560/tcp nebula-docker-compose_metad1_1 Use the CONTAINER ID to log in the container and troubleshoot. nebula-docker-compose ] $ docker exec -it 2a6c56c405f5 bash [ root@2a6c56c405f5 nebula ] # Next to do \u00b6 Connect to Nebula Graph","title":"Source manage service"},{"location":"reuse/source_manage-service/#manage_services_with_script","text":"You can use the nebula.service script to start, stop, restart, terminate, and check the Nebula Graph services. Note nebula.service is stored in the /usr/local/nebula/scripts directory by default. If you have customized the path, use the actual path in your environment.","title":"Manage services with script"},{"location":"reuse/source_manage-service/#syntax","text":"$ sudo /usr/local/nebula/scripts/nebula.service [ -v ] [ -c <config_file_path> ] <start | stop | restart | kill | status> <metad | graphd | storaged | all> Parameter Description -v Display detailed debugging information. -c Specify the configuration file path. The default path is /usr/local/nebula/etc/ . start Start the target services. stop Stop the target services. restart Restart the target services. kill Terminate the target services. status Check the status of the target services. metad Set the Meta Service as the target service. graphd Set the Graph Service as the target service. storaged Set the Storage Service as the target service. all Set all the Nebula Graph services as the target services.","title":"Syntax"},{"location":"reuse/source_manage-service/#manage_services_with_systemd","text":"For easy maintenance, Nebula Graph supports managing services with systemd. You can start, stop, restart, and check services with systemctl commands. Note After installing Nebula Graph, the .service files required by systemd are located in the etc/unit path in the installation directory. Nebula Graph installed with the RPM/DEB package automatically places the .service files into the path /usr/lib/systemd/system and the parameter ExecStart is generated based on the specified Nebula Graph installation path, so you can use systemctl commands directly. Otherwise, users need to move the .service files manually into the directory /usr/lib/systemd/system , and modify the file path of the parameter ExecStart in the .service files.","title":"Manage services with systemd"},{"location":"reuse/source_manage-service/#syntax_1","text":"$ systemctl <start | stop | restart | status > <nebula | nebula-metad | nebula-graphd | nebula-storaged> Parameter Description start Start the target services. stop Stop the target services. restart Restart the target services. status Check the status of the target services. nebula Set all the Nebula Graph services as the target services. nebula-metad Set the Meta Service as the target service. nebula-graphd Set the Graph Service as the target service. nebula-storaged Set the Storage Service as the target service.","title":"Syntax"},{"location":"reuse/source_manage-service/#start_nebula_graph","text":"","title":"Start Nebula Graph"},{"location":"reuse/source_manage-service/#in_non-container_environment","text":"Run the following command to start Nebula Graph. $ sudo /usr/local/nebula/scripts/nebula.service start all [ INFO ] Starting nebula-metad... [ INFO ] Done [ INFO ] Starting nebula-graphd... [ INFO ] Done [ INFO ] Starting nebula-storaged... [ INFO ] Done Users can also run the following command: $ systemctl start nebula If users want to automatically start Nebula Graph when the machine starts, run the following command: $ systemctl enable nebula","title":"In non-container environment"},{"location":"reuse/source_manage-service/#in_docker_container_deployed_with_docker-compose","text":"Run the following command in the nebula-docker-compose/ directory to start Nebula Graph. [ nebula-docker-compose ] $ docker-compose up -d Building with native build. Learn about native build in Compose here: https://docs.docker.com/go/compose-native-build/ Creating network \"nebula-docker-compose_nebula-net\" with the default driver Creating nebula-docker-compose_metad0_1 ... done Creating nebula-docker-compose_metad2_1 ... done Creating nebula-docker-compose_metad1_1 ... done Creating nebula-docker-compose_storaged2_1 ... done Creating nebula-docker-compose_graphd1_1 ... done Creating nebula-docker-compose_storaged1_1 ... done Creating nebula-docker-compose_storaged0_1 ... done Creating nebula-docker-compose_graphd2_1 ... done Creating nebula-docker-compose_graphd_1 ... done","title":"In docker container (deployed with docker-compose)"},{"location":"reuse/source_manage-service/#stop_nebula_graph","text":"Danger Do not run kill -9 to forcibly terminate the processes. Otherwise, there is a low probability of data loss.","title":"Stop Nebula Graph"},{"location":"reuse/source_manage-service/#in_non-container_environment_1","text":"Run the following command to stop Nebula Graph. $ sudo /usr/local/nebula/scripts/nebula.service stop all [ INFO ] Stopping nebula-metad... [ INFO ] Done [ INFO ] Stopping nebula-graphd... [ INFO ] Done [ INFO ] Stopping nebula-storaged... [ INFO ] Done Users can also run the following command: $ systemctl stop nebula","title":"In non-container environment"},{"location":"reuse/source_manage-service/#in_docker_container_deployed_with_docker-compose_1","text":"Run the following command in the nebula-docker-compose/ directory to stop Nebula Graph. nebula-docker-compose ] $ docker-compose down Stopping nebula-docker-compose_graphd_1 ... done Stopping nebula-docker-compose_graphd2_1 ... done Stopping nebula-docker-compose_storaged0_1 ... done Stopping nebula-docker-compose_storaged1_1 ... done Stopping nebula-docker-compose_graphd1_1 ... done Stopping nebula-docker-compose_storaged2_1 ... done Stopping nebula-docker-compose_metad1_1 ... done Stopping nebula-docker-compose_metad2_1 ... done Stopping nebula-docker-compose_metad0_1 ... done Removing nebula-docker-compose_graphd_1 ... done Removing nebula-docker-compose_graphd2_1 ... done Removing nebula-docker-compose_storaged0_1 ... done Removing nebula-docker-compose_storaged1_1 ... done Removing nebula-docker-compose_graphd1_1 ... done Removing nebula-docker-compose_storaged2_1 ... done Removing nebula-docker-compose_metad1_1 ... done Removing nebula-docker-compose_metad2_1 ... done Removing nebula-docker-compose_metad0_1 ... done Removing network nebula-docker-compose_nebula-net Note If you are using a developing or nightly version for testing and have compatibility issues, try to run docker-compose down -v to DELETE all data stored in Nebula Graph and import data again.","title":"In docker container (deployed with docker-compose)"},{"location":"reuse/source_manage-service/#check_the_service_status","text":"","title":"Check the service status"},{"location":"reuse/source_manage-service/#in_non-container_environment_2","text":"Run the following command to check the service status of Nebula Graph. $ sudo /usr/local/nebula/scripts/nebula.service status all Nebula Graph is running normally if the following information is returned. [ INFO ] nebula-metad ( 02b2091 ) : Running as 26601 , Listening on 9559 [ INFO ] nebula-graphd ( 02b2091 ) : Running as 26644 , Listening on 9669 [ INFO ] nebula-storaged ( 02b2091 ) : Running as 26709 , Listening on 9779 Note After starting Nebula Graph, the port of the nebula-storaged process is shown in red. Because the nebula-storaged process waits for the nebula-metad to add the current Storage service during the startup process. The Storage works after it receives the ready signal. Starting from Nebula Graph 3.0.0, the Meta service cannot directly read or write data in the Storage service that you add in the configuration file. The configuration file only registers the Storage service to the Meta service. You must run the ADD HOSTS command to enable the Meta to read and write data in the Storage service. For more information, see Manage Storage hosts . If the returned result is similar to the following one, there is a problem. You may also go to the Nebula Graph community for help. [ INFO ] nebula-metad: Running as 25600 , Listening on 9559 [ INFO ] nebula-graphd: Exited [ INFO ] nebula-storaged: Running as 25646 , Listening on 9779 Users can also run the following command: $ systemctl status nebula \u25cf nebula.service Loaded: loaded ( /usr/lib/systemd/system/nebula.service ; disabled ; vendor preset: disabled ) Active: active ( exited ) since \u4e00 2022 -03-28 04 :13:24 UTC ; 1h 47min ago Process: 21772 ExecStart = /usr/local/ent-nightly/scripts/nebula.service start all ( code = exited, status = 0 /SUCCESS ) Main PID: 21772 ( code = exited, status = 0 /SUCCESS ) Tasks: 325 Memory: 424 .5M CGroup: /system.slice/nebula.service \u251c\u250021789 /usr/local/ent-nightly/bin/nebula-metad --flagfile /usr/local/ent-nightly/etc/nebula-metad.conf \u251c\u250021827 /usr/local/ent-nightly/bin/nebula-graphd --flagfile /usr/local/ent-nightly/etc/nebula-graphd.conf \u2514\u250021900 /usr/local/ent-nightly/bin/nebula-storaged --flagfile /usr/local/ent-nightly/etc/nebula-storaged.conf 3\u6708 28 04 :13:24 xxxxxx systemd [ 1 ] : Started nebula.service. ... The Nebula Graph services consist of the Meta Service, Graph Service, and Storage Service. The configuration files for all three services are stored in the /usr/local/nebula/etc/ directory by default. You can check the configuration files according to the returned result to troubleshoot problems.","title":"In non-container environment"},{"location":"reuse/source_manage-service/#in_docker_container_deployed_with_docker-compose_2","text":"Run the following command in the nebula-docker-compose/ directory to check the service status of Nebula Graph. nebula-docker-compose ] $ docker-compose ps Name Command State Ports --------------------------------------------------------------------------------------------------------------------------------------------------------------------- nebula-docker-compose_graphd1_1 /usr/local/nebula/bin/nebu ... Up ( healthy ) 0 .0.0.0:49223->19669/tcp, 0 .0.0.0:49222->19670/tcp, 0 .0.0.0:49224->9669/tcp nebula-docker-compose_graphd2_1 /usr/local/nebula/bin/nebu ... Up ( healthy ) 0 .0.0.0:49229->19669/tcp, 0 .0.0.0:49228->19670/tcp, 0 .0.0.0:49230->9669/tcp nebula-docker-compose_graphd_1 /usr/local/nebula/bin/nebu ... Up ( healthy ) 0 .0.0.0:49221->19669/tcp, 0 .0.0.0:49220->19670/tcp, 0 .0.0.0:9669->9669/tcp nebula-docker-compose_metad0_1 ./bin/nebula-metad --flagf ... Up ( healthy ) 0 .0.0.0:49212->19559/tcp, 0 .0.0.0:49211->19560/tcp, 0 .0.0.0:49213->9559/tcp, 9560 /tcp nebula-docker-compose_metad1_1 ./bin/nebula-metad --flagf ... Up ( healthy ) 0 .0.0.0:49209->19559/tcp, 0 .0.0.0:49208->19560/tcp, 0 .0.0.0:49210->9559/tcp, 9560 /tcp nebula-docker-compose_metad2_1 ./bin/nebula-metad --flagf ... Up ( healthy ) 0 .0.0.0:49206->19559/tcp, 0 .0.0.0:49205->19560/tcp, 0 .0.0.0:49207->9559/tcp, 9560 /tcp nebula-docker-compose_storaged0_1 ./bin/nebula-storaged --fl ... Up ( healthy ) 0 .0.0.0:49218->19779/tcp, 0 .0.0.0:49217->19780/tcp, 9777 /tcp, 9778 /tcp, 0 .0.0.0:49219->9779/tcp, 9780 /tcp nebula-docker-compose_storaged1_1 ./bin/nebula-storaged --fl ... Up ( healthy ) 0 .0.0.0:49215->19779/tcp, 0 .0.0.0:49214->19780/tcp, 9777 /tcp, 9778 /tcp, 0 .0.0.0:49216->9779/tcp, 9780 /tcp nebula-docker-compose_storaged2_1 ./bin/nebula-storaged --fl ... Up ( healthy ) 0 .0.0.0:49226->19779/tcp, 0 .0.0.0:49225->19780/tcp, 9777 /tcp, 9778 /tcp, 0 .0.0.0:49227->9779/tcp, 9780 /tcp If the service is abnormal, you can first confirm the abnormal container name (such as nebula-docker-compose_graphd2_1 ). Then you can execute docker ps to view the corresponding CONTAINER ID (such as 2a6c56c405f5 ). [ nebula-docker-compose ] $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 2a6c56c405f5 vesoft/nebula-graphd:nightly \"/usr/local/nebula/b\u2026\" 36 minutes ago Up 36 minutes ( healthy ) 0 .0.0.0:49230->9669/tcp, 0 .0.0.0:49229->19669/tcp, 0 .0.0.0:49228->19670/tcp nebula-docker-compose_graphd2_1 7042e0a8e83d vesoft/nebula-storaged:nightly \"./bin/nebula-storag\u2026\" 36 minutes ago Up 36 minutes ( healthy ) 9777 -9778/tcp, 9780 /tcp, 0 .0.0.0:49227->9779/tcp, 0 .0.0.0:49226->19779/tcp, 0 .0.0.0:49225->19780/tcp nebula-docker-compose_storaged2_1 18e3ea63ad65 vesoft/nebula-storaged:nightly \"./bin/nebula-storag\u2026\" 36 minutes ago Up 36 minutes ( healthy ) 9777 -9778/tcp, 9780 /tcp, 0 .0.0.0:49219->9779/tcp, 0 .0.0.0:49218->19779/tcp, 0 .0.0.0:49217->19780/tcp nebula-docker-compose_storaged0_1 4dcabfe8677a vesoft/nebula-graphd:nightly \"/usr/local/nebula/b\u2026\" 36 minutes ago Up 36 minutes ( healthy ) 0 .0.0.0:49224->9669/tcp, 0 .0.0.0:49223->19669/tcp, 0 .0.0.0:49222->19670/tcp nebula-docker-compose_graphd1_1 a74054c6ae25 vesoft/nebula-graphd:nightly \"/usr/local/nebula/b\u2026\" 36 minutes ago Up 36 minutes ( healthy ) 0 .0.0.0:9669->9669/tcp, 0 .0.0.0:49221->19669/tcp, 0 .0.0.0:49220->19670/tcp nebula-docker-compose_graphd_1 880025a3858c vesoft/nebula-storaged:nightly \"./bin/nebula-storag\u2026\" 36 minutes ago Up 36 minutes ( healthy ) 9777 -9778/tcp, 9780 /tcp, 0 .0.0.0:49216->9779/tcp, 0 .0.0.0:49215->19779/tcp, 0 .0.0.0:49214->19780/tcp nebula-docker-compose_storaged1_1 45736a32a23a vesoft/nebula-metad:nightly \"./bin/nebula-metad \u2026\" 36 minutes ago Up 36 minutes ( healthy ) 9560 /tcp, 0 .0.0.0:49213->9559/tcp, 0 .0.0.0:49212->19559/tcp, 0 .0.0.0:49211->19560/tcp nebula-docker-compose_metad0_1 3b2c90eb073e vesoft/nebula-metad:nightly \"./bin/nebula-metad \u2026\" 36 minutes ago Up 36 minutes ( healthy ) 9560 /tcp, 0 .0.0.0:49207->9559/tcp, 0 .0.0.0:49206->19559/tcp, 0 .0.0.0:49205->19560/tcp nebula-docker-compose_metad2_1 7bb31b7a5b3f vesoft/nebula-metad:nightly \"./bin/nebula-metad \u2026\" 36 minutes ago Up 36 minutes ( healthy ) 9560 /tcp, 0 .0.0.0:49210->9559/tcp, 0 .0.0.0:49209->19559/tcp, 0 .0.0.0:49208->19560/tcp nebula-docker-compose_metad1_1 Use the CONTAINER ID to log in the container and troubleshoot. nebula-docker-compose ] $ docker exec -it 2a6c56c405f5 bash [ root@2a6c56c405f5 nebula ] #","title":"In docker container (deployed with docker-compose)"},{"location":"reuse/source_manage-service/#next_to_do","text":"Connect to Nebula Graph","title":"Next to do"},{"location":"synchronization-and-migration/2.balance-syntax/","text":"BALANCE syntax \u00b6 The BALANCE statements support the load balancing operations of the Nebula Graph Storage services. For more information about storage load balancing and examples for using the BALANCE statements, see Storage load balance . The BALANCE statements are listed as follows. Syntax Description BALANCE DATA Starts a job to balance the distribution of storage partitions in the current graph space. It returns the job ID. BALANCE DATA REMOVE <ip:port> [,<ip>:<port> ...] Migrate the partitions in the specified storage host to other storage hosts in the current graph space. BALANCE LEADER Starts a job to balance the distribution of storage leaders in the current graph space. It returns the job ID. For details about how to view, stop, and restart a job, see Job manager and the JOB statements \u3002","title":"Load balance"},{"location":"synchronization-and-migration/2.balance-syntax/#balance_syntax","text":"The BALANCE statements support the load balancing operations of the Nebula Graph Storage services. For more information about storage load balancing and examples for using the BALANCE statements, see Storage load balance . The BALANCE statements are listed as follows. Syntax Description BALANCE DATA Starts a job to balance the distribution of storage partitions in the current graph space. It returns the job ID. BALANCE DATA REMOVE <ip:port> [,<ip>:<port> ...] Migrate the partitions in the specified storage host to other storage hosts in the current graph space. BALANCE LEADER Starts a job to balance the distribution of storage leaders in the current graph space. It returns the job ID. For details about how to view, stop, and restart a job, see Job manager and the JOB statements \u3002","title":"BALANCE syntax"},{"location":"synchronization-and-migration/replication-between-clusters/","text":"Synchronize between two clusters \u00b6 Nebula Graph supports data synchronization from a primary to a secondary cluster in almost real-time. It applies to scenarios such as disaster recovery and load balancing, and helps reduce the risk of data loss and enhance data security. Enterpriseonly This feature applies to the Enterprise Edition only. Synchronization workflow \u00b6 The synchronization works as follows: The primary cluster sends any data written into it to the Meta listener or the Storage listener in the form of WALs or snapshots. The listener sends the data to the drainer in the form of WALs. The drainer sends the data to the partitions of the secondary cluster through the Meta client or the Storage client. Applicable Scenarios \u00b6 Remote disaster recovery: Data synchronization enables cross-data-center or cross-city disaster recovery. Data migration: The migration can be implemented by synchronizing data and then switching cluster roles, without stopping the service. Read/Write splitting: Enable only writing on the primary cluster and only reading on the secondary cluster to lower the system load, and improve stability and usability. Precautions \u00b6 The synchronization is based on graph spaces, i.e., from one graph space in the primary cluster to another in the secondary cluster. About the synchronization topology, Nebula Graph: Supports synchronizing from one primary cluster to one secondary cluster, but not multiple primary clusters to one secondary cluster. Supports chained synchronization but not synchronization from one primary cluster to multiple secondary clusters directly. An example of chained synchronization is from cluster A to cluster B, and then cluster B to cluster C. The synchronization is implemented asynchronously, but with low latency. The Meta listener listens to the Meta Service and the Storage listener listens to the Storage Service. Do not mix them up. One graph space can have one Meta listener and one to multiple Storage listeners. These listeners can work with one to multiple drainers: One listener with one drainer. Multiple listeners with one drainer. Multiple listeners with multiple drainers. The machines where the listeners and drainers run must have enough disk space to store the WAL or snapshot files. If the target graph space in the secondary cluster has data before the synchronization starts, data conflicts or inconsistencies may happen during the synchronization. It is recommended to keep the target graph space empty. Prerequisites \u00b6 Prepare at least two machines to deploy the primary and secondary clusters, the listeners, and the drainer. The listener and drainer can be deployed in a standalone way, or on the machines hosting the primary and secondary clusters. The latter way can increase the machine load and decrease the service performance. Prepare the license file for the Nebula Graph Enterprise Edition. Test environment \u00b6 The test environment for the operation example in this topic is as follows: The primary cluster runs on the machine with the IP address 192.168.10.101. The cluster has one nebula-graphd process, one nebula-metad process, and one nebula-storaged process. The secondary cluster runs on the machine with the IP address 192.168.10.102. The cluster has one nebula-graphd process, one nebula-metad process, and one nebula-storaged process. Note The primary and secondary clusters can have different cluster specifications, such as different numbers of machines, service processes, and data partitions. The processes for the Meta and Storage listeners run on the machine with the IP address 192.168.10.103. The process for the drainer runs on the machine with the IP address 192.168.10.104. Steps \u00b6 Step 1: Set up the clusters, listeners, and drainer \u00b6 Install Nebula Graph on all the machines. For installing Nebula Graph, see the instructions in the Compile and install Nebula directory . Modify the configuration files on all the machines. Note For newly installed services, remove the suffix .default or .production of a configuration template file in the conf directory to make it take effect. On the primary and secondary cluster machines, modify nebula-graphd.conf , nebula-metad.conf , and nebula-storaged.conf . In all three files, set real IP addresses for local_ip instead of 127.0.0.1 , and set the IP addresses and ports for their own nebula-metad processes as the meta_server_addrs values. In nebula-graphd.conf , set enable_authorize=true . On the Meta listener machine, modify nebula-metad-listener.conf . Set the IP addresses and ports of the primary cluster's nebula-metad processes for meta_server_addrs , and those of the listener process for meta_sync_listener . On the Storage listener machine, modify nebula-storaged-listener.conf . Set the IP addresses and ports of the primary cluster's nebula-metad processes for meta_server_addrs . On the drainer machine, modify nebula-drainerd.conf . Set the IP addresses and ports of the secondary cluster's nebula-metad processes for meta_server_addrs . For more information about the configurations, see Configurations . On the machines of the primary cluster, secondary cluster, and listeners, upload the license files into the share/resources/ directories in the Nebula Graph installation directories. Go to the Nebula Graph installation directories on the machines and start the needed services. On the primary and secondary machines, run sudo scripts/nebula.service start all . On the Meta listener machine, run sudo bin/nebula-metad --flagfile etc/nebula-metad-listener.conf . On the Storage listener machine, run sudo bin/nebula-storaged --flagfile etc/nebula-storaged-listener.conf . On the drainer machine, run sudo scripts/nebula-drainerd.service start . Log into the primary cluster, add the Storage hosts, and check the status of the listeners. # Add the Storage hosts first. nebula> ADD HOSTS 192 .168.10.101:9779 ; nebula> SHOW HOSTS STORAGE ; +------------------+------+----------+-----------+--------------+----------------------+ | Host | Port | Status | Role | Git Info Sha | Version | +------------------+------+----------+-----------+--------------+----------------------+ | \"192.168.10.101\" | 9779 | \"ONLINE\" | \"STORAGE\" | \"xxxxxxx\" | \"ent-3.1.0\" | +------------------+------+----------+-----------+--------------+----------------------+ # Check the status of the Storage listener. nebula> SHOW HOSTS STORAGE LISTENER ; +------------------+------+----------+--------------------+--------------+----------------------+ | Host | Port | Status | Role | Git Info Sha | Version | +------------------+------+----------+--------------------+--------------+----------------------+ | \"192.168.10.103\" | 9789 | \"ONLINE\" | \"STORAGE_LISTENER\" | \"xxxxxxx\" | \"ent-3.1.0\" | +------------------+------+----------+--------------------+--------------+----------------------+ # Check the status of the Meta listener. nebula> SHOW HOSTS META LISTENER ; +------------------+------+----------+-----------------+--------------+----------------------+ | Host | Port | Status | Role | Git Info Sha | Version | +------------------+------+----------+-----------------+--------------+----------------------+ | \"192.168.10.103\" | 9559 | \"ONLINE\" | \"META_LISTENER\" | \"xxxxxxx\" | \"ent-3.1.0\" | +------------------+------+----------+-----------------+--------------+----------------------+ Log into the secondary cluster, add the Storage hosts, and check the status of the drainer. nebula> ADD HOSTS 192 .168.10.102:9779 ; nebula> SHOW HOSTS STORAGE ; +------------------+------+----------+-----------+--------------+----------------------+ | Host | Port | Status | Role | Git Info Sha | Version | +------------------+------+----------+-----------+--------------+----------------------+ | \"192.168.10.102\" | 9779 | \"ONLINE\" | \"STORAGE\" | \"xxxxxxx\" | \"ent-3.1.0\" | +------------------+------+----------+-----------+--------------+----------------------+ nebula> SHOW HOSTS DRAINER ; +------------------+------+----------+-----------+--------------+----------------------+ | Host | Port | Status | Role | Git Info Sha | Version | +------------------+------+----------+-----------+--------------+----------------------+ | \"192.168.10.104\" | 9889 | \"ONLINE\" | \"DRAINER\" | \"xxxxxxx\" | \"ent-3.1.0\" | +------------------+------+----------+-----------+--------------+----------------------+ Step 2: Set up the synchronization \u00b6 Log into the primary cluster and create a graph space basketballplayer . nebula> CREATE SPACE basketballplayer(partition_num=15, \\ replica_factor=1, \\ vid_type=fixed_string(30)); Use the graph space basketballplayer and register the drainer service. nebula> USE basketballplayer ; # Register the drainer service. nebula> SIGN IN DRAINER SERVICE ( 192 .168.10.104:9889 ) ; # Check if the drainer service is successfully signed in. nebula> SHOW DRAINER CLIENTS ; +-----------+------------------+------+ | Type | Host | Port | +-----------+------------------+------+ | \"DRAINER\" | \"192.168.10.104\" | 9889 | +-----------+------------------+------+ Configure the listener service. # replication_basketballplayer is the synchronization target. It will be created in the following steps. nebula> ADD LISTENER SYNC \\ META 192 .168.10.103:9569 \\ STORAGE 192 .168.10.103:9789 \\ TO SPACE replication_basketballplayer ; # Check the listener status. nebula> SHOW LISTENER SYNC ; +--------+--------+------------------------+--------------------------------+----------+ | PartId | Type | Host | SpaceName | Status | +--------+--------+------------------------+--------------------------------+----------+ | 0 | \"SYNC\" | \"\" 192 .168.10.103 \":9569\" | \"replication_basketballplayer\" | \"ONLINE\" | | 1 | \"SYNC\" | \"\" 192 .168.10.103 \":9789\" | \"replication_basketballplayer\" | \"ONLINE\" | | 2 | \"SYNC\" | \"\" 192 .168.10.103 \":9789\" | \"replication_basketballplayer\" | \"ONLINE\" | | 3 | \"SYNC\" | \"\" 192 .168.10.103 \":9789\" | \"replication_basketballplayer\" | \"ONLINE\" | | 4 | \"SYNC\" | \"\" 192 .168.10.103 \":9789\" | \"replication_basketballplayer\" | \"ONLINE\" | | 5 | \"SYNC\" | \"\" 192 .168.10.103 \":9789\" | \"replication_basketballplayer\" | \"ONLINE\" | | 6 | \"SYNC\" | \"\" 192 .168.10.103 \":9789\" | \"replication_basketballplayer\" | \"ONLINE\" | | 7 | \"SYNC\" | \"\" 192 .168.10.103 \":9789\" | \"replication_basketballplayer\" | \"ONLINE\" | | 8 | \"SYNC\" | \"\" 192 .168.10.103 \":9789\" | \"replication_basketballplayer\" | \"ONLINE\" | | 9 | \"SYNC\" | \"\" 192 .168.10.103 \":9789\" | \"replication_basketballplayer\" | \"ONLINE\" | | 10 | \"SYNC\" | \"\" 192 .168.10.103 \":9789\" | \"replication_basketballplayer\" | \"ONLINE\" | | 11 | \"SYNC\" | \"\" 192 .168.10.103 \":9789\" | \"replication_basketballplayer\" | \"ONLINE\" | | 12 | \"SYNC\" | \"\" 192 .168.10.103 \":9789\" | \"replication_basketballplayer\" | \"ONLINE\" | | 13 | \"SYNC\" | \"\" 192 .168.10.103 \":9789\" | \"replication_basketballplayer\" | \"ONLINE\" | | 14 | \"SYNC\" | \"\" 192 .168.10.103 \":9789\" | \"replication_basketballplayer\" | \"ONLINE\" | | 15 | \"SYNC\" | \"\" 192 .168.10.103 \":9789\" | \"replication_basketballplayer\" | \"ONLINE\" | +--------+--------+------------------------+--------------------------------+----------+ Log into the secondary cluster and create graph space replication_basketballplayer . nebula> CREATE SPACE replication_basketballplayer ( partition_num = 15 , \\ replica_factor = 1 , \\ vid_type = fixed_string ( 30 )) ; Use replication_basketballplayer and configure the drainer service. # Configure the drainer service. nebula> ADD DRAINER 192 .168.10.104:9889 ; # Check the drainer status. nebula> SHOW DRAINERS ; +-------------------------+----------+ | Host | Status | +-------------------------+----------+ | \"\" 192 .168.10.104 \":9889\" | \"ONLINE\" | +-------------------------+----------+ Set the target graph space as read-only to avoid data inconsistency. Note This step only sets the target graph space, not other graph spaces. nebula> USE replication_basketballplayer ; # Set the working graph space as read-only. nebula> SET VARIABLES read_only = true ; # Check the read_only status of the working graph space. nebula> GET VARIABLES read_only ; +-------------+--------+-------+ | name | type | value | +-------------+--------+-------+ | \"read_only\" | \"bool\" | true | +-------------+--------+-------+ Step 3: Validate the data \u00b6 Log into the primary cluster, create the schema, and insert data. nebula> USE basketballplayer ; nebula> CREATE TAG player ( name string, age int ) ; nebula> CREATE EDGE follow ( degree int ) ; nebula> INSERT VERTEX player ( name, age ) VALUES \"player100\" : ( \"Tim Duncan\" , 42 ) ; nebula> INSERT VERTEX player ( name, age ) VALUES \"player101\" : ( \"Tony Parker\" , 36 ) ; nebula> INSERT EDGE follow ( degree ) VALUES \"player101\" -> \"player100\" : ( 95 ) ; Log into the secondary cluster and validate the data. nebula> USE replication_basketballplayer ; nebula> SUBMIT JOB STATS ; nebula> SHOW STATS ; +---------+------------+-------+ | Type | Name | Count | +---------+------------+-------+ | \"Tag\" | \"player\" | 2 | | \"Edge\" | \"follow\" | 1 | | \"Space\" | \"vertices\" | 2 | | \"Space\" | \"edges\" | 1 | +---------+------------+-------+ nebula> FETCH PROP ON player \"player100\" \\ YIELD properties ( vertex ) ; +-------------------------------+ | properties ( VERTEX ) | +-------------------------------+ | { age: 42 , name: \"Tim Duncan\" } | +-------------------------------+ nebula> GO FROM \"player101\" OVER follow \\ YIELD dst ( edge ) ; +-------------+ | dst ( EDGE ) | +-------------+ | \"player100\" | +-------------+ Switch between primary and secondary clusters \u00b6 To migrate data or implement disaster recovery, manually switch between the primary and secondary clusters. Note Before the switching, set up a listener for the new primary cluster, and a drainer for the new secondary cluster. In the following example, the listener has IP address 192.168.10.105 and drainer 192.168.10.106. Log into the primary cluster and remove the old drainer and listener. nebula> USE basketballplayer ; nebula> SIGN OUT DRAINER SERVICE ; nebula> REMOVE LISTENER SYNC ; Set the working graph space as read-only to avoid data inconsistency. nebula> SET VARIABLES read_only = true ; Log into the secondary cluster, disable read-only, and remove the old drainer. nebula> USE replication_basketballplayer ; nebula> SET VARIABLES read_only = false ; nebula> REMOVE DRAINER ; Change the secondary cluster to the new primary cluster. nebula> SIGN IN DRAINER SERVICE ( 192 .168.10.106:9889 ) ; nebula> ADD LISTENER SYNC META 192 .168.10.105:9559 STORAGE 192 .168.10.105:9789 TO SPACE basketballplayer ; nebula> REMOVE DRAINER ; Log into the old primary cluster and change it to the new secondary cluster. nebula> USE basketballplayer ; # Disable read-only for the working graph space, otherwise adding drainer fails. nebula> SET VARIABLES read_only = false ; nebula> ADD DRAINER 192 .168.10.106:9889 ; nebula> SET VARIABLES read_only = true ; FAQ \u00b6 Can the pre-existent data in the primary cluster be synchronized to the secondary cluster? \u00b6 Yes. After receiving the WAL from the listener, if the drainer finds that the data to be updated does not exist in the secondary cluster, it starts the synchronization of the complete data set. Will the pre-existent data in the secondary cluster affect the synchronization? \u00b6 If the pre-existent data in the secondary cluster is a subset of the data in the primary cluster, the data in the primary and secondary clusters will eventually become consistent through synchronization. The pre-existent data that is not in the primary cluster will stay in the secondary cluster until manually deleted. Will the pre-existent schema information in the secondary cluster affect the synchronization? \u00b6 The pre-existent schema information must not conflict with the schema of the primary cluster. Otherwise, it will be overwritten, and related data in the secondary cluster might become invalid. Should the number of machines, replicas, and partitions in the primary and secondary clusters be the same? \u00b6 No. The synchronization is based on graph spaces, not other elements such as partitions and replicas. The primary and secondary clusters do not need to have the exact specifications. Does altering the schema in the primary cluster affect the synchronization? \u00b6 Altering the schema may increase the synchronization latency. The schema data is synchronized through the Meta listener, while the vertex/edge data is through the Storage listener. When synchronizing the vertex/edge data, the system checks the schema version of the data. If the system finds that the version number of the schema is greater than that in the secondary cluster, it pauses the vertex/edge data update, and updates the schema data first. How to deal with synchronization failures? \u00b6 If problems happen on the primary cluster, the synchronization will be paused. Fixing the problems and then restarting the primary cluster can continue the synchronization. If problems happen on the secondary cluster, listeners or drainers, when the problem is fixed, the services that had the problems will receive the WALs accumulated from its upstream and the synchronization will continue. If the faulty machine is replaced with a new one, all the data of the synchronization services on the faulty machine must be copied to the new machine. Otherwise, the synchronization of the complete data set starts. How to check the data synchronization status and progress? \u00b6 There is no tool specially designed to do this for now.","title":"Synchronize between two clusters"},{"location":"synchronization-and-migration/replication-between-clusters/#synchronize_between_two_clusters","text":"Nebula Graph supports data synchronization from a primary to a secondary cluster in almost real-time. It applies to scenarios such as disaster recovery and load balancing, and helps reduce the risk of data loss and enhance data security. Enterpriseonly This feature applies to the Enterprise Edition only.","title":"Synchronize between two clusters"},{"location":"synchronization-and-migration/replication-between-clusters/#synchronization_workflow","text":"The synchronization works as follows: The primary cluster sends any data written into it to the Meta listener or the Storage listener in the form of WALs or snapshots. The listener sends the data to the drainer in the form of WALs. The drainer sends the data to the partitions of the secondary cluster through the Meta client or the Storage client.","title":"Synchronization workflow"},{"location":"synchronization-and-migration/replication-between-clusters/#applicable_scenarios","text":"Remote disaster recovery: Data synchronization enables cross-data-center or cross-city disaster recovery. Data migration: The migration can be implemented by synchronizing data and then switching cluster roles, without stopping the service. Read/Write splitting: Enable only writing on the primary cluster and only reading on the secondary cluster to lower the system load, and improve stability and usability.","title":"Applicable Scenarios"},{"location":"synchronization-and-migration/replication-between-clusters/#precautions","text":"The synchronization is based on graph spaces, i.e., from one graph space in the primary cluster to another in the secondary cluster. About the synchronization topology, Nebula Graph: Supports synchronizing from one primary cluster to one secondary cluster, but not multiple primary clusters to one secondary cluster. Supports chained synchronization but not synchronization from one primary cluster to multiple secondary clusters directly. An example of chained synchronization is from cluster A to cluster B, and then cluster B to cluster C. The synchronization is implemented asynchronously, but with low latency. The Meta listener listens to the Meta Service and the Storage listener listens to the Storage Service. Do not mix them up. One graph space can have one Meta listener and one to multiple Storage listeners. These listeners can work with one to multiple drainers: One listener with one drainer. Multiple listeners with one drainer. Multiple listeners with multiple drainers. The machines where the listeners and drainers run must have enough disk space to store the WAL or snapshot files. If the target graph space in the secondary cluster has data before the synchronization starts, data conflicts or inconsistencies may happen during the synchronization. It is recommended to keep the target graph space empty.","title":"Precautions"},{"location":"synchronization-and-migration/replication-between-clusters/#prerequisites","text":"Prepare at least two machines to deploy the primary and secondary clusters, the listeners, and the drainer. The listener and drainer can be deployed in a standalone way, or on the machines hosting the primary and secondary clusters. The latter way can increase the machine load and decrease the service performance. Prepare the license file for the Nebula Graph Enterprise Edition.","title":"Prerequisites"},{"location":"synchronization-and-migration/replication-between-clusters/#test_environment","text":"The test environment for the operation example in this topic is as follows: The primary cluster runs on the machine with the IP address 192.168.10.101. The cluster has one nebula-graphd process, one nebula-metad process, and one nebula-storaged process. The secondary cluster runs on the machine with the IP address 192.168.10.102. The cluster has one nebula-graphd process, one nebula-metad process, and one nebula-storaged process. Note The primary and secondary clusters can have different cluster specifications, such as different numbers of machines, service processes, and data partitions. The processes for the Meta and Storage listeners run on the machine with the IP address 192.168.10.103. The process for the drainer runs on the machine with the IP address 192.168.10.104.","title":"Test environment"},{"location":"synchronization-and-migration/replication-between-clusters/#steps","text":"","title":"Steps"},{"location":"synchronization-and-migration/replication-between-clusters/#step_1_set_up_the_clusters_listeners_and_drainer","text":"Install Nebula Graph on all the machines. For installing Nebula Graph, see the instructions in the Compile and install Nebula directory . Modify the configuration files on all the machines. Note For newly installed services, remove the suffix .default or .production of a configuration template file in the conf directory to make it take effect. On the primary and secondary cluster machines, modify nebula-graphd.conf , nebula-metad.conf , and nebula-storaged.conf . In all three files, set real IP addresses for local_ip instead of 127.0.0.1 , and set the IP addresses and ports for their own nebula-metad processes as the meta_server_addrs values. In nebula-graphd.conf , set enable_authorize=true . On the Meta listener machine, modify nebula-metad-listener.conf . Set the IP addresses and ports of the primary cluster's nebula-metad processes for meta_server_addrs , and those of the listener process for meta_sync_listener . On the Storage listener machine, modify nebula-storaged-listener.conf . Set the IP addresses and ports of the primary cluster's nebula-metad processes for meta_server_addrs . On the drainer machine, modify nebula-drainerd.conf . Set the IP addresses and ports of the secondary cluster's nebula-metad processes for meta_server_addrs . For more information about the configurations, see Configurations . On the machines of the primary cluster, secondary cluster, and listeners, upload the license files into the share/resources/ directories in the Nebula Graph installation directories. Go to the Nebula Graph installation directories on the machines and start the needed services. On the primary and secondary machines, run sudo scripts/nebula.service start all . On the Meta listener machine, run sudo bin/nebula-metad --flagfile etc/nebula-metad-listener.conf . On the Storage listener machine, run sudo bin/nebula-storaged --flagfile etc/nebula-storaged-listener.conf . On the drainer machine, run sudo scripts/nebula-drainerd.service start . Log into the primary cluster, add the Storage hosts, and check the status of the listeners. # Add the Storage hosts first. nebula> ADD HOSTS 192 .168.10.101:9779 ; nebula> SHOW HOSTS STORAGE ; +------------------+------+----------+-----------+--------------+----------------------+ | Host | Port | Status | Role | Git Info Sha | Version | +------------------+------+----------+-----------+--------------+----------------------+ | \"192.168.10.101\" | 9779 | \"ONLINE\" | \"STORAGE\" | \"xxxxxxx\" | \"ent-3.1.0\" | +------------------+------+----------+-----------+--------------+----------------------+ # Check the status of the Storage listener. nebula> SHOW HOSTS STORAGE LISTENER ; +------------------+------+----------+--------------------+--------------+----------------------+ | Host | Port | Status | Role | Git Info Sha | Version | +------------------+------+----------+--------------------+--------------+----------------------+ | \"192.168.10.103\" | 9789 | \"ONLINE\" | \"STORAGE_LISTENER\" | \"xxxxxxx\" | \"ent-3.1.0\" | +------------------+------+----------+--------------------+--------------+----------------------+ # Check the status of the Meta listener. nebula> SHOW HOSTS META LISTENER ; +------------------+------+----------+-----------------+--------------+----------------------+ | Host | Port | Status | Role | Git Info Sha | Version | +------------------+------+----------+-----------------+--------------+----------------------+ | \"192.168.10.103\" | 9559 | \"ONLINE\" | \"META_LISTENER\" | \"xxxxxxx\" | \"ent-3.1.0\" | +------------------+------+----------+-----------------+--------------+----------------------+ Log into the secondary cluster, add the Storage hosts, and check the status of the drainer. nebula> ADD HOSTS 192 .168.10.102:9779 ; nebula> SHOW HOSTS STORAGE ; +------------------+------+----------+-----------+--------------+----------------------+ | Host | Port | Status | Role | Git Info Sha | Version | +------------------+------+----------+-----------+--------------+----------------------+ | \"192.168.10.102\" | 9779 | \"ONLINE\" | \"STORAGE\" | \"xxxxxxx\" | \"ent-3.1.0\" | +------------------+------+----------+-----------+--------------+----------------------+ nebula> SHOW HOSTS DRAINER ; +------------------+------+----------+-----------+--------------+----------------------+ | Host | Port | Status | Role | Git Info Sha | Version | +------------------+------+----------+-----------+--------------+----------------------+ | \"192.168.10.104\" | 9889 | \"ONLINE\" | \"DRAINER\" | \"xxxxxxx\" | \"ent-3.1.0\" | +------------------+------+----------+-----------+--------------+----------------------+","title":"Step 1: Set up the clusters, listeners, and drainer"},{"location":"synchronization-and-migration/replication-between-clusters/#step_2_set_up_the_synchronization","text":"Log into the primary cluster and create a graph space basketballplayer . nebula> CREATE SPACE basketballplayer(partition_num=15, \\ replica_factor=1, \\ vid_type=fixed_string(30)); Use the graph space basketballplayer and register the drainer service. nebula> USE basketballplayer ; # Register the drainer service. nebula> SIGN IN DRAINER SERVICE ( 192 .168.10.104:9889 ) ; # Check if the drainer service is successfully signed in. nebula> SHOW DRAINER CLIENTS ; +-----------+------------------+------+ | Type | Host | Port | +-----------+------------------+------+ | \"DRAINER\" | \"192.168.10.104\" | 9889 | +-----------+------------------+------+ Configure the listener service. # replication_basketballplayer is the synchronization target. It will be created in the following steps. nebula> ADD LISTENER SYNC \\ META 192 .168.10.103:9569 \\ STORAGE 192 .168.10.103:9789 \\ TO SPACE replication_basketballplayer ; # Check the listener status. nebula> SHOW LISTENER SYNC ; +--------+--------+------------------------+--------------------------------+----------+ | PartId | Type | Host | SpaceName | Status | +--------+--------+------------------------+--------------------------------+----------+ | 0 | \"SYNC\" | \"\" 192 .168.10.103 \":9569\" | \"replication_basketballplayer\" | \"ONLINE\" | | 1 | \"SYNC\" | \"\" 192 .168.10.103 \":9789\" | \"replication_basketballplayer\" | \"ONLINE\" | | 2 | \"SYNC\" | \"\" 192 .168.10.103 \":9789\" | \"replication_basketballplayer\" | \"ONLINE\" | | 3 | \"SYNC\" | \"\" 192 .168.10.103 \":9789\" | \"replication_basketballplayer\" | \"ONLINE\" | | 4 | \"SYNC\" | \"\" 192 .168.10.103 \":9789\" | \"replication_basketballplayer\" | \"ONLINE\" | | 5 | \"SYNC\" | \"\" 192 .168.10.103 \":9789\" | \"replication_basketballplayer\" | \"ONLINE\" | | 6 | \"SYNC\" | \"\" 192 .168.10.103 \":9789\" | \"replication_basketballplayer\" | \"ONLINE\" | | 7 | \"SYNC\" | \"\" 192 .168.10.103 \":9789\" | \"replication_basketballplayer\" | \"ONLINE\" | | 8 | \"SYNC\" | \"\" 192 .168.10.103 \":9789\" | \"replication_basketballplayer\" | \"ONLINE\" | | 9 | \"SYNC\" | \"\" 192 .168.10.103 \":9789\" | \"replication_basketballplayer\" | \"ONLINE\" | | 10 | \"SYNC\" | \"\" 192 .168.10.103 \":9789\" | \"replication_basketballplayer\" | \"ONLINE\" | | 11 | \"SYNC\" | \"\" 192 .168.10.103 \":9789\" | \"replication_basketballplayer\" | \"ONLINE\" | | 12 | \"SYNC\" | \"\" 192 .168.10.103 \":9789\" | \"replication_basketballplayer\" | \"ONLINE\" | | 13 | \"SYNC\" | \"\" 192 .168.10.103 \":9789\" | \"replication_basketballplayer\" | \"ONLINE\" | | 14 | \"SYNC\" | \"\" 192 .168.10.103 \":9789\" | \"replication_basketballplayer\" | \"ONLINE\" | | 15 | \"SYNC\" | \"\" 192 .168.10.103 \":9789\" | \"replication_basketballplayer\" | \"ONLINE\" | +--------+--------+------------------------+--------------------------------+----------+ Log into the secondary cluster and create graph space replication_basketballplayer . nebula> CREATE SPACE replication_basketballplayer ( partition_num = 15 , \\ replica_factor = 1 , \\ vid_type = fixed_string ( 30 )) ; Use replication_basketballplayer and configure the drainer service. # Configure the drainer service. nebula> ADD DRAINER 192 .168.10.104:9889 ; # Check the drainer status. nebula> SHOW DRAINERS ; +-------------------------+----------+ | Host | Status | +-------------------------+----------+ | \"\" 192 .168.10.104 \":9889\" | \"ONLINE\" | +-------------------------+----------+ Set the target graph space as read-only to avoid data inconsistency. Note This step only sets the target graph space, not other graph spaces. nebula> USE replication_basketballplayer ; # Set the working graph space as read-only. nebula> SET VARIABLES read_only = true ; # Check the read_only status of the working graph space. nebula> GET VARIABLES read_only ; +-------------+--------+-------+ | name | type | value | +-------------+--------+-------+ | \"read_only\" | \"bool\" | true | +-------------+--------+-------+","title":"Step 2: Set up the synchronization"},{"location":"synchronization-and-migration/replication-between-clusters/#step_3_validate_the_data","text":"Log into the primary cluster, create the schema, and insert data. nebula> USE basketballplayer ; nebula> CREATE TAG player ( name string, age int ) ; nebula> CREATE EDGE follow ( degree int ) ; nebula> INSERT VERTEX player ( name, age ) VALUES \"player100\" : ( \"Tim Duncan\" , 42 ) ; nebula> INSERT VERTEX player ( name, age ) VALUES \"player101\" : ( \"Tony Parker\" , 36 ) ; nebula> INSERT EDGE follow ( degree ) VALUES \"player101\" -> \"player100\" : ( 95 ) ; Log into the secondary cluster and validate the data. nebula> USE replication_basketballplayer ; nebula> SUBMIT JOB STATS ; nebula> SHOW STATS ; +---------+------------+-------+ | Type | Name | Count | +---------+------------+-------+ | \"Tag\" | \"player\" | 2 | | \"Edge\" | \"follow\" | 1 | | \"Space\" | \"vertices\" | 2 | | \"Space\" | \"edges\" | 1 | +---------+------------+-------+ nebula> FETCH PROP ON player \"player100\" \\ YIELD properties ( vertex ) ; +-------------------------------+ | properties ( VERTEX ) | +-------------------------------+ | { age: 42 , name: \"Tim Duncan\" } | +-------------------------------+ nebula> GO FROM \"player101\" OVER follow \\ YIELD dst ( edge ) ; +-------------+ | dst ( EDGE ) | +-------------+ | \"player100\" | +-------------+","title":"Step 3: Validate the data"},{"location":"synchronization-and-migration/replication-between-clusters/#switch_between_primary_and_secondary_clusters","text":"To migrate data or implement disaster recovery, manually switch between the primary and secondary clusters. Note Before the switching, set up a listener for the new primary cluster, and a drainer for the new secondary cluster. In the following example, the listener has IP address 192.168.10.105 and drainer 192.168.10.106. Log into the primary cluster and remove the old drainer and listener. nebula> USE basketballplayer ; nebula> SIGN OUT DRAINER SERVICE ; nebula> REMOVE LISTENER SYNC ; Set the working graph space as read-only to avoid data inconsistency. nebula> SET VARIABLES read_only = true ; Log into the secondary cluster, disable read-only, and remove the old drainer. nebula> USE replication_basketballplayer ; nebula> SET VARIABLES read_only = false ; nebula> REMOVE DRAINER ; Change the secondary cluster to the new primary cluster. nebula> SIGN IN DRAINER SERVICE ( 192 .168.10.106:9889 ) ; nebula> ADD LISTENER SYNC META 192 .168.10.105:9559 STORAGE 192 .168.10.105:9789 TO SPACE basketballplayer ; nebula> REMOVE DRAINER ; Log into the old primary cluster and change it to the new secondary cluster. nebula> USE basketballplayer ; # Disable read-only for the working graph space, otherwise adding drainer fails. nebula> SET VARIABLES read_only = false ; nebula> ADD DRAINER 192 .168.10.106:9889 ; nebula> SET VARIABLES read_only = true ;","title":"Switch between primary and secondary clusters"},{"location":"synchronization-and-migration/replication-between-clusters/#faq","text":"","title":"FAQ"},{"location":"synchronization-and-migration/replication-between-clusters/#can_the_pre-existent_data_in_the_primary_cluster_be_synchronized_to_the_secondary_cluster","text":"Yes. After receiving the WAL from the listener, if the drainer finds that the data to be updated does not exist in the secondary cluster, it starts the synchronization of the complete data set.","title":"Can the pre-existent data in the primary cluster be synchronized to the secondary cluster?"},{"location":"synchronization-and-migration/replication-between-clusters/#will_the_pre-existent_data_in_the_secondary_cluster_affect_the_synchronization","text":"If the pre-existent data in the secondary cluster is a subset of the data in the primary cluster, the data in the primary and secondary clusters will eventually become consistent through synchronization. The pre-existent data that is not in the primary cluster will stay in the secondary cluster until manually deleted.","title":"Will the pre-existent data in the secondary cluster affect the synchronization?"},{"location":"synchronization-and-migration/replication-between-clusters/#will_the_pre-existent_schema_information_in_the_secondary_cluster_affect_the_synchronization","text":"The pre-existent schema information must not conflict with the schema of the primary cluster. Otherwise, it will be overwritten, and related data in the secondary cluster might become invalid.","title":"Will the pre-existent schema information in the secondary cluster affect the synchronization?"},{"location":"synchronization-and-migration/replication-between-clusters/#should_the_number_of_machines_replicas_and_partitions_in_the_primary_and_secondary_clusters_be_the_same","text":"No. The synchronization is based on graph spaces, not other elements such as partitions and replicas. The primary and secondary clusters do not need to have the exact specifications.","title":"Should the number of machines, replicas, and partitions in the primary and secondary clusters be the same?"},{"location":"synchronization-and-migration/replication-between-clusters/#does_altering_the_schema_in_the_primary_cluster_affect_the_synchronization","text":"Altering the schema may increase the synchronization latency. The schema data is synchronized through the Meta listener, while the vertex/edge data is through the Storage listener. When synchronizing the vertex/edge data, the system checks the schema version of the data. If the system finds that the version number of the schema is greater than that in the secondary cluster, it pauses the vertex/edge data update, and updates the schema data first.","title":"Does altering the schema in the primary cluster affect the synchronization?"},{"location":"synchronization-and-migration/replication-between-clusters/#how_to_deal_with_synchronization_failures","text":"If problems happen on the primary cluster, the synchronization will be paused. Fixing the problems and then restarting the primary cluster can continue the synchronization. If problems happen on the secondary cluster, listeners or drainers, when the problem is fixed, the services that had the problems will receive the WALs accumulated from its upstream and the synchronization will continue. If the faulty machine is replaced with a new one, all the data of the synchronization services on the faulty machine must be copied to the new machine. Otherwise, the synchronization of the complete data set starts.","title":"How to deal with synchronization failures?"},{"location":"synchronization-and-migration/replication-between-clusters/#how_to_check_the_data_synchronization_status_and_progress","text":"There is no tool specially designed to do this for now.","title":"How to check the data synchronization status and progress?"}]}