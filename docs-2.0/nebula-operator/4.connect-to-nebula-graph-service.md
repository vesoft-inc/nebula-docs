# Connect to Nebula Graph databases with Nebular Operator

After creating a Nebula Graph cluster with Nebula Operator on Kubernetes, you can connect to Nebula Graph databases from within the cluster and outside the cluster.

## Prerequisites

Create a Nebula Graph cluster with Nebula Operator on Kubernetes. For more information, see [Deploy Nebula Graph clusters with Kubectl](3.deploy-nebula-graph-cluster/3.1create-cluster-with-kubectl.md) or [Deploy Nebula Graph clusters with Helm](3.deploy-nebula-graph-cluster/3.2create-cluster-with-helm.md).

## Connect to Nebula Graph databases from within a Nebula Graph cluster

When a Nebula Graph cluster is created, Nebula Operator automatically creates a Service named `<cluster-name>-graphd-svc` with the type `ClusterIP` under the same namespace. With the IP of the Service and the port number of the Nebula Graph database, you can connect to the Nebula Graph database.

1. Run the following command to check the IP of the Service:

  ```bash
  $ kubectl get service -l app.kubernetes.io/cluster=<nebula>  #<nebula> is a variable value. Replace it with the desired name.
  NAME                       TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                                          AGE
  nebula-graphd-svc          ClusterIP   10.98.213.34   <none>        9669/TCP,19669/TCP,19670/TCP                     23h
  nebula-metad-headless      ClusterIP   None           <none>        9559/TCP,19559/TCP,19560/TCP                     23h
  nebula-storaged-headless   ClusterIP   None           <none>        9779/TCP,19779/TCP,19780/TCP,9778/TCP            23h
  ```

  Services of the `ClusterIP` type only can be accessed by other applications in a cluster. For more information, see [ClusterIP](https://kubernetes.io/docs/concepts/services-networking/service/).

2. Run the following command to connect to the Nebula Graph database using the IP of the `<cluster-name>-graphd-svc` Service above:

  ```bash
  kubectl run -ti --image vesoft/nebula-console:{{console.branch}} --restart=Never -- <nebula_console_name> -addr <cluster_ip>  -port <service_port> -u <username> -p <password>
  ```

  For example:

  ```bash
  kubectl run -ti --image vesoft/nebula-console:{{console.branch}} --restart=Never -- nebula-console -addr 10.98.213.34  -port 9669 -u root -p vesoft

  - `--image`: The image for the tool Nebula Console used to connect to Nebula Graph databases.
  - `<nebula-console>`: The custom Pod name.
  - `-addr`: The IP of the `ClusterIP` Service, used to connect to Graphd services.
  - `-port`: The port to connect to Graphd services, the default port of which is 9669.
  - `-u`: The username of your Nebula Graph account. Before enabling authentication, you can use any existing username. The default username is root.
  - `-p`: The password of your Nebula Graph account. Before enabling authentication, you can use any characters as the password.

  A successful connection to the database is indicated if the following is returned:

  ```bash
  If you don't see a command prompt, try pressing enter.

  (root@nebula) [(none)]>
  ```

You can also connect to Nebula Graph databases with **Fully Qualified Domain Name (FQDN)**. The domain format is `<cluster-name>-graphd.<cluster-namespace>.svc.<CLUSTER_DOMAIN>`:

```bash
kubectl run -ti --image vesoft/nebula-console:{{console.branch}} --restart=Never -- <nebula_console_name> -addr <cluster_name>-graphd-svc.default.svc.cluster.local -port <service_port> -u <username> -p <password>
```

The default value of `CLUSTER_DOMAIN` is `cluster.local`.

## Connect to Nebula Graph databases from outside a Nebula Graph cluster via `NodePort`

You can create a Service of type `NodePort` to connect to Nebula Graph databases from outside a Nebula Graph cluster with a node IP and an exposed node port. You can also use load balancing software provided by cloud providers (such as Azure, AWS, etc.) and set the Service of type `LoadBalancer`.

The Service of type `NodePort` forwards the front-end requests via the label selector `spec.selector` to Graphd pods with labels `app.kubernetes.io/cluster: <cluster-name>` and `app.kubernetes.io/component: graphd`.

Steps:

1. Create a YAML file named `graphd-nodeport-service.yaml`. The file contents are as follows:

  ```yaml
  apiVersion: v1
  kind: Service
  metadata:
    labels:
      app.kubernetes.io/cluster: nebula
      app.kubernetes.io/component: graphd
      app.kubernetes.io/managed-by: nebula-operator
      app.kubernetes.io/name: nebula-graph
    name: nebula-graphd-svc-nodeport
    namespace: default
  spec:
    externalTrafficPolicy: Local
    ports:
    - name: thrift
      port: 9669
      protocol: TCP
      targetPort: 9669
    - name: http
      port: 19669
      protocol: TCP
      targetPort: 19669
    selector:
      app.kubernetes.io/cluster: nebula
      app.kubernetes.io/component: graphd
      app.kubernetes.io/managed-by: nebula-operator
      app.kubernetes.io/name: nebula-graph
    type: NodePort
  ```

  - Nebula Graph uses port `9669` by default. `19669` is the port of the Graph service in a Nebula Graph cluster.
  - The value of `targetPort` is the port mapped to the database Pods, which can be customized.

2. Run the following command to create a NodePort Service.

  ```bash
  kubectl create -f graphd-nodeport-service.yaml
  ```

3. Check the port mapped on all of your cluster nodes.

  ```bash
  kubectl get services
  ```

  Output:

  ```bash
  NAME                           TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                                          AGE
  nebula-graphd-svc              ClusterIP   10.98.213.34   <none>        9669/TCP,19669/TCP,19670/TCP                     23h
  nebula-graphd-svc-nodeport     NodePort    10.107.153.129 <none>        9669:32236/TCP,19669:31674/TCP,19670:31057/TCP   24h
  nebula-metad-headless          ClusterIP   None           <none>        9559/TCP,19559/TCP,19560/TCP                     23h
  nebula-storaged-headless       ClusterIP   None           <none>        9779/TCP,19779/TCP,19780/TCP,9778/TCP            23h
  ```

  As you see, the mapped port of Nebula Graph databases on all cluster nodes is `32236`.

4. Connect to Nebula Graph databases with your node IP and the node port above.
  
  ```bash
  kubectl run -ti --image vesoft/nebula-console:{{console.branch}} --restart=Never -- <nebula_console_name> -addr <node_ip> -port <node_port> -u <username> -p <password>
  ```

  For example:

  ```bash
  kubectl run -ti --image vesoft/nebula-console:{{console.branch}} --restart=Never -- nebula-console2 -addr 192.168.8.24 -port 32236 -u root -p vesoft
  If you don't see a command prompt, try pressing enter.

  (root@nebula) [(none)]>
  ```

  - `--image`: The image for the tool Nebula Console used to connect to Nebula Graph databases.
  - `<nebula-console>`: The custom Pod name. The above example uses `nebula-console2`.
  - `-addr`: The IP of any node in a Nebula Graph cluster. The above example uses `192.168.8.24`.
  - `-port`: The mapped port of Nebula Graph databases on all cluster nodes. The above example uses `32236`.
  - `-u`: The username of your Nebula Graph account. Before enabling authentication, you can use any existing username. The default username is root.
  - `-p`: The password of your Nebula Graph account. Before enabling authentication, you can use any characters as the password.
  
## Connect to Nebula Graph databases from outside a Nebula Graph cluster via Ingress

Nginx Ingress is an implementation of Kubernetes Ingress. Nginx Ingress watches the Ingress resource of a Kubernetes cluster and generates the Ingress rules into Nginx configurations that enable Nginx to forward Layer 7 traffic.

You can use Nginx Ingress to connect to a Nebula Graph cluster from outside the cluster using a combination of the HostNetwork and DaemonSet pattern.

As HostNetwork is used，Nginx Ingress pods cannot be scheduled to the same node. To avoid listening port conflicts, some nodes can be selected and labeled as edge nodes in advance, which are specially used for Nginx Ingress deployment. Nginx Ingress is then deployed on these nodes in a DaemonSet mode.

Ingress does not support TCP or UDP services. For this reason，the nginx-ingress-controller pod uses the flags `--tcp-services-configmap` and `--udp-services-configmap` to point to an existing ConfigMap where the key refers to the external port to be used and the value refers to the format of the service to be exposed. The format of the value `<namespace/service_name>:<service_port>`.

For example, the configurations of the ConfigMap named as `tcp-services` is as follows:

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: tcp-services
  namespace: nginx-ingress
data:
  # update 
  9769: "default/nebula-graphd-svc:9669"
```

After configuring the ConfigMap named `tcp-services`, the defined port in the ConfigMap needs to be exposed in the Service where Nginx Ingress is defined.

```yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: nginx-ingress
    component: default-backend
  name: nginx-ingress-proxy-tcp
  namespace: nginx-ingress
spec:
  ports:
    - name: proxied-tcp
      port: 9769
      protocol: TCP
      targetPort: 9669
  selector:
    app: nginx-ingress
    component: default-backend
  type: "ClusterIP"
```

- The value `9769` for `port` specifies an external port. You can custom the value.
- The value `9669` for `targetPort` refers to the port of the graphd service to be accessed.

The complete example and steps are as follows.

1. Create a file named `nginx-ingress-daemonset-hostnetwork.yaml`. The YAML file reads as follows:

 ```yaml
  apiVersion: v1
  kind: ConfigMap
  metadata:
    name: nginx-ingress-controller
    namespace: nginx-ingress
  data:
    keep-alive-requests: "100"
    upstream-keepalive-connections: "200"
    max-worker-connections: "65536"
  ---
  apiVersion: v1
  kind: ConfigMap
  metadata:
    name: tcp-services
    namespace: nginx-ingress
  data:
    9769: "default/nebula-graphd-svc:9669"
  ---
  apiVersion: v1
  kind: ServiceAccount
  metadata:
    labels:
      app: nginx-ingress
    name: nginx-ingress
    namespace: nginx-ingress
  ---
  apiVersion: v1
  kind: ServiceAccount
  metadata:
    labels:
      app: nginx-ingress
    name: nginx-ingress-backend
    namespace: nginx-ingress
  ---
  apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    labels:
      app: nginx-ingress
    name: nginx-ingress
  rules:
    - apiGroups:
        - ""
      resources:
        - configmaps
        - endpoints
        - nodes
        - pods
        - secrets
      verbs:
        - list
        - watch
    - apiGroups:
        - ""
      resources:
        - nodes
      verbs:
        - get
    - apiGroups:
        - ""
      resources:
        - services
      verbs:
        - get
        - list
        - update
        - watch
    - apiGroups:
        - extensions
        - "networking.k8s.io" # k8s 1.14+
      resources:
        - ingresses
      verbs:
        - get
        - list
        - watch
    - apiGroups:
        - ""
      resources:
        - events
      verbs:
        - create
        - patch
    - apiGroups:
        - extensions
        - "networking.k8s.io" # k8s 1.14+
      resources:
        - ingresses/status
      verbs:
        - update
  ---
  apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    labels:
      app: nginx-ingress
    name: nginx-ingress
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: nginx-ingress
  subjects:
    - kind: ServiceAccount
      name: nginx-ingress
      namespace: nginx-ingress
  ---
  apiVersion: rbac.authorization.k8s.io/v1
  kind: Role
  metadata:
    labels:
      app: nginx-ingress
    name: nginx-ingress
    namespace: nginx-ingress
  rules:
    - apiGroups:
        - ""
      resources:
        - namespaces
      verbs:
        - get
    - apiGroups:
        - ""
      resources:
        - configmaps
        - pods
        - secrets
        - endpoints
      verbs:
        - get
        - list
        - watch
    - apiGroups:
        - ""
      resources:
        - services
      verbs:
        - get
        - list
        - update
        - watch
    - apiGroups:
        - extensions
        - "networking.k8s.io" # k8s 1.14+
      resources:
        - ingresses
      verbs:
        - get
        - list
        - watch
    - apiGroups:
        - extensions
        - "networking.k8s.io" # k8s 1.14+
      resources:
        - ingresses/status
      verbs:
        - update
    - apiGroups:
        - ""
      resources:
        - configmaps
      resourceNames:
        - ingress-controller-leader-nginx
      verbs:
        - get
        - update
    - apiGroups:
        - ""
      resources:
        - configmaps
      verbs:
        - create
    - apiGroups:
        - ""
      resources:
        - endpoints
      verbs:
        - create
        - get
        - update
    - apiGroups:
        - ""
      resources:
        - events
      verbs:
        - create
        - patch
  ---
  apiVersion: rbac.authorization.k8s.io/v1
  kind: RoleBinding
  metadata:
    labels:
      app: nginx-ingress
    name: nginx-ingress
    namespace: nginx-ingress
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: Role
    name: nginx-ingress
  subjects:
    - kind: ServiceAccount
      name: nginx-ingress
      namespace: nginx-ingress
  ---
  apiVersion: v1
  kind: Service
  metadata:
    labels:
      app: nginx-ingress
      component: controller
    name: nginx-ingress-controller-metrics
    namespace: nginx-ingress
  spec:
    ports:
      - name: metrics
        port: 9913
        targetPort: metrics
    selector:
      app: nginx-ingress
      component: controller
    type: "ClusterIP"
  ---
  apiVersion: v1
  kind: Service
  metadata:
    labels:
      app: nginx-ingress
      component: default-backend
    name: nginx-ingress-default-backend
    namespace: nginx-ingress
  spec:
    ports:
      - name: http
        port: 80
        protocol: TCP
        targetPort: http
    selector:
      app: nginx-ingress
      component: default-backend
    type: "ClusterIP"
  ---
  apiVersion: v1
  kind: Service
  metadata:
    labels:
      app: nginx-ingress
      component: default-backend
    name: nginx-ingress-proxy-tcp
    namespace: nginx-ingress
  spec:
    ports:
      - name: proxied-tcp
        port: 9769
        protocol: TCP
        targetPort: 9669
    selector:
      app: nginx-ingress
      component: default-backend
    type: "ClusterIP"
  ---
  apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    labels:
      app: nginx-ingress
      component: controller
    name: nginx-ingress-controller
    namespace: nginx-ingress
  spec:
    selector:
      matchLabels:
        app: nginx-ingress
        component: controller
    template:
      metadata:
        labels:
          app: nginx-ingress
          component: controller
      spec:
        dnsPolicy: ClusterFirst
        initContainers:
          - name: setsysctl
            image: busybox
            securityContext:
              privileged: true
            command:
              - sh
              - -c
              - |
                sysctl -w net.core.somaxconn=65535
                sysctl -w net.ipv4.ip_local_port_range="1024 65535"
                sysctl -w net.ipv4.tcp_tw_reuse=1
                sysctl -w fs.file-max=1048576
        containers:
          - name: nginx-ingress-controller
            image: "ccr.ccs.tencentyun.com/mirrors/nginx-ingress-controller:v0.34.1"
            imagePullPolicy: IfNotPresent
            args:
              - /nginx-ingress-controller
              - --default-backend-service=$(POD_NAMESPACE)/nginx-ingress-default-backend
              - --election-id=ingress-controller-leader
              - --ingress-class=nginx
              - --configmap=$(POD_NAMESPACE)/nginx-ingress-controller
              - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services
            securityContext:
              capabilities:
                drop:
                  - ALL
                add:
                  - NET_BIND_SERVICE
              runAsUser: 101
              allowPrivilegeEscalation: true
            env:
              - name: POD_NAME
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.name
              - name: POD_NAMESPACE
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.namespace
            livenessProbe:
              httpGet:
                path: /healthz
                port: 10254
                scheme: HTTP
              initialDelaySeconds: 10
              periodSeconds: 10
              timeoutSeconds: 1
              successThreshold: 1
              failureThreshold: 3
            ports:
              - name: http
                containerPort: 80
                protocol: TCP
              - name: proxied-tcp
                containerPort: 9769
                protocol: TCP
              - name: https
                containerPort: 443
                protocol: TCP
              - name: metrics
                containerPort: 10254
                protocol: TCP
            readinessProbe:
              httpGet:
                path: /healthz
                port: 10254
                scheme: HTTP
              initialDelaySeconds: 10
              periodSeconds: 10
              timeoutSeconds: 1
              successThreshold: 1
              failureThreshold: 3
        hostNetwork: true
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
                - matchExpressions:  //Specify labels for the node running this DaemonSet.
                    - key: nginx-ingress
                      operator: In
                      values:
                        - "true"
        serviceAccountName: nginx-ingress
        terminationGracePeriodSeconds: 60  
  ```

  !!! note

        The resource objects in the YAML file above use the namespace `nginx-ingress`. You can run `kubectl create namespace nginx-ingress` to create this namespace, or you can custom a different namespace.

2. Label a node where runs the DaemonSet named `nginx-ingress-controller` in the above YAML file (The node used in this example is named `worker2` with an IP of `192.168.8.160`).

  ```bash
  kubectl label node worker2 nginx-ingress=true
  ```

3. Run the following command to enable Nginx Ingress in the cluster you created. 

  ```bash
  kubectl create -f nginx-ingress-daemonset-hostnetwork.yaml
  ```

  Output:

  ```bash
  configmap/nginx-ingress-controller created
  configmap/tcp-services created
  serviceaccount/nginx-ingress created
  serviceaccount/nginx-ingress-backend created
  clusterrole.rbac.authorization.k8s.io/nginx-ingress created
  clusterrolebinding.rbac.authorization.k8s.io/nginx-ingress created
  role.rbac.authorization.k8s.io/nginx-ingress created
  rolebinding.rbac.authorization.k8s.io/nginx-ingress created
  service/nginx-ingress-controller-metrics created
  service/nginx-ingress-default-backend created
  service/nginx-ingress-proxy-tcp created
  daemonset.apps/nginx-ingress-controller created
  ```

  Since the network type is configured in Nginx Ingress is `hostNetwork`, after successfully deploying Nginx Ingress, you can access Nebula Graph through the IP (`192.168.8.160`) of the node where Nginx Ingress deployed and the external port (`9769`) you define.

4. Run the following command to create a console used to connect to Nebula Graph and to access Nebula Graph databases with the IP (`192.168.8.160`) of the node where Nginx Ingress deployed and the external port you define.

  ```bash
  kubectl run -ti --image vesoft/nebula-console:{{console.branch}} --restart=Never -- <nebula_console_name> -addr <host_ip> -port <external_port> -u <username> -p <password>
  ```

  Output:

  ```bash
  kubectl run -ti --image vesoft/nebula-console:{{console.branch}} --restart=Never -- nebula-console -addr 192.168.8.160 -port 9769 -u root -p vesoft
  ```

  - `--image`: The image for the tool Nebula Console used to connect to Nebula Graph databases.
  - `<nebula-console>` The custom Pod name. The above example uses `nebula-console`.
  - `-addr`: The IP of the node where Nginx Ingress is deployed. The above example uses `192.168.8.160`.
  - `-port`: The port used for external network access. The above example uses `9769`.
  - `-u`: The username of your Nebula Graph account. Before enabling authentication, you can use any existing username. The default username is root.
  - `-p`: The password of your Nebula Graph account. Before enabling authentication, you can use any characters as the password.

  A successful connection to the database is indicated if the following is returned:

  ```bash
  If you don't see a command prompt, try pressing enter.
  (root@nebula) [(none)]>
  ```