# Reclaim PVs

NebulaGraph Operator uses PVs (Persistent Volumes) and PVCs (Persistent Volume Claims) to store persistent data. If you accidentally deletes a NebulaGraph cluster, by default, PV and PVC objects and the relevant data will be retained to ensure data security.

You can also define the automatic deletion of PVCs to release data by setting the parameter `spec.enablePVReclaim` to `true` in the configuration file of the cluster instance. As for whether PV will be deleted automatically after PVC is deleted, you need to customize the PV reclaim policy. See [reclaimPolicy in StorageClass](https://kubernetes.io/docs/concepts/storage/storage-classes/#reclaim-policy) and [PV Reclaiming](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#reclaiming) for details.

## Notes

The NebulaGraph Operator currently does not support the resizing of Persistent Volume Claims (PVCs), but this feature is expected to be supported in version 1.6.1. Additionally, the Operator does not support dynamically adding or mounting storage volumes to a running storaged instance.

## Prerequisites

You have created a cluster. For how to create a cluster with Kubectl, see [Create a cluster with Kubectl](../3.deploy-nebula-graph-cluster/3.1create-cluster-with-kubectl.md). 

## Steps

The following example uses a cluster named `nebula` and the cluster's configuration file named `nebula_cluster.yaml` to show how to set `enablePVReclaim`:

1.  Run the following command to edit the `nebula` cluster's configuration file.
   
  ```bash
  kubectl edit nebulaclusters.apps.nebula-graph.io nebula
  ```

2. Add `enablePVReclaim` and set its value to `true` under `spec`.

  ```yaml
  apiVersion: apps.nebula-graph.io/v1alpha1
  kind: NebulaCluster
  metadata:
    name: nebula
  spec:
    enablePVReclaim: true  //Set its value to true.
    graphd:
      image: vesoft/nebula-graphd
      logVolumeClaim:
        resources:
          requests:
            storage: 2Gi
        storageClassName: fast-disks
      replicas: 1
      resources:
        limits:
          cpu: "1"
          memory: 1Gi
        requests:
          cpu: 500m
          memory: 500Mi
      version: {{nebula.tag}}
    imagePullPolicy: IfNotPresent
    metad:
      dataVolumeClaim:
        resources:
          requests:
            storage: 2Gi
        storageClassName: fast-disks
      image: vesoft/nebula-metad
      logVolumeClaim:
        resources:
          requests:
            storage: 2Gi
        storageClassName: fast-disks
      replicas: 1
      resources:
        limits:
          cpu: "1"
          memory: 1Gi
        requests:
          cpu: 500m
          memory: 500Mi
      version: {{nebula.tag}}
    nodeSelector:
      nebula: cloud
    reference:
      name: statefulsets.apps
      version: v1
    schedulerName: default-scheduler
    storaged:
      dataVolumeClaims:
      - resources:
          requests:
            storage: 2Gi
        storageClassName: fast-disks
      - resources:
          requests:
            storage: 2Gi
        storageClassName: fast-disks
      image: vesoft/nebula-storaged
      logVolumeClaim:
        resources:
          requests:
            storage: 2Gi
        storageClassName: fast-disks
      replicas: 3
      resources:
        limits:
          cpu: "1"
          memory: 1Gi
        requests:
          cpu: 500m
          memory: 500Mi
      version: {{nebula.tag}}
  ...    
  ```

3. Run `kubectl apply -f nebula_cluster.yaml` to push your configuration changes to the cluster.
