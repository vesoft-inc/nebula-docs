# Deploy NebulaGraph clusters with Kubectl

!!! Compatibility "Legacy version compatibility"

    The 1.x version NebulaGraph Operator is not compatible with NebulaGraph of version below v3.x.

## Prerequisites

- [You have installed NebulaGraph Operator](../2.deploy-nebula-operator.md)

- [You have created StorageClass](https://kubernetes.io/docs/concepts/storage/storage-classes/)
  
{{ ent.ent_begin }}

- [LM has been installed and the License Key has been successfully loaded](3.0.deploy-lm.md) (Enterprise only)

{{ ent.ent_end }}

## Create clusters

The following example shows how to create a NebulaGraph cluster by creating a cluster named `nebula`.

1. Create a namespace, for example, `nebula`. If not specified, the `default` namespace is used.

  ```bash
  kubectl create namespace nebula
  ```

  {{ent.ent_begin}}
2. Create a Secret for pulling the NebulaGraph Enterprise image from a private repository.

  !!! note
        Skip this step if you are using NebulaGraph Community Edition.

  ```bash
  kubectl -n <nebula> create secret docker-registry <image-pull-secret> \
  --docker-server=DOCKER_REGISTRY_SERVER \
  --docker-username=DOCKER_USER \
  --docker-password=DOCKER_PASSWORD
  ```

  - `<nebula>`: The namespace where this Secret will be stored.
  - `<image-pull-secret>`: Specify the name of the Secret.
  - `DOCKER_REGISTRY_SERVER`: Specify the server address of the private repository from which the image will be pulled, such as `reg.example-inc.com`.
  - `DOCKER_USER`: The username for the image repository.
  - `DOCKER_PASSWORD`: The password for the image repository.

  {{ent.ent_end}}

3. Create a file named `apps_v1alpha1_nebulacluster.yaml`.

  - For a NebulaGraph Community cluster
  
    For the file content, see the [sample configuration](https://github.com/vesoft-inc/nebula-operator/blob/v{{operator.release}}/config/samples/apps_v1alpha1_nebulacluster.yaml).

    ??? Info "Expand to show sample parameter descriptions"

        | Parameter    | Default value  | Description    |
        | :---- | :--- | :--- |
        | `metadata.name`              | -                                                            | The name of the created NebulaGraph cluster. |
        |`spec.console`|-| Configuration of the Console service. For details, see [nebula-console](https://github.com/vesoft-inc/nebula-operator/blob/v{{operator.release}}/doc/user/nebula_console.md#nebula-console).|
        | `spec.graphd.replicas`       | `1`                                                          | The numeric value of replicas of the Graphd service.         |
        | `spec.graphd.image`         | `vesoft/nebula-graphd`                                       | The container image of the Graphd service.      |
        | `spec.graphd.version`        | `{{nebula.tag}}`                                          | The version of the Graphd service.         |
        | `spec.graphd.service`        | -                                                            | The Service configurations for the Graphd service.      |
        | `spec.graphd.logVolumeClaim.storageClassName`   | -                                         | The log disk storage configurations for the Graphd service.         |
        | `spec.metad.replicas`        | `1`                                                          | The numeric value of replicas of the Metad service.          |
        | `spec.metad.image`          | `vesoft/nebula-metad`                                        | The container image of the Metad service.        |
        | `spec.metad.version`         | `{{nebula.tag}}`                                          | The version of the Metad service.          |
        | `spec.metad.dataVolumeClaim.storageClassName`    | -                                        | The data disk storage configurations for the Metad service.          |
        | `spec.metad.logVolumeClaim.storageClassName`|-                                              | The log disk storage configurations for the Metad service.|
        | `spec.storaged.replicas`     | `3`                                                          | The numeric value of replicas of the Storaged service.       |
        | `spec.storaged.image`       | `vesoft/nebula-storaged`                                     | The container image of the Storaged service.     |
        | `spec.storaged.version`      | `{{nebula.tag}}`                                          | The version of the Storaged service.       |
        | `spec.storaged.dataVolumeClaims.resources.requests.storage` | - | Data disk storage size for the Storaged service. You can specify multiple data disks to store data. When multiple disks are specified, the storage path is `/usr/local/nebula/data1`, `/usr/local/nebula/data2`, etc.|
        | `spec.storaged.dataVolumeClaims.resources.storageClassName` | - | The data disk storage configurations for Storaged. If not specified, the global storage parameter is applied. |
        | `spec.storaged.logVolumeClaim.storageClassName`|-                                           | The log disk storage configurations for the Storaged service.|
        | `spec.storaged.enableAutoBalance` | `true` |Whether to balance data automatically. |
        |`spec.agent`|`{}`| Configuration of the Agent service. This is used for backup and recovery as well as log cleanup functions. If you do not customize this configuration, the default configuration will be used.|
        | `spec.reference.name`        | -                                                            | The name of the dependent controller.           |
        | `spec.schedulerName`         | -                                                            | The scheduler name.                 |
        | `spec.imagePullPolicy`       | The image policy to pull the NebulaGraph image. For details, see [Image pull policy](https://kubernetes.io/docs/concepts/containers/images/#image-pull-policy). | The image pull policy in Kubernetes.               |
        |`spec.logRotate`| - |Log rotation configuration. For more information, see [Manage cluster logs](../8.custom-cluster-configurations/8.4.manage-running-logs.md).|
        |`spec.enablePVReclaim`|`false`|Define whether to automatically delete PVCs and release data after deleting the cluster. For more information, see [Reclaim PVs](../8.custom-cluster-configurations/8.2.pv-reclaim.md).|

  {{ ent.ent_begin }}

  - For a NebulaGraph Enterprise cluster

    Contact our sales team to get a complete NebulaGraph Enterprise Edition cluster YAML example.

    !!! enterpriseonly

        Make sure that you have access to NebulaGraph Enterprise Edition images before pulling the image. 

    === "Cluster without Zones"

        You must set the following parameters in the configuration file for the enterprise edition. Other parameters can be changed as needed. For information on other parameters, see the [sample configuration](https://github.com/vesoft-inc/nebula-operator/blob/v{{operator.release}}/config/samples/apps_v1alpha1_nebulacluster.yaml). 


        | Parameter    | Default value  | Description    |
        | :---- | :--- | :--- |
        | `spec.metad.licenseManagerURL` | - | Configure the URL that points to the LM, which consists of the access address and port number (default port `9119`) of the LM. For example, `192.168.8.100:9119`. **You must configure this parameter in order to obtain the license information; otherwise, the enterprise edition cluster cannot be used.** |
        |`spec.<graphd|metad|storaged>.image`|-|The container image of the Graph, Meta, or Storage service of the enterprise edition.|
        |`spec.imagePullSecrets`| - |Specifies the Secret for pulling the NebulaGraph Enterprise service images from a private repository.|


    === "Cluster with Zones"

        NebulaGraph Operator supports creating a cluster with [Zones](../../4.deployment-and-installation/5.zone.md). 
        
        You must set the following parameters for creating a cluster with Zones. Other parameters can be changed as needed. For more information on other parameters, see the [sample configuration](https://github.com/vesoft-inc/nebula-operator/blob/v{{operator.release}}/config/samples/apps_v1alpha1_nebulacluster.yaml). 

        | Parameter    | Default value  | Description    |
        | :---- | :--- | :--- |
        | `spec.metad.licenseManagerURL` | - | Configure the URL that points to the LM, which consists of the access address and port number (default port `9119`) of the LM. For example, `192.168.8.100:9119`. **You must configure this parameter in order to obtain the license information; otherwise, the enterprise edition cluster cannot be used.** |
        |`spec.<graphd|metad|storaged>.image`|-|The container image of the Graph, Meta, or Storage service of the enterprise edition.|
        |`spec.imagePullSecrets`| - |Specifies the Secret for pulling the NebulaGraph Enterprise service images from a private repository.|
        |`spec.alpineImage`|`reg.vesoft-inc.com/nebula-alpine:latest`|The Alpine Linux image, used to obtain the Zone information where nodes are located.| 
        |`spec.metad.config.zone_list`|-|A list of zone names, split by comma. For example: zone1,zone2,zone3. <br/>**Zone names CANNOT be modified once be set.**|
        |`spec.graphd.config.prioritize_intra_zone_reading`|`false`|Specifies whether to prioritize sending queries to the storage nodes in the same zone.<br/>When set to `true`, the query is sent to the storage nodes in the same zone. If reading fails in that Zone, it will decide based on `stick_to_intra_zone_on_failure` whether to read the leader partition replica data from other Zones. |
        |`spec.graphd.config.stick_to_intra_zone_on_failure`|`false`|Specifies whether to stick to intra-zone routing if unable to find the requested partitions in the same zone. When set to `true`, if unable to find the partition replica in that Zone, it does not read data from other Zones.|

        ???+ note "Learn more about Zones in NebulaGraph Operator"

            **Understanding NebulaGraph's Zone Feature**

            NebulaGraph utilizes a feature called Zones to efficiently manage its distributed architecture. Each Zone represents a logical grouping of Storage pods and Graph pods, responsible for storing the complete graph space data. The data within NebulaGraph's spaces is partitioned, and replicas of these partitions are evenly distributed across all available Zones. The utilization of Zones can significantly reduce inter-Zone network traffic costs and boost data transfer speeds. Moreover, intra-zone-reading allows for increased availability, because replicas of a partition spread out among different zones.

            **Configuring NebulaGraph Zones**

            To make the most of the Zone feature, you first need to determine the actual Zone where your cluster nodes are located. Typically, nodes deployed on cloud platforms are labeled with their respective Zones. Once you have this information, you can configure it in your cluster's configuration file by setting the `spec.metad.config.zone_list` parameter. This parameter should be a list of Zone names, separated by commas, and should match the actual Zone names where your nodes are located. For example, if your nodes are in Zones `az1`, `az2`, and `az3`, your configuration would look like this:

            ```yaml
            spec:
              metad:
                config:
                  zone_list: az1,az2,az3
            ```

            **Operator's Use of Zone Information**

            NebulaGraph Operator leverages Kubernetes' [TopoloySpread](https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/) feature to manage the scheduling of Storage and Graph pods. Once the `zone_list` is configured, Storage services are automatically assigned to their respective Zones based on the `topology.kubernetes.io/zone` label.

            For intra-zone data access, the Graph service dynamically assigns itself to a Zone using the `--assigned_zone=$NODE_ZONE` parameter. It identifies the Zone name of the node where the Graph service resides by utilizing an init-container to fetch this information. The Alpine Linux image specified in `spec.alpineImage` (default: `reg.vesoft-inc.com/nebula-alpine:latest`) plays a role in obtaining Zone information.

            **Prioritizing Intra-Zone Data Access**

            By setting `spec.graphd.config.prioritize_intra_zone_reading` to `true` in the cluster configuration file, you enable the Graph service to prioritize sending queries to Storage services within the same Zone. In the event of a read failure within that Zone, the behavior depends on the value of `spec.graphd.config.stick_to_intra_zone_on_failure`. If set to `true`, the Graph service avoids reading data from other Zones and returns an error. Otherwise, it reads data from leader partition replicas in other Zones.

            ```yaml
            spec:
              alpineImage: reg.vesoft-inc.com/cloud-dev/nebula-alpine:latest
              graphd:
                config:
                  prioritize_intra_zone_reading: "true"
                  stick_to_intra_zone_on_failure: "false"
            ```

            **Zone Mapping for Resilience**

            Once Storage and Graph services are assigned to Zones, the mapping between the pod and its corresponding Zone is stored in a configmap named `<cluster_name>-graphd|storaged-zone`. This mapping facilitates pod scheduling during rolling updates and pod restarts, ensuring that services return to their original Zones as needed.    

            !!! caution

                DO NOT manually modify the configmaps created by NebulaGraph Operator. Doing so may cause unexpected behavior.  


        Other optional parameters for the enterprise edition are as follows:

        | Parameter    | Default value  | Description    |
        | :---- | :--- | :--- |
        |`spec.storaged.enableAutoBalance`| `false`| Specifies whether to enable automatic data balancing. For more information, see [Balance storage data after scaling out](../8.custom-cluster-configurations/8.3.balance-data-when-scaling-storage.md).|
        |`spec.enableBR`|`false`|Specifies whether to enable the BR tool. For more information, see [Backup and restore](../10.backup-restore-using-operator.md).|
        |`spec.graphd.enable_graph_ssl`|`false`| Specifies whether to enable SSL for the Graph service. For more details, see [Enable mTLS](../8.custom-cluster-configurations/8.5.enable-ssl.md). |


        ??? info "Expand to view sample cluster configurations"

            ```yaml
            apiVersion: apps.nebula-graph.io/v1alpha1
            kind: NebulaCluster
            metadata:
              name: nebula
              namespace: default
            spec:
              alpineImage: "reg.vesoft-inc.com/cloud-dev/nebula-alpine:latest"
              agent:
                image: reg.vesoft-inc.com/cloud-dev/nebula-agent
                version: v3.6.0-sc
              exporter:
                image: vesoft/nebula-stats-exporter
                replicas: 1
                maxRequests: 20
              console:
                version: "nightly"
              graphd:
                config:
                  accept_partial_success: "true"
                  ca_client_path: certs/root.crt
                  ca_path: certs/root.crt
                  cert_path: certs/server.crt
                  key_path: certs/server.key
                  enable_graph_ssl: "true"
                  enable_intra_zone_reading: "true"
                  stick_to_intra_zone_on_failure: "true" 
                  logtostderr: "1"
                  redirect_stdout: "false"
                  stderrthreshold: "0" 
                  enable_graph_ssl: "true"
                initContainers:
                  - name: init-auth-sidecar
                    imagePullPolicy: IfNotPresent
                    image: 496756745489.dkr.ecr.us-east-1.amazonaws.com/auth-sidecar:v1.60.0
                    env:
                      - name: AUTH_SIDECAR_CONFIG_FILENAME
                        value: sidecar-init
                    volumeMounts:
                      - name: credentials 
                        mountPath: /credentials 
                      - name: auth-sidecar-config 
                        mountPath: /etc/config 
                sidecarContainers:
                  - name: auth-sidecar
                    image: 496756745489.dkr.ecr.us-east-1.amazonaws.com/auth-sidecar:v1.60.0
                    imagePullPolicy: IfNotPresent
                    resources:
                      requests:
                        cpu: 100m
                        memory: 500Mi
                    env:
                      - name: LOCAL_POD_IP
                        valueFrom:
                          fieldRef:
                            fieldPath: status.podIP
                      - name: LOCAL_POD_NAME
                        valueFrom:
                          fieldRef:
                            fieldPath: metadata.name
                      - name: LOCAL_POD_NAMESPACE
                        valueFrom:
                          fieldRef:
                            fieldPath: metadata.namespace
                    readinessProbe:
                      httpGet:
                        path: /ready
                        port: 8086
                      initialDelaySeconds: 5
                      periodSeconds: 10
                      successThreshold: 1
                      failureThreshold: 3
                    livenessProbe:
                      httpGet:
                        path: /live
                        port: 8086
                      initialDelaySeconds: 5
                      periodSeconds: 10
                      successThreshold: 1
                      failureThreshold: 3
                    volumeMounts:
                      - name: credentials 
                        mountPath: /credentials 
                      - name: auth-sidecar-config 
                        mountPath: /etc/config 
                volumes: 
                  - name: credentials
                    emptyDir:
                      medium: Memory
                volumeMounts: 
                  - name: credentials
                    mountPath: /usr/local/nebula/certs
                resources:
                  requests:
                    cpu: "2"
                    memory: "2Gi"
                  limits:
                    cpu: "2"
                    memory: "2Gi"
                replicas: 1
                image: reg.vesoft-inc.com/rc/nebula-graphd-ent
                version: v3.5.0-sc
              metad:
                config:
                  redirect_stdout: "false"
                  stderrthreshold: "0"
                  logtostder: "true"
                  # Zone names CANNOT be modified once set.
                  # It's suggested to set an odd number of Zones.
                  zone_list: az1,az2,az3 
                licenseManagerURL: "192.168.8.xxx:9119"
                resources:
                  requests:
                    cpu: "300m"
                    memory: "500Mi"
                  limits:
                    cpu: "1"
                    memory: "1Gi"
                replicas: 3
                image: reg.vesoft-inc.com/rc/nebula-metad-ent
                version: v3.5.0-sc
                dataVolumeClaim:
                  resources:
                    requests:
                      storage: 2Gi
                  storageClassName: local-path
              storaged:
                config:
                  redirect_stdout: "false"
                  stderrthreshold: "0"
                  logtostder: "true"
                resources:
                  requests:
                    cpu: "1"
                    memory: "1Gi"
                  limits:
                    cpu: "2"
                    memory: "2Gi"
                replicas: 3
                image: reg.vesoft-inc.com/rc/nebula-storaged-ent
                version: v3.5.0-sc
                dataVolumeClaims:
                - resources:
                    requests:
                      storage: 2Gi
                  storageClassName: local-path
                enableAutoBalance: true
              reference:
                name: statefulsets.apps
                version: v1
              schedulerName: nebula-scheduler
              nodeSelector:
                nebula: cloud
              imagePullPolicy: Always
              imagePullSecrets:
              - name: nebula-image
              topologySpreadConstraints:
              - topologyKey: "topology.kubernetes.io/zone"
                whenUnsatisfiable: "DoNotSchedule"
            ```

  {{ ent.ent_end }}

1. Create a NebulaGraph cluster.

  ```bash
  kubectl create -f apps_v1alpha1_nebulacluster.yaml
  ```

  Output:

  ```bash
  nebulacluster.apps.nebula-graph.io/nebula created
  ```

5. Check the status of the NebulaGraph cluster.
   
  ```bash
  kubectl get nebulaclusters.apps.nebula-graph.io nebula
  ```

  Output:

  ```bash
  NAME     GRAPHD-DESIRED   GRAPHD-READY   METAD-DESIRED   METAD-READY   STORAGED-DESIRED   STORAGED-READY   AGE
  nebula   1                1              1               1             3                  3                86s
  ```

## Scaling clusters

- The cluster scaling feature is for NebulaGraph Enterprise Edition only. 

{{ ent.ent_begin }}

- Scaling a NebulaGraph cluster for Enterprise Edition is supported only with NebulaGraph Operator version 1.1.0 or later. 

You can modify the value of `replicas` in `apps_v1alpha1_nebulacluster.yaml` to scale a NebulaGraph cluster.

### Scale out clusters

The following shows how to scale out a NebulaGraph cluster by changing the number of Storage services to 5:

1. Change the value of the `storaged.replicas` from `3` to `5` in `apps_v1alpha1_nebulacluster.yaml`.

  ```yaml
    storaged:
      resources:
        requests:
          cpu: "500m"
          memory: "500Mi"
        limits:
          cpu: "1"
          memory: "1Gi"
      replicas: 5
      image: vesoft/nebula-storaged
      version: {{nebula.tag}}
      dataVolumeClaims:
      - resources:
          requests:
            storage: 2Gi
        storageClassName: fast-disks
      - resources:
          requests:
            storage: 2Gi
        storageClassName: fast-disks
      logVolumeClaim:
        resources:
          requests:
            storage: 2Gi
        storageClassName: fast-disks
    reference:
      name: statefulsets.apps
      version: v1
    schedulerName: default-scheduler
  ```

2. Run the following command to update the NebulaGraph cluster CR.

  ```bash
  kubectl apply -f apps_v1alpha1_nebulacluster.yaml
  ```
  
3. Check the number of Storage services.

  ```bash
  kubectl get pods -l app.kubernetes.io/cluster=nebula
  ```

  Output:

  ```bash
  NAME                READY   STATUS    RESTARTS   AGE
  nebula-graphd-0     1/1     Running   0          2m
  nebula-metad-0      1/1     Running   0          2m
  nebula-storaged-0   1/1     Running   0          2m
  nebula-storaged-1   1/1     Running   0          2m
  nebula-storaged-2   1/1     Running   0          2m
  nebula-storaged-3   1/1     Running   0          5m
  nebula-storaged-4   1/1     Running   0          5m
  ```

  As you can see above, the number of Storage services is scaled up to 5.

### Scale in clusters

The principle of scaling in a cluster is the same as scaling out a cluster. You scale in a cluster if the numeric value of the `replicas` in `apps_v1alpha1_nebulacluster.yaml` is changed smaller than the current number. For more information, see the **Scale out clusters** section above.

!!! caution

    NebulaGraph Operator currently only supports scaling Graph and Storage services and does not support scale Meta services.

{{ ent.ent_end }}

## Delete clusters

Run the following command to delete a NebulaGraph cluster with Kubectl:

```bash
kubectl delete -f apps_v1alpha1_nebulacluster.yaml
```

## What's next

[Connect to NebulaGraph databases](../4.connect-to-nebula-graph-service.md)
